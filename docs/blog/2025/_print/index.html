<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://gardener.cloud/blog/2025/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/blog/2025/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>2025 | Gardener</title><meta name=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:url" content="https://gardener.cloud/blog/2025/"><meta property="og:site_name" content="Gardener"><meta property="og:title" content="2025"><meta property="og:description" content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:locale" content="en_US"><meta property="og:type" content="website"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="2025"><meta itemprop=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta itemprop=datePublished content="2025-06-25T00:00:00+00:00"><meta itemprop=dateModified content="2025-06-25T00:00:00+00:00"><meta itemprop=image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="2025"><meta name=twitter:description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><link rel=preload href=/scss/main.min.64d56283aba037cc3a217d684edadfb4e3c57ca54122947d2f030f74bcd28a27.css as=style integrity="sha256-ZNVig6ugN8w6IX1oTtrftOPFfKVBIpR9LwMPdLzSiic=" crossorigin=anonymous><link href=/scss/main.min.64d56283aba037cc3a217d684edadfb4e3c57ca54122947d2f030f74bcd28a27.css rel=stylesheet integrity="sha256-ZNVig6ugN8w6IX1oTtrftOPFfKVBIpR9LwMPdLzSiic=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class="td-section td-blog"><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg width="90" height="90" viewBox="0 0 90 90" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009F76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=navbar-brand__name>Gardener</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://demo.gardener.cloud target=_blank rel=noopener><span>Demo</span></a></li><li class=nav-item><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class=nav-item><a class=nav-link href=/docs><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/blog><span>Blogs</span></a></li><li class=nav-item><a class=nav-link href=/community><span>Community</span></a></li><li class=nav-item><a class=nav-link href=https://join.slack.com/t/gardener-cloud/shared_invite/zt-33c9daems-3oOorhnqOSnldZPWqGmIBw target=_blank rel=noopener><span>Join us on</span></a></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site‚Ä¶" aria-label="Search this site‚Ä¶" autocomplete=off data-offline-search-index-json-src=/offline-search-index.e3780e2fc3fb3876bd51a1d4edb6c0f2.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 ps-md-5 pe-md-4" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/blog/2025/>Return to the regular view of this page</a>.</p></div><h1 class=title>2025</h1><div class=content></div></div><div class=td-content><h1 id=pg-8db05f0fa0ec943dfa70591116baa517>Enabling Seamless IPv4 to Dual-Stack Migration for Kubernetes Clusters on GCP</h1><div class="td-byline mb-4"><time datetime=2025-06-25 class=text-body-secondary>Wednesday, June 25, 2025</time></div><p>Gardener continues to enhance its networking capabilities, now offering a streamlined migration path for existing IPv4-only shoot clusters on Google Cloud Platform (GCP) to dual-stack (IPv4 and IPv6). This allows clusters to leverage the benefits of IPv6 networking while maintaining IPv4 compatibility.</p><h3 id=the-shift-to-dual-stack-what-changes>The Shift to Dual-Stack: What Changes?<a class=td-heading-self-link href=#the-shift-to-dual-stack-what-changes aria-label="Heading self-link"></a></h3><p>Transitioning a Gardener-managed Kubernetes cluster on GCP from a single-stack IPv4 to a dual-stack setup involves several key modifications to the underlying infrastructure and networking configuration.</p><h4 id=triggering-the-migration>Triggering the Migration<a class=td-heading-self-link href=#triggering-the-migration aria-label="Heading self-link"></a></h4><p>The migration process is initiated by updating the shoot specification. Users simply need to add <code>IPv6</code> to the <code>spec.networking.ipFamilies</code> field, changing it from <code>[IPv4]</code> to <code>[IPv4, IPv6]</code>.</p><h4 id=infrastructure-adaptations>Infrastructure Adaptations<a class=td-heading-self-link href=#infrastructure-adaptations aria-label="Heading self-link"></a></h4><p>Once triggered, Gardener orchestrates the necessary infrastructure changes on GCP:</p><ul><li><strong>IPv6-Enabled Subnets:</strong> The existing subnets (node subnet and internal subnet) within the Virtual Private Cloud (VPC) get external IPv6 ranges assigned.</li><li><strong>New IPv6 Service Subnet:</strong> A new subnet is provisioned specifically for services, also equipped with an external IPv6 range.</li><li><strong>Secondary IPv4 Range for Node Subnet:</strong> The node subnet is allocated an additional (secondary) IPv4 range. This is crucial as dual-stack load balancing on GCP, is managed via <code>ingress-gce</code>, which utilizes alias IP ranges.</li></ul><h4 id=enhanced-pod-routing-on-gcp>Enhanced Pod Routing on GCP<a class=td-heading-self-link href=#enhanced-pod-routing-on-gcp aria-label="Heading self-link"></a></h4><p>A significant change occurs in how pod traffic is routed. In the IPv4-only setup with native routing, the Cloud Controller Manager (CCM) creates individual routes in the VPC route table for each node&rsquo;s pod CIDR. During the migration to dual-stack:</p><ul><li>These existing pod-specific cloud routes are systematically deleted from the VPC routing table.</li><li>To maintain connectivity, the corresponding alias IP ranges are directly added to the Network Interface Card (NICs) of the Kubernetes worker nodes (VM instances).</li></ul><h3 id=the-migration-journey>The Migration Journey<a class=td-heading-self-link href=#the-migration-journey aria-label="Heading self-link"></a></h3><p>The migration is a multi-phase process, tracked by a <code>DualStackNodesMigrationReady</code> constraint in the shoot&rsquo;s status, which gets removed after a successfull migration.</p><ul><li><p><strong>Phase 1: Infrastructure Preparation</strong>
Immediately after the <code>ipFamilies</code> field is updated, an infrastructure reconciliation begins. This phase includes the subnet modifications mentioned above. A critical step here is the transition from VPC routes to alias IP ranges for existing nodes. The system carefully manages the deletion of old routes and the creation of new alias IP ranges on the virtual machines to ensure a smooth transition. Information about the routes to be migrated is temporarily persisted during this step in the infrastructure state.</p></li><li><p><strong>Phase 2: Node Upgrades</strong>
For nodes to become dual-stack aware (i.e., receive IPv6 addresses for themselves and their pods), they need to be rolled out. This can happen during the next scheduled Kubernetes version or gardenlinux update or can be expedited by manually deleting the nodes, allowing Gardener to recreate the nodes with a new dual-stack configuration. Once all nodes have been updated and posses IPv4 and IPv6 pod CIDRs, the <code>DualStackNodesMigrationReady</code> constraint will change to <code>True</code>.</p></li><li><p><strong>Phase 3: Finalizing Dual-Stack Activation</strong>
With the infrastructure and nodes prepared, the final step involves configuring the remaining control plane components like kube-apiserver and the Container Network Interface (CNI) plugin like Calico or Cilium for dual-stack operation. After these components are fully dual-stack enabled, the migration constraint is removed, and the cluster operates in a full dual-stack mode. Existing IPv4 pods keep their IPv4 address, new ones receive both (IPv4 and IPv6) addresses.</p></li></ul><h3 id=important-considerations-for-gcp-users>Important Considerations for GCP Users<a class=td-heading-self-link href=#important-considerations-for-gcp-users aria-label="Heading self-link"></a></h3><p>Before initiating the migration, please note the following:</p><ul><li><strong>Native Routing Prerequisite:</strong> The IPv4-only cluster must be operating in native routing mode this. This means the pod overlay network needs to be disabled.</li><li><strong>GCP Route Quotas:</strong> When using native routing, especially for larger clusters, be mindful of GCP&rsquo;s default quota for static routes per VPC (often 200, referred to as <code>STATIC_ROUTES_PER_NETWORK</code>). It might be necessary to request a quota increase via the GCP cloud console before enabling native routing or migrating to dual-stack to avoid hitting limits.</li></ul><p>This enhancement provides a clear path for Gardener users on GCP to adopt IPv6, paving the way for future-ready network architectures.</p><p>For further details, you can refer to the <a href=https://github.com/gardener/gardener-extension-provider-gcp/pull/1010>official pull request</a> and the <a href="https://youtu.be/HguO_KY86ac?t=82">relevant segment of the developer talk</a>. Additional documentation can also be found within the <a href=https://gardener.cloud/docs/gardener/networking/dual-stack-networking-migration/>Gardener documentation</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-402c3d3752ec5582c19acc98c47deff5>June</h1><div class="td-byline mb-4"><time datetime=2025-06-17 class=text-body-secondary>Tuesday, June 17, 2025</time></div></div><div class=td-content><h1 id=pg-972a525e60e64cc80b18520614b6e451>Taking Gardener to the Next Level: Highlights from the 7th Gardener Community Hackathon in Schelklingen</h1><div class="td-byline mb-4"><time datetime=2025-06-17 class=text-body-secondary>Tuesday, June 17, 2025</time></div><h1 id=taking-gardener-to-the-next-level-highlights-from-the-7th-gardener-community-hackathon-in-schelklingen>Taking Gardener to the Next Level: Highlights from the 7th Gardener Community Hackathon in Schelklingen<a class=td-heading-self-link href=#taking-gardener-to-the-next-level-highlights-from-the-7th-gardener-community-hackathon-in-schelklingen aria-label="Heading self-link"></a></h1><p>The latest &ldquo;Hack The Garden&rdquo; event, held in June 2025 at <a href=https://schlosshof-info.de/>Schlosshof in Schelklingen</a>, brought together members of the Gardener community for an intensive week of collaboration, coding, and problem-solving.
The hackathon focused on a wide array of topics aimed at enhancing Gardener&rsquo;s capabilities, modernizing its stack, and improving user experience.
You can find a <a href=https://github.com/gardener-community/hackathon/blob/main/2025-06_Schelklingen/README.md>full summary of all topics on GitHub</a> and watch the <a href=https://youtu.be/TCLXovw43HA>wrap-up presentations on YouTube</a>.</p><p><img src=/blog/2025/06/images/group-picture.png alt="Group picture of the 7th Hack the Garden event in Schelklingen"></p><p>Here&rsquo;s a look at some of the key achievements and ongoing efforts:</p><h2 id=-modernizing-core-infrastructure-and-networking>üöÄ Modernizing Core Infrastructure and Networking<a class=td-heading-self-link href=#-modernizing-core-infrastructure-and-networking aria-label="Heading self-link"></a></h2><p>A significant focus was on upgrading and refining Gardener&rsquo;s foundational components.</p><p>One major undertaking was the <strong>replacement of <a href=https://openvpn.net/>OpenVPN</a> with <a href=https://www.wireguard.com/>Wireguard</a></strong> (<a href="https://youtu.be/TCLXovw43HA?t=104s">watch presentation</a>).
The goal is to modernize the VPN stack for communication between control and data planes, leveraging Wireguard&rsquo;s reputed performance and simplicity.
OpenVPN, while established, presents challenges like TCP-in-TCP. The team developed a Proof of Concept (POC) for a Wireguard-based VPN connection for a single shoot in a local setup, utilizing a reverse proxy like <a href=https://github.com/apernet/mwgp>mwgp</a> to manage connections without needing a load balancer per control plane.
A <a href=https://github.com/axel7born/vpn2/blob/wireguard/docs/wireguard.md>document describing the approach</a> is available.
Next steps involve thorough testing of resilience and throughput, aggregating secrets for MWGP configuration, and exploring ways to update MWGP configuration without restarts.
Code contributions can be found in forks of <a href=https://github.com/axel7born/gardener/tree/wireguard>gardener</a>, <a href=https://github.com/axel7born/vpn2/tree/wireguard>vpn2</a>, and <a href=https://github.com/majst01/mwgp>mwgp</a>.</p><p>Another critical area is <strong>overcoming the 450 <code>Node</code> limit on Azure</strong> (<a href="https://youtu.be/TCLXovw43HA?t=3076s">watch presentation</a>).
Current Azure networking for Gardener relies on route tables, which have size limitations.
The team analyzed the hurdles and discussed a potential solution involving a combination of route tables and virtual networks.
Progress here depends on an upcoming Azure preview feature.</p><p>The hackathon also saw progress on <strong>cluster-internal L7 Load-Balancing for <code>kube-apiserver</code>s</strong>.
Building on previous work for external endpoints, this initiative aims to provide L7 load-balancing for internal traffic from components like <code>gardener-resource-manager</code>.
Achievements include an implementation leveraging generic token kubeconfig and a dedicated ClusterIP service for Istio ingress gateway pods.
The <a href=https://github.com/gardener/gardener/pull/12260>PR #12260</a> is awaiting review to merge this improvement, addressing <a href=https://github.com/gardener/gardener/issues/8810>issue #8810</a>.</p><h2 id=-enhancing-observability-and-operations>üî≠ Enhancing Observability and Operations<a class=td-heading-self-link href=#-enhancing-observability-and-operations aria-label="Heading self-link"></a></h2><p>Improving how users monitor and manage Gardener clusters was another key theme.</p><p>A significant step towards Gardener&rsquo;s <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/34-observability2.0-opentelemtry-operator-and-collectors.md>Observability 2.0 initiative</a> was made with the <strong>OpenTelemetry Transport for <code>Shoot</code> Metrics</strong> (<a href="https://youtu.be/TCLXovw43HA?t=808s">watch presentation</a>).
The current method of collecting shoot metrics via the Kubernetes API server <code>/proxy</code> endpoint lacks fine-tuning capabilities.
The hackathon proved the viability of collecting and filtering shoot metrics via OpenTelemetry collector instances on shoots, transporting them to Prometheus OTLP ingestion endpoints on seeds. This allows for more flexible and modern metrics collection.</p><p>For deeper network insights, the <strong>Cluster Network Observability</strong> project (<a href="https://youtu.be/TCLXovw43HA?t=1342s">watch presentation</a>) enhanced the <a href=https://github.com/microsoft/retina>Retina tool</a> by Microsoft.
The team successfully added labeling for source and destination availability zones to Retina&rsquo;s traffic metrics (<a href=https://github.com/microsoft/retina/issues/1654>see issue #1654</a> and <a href=https://github.com/microsoft/retina/pull/1657>PR #1657</a>).
This will help identify cross-AZ traffic, potentially reducing costs and latency.</p><p>To support lightweight deployments, efforts were made to <strong>make <code>gardener-operator</code> Single-Node Ready</strong> (<a href="https://youtu.be/TCLXovw43HA?t=511s">watch presentation</a>).
This involved making several components, including Prometheus deployments, configurable to reduce resource overhead in single-node or bare-metal scenarios.
Relevant PRs include those for <a href=https://github.com/gardener/gardener-extension-provider-gcp/pull/1052>gardener-extension-provider-gcp #1052</a>, <a href=https://github.com/gardener/gardener-extension-provider-openstack/pull/1042>gardener-extension-provider-openstack #1042</a>, <a href=https://github.com/fluent/fluent-operator/pull/1616>fluent-operator #1616</a>, and <a href=https://github.com/gardener/gardener/pull/12248>gardener #12248</a>, along with fixes in forked Cortex and Vali repositories.</p><p>Streamlining node management was the focus of the <strong>Worker Group Node Roll-out</strong> project (<a href="https://youtu.be/TCLXovw43HA?t=3806s">watch presentation</a>).
A PoC was created (see <a href=https://github.com/rrhubenov/gardener/tree/worker-pool-rollout>rrhubenov/gardener branch</a>) allowing users to trigger a node roll-out for specific worker groups via a shoot annotation (<code>gardener.cloud/operation=rollout-workers=&lt;pool-names></code>), which is particularly useful for scenarios like dual-stack migration.</p><p>Proactive workload management is the aim of the <strong>Instance Scheduled Events Watcher</strong> (<a href="https://youtu.be/TCLXovw43HA?t=4010s">watch presentation</a>).
This initiative seeks to create an agent that monitors cloud provider VM events (like reboots or retirements) and exposes them as node events or dashboard warnings.
A <a href=https://github.com/kubernetes-sigs/cloud-provider-azure/pull/9170>PR #9170 for cloud-provider-azure</a> was raised to enable this for Azure, allowing users to take timely action.</p><h2 id=-bolstering-security-and-resource-management>üõ°Ô∏è Bolstering Security and Resource Management<a class=td-heading-self-link href=#-bolstering-security-and-resource-management aria-label="Heading self-link"></a></h2><p>Security and efficient resource handling remain paramount.</p><p>The <strong>Signing of <code>ManagedResource</code> Secrets</strong> project (<a href="https://youtu.be/TCLXovw43HA?t=1556s">watch presentation</a>) addressed a potential privilege escalation vector.
A PoC demonstrated that signing <code>ManagedResource</code> secrets with a key known only to the Gardener Resource Manager (GRM) is feasible, allowing GRM to verify secret integrity.
This work is captured in <a href=https://github.com/gardener/gardener/pull/12247>gardener PR #12247</a>.</p><p>Simplifying operations was the goal of <strong>Migrating Control Plane Reconciliation of Provider Extensions to <code>ManagedResource</code>s</strong> (<a href="https://youtu.be/TCLXovw43HA?t=1785s">watch presentation</a>). Instead of using the chart applier, this change wraps control-plane components in <code>ManagedResource</code>s, improving scalability and automation (e.g., scaling components imperatively).
<a href=https://github.com/gardener/gardener/pull/12251>Gardener PR #12251</a> was created for this, with a stretch goal related to <a href=https://github.com/gardener/gardener/issues/12250>issue #12250</a> explored in a <a href=https://github.com/gardener/gardener/compare/master...metal-stack:gardener:controlplane-objects-provider-interface>compare branch</a>.</p><p>A quick win, marked as a üèéÔ∏è fast-track item, was to <strong>Expose EgressCIDRs in the shoot-info <code>ConfigMap</code></strong> (<a href="https://youtu.be/TCLXovw43HA?t=2961s">watch presentation</a>).
This makes egress CIDRs available to workloads within the shoot cluster, useful for controllers like <a href=https://www.crossplane.io/>Crossplane</a>.
This was implemented and merged during the hackathon via <a href=https://github.com/gardener/gardener/pull/12252>gardener PR #12252</a>.</p><h2 id=-improving-user-and-developer-experience>‚ú® Improving User and Developer Experience<a class=td-heading-self-link href=#-improving-user-and-developer-experience aria-label="Heading self-link"></a></h2><p>Enhancing the usability of Gardener tools is always a priority.</p><p>The <strong>Dashboard Usability Improvements</strong> project (<a href="https://youtu.be/TCLXovw43HA?t=2052s">watch presentation</a>) tackled several areas based on <a href=https://github.com/gardener/dashboard/issues/2469>dashboard issue #2469</a>. Achievements include:</p><ul><li>Allowing custom display names for projects via annotations (<a href=https://github.com/gardener/dashboard/pull/2470>dashboard PR #2470</a>).</li><li>Configurable default values for Shoot creation, like AutoScaler min/max replicas (<a href=https://github.com/gardener/dashboard/pull/2476>dashboard PR #2476</a>).</li><li>The ability to hide certain UI elements, such as Control Plane HA options (<a href=https://github.com/gardener/dashboard/pull/2478>dashboard PR #2478</a>).</li></ul><p>The <strong>Documentation Revamp</strong> (<a href="https://youtu.be/TCLXovw43HA?t=2551s">watch presentation</a>) aimed to improve the structure and discoverability of Gardener&rsquo;s documentation.
Metadata for pages was enhanced (<a href=https://github.com/gardener/documentation/pull/652>documentation PR #652</a>), the glossary was expanded (<a href=https://github.com/gardener/documentation/pull/653>documentation PR #653</a>), and a PoC for using VitePress as a more modern documentation generator was created.</p><h2 id=-advancing-versioning-and-deployment-strategies>üîÑ Advancing Versioning and Deployment Strategies<a class=td-heading-self-link href=#-advancing-versioning-and-deployment-strategies aria-label="Heading self-link"></a></h2><p>Flexibility in managing Gardener versions and deployments was also explored.</p><p>The topic of <strong>Multiple Parallel Versions in a Gardener Landscape</strong> (formerly Canary Deployments) (<a href="https://youtu.be/TCLXovw43HA?t=3482s">watch presentation</a>) investigated ways to overcome tight versioning constraints.
It was discovered that the current implementation already allows rolling out different extension versions across different seeds using controller registration seat selectors.
Further discussion is needed on some caveats, particularly around the <code>primary</code> field in <code>ControllerRegistration</code> resources.</p><p>Progress was also made on <strong>GEP-32 ‚Äì Version Classification Lifecycles</strong> (üèéÔ∏è fast-track).
This initiative, started in a previous hackathon, aims to automate version lifecycle management.
The previous PR (<a href=https://github.com/metal-stack/gardener/pull/9>metal-stack/gardener #9</a>) was rebased and broken into smaller, more reviewable PRs.</p><h2 id=-conclusion>üå± Conclusion<a class=td-heading-self-link href=#-conclusion aria-label="Heading self-link"></a></h2><p>The Hack The Garden event in Schelklingen was a testament to the community&rsquo;s dedication and collaborative spirit.
Numerous projects saw significant progress, from PoCs for major architectural changes to practical improvements in daily operations and user experience.
Many of these efforts are now moving into further development, testing, and review, promising exciting enhancements for the Gardener ecosystem.</p><p>Stay tuned for more updates as these projects mature and become integrated into Gardener!</p><p>The next hackathon takes place in early December 2025.
If you&rsquo;d like to join, head over to the <a href=https://join.slack.com/t/gardener-cloud/shared_invite/zt-33c9daems-3oOorhnqOSnldZPWqGmIBw>Gardener Slack</a>.
Happy to meet you there! ‚úåÔ∏è</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ff1457f66818b35c95a0a8ac6e54bda7>May</h1><div class="td-byline mb-4"><time datetime=2025-05-21 class=text-body-secondary>Wednesday, May 21, 2025</time></div></div><div class=td-content><h1 id=pg-c9f1fffb57fbf0b00ef79ab5f034da98>Fine-Tuning kube-proxy Readiness: Ensuring Accurate Health Checks During Node Scale-Down</h1><div class="td-byline mb-4"><time datetime=2025-05-21 class=text-body-secondary>Wednesday, May 21, 2025</time></div><p>Gardener has recently refined how it determines the readiness of <code>kube-proxy</code> components within managed Kubernetes clusters. This adjustment leads to more accurate system health reporting, especially during node scale-down operations orchestrated by <code>cluster-autoscaler</code>.</p><h3 id=the-challenge-kube-proxy-readiness-during-node-scale-down>The Challenge: kube-proxy Readiness During Node Scale-Down<a class=td-heading-self-link href=#the-challenge-kube-proxy-readiness-during-node-scale-down aria-label="Heading self-link"></a></h3><p>Previously, Gardener utilized <code>kube-proxy</code>&rsquo;s <code>/healthz</code> endpoint for its readiness probe. While generally effective, this endpoint&rsquo;s behavior changed in Kubernetes 1.28 (as part of <a href="https://github.com/alexanderConstantinescu/kubernetes-enhancements/blob/e3d8adae9cf79338add2149db0900e47a4c64338/keps/sig-network/3836-kube-proxy-improved-ingress-connectivity-reliability/README.md?plain=1#L105-L107">KEP-3836</a> and implemented in <a href=https://github.com/kubernetes/kubernetes/pull/116470>kubernetes/kubernetes#116470</a>). The <code>/healthz</code> endpoint now reports <code>kube-proxy</code> as unhealthy if its node is marked for deletion by <code>cluster-autoscaler</code> (e.g., via a specific taint) or has a deletion timestamp.</p><p>This behavior is intended to help external load balancers (particularly those using <code>externalTrafficPolicy: Cluster</code> on infrastructures like GCP) avoid sending <em>new</em> traffic to nodes that are about to be terminated. However, for Gardener&rsquo;s internal system component health checks, this meant that <code>kube-proxy</code> could appear unready for extended periods if node deletion was delayed due to <code>PodDisruptionBudgets</code> or long <code>terminationGracePeriodSeconds</code>. This could lead to misleading &ldquo;unhealthy&rdquo; states for the cluster&rsquo;s system components.</p><h3 id=the-solution-aligning-with-upstream-kube-proxy-enhancements>The Solution: Aligning with Upstream kube-proxy Enhancements<a class=td-heading-self-link href=#the-solution-aligning-with-upstream-kube-proxy-enhancements aria-label="Heading self-link"></a></h3><p>To address this, Gardener now leverages the <code>/livez</code> endpoint for <code>kube-proxy</code>&rsquo;s readiness probe in clusters running Kubernetes version 1.28 and newer. The <code>/livez</code> endpoint, also introduced as part of the aforementioned <code>kube-proxy</code> improvements, checks the actual liveness of the <code>kube-proxy</code> process itself, without considering the node&rsquo;s termination status.</p><p>For clusters running Kubernetes versions 1.27.x and older (where <code>/livez</code> is not available), Gardener will continue to use the <code>/healthz</code> endpoint for the readiness probe.</p><p>This change, detailed in <a href=https://github.com/gardener/gardener/pull/12015>gardener/gardener#12015</a>, ensures that Gardener&rsquo;s readiness check for <code>kube-proxy</code> accurately reflects <code>kube-proxy</code>&rsquo;s operational status rather than the node&rsquo;s lifecycle state. It&rsquo;s important to note that this adjustment does not interfere with the goals of KEP-3836; cloud controller managers can still utilize the <code>/healthz</code> endpoint for their load balancer health checks as intended.</p><h3 id=benefits-for-gardener-operators>Benefits for Gardener Operators<a class=td-heading-self-link href=#benefits-for-gardener-operators aria-label="Heading self-link"></a></h3><p>This enhancement brings a key benefit to Gardener operators:</p><ul><li><strong>More Accurate System Health:</strong> The system components health check will no longer report <code>kube-proxy</code> as unhealthy simply because its node is being gracefully terminated by <code>cluster-autoscaler</code>. This reduces false alarms and provides a clearer view of the cluster&rsquo;s actual health.</li><li><strong>Smoother Operations:</strong> Operations teams will experience fewer unnecessary alerts related to <code>kube-proxy</code> during routine scale-down events, allowing them to focus on genuine issues.</li></ul><p>By adapting its <code>kube-proxy</code> readiness checks, Gardener continues to refine its operational robustness, providing a more stable and predictable management experience.</p><h3 id=further-information>Further Information<a class=td-heading-self-link href=#further-information aria-label="Heading self-link"></a></h3><ul><li><strong>GitHub Pull Request:</strong> <a href=https://github.com/gardener/gardener/pull/12015>gardener/gardener#12015</a></li><li><strong>Recording of the presentation segment:</strong> <a href="https://youtu.be/ssvXpPliOY0?t=1151">Watch on YouTube (starts at the relevant section)</a></li><li><strong>Upstream KEP:</strong> <a href="https://github.com/alexanderConstantinescu/kubernetes-enhancements/blob/e3d8adae9cf79338add2149db0900e47a4c64338/keps/sig-network/3836-kube-proxy-improved-ingress-connectivity-reliability/README.md?plain=1#L105-L107">KEP-3836: Kube-proxy improved ingress connectivity reliability</a></li><li><strong>Upstream Kubernetes PR:</strong> <a href=https://github.com/kubernetes/kubernetes/pull/116470>kubernetes/kubernetes#116470</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-584d5f8b980f6c0ea31f0083ecf96c90>New in Gardener: Forceful Redeployment of gardenlets for Enhanced Operational Control</h1><div class="td-byline mb-4"><time datetime=2025-05-21 class=text-body-secondary>Wednesday, May 21, 2025</time></div><p>Gardener continues to enhance its operational capabilities, and a recent improvement introduces a much-requested feature for managing gardenlets: the ability to forcefully trigger their redeployment. This provides operators with greater control and a streamlined recovery path for specific scenarios.</p><h3 id=the-standard-gardenlet-lifecycle>The Standard gardenlet Lifecycle<a class=td-heading-self-link href=#the-standard-gardenlet-lifecycle aria-label="Heading self-link"></a></h3><p>gardenlets, crucial components in the Gardener architecture, are typically deployed into seed clusters. For setups utilizing the <code>seedmanagement.gardener.cloud/v1alpha1.Gardenlet</code> resource, particularly in unmanaged seeds (those not backed by a shoot cluster and <code>ManagedSeed</code> resource), the <code>gardener-operator</code> handles the initial deployment of the gardenlet.</p><p>Once this initial deployment is complete, the gardenlet takes over its own lifecycle, leveraging a self-upgrade strategy to keep itself up-to-date. Under normal circumstances, the <code>gardener-operator</code> does not intervene further after this initial phase.</p><h3 id=when-things-go-awry-the-need-for-intervention>When Things Go Awry: The Need for Intervention<a class=td-heading-self-link href=#when-things-go-awry-the-need-for-intervention aria-label="Heading self-link"></a></h3><p>While the self-upgrade mechanism is robust, certain situations can arise where a gardenlet might require a more direct intervention. For example:</p><ul><li>The gardenlet&rsquo;s client certificate to the virtual garden cluster might have expired or become invalid.</li><li>The gardenlet <code>Deployment</code> in the seed cluster might have been accidentally deleted or become corrupted.</li></ul><p>In such cases, because the <code>gardener-operator</code>&rsquo;s responsibility typically ends after the initial deployment, the gardenlet might not be able to recover on its own, potentially leading to operational issues.</p><h3 id=empowering-operators-the-force-redeploy-annotation>Empowering Operators: The Force-Redeploy Annotation<a class=td-heading-self-link href=#empowering-operators-the-force-redeploy-annotation aria-label="Heading self-link"></a></h3><p>To address these challenges, Gardener now allows operators to instruct the <code>gardener-operator</code> to forcefully redeploy a gardenlet. This is achieved by annotating the specific <code>Gardenlet</code> resource with:</p><pre tabindex=0><code>gardener.cloud/operation=force-redeploy
</code></pre><p>When this annotation is applied, it signals the <code>gardener-operator</code> to re-initiate the deployment process for the targeted gardenlet, effectively overriding the usual hands-off approach after initial setup.</p><h3 id=how-it-works>How It Works<a class=td-heading-self-link href=#how-it-works aria-label="Heading self-link"></a></h3><p>The process for a forceful redeployment is straightforward:</p><ol><li>An operator identifies a gardenlet that requires redeployment due to issues like an expired certificate or a missing deployment.</li><li>The operator applies the <code>gardener.cloud/operation=force-redeploy</code> annotation to the corresponding <code>seedmanagement.gardener.cloud/v1alpha1.Gardenlet</code> resource in the virtual garden cluster.</li><li><strong>Important:</strong> If the gardenlet is for a remote cluster and its kubeconfig <code>Secret</code> was previously removed (a standard cleanup step after initial deployment), this <code>Secret</code> must be recreated, and its reference (<code>.spec.kubeconfigSecretRef</code>) must be re-added to the <code>Gardenlet</code> specification.</li><li>The <code>gardener-operator</code> detects the annotation and proceeds to redeploy the gardenlet, applying its configurations and charts anew.</li><li>Once the redeployment is successfully completed, the <code>gardener-operator</code> automatically removes the <code>gardener.cloud/operation=force-redeploy</code> annotation from the <code>Gardenlet</code> resource. Similar to the initial deployment, it will also clean up the referenced kubeconfig <code>Secret</code> and set <code>.spec.kubeconfigSecretRef</code> to <code>nil</code> if it was provided.</li></ol><h3 id=benefits>Benefits<a class=td-heading-self-link href=#benefits aria-label="Heading self-link"></a></h3><p>This new feature offers significant advantages for Gardener operators:</p><ul><li><strong>Enhanced Recovery:</strong> Provides a clear and reliable mechanism to recover gardenlets from specific critical failure states.</li><li><strong>Improved Operational Flexibility:</strong> Offers more direct control over the gardenlet lifecycle when exceptional circumstances demand it.</li><li><strong>Reduced Manual Effort:</strong> Streamlines the process of restoring a misbehaving gardenlet, minimizing potential downtime or complex manual recovery procedures.</li></ul><p>This enhancement underscores Gardener&rsquo;s commitment to operational excellence and responsiveness to the needs of its user community.</p><h3 id=dive-deeper>Dive Deeper<a class=td-heading-self-link href=#dive-deeper aria-label="Heading self-link"></a></h3><p>To learn more about this feature, you can explore the following resources:</p><ul><li><strong>GitHub Pull Request:</strong> <a href=https://github.com/gardener/gardener/pull/11972>gardener/gardener#11972</a></li><li><strong>Official Documentation:</strong> <a href=https://github.com/gardener/gardener/tree/master/docs/deployment/deploy_gardenlet_via_operator.md#forceful-re-deployment>Forceful Re-Deployment of gardenlets</a></li><li><strong>Community Meeting Recording (starts at the relevant segment):</strong> <a href="https://youtu.be/ssvXpPliOY0?t=338">Gardener Review Meeting on YouTube</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-7f61ecaa1cf2430641f2c9565b2c53d1>Streamlined Node Onboarding: Introducing `gardenadm token` and `gardenadm join`</h1><div class="td-byline mb-4"><time datetime=2025-05-21 class=text-body-secondary>Wednesday, May 21, 2025</time></div><p>Gardener continues to enhance its <code>gardenadm</code> tool, simplifying the management of autonomous Shoot clusters. Recently, new functionalities have been introduced to streamline the process of adding worker nodes to these clusters: the <code>gardenadm token</code> command suite and the corresponding <code>gardenadm join</code> command. These additions offer a more convenient and Kubernetes-native experience for cluster expansion.</p><h3 id=managing-bootstrap-tokens-with-gardenadm-token>Managing Bootstrap Tokens with <code>gardenadm token</code><a class=td-heading-self-link href=#managing-bootstrap-tokens-with-gardenadm-token aria-label="Heading self-link"></a></h3><p>A key aspect of securely joining nodes to a Kubernetes cluster is the use of bootstrap tokens. The new <code>gardenadm token</code> command provides a set of subcommands to manage these tokens effectively within your autonomous Shoot cluster&rsquo;s control plane node. This functionality is analogous to the familiar <code>kubeadm token</code> commands.</p><p>The available subcommands include:</p><ul><li><strong><code>gardenadm token list</code></strong>: Displays all current bootstrap tokens. You can also use the <code>--with-token-secrets</code> flag to include the token secrets in the output for easier inspection.</li><li><strong><code>gardenadm token generate</code></strong>: Generates a cryptographically random bootstrap token. This command only prints the token; it does not create it on the server.</li><li><strong><code>gardenadm token create [token]</code></strong>: Creates a new bootstrap token on the server. If you provide a token (in the format <code>[a-z0-9]{6}.[a-z0-9]{16}</code>), it will be used. If no token is supplied, <code>gardenadm</code> will automatically generate a random one and create it.<ul><li>A particularly helpful option for this command is <code>--print-join-command</code>. When used, instead of just outputting the token, it prints the complete <code>gardenadm join</code> command, ready to be copied and executed on the worker node you intend to join. You can also specify flags like <code>--description</code>, <code>--validity</code>, and <code>--worker-pool-name</code> to customize the token and the generated join command.</li></ul></li><li><strong><code>gardenadm token delete &lt;token-value...></code></strong>: Deletes one or more bootstrap tokens from the server. You can specify tokens by their ID, the full token string, or the name of the Kubernetes Secret storing the token (e.g., <code>bootstrap-token-&lt;id></code>).</li></ul><p>These commands provide comprehensive control over the lifecycle of bootstrap tokens, enhancing security and operational ease.</p><h3 id=joining-worker-nodes-with-gardenadm-join>Joining Worker Nodes with <code>gardenadm join</code><a class=td-heading-self-link href=#joining-worker-nodes-with-gardenadm-join aria-label="Heading self-link"></a></h3><p>Once a bootstrap token is created (ideally using <code>gardenadm token create --print-join-command</code> on a control plane node), the new <code>gardenadm join</code> command facilitates the process of adding a new worker node to the autonomous Shoot cluster.</p><p>The command is executed on the prospective worker machine and typically looks like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>gardenadm join --bootstrap-token &lt;token_id.token_secret&gt; --ca-certificate &lt;base64_encoded_ca_bundle&gt; --gardener-node-agent-secret-name &lt;os_config_secret_name&gt; &lt;control_plane_api_server_address&gt;
</span></span></code></pre></div><p>Key parameters include:</p><ul><li><code>--bootstrap-token</code>: The token obtained from the <code>gardenadm token create</code> command.</li><li><code>--ca-certificate</code>: The base64-encoded CA certificate bundle of the cluster&rsquo;s API server.</li><li><code>--gardener-node-agent-secret-name</code>: The name of the Secret in the <code>kube-system</code> namespace of the control plane that contains the OperatingSystemConfig (OSC) for the <code>gardener-node-agent</code>. This OSC dictates how the node should be configured.</li><li><code>&lt;control_plane_api_server_address></code>: The address of the Kubernetes API server of the autonomous cluster.</li></ul><p>Upon execution, <code>gardenadm join</code> performs several actions:</p><ol><li>It discovers the Kubernetes version of the control plane using the provided bootstrap token and CA certificate.</li><li>It checks if the <code>gardener-node-agent</code> has already been initialized on the machine.</li><li>If not already joined, it prepares the <code>gardener-node-init</code> configuration. This involves setting up a systemd service (<code>gardener-node-init.service</code>) which, in turn, downloads and runs the <code>gardener-node-agent</code>.</li><li>The <code>gardener-node-agent</code> then uses the bootstrap token to securely download its specific OperatingSystemConfig from the control plane.</li><li>Finally, it applies this configuration, setting up the kubelet and other necessary components, thereby officially joining the node to the cluster.</li></ol><p>After the node has successfully joined, the bootstrap token used for the process will be automatically deleted by the <code>kube-controller-manager</code> once it expires. However, it can also be manually deleted immediately using <code>gardenadm token delete</code> on the control plane node for enhanced security.</p><p>These new <code>gardenadm</code> commands significantly simplify the expansion of autonomous Shoot clusters, providing a robust and user-friendly mechanism for managing bootstrap tokens and joining worker nodes.</p><h3 id=further-information>Further Information<a class=td-heading-self-link href=#further-information aria-label="Heading self-link"></a></h3><ul><li><strong><code>gardenadm token</code> Pull Request:</strong> <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/28-autonomous-shoot-clusters.md>GEP-28</a> <code>gardenadm token</code> (<a href=https://github.com/gardener/gardener/pull/11934>#11934</a>)</li><li><strong><code>gardenadm join</code> Pull Request:</strong> <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/28-autonomous-shoot-clusters.md>GEP-28</a> <code>gardenadm join</code> (<a href=https://github.com/gardener/gardener/pull/11942>#11942</a>)</li><li><strong>Recording of the demo:</strong> Watch the demo starting at <a href="https://youtu.be/ssvXpPliOY0?t=768">12m48s</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3d3c662dbe65fcd8da97232e9214b1e0>Enhanced Network Flexibility: Gardener Now Supports CIDR Overlap for Non-HA Shoots</h1><div class="td-byline mb-4"><time datetime=2025-05-19 class=text-body-secondary>Monday, May 19, 2025</time></div><p>Gardener is continually evolving to offer greater flexibility and efficiency in managing Kubernetes clusters. A significant enhancement has been introduced that addresses a common networking challenge: the requirement for completely disjoint network CIDR blocks between a shoot cluster and its seed cluster. Now, Gardener allows for IPv4 network overlap in specific scenarios, providing users with more latitude in their network planning.</p><h3 id=addressing-ip-address-constraints>Addressing IP Address Constraints<a class=td-heading-self-link href=#addressing-ip-address-constraints aria-label="Heading self-link"></a></h3><p>Previously, all shoot cluster networks (pods, services, nodes) had to be distinct from the seed cluster&rsquo;s networks. This could be challenging in environments with limited IP address space or complex network topologies. With this new feature, IPv4 or dual-stack shoot clusters can now define pod, service, and node networks that overlap with the IPv4 networks of their seed cluster.</p><h3 id=how-it-works-nat-for-seamless-connectivity>How It Works: NAT for Seamless Connectivity<a class=td-heading-self-link href=#how-it-works-nat-for-seamless-connectivity aria-label="Heading self-link"></a></h3><p>This capability is enabled through a double Network Address Translation (NAT) mechanism within the VPN connection established between the shoot and seed clusters. When IPv4 network overlap is configured, Gardener intelligently maps the overlapping shoot and seed networks to a dedicated set of newly reserved IPv4 ranges. These ranges are used exclusively within the VPN pods to ensure seamless communication, effectively resolving any conflicts that would arise from the overlapping IPs.</p><p>The reserved mapping ranges are:</p><ul><li><code>241.0.0.0/8</code>: Seed Pod Mapping Range</li><li><code>242.0.0.0/8</code>: Shoot Node Mapping Range</li><li><code>243.0.0.0/8</code>: Shoot Service Mapping Range</li><li><code>244.0.0.0/8</code>: Shoot Pod Mapping Range</li></ul><h3 id=conditions-for-utilizing-overlapping-networks>Conditions for Utilizing Overlapping Networks<a class=td-heading-self-link href=#conditions-for-utilizing-overlapping-networks aria-label="Heading self-link"></a></h3><p>To leverage this new network flexibility, the following conditions must be met:</p><ol><li><strong>Non-Highly-Available VPN:</strong> The shoot cluster must utilize a non-highly-available (non-HA) VPN. This is typically the configuration for shoots with a non-HA control plane.</li><li><strong>IPv4 or Dual-Stack Shoots:</strong> The shoot cluster must be configured as either single-stack IPv4 or dual-stack (IPv4/IPv6). The overlap feature specifically pertains to IPv4 networks.</li><li><strong>Non-Use of Reserved Ranges:</strong> The shoot cluster&rsquo;s own defined networks (for pods, services, and nodes) must not utilize any of the Gardener-reserved IP ranges, including the newly introduced mapping ranges listed above, or the existing <code>240.0.0.0/8</code> range (Kube-ApiServer Mapping Range).</li></ol><p>It&rsquo;s important to note that Gardener will prevent the migration of a non-HA shoot to an HA setup if its network ranges currently overlap with the seed, as this feature is presently limited to non-HA VPN configurations. For single-stack IPv6 shoots, Gardener continues to enforce non-overlapping IPv6 networks to avoid any potential issues, although IPv6 address space exhaustion is less common.</p><h3 id=benefits-for-gardener-users>Benefits for Gardener Users<a class=td-heading-self-link href=#benefits-for-gardener-users aria-label="Heading self-link"></a></h3><p>This enhancement offers increased flexibility in IP address management, particularly beneficial for users operating numerous shoot clusters or those in environments with constrained IPv4 address availability. By relaxing the strict disjointedness requirement for non-HA shoots, Gardener simplifies network allocation and reduces the operational overhead associated with IP address planning.</p><h3 id=explore-further>Explore Further<a class=td-heading-self-link href=#explore-further aria-label="Heading self-link"></a></h3><p>To dive deeper into this feature, you can review the original pull request and the updated documentation:</p><ul><li><strong>GitHub PR:</strong> <a href=https://github.com/gardener/gardener/pull/11582>feat: Allow CIDR overlap for non-HA VPN shoots (#11582)</a></li><li><strong>Gardener Documentation:</strong> <a href=/docs/gardener/networking/shoot_networking/#overlapping-ipv4-networks-between-seed-and-shoot>Shoot Networking</a></li><li><strong>Developer Talk Recording:</strong> <a href="https://youtu.be/ZwurVm1IJ7o?t=0">Gardener Development - Sprint Review #131</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0ccde31ce45ecbf3c0a91d5eed0a85e4>Enhanced Node Management: Introducing In-Place Updates in Gardener</h1><div class="td-byline mb-4"><time datetime=2025-05-19 class=text-body-secondary>Monday, May 19, 2025</time></div><p>Gardener is committed to providing efficient and flexible Kubernetes cluster management. Traditionally, updates to worker pool configurations, such as machine image or Kubernetes minor version changes, trigger a rolling update. This process involves replacing existing nodes with new ones, which is a robust approach for many scenarios. However, for environments with physical or bare-metal nodes, or stateful workloads sensitive to node replacement, or if the virtual machine type is scarce, this can introduce challenges like extended update times and potential disruptions.</p><p>To address these needs, Gardener now introduces <strong>In-Place Node Updates</strong>. This new capability allows certain updates to be applied directly to existing worker nodes without requiring their replacement, significantly reducing disruption and speeding up update processes for compatible changes.</p><h3 id=new-update-strategies-for-worker-pools>New Update Strategies for Worker Pools<a class=td-heading-self-link href=#new-update-strategies-for-worker-pools aria-label="Heading self-link"></a></h3><p>Gardener now supports three distinct update strategies for your worker pools, configurable via the <code>updateStrategy</code> field in the <code>Shoot</code> specification&rsquo;s worker pool definition:</p><ul><li><strong><code>AutoRollingUpdate</code></strong>: This is the classic and default strategy. When updates occur, nodes are cordoned, drained, terminated, and replaced with new nodes incorporating the changes.</li><li><strong><code>AutoInPlaceUpdate</code></strong>: With this strategy, compatible updates are applied directly to the existing nodes. The MachineControllerManager (MCM) automatically selects nodes, cordons and drains them, and then signals the Gardener Node Agent (GNA) to perform the update. Once GNA confirms success, MCM uncordons the node.</li><li><strong><code>ManualInPlaceUpdate</code></strong>: This strategy also applies updates directly to existing nodes but gives operators fine-grained control. After an update is specified, MCM marks all nodes in the pool as candidates. Operators must then manually label individual nodes to select them for the in-place update process, which then proceeds similarly to the <code>AutoInPlaceUpdate</code> strategy.</li></ul><p>The <code>AutoInPlaceUpdate</code> and <code>ManualInPlaceUpdate</code> strategies are available when the <code>InPlaceNodeUpdates</code> feature gate is enabled in the <code>gardener-apiserver</code>.</p><h3 id=what-can-be-updated-in-place>What Can Be Updated In-Place?<a class=td-heading-self-link href=#what-can-be-updated-in-place aria-label="Heading self-link"></a></h3><p>In-place updates are designed to handle a variety of common operational tasks more efficiently:</p><ul><li><strong>Machine Image Updates</strong>: Newer versions of a machine image can be rolled out by executing an update command directly on the node, provided the image and cloud profile are configured to support this.</li><li><strong>Kubernetes Minor Version Updates</strong>: Updates to the Kubernetes minor version of worker nodes can be applied in-place.</li><li><strong>Kubelet Configuration Changes</strong>: Modifications to the Kubelet configuration can be applied directly.</li><li><strong>Credentials Rotation</strong>: Critical for security, rotation of Certificate Authorities (CAs) and ServiceAccount signing keys can now be performed on existing nodes without replacement.</li></ul><p>However, some changes still necessitate a rolling update (node replacement):</p><ul><li>Changing the machine image name (e.g., switching from Ubuntu to Garden Linux).</li><li>Modifying the machine type.</li><li>Altering volume types or sizes.</li><li>Changing the Container Runtime Interface (CRI) name (e.g., from Docker to containerd).</li><li>Enabling or disabling node-local DNS.</li></ul><h3 id=key-api-and-component-adaptations>Key API and Component Adaptations<a class=td-heading-self-link href=#key-api-and-component-adaptations aria-label="Heading self-link"></a></h3><p>Several Gardener components and APIs have been enhanced to support in-place updates:</p><ul><li><strong>CloudProfile</strong>: The <code>CloudProfile</code> API now allows specifying <code>inPlaceUpdates</code> configuration within <code>machineImage.versions</code>. This includes a boolean <code>supported</code> field to indicate if a version supports in-place updates and an optional <code>minVersionForUpdate</code> string to define the minimum OS version from which an in-place update to the current version is permissible.</li><li><strong>Shoot Specification</strong>: As mentioned, the <code>spec.provider.workers[].updateStrategy</code> field allows selection of the desired update strategy. Additionally, <code>spec.provider.workers[].machineControllerManagerSettings</code> now includes <code>machineInPlaceUpdateTimeout</code> and <code>disableHealthTimeout</code> (which defaults to <code>true</code> for in-place strategies to prevent premature machine deletion during lengthy updates). For <code>ManualInPlaceUpdate</code>, <code>maxSurge</code> defaults to <code>0</code> and <code>maxUnavailable</code> to <code>1</code>.</li><li><strong>OperatingSystemConfig (OSC)</strong>: The OSC resource, managed by OS extensions, now includes <code>status.inPlaceUpdates.osUpdate</code> where extensions can specify the <code>command</code> and <code>args</code> for the Gardener Node Agent to execute for machine image (Operating System) updates. The <code>spec.inPlaceUpdates</code> field in the OSC will carry information like the target Operating System version, Kubelet version, and credential rotation status to the node.</li><li><strong>Gardener Node Agent (GNA)</strong>: GNA is responsible for executing the in-place updates on the node. It watches for a specific node condition ( <code>InPlaceUpdate</code> with reason <code>ReadyForUpdate</code>) set by MCM, performs the OS update, Kubelet updates, or credentials rotation, restarts necessary pods (like DaemonSets), and then labels the node with the update outcome.</li><li><strong>MachineControllerManager (MCM)</strong>: MCM orchestrates the in-place update process. For in-place strategies, while new machine classes and machine sets are created to reflect the desired state, the actual machine objects are not deleted and recreated. Instead, their ownership is transferred to the new machine set. MCM handles cordoning, draining, and setting node conditions to coordinate with GNA.</li><li><strong>Shoot Status & Constraints</strong>: To provide visibility, the <code>status.inPlaceUpdates.pendingWorkerUpdates</code> field in the <code>Shoot</code> now lists worker pools pending <code>autoInPlaceUpdate</code> or <code>manualInPlaceUpdate</code>. A new <code>ShootManualInPlaceWorkersUpdated</code> constraint is added if any manual in-place updates are pending, ensuring users are aware.</li><li><strong>Worker Status</strong>: The <code>Worker</code> extension resource now includes <code>status.inPlaceUpdates.workerPoolToHashMap</code> to track the configuration hash of worker pools that have undergone in-place updates. This helps Gardener determine if a pool is up-to-date.</li><li><strong>Forcing Updates</strong>: If an in-place update is stuck, the <code>gardener.cloud/operation=force-in-place-update</code> annotation can be added to the Shoot to allow subsequent changes or retries.</li></ul><h3 id=benefits-of-in-place-updates>Benefits of In-Place Updates<a class=td-heading-self-link href=#benefits-of-in-place-updates aria-label="Heading self-link"></a></h3><ul><li><strong>Reduced Disruption</strong>: Minimizes workload interruptions by avoiding full node replacements for compatible updates.</li><li><strong>Faster Updates</strong>: Applying changes directly can be quicker than provisioning new nodes, especially for OS patches or configuration changes.</li><li><strong>Bare-Metal Efficiency</strong>: Particularly beneficial for bare-metal environments where node provisioning is more time-consuming and complex.</li><li><strong>Stateful Workload Friendly</strong>: Lessens the impact on stateful applications that might be sensitive to node churn.</li></ul><p>In-place node updates represent a significant step forward in Gardener&rsquo;s operational flexibility, offering a more nuanced and efficient approach to managing node lifecycles, especially in demanding or specialized environments.</p><h3 id=dive-deeper>Dive Deeper<a class=td-heading-self-link href=#dive-deeper aria-label="Heading self-link"></a></h3><p>To explore the technical details and contributions that made this feature possible, refer to the following resources:</p><ul><li><strong>Parent Issue for &ldquo;[GEP-31] Support for In-Place Node Updates&rdquo;</strong>: <a href=https://github.com/gardener/gardener/issues/10219>Issue #10219</a></li><li><strong>GEP-31: In-Place Node Updates of Shoot Clusters</strong>: <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/31-inplace-node-update.md>GEP-31: In-Place Node Updates of Shoot Clusters</a></li><li><strong>Developer Talk Recording (starting at 39m37s)</strong>: <a href="https://youtu.be/ZwurVm1IJ7o?t=2377">Youtube</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d120589c63a3e999d70ebea3e76f10bf>Gardener Dashboard 1.80: Streamlined Credentials, Enhanced Cluster Views, and Real-Time Updates</h1><div class="td-byline mb-4"><time datetime=2025-05-19 class=text-body-secondary>Monday, May 19, 2025</time></div><p>Gardener Dashboard version 1.80 introduces several significant enhancements aimed at improving user experience, credentials management, and overall operational efficiency. These updates bring more clarity to credential handling, a smoother experience for managing large numbers of clusters, and a move towards a more reactive interface.</p><h3 id=unified-and-enhanced-credentials-management>Unified and Enhanced Credentials Management<a class=td-heading-self-link href=#unified-and-enhanced-credentials-management aria-label="Heading self-link"></a></h3><p>The management of secrets and credentials has been significantly revamped for better clarity and functionality:</p><ul><li><strong>Introducing CredentialsBindings:</strong> The dashboard now fully supports <code>CredentialsBinding</code> resources alongside the existing <code>SecretBinding</code> resources. This allows for referencing both Secrets and, in the future, Workload Identities more explicitly. While <code>CredentialsBindings</code> referencing Workload Identity resources are visible for cluster creation, editing or deleting them via the dashboard is not yet supported.</li><li><strong>&ldquo;Credentials&rdquo; Page:</strong> The former &ldquo;Secrets&rdquo; page has been renamed to &ldquo;Credentials.&rdquo; It features a new &ldquo;Kind&rdquo; column and distinct icons to clearly differentiate between <code>SecretBinding</code> and <code>CredentialsBinding</code> types, especially useful when resources share names. The column showing the referenced credential resource name has been removed as this information is part of the binding&rsquo;s details.</li><li><strong>Contextual Information and Safeguards:</strong> When editing a secret, all its associated data is now displayed, providing better context. If an underlying secret is referenced by multiple bindings, a hint is shown to prevent unintended impacts. Deletion of a binding is prevented if the underlying secret is still in use by another binding.</li><li><strong>Simplified Creation and Editing:</strong> New secrets created via the dashboard will now automatically generate a <code>CredentialsBinding</code>. While existing <code>SecretBindings</code> remain updatable, the creation of new <code>SecretBindings</code> through the dashboard is no longer supported, encouraging the adoption of the more versatile <code>CredentialsBinding</code>. The edit dialog for secrets now pre-fills current data, allowing for easier modification of specific fields.</li><li><strong>Handling Missing Secrets:</strong> The UI now provides clear information and guidance if a <code>CredentialsBinding</code> or <code>SecretBinding</code> references a secret that no longer exists.</li></ul><h3 id=revamped-cluster-list-for-improved-scalability>Revamped Cluster List for Improved Scalability<a class=td-heading-self-link href=#revamped-cluster-list-for-improved-scalability aria-label="Heading self-link"></a></h3><p>Navigating and managing a large number of clusters is now more efficient:</p><ul><li><strong>Virtual Scrolling:</strong> The cluster list has adopted virtual scrolling. Rows are rendered dynamically as you scroll, replacing the previous pagination system. This significantly improves performance and provides a smoother browsing experience, especially for environments with hundreds or thousands of clusters.</li><li><strong>Optimized Row Display:</strong> The height of individual rows in the cluster list has been reduced, allowing more clusters to be visible on the screen at once. Additionally, expandable content within a row (like worker details or ticket labels) now has a maximum height with internal scrolling, ensuring consistent row sizes and smooth virtual scrolling performance.</li></ul><h3 id=real-time-updates-for-projects>Real-Time Updates for Projects<a class=td-heading-self-link href=#real-time-updates-for-projects aria-label="Heading self-link"></a></h3><p>The dashboard is becoming more dynamic with the introduction of real-time updates:</p><ul><li><strong>Instant Project Changes:</strong> Modifications to projects, such as creation or deletion, are now reflected instantly in the project list and interface without requiring a page reload. This is achieved through WebSocket communication.</li><li><strong>Foundation for Future Reactivity:</strong> This enhancement for projects lays the groundwork for bringing real-time updates to other resources within the dashboard, such as Seeds and the Garden resource, in future releases.</li></ul><h3 id=other-notable-enhancements>Other Notable Enhancements<a class=td-heading-self-link href=#other-notable-enhancements aria-label="Heading self-link"></a></h3><ul><li><strong>Kubeconfig Update:</strong> The kubeconfig generated for garden cluster access via the &ldquo;Account&rdquo; page now uses the <code>--oidc-pkce-method</code> flag, replacing the deprecated <code>--oidc-use-pkce</code> flag. Users encountering deprecation messages should redownload their kubeconfig.</li><li><strong>Notification Behavior:</strong> Kubernetes warning notifications are now automatically dismissed after 5 seconds. However, all notifications will remain visible as long as the mouse cursor is hovering over them, giving users more time to read important messages.</li><li><strong>API Server URL Path:</strong> Support has been added for kubeconfigs that include a path in the API server URL.</li></ul><p>These updates in Gardener Dashboard 1.80 collectively enhance usability, provide better control over credentials, and improve performance for large-scale operations.</p><p>For a comprehensive list of all features, bug fixes, and contributor acknowledgments, please refer to the <a href=https://github.com/gardener/dashboard/releases/tag/1.80.0>official release notes</a>.
You can also view the segment of the community call discussing these dashboard updates <a href="https://youtu.be/ZwurVm1IJ7o?t=1793">here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bf04bed69dc8e0a4cc4625a9d8e34906>Gardener: Powering Enterprise Kubernetes at Scale and Europe's Sovereign Cloud Future</h1><div class="td-byline mb-4"><time datetime=2025-05-12 class=text-body-secondary>Monday, May 12, 2025</time></div><p>The Kubernetes ecosystem is dynamic, offering a wealth of tools to manage the complexities of modern cloud-native applications. For enterprises seeking to provision and manage Kubernetes clusters efficiently, securely, and at scale, a robust and comprehensive solution is paramount. Gardener, born from years of managing tens of thousands of clusters efficiently across diverse platforms and in demanding environments, stands out as a fully open-source choice for delivering fully managed Kubernetes Clusters as a Service. It already empowers organizations like SAP, STACKIT, T-Systems, and others (see <a href=https://gardener.cloud/adopter>adopters</a>) and has become a core technology for <a href=https://neonephos.org/projects>NeoNephos</a>, a project aimed at advancing digital autonomy in Europe (see <a href="https://www.youtube.com/watch?v=85MDID9Ju04&amp;t=621s">KubeCon London 2025 Keynote</a> and <a href=https://neonephos.org/2025/03/31/the-linux-foundation-announces-the-launch-of-neonephos-to-advance-digital-autonomy-in-europe>press announcement</a>).</p><h3 id=the-gardener-approach-an-architecture-forged-by-experience>The Gardener Approach: An Architecture Forged by Experience<a class=td-heading-self-link href=#the-gardener-approach-an-architecture-forged-by-experience aria-label="Heading self-link"></a></h3><p>At the heart of Gardener&rsquo;s architecture is the concept of &ldquo;Kubeception&rdquo; (see <a href="https://github.com/gardener/gardener?tab=readme-ov-file#gardener">readme</a> and <a href=/docs/gardener/concepts/architecture/>architecture</a>). This approach involves using Kubernetes to manage Kubernetes. Gardener runs on a Kubernetes cluster (called a <strong>runtime cluster</strong>), facilitates access through a self-managed node-less Kubernetes cluster (the <strong>garden cluster</strong>), manages Kubernetes control planes as pods within other self-managed Kubernetes clusters that provide high scalability (called <strong>seed clusters</strong>), and ultimately provisions end-user Kubernetes clusters (called <strong>shoot clusters</strong>).</p><p>This multi-layered architecture isn&rsquo;t complexity for its own sake. Gardener&rsquo;s design and extensive feature set are the product of over eight years of continuous development and refinement, directly shaped by the high-scale, security-sensitive, and enterprise-grade requirements of its users. Experience has shown that such a sophisticated structure is key to addressing significant challenges in scalability, security, and operational manageability. For instance:</p><ul><li><strong>Scalability:</strong> Gardener achieves considerable scalability through its use of <strong>seed clusters</strong>, which it also manages. This allows for the distribution of control planes, preventing bottlenecks. The design even envisions leveraging Gardener to host its own management components (as an <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/28-autonomous-shoot-clusters.md><strong>autonomous cluster</strong></a>), showcasing its resilience without risking circular dependencies.</li><li><strong>Security:</strong> A fundamental principle in Gardener is the strict isolation of control planes from data planes. This extends to Gardener itself, which runs in a dedicated management cluster but exposes its API to end-users through a workerless virtual cluster. This workerless cluster acts as an isolated access point, presenting no compute surface for potentially malicious pods, thereby significantly enhancing security.</li><li><strong>API Power & User Experience:</strong> Gardener utilizes the full capabilities of the Kubernetes API server. This enables advanced functionalities and sophisticated API change management. Crucially, for the end-user, interaction remains 100% Kubernetes-native. Users employ standard custom resources to instruct Gardener, meaning any tool, library, or language binding that supports Kubernetes CRDs inherently supports Gardener.</li></ul><h3 id=delivering-fully-managed-kubernetes-clusters-as-a-service>Delivering Fully Managed Kubernetes Clusters as a Service<a class=td-heading-self-link href=#delivering-fully-managed-kubernetes-clusters-as-a-service aria-label="Heading self-link"></a></h3><p>Gardener provides a comprehensive &ldquo;fully managed Kubernetes Clusters as a Service&rdquo; offering. This means it handles much more than just spinning up a cluster; it manages the entire lifecycle and operational aspects. Here‚Äôs a glimpse into its capabilities:</p><ol><li><p><strong>Full Cluster Lifecycle Management:</strong></p><ul><li><strong>Infrastructure Provisioning:</strong> Gardener takes on the provisioning and management of underlying cloud infrastructure, including VPCs, subnets, NAT gateways, security groups, IAM roles, and virtual machines across a wide range of providers like AWS, Azure, GCP, OpenStack, and more.</li><li><strong>Worker Node Management:</strong> It meticulously manages worker pools, covering OS images, machine types, autoscaling configurations (min/max/surge), update strategies, volume management, CRI configuration, and provider-specific settings.</li></ul></li><li><p><strong>Enterprise Platform Governance:</strong></p><ul><li><strong>Cloud Profiles:</strong> Gardener is designed with the comprehensive needs of enterprise platform operators in mind. Managing a fleet of clusters for an organization requires more than just provisioning; it demands clear governance over available resources, versions, and their lifecycle. Gardener addresses this through its declarative API, allowing platform administrators to define and enforce policies such as which Kubernetes versions are &ldquo;supported,&rdquo; &ldquo;preview,&rdquo; or &ldquo;deprecated,&rdquo; along with their expiration dates. Similarly, it allows control over available machine images, their versions, and lifecycle status. This level of granular control and lifecycle management for the underlying components of a Kubernetes service is crucial for enterprise adoption and stable operations. This is a key consideration often left as an additional implementation burden for platform teams using other cluster provisioning tools, where such governance features must be built on top. Gardener, by contrast, integrates these concerns directly into its API and operational model, simplifying the task for platform operators.</li></ul></li><li><p><strong>Advanced Networking:</strong></p><ul><li><strong>CNI Plugin Management:</strong> Gardener manages the deployment and configuration of CNI plugins such as Calico or Cilium.</li><li><strong>Dual-Stack Networking:</strong> It offers comprehensive support for IPv4, IPv6, and dual-stack configurations for pods, services, and nodes.</li><li><strong>NodeLocal DNS Cache:</strong> To enhance DNS performance and reliability, Gardener can deploy and manage NodeLocal DNS.</li></ul></li><li><p><strong>Comprehensive Autoscaling:</strong></p><ul><li><strong>Cluster Autoscaler:</strong> Gardener manages the Cluster Autoscaler for worker nodes, enabling dynamic scaling based on pod scheduling demands.</li><li><strong>Horizontal and Vertical Pod Autoscaler (VPA):</strong> It manages HPA/VPA for workloads and applies it to control plane components, optimizing resource utilization (see <a href=https://gardener.cloud/blog/2025/04-17-leaner-clusters-lower-bills>blog</a>).</li></ul></li><li><p><strong>Operational Excellence & Maintenance:</strong></p><ul><li><strong>Automated Kubernetes Upgrades:</strong> Gardener handles automated Kubernetes version upgrades for both control plane and worker nodes, with configurable maintenance windows.</li><li><strong>Automated OS Image Updates:</strong> It manages automated machine image updates for worker nodes.</li><li><strong>Cluster Hibernation:</strong> To optimize costs, Gardener supports hibernating clusters, scaling down components during inactivity.</li><li><strong>Scheduled Maintenance:</strong> It allows defining specific maintenance windows for predictability.</li><li><strong>Robust Credentials Rotation:</strong> Gardener features automated mechanisms for rotating <strong>all</strong> credentials. It provisions fine-grained, dedicated, and individual CAs, certificates, credentials, and secrets for each component ‚Äî whether Kubernetes-related (such as service account keys or etcd encryption keys) or Gardener-specific (such as opt-in SSH keys or observability credentials). The Gardener installation, the seeds, and all shoots have their own distinct sets of credentials ‚Äî amounting to more than 150 per shoot cluster control plane and hundreds of thousands for larger Gardener installations overall. All these credentials are rotated automatically and without downtime ‚Äî most continuously, while some (like the API server CA) require user initiation to ensure operational awareness. For a deeper dive into Gardener&rsquo;s credential rotation, see our <a href="https://www.youtube.com/watch?v=3V8oFQ16mTg&amp;t=29s">Cloud Native Rejekts talk</a>). This granular approach effectively prevents lateral movement, significantly strengthening the security posture.</li></ul></li><li><p><strong>Enhanced Security & Access Control:</strong></p><ul><li><strong>OIDC Integration:</strong> Gardener supports OIDC configuration for the <code>kube-apiserver</code> for secure user authentication.</li><li><strong>Customizable Audit Policies:</strong> It allows specifying custom audit policies for detailed logging.</li><li><strong>Managed Service Account Issuers:</strong> Gardener can manage service account issuers, enhancing workload identity security.</li><li><strong>SSH Access Control:</strong> It provides mechanisms to manage SSH access to worker nodes securely if opted in (Gardener itself doesn&rsquo;t require SSH access to worker nodes).</li><li><strong>Workload Identity:</strong> Gardener supports workload identity features, allowing pods to securely authenticate to cloud provider services.</li></ul></li><li><p><strong>Powerful Extensibility:</strong></p><ul><li><strong>Extension Framework and Ecosystem:</strong> Gardener features a robust extension mechanism for deep integration of cloud providers, operating systems, container runtimes, or services like DNS management, certificate management, registry caches, network filtering, image signature verification, and more.</li><li><strong>Catered to Platform Builders:</strong> This extensibility also allows platform builders to deploy custom extensions into the self-managed seed cluster infrastructure that hosts shoot cluster control planes. This offers robust isolation for these custom components from the user&rsquo;s shoot cluster worker nodes, enhancing both security and operational stability.</li></ul></li><li><p><strong>Integrated DNS and Certificate Management:</strong></p><ul><li><strong>External DNS Management:</strong> Gardener can manage DNS records for the cluster&rsquo;s API server and services via its <code>shoot-dns-service</code> extension.</li><li><strong>Automated Certificate Management:</strong> Through extensions like <code>shoot-cert-service</code>, it manages TLS certificates, including ACME integration. Gardener also provides its own robust DNS (<code>dns-management</code>) and certificate (<code>cert-management</code>) solutions designed for enterprise scale. These custom solutions were developed because, at the scale Gardener operates, many deep optimizations were necessary, e.g., to avoid being rate-limited by upstream providers.</li></ul></li></ol><h3 id=a-kubernetes-native-foundation-for-sovereign-cloud>A Kubernetes-Native Foundation for Sovereign Cloud<a class=td-heading-self-link href=#a-kubernetes-native-foundation-for-sovereign-cloud aria-label="Heading self-link"></a></h3><p>The modern IT landscape is rapidly evolving away from primitive virtual machines towards distributed systems. Kubernetes has emerged as the de facto standard for deploying and managing these modern, cloud-native applications and services at scale. Gardener is squarely positioned at the forefront of this shift, offering a Kubernetes-native approach to managing Kubernetes clusters themselves. It possesses a mature, declarative, Kubernetes-native API for full cluster lifecycle management. Unlike services that might expose proprietary APIs, Gardener‚Äôs approach is inherently Kubernetes-native and multi-cloud. This unified API is comprehensive, offering a consistent way to manage diverse cluster landscapes.</p><p>Its nature as a fully open-source project is particularly relevant for initiatives like NeoNephos, which aim to build sovereign cloud solutions. All core features, stable releases, and essential operational components are available to the community. This inherent cloud-native, Kubernetes-centric design, coupled with its open-source nature and ability to run on diverse infrastructures (including on-premise and local cloud providers), provides the transparency, control, and technological independence crucial for digital sovereignty. Gardener delivers full sovereign control <em>today</em>, enabling organizations to run all modern applications and services at scale with complete authority over their infrastructure and data. This is a significant reason why many cloud providers and enterprises that champion sovereignty are choosing Gardener as their foundation and actively contributing to its ecosystem.</p><h3 id=operational-depth-reflecting-real-world-scale>Operational Depth Reflecting Real-World Scale<a class=td-heading-self-link href=#operational-depth-reflecting-real-world-scale aria-label="Heading self-link"></a></h3><p>Gardener&rsquo;s operational maturity is a direct reflection of its long evolution, shaped by the demands of enterprise users and real-world, large-scale deployments. This maturity translates into statistical evidence and track records of uptime for end-users and their critical services. For instance, Gardener includes fully automated, incremental etcd backups with a recovery point objective (RPO) of five minutes and supports autonomous, hands-off restoration workflows via <code>etcd-druid</code>. Features like Vertical Pod Autoscalers (VPAs), PodDisruptionBudgets (PDBs), NetworkPolicies, PriorityClasses, and sophisticated pod placement strategies are integral to Gardener&rsquo;s offering, ensuring high availability and fault tolerance. Gardener&rsquo;s automation deals with many of the usual exceptions and does not require human DevOps intervention for most operational tasks. Gardener&rsquo;s commitment to robust security is evident in <a href=https://gardener.cloud/blog/2021/09.12-navigating-cloud-native-security/#gardeners-proactive-security-posture>Gardener&rsquo;s proactive security posture</a>, which has proven effective in real-world scenarios. This depth of experience and automation ultimately translates into first-class Service Level Agreements (SLAs) that businesses can trust and rely on. As a testament to this, SAP entrusts Gardener with its Systems of Record. This level of operational excellence enables Gardener to meet the expectations of today‚Äôs most demanding Kubernetes use cases.</p><h3 id=conclusion-a-solid-foundation-for-your-kubernetes-strategy>Conclusion: A Solid Foundation for Your Kubernetes Strategy<a class=td-heading-self-link href=#conclusion-a-solid-foundation-for-your-kubernetes-strategy aria-label="Heading self-link"></a></h3><p>For enterprises and organizations seeking a comprehensive, truly open-source solution for managing the full lifecycle of Kubernetes clusters at scale, Gardener offers a compelling proposition. Its mature architecture, rich feature set, operational robustness, built-in enterprise governance capabilities, and commitment to the open-source community provide a solid foundation for running demanding Kubernetes workloads with confidence. This makes it a suitable technical underpinning for ambitious projects like NeoNephos, contributing to a future of greater digital autonomy.</p><p>We invite you to explore <a href=https://gardener.cloud/>Gardener</a> and discover how it can empower your enterprise-grade and -scale Kubernetes journey.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-58f0aa5a00b559e8bf3d94f8b205a4a9>April</h1><div class="td-byline mb-4"><time datetime=2025-04-17 class=text-body-secondary>Thursday, April 17, 2025</time></div></div><div class=td-content><h1 id=pg-8494dc13a2a0383601c075af04a527f9>Leaner Clusters, Lower Bills: How Gardener Optimized Kubernetes Compute Costs</h1><div class="td-byline mb-4"><time datetime=2025-04-17 class=text-body-secondary>Thursday, April 17, 2025</time></div><p>As organizations embrace Kubernetes for managing containerized applications at scale, the underlying infrastructure costs, particularly for compute resources, become a critical factor. Gardener, the open-source Kubernetes management platform, empowers organizations like SAP, STACKIT, T-Systems, and others (see <a href=https://gardener.cloud/adopter>adopters</a>) to operate tens of thousands of Kubernetes clusters efficiently across diverse environments. Gardener&rsquo;s role as a core technology in initiatives like <a href=https://neonephos.org/projects>NeoNephos</a>, aimed at advancing digital autonomy in Europe (see <a href="https://www.youtube.com/watch?v=85MDID9Ju04&amp;t=621s">KubeCon London 2025 Keynote</a> and <a href=https://neonephos.org/2025/03/31/the-linux-foundation-announces-the-launch-of-neonephos-to-advance-digital-autonomy-in-europe>press announcement</a>), further underscores the need for cost-effective and sustainable operations.</p><p>At the heart of Gardener&rsquo;s architecture is the concept of &ldquo;Kubeception&rdquo; (see <a href="https://github.com/gardener/gardener?tab=readme-ov-file#gardener">readme</a> and <a href=/docs/gardener/concepts/architecture/>architecture</a>): Gardener runs <em>on</em> Kubernetes (called a <strong>runtime cluster</strong>), facilitates access <em>through</em> a self-managed node-less Kubernetes cluster (called the <strong>garden cluster</strong>), manages Kubernetes control planes as pods <em>within</em> self-managed Kubernetes clusters that provide high scalability to Gardener (called <strong>seed clusters</strong>), and <em>provisions</em> end-user Kubernetes clusters (called <strong>shoot clusters</strong>). Therefore, optimizing Gardener&rsquo;s own Kubernetes-related resource consumption directly translates into cost savings across all these layers, benefiting both Gardener service providers and the end-users consuming the managed clusters.</p><p>While infrastructure costs span compute, storage, and networking, compute resources (the virtual machines running Kubernetes nodes) typically represent the largest share of the bill. Over the past years, the Gardener team has undertaken a significant effort to optimize these costs. This blog post details our journey, focusing heavily on the compute optimizations that go beyond standard autoscaling practices, ultimately delivering substantial savings that benefit the entire Gardener ecosystem.</p><p>We&rsquo;ll build upon the foundations laid out in our <a href=/docs/guides/applications/shoot-pod-autoscaling-best-practices/>Pod Autoscaling Best Practices Guide</a>. You may want to check it out beforehand, as we&rsquo;ll only touch upon a few key recommendations from it in this blog post, not delving into the full depth required for effective pod autoscaling ‚Äì a prerequisite for the compute optimizations discussed here.</p><h2 id=visibility-and-initial-measures>Visibility and Initial Measures<a class=td-heading-self-link href=#visibility-and-initial-measures aria-label="Heading self-link"></a></h2><h3 id=know-your-spending-leveraging-observability-and-iaas-cost-tools>Know Your Spending: Leveraging Observability and IaaS Cost Tools<a class=td-heading-self-link href=#know-your-spending-leveraging-observability-and-iaas-cost-tools aria-label="Heading self-link"></a></h3><p>You can&rsquo;t optimize what you can&rsquo;t measure. Our first step was to gain deep visibility into our spending patterns. We leveraged:</p><ul><li><strong>IaaS Cost Reports & Alerts:</strong> Regularly analyzing detailed cost breakdowns from cloud providers (AWS Cost Explorer, Azure Cost Management, GCP Billing Reports) helped us identify major cost drivers across compute, storage, and network usage. Setting up alerts for cost anomalies makes us aware of regressions and unexpected budget overruns.</li><li><strong>Cloud Provider Recommendation Tools:</strong> Tools like AWS Trusted Advisor, Azure Advisor&rsquo;s Cost recommendations, and Google Cloud&rsquo;s machine type rightsizing recommendations provided initial, manual pointers towards obvious inefficiencies like underutilized virtual machines or suboptimal instance types.</li><li><strong>Internal Usage Reports:</strong> We generated custom reports detailing our own resource consumption. This helped identify and drive down the number and uptime of development and other non-production clusters. Automating the configuration of Gardener&rsquo;s <a href=/docs/gardener/shoot/shoot_hibernate/>cluster hibernation feature</a> or reporting on clusters with poor hibernation schedules further curbed unnecessary spending. These insights are now integrated into the Gardener Dashboard (our GUI).</li></ul><h3 id=the-reserved-instance--savings-plan-imperative-planning-for-discounts>The Reserved Instance / Savings Plan Imperative: Planning for Discounts<a class=td-heading-self-link href=#the-reserved-instance--savings-plan-imperative-planning-for-discounts aria-label="Heading self-link"></a></h3><p>Cloud providers offer significant discounts for commitment: Reserved Instances (RIs) on AWS/Azure, Savings Plans (SPs) on AWS/Azure, and Committed Use Discounts (CUDs) on GCP. However, maximizing their benefit requires careful planning, which is not the primary subject of this blog post. Companies typically have tools that generate recommendations from cost reports, suggesting the purchase of new RIs, SPs, or CUDs if on-demand usage consistently increases. Two key learnings emerged in this context, though:</p><ul><li><strong>Coordination between Operations and Controlling:</strong> We discovered that technical optimizations and discount commitment purchases <em>must</em> go hand-in-hand. A significant 20% utilization improvement can be completely negated if the remaining workload runs on expensive on-demand instances because the RI/SP/CUD purchase didn&rsquo;t account for the change. On-demand pricing can easily be twice or more expensive than committed pricing.</li><li><strong>Commitments vs. Spot Pricing:</strong> While Spot Instances/Preemptible virtual machines offer deep discounts, their ephemeral nature makes them unsuitable for critical control plane components. For predictable baseline workloads, well-planned RIs/SPs/CUDs provide substantial, reliable savings and are often more beneficial overall. Spot Instance/Preemptible VM discounts are generally not higher than, and often less than, RI/SP/CUD discounts for comparable commitment levels.</li></ul><h3 id=early-wins-finding-and-eliminating-resource-waste>Early Wins: Finding and Eliminating Resource Waste<a class=td-heading-self-link href=#early-wins-finding-and-eliminating-resource-waste aria-label="Heading self-link"></a></h3><p>We also actively looked for waste, specifically orphaned resources. Development and experimentation inevitably lead to forgotten resources (virtual machines, disks, load balancers, etc.). We implemented processes like requiring all resources to include a personal identifier in the name or as a label/tag to facilitate later cleanup. Initially, we generated simple reports, but it became clear that this task required a more professional approach. Unaccounted-for resources aren&rsquo;t just costly; they can also pose security risks or indicate security incidents. Therefore, we developed the <a href=https://github.com/gardener/inventory><code>gardener/inventory</code></a> tool. This tool understands Gardener installations and cross-references expected cloud provider resources (based on Gardener&rsquo;s desired state and implementation) against actually existing resources. It acts as an additional safety net, alerting on discrepancies (e.g., unexpected load balancers for a seed, unmanaged virtual machines in a VPC) which could indicate either cost leakage or a potential security issue, complementing Gardener&rsquo;s existing security measures like high-frequency credentials rotation, image signing and admission, network policies, Falco, etc.</p><h3 id=consolidation-avoiding-a-fragmented-seed-landscape>Consolidation: Avoiding a Fragmented Seed Landscape<a class=td-heading-self-link href=#consolidation-avoiding-a-fragmented-seed-landscape aria-label="Heading self-link"></a></h3><p>If possible, avoid operating too many small seeds unless required by regulations or driven by end-user demand. As Gardener supports control plane migration, you can consolidate your control planes into fewer, larger seeds where reasonable. Since starting Gardener in production in 2017, we&rsquo;ve encountered technological advancements (e.g., Azure Availability Sets to Zones) and corrected initial misconfigurations (e.g., too-small CIDR ranges limiting pod/node counts) that necessitated recreating seeds. While hard conflicts (like seed/shoot cluster IP address overlaps) can sometimes block migration to differently configured seeds, you can often at least merge multiple seeds into one or fewer. The key takeaway is that a less fragmented seed landscape generally leads to better efficiency.</p><p>However, there is a critical caveat: Gardener allows control planes to reside in different regions (or even different cloud providers) than their worker nodes. This flexibility comes at the cost of inter-regional or internet network traffic. These additional network-related costs can easily negate efficiency gains from seed consolidation. Therefore, consolidate thoughtfully, being mindful that excessive consolidation across regions can significantly increase network costs (intra-region traffic is cheaper than inter-region traffic, and internet traffic is usually the most expensive).</p><h2 id=quick-wins-in-networking-and-storage>Quick Wins in Networking and Storage<a class=td-heading-self-link href=#quick-wins-in-networking-and-storage aria-label="Heading self-link"></a></h2><p>While compute was our main focus, we also addressed significant cost drivers in networking and storage early on.</p><h3 id=centralized-ingress--caching>Centralized Ingress & Caching<a class=td-heading-self-link href=#centralized-ingress--caching aria-label="Heading self-link"></a></h3><ul><li><strong>Centralized Ingress:</strong> In Gardener&rsquo;s early days, each shoot control plane had its own Load Balancer (LB), plus another for the reverse tunnel connection to worker nodes (to reach webhooks, scrape metrics, stream logs, <code>exec</code> into pods, etc.). This proliferation of LBs was expensive. We transitioned to a model using a central Istio ingress-gateway per seed cluster with a single LB, leveraging SNI (Server Name Indication) routing to direct traffic to the correct control plane API servers. We also reversed the connection direction: shoots now connect <em>to</em> seed clusters, and seeds connect <em>to</em> the garden cluster. This reduced the need for LBs exposing seed components and enabled <em>private</em> shoots or even <em>private</em> seeds behind firewalls.</li><li><strong>Registry Cache:</strong> Pulling container images for essential components (like CNI, CSI drivers, kube-proxy) on every new node startup generated significant network traffic and costs. We implemented a <a href=https://github.com/gardener/gardener-extension-registry-cache>registry cache extension</a>, drastically reducing external image pulls (see <a href=https://gardener.cloud/blog/2024/04-22-gardeners-registry-cache-extension-another-cost-saving-win-and-more>blog post</a>).</li></ul><h3 id=smarter-networking-habits>Smarter Networking Habits<a class=td-heading-self-link href=#smarter-networking-habits aria-label="Heading self-link"></a></h3><ul><li><strong>Efficient API Usage:</strong> Well-implemented controllers use <code>watch</code> requests rather than frequent <code>list</code> requests to minimize API server load and improve responsiveness. Leveraging server-side filtering via <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors>label selectors</a> and <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors>field selectors</a> reduces the amount of data transferred.</li><li><strong>Reducing Cross-Zonal Traffic:</strong> Data transfer between availability zones, necessary for highly available control planes, is generally more expensive than within a single zone. We enabled Kubernetes&rsquo; <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints>Topology Aware Routing</a> to help route API server traffic within the same zone where possible, reducing cross-zonal traffic and therefore costs (see <a href=https://github.com/gardener/gardener/issues/6718>Gardener Issue #6718</a>).</li><li><strong>Avoiding Large Resources:</strong> Storing large amounts of data directly in Kubernetes resources (ConfigMaps, Secrets) is inefficient and strains etcd and the network. We utilize blob stores for large payloads, such as control plane etcd or state backups used for automated restoration or control plane migration (with data compressed and encrypted in transit and at rest).</li><li><strong>Regression Monitoring:</strong> Implementing regression monitoring for network traffic helped catch seemingly innocent code changes that could inadvertently cause massive spikes in data transfer costs.</li></ul><h3 id=conscious-storage-consumption>Conscious Storage Consumption<a class=td-heading-self-link href=#conscious-storage-consumption aria-label="Heading self-link"></a></h3><p>Storage costs were addressed by being mindful of Persistent Volume Claim (PVC) size and performance tiers (e.g., standard HDD vs. premium SSD). Choosing the right storage class based on actual workload needs prevents overspending on unused capacity or unnecessary IOPS.</p><h2 id=deep-dive-into-compute-cost-optimization>Deep Dive into Compute Cost Optimization<a class=td-heading-self-link href=#deep-dive-into-compute-cost-optimization aria-label="Heading self-link"></a></h2><p>This is where the most significant savings were realized. Optimizing compute utilization in Kubernetes is a multi-faceted challenge involving the interplay of several components.</p><h3 id=understanding-utilization-the-interplay-of-scheduler-cluster-autoscaler-hpa-and-vpa>Understanding Utilization: The Interplay of Scheduler, Cluster Autoscaler, HPA, and VPA<a class=td-heading-self-link href=#understanding-utilization-the-interplay-of-scheduler-cluster-autoscaler-hpa-and-vpa aria-label="Heading self-link"></a></h3><p>We think of utilization optimization in two stages:</p><ol><li><strong>Packing Pods onto Nodes (Requests vs. Allocatable):</strong> How efficiently are the resource <em>requests</em> of your pods filling up the <em>allocatable</em> capacity of your nodes? This is primarily influenced by the Kube-Scheduler and the Cluster Autoscaler (CA).</li><li><strong>Right-Sizing Pods (Usage vs. Requests):</strong> How closely does the actual resource <em>usage</em> of your pods match their <em>requests</em>? This is where Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA) come in.</li></ol><p>You need to optimize <em>both</em> stages for maximum efficiency.</p><h3 id=optimizing-scheduling-bin-packing-and-pod-priorities-with-kube-scheduler>Optimizing Scheduling: Bin-Packing and Pod Priorities with Kube-Scheduler<a class=td-heading-self-link href=#optimizing-scheduling-bin-packing-and-pod-priorities-with-kube-scheduler aria-label="Heading self-link"></a></h3><ul><li><strong>Bin-Packing:</strong> By default, Kube-Scheduler tries to spread pods across nodes (using the <code>LeastAllocated</code> strategy). For cost optimization, <em>packing</em> pods tightly onto fewer nodes (using the <code>MostAllocated</code> strategy, often called bin-packing) is more effective. Gardener runs Kubernetes control planes as pods on seed clusters. Switching the Kube-Scheduler profile in our seed clusters to prioritize bin-packing yielded over 20% reduction in machine costs for these clusters simply by requiring fewer nodes. We also made this scheduling profile available for shoot clusters (see <a href=https://github.com/gardener/gardener/pull/6251>Gardener PR #6251</a>).</li><li><strong>Pod Priorities:</strong> Assigning proper <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption>Pod Priorities</a> is important not just for stability but also for cost. High-priority pods (like control plane components) can preempt lower-priority pods if necessary, reducing the need to maintain excess capacity just in case a critical pod needs scheduling space. This avoids unnecessary over-provisioning.</li></ul><h3 id=voluntary-disruptions-pod-disruption-budgets>Voluntary Disruptions: Pod Disruption Budgets<a class=td-heading-self-link href=#voluntary-disruptions-pod-disruption-budgets aria-label="Heading self-link"></a></h3><ul><li><strong>Pod Disruption Budgets:</strong> Defining proper <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb>Pod Disruption Budgets (PDBs)</a> helps manage and steer voluntary disruptions safely. We define them consistently for all Gardener components. This provides the necessary control to rebalance, compact, or generally replace underlying machines as needed by us or our automation, contributing to cost efficiency by enabling node consolidation.</li></ul><h3 id=enabling-higher-pod-density-per-node>Enabling Higher Pod Density per Node<a class=td-heading-self-link href=#enabling-higher-pod-density-per-node aria-label="Heading self-link"></a></h3><ul><li><strong>Node Configuration:</strong> To effectively utilize larger instance types and enable better bin-packing, nodes must be configured to handle more pods. We observed nodes becoming pod-bound (unable to schedule more pods despite available CPU/memory). To prevent this, ensure you provide:<ul><li>A large enough <code>--node-cidr-mask-size</code> (e.g., <code>/22</code> for ~1024 IPs, though assume ~80% effective due to IP reuse; see <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager>kube-controller-manager docs</a>) to allocate sufficient IPs per node.</li><li>Sufficient <code>--kube-reserved</code> resources (see <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet>kubelet docs</a>) to account for system overhead.</li><li>An increased <code>--max-pods</code> value (again, see <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet>kubelet docs</a>) to inform the kubelet and scheduler of the node&rsquo;s actual pod capacity.</li></ul></li></ul><h3 id=fine-tuning-the-cluster-autoscaler-scaling-nodes-efficiently>Fine-Tuning the Cluster Autoscaler: Scaling Nodes Efficiently<a class=td-heading-self-link href=#fine-tuning-the-cluster-autoscaler-scaling-nodes-efficiently aria-label="Heading self-link"></a></h3><p>The cluster autoscaler (CA) adds or removes nodes based on pending pods and node utilization. We tuned its behavior for better cost efficiency:</p><ul><li><code>--scale-down-unneeded-time=15m</code>: Time a node must be underutilized before CA considers it for removal, allowing removal of persistently unneeded capacity.</li><li><code>--scale-down-delay-after-add=30m</code>: Prevents CA from removing a node too soon after adding one, reducing potential node thrashing during fluctuating load.</li><li><code>--scale-down-utilization-threshold=0.9</code>: We significantly increased this threshold (default is 0.5). It instructs CA to attempt removing any node running below 90% utilization <em>if</em> it can safely reschedule the existing pods onto other available nodes; otherwise, it does nothing. We have run with this setting successfully for a long time, supported by properly tuned pod priorities, PDBs managing voluntary disruptions, highly available control planes, and Kubernetes&rsquo; level-triggered, asynchronous nature.</li></ul><h3 id=mastering-pod-autoscaling-hpa-vpa-and-best-practices>Mastering Pod Autoscaling: HPA, VPA, and Best Practices<a class=td-heading-self-link href=#mastering-pod-autoscaling-hpa-vpa-and-best-practices aria-label="Heading self-link"></a></h3><p>Right-sizing pods dynamically is key. Kubernetes offers HPA and VPA:</p><ul><li><strong>Horizontal Pod Autoscaling (HPA):</strong> Scales the <em>number</em> of pod replicas based on metrics (CPU/memory utilization, custom metrics). Ideal for stateless applications handling variable request loads.</li><li><strong>Vertical Pod Autoscaler (VPA):</strong> Adjusts the CPU/memory <em>requests</em> of existing pods. Ideal for stateless and also stateful applications or workloads with fluctuating resource needs over time, without changing replica count.</li></ul><h3 id=our-best-practices--learnings>Our Best Practices & Learnings:<a class=td-heading-self-link href=#our-best-practices--learnings aria-label="Heading self-link"></a></h3><ul><li><strong>Combine HPA and VPA for API Servers Safely:</strong> You <em>can</em> use HPA and VPA together, even on the same metric (like CPU), but careful configuration is essential. The key is to configure HPA to scale based on the <em>average value</em> (<code>target.type: AverageValue</code>) rather than <em>utilization percentage</em> (<code>target.type: Utilization</code>). This prevents conflicts where VPA changes the requests, which would otherwise immediately invalidate HPA&rsquo;s utilization calculation.<ul><li><em>Example HPA targeting average CPU/Memory values:</em><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  minReplicas: 3
</span></span><span style=display:flex><span>  maxReplicas: 12
</span></span><span style=display:flex><span>  metrics:
</span></span><span style=display:flex><span>  - resource:
</span></span><span style=display:flex><span>      name: cpu
</span></span><span style=display:flex><span>      target:
</span></span><span style=display:flex><span>        averageValue: 6 # Target 6 cores average usage per pod (Note: String value often required)
</span></span><span style=display:flex><span>        type: AverageValue
</span></span><span style=display:flex><span>    type: Resource
</span></span><span style=display:flex><span>  - resource:
</span></span><span style=display:flex><span>      name: memory
</span></span><span style=display:flex><span>      target:
</span></span><span style=display:flex><span>        averageValue: 24Gi <span style=color:green># Target 24Gi average usage per pod</span>
</span></span><span style=display:flex><span>        type: AverageValue
</span></span><span style=display:flex><span>    type: Resource
</span></span><span style=display:flex><span>  behavior: <span style=color:green># Fine-tune scaling behavior</span>
</span></span><span style=display:flex><span>    scaleDown:
</span></span><span style=display:flex><span>      policies:
</span></span><span style=display:flex><span>      - periodSeconds: 300
</span></span><span style=display:flex><span>        type: Pods
</span></span><span style=display:flex><span>        value: 1
</span></span><span style=display:flex><span>      selectPolicy: Max
</span></span><span style=display:flex><span>      stabilizationWindowSeconds: 1800
</span></span><span style=display:flex><span>    scaleUp:
</span></span><span style=display:flex><span>      policies:
</span></span><span style=display:flex><span>      - periodSeconds: 60
</span></span><span style=display:flex><span>        type: Percent
</span></span><span style=display:flex><span>        value: 100
</span></span><span style=display:flex><span>      selectPolicy: Max
</span></span><span style=display:flex><span>      stabilizationWindowSeconds: 60
</span></span><span style=display:flex><span>  scaleTargetRef:
</span></span><span style=display:flex><span>    apiVersion: apps/v1
</span></span><span style=display:flex><span>    kind: Deployment
</span></span><span style=display:flex><span>    name: kube-apiserver
</span></span></code></pre></div></li></ul></li><li><strong>Tune VPA Configuration:</strong><ul><li>We adjusted VPA parameters like <code>--target-cpu-percentile</code> / <code>--target-memory-percentile</code> (determining the percentile of historical usage data to include in target recommendations, ignoring spikes above) and margin/bound parameters to make VPA less sensitive to tiny spikes and react faster and more accurately to sustained changes.</li><li>We also tuned parameters like <code>--cpu-histogram-decay-half-life</code> (from 24h to 15m) and <code>--recommendation-lower-bound-cpu-percentile</code> (from 0.5 to 0.7) to follow changes in CPU utilization more closely (work on memory is ongoing).</li><li><strong>VPA <code>minAllowed</code>:</strong> We set <code>minAllowed</code> (per VPA resource) based on observed usage patterns and historical outage data related to VPA scaling down too aggressively.</li><li><strong>VPA <code>maxAllowed</code>:</strong> We set <code>maxAllowed</code> (per VPA controller) to prevent request recommendations from exceeding node capacity. We found <code>maxAllowed</code> couldn&rsquo;t be configured centrally in the VPA controller, so we contributed this feature upstream (see <a href=https://github.com/kubernetes/autoscaler/issues/7147>Kubernetes Autoscaler Issue #7147</a> and <a href=https://github.com/kubernetes/autoscaler/pull/7560>corresponding PR</a>).</li></ul></li><li><strong>Set Pod Requests:</strong> We always set CPU and memory requests for our containers or let VPA manage those.</li><li><strong>Tune Pod Requests:</strong> We systematically processed hundreds of components:<ul><li>Some deployments were placed under VPA management. Others (very small, below VPA&rsquo;s resolution of ~10m cores / 10Mi memory) were removed from VPA and given static requests.</li><li><strong>&ldquo;Initial&rdquo; Requests:</strong> For pods managed by VPA, we set initial requests to the observed P5 (5th percentile) of historical usage. This provides a reasonable starting point for VPA.</li><li><strong>&ldquo;Static&rdquo; Requests:</strong> For pods not managed by VPA, we set requests to the P95 (95th percentile). This ensures they generally have enough resources; only exceptional spikes might cause issues, where VPA wouldn&rsquo;t typically help either.</li></ul></li><li><strong>Quality of Service (QoS):</strong> Prefer the <code>Burstable</code> QoS class (requests set, ideally no limits) for most workloads. Avoid <code>BestEffort</code> (no requests/limits), as these pods are the first to be evicted under pressure. Avoid <code>Guaranteed</code> (requests match limits), as limits often cause more harm than good. See our <a href=/docs/guides/applications/shoot-pod-autoscaling-best-practices/#quality-of-service-qos>Pod Autoscaling Best Practices Guide</a>. Pods in the <code>Guaranteed</code> QoS class, or generally those with limits, will be actively CPU-throttled and can be OOMKilled even if the node has ample spare capacity. Worse, if containers in the pod are under VPA, their CPU requests/limits often won&rsquo;t scale up appropriately because CPU throttling goes unnoticed by VPA.<ul><li><strong>Avoid Limits:</strong> In Gardener&rsquo;s context (and often also elsewhere), setting CPU limits offers few advantages and significant disadvantages, primarily unnecessary throttling. Setting memory limits <em>can</em> prevent runaway processes but may also prematurely kill pods. We generally avoid setting limits unless the theoretical maximum resource consumption of a component is well understood. When unsure, let VPA manage requests and rely on monitoring/alerting for excessive usage.</li></ul></li></ul><h2 id=data-driven-machine-type-selection>Data-Driven Machine Type Selection<a class=td-heading-self-link href=#data-driven-machine-type-selection aria-label="Heading self-link"></a></h2><h3 id=continuous-monitoring-understanding-how-well-our-machines-are-utilized>Continuous Monitoring: Understanding How Well Our Machines are Utilized<a class=td-heading-self-link href=#continuous-monitoring-understanding-how-well-our-machines-are-utilized aria-label="Heading self-link"></a></h3><p>Before optimizing machine type selection, we established comprehensive machine utilization monitoring. This was important during individual improvement steps to validate their effectiveness. We collect key metrics per Gardener installation, cloud provider, seed, and worker pool, and created dashboards to visualize and monitor our machine costs. These dashboards include:</p><ul><li>Total CPU [in thousand cores], Total Memory [in TB], Total Number of Control Planes [count]</li><li>Used Capacity CPU [%], Used Capacity Memory [%], Unused vs. Capacity Cost [Currency]</li><li>Requested Allocatable CPU [%], Requested Allocatable Memory [%], Unrequested vs. Allocatable Cost [Currency]</li><li>Used Requested CPU [%], Used Requested Memory [%], Unused vs. Requested Cost [Currency]</li><li>Used Reserved CPU [%, can exceed 100%], Used Reserved Memory [%, can exceed 100%], Unused vs. Reserved Cost [Currency]</li><li>Nodes with >99% filling levels, broken down by CPU, memory, volumes, and pods (to identify the most critical resource blocking further usage)</li><li>Effective CPU:memory ratio of the workload (more on that later)</li></ul><h3 id=why-machine-types-matter-size-ratios-generations-and-hidden-constraints>Why Machine Types Matter: Size, Ratios, Generations, and Hidden Constraints<a class=td-heading-self-link href=#why-machine-types-matter-size-ratios-generations-and-hidden-constraints aria-label="Heading self-link"></a></h3><p>Selecting the right machine type is critical for cost efficiency. Several factors come into play:</p><ul><li><strong>Size:</strong> Larger machines generally lead to less fragmentation (less wasted CPU/memory remainder per node) and better overhead efficiency (system components like kubelet/containerd consume a smaller percentage of total resources). However, smaller machines can be better for low-load scenarios while meeting high-availability constraints (e.g., needing to spread critical pods across 3 zones requires at least 3 nodes).</li><li><strong>CPU:Memory Ratio:</strong> Cloud providers offer instance families with different CPU:memory ratios (e.g., high-cpu 1:2, standard 1:4, high-memory 1:8). Matching the instance ratio to your workload&rsquo;s aggregate CPU:memory request ratio minimizes waste.</li><li><strong>Generations:</strong> Newer instance generations usually offer better performance and, crucially, better price-performance. This can also shift the effective CPU:memory ratio required by the workload due to performance differences.</li><li><strong>Hidden Constraints: Volume Limits:</strong> This proved to be a <em>major</em> factor, especially on AWS and Azure. Each instance type has a maximum number of network-attached volumes it can support. Gardener control planes, each with its own etcd cluster requiring persistent volumes for each replica, are heavily impacted. We often found ourselves limited by volume attachments long before hitting CPU or memory limits. Interestingly, ARM-based instance types on Azure support a slightly higher volume limit.</li></ul><h3 id=the-case-for-dedicated-pools-isolating-workloads>The Case for Dedicated Pools: Isolating Workloads<a class=td-heading-self-link href=#the-case-for-dedicated-pools-isolating-workloads aria-label="Heading self-link"></a></h3><p>While mixing diverse workloads seems efficient at first glance, dedicated node pools for specific workload types proved beneficial for several reasons:</p><ul><li><strong>Handling <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node><code>safe-to-evict: false</code></a>:</strong> Some pods (like single-replica stateful components for non-HA clusters) cannot be safely evicted by the Cluster Autoscaler. Mixing these with evictable pods on the same node can prevent the CA from scaling down that node, even if it&rsquo;s underutilized, negating cost savings. Placing these non-evictable pods in a dedicated pool (where scale-down might be disabled or carefully managed) isolates this behavior.</li><li><strong>Volume Concentration:</strong> Our &ldquo;etcd&rdquo; worker pools host primarily etcd pods (high volume count) and daemonsets, while &ldquo;standard&rdquo; pools host API servers, controllers, etc. (lower volume concentration). This difference influences the optimal machine type due to volume attachment limits.</li><li><strong>Preventing Scheduling Traps:</strong> Ensure critical, long-running pods (like Istio gateways) have node affinities/selectors to land only on their preferred, optimized node pools. Avoid them landing on temporary, large nodes spun up for short-lived bulky pods; if such a pod prevents the large node from scaling down (e.g., because the pool is at its minimum node count), the CA won&rsquo;t automatically replace the underutilized large node with a smaller one. That&rsquo;s a concept called &ldquo;workload consolidation&rdquo;, today only supported by <a href=https://github.com/kubernetes-sigs/karpenter>Karpenter</a>, which isn&rsquo;t supporting as many cloud providers as CA.</li></ul><h3 id=analyzing-workload-profiles-finding-the-optimal-instance-size-and-family>Analyzing Workload Profiles: Finding the Optimal Instance Size and Family<a class=td-heading-self-link href=#analyzing-workload-profiles-finding-the-optimal-instance-size-and-family aria-label="Heading self-link"></a></h3><p>Early on, we used a guide for operators to estimate a reasonable machine size for a seed cluster based on the number of hosted control planes, e.g.:</p><table><thead><tr><th>Optimal<br>Worker Pool (CPUxMem+Vols)</th><th>Very Low Seed Utilization<br>0 &lt;= |control planes| &lt; 15</th><th>Low Seed Utilization<br>5 &lt;= |control planes| &lt; 30</th><th>Medium Seed Utilization<br>10 &lt;= |control planes| &lt; 70</th><th>High Seed Utilization<br>30 &lt;= |control planes| &lt; 180</th><th>Very High Seed Utilization<br>120 &lt;= |control planes| &lt; ‚àû</th></tr></thead><tbody><tr><td>AWS</td><td><code>m5.large</code>(2x8+26)</td><td><code>r7i.large</code>(2x16+32)</td><td><code>r7i.xlarge</code>(4x32+32)</td><td><code>r7i.2xlarge</code>(8x64+32)</td><td><code>r7i.2xlarge</code>(8x64+32)</td></tr><tr><td>Azure</td><td><code>Standard_D2s_v5</code>(2x8+4)</td><td><code>Standard_D4s_v5</code>(4x16+8)</td><td><code>Standard_D8s_v5</code>(8x32+16)</td><td><code>Standard_D16s_v5</code>(16x64+32)</td><td><code>Standard_D16s_v5</code>(16x64+32)</td></tr><tr><td>GCP</td><td><code>n1-standard-2</code>(2x8+127)</td><td><code>n1-standard-4</code>(4x15+127)</td><td><code>n1-standard-8</code>(8x30+127)</td><td><code>n1-standard-16</code>(16x60+127)</td><td><code>n1-standard-16</code>(16x60+127)</td></tr></tbody></table><p>This guide also recommended specific instance families. Choosing the right family requires calculating the workload&rsquo;s aggregate CPU:memory ratio (total requested CPU : total requested memory across similar workloads). For example, 1000 cores and 6000 GB memory yields a 1:6 ratio.</p><p>Next, one must calculate the cost per core and per GB for different instance families and determine the break-even CPU:memory ratio ‚Äì the point where the resource waste of two families is equal. The cluster autoscaler doesn&rsquo;t perform this cost-aware analysis; it always weights CPU and memory equally (1:1).</p><p>To find the optimal family manually, we followed these steps when adding new generations/families:</p><ul><li><strong>Cost per Resource Unit:</strong> Determine the effective cost per core and per GB. Example:<ul><li>Instance A (2 cores, 4 GB) costs ‚Ç¨48/month.</li><li>Instance B (2 cores, 8 GB) costs ‚Ç¨64/month.</li><li>Difference: 4 GB and ‚Ç¨16 -> ‚Ç¨4 per GB.</li><li>Cost of 2 cores = ‚Ç¨48 - (4 GB * ‚Ç¨4/GB) = ‚Ç¨32 -> ‚Ç¨16 per core.</li></ul></li><li><strong>Break-Even Analysis:</strong> Using the unit costs, calculate the break-even CPU:memory ratio where the cost of waste balances out between two families for your specific workload ratio.</li></ul><p>For instance, if the break-even ratio between standard (1:4) and high-memory (1:8) families is 1:5.7, and your workload runs at 1:6, the high-memory family is likely more cost-effective.</p><h3 id=automating-the-choice-a-machine-type-recommender>Automating the Choice: A Machine Type Recommender<a class=td-heading-self-link href=#automating-the-choice-a-machine-type-recommender aria-label="Heading self-link"></a></h3><p>This manual process was tedious, error-prone, and infrequently performed, leading to suboptimal machine types running in many seeds. To address this, we developed an automated pool recommender based on the following principles:</p><ol><li><p><strong>Comprehensive Data Collection:</strong> The recommender gathers metrics across the entire Gardener installation for specific seed sets (groups of seeds with similar configurations like provider and region). For every relevant seed, it collects:</p><ul><li><strong>Node Metadata & Specs:</strong> Instance type, pool, zone, capacity, allocatable resources.</li><li><strong>CSI Node Info:</strong> Maximum attachable volume counts per node.</li><li><strong>Pod Specs:</strong> Resource requests (CPU, memory) for all pods, distinguishing daemonset pods.</li><li><strong>Actual Node Usage:</strong> Detailed usage statistics obtained directly from the <a href=https://kubernetes.io/docs/reference/instrumentation/node-metrics>kubelet summary API</a> (<code>/api/v1/nodes/NODENAME/proxy/stats/summary</code>). This provides actual cgroup-level data on CPU and memory consumption for kubelet, container runtime, system overhead, and individual pods. Especially for memory, this was the only reliable method we found to get accurate working set bytes overall (simply summing pod metrics is inaccurate due to page cache/sharing; see kernel docs for <a href=https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt>cgroup-v1</a> and <a href=https://www.kernel.org/doc/Documentation/cgroup-v2.txt>cgroup-v2</a>).</li></ul></li><li><p><strong>Analyzing the Data:</strong> Before recommending <em>new</em> types, the recommender calculates key metrics that act as predictors and provide context:</p><ul><li><strong>Workload Ratios:</strong> Requested Core : Requested GB, Attached Volume : Requested GB, Scheduled Pod : Requested GB.</li><li><strong>Measured Overhead Ratios:</strong> <em>Measured</em> Reserved Core : Pod Count, <em>Measured</em> Reserved GB : Pod Count.</li><li><strong>Aggregation:</strong> Machines are grouped by pool within a seed set.</li><li><strong>Performance Normalization:</strong> CPU metrics (usage) are normalized based on relative performance indicators of the analyzed machine type.</li></ul></li><li><p><strong>Simulating Workload on Candidate Machines:</strong> This is the core recommendation logic:</p><ul><li><strong>Candidate Iteration:</strong> The system iterates through all <em>potential</em> machine types available for the specific provider and region(s).</li><li><strong>Resource Calculation per Candidate:</strong> For each candidate machine type:<ul><li>Calculate <code>kube-reserved</code>: Estimates CPU/memory needed for kubelet/runtime using our measurement-based model, tailored to the candidate&rsquo;s capacity (more on that later).</li><li>Account for DaemonSets: Subtracts the average CPU/memory <em>requests</em> of DaemonSet pods (derived from current aggregated pool data).</li><li>Performance Adjustment: Adjusts CPU calculations (reserved, daemonset, workload requests) based on the candidate&rsquo;s performance factor relative to a baseline.</li><li>Calculate Allocatable Resources: Determines CPU/memory available for workload pods after subtracting reserved and DaemonSet resources.</li><li>Unschedulable Buffer: Reduces allocatable resources slightly (e.g., by the equivalent of an &ldquo;average pod&rdquo;) to account for resource fragmentation and imperfect bin-packing, slightly favoring larger nodes.</li></ul></li><li><strong>Constraint Checking & Usable Resources:</strong> Projects how much of the <em>aggregated current workload</em> (total requests) could fit onto the candidate. It considers multiple dimensions, converting them to a common unit (GB-equivalent) using the measured workload ratios:<ul><li>Performance-adjusted Allocatable CPU (converted to GB-equivalent)</li><li>Allocatable Memory (GB)</li><li>Attachable Volumes (converted to GB-equivalent)</li><li>Schedulable Pods (converted to GB-equivalent)
The <em>minimum</em> of these values determines the actual usable resources for that candidate machine type under the given workload profile ‚Äì identifying the <em><strong>true bottleneck</strong></em>, i.e. whether a candidate is CPU-, memory-, volume-, pod-, or load-bound and thus potentially suboptimal.</li></ul></li><li><strong>Cost & Waste Analysis:</strong><ul><li>Calculates the base <code>machine_costs</code> (<code>Cores * Cost per Core + GBs * Cost per GB</code>) for the candidate.</li><li>Estimates <code>excess_costs</code> (waste) per machine due to factors like:<ul><li><em>Imperfect Packing:</em> Assumes the &ldquo;last&rdquo; node in a zone is only half-utilized on average.</li><li><em>Scale-Down Disabled:</em> Increases estimated waste if scale-down is disabled.</li><li><em>Volume Packing:</em> Adds potential waste if the workload is heavily volume-constrained, assuming not all nodes can be packed efficiently with volumes.</li></ul></li></ul></li><li><strong>Efficiency Score Calculation:</strong> Computes a relative efficiency score for each candidate:
<code>Efficiency = (Cost_of_Usable_Resources) / (Base_Machine_Cost + Estimated_Excess_Cost)</code>
This score reflects how cost-effectively the candidate machine type can serve the workload, factoring in estimated waste.</li></ul></li><li><p><strong>Ranking & Selection:</strong></p><ul><li><strong>Sorting:</strong> Candidates are ranked primarily by <code>Efficiency / Cost per Core</code>. Dividing by cost per core helps prioritize newer/cheaper instance generations or those with better RI/SP coverage, while still heavily favoring the calculated efficiency.</li><li><strong>Preferred Type & Hysteresis:</strong> The top-ranked type is marked as <code>preferred</code> and receives the highest CA expander priority. A threshold (e.g., >5% efficiency improvement) prevents switching the preferred type too frequently, avoiding churn (flapping).</li><li><strong>Priority Assignment:</strong> Priorities are assigned for the cluster autoscaler expander, favoring the preferred type and then ranking others based on the sort order.</li><li><strong>Handling Existing/Legacy Pools:</strong> Ensures that pools with currently running nodes, even if suboptimal or using non-standard names, are preserved to avoid disruption. Legacy pools are tainted with a <code>NoSchedule</code> taint to allow workload to slowely migrate away from them.</li></ul></li></ol><p>This data-driven, simulation-based approach allowed us to abandon guides like above and manual operations and consistently select machine types that offer the best balance of performance and cost for the specific workloads running on our Gardener seeds.</p><h3 id=reserving-capacity-for-kubelet-and-container-runtime-tailoring-kube-reserved-beyond-workload-naive-formulas>Reserving Capacity for Kubelet and Container Runtime: Tailoring <code>kube-reserved</code> Beyond Workload-Naive Formulas<a class=td-heading-self-link href=#reserving-capacity-for-kubelet-and-container-runtime-tailoring-kube-reserved-beyond-workload-naive-formulas aria-label="Heading self-link"></a></h3><p>As pod packing density increases, accurately accounting for resources needed by the system itself (kubelet, container runtime, OS) becomes critical. Standard cloud provider formulas for <code>kube-reserved</code> (see <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet>kubelet options</a>) are often workload-naive, based only on total node CPU/memory capacity (see <a href=https://medium.com/@danielepolencic/reserved-cpu-and-memory-in-kubernetes-nodes-65aee1946afd>summary blog post</a>). They can either over-reserve (wasting resources) or under-reserve (risking node stability). Our experience showed that formulas considering only node capacity and potentially <code>maxPods</code> were often significantly inaccurate, leading to either waste or instability.</p><p>Therefore, instead of relying on static formulas, we adopted a measurement-based approach combined with predictive modeling:</p><ol><li><p><strong>Measure Actual Overhead:</strong> We utilize the data already retrieved via the kubelet summary API. By querying this endpoint across thousands of nodes for all our seeds, we collect the <em>actual</em> CPU (<code>usageNanoCores</code>) and memory (<code>workingSetBytes</code>) consumed by the <code>kubelet</code> and <code>runtime</code> system containers under various conditions (different machine types, workload profiles like ETCD pools, varying pod densities).</p></li><li><p><strong>Derive Workload-Aware Ratios:</strong> We then calculate key ratios that correlate overhead with workload characteristics, specifically pod density:</p><ul><li><code>ratio_1_used_reserved_core_to_pods</code>: Average number of pods running per actually <em>used</em> reserved core (performance-normalized across machine types).</li><li><code>ratio_1_used_reserved_gi_to_pods</code>: Average number of pods running per actually <em>used</em> reserved GB of memory.</li></ul><p>These ratios capture how much system overhead is typically generated <em>per pod</em> on average within a specific pool type for a given seed set. We explored other potential predictors (containers, probes) but found pod count to be the most useful predictor with acceptable standard deviation.</p></li><li><p><strong>Predict Expected <code>kube-reserved</code>:</strong> We use these measured ratios to <em>predict</em> the necessary <code>kube-reserved</code> for <em>any</em> candidate machine type considered by the Pool Recommender. The model works as follows:</p><ul><li><strong>Base Load:</strong> We observed a consistent base memory overhead even on lightly loaded nodes (e.g., ~200MiB with <a href="https://github.com/gardenlinux/gardenlinux?tab=readme-ov-file#garden-linux">Garden Linux</a>, Gardener&rsquo;s own Debian-based container-optimized OS) and negligible base CPU overhead.</li><li><strong>Estimate Pod-Driven Overhead:</strong> Using the predicted pod density for a candidate machine type (based on its capacity and the workload profile), we multiply this density by the measured <code>ratio_1_used_reserved_core_to_pods</code> and <code>ratio_1_used_reserved_gi_to_pods</code> to estimate the required <code>kube-reserved</code> CPU and memory, respectively. This tailors the reservation to the candidate&rsquo;s specific capacity and performance characteristics.</li></ul></li><li><p><strong>Apply Thresholds for Stability:</strong> To prevent minor fluctuations in calculated recommendations from causing constant configuration changes (increasing <code>kube-reserved</code> can trigger pod evictions), we apply thresholds (hysteresis).</p></li></ol><p>This tailored, data-driven approach to <code>kube-reserved</code> provides better cost optimization and enhanced stability compared to generic, workload-naive formulas.</p><p><em>Note on <code>system-reserved</code>:</em> You might wonder why we only discussed <code>kube-reserved</code> and not <code>system-reserved</code>. Similar to our reasoning against resource limits, configuring <code>system-reserved</code> can lead to unexpected CPU throttling or OOM kills for critical system processes outside Kubernetes&rsquo; direct management. Therefore, Gardener focuses on configuring <code>kube-reserved</code> and relies on the kubelet&rsquo;s eviction mechanisms to manage overall node pressure. See also <a href=https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#general-guidelines>Reserve Compute Resources for System Daemons</a>.</p><h2 id=looking-ahead-continuous-improvement-and-future-optimizations>Looking Ahead: Continuous Improvement and Future Optimizations<a class=td-heading-self-link href=#looking-ahead-continuous-improvement-and-future-optimizations aria-label="Heading self-link"></a></h2><p>Cost optimization is an ongoing process, not a one-time fix. We&rsquo;re actively exploring further improvements:</p><ul><li><strong>Addressing Load Imbalances:</strong> VPA assigns the same request to all pods in a managed group (Deployment/StatefulSet/DaemonSet). This is inefficient for workloads with inherent imbalances (e.g., controller leaders vs. followers, etcd leader vs. followers, uneven load distribution across DaemonSet pods).<ul><li><strong>Request-Based Load Balancing:</strong> For components like the <code>kube-apiserver</code>, default connection-based load balancing can lead to uneven load distribution that VPA handles poorly (resulting in over-provisioning for some pods, under-provisioning for others). We have implemented request-based load balancing to distribute load more evenly, allowing VPA to set more accurate requests (see <a href=https://github.com/gardener/gardener/pull/11085>related work</a>).</li><li><strong>In-place pod resource updates</strong> (a Kubernetes enhancement) would be particularly beneficial in the future, allowing VPA to adjust resources without requiring pod restarts, further improving efficiency and stability.</li></ul></li><li><strong>Exploring Cilium / Replacing kube-proxy:</strong> Initial tests suggest switching the CNI from Calico to Cilium could yield 5-10% CPU savings on worker nodes, partly because Cilium can replace kube-proxy, reducing overhead. Memory usage appears similar and Gardener has supported Cilium for years. Alternatively, to eliminate kube-proxy without changing CNIs, we could evaluate <a href=https://docs.tigera.io/calico/latest/operations/ebpf/enabling-ebpf>Calico&rsquo;s eBPF data plane</a>, which can also replace kube-proxy.</li><li><strong>ARM Architecture:</strong> We are evaluating ARM-based CPUs (AWS Graviton, Azure Cobalt, GCP Axion). They are generally cheaper per core. Even if slightly slower per core (but often with a better price-performance), they offer additional instance family options, potentially allowing a better match to the workload&rsquo;s CPU:memory ratio (e.g., a 1:6 workload x86 ratio might turn into a performance-adjusted 1:5 ARM ratio and thereby result in less waste than x86 instance families of either a 1:4 or 1:8 ratio). Additionally, Azure&rsquo;s ARM instances sometimes offer slightly higher volume attachment limits.</li></ul><h2 id=conclusion-sustainable-savings-and-key-takeaways>Conclusion: Sustainable Savings and Key Takeaways<a class=td-heading-self-link href=#conclusion-sustainable-savings-and-key-takeaways aria-label="Heading self-link"></a></h2><p>Optimizing Kubernetes compute costs at scale is a complex but rewarding endeavor. Our journey with Gardener involved a multi-pronged approach:</p><ol><li><strong>Establish Visibility:</strong> Use cloud cost tools and internal monitoring to understand spending.</li><li><strong>Strategic Purchasing:</strong> Tightly align RI/SP/CUD purchases with technical optimizations and workload forecasts.</li><li><strong>Clean Up Waste:</strong> Eliminate orphaned resources and leverage features like cluster hibernation.</li><li><strong>Tune Kubernetes Core Components:</strong> Utilize scheduler bin-packing, fine-tune cluster autoscaler parameters, and master HPA/VPA configurations, including safe combined usage.</li><li><strong>Data-Driven Machine Selection:</strong> Analyze workload profiles, use dedicated pools strategically, consider all constraints (especially non-obvious ones like volume limits), and automate machine type recommendations based on real data and simulation.</li><li><strong>Accurate Overheads:</strong> Measure and tailor <code>kube-reserved</code> based on actual system usage patterns rather than static formulas.</li></ol><p>These efforts have yielded substantial cost reductions for operating Gardener itself and, by extension, for all Gardener adopters running managed Kubernetes clusters. We hope sharing our journey provides valuable insights for your own optimization efforts, whether you&rsquo;re just starting or looking to refine your existing strategies.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3102f9ae3ddedc989ff766c7849bb8c2>March</h1><div class="td-byline mb-4"><time datetime=2025-03-18 class=text-body-secondary>Tuesday, March 18, 2025</time></div></div><div class=td-content><h1 id=pg-3f946694eaa084ffd57daad085a7d436>Gardener at KubeCon + CloudNativeCon Europe, London 2025</h1><div class="td-byline mb-4"><time datetime=2025-03-18 class=text-body-secondary>Tuesday, March 18, 2025</time></div><h1 id=gardener-at-kubecon--cloudnativecon-europe-london-2025>Gardener at KubeCon + CloudNativeCon Europe, London 2025<a class=td-heading-self-link href=#gardener-at-kubecon--cloudnativecon-europe-london-2025 aria-label="Heading self-link"></a></h1><p>The open-source project <a href=https://gardener.cloud/>Gardener</a> is set to showcase its cutting-edge Kubernetes-as-a-Service (KaaS) capabilities at <a href=https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/>KubeCon + CloudNativeCon Europe</a> 2025 in London.</p><p>Gardener has been pioneering hosted control planes long before they became mainstream and is now the default choice within SAP and <a href=https://gardener.cloud/adopter/>other organizations</a> for provisioning Kubernetes clusters.
Organizations looking to transform their Infrastructure-as-a-Service (IaaS) into a Kubernetes-as-a-Service (KaaS) platform can experience Gardener‚Äôs powerful automation, security, and multi-cloud extensibility firsthand at the event or <a href=https://demo.gardener.cloud/>directly in the browser</a>.</p><h2 id=revolutionizing-kubernetes-management-with-gardener>Revolutionizing Kubernetes Management with Gardener<a class=td-heading-self-link href=#revolutionizing-kubernetes-management-with-gardener aria-label="Heading self-link"></a></h2><p>Gardener provides a fully managed Kubernetes cluster solution that is:</p><ul><li><strong>Infinitely Extensible:</strong> Gardener offers limitless extensibility, supporting AWS, Azure, Alicloud, GCP, OpenStack, and other infrastructures. Run nodes with Garden Linux, SuSE, Ubuntu, or Flatcar OS while utilizing runc or gVisor for container runtime flexibility and Calico or Cilium for CNI. Explore the <a href=https://gardener.cloud/docs/extensions/>Gardener Extensions Library</a> for even more customization options.</li><li><strong>Automated Lifecycle Management:</strong> Simplified provisioning, scaling, and updates with built-in automation.</li><li><strong>Security & Compliance:</strong> Enforced policies and strict isolation to meet regulatory and enterprise security requirements, with support for automated credential rotation.</li><li><strong>Multi-Tenancy & Cost Efficiency:</strong> Designed for organizations running at scale, optimizing resources without sacrificing performance.</li></ul><p>Also, explore our other open-source projects:</p><p><strong><a href=https://github.com/gardener/etcd-druid/>etcd-druid</a></strong> ‚Äì Our in-house operator responsible for managing etcd instances in Gardener‚Äôs hosted control planes, ensuring the stability and performance of Kubernetes clusters.</p><p><strong><a href=https://github.com/openmcp-project>openMCP</a></strong> ‚Äì Our latest open-source offering that enables organizations to streamline application development using control plane methodology, making it easy to roll out, upgrade, and replicate cloud environments securely and seamlessly.</p><p>As Kubernetes adoption continues to accelerate, Gardener remains the go-to choice for managing Kubernetes at scale across multi-cloud and hybrid environments. Stop by our booth <em><strong>S561</strong></em> at KubeCon + CloudNativeCon Europe 2025 to experience Gardener firsthand and meet the team to see how Gardener empowers organizations to run secure, scalable, and efficient Kubernetes clusters with ease.</p></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://demo.gardener.cloud>Demo</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://gardener-cloud.slack.com/><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://x.com/GardenerProject><img src=/images/branding/x-logo-white.svg class=media-icon><div class=media-text>X</div></a></li></ul><span class=copyright>Copyright 2019-2025 Gardener project authors.
<a href=https://www.sap.com/about/legal/terms-of-use.html>Terms of Use
<i class="fa fa-external-link" aria-hidden=true></i>
</a>|
<a href=https://www.sap.com/about/legal/terms-of-use.html>Privacy Statement
<i class="fa fa-external-link" aria-hidden=true></i>
</a>|
<a href=https://www.sap.com/about/legal/terms-of-use.html>Legal Disclosure
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=/js/main.min.403ff095218c662472dab60ed98ecbb19431682de5ac7c6159891241cd366af5.js integrity="sha256-QD/wlSGMZiRy2rYO2Y7LsZQxaC3lrHxhWYkSQc02avU=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script><script src=/js/navbar.js></script><script src=/js/filtering.js></script><script src=/js/page-content.js></script><script src=/js/community-index.js></script></body></html>