<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.95.0"><link rel=canonical type=text/html href=https://gardener.cloud/docs/guides/monitoring_and_troubleshooting/shell-to-node/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/guides/monitoring_and_troubleshooting/shell-to-node/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Get a Shell to a Gardener Shoot Worker Node | Gardener</title><meta name=description content="Describes the methods for getting shell access to worker nodes."><meta property="og:title" content="Get a Shell to a Gardener Shoot Worker Node"><meta property="og:description" content="Describes the methods for getting shell access to worker nodes."><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/guides/monitoring_and_troubleshooting/shell-to-node/"><meta itemprop=name content="Get a Shell to a Gardener Shoot Worker Node"><meta itemprop=description content="Describes the methods for getting shell access to worker nodes."><meta name=twitter:card content="summary"><meta name=twitter:title content="Get a Shell to a Gardener Shoot Worker Node"><meta name=twitter:description content="Describes the methods for getting shell access to worker nodes."><link rel=preload href=/scss/main.min.bdb6db391c5e5922ee3c9dea909f094d46df773b84500b1c939e30ee964cfd30.css as=style><link href=/scss/main.min.bdb6db391c5e5922ee3c9dea909f094d46df773b84500b1c939e30ee964cfd30.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7N3XF5XLGV"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7N3XF5XLGV",{anonymize_ip:!1})}</script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.440ebd296d8511a615aea920695231b4.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/guides/monitoring_and_troubleshooting/shell-to-node/>Return to the regular view of this page</a>.</p></div><h1 class=title>Get a Shell to a Gardener Shoot Worker Node</h1><div class=lead>Describes the methods for getting shell access to worker nodes.</div><div class=content><h1 id=get-a-shell-to-a-kubernetes-node>Get a Shell to a Kubernetes Node</h1><p>To troubleshoot certain problems in a Kubernetes cluster, operators need access to the host of the Kubernetes node to troubleshoot
problems. This can be required if a node misbehaves or fails to join the cluster in the first place.</p><p>With access to the host, it is for instance possible to check the <code>kubelet</code> logs and interact with common tools such as <code>systemctl</code>and <code>journalctl</code>.</p><p>The first section of this guide explores options to get a shell to the node of a Gardener Kubernetes cluster.
The options described in the second section do not rely on Kubernetes capabilities to get shell access to a node and thus can also be used if an instance failed to join the cluster.</p><p>This guide only covers how to get access to the host, but does not cover troubleshooting methods.</p><ul><li><a href=#get-a-shell-to-a-kubernetes-node>Get a Shell to a Kubernetes Node</a></li><li><a href=#get-a-shell-to-an-operational-cluster-node>Get a Shell to an operational cluster node</a><ul><li><a href=#gardener-dashboard>Gardener Dashboard</a></li><li><a href=#gardenctl-shell>gardenctl shell</a></li><li><a href=#gardener-ops-toolbelt>Gardener Ops Toolbelt</a></li><li><a href=#custom-root-pod>Custom root pod</a></li></ul></li><li><a href=#ssh-access-to-a-node-that-failed-to-join-the-cluster>SSH access to a node that failed to join the cluster</a><ul><li><a href=#identifying-the-problematic-instance>Identifying the problematic instance</a></li><li><a href=#gardenctl-ssh>gardenctl ssh</a></li><li><a href=#ssh-with-manually-created-bastion-on-aws>SSH with manually created Bastion on AWS</a><ul><li><a href=#create-the-bastion-security-group>Create the Bastion Security Group</a></li><li><a href=#create-the-bastion-instance>Create the bastion instance</a></li></ul></li><li><a href=#connecting-to-the-target-instance>Connecting to the target instance</a></li><li><a href=#cleanup>Cleanup</a></li></ul></li></ul><h1 id=get-a-shell-to-an-operational-cluster-node>Get a Shell to an operational cluster node</h1><p>The following describes four different approaches to get a shell to an operational Shoot worker node.
As a prerequisite to troubleshooting a Kubernetes node, the node must have joined the cluster successfully and be able to run a pod.
All of the described approaches involve scheduling a pod with root permissions and mounting the root filesystem.</p><h2 id=gardener-dashboard>Gardener Dashboard</h2><p><strong>Prerequisite</strong>: the terminal feature is configured for the Gardener dashboard.</p><p>Navigate to the cluster overview page and find the <code>Terminal</code> in the <code>Access</code> tile.</p><img style=margin-left:0;width:80%;height:auto alt="Access Tile" src=/__resources/9fb6ca4ff9b7480f93debba833f48590_f160ec.png><br><p>Select the target Cluster (Garden, Seed / Control Plane, Shoot cluster) depending on the requirements and
access rights (only certain users have access to the Seed Control Plane).</p><p>To open the terminal configuration, click on the top right-hand corner of the screen.</p><img style=margin-left:0 alt="Terminal configuration" src=/__resources/db573582bfc544d294cbde8906a74e07_25a25d.png><br><p>Set the Terminal Runtime to &ldquo;Privileged.
Also specify the target node from the drop-down menu.</p><img style=margin-left:0;width:50%;height:auto alt="Dashboard terminal pod configuration" src=/__resources/f7b10d48edf44c17ba838ff5c429e39d_a24f67.png><br><p>The dashboard then schedules a pod and opens a shell session to the node.</p><p>To get access to common binaries installed on the host, prefix the command with <code>chroot /hostroot</code>.
Note that the path depends on where the root path is mounted in the container.
In the default image used by the Dashboard, it is under <code>/hostroot</code>.</p><img style=margin-left:0 alt="Dashboard terminal pod configuration" src=/__resources/3da659e9cc4744a2ad3e1c6a50d39c04_8b99f7.png><br><h2 id=gardenctl-shell>gardenctl shell</h2><p><strong>Prerequisite</strong>: <code>kubectl</code> and <a href=https://github.com/gardener/gardenctl>gardenctl are available and configured</a>.</p><p>First, target a Garden cluster containing all the Shoot definitions.</p><pre tabindex=0><code>$ gardenctl target garden &lt;target-garden&gt;
</code></pre><p>Target an available Shoot by name.
This sets up the context and configures the <code>kubeconfig</code> file of the Shoot cluster.
Subsequent commands will execute in this context.</p><pre tabindex=0><code>$ gardenctl target shoot &lt;target-shoot&gt;
</code></pre><p>Get the nodes of the Shoot cluster.</p><pre tabindex=0><code>$ gardenctl kubectl get nodes 
</code></pre><p>Pick a node name from the list above and get a root shell access to it.</p><pre tabindex=0><code>$ gardenctl shell &lt;target-node&gt;
</code></pre><h2 id=gardener-ops-toolbelt>Gardener Ops Toolbelt</h2><p><strong>Prerequisite</strong>: <code>kubectl</code> is available.</p><p>The <a href=https://github.com/gardener/ops-toolbelt>Gardener ops-toolbelt</a> can be used as a convenient way to deploy a root pod to a node.
The pod uses an image that is bundled with a bunch of useful <a href=https://github.com/gardener/ops-toolbelt/tree/master/dockerfile-configs>troubleshooting tools</a>.
This is also the same image that is used by default when using the Gardener Dashboard terminal feature as described in the <a href=#gardener-dashboard>previous section</a>.</p><p>The easiest way to use the <a href=https://github.com/gardener/ops-toolbelt>Gardener ops-toolbelt</a> is to execute
the <a href=https://github.com/gardener/ops-toolbelt/blob/master/hacks/ops-pod><code>ops-pod</code> script</a> in the <code>hacks</code> folder.
To get root shell access to a node, execute the aforementioned script by supplying the target node name as an argument:</p><pre tabindex=0><code>$ &lt;path-to-ops-toolbelt-repo&gt;/hacks/ops-pod &lt;target-node&gt;
</code></pre><h2 id=custom-root-pod>Custom root pod</h2><p>Alternatively, a pod can be <a href=https://kubernetes.io/docs/concepts/configuration/assign-pod-node/>assigned</a> to a target node and a shell can
be opened via <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/>standard Kubernetes means</a>.
To enable root access to the node, the pod specification requires proper <code>securityContext</code> and <code>volume</code> properties.</p><p>For instance you can use the following pod manifest, after changing <target-node-name>with the name of the node you want this pod attached to:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: privileged-pod
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  nodeSelector:
</span></span><span style=display:flex><span>    kubernetes.io/hostname: &lt;target-node-name&gt;
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - name: busybox
</span></span><span style=display:flex><span>    image: busybox
</span></span><span style=display:flex><span>    stdin: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    securityContext:
</span></span><span style=display:flex><span>      privileged: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    volumeMounts:
</span></span><span style=display:flex><span>    - name: host-root-volume
</span></span><span style=display:flex><span>      mountPath: /host
</span></span><span style=display:flex><span>      readOnly: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  volumes:
</span></span><span style=display:flex><span>  - name: host-root-volume
</span></span><span style=display:flex><span>    hostPath:
</span></span><span style=display:flex><span>      path: /
</span></span><span style=display:flex><span>  hostNetwork: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  hostPID: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  restartPolicy: Never
</span></span></code></pre></div><h1 id=ssh-access-to-a-node-that-failed-to-join-the-cluster>SSH access to a node that failed to join the cluster</h1><p>This section explores two options that can be used to get SSH access to a node that failed to join the cluster.
As it is not possible to schedule a pod on the node, the Kubernetes-based methods explored so far cannot be used in this scenario.</p><p>Additionally, Gardener typically provisions worker instances in a private subnet of the VPC, hence - there is no public IP address that could be used for direct SSH access.</p><p>For this scenario, cloud providers typically have extensive documentation (e.g <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstances.html>AWS</a> & <a href=https://cloud.google.com/compute/docs/instances/connecting-to-instance>GCP</a>
and in <a href=https://cloud.google.com/compute/docs/instances/connecting-advanced#vpn>some cases tooling support</a>).
However, these approaches are mostly cloud provider specific, require interaction via their CLI and API or sometimes
the installation of a <a href=https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-install-ssm-agent.html>cloud provider specific agent</a> one the node.</p><p>Alternatively, <code>gardenctl</code> can be used providing a cloud provider agnostic and out-of-the-box support to get ssh access to an instance in a private subnet.
Currently <code>gardenctl</code> supports AWS, GCP, Openstack, Azure and Alibaba Cloud.</p><h2 id=identifying-the-problematic-instance>Identifying the problematic instance</h2><p>First, the problematic instance has to be identified.
In Gardener, worker pools can be created in different cloud provider regions, zones and accounts.</p><p>The instance would typically show up as successfully started / running in the cloud provider dashboard or API and it is not immediately obvious which one has a problem.
Instead, we can use the Gardener API / CRDs to obtain the faulty instance identifier in a cloud-agnostic way.</p><p>Gardener uses the <a href=https://github.com/gardener/machine-controller-manager>Machine Controller Manager</a> to create the Shoot worker nodes.
For each worker node, the Machine Controller Manager creates a <code>Machine</code> CRD in the Shoot namespace in the respective <code>Seed</code> cluster.
Usually the problematic instance can be identified as the respective <code>Machine</code> CRD has status <code>pending</code>.</p><p>The instance / node name can be obtained from the <code>Machine</code> <code>.status</code> field:</p><pre tabindex=0><code>$ kubectl get machine &lt;machine-name&gt; -o json | jq -r .status.node
</code></pre><p>This is all the information needed to go ahead and use <code>gardenctl ssh</code> to get a shell to the node.
In addition, the used cloud provider, the specific identifier of the instance and the instance region can be identified from the <code>Machine</code> CRD.</p><p>Get the identifier of the instance via:</p><pre tabindex=0><code>$ kubectl get machine &lt;machine-name&gt; -o json | jq -r .spec.providerID // e.g aws:///eu-north-1/i-069733c435bdb4640
</code></pre><p>The identifier shows that the instance belongs to the cloud provider <code>aws</code> with the ec2 instance-id <code>i-069733c435bdb4640</code> in region <code>eu-north-1</code>.</p><p>To get more information about the instance, check out the <code>MachineClass</code> (e.g <code>AWSMachineClass</code>) that is associated with each <code>Machine</code> CRD in the <code>Shoot</code> namespace of the <code>Seed</code> cluster.
The <code>AWSMachineClass</code> contains the machine image (<a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html>ami</a>), machine-type, iam information, network-interfaces, subnets, security groups and attached volumes.</p><p>Of course, the information can also be used to get the instance with the cloud provider CLI / API.</p><h2 id=gardenctl-ssh>gardenctl ssh</h2><p>Using the node name of the problematic instance, we can use the <code>gardenctl ssh</code> command to get SSH access to the cloud provider
instance via an automatically set up <a href=https://en.wikipedia.org/wiki/Bastion_host>bastion host</a>.
<code>gardenctl</code> takes care of spinning up the <code>bastion</code> instance, setting up the SSH keys, ports and security groups and opens a root shell on the target instance.
After the SSH session has ended, <code>gardenctl</code> deletes the created cloud provider resources.</p><p>Use the following commands:</p><p>First, target a Garden cluster containing all the Shoot definitions.</p><pre tabindex=0><code>$ gardenctl target garden &lt;target-garden&gt;
</code></pre><p>Target an available Shoot by name.
This sets up the context, configures the <code>kubeconfig</code> file of the Shoot cluster and downloads the cloud provider credentials.
Subsequent commands will execute in this context.</p><pre tabindex=0><code>$ gardenctl target shoot &lt;target-shoot&gt;
</code></pre><p>This uses the cloud provider credentials to spin up the bastion and to open a shell on the target instance.</p><pre tabindex=0><code>$ gardenctl ssh &lt;target-node&gt;
</code></pre><h2 id=ssh-with-manually-created-bastion-on-aws>SSH with manually created Bastion on AWS</h2><p>In case you are not using <code>gardenctl</code> or want to control the bastion instance yourself, you can also manually set it up.
The steps described here are generally the same as <a href=https://github.com/gardener/gardenctl/blob/10a537942b94234914758c0f6d053dc1cf218ecd/pkg/cmd/ssh_aws.go#L53-L52>those used by <code>gardenctl</code> internally</a>.
Despite some cloud provider specifics they can be generalized to the following list:</p><ul><li>Open port 22 on the target instance.</li><li>Create an instance / VM in a public subnet (bastion instance needs to have public ip address).</li><li>Set-up security groups, roles and open port 22 for the bastion instance.</li></ul><p>The following diagram shows an overview how the SSH access to the target instance works:</p><img style=margin-left:0 alt="SSH Bastion diagram" src=/__resources/913441003e5641bc90249bdc07d55656_8710e0.png><br><p>This guide demonstrates the setup of a bastion on AWS.</p><p><strong>Prerequisites:</strong></p><ul><li>The <code>AWS CLI</code> is set up.</li><li>Obtain target instance-id (see <a href=#identifying-the-problematic-instance>here</a>).</li><li>Obtain the VPC ID the Shoot resources are created in. This can be found in the <code>Infrastructure</code> CRD in the <code>Shoot</code> namespace in the <code>Seed</code>.</li><li>Make sure that port 22 on the target instance is open (default for Gardener deployed instances).<ul><li>Extract security group via</li></ul><pre tabindex=0><code>$ aws ec2 describe-instances --instance-ids &lt;instance-id&gt;
</code></pre><ul><li>Check for rule that allows inbound connections on port 22:</li></ul><pre tabindex=0><code>$ aws ec2 describe-security-groups --group-ids=&lt;security-group-id&gt;
</code></pre><ul><li>If not available, create the rule with the following comamnd:</li></ul><pre tabindex=0><code>$ aws ec2 authorize-security-group-ingress --group-id &lt;security-group-id&gt;  --protocol tcp --port 22 --cidr 0.0.0.0/0
</code></pre></li></ul><h3 id=create-the-bastion-security-group>Create the Bastion Security Group</h3><ul><li><p>The common name of the security group is <code>&lt;shoot-name>-bsg</code>. Create the security group:</p><pre tabindex=0><code>$ aws ec2 create-security-group --group-name &lt;bastion-security-group-name&gt;  --description ssh-access --vpc-id &lt;VPC-ID&gt;
</code></pre></li><li><p>Optionally, create identifying tags for the security group:</p><pre tabindex=0><code>$ aws ec2 create-tags --resources &lt;bastion-security-group-id&gt; --tags Key=component,Value=&lt;tag&gt;
</code></pre></li><li><p>Create permission in the bastion security group that allows ssh access on port 22.</p><pre tabindex=0><code>$ aws ec2 authorize-security-group-ingress --group-id &lt;bastion-security-group-id&gt;  --protocol tcp --port 22 --cidr 0.0.0.0/0
</code></pre></li><li><p>Create an IAM role for the bastion instance with the name <code>&lt;shoot-name>-bastions</code>:</p><pre tabindex=0><code>$ aws iam create-role --role-name &lt;shoot-name&gt;-bastions
</code></pre><p>The content should be:</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>&#34;Version&#34;: <span style=color:#a31515>&#34;2012-10-17&#34;</span>,
</span></span><span style=display:flex><span>&#34;Statement&#34;: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>        &#34;Action&#34;: [
</span></span><span style=display:flex><span>            <span style=color:#a31515>&#34;ec2:DescribeRegions&#34;</span>
</span></span><span style=display:flex><span>        ],
</span></span><span style=display:flex><span>        &#34;Resource&#34;: [
</span></span><span style=display:flex><span>            <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>        ]
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><ul><li><p>Create the instance profile with name <code>&lt;shoot-name>-bastions</code>:</p><pre tabindex=0><code>$ aws iam create-instance-profile --instance-profile-name &lt;name&gt;
</code></pre></li><li><p>Add the created role to the instance profile:</p><pre tabindex=0><code>$ aws iam add-role-to-instance-profile --instance-profile-name &lt;instance-profile-name&gt; --role-name &lt;role-name&gt;
</code></pre></li></ul><h3 id=create-the-bastion-instance>Create the bastion instance</h3><p>Next, in order to be able to <code>ssh</code> into the bastion instance, the instance has to be set up with a user with a public ssh key.
Create a user <code>gardener</code> that has the same Gardener-generated public ssh key as the target instance.</p><ul><li><p>First, we need to get the public part of the <code>Shoot</code> ssh-key.
The ssh-key is stored in a secret in the the project namespace in the Garden cluster.
The name is: <code>&lt;shoot-name>-ssh-publickey</code>.
Get the key via:</p><pre tabindex=0><code>$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\&#34;id_rsa.pub\&#34;
</code></pre></li><li><p>A script handed over as <code>user-data</code> to the bastion <code>ec2</code> instance, can be used to create the <code>gardener</code> user and add the ssh-key.
For your convenience, you can use the following script to generate the <code>user-data</code>.</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#00f>#!/bin/bash -eu
</span></span></span><span style=display:flex><span><span style=color:#00f></span>saveUserDataFile () {
</span></span><span style=display:flex><span>  ssh_key=$1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cat &gt; gardener-bastion-userdata.sh <span style=color:#a31515>&lt;&lt;EOF
</span></span></span><span style=display:flex><span><span style=color:#a31515>#!/bin/bash -eu
</span></span></span><span style=display:flex><span><span style=color:#a31515>id gardener || useradd gardener -mU
</span></span></span><span style=display:flex><span><span style=color:#a31515>mkdir -p /home/gardener/.ssh
</span></span></span><span style=display:flex><span><span style=color:#a31515>echo &#34;$ssh_key&#34; &gt; /home/gardener/.ssh/authorized_keys
</span></span></span><span style=display:flex><span><span style=color:#a31515>chown gardener:gardener /home/gardener/.ssh/authorized_keys
</span></span></span><span style=display:flex><span><span style=color:#a31515>echo &#34;gardener ALL=(ALL) NOPASSWD:ALL&#34; &gt;/etc/sudoers.d/99-gardener-user
</span></span></span><span style=display:flex><span><span style=color:#a31515>EOF</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>if</span> [ -p /dev/stdin ]; <span style=color:#00f>then</span>
</span></span><span style=display:flex><span>    read -r input
</span></span><span style=display:flex><span>    cat | saveUserDataFile <span style=color:#a31515>&#34;</span>$input<span style=color:#a31515>&#34;</span>
</span></span><span style=display:flex><span><span style=color:#00f>else</span>
</span></span><span style=display:flex><span>    pbpaste | saveUserDataFile <span style=color:#a31515>&#34;</span>$input<span style=color:#a31515>&#34;</span>
</span></span><span style=display:flex><span><span style=color:#00f>fi</span>
</span></span></code></pre></div><ul><li><p>Use the script by handing-over the public ssh-key of the <code>Shoot</code> cluster:</p><pre tabindex=0><code>$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\&#34;id_rsa.pub\&#34; | ./generate-userdata.sh
</code></pre><p>This generates a file called <code>gardener-bastion-userdata.sh</code> in the same directory containing the <code>user-data</code>.</p></li><li><p>The following information is needed to create the bastion instance:</p><p><code>bastion-IAM-instance-profile-name</code></p><ul><li>Use the created instance profile with name <code>&lt;shoot-name>-bastions</code></li></ul><p><code>image-id</code></p><ul><li>Possible use the same image-id as for the target instance (or any other image). Has cloud provider specific format (AWS: <code>ami</code>).</li></ul><p><code>ssh-public-key-name</code></p><ul><li>This is the ssh key pair already created in the Shoot&rsquo;s cloud provider account by Gardener during the <code>Infrastructure</code> CRD reconciliation.</li><li>The name is usually: <code>&lt;shoot-name>-ssh-publickey</code></li></ul><p><code>subnet-id</code></p><ul><li>Choose a subnet that is attached to an <code>Internet Gateway</code> and <code>NAT Gateway</code> (bastion instance must have a public IP).</li><li>The Gardener created public subnet with the name <code>&lt;shoot-name>-public-utility-&lt;xy></code> can be used.
Please check the created subnets with the cloud provider.</li></ul><p><code>bastion-security-group-id</code></p><ul><li>Use the id of the created bastion security group.</li></ul><p><code>file-path-to-userdata</code></p><ul><li><p>Use the filepath to <code>user-data</code> file generated in the previous step.</p></li><li><p><code>bastion-instance-name</code></p><ul><li>Optional to tag the instance.</li><li>Usually <code>&lt;shoot-name>-bastions</code></li></ul></li></ul></li><li><p>Create the bastion instance via:</p></li></ul><pre tabindex=0><code>$ ec2 run-instances --iam-instance-profile Name=&lt;bastion-IAM-instance-profile-name&gt; --image-id &lt;image-id&gt;  --count 1 --instance-type t3.nano --key-name &lt;ssh-public-key-name&gt;  --security-group-ids &lt;bastion-security-group-id&gt; --subnet-id &lt;subnet-id&gt; --associate-public-ip-address --user-data &lt;file-path-to-userdata&gt; --tag-specifications ResourceType=instance,Tags=[{Key=Name,Value=&lt;bastion-instance-name&gt;},{Key=component,Value=&lt;mytag&gt;}] ResourceType=volume,Tags=[{Key=component,Value=&lt;mytag&gt;}]&#34;
</code></pre><p>Capture the <code>instance-id</code> from the reponse and wait until the <code>ec2</code> instance is running and has a public ip address.</p><h2 id=connecting-to-the-target-instance>Connecting to the target instance</h2><p>Save the private key of the ssh-key-pair in a temporary local file for later use.</p><pre tabindex=0><code>$ umask 077

$ kubectl get secret &lt;shoot-name&gt;.ssh-keypair -o json | jq -r .data.\&#34;id_rsa\&#34; | base64 -d &gt; id_rsa.key
</code></pre><p>Use the private ssh key to ssh into the bastion instance.</p><pre tabindex=0><code>$ ssh -i &lt;path-to-private-key&gt; gardener@&lt;public-bastion-instance-ip&gt; 
</code></pre><p>If that works, connect from your local terminal to the target instance via the bastion.</p><pre tabindex=0><code>$ ssh  -i &lt;path-to-private-key&gt; -o ProxyCommand=&#34;ssh -W %h:%p -i &lt;private-key&gt; -o IdentitiesOnly=yes -o StrictHostKeyChecking=no gardener@&lt;public-ip-bastion&gt;&#34; gardener@&lt;private-ip-target-instance&gt; -o IdentitiesOnly=yes -o StrictHostKeyChecking=no
</code></pre><h2 id=cleanup>Cleanup</h2><p>Do not forget to cleanup the created resources. Otherwise Gardener will eventually fail to delete the Shoot.</p></div></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2022 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid@8.13.4/dist/mermaid.min.js integrity="sha512-JERecFUBbsm75UpkVheAuDOE8NdHjQBrPACfEQYPwvPG+fjgCpHAz1Jw2ci9EXmd3DdfiWth3O3CQvcfEg8gsA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.7b24c0fb082ffb2de6cb14d6c95e9f8053053709ffcf8c761ef8e9ad2f8021e4.js integrity="sha256-eyTA+wgv+y3myxTWyV6fgFMFNwn/z4x2HvjprS+AIeQ=" crossorigin=anonymous></script></body></html>