[{"body":"Gardener     \nGardener implements the automated management and operation of Kubernetes clusters as a service and provides a fully validated extensibility framework that can be adjusted to any programmatic cloud or infrastructure provider.\nGardener is 100% Kubernetes-native and exposes its own Cluster API to create homogeneous clusters on all supported infrastructures. This API differs from SIG Cluster Lifecycle’s Cluster API that only harmonizes how to get to clusters, while Gardener’s Cluster API goes one step further and also harmonizes the make-up of the clusters themselves. That means, Gardener gives you homogeneous clusters with exactly the same bill of material, configuration and behavior on all supported infrastructures, which you can see further down below in the section on our K8s Conformance Test Coverage.\nIn 2020, SIG Cluster Lifecycle’s Cluster API made a huge step forward with v1alpha3 and the newly added support for declarative control plane management. This made it possible to integrate managed services like GKE or Gardener. We would be more than happy, if the community would be interested, to contribute a Gardener control plane provider. For more information on the relation between Gardener API and SIG Cluster Lifecycle’s Cluster API, please see here.\nGardener’s main principle is to leverage Kubernetes concepts for all of its tasks.\nIn essence, Gardener is an extension API server that comes along with a bundle of custom controllers. It introduces new API objects in an existing Kubernetes cluster (which is called garden cluster) in order to use them for the management of end-user Kubernetes clusters (which are called shoot clusters). These shoot clusters are described via declarative cluster specifications which are observed by the controllers. They will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.\nTo accomplish these tasks reliably and to offer a high quality of service, Gardener controls the main components of a Kubernetes cluster (etcd, API server, controller manager, scheduler). These so-called control plane components are hosted in Kubernetes clusters themselves (which are called seed clusters). This is the main difference compared to many other OSS cluster provisioning tools: The shoot clusters do not have dedicated master VMs. Instead, the control plane is deployed as a native Kubernetes workload into the seeds (the architecture is commonly referred to as kubeception or inception design). This does not only effectively reduce the total cost of ownership but also allows easier implementations for “day-2 operations” (like cluster updates or robustness) by relying on all the mature Kubernetes features and capabilities.\nGardener reuses the identical Kubernetes design to span a scalable multi-cloud and multi-cluster landscape. Such familiarity with known concepts has proven to quickly ease the initial learning curve and accelerate developer productivity:\n Kubernetes API Server = Gardener API Server Kubernetes Controller Manager = Gardener Controller Manager Kubernetes Scheduler = Gardener Scheduler Kubelet = Gardenlet Node = Seed cluster Pod = Shoot cluster  Please find more information regarding the concepts and a detailed description of the architecture in our Gardener Wiki and our blog posts on kubernetes.io: Gardener - the Kubernetes Botanist (17.5.2018) and Gardener Project Update (2.12.2019).\n K8s Conformance Test Coverage  Gardener takes part in the Certified Kubernetes Conformance Program to attest its compatibility with the K8s conformance testsuite. Currently Gardener is certified for K8s versions up to v1.24, see the conformance spreadsheet.\nContinuous conformance test results of the latest stable Gardener release are uploaded regularly to the CNCF test grid:\n   Provider/K8s v1.25 v1.24 v1.23 v1.22 v1.21 v1.20 v1.19 v1.18 v1.17     AWS N/A           Azure N/A           GCP N/A           OpenStack N/A           Alicloud N/A           Equinix Metal N/A N/A N/A N/A N/A N/A N/A N/A N/A   vSphere N/A N/A N/A N/A N/A N/A N/A N/A N/A    Get an overview of the test results at testgrid.\nStart using or developing the Gardener locally See our documentation in the /docs repository, please find the index here.\nSetting up your own Gardener landscape in the Cloud The quickest way to test drive Gardener is to install it virtually onto an existing Kubernetes cluster, just like you would install any other Kubernetes-ready application. You can do this with our Gardener Helm Chart.\nAlternatively you can use our garden setup project to create a fully configured Gardener landscape which also includes our Gardener Dashboard.\nFeedback and Support Feedback and contributions are always welcome!\nAll channels for getting in touch or learning about our project are listed under the community section. We are cordially inviting interested parties to join our bi-weekly meetings.\nPlease report bugs or suggestions about our Kubernetes clusters as such or the Gardener itself as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn More! Please find further resources about our project here:\n Our landing page gardener.cloud “Gardener Project Update” blog on kubernetes.io. “Gardener, the Kubernetes Botanist” blog on kubernetes.io SAP news article about “Project Gardener” Introduction movie: “Gardener - Planting the Seeds of Success in the Cloud” “Thinking Cloud Native” talk at EclipseCon 2018 Blog - “Showcase of Gardener at OSCON 2018”  ","categories":"","description":"The core component providing the extension API server of your Kubernetes cluster","excerpt":"The core component providing the extension API server of your …","ref":"/docs/gardener/","tags":"","title":"Gardener"},{"body":"Gardener API Reference  authentication.gardener.cloud API Group core.gardener.cloud API Group extensions.gardener.cloud API Group operations.gardener.cloud API Group resources.gardener.cloud API Group seedmanagement.gardener.cloud API Group settings.gardener.cloud API Group  ","categories":["Developers"],"description":"","excerpt":"Gardener API Reference  authentication.gardener.cloud API Group …","ref":"/docs/gardener/api-reference/","tags":"","title":"API Reference"},{"body":"Dashboard Architecture Overview Overview The dashboard frontend is a Single Page Application (SPA) built with Vue.js. The dashboard backend is web server build with Express and Node.js. The backend serves the bundled frontend as static content. The dashboard uses Socket.IO to enable real-time, bidirectional and event-based communication between the frontend and the backend. For the communication from the backend to different kube-apiservers the http/2 network protocol is used. Authentication at the apiserver of the garden cluster is done via JWT tokens. These can either be an ID Token issued by an OpenID Connect Provider or the token of a Kubernetes Service Account.\nFrontend The dashboard frontend consists of many Vue.js single file components that manage their state via a centralized store. The store defines mutations to modify the state synchronously. If several mutations have to be combined or the state in the backend has to be modified at the same time, the store provides asynchronous actions to do this job. The synchronization of the data with the backend is done by plugins that also use actions.\nBackend The backend is currently a monolithic Node.js application, but it performs several tasks that are actually independent.\n Static web server for the frontend single page application Forward real time events of the apiserver to the frontend Provide an HTTP Api Bootstrapping shoot and seed clusters to support web terminals Initiate and manage the end user login flow in order to obtain an ID Token Bidirectional integration with the github issue management  It is planed to split the backend into several independent containers to increase stability and performance.\nAuthentication The following diagram shows the authorization code flow in the gardener dashboard. When the user clicks the login button he is redirected to the authorization endpoint of the openid connect provider. In the case of Dex IDP, authentication is delegated to the connected IDP. After successful login, the OIDC provider redirects back to the dashboard backend with a one time authorization code. With this code the dashboard backend can now request an ID token for the logged in user. The ID token is encrypted and stored as a secure httpOnly session cookie.\n","categories":"","description":"","excerpt":"Dashboard Architecture Overview Overview The dashboard frontend is a …","ref":"/docs/dashboard/architecture/","tags":"","title":"Architecture"},{"body":"","categories":["Developers","Operators"],"description":"","excerpt":"","ref":"/docs/other-components/machine-controller-manager/docs/deployment/","tags":"","title":"Deployment"},{"body":"","categories":"","description":"Gardener extension controllers for the different infrastructures","excerpt":"Gardener extension controllers for the different infrastructures","ref":"/docs/extensions/infrastructure-extensions/","tags":"","title":"Infrastructure Extensions"},{"body":"","categories":"","description":"The infrastructure, networking, OS and other extension components for Gardener","excerpt":"The infrastructure, networking, OS and other extension components for …","ref":"/docs/extensions/","tags":"","title":"Gardener Extensions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/dashboard/concepts/","tags":"","title":"Concepts"},{"body":"","categories":["Operators","Developers"],"description":"","excerpt":"","ref":"/docs/gardener/concepts/","tags":"","title":"Concepts"},{"body":"","categories":["Developers"],"description":"","excerpt":"","ref":"/docs/other-components/machine-controller-manager/docs/development/","tags":"","title":"Development"},{"body":"","categories":"","description":"Gardener extension controllers for the supported operating systems","excerpt":"Gardener extension controllers for the supported operating systems","ref":"/docs/extensions/os-extensions/","tags":"","title":"Operating System Extensions"},{"body":"Gardener Dashboard  \nDemo Documentation Gardener Dashboard Documentation\nPeople The following SAP developers contributed to this project until this initial contribution was published as open source.\n   contributor commits (%) +lines -lines first commit last commit     Holger Koser 313 (42%) 57878 18562 2017-07-13 2018-01-23   Andreas Herz 307 (41%) 13666 11099 2017-07-14 2017-10-27   Peter Sutter 99 (13%) 4838 3967 2017-11-07 2018-01-23   Gross, Lukas 31 (4%) 400 267 2018-01-10 2018-01-23    It is derived from the historical, internal gardener-ui repository at commit eeb623d60c86e6037c0e1dc2bdd9e54663bf41a8.\nLicense Apache License 2.0\nCopyright 2020 The Gardener Authors\n","categories":"","description":"The web UI for managing your projects and clusters","excerpt":"The web UI for managing your projects and clusters","ref":"/docs/dashboard/","tags":"","title":"Dashboard"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/dashboard/deployment/","tags":"","title":"Deployment"},{"body":"","categories":["Operators","Developers"],"description":"","excerpt":"","ref":"/docs/gardener/deployment/","tags":"","title":"Deployment"},{"body":"","categories":["Developers"],"description":"","excerpt":"","ref":"/docs/other-components/machine-controller-manager/docs/documents/","tags":"","title":"Documents"},{"body":"","categories":"","description":"Gardener extension controllers for the supported container network interfaces","excerpt":"Gardener extension controllers for the supported container network …","ref":"/docs/extensions/network-extensions/","tags":"","title":"Network Extensions"},{"body":"","categories":"","description":"Gardener extensions for the supported container runtime interfaces","excerpt":"Gardener extensions for the supported container runtime interfaces","ref":"/docs/extensions/container-runtime-extensions/","tags":"","title":"Container Runtime Extensions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/dashboard/development/","tags":"","title":"Development"},{"body":"","categories":["Developers"],"description":"","excerpt":"","ref":"/docs/gardener/development/","tags":"","title":"Development"},{"body":"gardenctl-v2   \nWhat is gardenctl? gardenctl is a command-line client for the Gardener. It facilitates the administration of one or many garden, seed and shoot clusters. Use this tool to configure access to clusters and configure cloud provider CLI tools. It also provides support for accessing cluster nodes via ssh.\nInstallation Install the latest release from Homebrew, Chocolatey or GitHub Releases.\nInstall using Package Managers # Homebrew (macOS and Linux) brew install gardener/tap/gardenctl-v2  # Chocolatey (Windows) choco install gardenctl-v2 Attention brew users: gardenctl-v2 uses the same binary name as the legacy gardenctl (gardener/gardenctl) CLI. If you have an existing installation you should remove it with brew uninstall gardenctl before attempting to install gardenctl-v2. Alternatively, you can choose to link the binary using a different name. If you try to install without removing or relinking the old installation, brew will run into an error and provide instructions how to resolve it.\nInstall from Github Release If you install via GitHub releases, you need to put the gardenctl binary on your path. The other install methods do this for you.\n# Example for macOS  # set operating system and architecture os=darwin # choose between darwin, linux, windows arch=amd64 # choose between amd64, arm64  # Get latest version. Alternatively set your desired version version=$(curl -s https://raw.githubusercontent.com/gardener/gardenctl-v2/master/LATEST)  # Download gardenctl curl -LO \"https://github.com/gardener/gardenctl-v2/releases/download/${version}/gardenctl_v2_${os}_${arch}\"  # Make the gardenctl binary executable chmod +x \"./gardenctl_v2_${os}_${arch}\"  # Move the binary in to your PATH sudo mv \"./gardenctl_v2_${os}_${arch}\" /usr/local/bin/gardenctl Configuration gardenctl requires a configuration file. The default location is in ~/.garden/gardenctl-v2.yaml.\nYou can modify this file directly using the gardenctl config command. It allows adding, modifying and deleting gardens.\nExample config command:\n# Adapt the path to your kubeconfig file for the garden cluster export KUBECONFIG=~/relative/path/to/kubeconfig.yaml  # Fetch cluster-identity of garden cluster CLUSTER_IDENTITY=$(kubectl -n kube-system get configmap cluster-identity -ojsonpath={.data.cluster-identity})  # Configure garden cluster gardenctl config set-garden $CLUSTER_IDENTITY --kubeconfig $KUBECONFIG This command will create or update a garden with the provided identity and kubeconfig path of your garden cluster.\nExample Config gardens: - identity: landscape-dev # Unique identity of the garden cluster. See cluster-identity ConfigMap in kube-system namespace of the garden cluster  kubeconfig: ~/relative/path/to/kubeconfig.yaml # context: different-context # Overrides the current-context of the garden cluster kubeconfig  # patterns: ~ # List of regex patterns for pattern targeting Note: You need to have gardenlogin installed as kubectl plugin in order to use the kubeconfigs for Shoot clusters provided by gardenctl.\nConfig Path Overwrite  The gardenctl config path can be overwritten with the environment variable GCTL_HOME. The gardenctl config name can be overwritten with the environment variable GCTL_CONFIG_NAME.  export GCTL_HOME=/alternate/garden/config/dir export GCTL_CONFIG_NAME=myconfig # without extension! # config is expected to be under /alternate/garden/config/dir/myconfig.yaml Shell Session The state of gardenctl is bound to a shell session and is not shared across windows, tabs or panes. A shell session is defined by the environment variable GCTL_SESSION_ID. If this is not defined, the value of the TERM_SESSION_ID environment variable is used instead. If both are not defined, this leads to an error and gardenctl cannot be executed. The target.yaml and temporary kubeconfig.*.yaml files are store in the following directory ${TMPDIR}/garden/${GCTL_SESSION_ID}.\nYou can make sure that GCTL_SESSION_ID or TERM_SESSION_ID is always present by adding the following code to your terminal profile ~/.profile, ~/.bashrc or comparable file.\nbash and zsh: [ -n \"$GCTL_SESSION_ID\" ] || [ -n \"$TERM_SESSION_ID\" ] || export GCTL_SESSION_ID=$(uuidgen) fish: [ -n \"$GCTL_SESSION_ID\" ] || [ -n \"$TERM_SESSION_ID\" ] || set -gx GCTL_SESSION_ID (uuidgen) powershell: if ( !(Test-Path Env:GCTL_SESSION_ID) -and !(Test-Path Env:TERM_SESSION_ID) ) { $Env:GCTL_SESSION_ID = [guid]::NewGuid().ToString() } Completion Gardenctl supports completion that will help you working with the CLI and save you typing effort. It will also help you find clusters by providing suggestions for gardener resources such as shoots or projects. Completion is supported for bash, zsh, fish and powershell. You will find more information on how to configure your shell completion for gardenctl by executing the help for your shell completion command. Example:\ngardenctl completion bash --help Usage Targeting You can set a target to use it in subsequent commands. You can also overwrite the target for each command individually.\nNote that this will not affect your KUBECONFIG env variable. To update the KUBECONFIG env for your current target see Configure KUBECONFIG section\nExample:\n# target control plane gardenctl target --garden landscape-dev --project my-project --shoot my-shoot --control-plane Find more information in the documentation.\nConfigure KUBECONFIG for Shoot Clusters Generate a script that points KUBECONFIG to the targeted cluster for the specified shell. Use together with eval to configure your shell. Example for bash:\neval $(gardenctl kubectl-env bash) Configure Cloud Provider CLIs Generate the cloud provider CLI configuration script for the specified shell. Use together with eval to configure your shell. Example for bash:\neval $(gardenctl provider-env bash) SSH Establish an SSH connection to a Shoot cluster’s node.\ngardenctl ssh my-node ","categories":"","description":"The command line interface to control your clusters","excerpt":"The command line interface to control your clusters","ref":"/docs/gardenctl-v2/","tags":"","title":"Gardenctl V2"},{"body":"","categories":["Developers"],"description":"","excerpt":"","ref":"/docs/other-components/machine-controller-manager/docs/proposals/","tags":"","title":"Proposals"},{"body":"","categories":"","description":"Other Gardener extensions","excerpt":"Other Gardener extensions","ref":"/docs/extensions/others/","tags":"","title":"Others"},{"body":"","categories":["Developers"],"description":"","excerpt":"","ref":"/docs/gardener/extensions/","tags":"","title":"Extensions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/guides/","tags":"","title":"Guides"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/dashboard/usage/","tags":"","title":"Usage"},{"body":"","categories":["Users","Operators"],"description":"","excerpt":"","ref":"/docs/other-components/machine-controller-manager/docs/usage/","tags":"","title":"Usage"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/tutorials/","tags":"","title":"Tutorials"},{"body":"Monitoring Roles of the different Prometheus instances Prometheus Deployed in the garden namespace. Important scrape targets:\n cadvisor node-exporter kube-state-metrics  Purpose: Acts as a cache for other Prometheus instances. The metrics are kept for a short amount of time (~2 hours) due to the high cardinality. For example if another Prometheus needs access to cadvisor metrics it will query this Prometheus instead of the cadvisor. This also reduces load on the kubelets and API Server.\nSome of the high cardinality metrics are aggregated with recording rules. These pre-aggregated metrics are scraped by the Aggregate Prometheus.\nThis Prometheus is not used for alerting.\nAggregate Prometheus Deployed in the garden namespace. Important scrape targets:\n other prometheus instances logging components  Purpose: Store pre-aggregated data from prometheus and shoot prometheus. An ingress exposes this Prometheus allowing it to be scraped from another cluster.\nSeed Prometheus Deployed in the garden namespace. Important scrape targets:\n pods in extension namespaces annotated with:  prometheus.io/scrape=true prometheus.io/port=\u003cport\u003e  cadvisor metrics from pods in the garden and extension namespaces  Purpose: Entrypoint for operators when debugging issues with extensions or other garden components.\nShoot Prometheus Deployed in the shoot control plane namespace. Important scrape targets:\n control plane components shoot nodes (node-exporter) blackbox-exporter used to measure connectivity  Purpose: Monitor all relevant components belonging to a shoot cluster managed by Gardener. Shoot owners can view the metrics in Grafana dashboards and receive alerts based on these metrics. Gardener operators will receive a different set of alerts. For alerting internals refer to this document.\nCollect all Shoot Prometheus with remote write An optional collection of all Shoot Prometheus metrics to a central prometheus (or cortex) instance is possible with the monitoring.shoot setting in GardenletConfiguration:\nmonitoring: shoot: remoteWrite: url: https://remoteWriteUrl # remote write URL keep:# metrics that should be forwarded to the external write endpoint. If empty all metrics get forwarded - kube_pod_container_info queueConfig: | # queue_config of prometheus remote write as multiline string max_shards: 100 batch_send_deadline: 20s min_backoff: 500ms max_backoff: 60s externalLabels: # add additional labels to metrics to identify it on the central instance additional: label If basic auth is needed it can be set via secret in garden namespace (Gardener API Server). Example secret\nDisable Gardener Monitoring If you wish to disable metric collection for every shoot and roll your own then you can simply set.\nmonitoring: shoot: enabled: false ","categories":["Operators"],"description":"","excerpt":"Monitoring Roles of the different Prometheus instances Prometheus …","ref":"/docs/gardener/monitoring/","tags":"","title":"Monitoring"},{"body":"","categories":"","description":"Commonly asked questions about Gardener","excerpt":"Commonly asked questions about Gardener","ref":"/docs/faq/","tags":"","title":"FAQ"},{"body":"Gardener Enhancement Proposal (GEP) Changes to the Gardener code base are often incorporated directly via pull requests which either themselves contain a description about the motivation and scope of a change or a linked GitHub issue does.\nIf a perspective feature has a bigger extent, requires the involvement of several parties or more discussion is needed before the actual implementation can be started, you may consider filing a pull request with a Gardener Enhancement Proposal (GEP) first.\nGEPs are a measure to propose a change or to add a feature to Gardener, help you to describe the change(s) conceptionally, and to list the steps that are necessary to reach this goal. It helps the Gardener maintainers as well as the community to understand the motivation and scope around your proposed change(s) and encourages their contribution to discussions and future pull requests. If you are familiar with the Kubernetes community, GEPs are analogue to Kubernetes Enhancement Proposals (KEPs).\nReasons for a GEP You may consider filing a GEP for the following reasons:\n A Gardener architectural change is intended / necessary Major changes to the Gardener code base A phased implementation approach is expected because of the widespread scope of the change Your proposed changes may be controversial  We encourage you to take a look at already merged GEPs since they give you a sense of what a typical GEP comprises.\nBefore creating a GEP Before starting your work and creating a GEP, please take some time to familiarize yourself with our general Gardener Contribution Guidelines.\nIt is recommended to discuss and outline the motivation of your prospective GEP as a draft with the community before you take the investment of creating the actual GEP. This early briefing supports the understanding for the broad community and leads to a fast feedback for your proposal from the respective experts in the community. An appropriate format for this may be the regular Gardener community meetings.\nHow to file a GEP GEPs should be created as Markdown .md files and are submitted through a GitHub pull request to their current home in docs/proposals. Please use the provided template or follow the structure of existing GEPs which makes reviewing easier and faster. Additionally, please link the new GEP in our documentation index.\nIf not already done, please present your GEP in the regular community meetings to brief the community about your proposal (we strive for personal communication :) ). Also consider that this may be an important step to raise awareness and understanding for everyone involved.\nThe GEP template contains a small set of metadata, which is helpful for keeping track of the enhancement in general and especially of who is responsible for implementing and reviewing PRs that are part of the enhancement.\nMain Reviewers Apart from general metadata, the GEP should name at least one “main reviewer”. You can find a main reviewer for your GEP either when discussing the proposal in the community meeting, by asking in our Slack Channel or at latest during the GEP PR review. New GEPs should only be accepted once at least one main reviewer is nominated/assigned.\nThe main reviewers are charged with the following tasks:\n familiarizing themselves with the details of the proposal reviewing the GEP PR itself and any further updates to the document discussing design details and clarifying implementation questions with the author before and after the proposal was accepted reviewing PRs related to the GEP in-depth  Other community members are of course also welcome to help the GEP author, review his work and raise general concerns with the enhancement. Nevertheless, the main reviewers are supposed to focus on more in-depth reviews and accompaning the whole GEP process end-to-end, which helps with getting more high-quality reviews and faster feedback cycles instead of having more people looking at the process with lower priority and less focus.\nGEP Process  Pre-discussions about GEP (if necessary) Find a main reviewer for your enhancement GEP is filed through GitHub PR Presentation in Gardener community meeting (if possible) Review of GEP from maintainers/community GEP is merged if accepted Implementation of GEP Consider keeping GEP up-to-date in case implementation differs essentially  ","categories":["Developers"],"description":"","excerpt":"Gardener Enhancement Proposal (GEP) Changes to the Gardener code base …","ref":"/docs/gardener/proposals/","tags":"","title":"Proposals"},{"body":"","categories":["Operators","Users"],"description":"","excerpt":"","ref":"/docs/gardener/usage/","tags":"","title":"Usage"},{"body":"","categories":"","description":"Other components included in the Gardener project","excerpt":"Other components included in the Gardener project","ref":"/docs/other-components/","tags":"","title":"Other Components"},{"body":"A curated list of awesome Kubernetes sources. Inspired by @sindresorhus’ awesome\nSetup  Install Docker for Mac Install Docker for Windows Run a Kubernetes Cluster on your local machine  A Place That Marks the Beginning of a Journey  Kubernetes Community Overview and Contributions Guide by Ihor Dvoretskyi An Intro to Google’s Kubernetes and How to Use It by Laura Frank Getting Started on Kubernetes by Rajdeep Dua Kubernetes: The Future of Cloud Hosting by Meteorhacks Kubernetes by Google by Gaston Pantana Application Containers: Kubernetes and Docker from Scratch by Keith Tenzer Learn the Kubernetes Key Concepts in 10 Minutes by Omer Dawelbeit The Children’s Illustrated Guide to Kubernetes by Deis :-) The ‘kubectl run’ command by Michael Hausenblas Docker Kubernetes Lab Handbook by Peng Xiao  Interactive Learning Environments Learn Kubernetes using an interactive environment without requiring downloads or configuration\n Interactive Tutorial Play with Kubernetes Kubernetes Bootcamp  Massive Open Online Courses / Tutorials List of available free online courses(MOOC) and tutorials\n DevOps with Kubernetes Introduction to Kubernetes  Courses  Scalable Microservices with Kubernetes at Udacity Introduction to Kubernetes at edX  Tutorials  Kubernetes Tutorials by Kubernetes Team Kubernetes By Example by OpenShift Team Kubernetes Tutorial by Tutorialspoint  Package Managers  Helm KPM  RPC  gRPC  Secret Generation and Management  Vault auth plugin backend: Kubernetes Vault controller kube-lego k8sec kubernetes-vault kubesec - Secure Secret management  Machine Learning  TensorFlow k8s mxnet-operator - Tools for ML/MXNet on Kubernetes. kubeflow - Machine Learning Toolkit for Kubernetes. seldon-core - Open source framework for deploying machine learning models on Kubernetes  Raspberry Pi Some of the awesome findings and experiments on using Kubernetes with Raspberry Pi.\n Kubecloud Setting up a Kubernetes on ARM cluster Setup Kubernetes on a Raspberry Pi Cluster easily the official way! by Mathias Renner and Lucas Käldström How to Build a Kubernetes Cluster with ARM Raspberry Pi then run .NET Core on OpenFaas by Scott Hanselman  Contributing Contributions are most welcome!\nThis list is just getting started, please contribute to make it super awesome.\n","categories":"","description":"Interesting and useful content on Kubernetes","excerpt":"Interesting and useful content on Kubernetes","ref":"/curated-links/","tags":"","title":"Curated Links"},{"body":"Contributing to Gardener Welcome Welcome to the Contributor section of Gardener. Here you can learn how it is possible for you to contribute your ideas and expertise to the project and have it grow even more.\nPrerequisites Before you begin contributing to Gardener, there are a couple of things you should become familiar with and complete first.\nCode of Conduct All members of the Gardener community must abide by the CNCF Code of Conduct. Only by respecting each other can we develop a productive, collaborative community. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting gardener.opensource@sap.com and/or a Gardener project maintainer.\nDeveloper Certificate of Origin Due to legal reasons, contributors will be asked to accept a Developer Certificate of Origin (DCO) before they submit the first pull request to this projects, this happens in an automated fashion during the submission process. We use the standard DCO text of the Linux Foundation.\nLicense Your contributions to Gardener must be licensed properly:\n Code contributions must be licensed under the Apache 2.0 License Documentation contributions must be licensed under the Creative Commons Attribution 4.0 International License  Contributing Gardener uses GitHub to manage reviews of pull requests.\n  If you are a new contributor see: Steps to Contribute\n  If you have a trivial fix or improvement, go ahead and create a pull request.\n  If you plan to do something more involved, first discuss your ideas on our mailing list. This will avoid unnecessary work and surely give you and us a good deal of inspiration.\n  Relevant coding style guidelines are the Go Code Review Comments and the Formatting and style section of Peter Bourgon’s Go: Best Practices for Production Environments.\n  Steps to Contribute Should you wish to work on an issue, please claim it first by commenting on the GitHub issue that you want to work on it. This is to prevent duplicated efforts from contributors on the same issue.\nIf you have questions about one of the issues, with or without the tag, please comment on them and one of the maintainers will clarify it.\nWe kindly ask you to follow the Pull Request Checklist to ensure reviews can happen accordingly.\nPull Request Checklist   Branch from the master branch and, if needed, rebase to the current master branch before submitting your pull request. If it doesn’t merge cleanly with master you may be asked to rebase your changes.\n  Commits should be as small as possible, while ensuring that each commit is correct independently (i.e., each commit should compile and pass tests).\n  Test your changes as thoroughly as possible before your commit them. Preferably, automate your testing with unit / integration tests. If tested manually, provide information about the test scope in the PR description (e.g. “Test passed: Upgrade K8s version from 1.14.5 to 1.15.2 on AWS, Azure, GCP, Alicloud, Openstack.”).\n  Create Work In Progress [WIP] pull requests only if you need a clarification or an explicit review before you can continue your work item.\n  If your patch is not getting reviewed or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment, or you can ask for a review on our mailing list.\n  If you add new features, make sure that they are documented in the Gardener documentation.\n  If your changes are relevant for operators, consider to update the ops toolbelt image.\n  Post review:\n If a review requires you to change your commit(s), please test the changes again. Amend the affected commit(s) and force push onto your branch. Set respective comments in your GitHub review to resolved. Create a general PR comment to notify the reviewers that your amendments are ready for another round of review.    Contributing Bigger Changes If you want to contribute bigger changes to Gardener, such as when introducing new API resources and their corresponding controllers, or implementing an approved Gardener Enhancement Proposal, follow the guidelines outlined here.\nIssues and Planning We use GitHub issues to track bugs and enhancement requests. Please provide as much context as possible when you open an issue. The information you provide must be comprehensive enough to reproduce that issue for the assignee. Therefore, contributors may use but aren’t restricted to the issue template provided by the Gardener maintainers.\nZenHub is used for planning:\n Install the ZenHub Chrome plugin Login to ZenHub Open the Gardener ZenHub workspace  Security Release Process See Security Release Process.\nCommunity Slack Channel #gardener, sign up here.\nMailing List gardener@googlegroups.com\nThe mailing list is hosted through Google Groups. To receive the lists’ emails, join the group, as you would any other Google Group.\nOther For additional channels where you can reach us, as well as links to our bi-weekly meetings, visit the Community page.\n","categories":"","description":"The project's contributor guide for code and documentation","excerpt":"The project's contributor guide for code and documentation","ref":"/docs/contribute/","tags":"","title":"Contribute"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/guides/client_tools/","tags":"","title":"Set Up Client Tools"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/guides/install_gardener/","tags":"","title":"Install Gardener"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/guides/administer_shoots/","tags":"","title":"Administer Client (Shoot) Clusters"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/guides/monitoring_and_troubleshooting/","tags":"","title":"Monitor and Troubleshoot"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/guides/applications/","tags":"","title":"Applications"},{"body":"Community If you would like to join the growing Gardener community, here are the places where you can meet like-minded contributors.\nSlack Channel #gardener, sign up here.\nMailing List gardener@googlegroups.com\nThe mailing list is hosted through Google Groups. To receive the lists’ emails, join the group, as you would any other Google Group.\nTwitter Follow @GardenerProject on Twitter. Please mention @GardenerProject in your own posts about Gardener.\nAccessing community documents In order to foster real time collaboration there are working documents and notes that are taken in Google Docs, and then transferred to this repository if appropriate.\nTo gain edit access for these documents, you must subscribe to the gardener mailing list, as these documents are shared automatically with anyone who subscribes to that list.\nBi-Weekly Meetings We have a PUBLIC and RECORDED bi-weekly meeting. We meet every other Friday at 10:00 CET over Zoom. Find recordings in the Gardener Youtube channel. Let us know if you want to participate and live in a timezone where 10:00 CET is in the night, we can also schedule meetings on Thursday 17:00 CET.\nSee the meeting calendar on the web at calendar.google.com, or paste this iCal url into any iCal client.\nIf you have a topic you’d like to present or would like to see discussed, please propose a specific date on the Gardener Community Meeting Agenda. Find minutes in the same document. Please upload slides or other documents you presented to the Gardener Community Meeting folder. Subscribe to the gardener mailing list to get edit permissions.\n","categories":"","description":"","excerpt":"Community If you would like to join the growing Gardener community, …","ref":"/docs/contribute/30_community/","tags":"","title":"Community"},{"body":"Contributing Bigger Changes Here are the guidelines you should follow when contributing larger changes to Gardener:\n  Avoid proposing a big change in one single PR. Instead, split your work into multiple stages which are independently mergeable and create one PR for each stage. For example, if introducing a new API resource and its controller, these stages could be:\n API resource types, including defaults and generated code. API resource validation. API server storage. Admission plugin(s), if any. Controller(s), including changes to existing controllers. Split this phase further into different functional subsets if appropriate.    If you realize later that changes to artifacts introduced in a previous stage are required, by all means make them and explain in the PR why they were needed.\n  Consider splitting a big PR further into multiple commits to allow for more focused reviews. For example, you could add unit tests / documentation in separate commits from the rest of the code. If you have to adapt your PR to review feedback, prefer doing that also in a separate commit to make it easier for reviewers to check how their feedback has been addressed.\n  To make the review process more efficient and avoid too many long discussions in the PR itself, ask for a “main reviewer” to be assigned to your change, then work with this person to make sure he or she understands it in detail, and agree together on any improvements that may be needed. If you can’t reach an agreement on certain topics, comment on the PR and invite other people to join the discussion.\n  Even if you have a “main reviewer” assigned, you may still get feedback from other reviewers. In general, these “non-main reviewers” are advised to focus more on the design and overall approach rather than the implementation details. Make sure that you address any concerns on this level appropriately.\n  ","categories":"","description":"","excerpt":"Contributing Bigger Changes Here are the guidelines you should follow …","ref":"/docs/contribute/10_code/10_contributing_bigger_changes/","tags":"","title":"Contributing Bigger Changes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/2022/","tags":"","title":"2022"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/","tags":"","title":"Blogs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/","tags":"","title":"Gardener"},{"body":"Presenters This community call will be led by Pawel Palucki.\nTopicss This meeting will explore the uses of CRI-resource-manager, a Container Runtime Interface proxy service for Kubernetes that boasts workload placement policies with hardware resource awareness.\nViewers will understand what problems CRI-resource-manager solves and what quality of life features are a must-have for an extension to become “ready to production”. Later on in the meeting, a demonstration will be given on how to dynamically configure Gardener extension and debug propagation when it fails.\nQ\u0026A is planned after the main part to look at any issues you might have encountered. Come join us!\nMeeting Link You can join this meeting on Zoom (Meeting ID: 982 7843 1856, Passcode: 495335) on October 20, 2022 10:00-11:00 AM CEST.\n","categories":"","description":"","excerpt":"Presenters This community call will be led by Pawel Palucki.\nTopicss …","ref":"/blog/2022/10.12-upcoming-gardener-community-meeting-october-2/","tags":"","title":"Upcoming Community Call - Get more computing power in Gardener by overcoming Kubelet limitations with CRI-resource-manager"},{"body":"Presenters This community call was led by Raymond de Jong.\nTopics This meeting explores the uses of Cilium, an open source software used to secure the network connectivity between application services deployed using Kubernetes, and Hubble, the networking and security observability platform built on top of it.\nRaymond de Jong begins the meeting by giving an introduction of Cillium and eBPF and how they are both used in Kubernetes networking and services. He then goes over the ways of running Cillium - either by using a supported cloud provider or by CNI chaining.\nThe next topic introduced is the Cluster Mesh and the different use cases for it, offering high availability, shared services, local and remote service affinity, and the ability to split services.\nIn regards to security, being an identity-based security solution utilizing API-aware authorization, Cillium implements Hubble in order to increase its observability. Hubble combines hubble UI, hubble API and hubble Metrics - Grafana and Prometheus, in order to provide service dependency maps, detailed flow visibility and built-in metrics for operations and applications stability.\nThe final topic covered is the Service Mesh, offering service maps and the ability to integrate Cluster Mesh features.\nIf you are left with any questions regarding the content, you might find the answers at the Q\u0026A session and discussion held at the end, as well as the questions asked and answered throughout the meeting.\nRecording   ","categories":"","description":"","excerpt":"Presenters This community call was led by Raymond de Jong.\nTopics This …","ref":"/blog/2022/10.06-gardener-community-meeting-october/","tags":"","title":"Community Call - Cilium / Isovalent Presentation"},{"body":"Gardener Extension for certificate services  \nProject Gardener implements the automated management and operation of Kubernetes clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.\nRecently, most of the vendor specific logic has been developed in-tree. However, the project has grown to a size where it is very hard to extend, maintain, and test. With GEP-1 we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.\nConfiguration Example configuration for this extension controller:\napiVersion: shoot-cert-service.extensions.config.gardener.cloud/v1alpha1 kind: Configuration issuerName: gardener restrictIssuer: true # restrict issuer to any sub-domain of shoot.spec.dns.domain (default) acme:  email: john.doe@example.com  server: https://acme-v02.api.letsencrypt.org/directory # privateKey: | # Optional key for Let's Encrypt account. # -----BEGIN BEGIN RSA PRIVATE KEY----- # ... # -----END RSA PRIVATE KEY----- Extension-Resources Example extension resource:\napiVersion: extensions.gardener.cloud/v1alpha1 kind: Extension metadata:  name: \"extension-certificate-service\"  namespace: shoot--project--abc spec:  type: shoot-cert-service When an extension resource is reconciled, the extension controller will create an instance of Cert-Management as well as an Issuer with the ACME information provided in the configuration above. These resources are placed inside the shoot namespace on the seed. Also, the controller takes care about generating necessary RBAC resources for the seed as well as for the shoot.\nPlease note, this extension controller relies on the Gardener-Resource-Manager to deploy k8s resources to seed and shoot clusters, i.e. it never deploys them directly.\nHow to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the ./dev/kubeconfig file. Static code checks and tests can be executed by running make verify. We are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io Gardener Extensions Golang library GEP-1 (Gardener Enhancement Proposal) on extensibility Extensibility API documentation  ","categories":"","description":"Gardener extension controller for certificate services for shoot clusters","excerpt":"Gardener extension controller for certificate services for shoot …","ref":"/docs/extensions/others/gardener-extension-shoot-cert-service/","tags":"","title":"Certificate services"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/","tags":"","title":"Docs"},{"body":"Manage certificates with Gardener for default domain Introduction Dealing with applications on Kubernetes which offer a secure service endpoints (e.g. HTTPS) also require you to enable a secured communication via SSL/TLS. With the certificate extension enabled, Gardener can manage commonly trusted X.509 certificate for your application endpoint. From initially requesting certificate, it also handeles their renewal in time using the free Let’s Encrypt API.\nThere are two senarios with which you can use the certificate extension\n You want to use a certificate for a subdomain the shoot’s default DNS (see .spec.dns.domain of your shoot resource, e.g. short.ingress.shoot.project.default-domain.gardener.cloud). If this is your case, please keep reading this article. You want to use a certificate for a custom domain. If this is your case, please see Manage certificates with Gardener for public domain  Prerequisites Before you start this guide there are a few requirements you need to fulfill:\n You have an existing shoot cluster  Since you are using the default DNS name, all DNS configuration should already be done and ready.\nIssue a certificate Every X.509 certificate is represented by a Kubernetes custom resource certificate.cert.gardener.cloud in your cluster. A Certificate resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener’s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.\n Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.\n Certificates can be requested via 3 resources type\n Ingress Service (type LoadBalancer) certificate (Gardener CRD)  If either of the first 2 are used, a corresponding Certificate resource will automatically be created.\nUsing an ingress Resource apiVersion: networking.k8s.io/v1 kind: Ingress metadata:  name: amazing-ingress  annotations:  cert.gardener.cloud/purpose: managed  #cert.gardener.cloud/issuer: custom-issuer spec:  tls:  - hosts:  # Must not exceed 64 characters.  - short.ingress.shoot.project.default-domain.gardener.cloud  # Certificate and private key reside in this secret.  secretName: tls-secret  rules:  - host: short.ingress.shoot.project.default-domain.gardener.cloud  http:  paths:  - pathType: Prefix  path: \"/\"  backend:  service:  name: amazing-svc  port:  number: 8080 Using a service type LoadBalancer apiVersion: v1 kind: Service metadata:  annotations:  cert.gardener.cloud/purpose: managed  # Certificate and private key reside in this secret.  cert.gardener.cloud/secretname: tls-secret  # You may add more domains separated by commas (e.g. \"service.shoot.project.default-domain.gardener.cloud, amazing.shoot.project.default-domain.gardener.cloud\")  dns.gardener.cloud/dnsnames: \"service.shoot.project.default-domain.gardener.cloud\"  dns.gardener.cloud/ttl: \"600\"  #cert.gardener.cloud/issuer: custom-issuer  name: test-service  namespace: default spec:  ports:  - name: http  port: 80  protocol: TCP  targetPort: 8080  type: LoadBalancer Using the custom Certificate resource apiVersion: cert.gardener.cloud/v1alpha1 kind: Certificate metadata:  name: cert-example  namespace: default spec:  commonName: short.ingress.shoot.project.default-domain.gardener.cloud  secretRef:  name: tls-secret  namespace: default  # Optionnal if using the default issuer  issuerRef:  name: garden If you’re interested in the current progress of your request, you’re advised to consult the description, more specifically the status attribute in case the issuance failed.\nRequest a wildcard certificate In order to avoid the creation of multiples certificates for every single endpoints, you may want to create a wildcard certificate for your shoot’s default cluster.\napiVersion: networking.k8s.io/v1 kind: Ingress metadata:  name: amazing-ingress  annotations:  cert.gardener.cloud/purpose: managed  cert.gardener.cloud/commonName: \"*.ingress.shoot.project.default-domain.gardener.cloud\" spec:  tls:  - hosts:  - amazing.ingress.shoot.project.default-domain.gardener.cloud  secretName: tls-secret  rules:  - host: amazing.ingress.shoot.project.default-domain.gardener.cloud  http:  paths:  - pathType: Prefix  path: \"/\"  backend:  service:  name: amazing-svc  port:  number: 8080 Please note that this can also be achived by directly adding an annotation to a Service type LoadBalancer. You could also create a Certificate object with a wildcard domain.\nMore information For more information and more examples about using the certificate extension, please see Manage certificates with Gardener for public domain\n","categories":"","description":"Use the Gardener cert-management to get fully managed, publicly trusted TLS certificates","excerpt":"Use the Gardener cert-management to get fully managed, publicly …","ref":"/docs/extensions/others/gardener-extension-shoot-cert-service/docs/usage/request_default_domain_cert/","tags":["task"],"title":"Manage certificates with Gardener for default domain"},{"body":"Manage certificates with Gardener for public domain Introduction Dealing with applications on Kubernetes which offer a secure service endpoints (e.g. HTTPS) also require you to enable a secured communication via SSL/TLS. With the certificate extension enabled, Gardener can manage commonly trusted X.509 certificate for your application endpoint. From initially requesting certificate, it also handeles their renewal in time using the free Let’s Encrypt API.\nThere are two senarios with which you can use the certificate extension\n You want to use a certificate for a subdomain the shoot’s default DNS (see .spec.dns.domain of your shoot resource, e.g. short.ingress.shoot.project.default-domain.gardener.cloud). If this is your case, please see Manage certificates with Gardener for default domain You want to use a certificate for a custom domain. If this is your case, please keep reading this article.  Prerequisites Before you start this guide there are a few requirements you need to fulfill:\n You have an existing shoot cluster Your custom domain is under a public top level domain (e.g. .com) Your custom zone is resolvable with a public resolver via the internet (e.g. 8.8.8.8) You have a custom DNS provider configured and working (see “DNS Providers”)  As part of the Let’s Encrypt ACME challenge validation process, Gardener sets a DNS TXT entry and Let’s Encrypt checks if it can both resolve and authenticate it. Therefore, it’s important that your DNS-entries are publicly resolvable. You can check this by querying e.g. Googles public DNS server and if it returns an entry your DNS is publicly visible:\n# returns the A record for cert-example.example.com using Googles DNS server (8.8.8.8) dig cert-example.example.com @8.8.8.8 A DNS provider In order to issue certificates for a custom domain you need to specify a DNS provider which is permitted to create DNS records for subdomains of your requested domain in the certificate. For example, if you request a certificate for host.example.com your DNS provider must be capable of managing subdomains of host.example.com.\nDNS providers are normally specified in the shoot manifest. To learn more on how to configure one, please see the DNS provider documentation.\nIssue a certificate Every X.509 certificate is represented by a Kubernetes custom resource certificate.cert.gardener.cloud in your cluster. A Certificate resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener’s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.\n Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.\n Certificates can be requested via 3 resources type\n Ingress Service (type LoadBalancer) Certificate (Gardener CRD)  If either of the first 2 are used, a corresponding Certificate resource will created automatically.\nUsing an ingress Resource apiVersion: networking.k8s.io/v1 kind: Ingress metadata:  name: amazing-ingress  annotations:  cert.gardener.cloud/purpose: managed  # Optional but recommended, this is going to create the DNS entry at the same time  dns.gardener.cloud/class: garden  dns.gardener.cloud/ttl: \"600\" spec:  tls:  - hosts:  # Must not exceed 64 characters.  - amazing.example.com  # Certificate and private key reside in this secret.  secretName: tls-secret  rules:  - host: amazing.example.com  http:  paths:  - pathType: Prefix  path: \"/\"  backend:  service:  name: amazing-svc  port:  number: 8080 Replace the hosts and rules[].host value again with your own domain and adjust the remaining Ingress attributes in accordance with your deployment (e.g. the above is for an istio Ingress controller and forwards traffic to a service1 on port 80).\nUsing a service type LoadBalancer apiVersion: v1 kind: Service metadata:  annotations:  cert.gardener.cloud/secretname: tls-secret  dns.gardener.cloud/dnsnames: example.example.com  dns.gardener.cloud/class: garden  # Optional  dns.gardener.cloud/ttl: \"600\"  cert.gardener.cloud/commonname: \"*.example.example.com\"  cert.gardener.cloud/dnsnames: \"\"  name: test-service  namespace: default spec:  ports:  - name: http  port: 80  protocol: TCP  targetPort: 8080  type: LoadBalancer Using the custom Certificate resource apiVersion: cert.gardener.cloud/v1alpha1 kind: Certificate metadata:  name: cert-example  namespace: default spec:  commonName: amazing.example.com  secretRef:  name: tls-secret  namespace: default  # Optionnal if using the default issuer  issuerRef:  name: garden Supported attributes Here is a list of all supported annotations regarding the certificate extension:\n   Path Annotation Value Required Description     N/A cert.gardener.cloud/purpose: managed Yes when using annotations Flag for Gardener that this specific Ingress or Service requires a certificate   spec.commonName cert.gardener.cloud/commonname: E.g. “*.demo.example.com” or “special.example.com” Certificate and Ingress : No  Service: yes Specifies for which domain the certificate request will be created. If not specified, the names from spec.tls[].hosts are used. This entry must comply with the 64 character limit.   spec.dnsName cert.gardener.cloud/dnsnames: E.g. “special.example.com” Certificate and Ingress : No  Service: yes Additional domains the certificate should be valid for (Subject Alternative Name). If not specified, the names from spec.tls[].hosts are used. Entries in this list can be longer than 64 characters.   spec.secretRef.name cert.gardener.cloud/secretname: any-name Yes for certificate and Service Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it’ll be created automatically as soon as the certificate has been issued.   spec.issuerRef.name cert.gardener.cloud/issuer: E.g. gardener No Specifies the issuer you want to use. Only necessary if you request certificates for custom domains.   N/A cert.gardener.cloud/revoked: true otherwise always false No Use only to revoke a certificate, see reference for more details    Request a wildcard certificate In order to avoid the creation of multiples certificates for every single endpoints, you may want to create a wildcard certificate for your shoot’s default cluster.\napiVersion: networking.k8s.io/v1 kind: Ingress metadata:  name: amazing-ingress  annotations:  cert.gardener.cloud/purpose: managed  cert.gardener.cloud/commonName: \"*.example.com\" spec:  tls:  - hosts:  - amazing.example.com  secretName: tls-secret  rules:  - host: amazing.example.com  http:  paths:  - pathType: Prefix  path: \"/\"  backend:  service:  name: amazing-svc  port:  number: 8080 Please note that this can also be achived by directly adding an annotation to a Service type LoadBalancer. You could also create a Certificate object with a wildcard domain.\nUsing a custom Issuer Most Gardener deployment with the certification extension enabled have a preconfigured garden issuer. It is also usually configured to use Let’s Encrypt as the certificate provider.\nIf you need a custom issuer for a specific cluster, please see Using a custom Issuer\nQuotas For security reasons there may be a default quota on the certificate requests per day set globally in the controller registration of the shoot-cert-service.\nThe default quota only applies if there is no explicit quota defined for the issuer itself with the field requestsPerDayQuota, e.g.:\nkind: Shoot ... spec:  extensions:  - type: shoot-cert-service  providerConfig:  apiVersion: service.cert.extensions.gardener.cloud/v1alpha1  kind: CertConfig  issuers:  - email: your-email@example.com  name: custom-issuer # issuer name must be specified in every custom issuer request, must not be \"garden\"  server: 'https://acme-v02.api.letsencrypt.org/directory'  requestsPerDayQuota: 10 DNS Propagation As stated before, cert-manager uses the ACME challenge protocol to authenticate that you are the DNS owner for the domain’s certificate you are requesting. This works by creating a DNS TXT record in your DNS provider under _acme-challenge.example.example.com containing a token to compare with. The TXT record is only visible during the domain validation. Typically, the record is propagated within a few minutes. But if the record is not visible to the ACME server for any reasons, the certificate request is retried again after several minutes. This means you may have to wait up to one hour after the propagation problem has been resolved before the certificate request is retried. Take a look in the events with kubectl describe ingress example for troubleshooting.\nCharacter Restrictions Due to the ACME protocol specification, at least one domain of the domains you request a certificate for must not exceed a character limit of 64 (CN - Common Name).\nFor example, the following request is invalid:\napiVersion: cert.gardener.cloud/v1alpha1 kind: Certificate metadata:  name: cert-invalid  namespace: default spec:  commonName: morethan64characters.ingress.shoot.project.default-domain.gardener.cloud But it is valid to request a certificate for this domain if you have at least one domain which does not exceed the mentioned limit:\napiVersion: cert.gardener.cloud/v1alpha1 kind: Certificate metadata:  name: cert-example  namespace: default spec:  commonName: short.ingress.shoot.project.default-domain.gardener.cloud  dnsNames:  - morethan64characters.ingress.shoot.project.default-domain.gardener.cloud References  Gardener cert-management Managing DNS with Gardener  ","categories":"","description":"Use the Gardener cert-management to get fully managed, publicly trusted TLS certificates","excerpt":"Use the Gardener cert-management to get fully managed, publicly …","ref":"/docs/extensions/others/gardener-extension-shoot-cert-service/docs/usage/request_cert/","tags":["task"],"title":"Manage certificates with Gardener for public domain"},{"body":"Using a custom Issuer Another possibility to request certificates for custom domains is a dedicated issuer.\n Note: This is only needed if the default issuer provided by Gardener is restricted to shoot related domains or you are using domain names not visible to public DNS servers. Which means that your senario most likely doesn’t require your to add an issuer.\n The custom issuers are specified normally in the shoot manifest. If the shootIssuers feature is enabled, it can alternatively be defined in the shoot cluster.\nCustom issuer in the shoot manifest kind: Shoot ... spec:  extensions:  - type: shoot-cert-service  providerConfig:  apiVersion: service.cert.extensions.gardener.cloud/v1alpha1  kind: CertConfig  issuers:  - email: your-email@example.com  name: custom-issuer # issuer name must be specified in every custom issuer request, must not be \"garden\"  server: 'https://acme-v02.api.letsencrypt.org/directory'  privateKeySecretName: my-privatekey # referenced resource, the private key must be stored in the secret at `data.privateKey` (optionally, only needed as alternative to auto registration)   #precheckNameservers: # to provide special set of nameservers to be used for prechecking DNSChallenges for an issuer  #- dns1.private.company-net:53  #- dns2.private.company-net:53\"   #shootIssuers:  # if true, allows to specify issuers in the shoot cluster  #enabled: true   resources:  - name: my-privatekey  resourceRef:  apiVersion: v1  kind: Secret  name: custom-issuer-privatekey # name of secret in Gardener project If you are using an ACME provider for private domains, you may need to change the nameservers used for checking the availability of the DNS challenge’s TXT record before the certificate is requested from the ACME provider. By default, only public DNS servers may be used for this purpose. At least one of the precheckNameservers must be able to resolve the private domain names.\nUsing an issuer in the shoot cluster Prerequiste: The shootIssuers feature has to be enabled. It is either enabled globally in the ControllerDeployment or in the shoot manifest with:\nkind: Shoot ... spec:  extensions:  - type: shoot-cert-service  providerConfig:  apiVersion: service.cert.extensions.gardener.cloud/v1alpha1  kind: CertConfig  shootIssuers:  enabled: true # if true, allows to specify issuers in the shoot cluster ... Example for specifying an Issuer resource and its Secret directly in any namespace of the shoot cluster:\napiVersion: cert.gardener.cloud/v1alpha1 kind: Issuer metadata:  name: my-own-issuer  namespace: my-namespace spec:  acme:  domains:  include:  - my.own.domain.com  email: some.user@my.own.domain.com  privateKeySecretRef:  name: my-own-issuer-secret  namespace: my-namespace  server: https://acme-v02.api.letsencrypt.org/directory --- apiVersion: v1 kind: Secret metadata:  name: my-own-issuer-secret  namespace: my-namespace type: Opaque data:  privateKey: ... # replace '...' with valus encoded as base64 ","categories":"","description":"How to define a custom issuer forma shoot cluster","excerpt":"How to define a custom issuer forma shoot cluster","ref":"/docs/extensions/others/gardener-extension-shoot-cert-service/docs/usage/custom_shoot_issuer/","tags":["task"],"title":"Using a custom Issuer"},{"body":"Presenters This community call was led by Jens Schneider and Lothar Gesslein.\nOverview Starting the development of a new Gardener extension can be challenging, when you are not an expert in the Gardener ecosystem yet. Therefore, the first half of this community call led by Jens Schneider aims to provide a “getting started tutorial” at a beginner level. 23Technologies have developed a minimal working example for Gardener extensions, gardener-extension-mwe, hosted in a Github repository. Jens is following the Getting started with Gardener extension development tutorial, which\nIn the second part of the community call, Lothar Gesslein introduces the gardener-extension-shoot-flux, which allows for the automated installation of arbitrary Kubernetes resources into shoot clusters. As this extension relies on Flux, an overview of Flux’s capabilities is also provided.\nIf you are left with any questions regarding the content, you might find the answers at the Q\u0026A session and discussion held at the end.\nYou can find the tutorials in this community call at:\n Getting started with Gardener extension development A Gardener Extension for universal Shoot Configuration  If you are left with any questions regarding the content, you might find the answers at the Q\u0026A session and discussion held at the end of the meeting.\nRecording   ","categories":"","description":"","excerpt":"Presenters This community call was led by Jens Schneider and Lothar …","ref":"/blog/2022/06.17-gardener-community-meeting-june/","tags":"","title":"Community Call - Gardener Extension Development"},{"body":"Presenters This community call was led by Tim Ebert and Rafael Franzke.\nOverview So far, deploying Gardener locally was not possible end-to-end. While you certainly could run the Gardener components in a minikube or kind cluster, creating shoot clusters always required to register seeds backed by cloud provider infrastructure like AWS, Azure, etc..\nConsequently, developing Gardener locally was similarly complicated, and the entry barrier for new contributors was way too high.\nIn a previous community call (Hackathon “Hack The Metal”), we already presented a new approach for overcoming these hurdles and complexities.\nNow we would like to present the Local Provider Extension for Gardener and show how it can be used to deploy Gardener locally, allowing you to quickly get your feet wet with the project.\nIn this session, Tim Ebert goes through the process of setting up a local Gardener cluster. After his demonstration, Rafael Franzke showcases a different approach to building your clusters locally, which, while more complicated, offers a much faster build time.\nYou can find the tutorials in this community call at:\n Deploying Gardener locally Running Gardener locally  If you are left with any questions regarding the content, you might find the answers in the questions asked and answered throughout the meeting.\nRecording   ","categories":"","description":"","excerpt":"Presenters This community call was led by Tim Ebert and Rafael …","ref":"/blog/2022/03.23-gardener-community-meeting-march/","tags":"","title":"Community Call - Deploying and Developing Gardener Locally"},{"body":"Presenters This community call was led by Holger Kosser, Lukas Gross and Peter Sutter.\nOverview Watch the recording of our February 2022 Community call to see how to get started with the gardenctl-v2 and watch a walkthrough for gardenctl-v2 features. You’ll learn about targeting, secure shoot cluster access, SSH, and how to use cloud provider CLIs natively.\nThe session is led by Lukas Gross, who begins by giving some information on the motivations behind creating a new version of gardenctl - providing secure access to shoot clustes, enabling direct usage of kubectl and cloud provider CLIs and managing cloud provider resources for SSH access.\nHolger Kosser then takes over in order to delve deeper into the concepts behind the implementation of gardenctl-2, going over Targeting, Gardenlogin and Cloud Provider CLIs. After that, Peter Sutter does the first demo, where he presents the main features in gardenctl-2.\nThe next part details how to get started with gardenctl, followed by another demo. The landscape requirements are also discussed, as well as future plans and enhancement requests.\nYou can find the slides for this community call at Google Slides.\nIf you are left with any questions regarding the content, you might find the answers at the Q\u0026A session and discussion held at the end, as well as the questions asked and answered throughout the meeting.\nRecording   ","categories":"","description":"","excerpt":"Presenters This community call was led by Holger Kosser, Lukas Gross …","ref":"/blog/2022/02.17-gardener-community-meeting-february/","tags":"","title":"Community Call - Gardenctl-v2"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/2021/","tags":"","title":"2021"},{"body":"Happy New Year Gardeners! As we greet 2021, we also celebrate Gardener’s third anniversary. Gardener was born with its first open source commit on 10.1.2018 (its inception within SAP was of course some 9 months earlier):\ncommit d9619d01845db8c7105d27596fdb7563158effe1 Author: Gardener Development Community \u003cgardener.opensource@sap.com\u003e Date: Wed Jan 10 13:07:09 2018 +0100  Initial version of gardener  This is the initial contribution to the Open Source Gardener project. ... Looking back, three years down the line, the project initiators were working towards a special goal: Publishing Gardener as an open source project on Github.com. Join us as we look back at how it all began, the challenges Gardener aims to solve and why open source and the community was and is the project’s key enabler.\nGardener Kick-Off: “We opted to BUILD ourselves” Early 2017, SAP put together a small, jelled team of experts with a clear mission: work out how SAP could serve Kubernetes based environments (as a service) for all teams within the company. Later that same year SAP also joined the CNCF as platinum member.\nWe first deliberated intensively on the BUY options (including acquisitions, due to the size and estimated volume needed at SAP). There were some early products from commercial vendors and startups available that did not bind exclusively to one of the hyperscalers. But these products did not cover many of our crucial and immediate requirements for a multi-cloud environment.\nUltimately, we opted to BUILD ourselves. This decision was not made lightly, because right from the start, we knew that we would have to cover thousands of clusters, across the globe, on all kinds of infrastructures. We would have to be able to create them at scale as well as manage them 24x7. And thus, we predicted the need to invest into automation of all aspects, to keep the service TCO at a minimum, and to offer an enterprise worthy SLA early on. This particular endeavor grew into launching project Gardener, first internally, and ultimately fulfilling all checks, externally based on open source. Its mission statement, in a nutshell, is “Universal Kubernetes at scale”. Now, that’s quite bold. But we also had a nifty innovation that helped us tremendously along the way. And we can openly reveal the secret here: Gardener was built, not only for creating Kubernetes at scale, but it was built (recursively) in Kubernetes itself.\nWhat Do You Get with Gardener? Gardener offers managed and homogenous Kubernetes clusters on IaaS providers like AWS, Azure, GCP, AliCloud, Open Telekom Cloud, SCS, OVH and more, but also covers versatile infrastructures like OpenStack, VMware or bare metal. Day-1 and Day-2 operations are an integral part of a cluster’s feature set. This means, that Gardener is not only capable of provisioning or de-provisioning thousands of clusters, but also of monitoring your cluster’s health state, upgrading components in a rolling fashion, or scaling the control plane as well as worker nodes up and down depending on the current resource demand.\nSome features mentioned above might sound familiar to you, simply because they’re squarely derived from Kubernetes. Concretely, if you explore a Gardener managed end-user cluster, you’ll never see so-called “control plane components” (Kube-Apiserver, Kube-Controller-Manager, Kube-Scheduler, etc.) The reason is they run as Pods inside another, hosting/seeding Kubernetes cluster. Speaking in Gardener terms, the latter is called a Seed cluster, and the end-user cluster is called a Shoot cluster; and thus the botanical naming scheme for Gardener was born. Further assets like infrastructure components or worker machines are modelled as managed Kubernetes objects too. This allows Gardener to leverage all the great and production proven features of Kubernetes - for managing Kubernetes clusters. Our blog post on Kubernetes.io reveals more details about architectural refinements.\nFigure 1: Gardener architecture overview End-users directly benefit from Gardener’s recursive architecture. Many requirements we identified for the Gardener service, turned out to be highly convenient for shoot owners. For instance, Seed clusters are usually equipped with DNS and x509 services. At the same time, these service offerings can be extended to requests coming from the Shoot clusters i.e., end-users get domain names and certificates for their applications out of the box.\nRecognizing the Power of Open Source The Gardener team immediately profited from open source: from Kubernetes obviously, and all its ecosystem projects. That all facilitated our project’s very fast and robust development. But it does not answer:\n“Why would SAP open source a tool that clearly solves a monetizable enterprise requirement?\"\nShort spoiler alert: it initially involved a leap of faith. If we just look at our own decision path, it is undeniable, developers, and with them entire industries, gravitate towards open source. We chose Linux, Containers and Kubernetes, exactly because they are open, and we could bet on network effects, especially around skills. The same decision process is currently replicated in thousands of companies, with the same results. Why? Because all companies are digitally transforming. They are becoming software companies as well to a certain extent. Many of them are also our customers and in many discussions, we recognized that they have the same challenges that we are solving with Gardener. This, in essence, was a key eye opener. We were confident that if we developed Gardener open source, we’d not only seize the opportunity to shape a Kubernetes management tool that finds broad interest and adoption outside our use case at SAP, but we could solve common challenges faster with the help of a community, and that in consequence would sustain continuous feature development.\nCoincidently that was also when the SAP Open Source Program Office (OSPO) was launched. It supported us making a case to develop Gardener completely as open source. Today, we can witness that this strategy has unfolded. It opened the gates not only for adoption, but for co-innovation, investment security, and user feedback directly in code. Below you can see an example of how the Gardener project benefits from this external community power as contributions are submitted right away.\nFigure 2: Example immediate community contribution Differentiating Gardener from Other Kubernetes Management Solutions Imagine you created a modern solid cloud native app or service, fully scalable, in containers. And the business case requires you to run the service on multiple clouds, like AWS, AliCloud, Azure, … maybe even on-premises like OpenStack or VMware. Your development team did everything to ensure that the workload was highly portable. But they would need to qualify each providers’ managed Kubernetes offering and their custom Bill-of-Material (BoM), their versions, their deprecation plan, roadmap etc. Your TCD would explode and this is exactly what teams at SAP experienced. Now, with Gardener you can, instead, roll out homogeneous clusters and stay in control of your versions and a single roadmap. Across all supported providers!\nAlso, teams that have serious, or say, more demanding workloads running on Kubernetes will come to the same conclusion: They require the full management control of the Kubernetes underlay. Not only that, they need access, visibility, and all tuning options for the control plane to safeguard their service. This is a conclusion not only from teams at SAP, but also from our community members, like PingCap who use Gardener to serve TiDB Cloud service. Whenever you need to get serious and need more than one or two clusters, Gardener is your friend.\nWho Is Using Gardener? Well, there is SAP itself of course, but also the number of Gardener adopters and companies interested in Gardener is growing (~1700 GitHub stars), as more are challenged with multi-cluster and multi-cloud requirements.\nFlant, PingCap, StackIT, T-Systems, Sky, or b’nerd are among these companies, to name a few. They use Gardener to either run products they sell on top or offer managed Kubernetes clusters directly to their clients, or even only components that are re-usable from Gardener.\nAn interesting journey in the open source space started with Finanz Informatik Technologie Service (FI-TS), a European Central Bank regulated and certified hoster for banks. They operate in very restricted environments, as you can imagine, and as such, they re-designed their datacenter for cloud native workloads from scratch, that is from cabling, racking and stacking to an API that serves bare metal servers. For Kubernetes-as-a-Service they evaluated and chose Gardener because it was open and a perfect candidate. With Gardener’s extension capabilities, it was possible to bring managed Kubernetes clusters to their very own bare metal stack, metal-stack.io. Of course, this meant implementation effort. But by reusing the Gardener project, FI-TS was able to leverage our standard with minimal adjustments for their special use-case. Subsequently, with their contributions, SAP was able to make Gardener more open for the community.\nFull Speed Ahead with the Community in 2021 Some of the current and most active topics are about the installer (Landscaper), control plane migration, automated seed management and documentation. Even though once you are into Kubernetes and then Gardener, all complexity falls into place, you can make all the semantic connections yourself. But beginners that join the community without much prior knowledge should experience a ramp-up with slighter slope. And that is currently a pain point. Experts directly ask questions about documentation not being up-to-date or clear enough. We prioritized the functionality of what you get with Gardener at the outset and need to catch up. But here is the good part: Now that we are starting the installation subject, later, we will have a much broader picture of what we need to install and maintain Gardener, and how we will build it.\n In a community call last summer, we gave an overview of what we are building: The Landscaper. With this tool, we will be able to not only install a full Gardener landscape, but we will also streamline patches, updates and upgrades with the Landscaper. Gardener adopters can then attach to a release train from the project and deploy Gardener into a dev, canary and multiple production environments sequentially. Like we do at SAP.\nKey Takeaways in Three Years of Gardener #1 Open Source is strategic Open Source is not just about using freely available libraries, components, or tools to optimize your own software production anymore. It is strategic, unfolds for projects like Gardener, and that in the meantime also reached the Board Room.\n#2 Solving Concrete Challenges by Co-Innovation Users of a particular product or service increasingly vote/decide for open source variants, such as project Gardener. Because that allows them to freely innovate and solve concrete challenges by developing exactly what they require (see FI-TS example). This user-centric process has tremendous advantages. It clears out the middleman and other vested interests. You have access to the full code. And lastly, if others start using and contributing to your innovation, it allows enterprises to secure their investments for the long term. And that’s re-enforces point #1 for enterprises that yet have to create a strategic Open Source Program Office.\n#3 Cloud Native Skills Gardener solves problems by applying Kubernetes and Kubernetes principles itself. Developers and operators who obtain familiarity with Kubernetes, will immediately notice, and appreciate our concept and can contribute intuitively. The Gardener maintainers feel responsible to facilitate community members and contributors. Barriers will further be reduced by our ongoing landscaper and documentation efforts. This is why we are so confident on Gardener adoption.\nThe Gardener team is gladly welcoming new community members, especially regarding adoption and contribution. Be invited to try out your very own Gardener installation, join our Slack channel or community calls. We’re looking forward to seeing you there!\n","categories":"","description":"","excerpt":"Happy New Year Gardeners! As we greet 2021, we also celebrate …","ref":"/blog/2021/02.01-happy-anniversary-gardener/","tags":"","title":"Happy Anniversary, Gardener! Three Years of Open Source Kubernetes Management"},{"body":"Kubernetes is a cloud-native enabler built around the principles for a resilient, manageable, observable, highly automated, loosely coupled system. We know that Kubernetes is infrastructure agnostic with the help of provider specific Cloud Controller Manager. But Kubernetes has explicitly externalized the mangement of the nodes. Once they appear - correctly configured - in the cluster, Kubernetes can use them. If nodes fail, Kubernetes can’t do anything about it, external tooling is required. But every tool, every provider is different. So, why not elevate node management to a first class Kubernetes citizen? Why not create a Kubernetes native resource that manages machines just like pods? Such an approach is brought to you by the Machine Controller Manager (aka MCM), which, of course, is an open sourced project. MCM gives you the following benefits:\n seamlessly manage machines/nodes with a declarative API (of course, across different cloud providers), integrate generically with the cluster autoscaler, plugin with tools such as the node-problem-detector, transport the immutability design principle to machine/nodes as well, and last but not least, implement e.g. rolling upgrades of machines/nodes.  Machine Controller Manager aka MCM Machine Controller Manager is a group of cooperative controllers that manage the lifecycle of the worker machines. It is inspired by the design of Kube Controller Manager in which various sub controllers manage their respective Kubernetes Clients.\nMachine Controller Manager reconciles a set of Custom Resources namely MachineDeployment, MachineSet and Machines which are managed \u0026 monitored by their controllers MachineDeployment Controller, MachineSet Controller, Machine Controller respectively along with another cooperative controller called the Safety Controller.\nUnderstanding the sub-controllers and Custom Resources of MCM The Custom Resources MachineDeployment, MachineSet and Machines are very much analogous to the native K8s resources of Deployment, ReplicaSet and Pods respectively. So, in the context of MCM:\n MachineDeployment provides a declarative update for MachineSet and Machines. MachineDeployment Controller reconciles the MachineDeployment objects and manages the lifecycle of MachineSet objects. MachineDeployment consumes provider specific MachineClass in its spec.template.spec which is the template of the VM spec that would be spawned on the cloud by MCM. MachineSet ensures that the specified number of Machine replicas are running at a given point of time. MachineSet Controller reconciles the MachineSet objects and manages the lifecycle of Machine objects. Machines are the actual VMs running on the cloud platform provided by one of the supported cloud providers. Machine Controller is the controller that actually communicates with the cloud provider to create/update/delete machines on the cloud. There is a Safety Controller responsible for handling the unidentified or unknown behaviours from the cloud providers. Along with the above Custom Controllers and Resources, MCM requires the MachineClass to use K8s Secret that stores cloudconfig (initialization scripts used to create VMs) and cloud specific credentials.  Workings of MCM Figure 1: In-Tree Machine Controller Manager In MCM, there are two K8s clusters in the scope — a Control Cluster and a Target Cluster. Control Cluster is the K8s cluster where the MCM is installed to manage the machine lifecycle of the Target Cluster. In other words, Control Cluster is the one where the machine-* objects are stored. Target Cluster is where all the node objects are registered. These clusters can be two distinct clusters or the same cluster, whichever fits.\nWhen a MachineDeployment object is created, MachineDeployment Controller creates the corresponding MachineSet object. The MachineSet Controller in-turn creates the Machine objects. The Machine Controller then talks to the cloud provider API and actually creates the VMs on the cloud.\nThe cloud initialization script that is introduced into the VMs via the K8s Secret consumed by the MachineClasses talks to the KCM (K8s Controller Manager) and creates the node objects. Nodes after registering themselves to the Target Cluster, start sending health signals to the machine objects. That is when MCM updates the status of the machine object from Pending to Running. More on Safety Controller Safety Controller contains following functions:\nOrphan VM Handling:\n It lists all the VMs in the cloud; matching the tag of given cluster name and maps the VMs with the Machine objects using the ProviderID field. VMs without any backing Machine objects are logged and deleted after confirmation. This handler runs every 30 minutes and is configurable via --machine-safety-orphan-vms-period flag.  Freeze Mechanism:\n Safety Controller freezes the MachineDeployment and MachineSet controller if the number of Machine objects goes beyond a certain threshold on top of Spec.Replicas. It can be configured by the flag --safety-up or --safety-down and also --machine-safety-overshooting-period. Safety Controller freezes the functionality of the MCM if either of the target-apiserver or the control-apiserver is not reachable. Safety Controller unfreezes the MCM automatically once situation is resolved to normal. A freeze label is applied on MachineDeployment/MachineSet to enforce the freeze condition.  Evolution of MCM from In-Tree to Out-of-Tree (OOT) MCM supports declarative management of machines in a K8s Cluster on various cloud providers like AWS, Azure, GCP, AliCloud, OpenStack, Metal-stack, Packet, KubeVirt, VMWare, Yandex. It can, of course, be easily extended to support other cloud providers.\nGoing ahead having the implementation of the Machine Controller Manager supporting too many cloud providers would be too much upkeep from both a development and a maintenance point of view. Which is why, the Machine Controller component of MCM has been moved to Out-of-Tree design where Machine Controller for respective cloud provider runs as an independent executable; even though typically packaged under the same deployment.\nFigure 2: Out-Of-Tree (OOT) Machine Controller Manager This OOT Machine Controller will implement a common interface to manage the VMs on the respective cloud provider. Now, while Machine Controller deals with the Machine objects, Machine Controller Manager (MCM) deals with higher level objects such as MachineSet and MachineDeployment objects.\nA lot of contributions are already being made towards OOT Machine Controller Manager for various cloud providers. Below are the links to the repositories:\n Out of Tree Machine Controller Manager for AliCloud Out of Tree Machine Controller Manager for AWS Out of Tree Machine Controller Manager for Azure Out of Tree Machine Controller Manager for GCP Out of Tree Machine Controller Manager for KubeVirt Out of Tree Machine Controller Manager for Metal Out of Tree Machine Controller Manager for vSphere Out of Tree Machine Controller Manager for Yandex  Watch this video our YouTube Gardener Project channel to understand more about OOT MCM.\nWho Uses MCM? Gardener\nMCM is originally developed and employed by a K8s Control Plane as a Service called Gardener. However, the MCM’s design is elegant enough to be employed when managing the machines of any independent K8s clusters, without having to necessarily associate it with Gardener.\nMetal Stack\nMetal-stack is a set of microservices that implements Metal as a Service (MaaS). It enables you to turn your hardware into elastic cloud infrastructure. Metal-stack employs the Machine Controller Manager adopted to their Metal API. Check out an introduction to here.\nSky UK Limited\nSky UK Limited (a broadcaster) migrated their Kubernetes node management from Ansible to Machine Controller Manager. Check out this video on our YouTube Gardener Project channel.\nAlso, other interesting use cases with MCM are implemented by Kubernetes enthusiasts, who for example adjusted the Machine Controller Manager to provision machines in the cloud to extend a local Raspberry-Pi K3s cluster. Read more about it here or Check out this video on our YouTube Gardener Project channel.\nConclusion Machine Controller Manager is the leading automation tool for machine management for, and in, Kubernetes. And the best part is that it is open sourced. It is freely (and easily) usable and extensible, and the community more than welcomes contributions.\nWhether you want to know more about Machine Controller Manager or find out about a similar scope for your solutions, then visit the GitHub page machine-controller-manager. We are so excited to see what you achieve with Machine Controller Manager.\n","categories":"","description":"","excerpt":"Kubernetes is a cloud-native enabler built around the principles for a …","ref":"/blog/2021/01.25-machine-controller-manager/","tags":"","title":"Machine Controller Manager"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/2020/","tags":"","title":"2020"},{"body":"STACKIT is a digital brand of Europe’s biggest retailer, the Schwarz Group, which consists of Lidl, Kaufland, as well as production and recycling companies. Following the industry trend, the Schwarz Group is in the process of a digital transformation. STACKIT enables this transformation by helping to modernize the internal IT of the company branches.\nWhat is STACKIT and the STACKIT Kubernetes Engine (SKE)? STACKIT started with colocation solutions for internal and external customers in Europe-based data centers, which was then expanded to a full cloud platform stack providing an IaaS layer with VMs, storage and network, as well as a PaaS layer including Cloud Foundry and a growing set of cloud services, like databases, messaging, etc.\nWith containers and Kubernetes becoming the lingua franca of the cloud, we are happy to announce the STACKIT Kubernetes Engine (SKE), which has been released as Beta in November this year. We decided to use Gardener as the cluster management engine underneath SKE - for good reasons as you will see – and we would like to share our experiences with Gardener when working on the SKE Beta release, and serve as a testimonial for this technology.\nFigure 1: STACKIT Component Diagram Why We Chose Gardener as a Cluster Management Tool We started with the Kubernetes endeavor in the beginning of 2020 with a newly formed agile team that consisted of software engineers, highly experienced in IT operations and development. After some exploration and a short conceptual phase, we had a clear-cut opinion on how the cluster management for STACKIT should look like: we were looking for a highly customizable tool that could be adapted to the specific needs of STACKIT and the Schwarz Group, e.g. in terms of network setup or the infrastructure layer it should be running on. Moreover, the tool should be scalable to a high number of managed Kubernetes clusters and should therefore provide a fully automated operation experience. As an open source project, contributing and influencing the tool, as well as collaborating with a larger community were important aspects that motivated us. Furthermore, we aimed to offer cluster management as a self-service in combination with an excellent user experience. Our objective was to have the managed clusters come with enterprise-grade SLAs – i.e. with “batteries included”, as some say.\nWith this mission, we started our quest through the world of Kubernetes and soon found Gardener to be a hot candidate of cluster management tools that seemed to fulfill our demands. We quickly got in contact and received a warm welcome from the Gardener community. As interested potential adopter, but in the early days of the COVID-19 lockdown, we managed to organize an online workshop during which we got an introduction and deep dive into Gardener and discussed the STACKIT use cases. We learned that Gardener is extensible in many dimensions, and that contributions are always welcome and encouraged. Once we understood the basic Gardener concepts of Garden, Shoot and Seed clusters, its inception design and how this extends Kubernetes concepts in a natural way, we were eager to evaluate this tool in more detail.\nAfter this evaluation, we were convinced that this tool fulfilled all our requirements - a decision was made and off we went.\nHow Gardener was Adapted and Extended by SKE After becoming familiar with Gardener, we started to look into its code base to adapt it to the specific needs of the STACKIT OpenStack environment. Changes and extensions were made in order to get it integrated into the STACKIT environment, and whenever reasonable, we contributed those changes back:\n To run smoothly with the STACKIT OpenStack layer, the Gardener configuration was adapted in different places, e.g. to support CSI driver or to configure the domains of shoot API server or ingress. Gardener was extended to support shoots and shooted seeds in dual stack and dual home setup. This is used in SKE for the communication between shooted seeds and the Garden cluster. SKE uses a private image registry for Gardener installation to resolve dependencies to public image registries and to have more control over the used Gardener versions. To install and run Gardener with the private image registry, some new configurations need to be introduced into Gardener. Gardener is a first-class API based service what allowed us to smoothly integrate it into the STACKIT User Interface. We were able to jump-start and utilize also the Gardener Dashboard for our Beta release by merely adjusting the look-\u0026-feel, i.e. colors, labels and icons.  Figure 2: Gardener Dashboard adapted to STACKIT UI style Experience with Gardener Operations As no OpenStack installation is identical to one another, getting Gardener to run stable on the STACKIT IaaS layer revealed some operational challenges. For instance, it was challenging to find the right configuration for Cinder CSI.\nTo test for its resilience, we tried to break the managed clusters with a Chaos Monkey test, e.g. by deleting services or components needed by Kubernetes and Gardener to work properly. The reconciliation feature of Gardener fixed all those problems automatically, so that damaged Shoot clusters became operational again after a short period of time. Thus, we were not able to break Shoot clusters from an end user perspective permanently, despite our efforts. Which again speaks for Gardener’s first-class cloud native design.\nWe also participated in a fruitful community support: For several challenges we contacted the community channel and help was provided in a timely manner. A lesson learned was that raising an issue in the community early on, before getting stuck too long on your own with an unresolved problem, is essential and efficient.\nSummary Gardener is used by SKE to provide a managed Kubernetes offering for internal use cases of the Schwarz Group as well as for the public cloud offering of STACKIT. Thanks to Gardener, it was possible to get from zero to a Beta release in only about half a year’s time – this speaks for itself. Within this period, we were able to integrate Gardener into the STACKIT environment, i.e. in its OpenStack IaaS layer, its management tools and its identity provisioning solution.\nGardener has become a vital building block in STACKIT’s cloud native platform offering. For the future, the possibility to manage clusters also on other infrastructures and hyperscalers is seen as another great opportunity for extended use cases. The open co-innovation exchange with the Gardener community member companies has also opened the door to commercial co-operation.\n","categories":"","description":"","excerpt":"STACKIT is a digital brand of Europe’s biggest retailer, the Schwarz …","ref":"/blog/2020/12.03-stackit-kubernetes-engine-with-gardener/","tags":"","title":"STACKIT Kubernetes Engine with Gardener"},{"body":"Use an identity provider to authenticate users to access shoot clusters.\nPrerequisites Please read the following background material on Authenticating.\nOverview Kubernetes on its own doesn’t provide any user management. In other words, users aren’t managed through Kubernetes resources. Whenever you refer to a human user it’s sufficient to use a unique ID, for example, an email address. Nevertheless, Gardener project owners can use an identity provider to authenticate user access for shoot clusters in the following way:\n Configure an Identity Provider using OpenID Connect (OIDC). Configure a local kubectl oidc-login to enable oidc-login. Configure the shoot cluster to share details of the OIDC-compliant identity provider with the Kubernetes API Server. Authorize an authenticated user using role-based access control (RBAC). Verify the result   Gardener allows administrators to modify aspects of the control plane setup. It gives administrators full control of how the control plane is parameterized. While this offers much flexibility, administrators need to ensure that they don’t configure a control plane that goes beyond the service level agreements of the responsible operators team.\n Configure an Identity Provider Create a tenant in an OIDC compatible Identity Provider. For simplicity, we use Auth0, which has a free plan.\n  In your tenant, create a client application to use authentication with kubectl:\n  Provide a Name, choose Native as application type, and choose CREATE.\n  On tab Settings, copy the following parameters to a local text file:\n  Domain\n Corresponds to the issuer in OIDC. It must be an https-secured endpoint (Auth0 requires a trailing / at the end). More information: Issuer Identifier.\n   Client ID\n  Client Secret\n    Configure the client to have a callback url of http://localhost:8000. This callback connects to your local kubectl oidc-login plugin:\n  Save your changes.\n  Verify that https://\u003cAuth0 Domain\u003e/.well-known/openid-configuration is reachable.\n  Choose Users \u0026 Roles \u003e Users \u003e CREATE USERS to create a user with a user and password:\n Users must have a verified email address.\n   Configure a local kubectl oidc-login   Install the kubectl plugin oidc-login. We highly recommend the krew install tool, which also makes other plugins easily available.\nkubectl krew install oidc-login The response looks like this:\nUpdated the local copy of plugin index. Installing plugin: oidc-login CAVEATS: \\ | You need to setup the OIDC provider, Kubernetes API server, role binding and kubeconfig. | See https://github.com/int128/kubelogin for more. / Installed plugin: oidc-login   Prepare a kubeconfig for later use:\ncp ~/.kube/config ~/.kube/config-oidc   Modify the configuration of ~/.kube/config-oidc as follows:\napiVersion: v1 kind: Config  ...  contexts: - context:  cluster: shoot--project--mycluster  user: my-oidc  name: shoot--project--mycluster  ...  users: - name: my-oidc  user:  exec:  apiVersion: client.authentication.k8s.io/v1beta1  command: kubectl  args:  - oidc-login  - get-token  - --oidc-issuer-url=https://\u003cIssuer\u003e/  - --oidc-client-id=\u003cClient ID\u003e  - --oidc-client-secret=\u003cClient Secret\u003e  - --oidc-extra-scope=email,offline_access,profile   To test our OIDC-based authentication, context shoot--project--mycluster of ~/.kube/config-oidc is used in a later step. For now, continue to use the configuration ~/.kube/config with administration rights for your cluster.\nConfigure the shoot cluster Modify the shoot cluster YAML as follows, using the client ID and the domain (as issuer) from the settings of the client application you created in Auth0:\nkind: Shoot apiVersion: garden.sapcloud.io/v1beta1 metadata:  name: mycluster  namespace: garden-project ... spec:  kubernetes:  kubeAPIServer:  oidcConfig:  clientID: \u003cClient ID\u003e  issuerURL: \"https://\u003cIssuer\u003e/\"  usernameClaim: email This change of the Shoot manifest triggers a reconciliation. Once the reconciliation is finished, your OIDC configuration is applied. It doesn’t invalidate other certificate-based authentication methods. Wait for Gardener to reconcile the change. It can take up to 5 minutes.\nAuthorize an authenticated user In Auth0, you created a user with a verified email address, test@test.com in our example. For simplicity, we authorize a single user identified by this email address with cluster role view:\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata:  name: viewer-test roleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: view subjects: - apiGroup: rbac.authorization.k8s.io  kind: User  name: test@test.com As administrator, apply the cluster role binding in your shoot cluster.\nVerify the result   To step into the shoes of your user, use the prepared kubeconfig file ~/.kube/config-oidc, and switch to the context that uses oidc-login:\ncd ~/.kube export KUBECONFIG=$(pwd)/config-oidc kubectl config use-context `shoot--project--mycluster`   kubectl delegates the authentication to plugin oidc-login the first time the user uses kubectl to contact the API server, for example:\nkubectl get all The plugin opens a browser for an interactive authentication session with Auth0, and in parallel serves a local webserver for the configured callback.\n  Enter your login credentials.\nYou should get a successful response from the API server:\nOpening in existing browser session. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 100.64.0.1 \u003cnone\u003e 443/TCP 86m  After a successful login, kubectl uses a token for authentication so that you don’t have to provide user and password for every new kubectl command. How long the token is valid can be configured. If you want to log in again earlier, reset plugin oidc-login:\n Delete directory ~/.kube/cache/oidc-login. Delete the browser cache.     To see if your user uses cluster role view, do some checks with kubectl auth can-i.\n  The response for the following commands should be no:\nkubectl auth can-i create clusterrolebindings kubectl auth can-i get secrets kubectl auth can-i describe secrets   The response for the following commands should be yes:\nkubectl auth can-i list pods kubectl auth can-i get pods     If the last step is successful, you’ve configured your cluster to authenticate against an identity provider using OIDC.\nRelated Links Auth0 Pricing\n","categories":"","description":"Authenticating with an Identity Provider using OpenID Connect","excerpt":"Authenticating with an Identity Provider using OpenID Connect","ref":"/docs/tutorials/oidc-login/","tags":"","title":"Authenticating with an Identity Provider"},{"body":"Dear community, we’re happy to announce a new minor release of Gardener, in fact, the 16th in 2020! v1.13 came out just today after a couple of weeks of code improvements and feature implementations. As usual, this blog post provides brief summaries for the most notable changes that we introduce with this version. Behind the scenes (and not explicitly highlighted below) we are progressing on internal code restructurings and refactorings to ease further extensions and to enhance development productivity. Speaking of those: You might be interested in watching the recording of the last Gardener Community Meeting which includes a detailed session for v2 of Terraformer, a complete rewrite in Golang and improved state handling.\nNotable Changes in v1.13 The main themes of Gardener’s v1.13 release are increments for feature gate promotions, scalability and robustness, and cleanups and refactorings. The community plans to continue on those and wants to deliver at least one more release in 2020.\nAutomatic Quotas for Gardener Resources (gardener/gardener#3072) Gardener already supports ResourceQuotas since the last release, however, it was still up to operators/administrators to create these objects in project namespaces. Obviously, in large Gardener installations with thousands of projects, this is a quite challenging task. With this release, we are shipping an improvement in the Project controller in the gardener-controller-manager that allows to automatically create ResourceQuotas based on configuration. Operators can distinguish via project label selectors which default quotas shall be defined for various projects. Please find more details here!\nResource Capacity and Reservations for Seeds (gardener/gardener#3075) The larger the Gardener landscape, the more seed cluster you require. Naturally, they have (based on constraints of the underlying infrastructure provider and/or seed cluster configuration) limits of how many shoots they can accommodate. Until this release, there were no means to prevent seed cluster from becoming overloaded (and potentially die due to this load). Now you define resource capacity and reservations in the gardenlet’s component configuration, similar to how the kubelet announces allocatable resources for Node objects. We are defaulting this to 250 shoots, but you might want to adapt this value for your own environment.\nDistributed Gardenlet Rollout for Shooted Seeds (gardener/gardener#3135) With the same motivation, i.e., to improve catering with large landscapes, we allow operators to configure distributed rollouts of gardenlets for shooted seeds. When a new Gardener version is being deployed in landscapes with a high number of shooted seeds, gardenlets of earlier versions were immediately re-deploying copies of themselves into the shooted seeds they manage. This leads to a large number of new gardenlet pods that all roughly start at the same time. Depending on the size of the landscape, this may trouble the gardener-apiservers as all of them are starting to fill their caches and create watches at the same time. By default, this rollout is now randomized within a 5m time window, i.e., it may take up to 5m until all gardenlets in all seeds have been updated.\nProgressing on Beta-Promotion for APIServerSNI Feature Gate (gardener/gardener#3082, gardener/gardener#3143) The alpha APIServerSNI feature will drastically reduce the costs for load balancers in the seed clusters, thus, it is effectively contributing to Gardener’s “minimal TCO” goal. In this release we are introducing an important improvement that optimizes the connectivity when pods talk to their control plane by avoiding an extra network hop. This is realized by a MutatingWebhookConfiguration whose server runs as a sidecar container in the kube-apiserver pod in the seed (only when the APIServerSNI feature gate is enabled). The webhook injects a KUBERNETES_SERVICE_HOST environment variable into pods in the shoot which prevents the additional network hop to the apiserver-proxy on all worker nodes. You can read more about it in this document.\nMore Control Plane Configurability (gardener/gardener#3141, gardener/gardener#3139) A main capability beloved by Gardener users is its openness when it comes to configurability and fine-tuning of the Kubernetes control plane components. Most managed Kubernetes offerings are not exposing options of the master components, but Gardener’s Shoot API offers a selected set of settings. With this release we are allowing to change the maximum number of (non-)mutating requests for the kube-apiserver of shoot clusters. Similarly, the grace period before deleting pods on failed nodes can now be fine-grained for the kube-controller-manager.\nImproved Project Resource Handling (gardener/gardener#3137, gardener/gardener#3136, gardener/gardener#3179) Projects are an important resource in the Gardener ecosystem as they enable collaboration with team members. A couple of improvements have landed into this release. Firstly, duplicates in the member list were not validated so far. With this release, the gardener-apiserver is automatically merging them, and in future releases requests with duplicates will be denied. Secondly, specific Projects may now be excluded from the stale checks if desired. Lastly, namespaces for Projects that were adopted (i.e., those that exist before the Project already) will now no longer deleted when the Project is being deleted. Please note that this only applies for newly created Projects.\nRemoval of Deprecated Labels and Annotations (gardener/gardener#3094) The core.gardener.cloud API group succeeded the old garden.sapcloud.io API group in the beginning of 2020, however, a lot of labels and annotations with the old API group name were still supported. We have continued with the process of removing those deprecated (but replaced with the new API group name) names. Concretely, the project labels garden.sapcloud.io/role=project and project.garden.sapcloud.io/name=\u003cproject-name\u003e are no longer supported now. Similarly, the shoot.garden.sapcloud.io/use-as-seed and shoot.garden.sapcloud.io/ignore-alerts annotations got deleted. We are not finished yet, but we do small increments and plan to progress on the topic until we finally got rid of all artifacts with the old API group name.\nNodeLocalDNS Network Policy Rules Adapted (gardener/gardener#3184) The alpha NodeLocalDNS feature was already introduced and explained with Gardener v1.8 with the motivation to overcome certain bottlenecks with the horizontally auto-scaled CoreDNS in all shoot cluster. Unfortunately, due to a bug in the network policy rules, it was not working in all environments. We have fixed this one now, so it should be ready for further tests and investigations. Come give it a try!\nPlease bear in mind that this blog post only highlights the most noticeable changes and improvements, but there is a whole bunch more, including a ton of bug fixes in older versions! Come check out the full release notes and share your feedback in our #gardener Slack channel!\n","categories":"","description":"","excerpt":"Dear community, we’re happy to announce a new minor release of …","ref":"/blog/2020/11.23-gardener-v1.13-released/","tags":"","title":"Gardener v1.13 Released"},{"body":" This is a guest commentary from metal-stack.\nmetal-stack is a software that provides an API for provisioning and managing physical servers in the data center. To categorize this product, the terms “Metal-as-a-Service” (MaaS) or “bare metal cloud” are commonly used.\n One reason you stumble upon this blog post could be that you saw errors like the following in your ETCD instances:\netcd-main-0 etcd 2020-09-03 06:00:07.556157 W | etcdserver: read-only range request \"key:\\\"/registry/deployments/shoot--pwhhcd--devcluster2/kube-apiserver\\\" \" with result \"range_response_count:1 size:9566\" took too long (13.95374909s) to execute As it turns out, 14 seconds are way too slow for running Kubernetes API servers. It makes them go into the crash loop (leader election fails). Even worse, this whole thing is self-amplifying: The longer a response takes, the more requests queue up, leading to response times increasing further and further. The system is very unlikely to recover. 😞\nOn Github, you can easily find the reason for this problem. Most probably your disks are too slow (see etcd-io/etcd#10860). So, when you are (like in our case) on GKE and run your ETCD on their default persistent volumes, consider moving from standard disks to SSDs and the error messages should disappear. A guide on how to use SSD volumes on GKE can be found here.\nCase closed? Well. For some people it might. But when you are seeing this in your Gardener infrastructure, likely, there is something going wrong. The entire ETCD management is fully managed by the Gardener, which makes the problem a bit more interesting to look at. This blog post strives topics such as:\n Gardener operating principles Gardener architecture and ETCD management Pitfalls with multi-cloud environments Migrating GCP volumes to a new storage class  We from metal-stack learned quite a lot about the capabilities of Gardener through this problem. We are happy to share this experience with a broader audience. Gardener adopters and operators read on.\nHow Gardener Manages ETCDs In our infrastructure, we use the Gardener to provision Kubernetes clusters on bare metal machines in our own data centers using metal-stack. Even if the entire stack could be running on-premise, our initial seed cluster and the metal control plane are hosted on GKE. This way, we do not need to manage a single Kubernetes cluster in our entire landscape manually. As soon as we have Gardener deployed on this initial cluster, we can spin up further Seeds in our own data centers through the concept of ManagedSeeds.\nTo make this easier to understand, let us give you a simplified picture of how our Gardener production setup looks like:\nFigure 1: Simplified View on Our Production Setup For every shoot cluster, Gardener deploys an individual, standalone ETCD as a stateful set into a shoot namespace. The deployment of the ETCD stateful set is managed by a controller called etcd-druid, which reconciles a special resource of the kind etcds.druid.gardener.cloud. This Etcd resource is getting deployed during the shoot provisioning flow in the Gardenlet.\nFor failure-safety, the etcd-druid deploys the official ETCD container image along with a sidecar project called etcd-backup-restore. The sidecar automatically takes backups of the ETCD and stores them at a cloud provider, e.g. in S3 Buckets, Google Buckets, or similar. In case the ETCD comes up without or with corrupted data, the sidecar looks into the backup buckets and automatically restores the latest backup before ETCD starts up. This entire approach basically takes away the pain for operators to manually have to restore data in the event of data loss.\n We found the etcd-backup-restore project very intriguing. It was the inspiration for us to come up with a similar sidecar for the databases we use with metal-stack. This project is called backup-restore-sidecar. We can cope with postgres and rethinkdb database at the moment and more to come. Feel free to check it out when you are interested.\n As it’s the nature for multi-cloud applications to act upon a variety of cloud providers, with a single installation of Gardener, it is easily possible to spin up new Kubernetes clusters not only on GCP, but on other supported cloud platforms, too.\nWhen the Gardenlet deploys a resource like the Etcd resource into a shoot namespace, a provider-specific extension-controller has the chance to manipulate it through a mutating webhook. This way, a cloud provider can adjust the generic Gardener resource to fit his provider-specific needs. For every cloud that Gardener supports, there is such an extension-controller. For metal-stack, we also maintain one, it’s called gardener-extension-provider-metal.\n A side note for cloud providers: Meanwhile, new cloud providers can be added fully out-of-tree, i.e. without touching any of the Gardener sources. This works through API extensions and CRDs. The Gardener handles generic resources and backpacks provider-specific configuration through raw extensions. When you are a cloud provider on your own, this is really encouraging because you can integrate with Gardener without any burdens. You can find documentation on how to integrate your cloud into the Gardener here and here.\n The Mistake is in the Deployment  This section contains code examples from Gardener v1.8.\n Now that we know how the ETCDs are managed by the Gardener, we can come back to the original problem from the beginning of this article. It turned out that the real problem was a misconfiguration in our deployment. The Gardener actually does use SSD-backed storage on GCP for ETCDs by default. During reconciliation, the gardener-extension-controller-gcp deploys a storage class called gardener.cloud-fast that enables accessing SSDs on GCP.\nBut for some reason, in our cluster we did not find such a storage class. And even more interesting, we did not use the gardener-extension-provider-gcp for any shoot reconciliation, only for ETCD backup purposes. And that was the big mistake we made: We reconciled the shoot control plane completely with gardener-extension-provider-metal even though our initial Seed actually runs on GKE and specific parts of the shoot control plane should be reconciled by the GCP extension-controller instead!\nThis is how the initial Seed resource looked like:\napiVersion: core.gardener.cloud/v1beta1 kind: Seed metadata: name: initial-seed spec: ... provider: region: gke type: metal ... ... Surprisingly, this configuration was working pretty well for a long time. The initial seed properly produced the Kubernetes control planes of our managed seeds that looked like this:\n$ kubectl get controlplanes.extensions.gardener.cloud NAME TYPE PURPOSE STATUS AGE fra-equ01 metal Succeeded 85d fra-equ01-exposure metal exposure Succeeded 85d And this is another interesting observation: There are two ControlPlane resources. One regular resource and one with an exposure purpose. Gardener distinguishes between two types for this exact reason: Environments where the shoot control plane runs on a different cloud provider than the Kubernetes worker nodes. The regular ControlPlane resource gets reconciled by the provider configured in the Shoot resource, the exposure type ControlPlane by the provider configured in the Seed resource.\nWith the existing configuration the gardener-extension-provider-gcp does not kick in and hence, it neither deploys the gardener.cloud-fast storage class nor does it mutate the Etcd resource to point to it. And in the end, we are left with ETCD volumes using the default storage class (which is what we do for ETCD stateful sets in the metal-stack seeds, because our default storage class uses csi-lvm that writes into logical volumes on the SSD disks in our physical servers).\nThe correction we had to make was a one-liner: Setting the provider type of the initial Seed resource to gcp.\n$ kubectl get seed initial-seed -o yaml apiVersion: core.gardener.cloud/v1beta1 kind: Seed metadata: name: initial-seed spec: ... provider: region: gke type: gcp # \u003c-- here ... ... This change moved over the control plane exposure reconciliation to the gardener-extension-provider-gcp:\n$ kubectl get -n \u003cshoot-namespace\u003e controlplanes.extensions.gardener.cloud NAME TYPE PURPOSE STATUS AGE fra-equ01 metal Succeeded 85d fra-equ01-exposure gcp exposure Succeeded 85d And boom, after some time of waiting for all sorts of magic reconciliations taking place in the background, the missing storage class suddenly appeared:\n$ kubectl get sc NAME PROVISIONER gardener.cloud-fast kubernetes.io/gce-pd standard (default) kubernetes.io/gce-pd Also, the Etcd resource was now configured properly to point to the new storage class:\n$ kubectl get -n \u003cshoot-namespace\u003e etcd etcd-main -o yaml apiVersion: druid.gardener.cloud/v1alpha1 kind: Etcd metadata: ... name: etcd-main spec: ... storageClass: gardener.cloud-fast # \u003c-- was pointing to default storage class before! volumeClaimTemplate: main-etcd ...  Only the etcd-main storage class gets changed to gardener.cloud-fast. The etcd-events configuration will still point to standard disk storage because this ETCD is much less occupied as compared to the etcd-main stateful set.\n The Migration Now that the deployment was in place such that this mistake would not repeat in the future, we still had the ETCDs running on the default storage class. The reconciliation does not delete the existing persistent volumes (PVs) on its own.\nTo bring production back up quickly, we temporarily moved the ETCD pods to other nodes in the GKE cluster. These were nodes which were less occupied, such that the disk throughput was a little higher than before. But surely that was not a final solution.\nFor a proper solution we had to move the ETCD data out of the standard disk PV into a SSD-based PV.\nEven though we had the etcd-backup-restore sidecar, we did not want to fully rely on the restore mechanism to do the migration. The backup should only be there for emergency situations when something goes wrong. Thus, we came up with another approach to introduce the SSD volume: GCP disk snapshots. This is how we did the migration:\n Scale down etcd-druid to zero in order to prevent it from disturbing your migration Scale down the kube-apiservers deployment to zero, then wait for the ETCD stateful to take another clean snapshot Scale down the ETCD stateful set to zero as well (in order to prevent Gardener from trying to bring up the downscaled resources, we used small shell constructs like while true; do kubectl scale deploy etcd-druid --replicas 0 -n garden; sleep 1; done) Take a drive snapshot in GCP from the volume that is referenced by the ETCD PVC Create a new disk in GCP from the snapshot on a SSD disk Delete the existing PVC and PV of the ETCD (oops, data is now gone!) Manually deploy a PV into your Kubernetes cluster that references this new SSD disk Manually deploy a PVC with the name of the original PVC and let it reference the PV that you have just created Scale up the ETCD stateful set and check that ETCD is running properly (if something went terribly wrong, you still have the backup from the etcd-backup-restore sidecar, delete the PVC and PV again and let the sidecar bring up ETCD instead) Scale up the kube-apiserver deployment again Scale up etcd-druid again (stop your shell hacks ;D)  This approach worked very well for us and we were able to fix our production deployment issue. And what happened: We have never seen any crashing kube-apiservers again. 🎉\nConclusion As bad as problems in production are, they are the best way for learning from your mistakes. For new users of the Gardener it can be pretty overwhelming to understand the rich configuration possibilities that the Gardener brings. However, once you get a hang of how the Gardener works, the application offers an exceptional versatility that makes it very much suitable for production use-cases like ours.\nThis example has shown how Gardener:\n Can handle arbitrary layers of infrastructure hosted by different cloud providers. Allows provider-specific tweaks to gain ideal performance for every cloud you want to support. Leverages Kubernetes core principles across the entire project architecture, making it vastly extensible and resilient. Brings useful disaster recovery mechanisms to your infrastructure (e.g. with etcd-backup-restore).  We hope that you could take away something new through this blog post. With this article we also want to thank the SAP Gardener team for helping us to integrate Gardener with metal-stack. It’s been a great experience so far. 😄 😍\n","categories":"","description":"In this case study, our friends from metal-stack lead you through their journey of migrating Gardener ETCD volumes in their production environment.","excerpt":"In this case study, our friends from metal-stack lead you through …","ref":"/blog/2020/11.20-case-study-migrating-etcd-volumes-in-production/","tags":"","title":"Case Study: Migrating ETCD Volumes in Production"},{"body":"Hibernate a Cluster Clusters are only needed 24 hours a day if they run productive workload. So whenever you do development in a cluster, or just use it for tests or demo purposes, you can save much money if you scale-down your Kubernetes resources whenever you don’t need them. However, scaling them down manually can become time-consuming the more resources you have.\nGardener offers a clever way to automatically scale-down all resources to zero: cluster hibernation. You can either hibernate a cluster by pushing a button or by defining a hibernation schedule.\n To save costs, it’s recommended to define a hibernation schedule before the creation of a cluster. You can hibernate your cluster or wake up your cluster manually even if there’s a schedule for its hibernation.\n  What is hibernated? What isn’t affected by the hibernation? Hibernate your cluster manually Wake up your cluster manually Create a schedule to hibernate your cluster  What is hibernated? When a cluster is hibernated, Gardener scales down worker nodes and the cluster’s control plane to free resources at the IaaS provider. This affects:\n Your workload, for example, pods, deployments, custom resources. The virtual machines running your workload. The resources of the control plane of your cluster.  What isn’t affected by the hibernation? To scale up everything where it was before hibernation, Gardener doesn’t delete state-related information, that is, information stored in persistent volumes. The cluster state as persistent in etcd is also preserved.\nHibernate your cluster manually The .spec.hibernation.enabled field specifies whether the cluster needs to be hibernated or not. If the field is set to true, the cluster’s desired state is to be hibernated. If it is set to false or not specified at all, the cluster’s desired state is to be awakened.\nTo hibernate your cluster you can run the following kubectl command:\n$ kubectl patch shoot -n $NAMESPACE $SHOOT_NAME -p '{\"spec\":{\"hibernation\":{\"enabled\": true}}}' Wake up your cluster manually To wake up your cluster you can run the following kubectl command:\n$ kubectl patch shoot -n $NAMESPACE $SHOOT_NAME -p '{\"spec\":{\"hibernation\":{\"enabled\": false}}}' Create a schedule to hibernate your cluster You can specify a hibernation schedule to automatically hibernate/wake up a cluster.\nLet’s have a look into the following example:\n hibernation:  enabled: false  schedules:  - start: \"0 20 * * *\" # Start hibernation every day at 8PM  end: \"0 6 * * *\" # Stop hibernation every day at 6AM  location: \"America/Los_Angeles\" # Specify a location for the cron to run in The above section configures a hibernation schedule that hibernates the cluster every day at 08:00 PM and wakes it up at 06:00 AM. The start or end fields can be omitted, though at least one of them has to be specified. Hence, it is possible to configure a hibernation schedule that only hibernates or wakes up a cluster. The location field is the time location used to evaluate the cron expressions.\n","categories":"","description":"","excerpt":"Hibernate a Cluster Clusters are only needed 24 hours a day if they …","ref":"/docs/gardener/usage/shoot_hibernate/","tags":["task"],"title":"Hibernate a Cluster"},{"body":"Two months after our last Gardener release update, we are happy again to present release v1.11 and v1.12 in this blog post. Control plane migration, load balancer consolidation, new security features are just a few topics we progressed with. As always, a detailed list of features, improvements, and bug fixes can be found in the release notes of each release. If you are going to update from a previous Gardener version, please take your time to go through the action items in the release notes.\nNotable Changes in v1.12 Release v1.12, fresh from the oven, is shipped with plenty of improvements, features and some API changes we want to pick up in the next sections.\nDrop Functionless DNS Providers (gardener/gardener#3036) This release drops the support for so-called functionless DNS providers. Those are providers in a shoot’s specification (.spec.dns.providers) which don’t serve the shoot’s domain (.spec.dns.domain), but are created by Gardener in the seed cluster to serve DNS requests coming from the shoot cluster. If such providers don’t specify a type or secretName the creation or update request for the corresponding shoot is denied.\nSeed Taints (gardener/gardener#2955) In an earlier release, we reserved a dedicated section in seed.spec.settings as a replacement for disable-capacity-reservation, disable-dns, invisible taints. These already deprecated taints were still considered and synced, which gave operators enough time to switch their integration to the new settings field. As of version v1.12, support for them has been discontinued and they are automatically removed from seed objects. You may use the actual taint names in a future release of Gardener again.\nLoad Balancer Events During Shoot Reconciliation (gardener/gardener#3028) As Gardener is capable of managing thousands of clusters, it is crucial to keep operation efforts at a minimum. This release demonstrates this endeavor by further improving error reporting to the end user. During a shoot’s reconciliation, Gardener creates Services of type LoadBalancer in the shoot cluster, e.g. for VPN or Nginx-Ingress addon, and waits for a successful creation. However, in the past we experienced that occurring issues caused by the party creating the load balancer (typically Cloud-Controller-Manager) are only exposed in the logs or as events. Gardener now fetches these event messages and propagates them to the shoot status in case of a failure. Users can then often fix the problem themselves, if for example the failure discloses an exhausted quota on the cloud provider.\nKonnectivityTunnel Feature per Shoot(gardener/gardener#3007) Since release v1.6 Gardener has been capable of reversing the tunnel direction from the seed to the shoot via the KonnectivityTunnel feature gate (more information). With this release we make it possible to control the feature per shoot. We recommend to selectively enable the KonnectivityTunnel, as it is still in alpha state.\nReference Protection (gardener/gardener#2771, gardener/gardener 1708419) Shoot clusters may refer to external objects, like Secrets for specified DNS providers or they have a reference to an audit policy ConfigMap. Deleting those objects while any shoot still references them causes sever errors, often only recoverable by an immense amount of manual operations effort. To prevent such scenarios, Gardener now adds a new finalizer gardener.cloud/reference-protection to these objects and removes it as soon as the object itself becomes releasable. Due to compatibility reasons, we decided that the handling for audit policy ConfigMaps is delivered as an opt-in feature first, so please familiarize yourself with the necessary settings in the Gardener Controller Manager component config if you already plan to enable it.\nSupport for Resource Quotas (gardener/gardener#2627) After the Kubernetes upstream change (kubernetes/kubernetes#93537) for externalizing the backing admission plugin has been accepted, we are happy to announce the support of ResourceQuotas for Gardener offered resource kinds. ResourceQuotas allow you to specify a maximum number of objects per namespace, especially for end-user objects like Shoots or SecretBindings in a project namespace. Even though the admission plugin is enabled by default in the Gardener API Server, make sure the Kube Controller Manager runs the resourcequota controller as well.\nWatch Out Developers, Terraformer v2 is Coming! (gardener/gardener#3034) Although not only related to Gardener core, but still an important milestone to mention, is the preparation towards Terraformer v2 in the extensions library. With Terraformer v2, Gardener extensions using Terraform scripts will benefit from great consistency improvements. Please check out #3034) which demonstrates necessary steps to transition to Terraformer v2 as soon as it’s been released.\nNotable Changes in v1.11 The Gardener community worked eagerly to deliver plenty of improvements with version v1.11. Those help us to further progress with topics like control plane migration, which is actively being worked on, or to harden our load balancer consolidation (APIServerSNI) feature. Besides improvements and fixes (full list available in release notes), this release as well contains major features and we don’t want to miss a chance to walk you through them.\nGardener Admission Controller (gardener/gardener#2832), (gardener/gardener#2781) In this release, all admission related HTTP handlers moved from the Gardener Controller Manager (GCM) to the new component Gardener Admission Controller. The admission controller is rather a small component as opposed to GCM with regards to memory footprint and CPU consumption, and thus allows you to run multiple replicas of it much cheaper than it was before. We certainly recommend specifying the admission controller deployment with more than one replica, since it reduces the odds of a system-wide outage and increases the performance of your Gardener service.\nBesides the already known Namespace and Kubeconfig Secret validation, a new admission handler Resource-Size-Validator was added to the admission controller. It allows operators to restrict the size for all kinds of Kubernetes objects, especially sent by end-users to the Kubernetes or Gardener API Server. We address a security concern with this feature to prevent denial of service attacks in which an attacker artificially increases the size of objects to exhaust your object store, API server caches, or to let Gardener and Kubernetes controllers run out-of-memory. The documentation reveals an approach of finding the right resource size for your setup and why you should create exceptions for technical users and operators.\nDeferring Shoot Progress Reporting (gardener/gardener#2909), Shoot progress reporting is the continuous update process of a shoot’s .status.lastOperation field while the shoot is being reconciled by Gardener. Many steps are involved during reconciliation and depending on the size of your setup, the updates might become an issue for the Gardener API Server which will refrain to process further requests for a certain period. With .controllers.shoot.progressReportPeriod in Gardenlet’s component configuration, you can now delay these updates for the specified period.\nNew Policy for Controller Registrations (gardener/gardener#2896), A while ago, we added support for different policies in ControllerRegistrations which determine under which circumstances the deployments of registration controllers happen in affected seed clusters. If you specify the new policy AlwaysExceptNoShoots, the respective extension controller will be deployed to all seed cluster hosting at least one shoot cluster. After all shoot clusters from a seed are gone, the extension deployment will be deleted again. A full list of supported policies can be found here.\n","categories":"","description":"","excerpt":"Two months after our last Gardener release update, we are happy again …","ref":"/blog/2020/11.04-gardener-v1.11-and-v1.12-released/","tags":"","title":"Gardener v1.11 and v1.12 Released"},{"body":"The Gardener team is happy to announce that Gardener now offers support for an additional, often requested, infrastructure/virtualization technology, namely KubeVirt! Gardener can now provide Kubernetes-conformant clusters using KubeVirt managed Virtual Machines in the environment of your choice. This integration has been tested and works with any qualified Kubernetes (provider) cluster that is compatibly configured to host the required KubeVirt components, in particular for example Red Hat OpenShift Virtualization.\nGardener enables Kubernetes consumers to centralize and operate efficiently homogenous Kubernetes clusters across different IaaS providers and even private environments. This way the same cloud-based application version can be hosted and operated by its vendor or consumer on a variety of infrastructures. When a new customer or your development team demands for a new infrastructure provider, Gardener helps you to quickly and easily on-board your workload. Furthermore, on this new infrastructure, Gardener keeps the seamless Kubernetes management experience for your Kubernetes operators, while upholding the consistency of the CI/CD pipeline of your software development team.\nArchitecture and Workflow Gardener is based on the idea of three types of clusters – Garden cluster, Seed cluster and Shoot cluster (see Figure 1). The Garden cluster is used to control the entire Kubernetes environment centrally in a highly scalable design. The highly available seed clusters are used to host the end users (shoot) clusters’ control planes. Finally, the shoot clusters consist only of worker nodes to host the cloud native applications.\nFigure 1: Gardener Architecture An integration of the Gardener open source project with a new cloud provider follows a standard Gardener extensibility approach. The integration requires two new components: a provider extension and a Machine Controller Manager (MCM) extension. Both components together enable Gardener to instruct the new cloud provider. They run in the Gardener seed clusters that host the control planes of the shoots based on that cloud provider. The role of the provider extension is to manage the provider-specific aspects of the shoot clusters’ lifecycle, including infrastructure, control plane, worker nodes, and others. It works in cooperation with the MCM extension, which in particular is responsible to handle machines that are provisioned as worker nodes for the shoot clusters. To get this job done, the MCM extension leverages the VM management/API capabilities available with the respective cloud provider.\nSetting up a Kubernetes cluster always involves a flow of interdependent steps (see Figure 2), beginning with the generation of certificates and preparation of the infrastructure, continuing with the provisioning of the control plane and the worker nodes, and ending with the deployment of system components. Gardener can be configured to utilize the KubeVirt extensions in its generic workflow at the right extension points, and deliver the desired outcome of a KubeVirt backed cluster.\nFigure 2: Generic cluster reconciliation flow with extension points Gardener Integration with KubeVirt in Detail Integration with KubeVirt follows the Gardener extensibility concept and introduces the two new components mentioned above: the KubeVirt Provider Extension and the KubeVirt Machine Controller Manager (MCM) Extension.\nFigure 3: Gardener integration with KubeVirt The KubeVirt Provider Extension consists of three separate controllers that handle respectively the infrastructure, the control plane, and the worker nodes of the shoot cluster.\nThe Infrastructure Controller configures the network communication between the shoot worker nodes. By default, shoot worker nodes only use the provider cluster’s pod network. To achieve higher level of network isolation and better performance, it is possible to add more networks and replace the default pod network with a different network using container network interface (CNI) plugins available in the provider cluster. This is currently based on Multus CNI and NetworkAttachmentDefinitions.\nExample infrastructure configuration in a shoot definition:\nprovider:  type: kubevirt  infrastructureConfig:  apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  tenantNetworks:  - name: network-1  config: |{ \"cniVersion\": \"0.4.0\", \"name\": \"bridge-firewall\", \"plugins\": [ { \"type\": \"bridge\", \"isGateway\": true, \"isDefaultGateway\": true, \"ipMasq\": true, \"ipam\": { \"type\": \"host-local\", \"subnet\": \"10.100.0.0/16\" } }, { \"type\": \"firewall\" } ] }  default: true The Control Plane Controller deploys a Cloud Controller Manager (CCM). This is a Kubernetes control plane component that embeds cloud-specific control logic. As any other CCM, it runs the Node controller that is responsible for initializing Node objects, annotating and labeling them with cloud-specific information, obtaining the node’s hostname and IP addresses, and verifying the node’s health. It also runs the Service controller that is responsible for setting up load balancers and other infrastructure components for Service resources that require them.\nFinally, the Worker Controller is responsible for managing the worker nodes of the Gardener shoot clusters.\nExample worker configuration in a shoot definition:\nprovider:  type: kubevirt  workers:  - name: cpu-worker  minimum: 1  maximum: 2  machine:  type: standard-1  image:  name: ubuntu  version: \"18.04\"  volume:  type: default  size: 20Gi  zones:  - europe-west1-c For more information about configuring the KubeVirt Provider Extension as an end-user, see Using the KubeVirt provider extension with Gardener as end-user.\nEnabling Your Gardener Setup to Leverage a KubeVirt Compatible Environment The very first step required is to define the machine types (VM types) for VMs that will be available. This is achieved via the CloudProfile custom resource. The machine types configuration includes details such as CPU, GPU, memory, OS image, and more.\nExample CloudProfile custom resource:\napiVersion: core.gardener.cloud/v1beta1 kind: CloudProfile metadata:  name: kubevirt spec:  type: kubevirt  providerConfig:  apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1  kind: CloudProfileConfig  machineImages:  - name: ubuntu  versions:  - version: \"18.04\"  sourceURL: \"https://cloud-images.ubuntu.com/bionic/current/bionic-server-cloudimg-amd64.img\"  kubernetes:  versions:  - version: \"1.18.5\"  machineImages:  - name: ubuntu  versions:  - version: \"18.04\"  machineTypes:  - name: standard-1  cpu: \"1\"  gpu: \"0\"  memory: 4Gi  volumeTypes:  - name: default  class: default  regions:  - name: europe-west1  zones:  - name: europe-west1-b  - name: europe-west1-c  - name: europe-west1-d Once a machine type is defined, it can be referenced in shoot definitions. This information is used by the KubeVirt Provider Extension to generate MachineDeployment and MachineClass custom resources required by the KubeVirt MCM extension for managing the worker nodes of the shoot clusters during the reconciliation process.\nFor more information about configuring the KubeVirt Provider Extension as an operator, see Using the KubeVirt provider extension with Gardener as operator.\nKubeVirt Machine Controller Manager (MCM) Extension The KubeVirt MCM Extension is responsible for managing the VMs that are used as worker nodes of the Gardener shoot clusters using the virtualization capabilities of KubeVirt. This extension handles all necessary lifecycle management activities, such as machines creation, fetching, updating, listing, and deletion.\nThe KubeVirt MCM Extension implements the Gardener’s common driver interface for managing VMs in different cloud providers. As already mentioned, the KubeVirt MCM Extension is using the MachineDeployments and MachineClasses – an abstraction layer that follows the Kubernetes native declarative approach - to get instructions from the KubeVirt Provider Extension about the required machines for the shoot worker nodes. Also, the cluster austoscaler integrates with the scale subresource of the MachineDeployment resource. This way, Gardener offers a homogeneous autoscaling experience across all supported providers.\nWhen a new shoot cluster is created or when a new worker node is needed for an existing shoot cluster, a new Machine will be created, and at that time, the KubeVirt MCM extension will create a new KubeVirt VirtualMachine in the provider cluster. This VirtualMachine will be created based on a set of configurations in the MachineClass that follows the specification of the KubeVirt provider.\nThe KubeVirt MCM Extension has two main components. The MachinePlugin is responsible for handling the machine objects, and the PluginSPI is in charge of making calls to the cloud provider interface, to manage its resources.\nFigure 4: KubeVirt MCM extension workflow and architecture As shown in Figure 4, the MachinePlugin receives a machine request from the MCM and starts its processing by decoding the request, doing partial validation, extracting the relevant information, and sending it to the PluginSPI.\nThe PluginSPI then creates, gets, or deletes VirtualMachines depending on the method called by the MachinePlugin. It extracts the kubeconfig of the provider cluster and handles all other required KubeVirt resources such as the secret that holds the cloud-init configurations, and DataVolumes that are mounted as disks to the VMs.\nSupported Environments The Gardener KubeVirt support is currently qualified on:\n KubeVirt v0.32.0 (and later) Red Hat OpenShift Container Platform 4.4 (and later)  There are also plans for further improvements and new features, for example integration with CSI drivers for storage management. Details about the implementation progress can be found in the Gardener project on GitHub.\nYou can find further resources about the open source project Gardener at https://gardener.cloud.\n","categories":"","description":"","excerpt":"The Gardener team is happy to announce that Gardener now offers …","ref":"/blog/2020/10.19-gardener-integrates-with-kubevirt/","tags":"","title":"Gardener Integrates with KubeVirt"},{"body":"Do you want to understand how Gardener creates and updates Kubernetes clusters (Shoots)? Well, it’s complicated, but if you are not afraid of large diagrams and are a visual learner like me, this might be useful to you.\nIntroduction In this blog post I will share a technical diagram which attempts to tie together the various components involved when Gardener creates a Kubernetes cluster. I have created and curated the diagram, which visualizes the Shoot reconciliation flow since I started developing on Gardener. Aside from serving as a memory aid for myself, I created it in hopes that it may potentially help contributors to understand a core piece of the complex Gardener machinery. Please be advised that the diagram and components involved are large. Although it can be easily divided into multiple diagrams, I want to show all the components and connections in a single diagram to create an overview of the reconciliation flow.\nThe goal is to visualize the interactions of the components involved in the Shoot creation. It is not intended to serve as a documentation of every component involved.\nBackground Taking a step back, the Gardener READ.me states\n In essence, Gardener is an extension API server that comes along with a bundle of custom controllers. It introduces new API objects in an existing Kubernetes cluster (which is called garden cluster) in order to use them for the management of end-user Kubernetes clusters (which are called shoot clusters). These shoot clusters are described via declarative cluster specifications which are observed by the controllers. They will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.\n This means that Gardener, just like any Kubernetes controller, creates Kubernetes clusters (Shoots) using a reconciliation loop.\nThe Gardenlet contains the controller and reconciliation loop responsible for the creation, update, deletion and migration of Shoot cluster (there are more, but we spare them in this article). In addition, the Gardener Controller Manager also reconciles Shoot resources, but only for seed-independent functionality such as Shoot hibernation, Shoot maintenance or quota control.\nThis blog post is about the reconciliation loop in the Gardenlet responsible for creating and updating Shoot clusters. The code can be found here. The reconciliation loops of the extension controllers can be found in their individual repositories.\nShoot Reconciliation Flow Diagram When Gardner creates a Shoot cluster, there are three conceptual layers involved: the Garden cluster, the Seed cluster and the Shoot cluster. Each layer represents a top-level section in the diagram (similar to a lane in a BPMN diagram).\nIt might seem confusing, that the Shoot cluster itself is a layer, because the whole flow in the first place is about creating the Shoot cluster. I decided to introduce this separate layer to make a clear distinction between which resources exist in the Seed API server (managed by Gardener) and which in the Shoot API server (accessible by the Shoot owner).\nEach section contains several components. Components are mostly Kubernetes resources in a Gardener installation (e.g. the gardenlet deployment in the Seed cluster).\nThis is the list of components:\n(Virtual) Garden Cluster\n Gardener Extension API server Validating Provider Webhooks Project Namespace  Seed Cluster\n Gardenlet Seed API server  every Shoot Control Plane has a dedicated namespace in the Seed.   Cloud Provider (owned by Stakeholder).  Arguably part of the Shoot cluster but used by components in the Seed cluster to create the infrastructure for the Shoot.   Gardener DNS extension Provider Extension (such as gardener-extension-provider-aws) Gardener Extension ETCD Druid Gardener Resource Manager Operating System Extension (such as gardener-extension-os-gardenlinux) Networking extension (such as gardener-extension-networking-cilium) Machine Controller Manager ContainerRuntime Extension (such as gardener-extension-runtime-gvisor) Shoot API server (in the Shoot Namespace in the Seed cluster)  Shoot Cluster\n Cloud Provider compute API (owned by Stakeholder) - for VM/Node creation. VM / Bare metal node hosted by Cloud Provider (in Stakeholder owned account).  How to Use the Diagram The diagram\n should be read from top to bottom - starting in the top left corner with the creation of the Shoot resource via the Gardener Extension API server. should not require an encompassing documentation / description. More detailed documentation on the components itself, can usually be found in the respective repository. does not show which activities execute in parallel (many) and also does not describe the exact dependencies between the steps. This can be found out by looking at the source code. It however tries to put the activities in a logical order of executing during the reconciliation flow.  Occasionally, there is an info box with additional information next to parts in the diagram that in my point of view require further explanation. Large example resource for the Gardener CRDs (e.g Worker CRD, Infrastructure CRD) are placed on the left side and are referenced by a dotted line (—–).\nBe aware, that Gardener is an evolving project, so the diagram will most likely be already outdated by the time you are reading this. Nevertheless, it should give a solid starting point for further explorations into the details of Gardener.\nFlow Diagram The diagram can be found below and on Github.com. There are multiple formats available (svg, vsdx, draw.io, html).\nPlease open an issue or open a PR in the repository if information is missing or is incorrect. Thanks!\n\n","categories":"","description":"","excerpt":"Do you want to understand how Gardener creates and updates Kubernetes …","ref":"/blog/2020/10.19-shoot-reconciliation-details/","tags":"","title":"Shoot Reconciliation Details"},{"body":"Summer holidays aren’t over yet, still, the Gardener community was able to release two new minor versions in the past weeks. Despite being limited in capacity these days, we were able to reach some major milestones, like adding Kubernetes v1.19 support and the long-delayed automated gardenlet certificate rotation. Whilst we continue to work on topics related to scalability, robustness, and better observability, we agreed to adjust our focus a little more into the areas of development productivity, code quality and unit/integration testing for the upcoming releases.\nNotable Changes in v1.10 Gardener v1.10 was a comparatively small release (measured by the number of changes) but it comes with some major features!\nKubernetes 1.19 Support (gardener/gardener#2799) The newest minor release of Kubernetes is now supported by Gardener (and all the maintained provider extensions)! Predominantly, we have enabled CSI migration for OpenStack now that it got promoted to beta, i.e. 1.19 shoots will no longer use the in-tree Cinder volume provisioner. The CSI migration enablement for Azure got postponed (to at least 1.20) due to some issues that the Kubernetes community is trying to fix in the 1.20 release cycle. As usual, the 1.19 release notes should be considered before upgrading your shoot clusters.\nAutomated Certificate Rotation for gardenlet (gardener/gardener#2542) Similar to the kubelet, the gardenlet supports TLS bootstrapping when deployed into a new seed cluster. It will request a client certificate for the garden cluster using the CertificateSigningRequest API of Kubernetes and store the generated results in a Secret object in the garden namespace of its seed. These certificates are usually valid for one year. We have now added support for automatic renewals if the expiration dates are approaching.\nImproved Monitoring Alerts (gardener/gardener#2776) We have worked on a larger refactoring to improve reliability and accuracy of our monitoring alerts for both shoot control planes in the seed as well as shoot system components running on worker nodes. The improvements are primarily for operators and should result in less false positive alerts. Also, the alerts should fire less frequently and are better grouped in order to reduce to overall amount of alerts.\nSeed Deletion Protection (gardener/gardener#2732) Our validation to improve robustness and countermeasures against accidental mistakes has been improved. Earlier, it was possible to remove the use-as-seed annotation for shooted seeds or directly set the deletionTimestamp on Seed objects, despite of the fact that they might still run shoot control planes. Seed deletion would not start in these cases, although, it would disrupt the system unnecessarily, and result in some unexpected behaviour. The Gardener API server is now forbidding such requests if the seeds are not completely empty yet.\nLogging Improvements for Loki (multiple PRs) After we released our large logging stack refactoring (from EFK to Loki) with Gardener v1.8, we have continued to work on reliability, quality and user feedback in general. We aren’t done yet, though, Gardener v1.10 includes a bunch of improvements which will help to graduate the Logging feature gate to beta and GA, eventually.\nNotable Changes in v1.9 The v1.9 release contained tons of small improvements and adjustments in various areas of the code base and a little less new major features. However, we don’t want to miss the opportunity to highlight a few of them.\nCRI Validation in CloudProfiles (gardener/gardener#2137) A couple of releases back we have introduced support for containerd and the ContainerRuntime extension API. The supported container runtimes are operating system specific, and until now it wasn’t possible for end-users to easily figure out whether they can enable containerd or other ContainerRuntime extensions for their shoots. With this change, Gardener administrators/operators can now provide that information in the .spec.machineImages section in the CloudProfile resource. This also allows for enhanced validation and prevents misconfigurations.\nNew Shoot Event Controller (gardener/gardener#2649) The shoot controllers in both the gardener-controller-manager and gardenlet fire several Events for some important operations (e.g., automated hibernation/wake-up due to hibernation schedule, automated Kubernetes/machine image version update during maintenance, etc.). Earlier, the only way to prolong the lifetime of these events was to modify the --event-ttl command line parameter of the garden cluster’s kube-apiserver. This came with the disadvantage that all events were kept for a longer time (not only those related to Shoots that an operator is usually interested in and ideally wants to store for a couple of days). The new shoot event controller allows to achieve this by deleting non-shoot events. This helps operators and end-users to better understand which changes were applied to their shoots by Gardener.\nEarly Deployment of the Logging Stack for New Shoots (gardener/gardener#2750) Since the first introduction of the Logging feature gate two years back the logging stack was only deployed at the very end of the shoot creation. This had the disadvantage that control plane pod logs were not kept in case the shoot creation flow is interrupted before the logging stack could be deployed. In some situations, this was preventing fetching relevant information about why a certain control plane component crashed. We now deploy the logging stack very early in the shoot creation flow to always have access to such information.\n","categories":"","description":"","excerpt":"Summer holidays aren’t over yet, still, the Gardener community was …","ref":"/blog/2020/09.11-gardener-v1.9-and-v1.10-released/","tags":"","title":"Gardener v1.9 and v1.10 Released"},{"body":"Even if we are in the midst of the summer holidays, a new Gardener release came out yesterday: v1.8.0! It’s main themes are the large change of our logging stack to Loki (which was already explained in detail on a blog post on grafana.com), more configuration options to optimize the utilization of a shoot, node-local DNS, new project roles, and significant improvements for the Kubernetes client that Gardener uses to interact with the many different clusters.\nNotable Changes Logging 2.0: EFK Stack Replaced by Loki (gardener/gardener#2515) Since two years or so Gardener could optionally provision a dedicated logging stack per seed and per shoot which was based on fluent-bit, fluentd, ElasticSearch and Kibana. This feature was still hidden behind an alpha-level feature gate and never got promoted to beta so far. Due to various limitations of this solution we decided to replace the EFK stack with Loki. As we already have Prometheus and Grafana deployments for both users and operators by default for all clusters the choice was just natural. Please find out more on this topic at this dedicated blog post.\nCluster Identities and DNSOwner Objects (gardener/gardener#2471, gardener/gardener#2576) The shoot control plane migration topic is ongoing since a few months already, and we are very much progressing with it. A first alpha version will probably make it out soon. As part of these endeavors, we introduced cluster identities and the usage of DNSOwner objects in this release. Both are needed to gracefully migrate the DNSEntry extension objects from the old seed to the new seed as part of the control plane migration process. Please find out more on this topic at this blog post.\nNew uam Role for Project Members to Limit User Access Management Privileges (gardener/gardener#2611) In order to allow external user access management system to integrate with Gardener and to fulfil certain compliance aspects, we have introduced a new role called uam for Project members (next to admin and viewer). Only if a user has this role then he/she is allowed to add/remove other human users to the respective Project. By default, all newly created Projects assign this role only to the owner while, for backwards-compatibility reasons, it will be assigned for all members for existing projects. Project owners can steadily revoke this access as desired. Interestingly, the uam role is backed by a custom RBAC verb called manage-members, i.e., the Gardener API server is only admitting changes to the human Project members if the respective user is bound to this RBAC verb.\nNew Node-Local DNS Feature for Shoots (gardener/gardener#2528) By default, we are using CoreDNS as DNS plugin in shoot clusters which we auto-scale horizontally using HPA. However, in some situations we are discovering certain bottlenecks with it, e.g., unreliable UDP connections, unnecessary node hopping, inefficient load balancing, etc. To further optimize the DNS performance for shoot clusters, it is now possible to enable a new alpha-level feature gate in the gardenlet’s componentconfig: NodeLocalDNS. If enabled, all shoots will get a new DaemonSet to run a DNS server on each node.\nMore kubelet and API Server Configurability (gardener/gardener#2574, gardener/gardener#2668) One large benefit of Gardener is that it allows you to optimize the usage of your control plane as well as worker nodes by exposing relevant configuration parameters in the Shoot API. In this version, we are adding support to configure kubelet’s values for systemReserved and kubeReserved resources as well as the kube-apiserver’s watch cache sizes. This allows end-users to get to better node utilization and/or performance for their shoot clusters.\nConfigurable Timeout Settings for machine-controller-manager (gardener/gardener#2563) One very central component in Project Gardener is the machine-controller-manager for managing the worker nodes of shoot clusters. It has extensive qualities with respect to node lifecycle management and rolling updates. As such, it uses certain timeout values, e.g. when creating or draining nodes, or when checking their health. Earlier, those were not customizable by end-users, but we are adding this possibility now. You can fine-grain these settings per worker pool in the Shoot API such that you can optimize the lifecycle management of your worker nodes even more!\nImproved Usage of Cached Client to Reduce Network I/O (gardener/gardener#2635, gardener/gardener#2637) In the last Gardener release v1.7 we have introduced a huge refactoring the clients that we use to interact with the many different Kubernetes clusters. This is to further optimize the network I/O performed by leveraging watches and caches as good as possible. It’s still an alpha-level feature that must be explicitly enabled in the Gardenlet’s component configuration, though, with this release we have improved certain things in order to pave the way for beta promotion. For example, we were initially also using a cached client when interacting with shoots. However, as the gardenlet runs in the seed as well (and thus can communicate cluster-internally with the kube-apiservers of the respective shoots) this cache is not necessary and just memory overhead. We have removed it again and saw the memory usage getting lower again. More to come!\nAWS EBS Volume Encryption by Default (gardener/gardener-extension-provider-aws#147) The Shoot API already exposed the possibility to encrypt the root disks of worker nodes since quite a while, but it was disabled by default (for backwards-compatibility reasons). With this release we have change this default, so new shoot worker nodes will be provisioned with encrypted root disks out-of-the-box. However, the g4dn instance types of AWS don’t support this encryption, so when you use them you have to explicitly disable the encryption in the worker pool configuration.\nLiveness Probe for Gardener API Server Deployment (gardener/gardener#2647) A small, but very valuable improvement is the introduction of a liveness probe for our Gardener API server. As it’s built with the same library like the Kubernetes API server, it exposes two endpoints at /livez and /readyz which were created exactly for the purpose of live- and readiness probes. With Gardener v1.8 the Helm chart contains a liveness probe configuration by default, and we are awaiting an upstream fix (kubernetes/kubernetes#93599) to also enable the readiness probe. This will help in a smoother rolling update of the Gardener API server pods, i.e., preventing clients from talking to a not yet initialized or already terminating API server instance.\nWebhook Ports Changed to Enable OpenShift (gardener/gardener#2660) In order to make it possible to run Gardener on OpenShift clusters as well, we had to make a change in the port configuration for the webhooks we are using in both Gardener and the extension controllers. Earlier, all the webhook servers directly exposed port 443, i.e., a system port which is a security concern and disallowed in OpenShift. We have changed this port now across all places and also adapted our network policies accordingly. This is most likely not the last necessary change to enable this scenario, however, it’s a great improvement to push the project forward.\nIf you’re interested in more details and even more improvements you can find all release notes for Gardener v1.8.0 here: https://github.com/gardener/gardener/releases/tag/v1.8.0\n","categories":"","description":"","excerpt":"Even if we are in the midst of the summer holidays, a new Gardener …","ref":"/blog/2020/08.06-gardener-v1.8.0-released/","tags":"","title":"Gardener v1.8.0 Released"},{"body":"Gardener is showing successful collaboration with its growing community of contributors and adopters. With this come some success stories, including PingCAP using Gardener to implement its managed service.\nAbout PingCAP and Its TiDB Cloud PingCAP started in 2015, when three seasoned infrastructure engineers working at leading Internet companies got sick and tired of the way databases were managed, scaled and maintained. Seeing no good solution on the market, they decided to build their own - the open-source way. With the help of a first-class team and hundreds of contributors from around the globe, PingCAP is building a distributed NewSQL, hybrid transactional and analytical processing (HTAP) database.\nIts flagship project, TiDB, is a cloud-native distributed SQL database with MySQL compatibility, and one of the most popular open-source database projects - with 23.5K+ stars and 400+ contributors. Its sister project TiKV is a Cloud Native Interactive Landscape project.\nPingCAP envisioned their managed TiDB service, known as TiDB Cloud, to be multi-tenant, secure, cost-efficient, and to be compatible with different cloud providers. As a result, the company turned to Gardener to build their managed TiDB cloud service offering.\nTiDB Cloud Beta Preview Limitations with Other Public Managed Kubernetes Services Previously, PingCAP encountered issues while using other public managed K8s cluster services, to develop the first version of its TiDB Cloud. Their worst pain point was that they felt helpless when encountering certain malfunctions. PingCAP wasn’t able to do much to resolve these issues, except waiting for the providers’ help. More specifically, they experienced problems due to cloud-provider specific Kubernetes system upgrades, delays in the support response (which could be avoided in exchange of a costly support fee), and no control over when things got fixed.\nThere was also a lot of cloud-specific integration work needed to follow a multi-cloud strategy, which proved to be expensive both to produce and maintain. With one of these managed K8s services, you would have to integrate the instance API, as opposed to a solution like Gardener, which provides a unified API for all clouds. Such a unified API eliminates the need to worry about cloud specific-integration work altogether.\nWhy PingCAP Chose Gardener to Build TiDB Cloud  “Gardener has similar concepts to Kubernetes. Each Kubernetes cluster is just like a Kubernetes pod, so the similar concepts apply, and the controller pattern makes Gardener easy to manage. It was also easy to extend, as the team was already very familiar with Kubernetes, so it wasn’t hard for us to extend Gardener. We also saw that Gardener has a very active community, which is always a plus!”- Aylei Wu, (Cloud Engineer) at PingCAP\n At first glance, PingCAP had initial reservations about using Gardener - mainly due to its adoption level (still at the beginning) and an apparent complexity of use. However, these were soon eliminated as they learned more about the solution. As Aylei Wu mentioned during the last Gardener community meeting, “a good product speaks for itself”, and once the company got familiar with Gardener, they quickly noticed that the concepts were very similar to Kubernetes, which they were already familiar with.\nThey recognized that Gardener would be their best option, as it is highly extensible and provides a unified abstraction API layer. In essence, the machines can be managed via a machine controller manager for different cloud providers - without having to worry about the individual cloud APIs.\nThey agreed that Gardener’s solution, although complex, was definitely worth it. Even though it is a relatively new solution, meaning they didn’t have access to other user testimonials, they decided to go with the service since it checked all the boxes (and as SAP was running it productively with a huge fleet). PingCAP also came to the conclusion that building a managed Kubernetes service themselves would not be easy. Even if they were to build a managed K8s service, they would have to heavily invest in development and would still end up with an even more complex platform than Gardener’s. For all these reasons combined, PingCAP decided to go with Gardener to build its TiDB Cloud.\nHere are certain features of Gardener that PingCAP found appealing:\n Cloud agnostic: Gardener’s abstractions for cloud-specific integrations dramatically reduce the investment in supporting more than one cloud infrastructure. Once the integration with Amazon Web Services was done, moving on to Google Cloud Platform proved to be relatively easy. (At the moment, TiDB Cloud has subscription plans available for both GCP and AWS, and they are planning to support Alibaba Cloud in the future.) Familiar concepts: Gardener is K8s native; its concepts are easily related to core Kubernetes concepts. As such, it was easy to onboard for a K8s experienced team like PingCAP’s SRE team. Easy to manage and extend: Gardener’s API and extensibility are easy to implement, which has a positive impact on the implementation, maintenance costs and time-to-market. Active community: Prompt and quality responses on Slack from the Gardener team tremendously helped to quickly onboard and produce an efficient solution.  How PingCAP Built TiDB Cloud with Gardener On a technical level, PingCAP’s set-up overview includes the following:\n A Base Cluster globally, which is the top-level control plane of TiDB Cloud A Seed Cluster per cloud provider per region, which makes up the fundamental data plane of TiDB Cloud A Shoot Cluster is dynamically provisioned per tenant per cloud provider per region when requested A tenant may create one or more TiDB clusters in a Shoot Cluster  As a real world example, PingCAP sets up the Base Cluster and Seed Clusters in advance. When a tenant creates its first TiDB cluster under the us-west-2 region of AWS, a Shoot Cluster will be dynamically provisioned in this region, and will host all the TiDB clusters of this tenant under us-west-2. Nevertheless, if another tenant requests a TiDB cluster in the same region, a new Shoot Cluster will be provisioned. Since different Shoot Clusters are located in different VPCs and can even be hosted under different AWS accounts, TiDB Cloud is able to achieve hard isolation between tenants and meet the critical security requirements for our customers.\nTo automate these processes, PingCAP creates a service in the Base Cluster, known as the TiDB Cloud “Central” service. The Central is responsible for managing shoots and the TiDB clusters in the Shoot Clusters. As shown in the following diagram, user operations go to the Central, being authenticated, authorized, validated, stored and then applied asynchronously in a controller manner. The Central will talk to the Gardener API Server to create and scale Shoot clusters. The Central will also access the Shoot API Service to deploy and reconcile components in the Shoot cluster, including control components (TiDB Operator, API Proxy, Usage Reporter for billing, etc.) and the TiDB clusters.\nTiDB Cloud on Gardener Architecture Overview What’s Next for PingCAP and Gardener With the initial success of using the project to build TiDB Cloud, PingCAP is now working heavily on the stability and day-to-day operations of TiDB Cloud on Gardener. This includes writing Infrastructure-as-Code scripts/controllers with it to achieve GitOps, building tools to help diagnose problems across regions and clusters, as well as running chaos tests to identify and eliminate potential risks. After benefiting greatly from the community, PingCAP will continue to contribute back to Gardener.\nIn the future, PingCAP also plans to support more cloud providers like AliCloud and Azure. Moreover, PingCAP may explore the opportunity of running TiDB Cloud in on-premise data centers with the constantly expanding support this project provides. Engineers at PingCAP enjoy the ease of learning from Gardener’s kubernetes-like concepts and being able to apply them everywhere. Gone are the days of heavy integrations with different clouds and worrying about vendor stability. With this project, PingCAP now sees broader opportunities to land TiDB Cloud on various infrastructures to meet the needs of their global user group.\nStay tuned, more blog posts to come on how Gardener is collaborating with its contributors and adopters to bring fully-managed clusters at scale everywhere! If you want to join in on the fun, connect with our community.\n","categories":"","description":"","excerpt":"Gardener is showing successful collaboration with its growing …","ref":"/blog/2020/05.27-pingcaps-experience/","tags":"","title":"PingCAP’s Experience in Implementing Their Managed TiDB Service with Gardener"},{"body":"The Gardener project website just received a serious facelift. Here are some of the highlights:\n A completely new landing page, emphasizing both on Gardener’s value proposition and the open community behind it. The Community page was reconstructed for quick access to the various community channels and will soon merge the Adopters page. It will provide a better insight into success stories from the communty. Improved blogs layout. One-click sharing options are available starting with simple URL copy link and twitter button and others will closely follow up. While we are at it, give it a try. Spread the word.  Website builds also got to a new level with:\n Containerization. The whole build environment is containerized now, eliminating differences between local and CI/CD setup and reducing content developers focus only to the /documentation repository. Running a local server for live preview of changes as you make them when developing content for the website, is now as easy as runing make serve in your local /documentation clone. Numerous improvements to the buld scripts. More configuration options, authenticated requests, fault tollerance and performance. Good news for Windows WSL users who will now nejoy a significantly support. See the updated README for details on that. A number of improvements in layouts styles, site assets and hugo site-building techniques.  But hey, THAT’S NOT ALL!\nStay tuned for more improvements around the corner. The biggest ones are aligning the documentation with the new theme and restructuring it along, more emphasis on community success stories all around, more sharing options and more than a handful of shortcodes for content development and … let’s cut the spoilers here.\nI hope you will like it. Let us know what you think about it. Feel free to leave comments and discuss on Twitter and Slack, or in case of issues - on GitHub.\nGo ahead and help us spread the word: https://gardener.cloud\n","categories":"","description":"","excerpt":"The Gardener project website just received a serious facelift. Here …","ref":"/blog/2020/05.11-new-website-same-green-flower/","tags":"","title":"New Website, Same Green Flower"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/2019/","tags":"","title":"2019"},{"body":"This tutorial describes how to overprovisioning of cluster nodes for scaling and failover. This is desired when you have work load that need to scale up quickly without waiting for the new cluster nodes to be created and join the cluster.\n![teaser](./images/teaser 2.png)\nA similar problem occurs when crashing a node from the Hyperscaler. This must be replaced by Kubernetes as fast as possible. The solution can be overprovisioning of nodes\n..read some more on Cluster Overprovisioning.\n","categories":"","description":"","excerpt":"This tutorial describes how to overprovisioning of cluster nodes for …","ref":"/blog/2019/06.11-cluster-overprovisioning/","tags":"","title":"Cluster Overprovisioning"},{"body":"Feature flags are used to change the behavior of a program at runtime without forcing a restart.\nAlthough they are essential in a native cloud environment, they cannot be implemented without significant effort on some platforms. Kubernetes has made this trivial. Here we will implement them through labels and annotations, but you can also implement them by connecting directly to the Kubernetes API Server.\n![teaser](./images/teaser 2.gif)\nPossible Use Cases\n turn on/off a specific instance turn on/off profiling of a specific instance change the logging level, to capture detailed logs during a specific event change caching strategy at runtime change timeouts in production toggle on/off some special verification  ","categories":"","description":"","excerpt":"Feature flags are used to change the behavior of a program at runtime …","ref":"/blog/2019/06.11-feature-flags-in-kubernetes-applications/","tags":"","title":"Feature Flags in Kubernetes Applications"},{"body":"Gardener has an excellent ability to automatically scale machines for the cluster. From the point of view of scalability, there is no need for manual intervention.\nThis tutorial is useful for those end-users who need specifically configured nodes, which are not yet supported by Gardener. For example: an end-user who wants some workload that requires runnc instead of runc as container runtime.\n..read some more on Adding Nodes to a Cluster.\n","categories":"","description":"","excerpt":"Gardener has an excellent ability to automatically scale machines for …","ref":"/blog/2019/06.11-manually-adding-a-node-to-an-existing-cluster/","tags":"","title":"Manually Adding a Node to an Existing Cluster"},{"body":"The kubectl command-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.\n![teaser](./images/teaser 1.svg)\n What happens if your kubeconfig file of your production cluster is leaked or published by accident?\n Since there is no possibility to rotate or revoke the initial kubeconfig, there is only one way to protect your infrastructure or application if it is has leaked - delete the cluster.\n..learn more on Work with kubeconfig files.\n","categories":"","description":"","excerpt":"The kubectl command-line tool uses kubeconfig files to find the …","ref":"/blog/2019/06.11-organizing-access-using-kubeconfig-files/","tags":"","title":"Organizing Access Using kubeconfig Files"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/2018/","tags":"","title":"2018"},{"body":"Green Tea Matcha Cookies For a team event during the Christmas season we decided to completely reinterpret the topic cookies. :-)\nMatcha cookies have the delicate flavor and color of green tea. These soft, pillowy and chewy green tea cookies are perfect with tea. And of course they fit perfectly to our logo.\nIngredients  1 stick butter, softened ⅞ cup of granulated sugar 1 cup + 2 tablespoons all-purpose flour 2 eggs 1¼ tablespoons culinary grade matcha powder 1 teaspoon baking powder pinch of salt  Instructions  Cream together the butter and sugar in a large mixing bowl - it should be creamy colored and airy. A hand blender or stand mixer works well for this. This helps the cookie become fluffy and chewy. Gently incorporate the eggs to the butter mixture one at a time. In a separate bowl, sift together all the dry ingredients. Add the dry ingredients to the wet by adding a little at a time and folding or gently mixing the batter together. Keep going until you’ve incorporated all the remaining flour mixture. The dough should be a beautiful green color. Chill the dough for at least an hour - up to overnight. The longer the better! Preheat your oven to 325 F. Roll the dough into balls the size of ping pong balls and place them on a non-stick cookie sheet. Bake them for 12-15 minutes until the bottoms just start to become golden brown and the cookie no longer looks wet in the middle. Note: you can always bake them at 350 F for a less moist, fluffy cookie. It will bake faster by about 2-4 minutes 350 F so watch them closely. Remove and let cool on a rack and enjoy!  Note Make sure you get culinary grade matcha powder. You should be able to find this in Asian or natural grocers.\n","categories":"","description":"","excerpt":"Green Tea Matcha Cookies For a team event during the Christmas season …","ref":"/blog/2018/12.25-gardener_cookies/","tags":"","title":"Gardener Cookies"},{"body":"…they mess up the figure.\nFor a team event during the Christmas season we decided to completely reinterpret the topic cookies… since the vegetables have gone on a well-deserved vacation. :-)\nGet recipe on Gardener Cookies.\n","categories":"","description":"","excerpt":"…they mess up the figure.\nFor a team event during the Christmas season …","ref":"/blog/2018/12.22-cookies-are-dangerous/","tags":"","title":"Cookies Are Dangerous..."},{"body":"You want to experiment with Kubernetes or have set up a customer scenario, but you don’t want to run the cluster 24 / 7 for reasons of cost?\n![](./images/teaser_patched 1.svg)\nThe Gardener gives you the possibility to scale your cluster down to zero nodes.\n..read some more on Hibernate a Cluster.\n","categories":"","description":"","excerpt":"You want to experiment with Kubernetes or have set up a customer …","ref":"/blog/2018/07.11-hibernate-a-cluster-to-save-money/","tags":"","title":"Hibernate a Cluster to Save Money"},{"body":"Running as Root User Whenever possible, do not run containers as root users. One could be tempted to say that Kubernetes Pods and Node are well separated. The host and the container share the same kernel. If the container is compromised, a root user can damage the underlying node. Use RUN groupadd -r anygroup \u0026\u0026 useradd -r -g anygroup myuser to create a group and a user in it. Use the USER command to switch to this user.\nStoring Data or Logs in Containers Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the container, as they are lost when the container is closed. If absolutely necessary, you can use persistence volumes instead to persist them outside the containers. However, an ELK stack is preferred for storing and processing log files.\n..read some more on Common Kubernetes Antipattern.\n","categories":"","description":"","excerpt":"Running as Root User Whenever possible, do not run containers as root …","ref":"/blog/2018/06.11-anti-patterns/","tags":"","title":"Anti Patterns"},{"body":"In summer 2018, the Gardener project team asked Kinvolk to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work is to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a Control-Plane-as-a-Service with a network air gap.\nAlong the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.\n..read some more on Auditing Kubernetes for Secure Setup.\n","categories":"","description":"","excerpt":"In summer 2018, the Gardener project team asked Kinvolk to execute …","ref":"/blog/2018/06.11-auditing-kubernetes-for-secure-setup/","tags":"","title":"Auditing Kubernetes for Secure Setup"},{"body":"Microservices tend to use smaller runtimes but you can use what you have today - and this can be a problem in kubernetes.\nSwitching your architecture from a monolith to microservices has many advantages, both in the way you write software and the way it is used throughout its lifecycle. In this post, my attempt is to cover one problem which does not get as much attention and discussion - size of the technology stack.\nGeneral Purpose Technology Stack There is a tendency to be more generalized in development and to apply this pattern to all services. One feels that a homogeneous image of the technology stack is good if it is the same for all services.\nOne forgets, however, that a large percentage of the integrated infrastructure is not used by all services in the same way, and is therefore only a burden. Thus, resources are wasted and the entire application becomes expensive in operation and scales very badly.\nLight Technology Stack Due to the lightweight nature of your service, you can run more containers on a physical server and virtual machines. The result is higher resource utilization.\nAdditionally, microservices are developed and deployed as containers independently of each another. This means that a development team can develop, optimize and deploy a microservice without impacting other subsystems.\n","categories":"","description":"","excerpt":"Microservices tend to use smaller runtimes but you can use what you …","ref":"/blog/2018/06.11-big-things-come-in-small-packages/","tags":"","title":"Big Things Come in Small Packages"},{"body":"For encrypted communication between the client to the load balancer, you need to specify a TLS private key and certificate to be used by the ingress controller.\nCreate a secret in the namespace of the ingress containing the TLS private key and certificate. Then configure the secret name in the TLS configuration section of the ingress specification.\n..read on HTTPS - Self Signed Certificates how to configure it.\n","categories":"","description":"","excerpt":"For encrypted communication between the client to the load balancer, …","ref":"/blog/2018/06.11-frontend-https/","tags":"","title":"Frontend HTTPS"},{"body":"The Gardener project team has analyzed the impact of the Gardener CVE-2018-2475 and the Kubernetes CVE-2018-1002105 on the Gardener Community Setup. Following some recommendations it is possible to mitigate both vulnerabilities.\nRead more on Hardening the Gardener Community Setup.\n","categories":"","description":"","excerpt":"The Gardener project team has analyzed the impact of the Gardener …","ref":"/blog/2018/06.11-hardening-the-gardener-community-setup/","tags":"","title":"Hardening the Gardener Community Setup"},{"body":"    Kubernetes is only available in Docker for Mac 17.12 CE and higher on the Edge channel. Kubernetes support is not included in Docker for Mac Stable releases. To find out more about Stable and Edge channels and how to switch between them, see general configuration.     Docker for Mac 17.12 CE (and higher) Edge includes a standalone Kubernetes server that runs on Mac, so that you can test deploying your Docker workloads on Kubernetes. The Kubernetes client command, kubectl, is included and configured to connect to the local Kubernetes server. If you have kubectl already installed and pointing to some other environment, such as minikube or a GKE cluster, be sure to change context so that kubectl is pointing to docker-for-desktop:\n…see more on Docker.com\nI recommend to setup your shell to see which KUBECONFIG is active.\n","categories":"","description":"","excerpt":"    Kubernetes is only available in Docker for Mac 17.12 CE and higher …","ref":"/blog/2018/06.11-kubernetes-is-available-in-docker-for-mac-17-12-ce/","tags":"","title":"Kubernetes is Available in Docker for Mac 17.12 CE"},{"body":"…or DENY all traffic from other namespaces\nYou can configure a NetworkPolicy to deny all traffic from other namespaces while allowing all traffic coming from the same namespace the pod is deployed to. There are many reasons why you may chose to configure Kubernetes network policies:\n Isolate multi-tenant deployments Regulatory compliance Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each another  ..read on Namespace Isolation how to configure it.\n","categories":"","description":"","excerpt":"…or DENY all traffic from other namespaces\nYou can configure a …","ref":"/blog/2018/06.11-namespace-isolation/","tags":"","title":"Namespace Isolation"},{"body":"Should I use:\n❌ one namespace per user/developer? ❌ one namespace per team? ❌ one per service type? ❌ one namespace per application type? 😄 one namespace per running instance of your application?  Apply the Principle of Least Privilege\nAll user accounts should run at all times as few privileges as possible, and also launch applications with as few privileges as possible. If you share a cluster for different user separated by a namespace, all user has access to all namespaces and services per default. It can happen that a user accidentally uses and destroys the namespace of a productive application or the namespace of another developer.\nKeep in mind: By default namespaces don’t provide:\n Network isolation Access Control Audit Logging on user level  ","categories":"","description":"","excerpt":"Should I use:\n❌ one namespace per user/developer? ❌ one namespace per …","ref":"/blog/2018/06.11-namespace-scope/","tags":"","title":"Namespace Scope"},{"body":"The efs-provisioner allows you to mount EFS storage as PersistentVolumes in kubernetes. It consists of a container that has access to an AWS EFS resource. The container reads a configmap containing the EFS filesystem ID, the AWS region and the name identifying the efs-provisioner. This name will be used later when you create a storage class.\nWhy EFS  When you have application running on multiple nodes which require shared access to a file system When you have an application that requires multiple virtual machines to access the same file system at the same time, AWS EFS is a tool that you can use. EFS supports encryption. EFS is SSD based storage and its storage capacity and pricing will scale in or out as needed, so there is no need for the system administrator to do additional operations. It can grow to a petabyte scale. EFS now supports NFSv4 lock upgrading and downgrading, so yes, you can use sqlite with EFS… even if it was possible before. Easy to setup  Why Not EFS  Sometimes when you think about using a service like EFS, you may also think about vendor lock-in and its negative sides Making an EFS backup may decrease your production FS performance; the throughput used by backup counts towards your total file system throughput. EFS is expensive compared to EBS (roughly twice the price of EBS storage) EFS is not the magical solution for all your distributed FS problems, it can be slow in many cases. Test, benchmark and measure to ensure your if EFS is a good solution for your use case. EFS distributed architecture results in a latency overhead for each file read/write operation. If you have the possibility to use a CDN, don’t use EFS, use it for the files which can’t be stored in a CDN. Don’t use EFS as a caching system, sometimes you could be doing this unintentionally. Last but not least, even if EFS is a fully managed NFS, you will face performance problems in many cases, resolving them takes time and needs effort.  ","categories":"","description":"","excerpt":"The efs-provisioner allows you to mount EFS storage as …","ref":"/blog/2018/06.11-readwritemany-dynamically-provisioned-persistent-volumes-using-amazon-efs/","tags":"","title":"ReadWriteMany - Dynamically Provisioned Persistent Volumes Using Amazon EFS"},{"body":"The storage is definitely the most complex and important part of an application setup, once this part is completed, one of the most problematic parts could be solved.\nMounting a S3 bucket into a pod using FUSE allows to access data stored in S3 via the filesystem. The mount is a pointer to an S3 location, so the data is never synced locally. Once mounted, any pod can read or even write from that directory without the need for explicit keys.\nHowever, it can be used to import and parse large amounts of data into a database.\n..read on Shared S3 Storage how to configure it.\n","categories":"","description":"","excerpt":"The storage is definitely the most complex and important part of an …","ref":"/blog/2018/06.11-shared-storage-with-s3-backend/","tags":"","title":"Shared Storage with S3 Backend"},{"body":"One thing that always bothered me was that I couldn’t get logs of several pods at once with kubectl. A simple tail -f \u003cpath-to-logfile\u003e isn’t possible. Certainly you can use kubectl logs -f \u003cpod-id\u003e, but it doesn’t help if you want to monitor more than one pod at a time.\nThis is something you really need a lot, at least if you run several instances of a pod behind a deployment and you don’t have setup a log viewer service like Kibana.\nkubetail comes to the rescue, it is a small bash script that allows you to aggregate log files of several pods at the same time in a simple way. The script is called kubetail and is available at GitHub.\n","categories":"","description":"","excerpt":"One thing that always bothered me was that I couldn’t get logs of …","ref":"/blog/2018/06.11-watching-logs-of-several-pods/","tags":"","title":"Watching Logs of Several Pods"},{"body":"Gardener extensibility and extraction of cloud-specific/OS-specific knowledge (#308, #262) Table of Contents  Table of Contents Summary Motivation  Goals Non-Goals   Proposal  Modification of existing CloudProfile and Shoot resources  CloudProfiles Shoots   CRD definitions and workflow adaptation  Custom resource definitions  DNS records Infrastructure provisioning Backup infrastructure provisioning Cloud config (user-data) for bootstrapping machines Worker pools definition Generic resources   Shoot state Shoot health checks/conditions Reconciliation flow Deletion flow   Gardenlet Shoot control plane movement/migration   Registration of external controllers at Gardener Other cloud-specific parts  Defaulting and validation admission plugins DNS Hosted Zone admission plugin Shoot Quota admission plugin Shoot maintenance controller   Alternatives  Summary Gardener has evolved to a large compound of packages containing lots of highly specific knowledge which makes it very hard to extend (supporting a new cloud provider, new OS, …, or behave differently depending on the underlying infrastructure).\nThis proposal aims to move out the cloud-specific implementations (called “(cloud) botanists”) and the OS-specifics into dedicated controllers, and simultaneously to allow deviation from the standard Gardener deployment.\nMotivation Currently, it is too hard to support additional cloud providers or operation systems/distributions as everything must be done in-tree which might affect the implementation of other cloud providers as well. The various conditions and branches make the code hard to maintain and hard to test. Every change must be done centrally, requires to completely rebuild Gardener, and cannot be deployed individually. Similar to the motivation for Kubernetes to extract their cloud-specifics into dedicated cloud-controller-managers or to extract the container/storage/network/… specifics into CRI/CSI/CNI/…, we aim to do the same right now.\nGoals  Gardener does not contain any cloud-specific knowledge anymore but defines a clear contract allowing external controllers (botanists) to support different environments (AWS, Azure, GCP, …). Gardener does not contain any operation system-specific knowledge anymore but defines a clear contract allowing external controllers to support different operation systems/distributions (CoreOS, SLES, Ubuntu, …). It shall become much easier to move control planes of Shoot clusters between Seed clusters (#232) which is a necessary requirement of an automated setup for the Gardener Ring (#233).  Non-Goals  We want to also factor out the specific knowledge of the addon deployments (nginx-ingress, kubernetes-dashboard, …), but we already have dedicated projects/issues for that: https://github.com/gardener/bouquet and #246. We will keep the addons in-tree as part of this proposal and tackle their extraction separately. We do not want to make the Gardener a plain workflow engine that just executes a given template (which indeed would allow to be generic, open, and extensible in their highest forms but which would end-up in building a “programming/scripting language” inside a serialization format (YAML/JSON/…)). Rather, we want to have well-defined contracts and APIs, keeping Gardener responsible for the clusters management.  Proposal Gardener heavily relies on and implements Kubernetes principles, and its ultimate strategy is to use Kubernetes wherever applicable. The extension concept in Kubernetes is based on (next to others) CustomResourceDefinitions, ValidatingWebhookConfigurations and MutatingWebhookConfigurations, and InitializerConfigurations. Consequently, Gardener’s extensibility concept relies on these mechanisms.\nInstead of implementing all aspects directly in Gardener it will deploy some CRDs to the Seed cluster which will be watched by dedicated controllers (also running in the Seed clusters), each one implementing one aspect of cluster management. This way one complex strongly coupled Gardener implementation covering all infrastructures is decomposed into a set of loosely coupled controllers implementing aspects of APIs defined by Gardener. Gardener will just wait until the controllers report that they are done (or have faced an error) in the CRD’s .status field instead of doing the respective tasks itself. We will have one specific CRD for every specific operation (e.g., DNS, infrastructure provisioning, machine cloud config generation, …). However, there are also parts inside Gardener which can be handled generically (not by cloud botanists) because they are the same or very similar for all the environments. One example of those is the deployment of a Namespace in the Seed which will run the Shoot’s control plane Another one is the deployment of a Service for the Shoot’s kube-apiserver. In case a cloud botanist needs to cooperate and react on those operations it should register a ValidatingWebhookConfiguration, a MutatingWebhookConfiguration, or a InitializerConfiguration. With this approach it can validate, modify, or react on any resource created by Gardener to make it cloud infrastructure specific.\nThe web hooks should be registered with failurePolicy=Fail to ensure that a request made by Gardener fails if the respective web hook is not available.\nModification of existing CloudProfile and Shoot resources We will introduce the new API group gardener.cloud:\nCloudProfiles --- apiVersion: gardener.cloud/v1alpha1 kind: CloudProfile metadata:  name: aws spec:  type: aws # caBundle: | # -----BEGIN CERTIFICATE----- # ... # -----END CERTIFICATE-----  dnsProviders:  - type: aws-route53  - type: unmanaged  kubernetes:  versions:  - 1.12.1  - 1.11.0  - 1.10.5  machineTypes:  - name: m4.large  cpu: \"2\"  gpu: \"0\"  memory: 8Gi  # storage: 20Gi # optional (not needed in every environment, may only be specified if no volumeTypes have been specified)  ...  volumeTypes: # optional (not needed in every environment, may only be specified if no machineType has a `storage` field)  - name: gp2  class: standard  - name: io1  class: premium  providerConfig:  apiVersion: aws.cloud.gardener.cloud/v1alpha1  kind: CloudProfileConfig  constraints:  minimumVolumeSize: 20Gi  machineImages:  - name: coreos  regions:  - name: eu-west-1  ami: ami-32d1474b  - name: us-east-1  ami: ami-e582d29f  zones:  - region: eu-west-1  zones:  - name: eu-west-1a  unavailableMachineTypes: # list of machine types defined above that are not available in this zone  - name: m4.large  unavailableVolumeTypes: # list of volume types defined above that are not available in this zone  - name: gp2  - name: eu-west-1b  - name: eu-west-1c Shoots apiVersion: gardener.cloud/v1alpha1 kind: Shoot metadata:  name: johndoe-aws  namespace: garden-dev spec:  cloudProfileName: aws  secretBindingName: core-aws  cloud:  type: aws  region: eu-west-1  providerConfig:  apiVersion: aws.cloud.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  vpc: # specify either 'id' or 'cidr'  # id: vpc-123456  cidr: 10.250.0.0/16  internal:  - 10.250.112.0/22  public:  - 10.250.96.0/22  workers:  - 10.250.0.0/19  zones:  - eu-west-1a  workerPools:  - name: pool-01  # Taints, labels, and annotations are not yet implemented. This requires interaction with the machine-controller-manager, see  # https://github.com/gardener/machine-controller-manager/issues/174. It is only mentioned here as future proposal.  # taints:  # - key: foo  # value: bar  # effect: PreferNoSchedule  # labels:  # - key: bar  # value: baz  # annotations:  # - key: foo  # value: hugo  machineType: m4.large  volume: # optional, not needed in every environment, may only be specified if the referenced CloudProfile contains the volumeTypes field  type: gp2  size: 20Gi  providerConfig:  apiVersion: aws.cloud.gardener.cloud/v1alpha1  kind: WorkerPoolConfig  machineImage:  name: coreos  ami: ami-d0dcef3  zones:  - eu-west-1a  minimum: 2  maximum: 2  maxSurge: 1  maxUnavailable: 0  kubernetes:  version: 1.11.0  ...  dns:  provider: aws-route53  domain: johndoe-aws.garden-dev.example.com  maintenance:  timeWindow:  begin: 220000+0100  end: 230000+0100  autoUpdate:  kubernetesVersion: true  backup:  schedule: \"*/5 * * * *\"  maximum: 7  addons:  kube2iam:  enabled: false  kubernetes-dashboard:  enabled: true  cluster-autoscaler:  enabled: true  nginx-ingress:  enabled: true  loadBalancerSourceRanges: []  kube-lego:  enabled: true  email: john.doe@example.com ℹ The specifications for the other cloud providers Gardener already has an implementation for looks similar.\nCRD definitions and workflow adaptation In the following we are outlining the CRD definitions which define the API between Gardener and the dedicated controllers. After that we will take a look at the current reconciliation/deletion flow and describe how it would look like in case we would implement this proposal.\nCustom resource definitions Every CRD has a .spec.type field containing the respective instance of the dimension the CRD represents, e.g. the cloud provider, the DNS provider or the operation system name. Moreover, the .status field must contain\n observedGeneration (int64), a field indicating on which generation the controller last worked on. state (*runtime.RawExtension), a field which is not interpreted by Gardener but persisted; it should be treated opaque and only be used by the respective CRD-specific controller (it can store anything it needs to re-construct its own state). lastError (object), a field which is optional and only present if the last operation ended with an error state. lastOperation (object), a field which always exists and which indicates what the last operation of the controller was. conditions (list), a field allowing the controller to report health checks for its area of responsibility.  Some CRDs might have a .spec.providerConfig or a .status.providerStatus field containing controller-specific information that is treated opaque by Gardener and will only be copied to dependent or depending CRDs.\nDNS records Every Shoot needs two DNS records (or three, depending on whether nginx-ingress addon is enabled), one so-called “internal” record that Gardener uses in the kubeconfigs of the Shoot cluster’s system components, and one so-called “external” record which is used in the kubeconfig provided to the user.\n--- apiVersion: dns.gardener.cloud/v1alpha1 kind: DNSProvider metadata:  name: alicloud  namespace: default spec:  type: alicloud-dns  secretRef:  name: alicloud-credentials  domains:  include:  - my.own.domain.com --- apiVersion: dns.gardener.cloud/v1alpha1 kind: DNSEntry metadata:  name: dns  namespace: default spec:  dnsName: dns.my.own.domain.com  ttl: 600  targets:  - 8.8.8.8 status:  observedGeneration: 4  state: some-state  lastError:  lastUpdateTime: 2018-04-04T07:08:51Z  description: some-error message  codes:  - ERR_UNAUTHORIZED  lastOperation:  lastUpdateTime: 2018-04-04T07:24:51Z  progress: 70  type: Reconcile  state: Processing  description: Currently provisioning ...  conditions:  - lastTransitionTime: 2018-07-11T10:18:25Z  message: DNS record has been created and is available.  reason: RecordResolvable  status: \"True\"  type: Available  propagate: false  providerStatus:  apiVersion: aws.extensions.gardener.cloud/v1alpha1  kind: DNSStatus  ... Infrastructure provisioning The Infrastructure CRD contains the information about VPC, networks, security groups, availability zones, …, basically, everything that needs to be prepared before an actual VMs/load balancers/… can be provisioned.\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: Infrastructure metadata:  name: infrastructure  namespace: shoot--core--aws-01 spec:  type: aws  providerConfig:  apiVersion: aws.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  vpc:  cidr: 10.250.0.0/16  internal:  - 10.250.112.0/22  public:  - 10.250.96.0/22  workers:  - 10.250.0.0/19  zones:  - eu-west-1a  dns:  apiserver: api.aws-01.core.example.com  region: eu-west-1  secretRef:  name: my-aws-credentials  sshPublicKey: | base64(key) status:  observedGeneration: ...  state: ...  lastError: ..  lastOperation: ...  providerStatus:  apiVersion: aws.extensions.gardener.cloud/v1alpha1  kind: InfrastructureStatus  vpc:  id: vpc-1234  subnets:  - id: subnet-acbd1234  name: workers  zone: eu-west-1  securityGroups:  - id: sg-xyz12345  name: workers  iam:  nodesRoleARN: \u003csome-arn\u003e  instanceProfileName: foo  ec2:  keyName: bar Backup infrastructure provisioning The BackupInfrastructure CRD in the Seeds tells the cloud-specific controller to prepare a blob store bucket/container which can later be used to store etcd backups.\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: BackupInfrastructure metadata:  name: etcd-backup  namespace: shoot--core--aws-01 spec:  type: aws  region: eu-west-1  storageContainerName: asdasjndasd-1293912378a-2213  secretRef:  name: my-aws-credentials status:  observedGeneration: ...  state: ...  lastError: ..  lastOperation: ... Cloud config (user-data) for bootstrapping machines Gardener will continue to keep knowledge about the content of the cloud config scripts, but it will hand over it to the respective OS-specific controller which will generate the specific valid representation. Gardener creates two MachineCloudConfig CRDs, one for the cloud-config-downloader (which will later flow into the WorkerPool CRD) and one for the real cloud-config (which will be stored as a Secret in the Shoot’s kube-system namespace, and downloaded and executed from the cloud-config-downloader on the machines).\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: MachineCloudConfig metadata:  name: pool-01-downloader  namespace: shoot--core--aws-01 spec:  type: CoreOS  units:  - name: cloud-config-downloader.service  command: start  enable: true  content: |[Unit] Description=Downloads the original cloud-config from Shoot API Server and executes it After=docker.service docker.socket Wants=docker.socket [Service] Restart=always RestartSec=30 EnvironmentFile=/etc/environment ExecStart=/bin/sh /var/lib/cloud-config-downloader/download-cloud-config.sh  files:  - path: /var/lib/cloud-config-downloader/credentials/kubeconfig  permissions: 0644  content:  secretRef:  name: cloud-config-downloader  dataKey: kubeconfig  - path: /var/lib/cloud-config-downloader/download-cloud-config.sh  permissions: 0644  content:  inline:  encoding: b64  data: IyEvYmluL2Jhc2ggL... status:  observedGeneration: ...  state: ...  lastError: ..  lastOperation: ...  cloudConfig: | # base64-encoded  #cloud-config   coreos:  update:  reboot-strategy: off  units:  - name: cloud-config-downloader.service  command: start  enable: true  content: |[Unit] Description=Downloads the original cloud-config from Shoot API Server and execute it After=docker.service docker.socket Wants=docker.socket [Service] Restart=always RestartSec=30 ... ℹ The cloud-config-downloader script does not only download the cloud-config initially but at regular intervals, e.g., every 30s. If it sees an updated cloud-config then it applies it again by reloading and restarting all systemd units in order to reflect the changes. The way how this reloading of the cloud-config happens is OS-specific as well and not known to Gardener anymore, however, it must be part of the script already. On CoreOS, you have to execute /usr/bin/coreos-cloudinit --from-file=\u003cpath\u003e whereas on SLES you execute cloud-init --file \u003cpath\u003e single -n write_files --frequency=once. As Gardener doesn’t know these commands it will write a placeholder expression instead (e.g., {RELOAD-CLOUD-CONFIG-WITH-PATH:\u003cpath\u003e}) and the OS-specific controller is asked to replace it with the proper expression.\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: MachineCloudConfig metadata:  name: pool-01-original # stored as secret and downloaded later  namespace: shoot--core--aws-01 spec:  type: CoreOS  units:  - name: docker.service  drop-ins:  - name: 10-docker-opts.conf  content: |[Service] Environment=\"DOCKER_OPTS=--log-opt max-size=60m --log-opt max-file=3\"  - name: docker-monitor.service  command: start  enable: true  content: |[Unit] Description=Docker-monitor daemon After=kubelet.service [Service] Restart=always EnvironmentFile=/etc/environment ExecStart=/opt/bin/health-monitor docker  - name: kubelet.service  command: start  enable: true  content: |[Unit] Description=kubelet daemon Documentation=https://kubernetes.io/docs/admin/kubelet After=docker.service Wants=docker.socket rpc-statd.service [Service] Restart=always RestartSec=10 EnvironmentFile=/etc/environment ExecStartPre=/bin/docker run --rm -v /opt/bin:/opt/bin:rw k8s.gcr.io/hyperkube:v1.11.2 cp /hyperkube /opt/bin/ ExecStartPre=/bin/sh -c 'hostnamectl set-hostname $(cat /etc/hostname | cut -d '.' -f 1)' ExecStart=/opt/bin/hyperkube kubelet \\ --allow-privileged=true \\ --bootstrap-kubeconfig=/var/lib/kubelet/kubeconfig-bootstrap \\ ...  files:  - path: /var/lib/kubelet/ca.crt  permissions: 0644  content:  secretRef:  name: ca-kubelet  dataKey: ca.crt  - path: /var/lib/cloud-config-downloader/download-cloud-config.sh  permissions: 0644  content:  inline:  encoding: b64  data: IyEvYmluL2Jhc2ggL...  - path: /etc/sysctl.d/99-k8s-general.conf  permissions: 0644  content:  inline:  data: |vm.max_map_count = 135217728 kernel.softlockup_panic = 1 kernel.softlockup_all_cpu_backtrace = 1 ...  - path: /opt/bin/health-monitor  permissions: 0755  content:  inline:  data: |#!/bin/bash set -o nounset set -o pipefail function docker_monitoring { ... status:  observedGeneration: ...  state: ...  lastError: ..  lastOperation: ...  cloudConfig: ... Cloud-specific controllers which might need to add another kernel option or another flag to the kubelet, maybe even another file to the disk, can register a MutatingWebhookConfiguration to that resource and modify it upon creation/update. The task of the MachineCloudConfig controller is to only generate the OS-specific cloud-config based on the .spec field, but not to add or change any logic related to Shoots.\nWorker pools definition For every worker pool defined in the Shoot Gardener will create a WorkerPool CRD which shall be picked up by a cloud-specific controller and be translated to MachineClasses and MachineDeployments.\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: WorkerPool metadata:  name: pool-01  namespace: shoot--core--aws-01 spec:  cloudConfig: base64(downloader-cloud-config)  infrastructureProviderStatus:  apiVersion: aws.extensions.gardener.cloud/v1alpha1  kind: InfrastructureStatus  vpc:  id: vpc-1234  subnets:  - id: subnet-acbd1234  name: workers  zone: eu-west-1  securityGroups:  - id: sg-xyz12345  name: workers  iam:  nodesRoleARN: \u003csome-arn\u003e  instanceProfileName: foo  ec2:  keyName: bar  providerConfig:  apiVersion: aws.cloud.gardener.cloud/v1alpha1  kind: WorkerPoolConfig  machineImage:  name: CoreOS  ami: ami-d0dcef3b  machineType: m4.large  volumeType: gp2  volumeSize: 20Gi  zones:  - eu-west-1a  region: eu-west-1  secretRef:  name: my-aws-credentials  minimum: 2  maximum: 2 status:  observedGeneration: ...  state: ...  lastError: ..  lastOperation: ... Generic resources Some components are cloud-specific and must be deployed by the cloud-specific botanists. Others might need to deploy another pod next to the shoot’s control plane or must do anything else. Some of these might be important for a functional cluster (e.g., the cloud-controller-manager, or a CSI plugin in the future), and controllers should be able to report errors back to the user. Consequently, in order to trigger the controllers to deploy these components Gardener would write a Generic CRD to the Seed to trigger the deployment. No operation is depending on the status of these resources, however, the entire reconciliation flow is.\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: Generic metadata:  name: cloud-components  namespace: shoot--core--aws-01 spec:  type: cloud-components  secretRef:  name: my-aws-credentials  shootSpec:  ... status:  observedGeneration: ...  state: ...  lastError: ..  lastOperation: ... Shoot state In order to enable moving the control plane of a Shoot between Seed clusters (e.g., if a Seed cluster is not available anymore or entirely broken) Gardener must store some non-reconstructable state, potentially also the state written by the controllers. Gardener watches these extension CRDs and copies the .status.state in a ShootState resource into the Garden cluster. Any observed status change of the respective CRD-controllers must be immediately reflected in the ShootState resource. The contract between Gardener and those controllers is: Every controller must be capable of reconstructing its own environment based on both the state it has written before and on the real world’s conditions/state.\n--- apiVersion: gardener.cloud/v1alpha1 kind: ShootState metadata:  name: shoot--core--aws-01 shootRef:  name: aws-01  project: core state:  secrets:  - name: ca  data: ...  - name: kube-apiserver-cert  data: ...  resources:  - kind: DNS  name: record-1  state: \u003ccopied-state-of-dns-crd\u003e  - kind: Infrastructure  name: networks  state: \u003ccopied-state-of-infrastructure-crd\u003e  ...  \u003cother fields required to keep track of\u003e We cannot assume that Gardener is always online to observe the most recent states the controllers have written to their resources. Consequently, the information stored here must not be used as “single point of truth”, but the controllers must potentially check the real world’s status to reconstruct themselves. However, this must anyway be part of their normal reconciliation logic and is a general best practice for Kubernetes controllers.\nShoot health checks/conditions Some of the existing conditions already contain specific code which shall be simplified as well. All of the CRDs described above have a .status.conditions field to which the controllers may write relevant health information of their function area. Gardener will pick them up and copy them over to the Shoots .status.conditions (only those conditions setting propagate=true).\nReconciliation flow We are now examining the current Shoot creation/reconciliation flow and describe how it could look like when applying this proposal:\n   Operation Description     botanist.DeployNamespace Gardener creates the namespace for the Shoot in the Seed cluster.   botanist.DeployKubeAPIServerService Gardener creates a Service of type LoadBalancer in the Seed.\nAWS Botanist registers a Mutating Webhook and adds its AWS-specific annotation.   botanist.WaitUntilKubeAPIServerServiceIsReady Gardener checks the .status object of the just created Service in the Seed. The contract is that also clouds not supporting load balancers must react on the Service object and modify the .status to correctly reflect the kube-apiserver’s ingress IP.   botanist.DeploySecrets Gardener creates the secrets/certificates it needs like it does today, but it provides utility functions that can be adopted by Botanists/other controllers if they need additional certificates/secrets created on their own. (We should also add labels to all secrets)   botanist.Shoot.Components.DNS.Internal{Provider/Entry}.Deploy Gardener creates a DNS-specific CRD in the Seed, and the responsible DNS-controller picks it up and creates a corresponding DNS record (see CRD specification above).   botanist.Shoot.Components.DNS.External{Provider/Entry}.Deploy Gardener creates a DNS-specific CRD in the Seed, and the responsible DNS-controller picks it up and creates a corresponding DNS record: (see CRD specification above).   shootCloudBotanist.DeployInfrastructure Gardener creates a Infrastructure-specific CRD in the Seed, and the responsible Botanist picks it up and does its job: (see CRD above).   botanist.DeployBackupInfrastructure Gardener creates a BackupInfrastructure resource in the Garden cluster.\n(The BackupInfrastructure controller creates a BackupInfrastructure-specific CRD in the Seed, and the responsible Botanist picks it up and does its job: (see CRD above).)   botanist.WaitUntilBackupInfrastructureReconciled Gardener checks the .status object of the just created BackupInfrastructure resource.   hybridBotanist.DeployETCD Gardener does only deploy the etcd StatefulSet without backup-restore sidecar at all.\nThe cloud-specific Botanist registers a Mutating Webhook and adds the backup-restore sidecar, and it also creates the Secret needed by the backup-restore sidecar.   botanist.WaitUntilEtcdReady Gardener checks the .status object of the etcd Statefulset and waits until readiness is indicated.   hybridBotanist.DeployCloudProviderConfig Gardener does not execute this anymore because it doesn’t know anything about cloud-specific configuration.   hybridBotanist.DeployKubeAPIServer Gardener does only deploy the kube-apiserver Deployment without any cloud-specific flags/configuration.\nThe cloud-specific Botanist registers a Mutating Webhook and adds whatever is needed for the kube-apiserver to run in its cloud environment.   hybridBotanist.DeployKubeControllerManager Gardener does only deploy the kube-controller-manager Deployment without any cloud-specific flags/configuration.\nThe cloud-specific Botanist registers a Mutating Webhook and adds whatever is needed for the kube-controller-manager to run in its cloud environment (e.g., the cloud-config).   hybridBotanist.DeployKubeScheduler Gardener does only deploy the kube-scheduler Deployment without any cloud-specific flags/configuration.\nThe cloud-specific Botanist registers a Mutating Webhook and adds whatever is needed for the kube-scheduler to run in its cloud environment.   hybridBotanist.DeployCloudControllerManager Gardener does not execute this anymore because it doesn’t know anything about cloud-specific configuration. The Botanists would be responsible to deploy their own cloud-controller-manager now.\nThey would watch for the kube-apiserver Deployment to exist, and as soon as it does, they deploy the CCM.\n(Side note: The Botanist would also be responsible to deploy further controllers needed for this cloud environment, e.g. F5-controllers or CSI plugins).   botanist.WaitUntilKubeAPIServerReady Gardener checks the .status object of the kube-apiserver Deployment and waits until readiness is indicated.   botanist.InitializeShootClients Unchanged; Gardener creates a Kubernetes client for the Shoot cluster.   botanist.DeployMachineControllerManager Deleted, Gardener does no longer deploy MCM itself. See below.   hybridBotanist.ReconcileMachines Gardener creates a Worker CRD in the Seed, and the responsible Worker controller picks it up and does its job (see CRD above). It also deploys the machine-controller-manager.\nGardener waits until the status indicates that the controller is done.   hybridBotanist.DeployKubeAddonManager This function also computes the CoreOS cloud-config (because the secret storing it is managed by the kube-addon-manager).\nGardener would deploy the CloudConfig-specific CRD in the Seed, and the responsible OS controller picks it up and does its job (see CRD above).\nThe Botanists which would have to modify something would register a Webhook for this CloudConfig-specific resource and apply their changes.\nThe rest is mostly unchanged, Gardener generates the manifests for the addons and deploys the kube-addon-manager into the Seed.\nAWS Botanist registers a Webhook for nginx-ingress.\nAzure Botanist registers a Webhook for calico.\nGardener will no longer deploy the StorageClasses. Instead, the Botanists wait until the kube-apiserver is available and deploy them.\nIn the long term we want to get rid of optional addons inside the Gardener core and implement a sophisticated addon concept (see #246).   shootCloudBotanist.DeployKube2IAMResources This function would be removed (currently Gardener would execute a Terraform job creating the IAM roles specified in the Shoot manifest). We cannot keep this behavior, the user would be responsible to create the needed IAM roles on its own.   botanist.Shoot.Components.Nginx.DNSEtnry Gardener creates a DNS-specific CRD in the Seed, and the responsible DNS-controller picks it up and creates a corresponding DNS record (see CRD specification above).   botanist.WaitUntilVPNConnectionExists Unchanged, Gardener checks that it is possible to port-forward to a Shoot pod.   seedCloudBotanist.ApplyCreateHook This function would be removed (actually, only the AWS Botanist implements it).\nAWS Botanist deploys the aws-lb-readvertiser once the API Server is deployed and updates the ELB health check protocol one the load balancer pointing to the API server is created.   botanist.DeploySeedMonitoring Unchanged, Gardener deploys the monitoring stack into the Seed.   botanist.DeployClusterAutoscaler Unchanged, Gardener deploys the cluster-autoscaler into the Seed.    ℹ We can easily lift the contract later and allow dynamic network plugins or not using the VPN solution at all. We could also introduce a dedicated ControlPlane CRD and leave the complete responsibility of deploying kube-apiserver, kube-controller-manager, etc. to other controllers (if we need it at some point in time).\nDeletion flow We are now examining the current Shoot deletion flow and describe shortly how it could look like when applying this proposal:\n   Operation Description     botanist.DeploySecrets This is just refreshing the cloud provider secret in the Shoot namespace in the Seed (in case the user has changed it before triggering the deletion). This function would stay as it is.   hybridBotanist.RefreshMachineClassSecrets This function would disappear.\nWorker Pool controller needs to watch the referenced secret and update the generated MachineClassSecrets immediately.   hybridBotanist.RefreshCloudProviderConfig This function would disappear. Botanist needs to watch the referenced secret and update the generated cloud-provider-config immediately.   botanist.RefreshCloudControllerManagerChecksums See “hybridBotanist.RefreshCloudProviderConfig”.   botanist.RefreshKubeControllerManagerChecksums See “hybridBotanist.RefreshCloudProviderConfig”.   botanist.InitializeShootClients Unchanged; Gardener creates a Kubernetes client for the Shoot cluster.   botanist.DeleteSeedMonitoring Unchanged; Gardener deletes the monitoring stack.   botanist.DeleteKubeAddonManager Unchanged; Gardener deletes the kube-addon-manager.   botanist.DeleteClusterAutoscaler Unchanged; Gardener deletes the cluster-autoscaler.   botanist.WaitUntilKubeAddonManagerDeleted Unchanged; Gardener waits until the kube-addon-manager is deleted.   botanist.CleanCustomResourceDefinitions Unchanged, Gardener cleans the CRDs in the Shoot.   botanist.CleanKubernetesResources Unchanged, Gardener cleans all remaining Kubernetes resources in the Shoot.   hybridBotanist.DestroyMachines Gardener deletes the WorkerPool-specific CRD in the Seed, and the responsible WorkerPool-controller picks it up and does its job.\nGardener waits until the CRD is deleted.   shootCloudBotanist.DestroyKube2IAMResources This function would disappear (currently Gardener would execute a Terraform job deleting the IAM roles specified in the Shoot manifest). We cannot keep this behavior, the user would be responsible to delete the needed IAM roles on its own.   shootCloudBotanist.DestroyInfrastructure Gardener deletes the Infrastructure-specific CRD in the Seed, and the responsible Botanist picks it up and does its job.\nGardener waits until the CRD is deleted.   botanist.Shoot.Components.DNS.External{Provider/Entry}.Destroy Gardener deletes the DNS-specific CRD in the Seed, and the responsible DNS-controller picks it up and does its job.\nGardener waits until the CRD is deleted.   botanist.DeleteKubeAPIServer Unchanged; Gardener deletes the kube-apiserver.   botanist.DeleteBackupInfrastructure Unchanged; Gardener deletes the BackupInfrastructure object in the Garden cluster.\n(The BackupInfrastructure controller deletes the BackupInfrastructure-specific CRD in the Seed, and the responsible Botanist picks it up and does its job.\nThe BackupInfrastructure controller waits until the CRD is deleted.)   botanist.Shoot.Components.DNS.Internal{Provider/Entry}.Destroy Gardener deletes the DNS-specific CRD in the Seed, and the responsible DNS-controller picks it up and does its job.\nGardener waits until the CRD is deleted.   botanist.DeleteNamespace Unchanged; Gardener deletes the Shoot namespace in the Seed cluster.   botanist.WaitUntilSeedNamespaceDeleted Unchanged; Gardener waits until the Shoot namespace in the Seed has been deleted.   botanist.DeleteGardenSecrets Unchanged; Gardener deletes the kubeconfig/ssh-keypair Secret in the project namespace in the Garden.    Gardenlet One part of the whole extensibility work will also to further split Gardener itself. Inspired from Kubernetes itself we plan to move the Shoot reconciliation/deletion controller loops as well as the BackupInfrastructure reconciliation/deletion controller loops into a dedicated “gardenlet” component that will run in the Seed cluster. With that, it can talk locally to the responsible kube-apiserver and we do no longer need to perform every operation out of the Garden cluster. This approach will also help us with scalability, performance, maintainability, testability in general.\nThis architectural change implies that the Kubernetes API server of the Garden cluster must be exposed publicly (or at least be reachable by the registered Seeds). The Gardener controller-manager will remain and will keep its CloudProfile, SecretBinding, Quota, Project, and Seed controller loops. One part of the seed controller could be to deploy the “gardenlet” into the Seeds, however, this would require network connectivity to the Seed cluster.\nShoot control plane movement/migration Automatically moving control planes is difficult with the current implementation as some resources created in the old Seed must be moved to the new one. However, some of them are not under Gardener’s control (e.g., Machine resources). Moreover, the old control plane must be deactivated somehow to ensure that not two controllers work on the same things (e.g., virtual machines) from different environments.\nGardener does not only deploy a DNS controller into the Seeds but also into its own Garden cluster. For every Shoot cluster, Gardener commissions it to create a DNS TXT record containing the name of the Seed responsible for the Shoot (holding the control plane), e.g.\ndig -t txt aws-01.core.garden.example.com  ... ;; ANSWER SECTION: aws-01.core.garden.example.com. 120 IN\tTXT \"Seed=seed-01\" ... Gardener always keeps the DNS record up-to-date based on which Seed is responsible.\nIn the above CRD examples one object in the .spec section was omitted as it is needed to get Shoot control plane movement/migration working (the field is only explained now in this section and not before; it was omitted on purpose to support focusing on the relevant specifications first). Every CRD also has the following section in its .spec:\nleadership:  record: aws-01.core.garden.example.com  value: seed-01  leaseSeconds: 60 Before every operation the CRD-controllers check this DNS record (based on the .spec.leadership.leaseSeconds configuration) and verify that its result is equal to the .spec.leadership.value field. If both match they know that they should act on the resource, otherwise they stop doing anything.\nℹ We will provide an easy-to-use framework for the controllers containing all of these features out-of-the-box in order to allow the developers to focus on writing the actual controller logic.\nWhen a Seed control plane move is triggered, the .spec.cloud.seed field of the respective Shoot is changed. Gardener will change the respective DNS record’s value (aws-01.core.garden.example.com) to contain the new Seed name. After that it will wait 2*60s to be sure that all controllers have observed the change. Then it starts reconciling and applying the CRDs together with a preset .status.state into the new Seed (based on its last observations which were stored in the respective ShootState object stored in the Garden cluster). The controllers are - as per contract - asked to reconstruct their own environment based on the .status.state they have written before and the real world’s status. Apart from that, the normal reconciliation flow gets executed.\nGardener stores the list of Seeds that were responsible for hosting a Shoots control plane at some time in the Shoots .status.seeds list so that it knows which Seeds must be cleaned up (i.e., where the control plane must be deleted because it has been moved). Once cleaned up, the Seed’s name will be removed from that list.\nBackupInfrastructure migration One part of the reconciliation flow above is the provisioning of the infrastructure for the Shoot’s etcd backups (usually, this is a blob store bucket/container). Gardener already uses a separate BackupInfrastructure resource that is written into the Garden cluster and picked up by a dedicated BackupInfrastructure controller (bundled into the Gardener controller manager). This dedicated resource exists mainly for the reason to allow keeping backups for a certain “grace period” even after the Shoot deletion itself:\napiVersion: gardener.cloud/v1alpha1 kind: BackupInfrastructure metadata:  name: aws-01-bucket  namespace: garden-core spec:  seed: seed-01  shootUID: uuid-of-shoot The actual provisioning is executed in a corresponding Seed cluster as Gardener can only assume network connectivity to the underlying cloud environment in the Seed. We would like to keep the created artifacts in the Seed (e.g., Terraform state) near to the control plane. Consequently, when Gardener moves a control plane, it will update the .spec.seed field of the BackupInfrastructure resource as well. With the exact same logic described above the BackupInfrastructure controller inside the Gardener will move to the new Seed.\nRegistration of external controllers at Gardener We want to have a dynamic registration process, i.e. we don’t want to hard-code any information about which controllers shall be deployed. The ideal solution would be to not even requiring a restart of Gardener when a new controller registers.\nEvery controller is registered by a ControllerRegistration resource that introduces every controller together with its supported resources (dimension (kind) and shape (type) combination) to Gardener:\napiVersion: gardener.cloud/v1alpha1 kind: ControllerRegistration metadata:  name: dns-aws-route53 spec:  resources:  - kind: DNS  type: aws-route53 # deployment: # type: helm # providerConfig: # chart.tgz: base64(helm-chart) # values.yaml: | # foo: bar Every .kind/.type combination may only exist once in the system.\nWhen a Shoot shall be reconciled Gardener can identify based on the referenced Seed and the content of the Shoot specification which controllers are needed in the respective Seed cluster. It will demand the operators in the Garden cluster to deploy the controllers they are responsible for to a specific Seed. This kind of communication happens via CRDs as well:\napiVersion: gardener.cloud/v1alpha1 kind: ControllerInstallation metadata:  name: dns-aws-route53 spec:  registrationRef:  name: dns-aws-route53  seedRef:  name: seed-01 status:  conditions:  - lastTransitionTime: 2018-08-07T15:09:23Z  message: The controller has been successfully deployed to the seed.  reason: ControllerDeployed  status: \"True\"  type: Available The default scenario is that every controller is gets deployed by a dedicated operator that knows how to handle its lifecycle operations like deployment, update, upgrade, deletion. This operator watches ControllerInstallation resources and reacts on those it is responsible for (that it has created earlier). Gardener is responsible for writing the .spec field, the operator is responsible for providing information in the .status indicating whether the controller was successfully deployed and is ready to be used. Gardener will be also able to ask for deletion of controllers from Seeds when they are not needed there anymore by deleting the corresponding ControllerInstallation object.\nℹ The provided easy-to-use framework for the controllers will also contain these needed features to implement corresponding operators.\nFor most cases the controller deployment is very simple (just deploying it into the seed with some static configuration). In these cases it would produce unnecessary effort to ask for providing another component (the operator) that deploys the controller. To simplify this situation Gardener will be able to react on ControllerInstallations specifying .spec.registration.deployment.type=helm. The controller would be registered with the ControllerRegistration resources that would contain a Helm chart with all resources needed to deploy this controller into a seed (plus some static values). Gardener would render the Helm chart and deploy the resources into the seed. It will not react if .spec.registration.deployment.type!=helm which allows to also use any other deployment mechanism. Controllers that are getting deployed by operators would not specify the .spec.deployment section in the ControllerRegistration at all.\nℹ Any controller requiring dynamic configuration values (e.g., based on the cloud provider or the region of the seed) must be installed with the operator approach.\nOther cloud-specific parts The Gardener API server has a few admission controllers that contain cloud-specific code as well. We have to replace these parts as well.\nDefaulting and validation admission plugins Right now, the admission controllers inside the Gardener API server do perform a lot of validation and defaulting of fields in the Shoot specification. The cloud-specific parts of these admission controllers will be replaced by mutating admission webhooks that will get called instead. As we will have a dedicated operator running in the Garden cluster anyway it will also get the responsibility to register this webhook if it needs to validate/default parts of the Shoot specification.\nExample: The .spec.cloud.workerPools[*].providerConfig.machineImage field in the new Shoot manifest mentioned above could be omitted by the user and would get defaulted by the cloud-specific operator.\nDNS Hosted Zone admission plugin For the same reasons the existing DNS Hosted Zone admission plugin will be removed from the Gardener core and moved into the responsibility of the respective DNS-specific operators running in the Garden cluster.\nShoot Quota admission plugin The Shoot quota admission plugin validates create or update requests on Shoots and checks that the specified machine/storage configuration is defined as per referenced Quota objects. The cloud-specifics in this controller are no longer needed as the CloudProfile and the Shoot resource have been adapted: The machine/storage configuration is no longer in cloud-specific sections but hard-wired fields in the general Shoot specification (see example resources above). The quota admission plugin will be simplified and remains in the Gardener core.\nShoot maintenance controller Every Shoot cluster can define a maintenance time window in which Gardener will update the Kubernetes patch version (if enabled) and the used machine image version in the Shoot resource. While the Kubernetes version is not part of the providerConfig section in the CloudProfile resource, the machineImage field is, and thus Gardener can’t understand it any longer. In the future Gardener has to rely on the cloud-specific operator (probably the same doing the defaulting/validation mentioned before) to update this field. In the maintenance time window the maintenance controller will update the Kubernetes patch version (if enabled) and add a trigger.gardener.cloud=maintenance annotation in the Shoot resource. The already registered mutating web hook will call the operator who has to remove this annotation and update the machineImage in the .spec.cloud.workerPools[*].providerConfig sections.\nAlternatives  Alternative to DNS approach for Shoot control plane movement/migration: We have thought about rotating the credentials when a move is triggered which would make all controllers ineffective immediately. However, one problem with this is that we require IAM privileges for the users infrastructure account which might be not desired. Another, more complicated problem is that we cannot assume API access in order to create technical users for all cloud environments that might be supported.  ","categories":"","description":"","excerpt":"Gardener extensibility and extraction of cloud-specific/OS-specific …","ref":"/docs/gardener/proposals/01-extensibility/","tags":"","title":"01 Extensibility"},{"body":"Backup Infrastructure CRD and Controller Redesign Goal  As an operator, I would like to efficiently use the backup bucket for multiple clusters, thereby limiting the total number of buckets required. As an operator, I would like to use different cloud provider for backup bucket provisioning other than cloud provider used for seed infrastructure. Have seed independent backups, so that we can easily migrate a shoot from one seed to another. Execute the backup operations (including bucket creation and deletion) from a seed, because network connectivity may only be ensured from the seeds (not necessarily from the garden cluster). Preserve the garden cluster as source of truth (no information is missing in the garden cluster to reconstruct the state of the backups even if seed and shoots are lost completely). Do not violate the infrastructure limits in regards to blob store limits/quotas.  Motivation Currently, every shoot cluster has its own etcd backup bucket with a centrally configured retention period. With the growing number of clusters, we are soon running out of the quota limits of buckets on the cloud provider. Moreover, even if the clusters are deleted, the backup buckets do exist, for a configured period of retention. Hence, there is need of minimizing the total count of buckets.\nIn addition, currently we use seed infrastructure credentials to provision the bucket for etcd backups. This results in binding backup bucket provider to seed infrastructure provider.\nTerminology  Bucket : It is equivalent to s3 bucket, abs container, gcs bucket, swift container, alicloud bucket Object : It is equivalent s3 object, abs blob, gcs object, swift object, alicloud object, snapshot/backup of etcd on object store. Directory : As such there is no concept of directory in object store but usually the use directory as / separate common prefix for set of objects. Alternatively they use term folder for same. deletionGracePeriod: This means grace period or retention period for which backups will be persisted post deletion of shoot.  Current Spec: #BackupInfra spec Kind: BackupInfrastructure Spec:  seed: seedName  shootUID : shoot.status.uid Current naming conventions          SeedNamespace : Shoot–projectname–shootname   seed: seedname   ShootUID : shoot.status.UID   BackupInfraname: seednamespce+sha(uid)[:5]   Backup-bucket-name: BackupInfraName   BackupNamespace: backup–BackupInfraName    Proposal Considering Gardener extension proposal in mind, the backup infrastructure controller can be divided in two parts. There will be basically four backup infrastructure related CRD’s. Two on the garden apiserver. And two on the seed cluster. Before going into to workflow, let’s just first have look at the CRD.\nCRD on Garden cluster Just to give brief before going into the details, we will be sticking to the fact that Garden apiserver is always source of truth. Since backupInfra will be maintained post deletion of shoot, the info regarding this should always come from garden apiserver, we will continue to have BackupInfra resource on garden apiserver with some modifications.\napiVersion: garden.cloud/v1alpha1 kind: BackupBucket metadata:  name: packet-region1-uid[:5]  # No namespace needed. This will be cluster scope resource.  ownerReferences:  - kind: CloudProfile  name: packet spec:  provider: aws  region: eu-west-1  secretRef: # Required for root  name: backup-operator-aws  namespace: garden status:  lastOperation: ...  observedGeneration: ...  seed: ... apiVersion: garden.cloud/v1alpha1 kind: BackupEntry metadata:  name: shoot--dev--example--3ef42 # Naming convention explained before  namespace: garden-dev  ownerReferences:  - apiVersion: core.gardener.cloud/v1beta1  blockOwnerDeletion: false  controller: true  kind: Shoot  name: example  uid: 19a9538b-5058-11e9-b5a6-5e696cab3bc8 spec:  shootUID: 19a9538b-5058-11e9-b5a6-5e696cab3bc8 # Just for reference to find back associated shoot.  # Following section comes from cloudProfile or seed yaml based on granularity decision.  bucketName: packet-region1-uid[:5] status:  lastOperation: ...  observedGeneration: ...  seed: ... CRD on Seed cluster Considering the extension proposal, we want individual component to be handled by controller inside seed cluster. We will have Backup related resource in registered seed cluster as well.\napiVersion: extensions.gardener.cloud/v1alpha1 kind: BackupBucket metadata:  name: packet-random[:5]  # No namespace need. This will be cluster scope resource spec:  type: aws  region: eu-west-1  secretRef:  name: backup-operator-aws  namespace: backup-garden status:  observedGeneration: ...  state: ...  lastError: ..  lastOperation: ... There are two points for introducing BackupEntry resource.\n Cloud provider specific code goes completely in seed cluster. Network issue is also handled by moving deletion part to backup-extension-controller in seed cluster.  apiVersion: extensions.gardener.cloud/v1alpha1 kind: BackupEntry metadata:  name: shoot--dev--example--3ef42 # Naming convention explained later  # No namespace need. This will be cluster scope resource spec:  type: aws  region: eu-west-1  secretRef: # Required for root  name: backup-operator-aws  namespace: backup-garden status:  observedGeneration: ...  state: ...  lastError: ..  lastOperation: ... Workflow  Gardener administrator will configure the cloudProfile with backup infra credentials and provider config as follows.  # CloudProfile.yaml: Spec:  backup:  provider: aws  region: eu-west-1  secretRef:  name: backup-operator-aws  namespace: garden Here CloudProfileController will interpret this spec as follows:\n If spec.backup is nil  No backup for any shoot.   If spec.backup.region is not nil,  Then respect it, i.e. use the provider and unique region field mentioned there for BackupBucket. Here Preferably, spec.backup.region field will be unique, since for cross provider, it doesn’t make much sense. Since region name will be different for different providers.   Otherwise, spec.backup.region is nil then,  If same provider case i.e. spec.backup.provider = spec.(type-of-provider) or nil,  Then, for each region from spec.(type-of-provider).constraints.regions create a BackupBucket instance. This can be done lazily i.e. create BackupBucket instance for region only if some seed actually spawned in the region has been registered. This will avoid creating IaaS bucket even if no seed is registered in that region, but region is listed in cloudprofile. Shoot controller will choose backup container as per the seed region. (With shoot control plane migration also, seed’s availability zone might change but the region will be remaining same as per current scope.)   Otherwise cross provider case i.e. spec.backup.provider != spec.(type-of-provider)  Report validation error: Since, for example, we can’t expect spec.backup.provider = aws to support region in, spec.packet.constraint.region. Where type-of-provider is packet      Following diagram represent overall flow in details:\nReconciliation Reconciliation on backup entry in seed cluster mostly comes in picture at the time of deletion. But we can add initialization steps like creation of directory specific to shoot in backup bucket. We can simply create BackupEntry at the time of shoot deletion as well.\nDeletion  On shoot deletion, the BackupEntry instance i.e. shoot specific instance will get deletion timestamp because of ownerReference. If deletionGracePeriod configured in GCM component configuration is expired, BackupInfrastructure Controller will delete the backup folder associated with it from backup object store. Finally, it will remove the finalizer from backupEntry instance.  Alternative Discussion points / variations Manual vs dynamic bucket creation   As per limit observed on different cloud providers, we can have single bucket for backups on one cloud providers. So, we could avoid the little complexity introduced in above approach by pre-provisioning buckets as a part of landscape setup. But there won’t be anybody to detect bucket existence and its reconciliation. Ideally this should be avoided.\n  Another thing we can have is, we can let administrator register the pool of root backup infra resource and let the controller schedule backup on one of this.\n  One more variation here could be to create bucket dynamically per hash of shoot UID.\n  SDK vs Terraform Initial reason for going for terraform script is its stability and the provided parallelism/concurrency in resource creation. For backup infrastructure, Terraform scripts are very minimal right now. Its simply have bucket creation script. With shared bucket logic, if possible we might want to isolate access at directory level but again its additional one call. So, we will prefer switching to SDK for all object store operations.\nLimiting the number of shoots per bucket Again as per limit observed on different cloud providers, we can have single bucket for backups on one cloud providers. But if we want to limit the number of shoots associated with bucket, we can have central map of configuration in gardener-controller-component-configuration.yaml. Where we will mark supported count of shoots per cloud provider. Most probable space could be, controller.backupInfrastructures.quota. If limit is reached we can create new BucketBucket instance.\ne.g.\napiVersion: controllermanager.config.gardener.cloud/v1alpha1 kind: ControllerManagerConfiguration controllers:  backupInfrastructure:  quota:  - provider: aws  limit: 100 # Number mentioned here are random, just for example purpose.  - provider: azure  limit: 80  - provider: openstack  limit: 100  ... Backward compatibility Migration  Create shoot specific folder. Transfer old objects. Create manifest of objects on new bucket  Each entry will have status: None,Copied, NotFound. Copy objects one by one.   Scale down etcd-main with old config. ⚠️ Cluster down time Copy remaining objects Scale up etcd-main with new config. Destroy Old bucket and old backup namespace. It can be immediate or preferably lazy deletion.  Legacy Mode alternative  If Backup namespace present in seed cluster, then follow the legacy approach. i.e. reconcile creation/existence of shoot specific bucket and backup namespace. If backup namespace is not created, use shared bucket. Limitation Never know when the existing cluster will be deleted, and hence, it might be little difficult to maintain with next release of gardener. This might look simple and straight-forward for now but may become pain point in future, if in worst case, because of some new use cases or refactoring, we have to change the design again. Also, even after multiple garden release we won’t be able to remove deprecated existing BackupInfrastructure CRD  References  Gardener extension proposal Cloud providers object store limit comparison  ","categories":"","description":"","excerpt":"Backup Infrastructure CRD and Controller Redesign Goal  As an …","ref":"/docs/gardener/proposals/02-backupinfra/","tags":"","title":"02 Backupinfra"},{"body":"Network Extensibility Currently Gardener follows a mono network-plugin support model (i.e., Calico). Although this can seem to be the more stable approach, it does not completely reflect the real use-case. This proposal brings forth an effort to add an extra level of customizability to Gardener networking.\nMotivation Gardener is an open-source project that provides a nested user model. Basically, there are two types of services provided by Gardener to its users:\n Managed: users only request a Kubernetes cluster (Clusters-as-a-Service) Hosted: users utilize Gardener to provide their own managed version of Kubernetes (Cluster-Provisioner-as-a-service)  For the first set of users, the choice of network plugin might not be so important, however, for the second class of users (i.e., Hosted) it is important to be able to customize networking based on their needs.\nFurthermore, Gardener provisions clusters on different cloud-providers with different networking requirements. For example, Azure does not support Calico Networking [1], this leads to the introduction of manual exceptions in static add-on charts which is error prune and can lead to failures during upgrades.\nFinally, every provider is different, and thus the network always needs to adapt to the infrastructure needs to provider better performance. Consistency does not necessarily lie in the implementation but in the interface.\nGardener Network Extension The goal of the Gardener Network Extensions is to support different network plugin, therefore, the specification for the network resource won’t be fixed and will be customized based on the underlying network plugin. To do so, a NetworkConfig field in the spec will be provided where each plugin will define. Below is an example for deploy Calico as the cluster network plugin.\nLong Term Spec --- apiVersion: extensions.gardener.cloud/v1alpha1 kind: Network metadata:  name: calico-network  namespace: shoot--core--test-01 spec:  type: calico  clusterCIDR: 192.168.0.0/24  serviceCIDR: 10.96.0.0/24  providerConfig:  apiVersion: calico.extensions.gardener.cloud/v1alpha1  kind: NetworkConfig  ipam:  type: host-local  cidr: usePodCIDR  backend: bird  typha:  enabled: true status:  observedGeneration: ...  state: ...  lastError: ..  lastOperation: ...  providerStatus:  apiVersion: calico.extensions.gardener.cloud/v1alpha1  kind: NetworkStatus  components:  kubeControllers: true  calicoNodes: true  connectivityTests:  pods: true  services: true  networkModules:  arp_proxy: true  config:  clusterCIDR: 192.168.0.0/24  serviceCIDR: 10.96.0.0/24  ipam:  type: host-local  cidr: usePodCIDR First Implementation (Short Term) As an initial implementation the network plugin type will be specified by the user e.g., Calico (without further configuration in the provider spec). This will then be used to generate the Network resource in the seed. The Network operator will pick it up, and apply the configuration based on the spec.cloudProvider specified directly to the shoot or via the Gardener resource manager (still in the works).\nThe cloudProvider field in the spec is just an initial catalyst but not meant to be stay long-term. In the future, the network provider configuration will be customized to match the best needs of the infrastructure.\nHere is how the simplified initial spec would look like:\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: Network metadata:  name: calico-network  namespace: shoot--core--test-01 spec:  type: calico  cloudProvider: {aws,azure,...} status:  observedGeneration: 2  lastOperation: ...  lastError: ... Functionality The network resource need to be created early-on during cluster provisioning. Once created, the Network operator residing in every seed will create all the necessary networking resources and apply them to the shoot cluster.\nThe status of the Network resource should reflect the health of the networking components as well as additional tests if required.\nReferences [1] Azure support for Calico Networking\n","categories":"","description":"","excerpt":"Network Extensibility Currently Gardener follows a mono network-plugin …","ref":"/docs/gardener/proposals/03-networking-extensibility/","tags":"","title":"03 Networking Extensibility"},{"body":"Gardener Versioning Policy Please refer to this document for the documentation of the implementation of this GEP.\nGoal  As a Garden operator I would like to define a clear Kubernetes version policy, which informs my users about deprecated or expired Kubernetes versions. As an user of Gardener, I would like to get information which Kubernetes version is supported for how long. I want to be able to get this information via API (cloudprofile) and also in the Dashboard.  Motivation The Kubernetes community releases minor versions roughly every three months and usually maintains three minor versions (the actual and the last two) with bug fixes and security updates. Patch releases are done more frequently. Operators of Gardener should be able to define their own Kubernetes version policy. This GEP suggests the possibility for operators to classify Kubernetes versions, while they are going through their “maintenance life-cycle”.\nKubernetes Version Classifications An operator should be able to classify Kubernetes versions differently while they go through their “maintenance life-cycle”, starting with preview, supported, deprecated, and finally expired. This information should be programmatically available in the cloudprofiles of the Garden cluster as well as in the Dashboard. Please also note, that Gardener keeps the control plane and the workers on the same Kubernetes version.\nFor further explanation of the possible classifications, we assume that an operator wants to support four minor versions e.g. v1.16, v1.15, v1.14 and v1.13.\n  preview: After a fresh release of a new Kubernetes minor version (e.g. v1.17.0) the operator could tag it as preview until he has gained sufficient experience. It will not become the default in the Gardener Dashboard until he promotes that minor version to supported, which could happen a few weeks later with the first patch version.\n  supported: The operator would tag the latest Kubernetes patch versions of the actual (if not still in preview) and the last three minor Kubernetes versions as supported (e.g. v1.16.1, v1.15.4, v1.14.9 and v1.13.12). The latest of these becomes the default in the Gardener Dashboard (e.g. v1.16.1).\n  deprecated: The operator could decide, that he generally wants to classify every version that is not the latest patch version as deprecated and flag this versions accordingly (e.g. v1.16.0 and older, v1.15.3 and older, 1.14.8 and older as well as v1.13.11 and older). He could also tag all versions (latest or not) of every Kubernetes minor release that is neither the actual nor one of the last three minor Kubernetes versions as deprecated, too (e.g. v1.12.x and older). Deprecated versions will eventually expire (i.e., removed).\n  expired: This state is a logical state only. It doesn’t have to be maintained in the cloudprofile. All cluster versions whose expirationDate as defined in the cloudprofile is expired, are automatically in this logical state. After that date has passed, users cannot create new clusters with that version anymore and any cluster that is on that version will be forcefully migrated in its next maintenance time window, even if the owner has opted out of automatic cluster updates! The forceful update will pick the latest patch version of the current minor Kubernetes version. If the cluster was already on that latest patch version and the latest patch version is also expired, it will continue with latest patch version of the next minor Kubernetes version, so it will result in an update of a minor Kubernetes version, which is potentially harmful to your workload, so you should avoid that/plan ahead! If that’s expired as well, the update process repeats until a non-expired Kubernetes version is reached, so depending on the circumstances described above, it can happen that the cluster receives multiple consecutive minor Kubernetes version updates!\n  To fulfill his specific versioning policy, the Garden operator should be able to classify his versions as well set the expiration date in the cloudprofiles. The user should see this classifiers as well as the expiration date in the dashboard.\n","categories":"","description":"","excerpt":"Gardener Versioning Policy Please refer to this document for the …","ref":"/docs/gardener/proposals/05-versioning-policy/","tags":"","title":"05 Versioning Policy"},{"body":"Integrating etcd-druid with Gardener Etcd is currently deployed by garden-controller-manager as a Statefulset. The sidecar container spec contains details pertaining to cloud-provider object-store which is injected into the statefulset via a mutable webhook running as part of the gardener extension story. This approach restricts the operations on etcd such as scale-up and upgrade. Etcd-druid will eliminate the need to hijack statefulset creation to add cloudprovider details. It has been designed to provide an intricate control over the procedure of deploying and maintaining etcd. The roadmap for etcd-druid can be found here.\nThis document explains how Gardener deploys etcd and what resources it creates for etcd-druid to deploy an etcd cluster.\nResources required by etcd-druid (created by Gardener)  Secret containing credentials to access backup bucket in Cloud provider object store. TLS server and client secrets for etcd and backup-sidecar Etcd CRD resource that contains parameters pertaining to etcd, backup-sidecar and cloud-provider object store.  When an etcd resource is created in the cluster, the druid acts on it by creating an etcd statefulset, a service and a configmap containing etcd bootstrap script. The secrets containing the infrastructure credentials and the TLS certificates are mounted as volumes. If no secret/information regarding backups is stated then etcd data backups are not taken. Only data corruption checks are performed prior to starting etcd.\nGarden-controller-manager, being cloud agnostic, deploys the etcd resource. This will not contain any cloud-specific information other than the cloud-provider. The extension controller that contains the cloud specific implementation to create the backup bucket will create it if needed and create a secret containing the credentials to access the bucket. The etcd backup secret name should be exposed in the BackupEntry status. Then, Gardener can read it and write it into the ETCD resource. The secret will have to be made available in the namespace the etcd statefulset will be deployed. If etcd and backup-sidecar communicates over TLS then the CA certificates, server and client certificates, and keys will also have to be made available in the namespace as well. The etcd resource will have reference to these aforementioned secrets. etcd-druid will deploy the statefulset only if the secrets are available.\nWorkflow  etcd-druid will be deployed and etcd CRD will be created as part of the seed bootstrap. Garden-controller-manager creates backupBucket extension resource. Extension controller creates the backup bucket associated with the seed. Garden-controller-manager creates backupentry associated with each shoot in the seed namespace. Garden-controller-manager creates etcd resource with secretRefs and etcd information populated appropriately. etcd-druid acts on the etcd resource; druid creates the statefulset, the service and the configmap.  ","categories":"","description":"","excerpt":"Integrating etcd-druid with Gardener Etcd is currently deployed by …","ref":"/docs/gardener/proposals/06-etcd-druid/","tags":"","title":"06 Etcd Druid"},{"body":"Shoot Control Plane Migration Motivation Currently moving the control plane of a shoot cluster can only be done manually and requires deep knowledge of how exactly to transfer the resources and state from one seed to another. This can make it slow and prone to errors.\nAutomatic migration can be very useful in a couple of scenarios:\n Seed goes down and can’t be repaired (fast enough or at all) and it’s control planes need to be brought to another seed Seed needs to be changed, but this operation requires the recreation of the seed (e.g. turn a single-AZ seed into a multi-AZ seed) Seeds need to be rebalanced New seeds become available in a region closer to/in the region of the workers and the control plane should be moved there to improve latency Gardener ring, which is a self-supporting setup/underlay for a highly available (usually cross-region) Gardener deployment  Goals  Provide a mechanism to migrate the control plane of a shoot cluster from one seed to another The mechanism should support migration from a seed which is no longer reachable (Disaster Recovery) The shoot cluster nodes are preserved and continue to run the workload, but will talk to the new control plane after the migration completes Extension controllers implement a mechanism which allows them to store their state or to be restored from an already existing state on a different seed cluster. The already existing shoot reconciliation flow is reused for migration with minimum changes  Terminology Source Seed is the seed which currently hosts the control plane of a Shoot Cluster\nDestination Seed is the seed to which the control plane is being migrated\nResources and controller state which have to be migrated between two seeds: Note: The following lists are just FYI and are meant to show the current resources which need to be moved to the Destination Seed\nSecrets Gardener has preconfigured lists of needed secrets which are generated when a shoot is created and deployed in the seed. Following is a minimum set of secrets which must be migrated to the Destination Seed. Other secrets can be regenerated from them.\n ca ca-front-proxy static-token ca-kubelet ca-metrics-server etcd-encryption-secret kube-aggregator kube-apiserver-basic-auth kube-apiserver service-account-key ssh-keypair  Custom Resources and state of extension controllers Gardenlet deploys custom resources in the Source Seed cluster during shoot reconciliation which are reconciled by extension controllers. The state of these controllers and any additional resources they create is independent of the gardenlet and must also be migrated to the Destination Seed. Following is a list of custom resources, and the state which is generated by them that has to be migrated.\n BackupBucket: nothing relevant for migration BackupEntry: nothing relevant for migration ControlPlane: nothing relevant for migration DNSProvider/DNSEntry: nothing relevant for migration Extensions: migration of state needs to be handled individually Infrastructure: terraform state Network: nothing relevant for migration OperatingSystemConfig: nothing relevant for migration Worker: Machine-Controller-Manager related objects: machineclasses, machinedeployments, machinesets, machines  This list depends on the currently installed extensions and can change in the future\nProposal Custom Resource on the garden cluster The Garden cluster has a new Custom Resource which is stored in the project namespace of the Shoot called ShootState. It contains all the required data described above so that the control plane can be recreated on the Destination Seed.\nThis data is separated into two sections. The first is generated by the gardenlet and then either used to generate new resources (e.g secrets) or is directly deployed to the Shoot’s control plane on the Destination Seed.\nThe second is generated by the extension controllers in the seed.\napiVersion: core.gardener.cloud/v1alpha1 kind: ShootState metadata:  name: my-shoot  namespace: garden-core  ownerReference:  apiVersion: core.gardener.cloud/v1beta1  blockOwnerDeletion: true  controller: true  kind: Shoot  name: my-shoot  uid: ...  finalizers:  - gardener gardenlet:  secrets:  - name: ca  data:  ca.crt: ...  ca.key: ...  - name: ssh-keypair  data:  id_rsa: ...  - name: ... extensions: - kind: Infrastructure  state: ... (Terraform state) - kind: ControlPlane  purpose: normal  state: ... (Certificates generated by the extension) - kind: Worker  state: ... (Machine objects) The state data is saved as a runtime.RawExtension type, which can be encoded/decoded by the corresponding extension controller.\nThere can be sensitive data in the ShootState which has to be hidden from the end-users. Hence, it will be recommended to provide an etcd encryption configuration to the Gardener API server in order to encrypt the ShootState resource.\nSize limitations There are limits on the size of the request bodies sent to the kubernetes API server when creating or updating resources: by default ETCD can only accept request bodies which do not exceed 1.5 MiB (this can be configured with the --max-request-bytes flag); the kubernetes API Server has a request body limit of 3 MiB which cannot be set from the outside (with a command line flag); the gRPC configuration used by the API server to talk to ETCD has a limit of 2 MiB per request body which cannot be configured from the outside; and watch requests have a 16 MiB limit on the buffer used to stream resources.\nThis means that if ShootState is bigger than 1.5 MiB, the ETCD max request bytes will have to be increased. However, there is still an upper limit of 2 MiB imposed by the gRPC configuration.\nIf ShootState exceeds this size limitation it must make use of configmap/secret references to store the state of extension controllers. This is an implementation detail of Gardener and can be done at a later time if necessary as extensions will not be affected.\nSplitting the ShootState into multiple resources could have a positive benefit on performance as the Gardener API Server and Gardener Controller Manager would handle multiple small resources instead of one big resource.\nGardener extensions changes All extension controllers which require state migration must save their state in a new status.state field and act on an annotation gardener.cloud/operation=restore in the respective Custom Resources which should trigger a restoration operation instead of reconciliation. A restoration operation means that the extension has to restore its state in the Shoot’s namespace on the Destination Seed from the status.state field.\nAs an example: the Infrastructure resource must save the terraform state.\napiVersion: extensions.gardener.cloud/v1alpha1 kind: Infrastructure metadata: name: infrastructure namespace: shoot--foo--bar spec: type: azure region: eu-west-1 secretRef: name: cloudprovider namespace: shoot--foo--bar providerConfig: apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1 kind: InfrastructureConfig resourceGroup: name: mygroup networks: vnet: # specify either 'name' or 'cidr' # name: my-vnet cidr: 10.250.0.0/16 workers: 10.250.0.0/19 status: state: | { \"version\": 3, \"terraform_version\": \"0.11.14\", \"serial\": 2, \"lineage\": \"3a1e2faa-e7b6-f5f0-5043-368dd8ea6c10\", \"modules\": [ { } ] ... } Extensions which do not require state migration should set status.state=nil in their Custom Resources and trigger a normal reconciliation operation if the CR contains the core.gardener.cloud/operation=restore annotation.\nSimilar to the contract for the reconcile operation, the extension controller has to remove the restore annotation after the restoration operation has finished.\nAn additional annotation gardener.cloud/operation=migrate is added to the Custom Resources. It is used to tell the extension controllers in the Source Seed that they must stop reconciling resources (in case they are requeued due to errors) and should perform cleanup activities in the Shoot’s control plane. These cleanup activities involve removing the finalizers on Custom Resources and deleting them without actually deleting any infrastructure resources.\nNote: The same size limitations from the previous section are relevant here as well.\nShoot reconciliation flow changes The only data which must be stored in the ShootState by the gardenlet is secrets (e.g ca for the API server). Therefore the botanist.DeploySecrets step is changed. It is split into two functions which take a list of secrets that have to be generated.\n botanist.GenerateSecretState Generates certificate authorities and other secrets which have to be persisted in the ShootState and must not be regenerated on the Destination Seed. botanist.DeploySecrets Takes secret data from the ShootState, generates new ones (e.g. client tls certificates from the saved certificate authorities) and deploys everything in the Shoot’s control plane on the Destination Seed  ShootState synchronization controller The ShootState synchronization controller will become part of the gardenlet. It syncs the state of extension custom resources from the shoot namespace to the garden cluster and updates the corresponding spec.extension.state field in the ShootState resource. The controller can watch Custom Resources used by the extensions and update the ShootState only when changes occur.\nMigration workflow  Starting migration  Migration can only be started after a Shoot cluster has been successfully created so that the status.seed field in the Shoot resource has been set The Shoot resource’s field spec.seedName=\"new-seed\" is edited to hold the name of the Destination Seed and reconciliation is automatically triggered The Garden Controller Manager checks if the equality between spec.seedName and status.seed, detects that they are different and triggers migration.   The Garden Controller Manager waits for the Destination Seed to be ready Shoot’s API server is stopped Backup the Shoot’s ETCD. Extension resources in the Source Seed are annotated with gardener.cloud/operation=migrate Scale Down the Shoot’s control plane in the Source Seed. The gardenlet in the Destination Seed fetches the state of extension resources from the ShootState resource in the garden cluster. Normal reconciliation flow is resumed in the Destination Seed. Extension resources are annotated with gardener.cloud/operation=restore to instruct the extension controllers to reconstruct their state. The Shoot’s namespace in Source Seed is deleted.  ","categories":"","description":"","excerpt":"Shoot Control Plane Migration Motivation Currently moving the control …","ref":"/docs/gardener/proposals/07-shoot-control-plane-migration/","tags":"","title":"07 Shoot Control Plane Migration"},{"body":"Gardener integration test framework Motivation As we want to improve our code coverage in the next months we will need a simple and easy to use test framework. The current testframework already contains a lot of general test functions that ease the work for writing new tests. However there are multiple disadvantages with the current structure of the tests and the testframework:\n Every new test is an own testsuite and therefore needs its own TestDef (https://github.com/gardener/gardener/tree/master/.test-defs). With this approach there will be hundreds of test definitions, growing with every new test (or at least new test suite). But in most cases new tests do not need their own special TestDef: it’s just the wrong scope for the testmachinery and will result in unnecessary complex testruns and configurations. In addition it would result in additional maintenance for a huge number of TestDefs. The testsuites currently have their own specific interface/configuration that they need in order to be executed correctly (see K8s Update test). Consequently the configuration has to be defined in the testruns which result in one step per test with their very own configuration which means that the testmachinery cannot simply select testdefinitions by label. As the testmachinery cannot make use of its ability to run labeled tests (e.g. run all tests labeled default), the testflow size increases with every new tests and the testruns have to be manually adjusted with every new test. The current gardener test framework contains multiple test operations where some are just used for specific tests (e.g. plant_operations) and some are more general (garden_operation). Also the functions offered by the operations vary in their specialization as some are really specific to just one test e.g. shoot test operation with WaitUntilGuestbookAppIsAvailable whereas others are more general like WaitUntilPodIsRunning.\nThis structure makes it hard for developers to find commonly used functions and also hard to integrate as the common framework grows with specialized functions.  Goals In order to clean the testframework, make it easier for new developers to write tests and easier to add and maintain test execution within the testmachinery, the following goals are defined:\n Have a small number of test suites (gardener, shoots see test flavors) to only maintain a fixed number of testdefinitions. Use ginkgo test labels (inspired by the k8s e2e tests) to differentiate test behavior, test execution and test importance. Use standardized configuration for all tests (differ depending on the test suite) but provide better tooling to dynamically read additional configuration from configuration files like the cloudprofile. Clean the testframework to only contain general functionality and keep specific functions inside the tests  Proposal The proposed new test framework consists of the following changes to tackle the above described goals. ​\nTest Flavors Reducing the number of test definitions is done by ​combining the current specified test suites into the following 3 general ones:\n System test suite  e.g. create-shoot, delete-shoot, hibernate need their own testdef because they have a special meaning in the context of the testmachinery   Gardener test suite  e.g. RBAC, scheduler All tests that only need a gardener installation but no shoot cluster Possible functions/environment:  New project for test suite (copy secret binding, cleanup)?     Shoot test suite  e.g. shoot app, network Test that require a running shoot Possible functions:  Namespace per test cleanup of ns      As inspired by the k8s e2e tests, test labels are used to differentiate the tests by their behavior, their execution and their importance. Test labels means that tests are described using predefined labels in the test’s text (e.g ginkgo.It(\"[BETA] this is a test\")). With this labeling strategy, it is also possible to see the test properties directly in the code and promoting a test can be done via a pullrequest and will then be automatically recognized by the testmachinery with the next release.\nUsing ginkgo focus to only run desired tests and combined testsuites, an example test definition will look like the following.\nkind: TestDefinition metadata:  name: gardener-beta-suite spec:  description: Test suite that runs all gardener tests that are labeled as beta  activeDeadlineSeconds: 7200  labels: [\"gardener\", \"beta\"] ​  command: [bash, -c]  args:  - \u003e-go test -timeout=0 -mod=vendor ./test/integration/suite --v -ginkgo.v -ginkgo.progress -ginkgo.no-color -ginkgo.focus=\"[GARDENER] [BETA]\" Using this approach, the overall number of testsuites is then reduced to a fixed number (excluding the system steps) of test suites * labelCombinations.\nFramework The new framework will consist of a common framework, a gardener framework (integrating the commom framework) and a shoot framework (integrating the gardener framework).\nAll of these frameworks will have their own configuration that is exposed via commandline flags so that for example the shoot test framework can be executed by go test -timeout=0 -mod=vendor ./test/integration/suite --v -ginkgo.v -ginkgo.focus=\"[SHOOT]\" --kubecfg=/path/to/config --shoot-name=xx.\nThe available test labels should be declared in the code with predefined values and in a predefined order so that everyone is aware about possible labels and the tests are labeled similarly across all integration tests. This approach is somehow similar to what kubernetes is doing in their e2e test suite but with some more restrictions (compare example k8s e2e test).\nA possible solution to have consistent labeling would be to define them with every new ginkgo.It definition: f.Beta().Flaky().It(\"my test\") which internally orders them and would produce a ginkgo test with the text : [BETA] [FLAKY] my test.\nGeneral Functions The test framework should include some general functions that can and will be reused by every test. These general functions may include: ​\n Logging State Dump Detailed test output (status, duration, etc..) Cleanup handling per test (It) General easy to use functions like WaitUntilDeploymentCompleted, GetLogs, ExecCommand, AvailableCloudprofiles, etc.. ​  Example A possible test with the new test framework would look like:\nvar _ = ginkgo.Describe(\"Shoot network testing\", func() {  // the testframework registers some cleanup handling for a state dump on failure and maybe cleanup of created namespaces  f := framework.NewShootFramework()  f.CAfterEach(func(ctx context.Context) {  ginkgo.By(\"cleanup network test daemonset\")  err := f.ShootClient.Client().Delete(ctx, \u0026appsv1.DaemonSet{ObjectMeta: metav1.ObjectMeta{Name: name, Namespace: namespace}})  if err != nil {  if !apierrors.IsNotFound(err) {  Expect(err).To(HaveOccurred())  }  }  }, FinalizationTimeout)  f.Release().Default().CIt(\"should reach all webservers on all nodes\", func(ctx context.Context) {  ginkgo.By(\"Deploy the net test daemon set\")  templateFilepath := filepath.Join(f.ResourcesDir, \"templates\", nginxTemplateName)  err := f.RenderAndDeployTemplate(f.Namespace(), tempalteFilepath)  Expect(err).ToNot(HaveOccurred())  err = f.WaitUntilDaemonSetIsRunning(ctx, f.ShootClient.Client(), name, namespace)  Expect(err).NotTo(HaveOccurred())  pods := \u0026corev1.PodList{}  err = f.ShootClient.Client().List(ctx, pods, client.MatchingLabels{\"app\": \"net-nginx\"})  Expect(err).NotTo(HaveOccurred())  // check if all webservers can be reached from all nodes  ginkgo.By(\"test connectivity to webservers\")  shootRESTConfig := f.ShootClient.RESTConfig()  var res error  for _, from := range pods.Items {  for _, to := range pods.Items {  // test pods  f.Logger.Infof(\"%s to %s: %s\", from.GetName(), to.GetName(), data)  }  }  Expect(res).ToNot(HaveOccurred())  }, NetworkTestTimeout) }) Future Plans Ownership When the test coverage is increased and there will be more tests, we will need to track ownership for tests. At the beginning the ownership will be shared across all maintainers of the residing repository but this is not suitable anymore as tests will grow and get more complex.\nTherefore the test ownership should be tracked via subgroups (in kubernetes this would be a SIG (comp. sig apps e2e test)). These subgroup will then be tracked via labels and the members of these groups will then be notified if tests fail.\n","categories":"","description":"","excerpt":"Gardener integration test framework Motivation As we want to improve …","ref":"/docs/gardener/proposals/09-test-framework/","tags":"","title":"09 Test Framework"},{"body":"Gardener extensibility to support shoot additional container runtimes Table of Contents  Gardener extensibility to support shoot additional container runtimes  Table of Contents Summary Motivation  Goals   Proposal Design Details    Summary Gardener-managed Kubernetes clusters are sometimes used to run sensitive workloads, which sometimes are comprised of OCI images originating from untrusted sources. Additional use-cases want to leverage economy-of-scale to run workloads for multiple tenants on the same cluster. In some cases, Gardener users want to use operating systems which do not easily support the Docker engine.\nThis proposal aims to allow Gardener Shoot clusters to use CRI instead of the legacy Docker API, and to provide extension type for adding CRI shims (like GVisor and Kata Containers) which can be used to add support in Gardener Shoot clusters for these runtimes.\nMotivation While pods and containers are intended to create isolated areas for concurrently running workloads on nodes, this isolation is not as robust as could be expected. Containers leverage the core Linux CGroup and Namespace features to isolate workloads, and many kernel vulnerabilities have the potential to allow processes to escape from their isolation. Once a process has escaped from its container, any other process running on the same node is compromised. Several projects try to mitigate this problem; for example Kata Containers allow isolating a Kubernetes Pod in a micro-vm, gVisor reduces the kernel attack surface by adding another level of indirection between the actual payload and the real kernel.\nKubernetes supports running pods using these alternate runtimes via the RuntimeClass concept, which was promoted to Beta in Kubernetes 1.14. Once Kubernetes is configured to use the Container Runtime Interface to control pods, it becomes possible to leverage CRI and run specific pods using different Runtime Classes. Additionally, configuring Kubernetes to use CRI instead of the legacy Dockershim is faster.\nThe motivation behind this proposal is to make all of this functionality accessible to Shoot clusters managed by Gardener.\nGoals  Gardener must allow to configue its managed clusters with the CRI interface instead of the legacy Dockershim. Low-level runtimes like gVisor or Kata Containers are provided as gardener extensions which are (optionally) installed into a landscape by the Gardener operator. There must be no runtime-specific knowledge in the core Gardener code. It shall be possible to configure multiple low-level runtimes in Shoot clusters, on the Worker Group level.  Proposal Gardener today assumes that all supported operating systems have Docker pre-installed in the base image. Starting with Docker Engine 1.11, Docker itself was refactored and cleaned-up to be based on the containerd library. The first phase would be to allow the change of the Kubelet configuration as described here so that Kubernetes would use containerd instead of the default Dockershim. This will be implemented for CoreOS, Ubuntu, and SuSE-CHost.\nWe will implement two Gardener extensions, providing gVisor and Kata Containers as options for Gardener landscapes. The WorkerGroup specification will be extended to allow specifying the CRI name and a list of additional required Runtimes for nodes in that group. For example:\nworkers: - name: worker-b8jg5  machineType: m5.large  volumeType: gp2  volumeSize: 50Gi  autoScalerMin: 1  autoScalerMax: 2  maxSurge: 1  cri:  name: containerd  containerRuntimes:  - type: gvisor  - type: kata-containers  machineImage:  name: coreos  version: 2135.6.0 Each extension will need to address the following concern:\n Add the low-level runtime binaries to the worker nodes. Each extension should get the runtime binaries from a container. Hook the runtime binary into the containerd configuration file, so that the runtime becomes available to containerd. Apply a label to each node that allows identifying nodes where the runtime is available. Apply the relevant RuntimeClass to the Shoot cluster, to expose the functionality to users. Provide a separate binary with a ValidatingWebhook (deployable to the garden cluster) to catch invalid configurations. For example, Kata Containers on AWS requires a machineType of i3.metal, so any Shoot requests with a Kata Containers runtime and a different machine type on AWS should be rejected.  Design Details   Change the nodes container runtime to work with CRI and ContainerD (Only if specified in the Shoot spec):\n  In order to configure each worker machine in the cluster to work with CRI, the following configurations should be done:\n Add kubelet execution flags:  –container-runtime=remote –container-runtime-endpoint=unix:///run/containerd/containerd.sock   Make sure that default containerd configuration file exist in path /etc/containerd/config.toml.    ContainerD and Docker configurations are different for each OS. To make sure the default configurations above works well in each worker machine, each OS extension would be responsible to configure them during the reconciliation of the OperatingSystemConfig:\n os-ubuntu -  Create ContainerD unit Drop-In to execute ContainerD with the default configurations file in path /etc/containerd/config.toml. Create the container runtime metadata file with a OS path for binaries installations: /usr/bin.   os-coreos -  Create ContainerD unit Drop-In to execute ContainerD with the default configurations file in path /etc/containerd/config.toml. Create Docker Drop-In unit to execute Docker with the correct socket path of ContainerD. Create the container runtime metadata file with a OS path for binaries installations: /var/bin.   os-suse-chost -  Create ContainerD service unit and execute ContainerD with the default configurations file in path /etc/containerd/config.toml. Download and install ctr-cli which is not shipped with the current SuSe image. Create the container runtime metadata file with a OS path for binaries installations /usr/sbin.      To rotate the ContainerD (CRI) logs we will activate the kubelet feature flag: CRIContainerLogRotation=true.\n  Docker monitor service will be replaced with equivalent ContainerD monitor service.\n    Validate workers additional runtime configurations:\n Disallow additional runtimes with shoots \u003c 1.14 kata-container validation: Machine type support nested virtualization.    Add support for each additional container runtime in the cluster.\n  In order to install each additional available runtime in the cluster we should:\n Install the runtime binaries in each Worker’s pool nodes that specified the runtime support. Apply the relevant RuntimeClass to the cluster.    The installation above should be done by a new kind of extension: ContainerRuntime resource. For each container runtime type (Kata-container/gvisor) a dedicate extension controller will be created.\n  A label for each container runtime support will be added to every node that belongs to the worker pool. This should be done similar to the way labels created today for each node, through kubelet execution parameters (_kubelet.flags: –node-labels). When creating the OperatingSystemConfig (original) for the worker each container runtime support should be mapped to a label on the node. For Example: label: container.runtime.kata-containers=true (shoot.spec.cloud..worker.containerRuntimes.kata-container) label: container.runtime.gvisor=true (shoot.spec.cloud..worker.containerRuntimes.gvisor)\n  During the Shoot reconciliation (Similar steps to the Extensions today) Gardener will create new ContainerRuntime resource if a container runtime exist in at least one worker spec:\napiVersion: extensions.gardener.cloud/v1alpha1 kind: ContainerRuntime metadata:  name: kata-containers-runtime-extension  namespace: shoot--foo--bar spec:  type: kata-containers Gardener will wait that all ContainerRuntimes extensions will be reconciled by the appropriate extensions controllers.\n  Each runtime extension controller will be responsible to reconcile it’s RuntimeContainer resource type. rc-kata-containers extension controller will reconcile RuntimeContainer resource from type kata-container and rc-gvisor will reconcile RuntimeContainer resource from gvisor. Reconciliation process by container runtime extension controllers:\n Runtime extension controller from specific type should apply a chart which responsible for the installation of the runtime container in the cluster:  DaemonSet which will run a privileged pod on each node with the label: container.runtime.:true The pod will be responsible for:  Copy the runtime container binaries (From extension package ) to the relevant path in the host OS. Add the relevant container runtime plugin section to the containerd configuration file (/etc/containerd/config.toml). Restart containerd in the node.   RuntimeClasses in the cluster to support the runtime class. for example: apiVersion: node.k8s.io/v1beta1 kind: RuntimeClass metadata:  name: gvisor handler: runsc    Update the status of the relevant RuntimeContainer resource to succeeded.        ","categories":"","description":"","excerpt":"Gardener extensibility to support shoot additional container runtimes …","ref":"/docs/gardener/proposals/10-shoot-additional-container-runtimes/","tags":"","title":"10 Shoot Additional Container Runtimes"},{"body":"OIDC Webhook Authenticator Problem In Kubernetes you can authenticate via several authentication strategies:\n x509 Client Certificates Static Token Files Bootstrap Tokens Static Password File (Basic authentication - deprecated and removed in 1.19) Service Account Tokens OpenID Connect Tokens Webhook Token Authentication Authenticating Proxy  End-users should use OpenID Connect (OIDC) Tokens created by OIDC-compatible Identity Provider (IDP) and present id_token to the kube-apiserver. If the kube-apiserver is configured to trust the IDP and the token is valid, then the user is authenticated and the UserInfo is send to the authorization stack.\nIdeally, operators of the Gardener cluster should be able to authenticate to end-user Shoot clusters with id_token generated by OIDC IDP, but in many cases, end-users might have already configured OIDC for their cluster and more than one OIDC configurations are not allowed.\nAnother interesting application of multiple OIDC providers would be per Project OIDC provider where end-users of Gardener can add their own OIDC-compatible IDPs.\nTo workaround the one OIDC per kube-apiserver limitation, a new OIDC Webhook Authenticator (OWA) could be implemented.\nGoals  Dynamic registrations of OpenID Connect configurations. Close as possible to the Kubernetes build-in OIDC Authenticator. Build as an optional extension and not required for functional Shoot or Gardener cluster.  Non-goals  Dynamic Authorization is out of scope.  Proposal The kube-apiserver can use Webhook Token Authentication to send a Bearer Tokens (id_token) to external webhook for validation:\n{  \"apiVersion\": \"authentication.k8s.io/v1beta1\",  \"kind\": \"TokenReview\",  \"spec\": {  \"token\": \"(BEARERTOKEN)\"  } } Where upon verification, the remote webhook returns the identity of the user (if authentication succeeds):\n{  \"apiVersion\": \"authentication.k8s.io/v1beta1\",  \"kind\": \"TokenReview\",  \"status\": {  \"authenticated\": true,  \"user\": {  \"username\": \"janedoe@example.com\",  \"uid\": \"42\",  \"groups\": [  \"developers\",  \"qa\"  ],  \"extra\": {  \"extrafield1\": [  \"extravalue1\",  \"extravalue2\"  ]  }  }  } } Registration of new OpenIDConnect This new OWA can be configured with multiple OIDC providers and the entire flow can look like this:\n  Admin adds a new OpenIDConnect resource (via CRD) to the cluster.\napiVersion: authentication.gardener.cloud/v1alpha1 kind: OpenIDConnect metadata:  name: foo spec:  issuerURL: https://foo.bar  clientID: some-client-id  usernameClaim: email  usernamePrefix: \"test-\"  groupsClaim: groups  groupsPrefix: \"baz-\"  supportedSigningAlgs:  - RS256  requiredClaims:  baz: bar  caBundle: LS0tLS1CRUdJTiBDRVJU...base64-encoded CA certs for issuerURL.   OWA watches for changes on this resource and does OIDC discovery. The OIDC provider’s configuration has to be accessible under the spec.issuerURL with a well-known path (.well-known/openid-configuration).\n  OWA uses the jwks_uri obtained from the OIDC providers configuration, to fetch the OIDC provider’s public keys from that endpoint.\n  OWA uses those keys, issuer, client_id and other settings to add an OIDC authenticator to an in-memory list of Token Authenticators.\n  End-user authentication via new OpenIDConnect IDP When a user presents an id_token obtained from a OpenID Connect the flow looks like this:\n  The user authenticates against a Custom IDP.\n  id_token is obtained from the Custom IDP.\n  The user uses id_token to perform an API call to kube-apiserver.\n  As the id_token is not matched by any build-in or configured authenticators in the kube-apiserver, it is send to OWA for validation.\n{  \"TokenReview\": {  \"kind\": \"TokenReview\",  \"apiVersion\": \"authentication.k8s.io/v1beta1\",  \"spec\": {  \"token\": \"ddeewfwef...\"  }  } }   OWA uses TokenReview to authenticate the calling API server (the kube-apiserver for delegation of authentication and authorization may be different from the calling kube-apiserver).\n{  \"TokenReview\": {  \"kind\": \"TokenReview\",  \"apiVersion\": \"authentication.k8s.io/v1beta1\",  \"spec\": {  \"token\": \"api-server-token...\"  }  } }   After the Authentication API server returns the identity of the calling API server:\n{  \"apiVersion\": \"authentication.k8s.io/v1\",  \"kind\": \"TokenReview\",  \"metadata\": {  \"creationTimestamp\": null  },  \"spec\": {  \"token\": \"eyJhbGciOiJSUzI1NiIsImtpZCI6InJocEdLTXZlYjV1OE5heD...\"  },  \"status\": {  \"authenticated\": true,  \"user\": {  \"groups\": [  \"system:serviceaccounts\",  \"system:serviceaccounts:shoot--abcd\",  \"system:authenticated\"  ],  \"uid\": \"14db103e-88bb-4fb3-8efd-ca9bec91c7bf\",  \"username\": \"system:serviceaccount:shoot--abcd:kube-apiserver\"  }  } } OWA makes a SubjectAccessReview call to the Authorization API server to ensure that calling API server is allowed to validate tokens:\n{  \"apiVersion\": \"authorization.k8s.io/v1\",  \"kind\": \"SubjectAccessReview\",  \"spec\": {  \"groups\": [  \"system:serviceaccounts\",  \"system:serviceaccounts:shoot--abcd\",  \"system:authenticated\"  ],  \"nonResourceAttributes\": {  \"path\": \"/validate-token\",  \"verb\": \"post\"  },  \"user\": \"system:serviceaccount:shoot--abcd:kube-apiserver\"  },  \"status\": {  \"allowed\": true,  \"reason\": \"RBAC: allowed by RoleBinding \\\"kube-apiserver\\\" of ClusterRole \\\"kube-apiserver\\\" to ServiceAccount \\\"system:serviceaccount:shoot--abcd:kube-apiserver\\\"\"  } }   OWA then iterates over all registered OpenIDConnect Token authenticators and tries to validate the token.\n  Upon a successful validation it returns the TokeReview with user, groups and extra parameters:\n{  \"TokenReview\": {  \"kind\": \"TokenReview\",  \"apiVersion\": \"authentication.k8s.io/v1beta1\",  \"spec\": {  \"token\": \"ddeewfwef...\"  },  \"status\": {  \"authenticated\": true,  \"user\": {  \"username\": \"test-foo@bar.com\",  \"groups\": [  \"baz-employee\"  ],  \"extra\": {  \"gardener.cloud/authenticator/name\": [  \"foo\"  ],  \"gardener.cloud/authenticator/uid\": [  \"e5062528-e5a4-4b97-ad83-614d015b0979\"  ]  }  }  }  } } It also adds some extra information which can be used by custom authorizers later on:\n gardener.cloud/authenticator/name contains the name of the OpenIDConnect authenticator which was used. gardener.cloud/authenticator/uid contains the metadata.uid of the OpenIDConnect authenticator which was used.    The kube-apiserver proceeds with authorization checks and returns response.\n  An overview of the flow:\nDeployment for Shoot clusters OWA can be deployed per Shoot cluster via the Shoot OIDC Service Extension. The shoot’s kube-apiserver is mutated so that it has the following flag configured.\n--authentication-token-webhook-config-file=/etc/webhook/kubeconfig OWA on the other hand uses the shoot’s kube-apiserver and delegates auth capabilities to it. This means that the needed RBAC is managed in the shoot cluster. By default only the shoot’s kube-apiserver has permissions to validate tokens against OWA.\n","categories":"","description":"","excerpt":"OIDC Webhook Authenticator Problem In Kubernetes you can authenticate …","ref":"/docs/gardener/proposals/12-oidc-webhook-authenticator/","tags":"","title":"12 Oidc Webhook Authenticator"},{"body":"Automated Seed Management Automated seed management involves automating certain aspects of managing seeds in Garden clusters, such as:\n Ensuring that the seeds capacity for shoots is not exceeded Creating, deleting, and updating seeds declaratively as “managed seeds” Declaratively managing sets of similar “managed seeds” as “managed seed sets” which can be scaled up/down Auto-scaling seeds upon reaching capacity thresholds  Implementing the above features would involve changes to various existing Gardener components, as well as perhaps introducing new ones. This document describes these features in more detail and proposes a design approach for some of them.\nIn Gardener, scheduling shoots onto seeds is quite similar to scheduling pods onto nodes in Kubernetes. Therefore, a guiding principle behind the proposed design approaches is taking advantage of best practices and existing components already used in Kubernetes.\nEnsuring Seeds Capacity for Shoots Is Not Exceeded Seeds have a practical limit of how many shoots they can accommodate. Exceeding this limit is undesirable as the system performance will be noticeably impacted. Therefore, it is important to ensure that a seed’s capacity for shoots is not exceeded by introducing a maximum number of shoots that can be scheduled onto a seed and making sure that it is taken into account by the scheduler.\nAn initial discussion of this topic is available in Issue #2938. The proposed solution is based on the following flow:\n The gardenlet is configured with certain resources and their total capacity (and, for certain resources, the amount reserved for Gardener). The gardenlet seed controller updates the Seed status with the capacity of each resource and how much of it is actually available to be consumed by shoots, using capacity and allocatable fields that are very similar to the corresponding fields in the Node status. When scheduling shoots, gardener-scheduler is influenced by the remaining capacity of the seed. In the simplest possible implementation, it never schedules shoots onto a seed that has already reached its capacity for a resource needed by the shoot.  Initially, the only resource considered would be the maximum number of shoots that can be scheduled onto a seed. Later, more resources could be added to make more precise scheduling calculations.\nNote: Resources could also be requested by shoots, similarly to how pods can request node resources, and the scheduler could then ensure that such requests are taken into account when scheduling shoots onto seeds. However, the user is rarely, if at all, concerned with what resources does a shoot consume from a seed, and this should also be regarded as an implementation detail that could change in the future. Therefore, such resource requests are not included in this GEP.\nIn addition, an extensibility plugin framework could be introduced in the future in order to advertise custom resources, including provider-specific resources, so that gardenlet would be able to update the seed status with their capacity and allocatable values, for example load balancers on Azure. Such a concept is not described here in further details as it is sufficiently complex to require a separate GEP.\nExample Seed status with capacity and allocatable fields:\nstatus:  capacity:  shoots: \"100\"  persistent-volumes: \"200\" # Built-in resource  azure.provider.extensions.gardener.cloud/load-balancers: \"30\" # Custom resource advertised by an Azure-specific plugin  allocatable:  shoots: \"100\"  persistent-volumes: \"197\" # 3 persistent volumes are reserved for Gardener  azure.provider.extensions.gardener.cloud/load-balancers: \"300\" Gardenlet Configuration As mentioned above, the total resource capacity for built-in resources such as the number of shoots is specified as part of the gardenlet configuration, not in the Seed spec. The gardenlet configuration itself could be specified in the spec of the newly introduced ManagedSeed resource. Here it is assumed that in the future this could become the recommended and most widely used way to manage seeds. If the same gardenlet is responsible for multiple seeds, they would all share the same capacity settings.\nTo specify the total resource capacity for built-in resources, as well as the amount of such resources reserved for Gardener, the 2 new fields resources.capacity and resources.reserved are introduced in the GardenletConfiguration resource. The gardenlet seed controller would then initialize the capacity and allocatable fields in the seed status as follows:\n The capacity value is set to the configured resources.capacity. The allocatable value is set to the configured resources.capacity minus resources.reserved.  Example GardenletConfiguration with resources.capacity and resources.reserved field:\nresources:  capacity:  shoots: 100  persistent-volumes: 200  reserved:  persistent-volumes: 3 Scheduling Algorithm Currently gardener-scheduler uses a simple non-extensible algorithm in order to schedule shoots onto seeds. It goes through the following stages:\n Filter out seeds that don’t meet scheduling requirements such as being ready, matching cloud profile and shoot label selectors, matching the shoot provider, and not having taints that are not tolerated by the shoot. From the remaining seeds, determine candidates that are considered best based on their region, by using a strategy that can be either “same region” or “minimal distance”. Among these candidates, choose the one with the least number of shoots.  This scheduling algorithm should be adapted in order to properly take into account resources capacity and requests. As a first step, during the filtering stage, any seeds that would exceed their capacity for shoots, or their capacity for any resources requested by the shoot, should simply be filtered out and not considered during the next stages.\nLater, the scheduling algorithm could be further enhanced by replacing the step in which the region strategy is applied by a scoring step similar to the one in Kubernetes Scheduler. In this scoring step, the scheduler would rank the remaining seeds to choose the most suitable shoot placement. It would assign a score to each seed that survived filtering based on a list of scoring rules. These rules might include for example MinimalDistance and SeedResourcesLeastAllocated, among others. Each rule would produce its own score for the seed, and the overall seed score would be calculated as a weighted sum of all such scores. Finally, the scheduler would assign the shoot to the seed with the highest ranking.\nManagedSeeds When all or most of the existing seeds are near capacity, new seeds should be created in order to accommodate more shoots. Conversely, sometimes there could be too many seeds for the number of shoots, and so some of the seeds could be deleted to save resources. Currently, the process of creating a new seed involves a number of manual steps, such as creating a new shoot that meets certain criteria, and then registering it as a seed in Gardener. This could be automated to some extent by annotating a shoot with the use-as-seed annotation, in order to create a “shooted seed”. However, adding more than one similar seeds still requires manually creating all needed shoots, annotating them appropriately, and making sure that they are successfully reconciled and registered.\nTo create, delete, and update seeds effectively in a declarative way and allow auto-scaling, a “creatable seed” resource along with a “set” (and in the future, perhaps also a “deployment”) of such creatable seeds should be introduced, similar to Kubernetes Pod, ReplicaSet, and Deployment (or to MCM Machine, MachineSet, and MachineDeployment) resources. With such resources (and their respective controllers), creating a new seed based on a template would become as simple as increasing the replicas field in the “set” resource.\nIn Issue #2181 it is already proposed that the use-as-seed annotation is replaced by a dedicated ShootedSeed resource. The solution proposed here further elaborates on this idea.\nManagedSeed Resource The ManagedSeed resource is a dedicated custom resource that represents an evolution of the “shooted seed” and properly replaces the use-as-seed annotation. This resource contains:\n The name of the Shoot that should be registered as a Seed. An optional seedTemplate section that contains the Seed spec and parts of the metadata, such as labels and annotations. An optional gardenlet section that contains:  gardenlet deployment parameters, such as the number of replicas, the image, etc. The GardenletConfiguration resource that contains controllers configuration, feature gates, and a seedConfig section that contains the Seed spec and parts of its metadata. Additional configuration parameters, such as the garden connection bootstrap mechanism (see TLS Bootstrapping), and whether to merge the provided configuration with the configuration of the parent gardenlet.    Either the seedTemplate or the gardenlet section must be specified, but not both:\n If the seedTemplate section is specified, gardenlet is not deployed to the shoot, and a new Seed resource is created based on the template. If the gardenlet section is specified, gardenlet is deployed to the shoot, and it registers a new seed upon startup based on the seedConfig section of the GardenletConfiguration resource.  A ManagedSeed allows fine-tuning the seed and the gardenlet configuration of shooted seeds in order to deviate from the global defaults, e.g. lower the concurrent sync for some of the seed’s controllers or enable a feature gate only on certain seeds. Also, it simplifies the deletion protection of such seeds.\nAlso, the ManagedSeed resource is a more powerful alternative to the use-as-seed annotation. The implementation of the use-as-seed annotation itself could be refactored to use a ManagedSeed resource extracted from the annotation by a controller.\nAlthough in this proposal a ManagedSeed is always a “shooted seed”, that is a Shoot that is registered as a Seed, this idea could be further extended in the future by adding a type field that could be either Shoot (implied in this proposal), or something different. Such an extension would allow to register and manage as Seed a cluster that is not a Shoot, e.g. a GKE cluster.\nLast but not least, ManagedSeeds could be used as the basis for creating and deleting seeds automatically via the ManagedSeedSet resource that is described in ManagedSeedSets.\nUnlike the Seed resource, the ManagedSeed resource is namespaced. If created in the garden namespace, the resulting seed is globally available. If created in a project namespace, the resulting seed can be used as a “private seed” by shoots in the project, either by being decorated with project-specific taints and labels, or by being of the special PrivateSeed kind that is also namespaced. The concept of private seeds / cloudprofiles is described in Issue #2874. Until this concept is implemented, ManagedSeed resources might need to be restricted to the garden namespace, similarly to how shoots with the use-as-seed annotation currently are.\nExample ManagedSeed resource with a seedTemplate section:\napiVersion: seedmanagement.gardener.cloud/v1alpha1 kind: ManagedSeed metadata:  name: crazy-botany  namespace: garden spec:  shoot:  name: crazy-botany # Shoot that should be registered as a Seed  seedTemplate: # Seed template, including spec and parts of the metadata  metadata:  labels:  foo: bar  spec:  provider:  type: gcp  region: europe-west1  taints:  - key: seed.gardener.cloud/protected  ... Example ManagedSeed resource with a gardenlet section:\napiVersion: seedmanagement.gardener.cloud/v1alpha1 kind: ManagedSeed metadata:  name: crazy-botany  namespace: garden spec:  shoot:  name: crazy-botany # Shoot that should be registered as a Seed  gardenlet:  deployment: # Gardenlet deployment configuration  replicaCount: 1  revisionHistoryLimit: 10  serviceAccountName: gardenlet  image:  repository: eu.gcr.io/gardener-project/gardener/gardenlet  tag: latest  pullPolicy: IfNotPresent  resources:  ...  podLabels:  ...  podAnnotations:  ...  additionalVolumes:  ...  additionalVolumeMounts:  ...  env:  ...  vpa: false  config: # GardenletConfiguration resource  apiVersion: gardenlet.config.gardener.cloud/v1alpha1  kind: GardenletConfiguration  seedConfig: # Seed template, including spec and parts of the metadata  metadata:  labels:  foo: bar  spec:  provider:  type: gcp  region: europe-west1  taints:  - key: seed.gardener.cloud/protected  ...  controllers:  shoot:  concurrentSyncs: 20  featureGates:  ...  ...  bootstrap: BootstrapToken  mergeWithParent: true ManagedSeed Controller ManagedSeeds are reconciled by a new managed seed controller in gardenlet. Its implementation is very similar to the current seed registration controller, and in fact could be regarded as a refactoring of the latter, with the difference that it uses the ManagedSeed resource rather than the use-as-seed annotation on a Shoot. The gardenlet only reconciles ManagedSeeds that refer to Shoots scheduled on Seeds the gardenlet is responsible for.\nOnce this controller is considered sufficiently stable, the current use-as-seed annotation and the controller mentioned above should be marked as deprecated and eventually removed.\nA ManagedSeed that is in use by shoots cannot be deleted, unless the shoots are either deleted or moved to other seeds first. The managed seed controller ensures that this is the case by only allowing a ManagedSeed to be deleted if its Seed has been already deleted.\nManagedSeed Admission Plugins In addition to the managed seed controller mentioned above, new gardener-apiserver admission plugins should be introduced to properly validate the creation and update of ManagedSeeds, as well as the deletion of shoots registered as seeds. These plugins should ensure that:\n A Shoot that is being referred to by a ManagedSeed cannot be deleted. Certain Seed spec fields, for example the provider type and region, networking CIDRs for pods, services, and nodes, etc., are the same as (or compatible with) the corresponding Shoot spec fields of the shoot that is being registered as seed. If such Seed spec fields are omitted or empty, the plugins should supply proper defaults based on the values in the Shoot resource.  Provider-specific Seed Bootstrapping Actions Bootstrapping a new seed might require additional provider-specific actions to the ones performed automatically by the managed seed controller. For example, on Azure this might include getting a new subscription, extending quotas, etc. This could eventually be automated by introducing an extension mechanism for the Gardener seed bootstrapping flow, to be handled by a new type of controller in the provider extensions. However, such an extension mechanism is not in the scope of this proposal and might require a separate GEP.\nOne idea that could be further explored is the use shoot readiness gates, similar to Kubernetes pod readiness gates, in order to control whether a Shoot is considered Ready before it could be registered as a Seed. A provider-specific extension could set the special condition that is specified as a readiness gate to True only after it has successfully performed the provider-specific actions needed.\nChanges to Existing Controllers Since the Shoot registration as a Seed is decoupled from the Shoot reconciliation, existing gardenlet controllers would not have to be changed in order to properly support ManagedSeeds. The main change to gardenlet that would be needed is introducing the new managed seed controller mentioned above, and possibly retiring the old one at some point. In addition, the Shoot controller would need to be adapted as it currently performs certain actions differently if the shoot has a “shooted seed”.\nThe introduction of the ManagedSeed resource would also require no changes to existing gardener-controller-manager controllers that operate on Shoots (for example, shoot hibernation and maintenance controllers).\nManagedSeedSets Similarly to a ReplicaSet, the purpose of a ManagedSeedSet is to maintain a stable set of replica ManagedSeeds available at any given time. As such, it is used to guarantee the availability of a specified number of identical ManagedSeeds, on an equal number of identical Shoots.\nManagedSeedSet Resource The ManagedSeedSet resource has a selector field that specifies how to identify ManagedSeeds it can acquire, a number of replicas indicating how many ManagedSeeds (and their corresponding Shoots) it should be maintaining, and a two templates:\n A ManagedSeed template (template) specifying the data of new ManagedSeeds it should create to meet the number of replicas criteria. A Shoot template (shootTemplate) specifying the data of new Shoots it should create to host the ManagedSeeds.  A ManagedSeedSet then fulfills its purpose by creating and deleting ManagedSeeds (and their corresponding Shoots) as needed to reach the desired number.\nA ManagedSeedSet is linked to its ManagedSeeds and Shoots via the metadata.ownerReferences field, which specifies what resource the current object is owned by. All ManagedSeeds and Shoots acquired by a ManagedSeedSet have their owning ManagedSeedSet’s identifying information within their ownerReferences field.\nExample ManagedSeedSet resource:\napiVersion: seedmanagement.gardener.cloud/v1alpha1 kind: ManagedSeedSet metadata:  name: crazy-botany  namespace: garden spec:  replicas: 3  selector:  matchLabels:  foo: bar  updateStrategy:  type: RollingUpdate # Update strategy, must be `RollingUpdate`  rollingUpdate:  partition: 2 # Only update the last replica (#2), assuming there are no gaps (\"rolling out a canary\")  template: # ManagedSeed template, including spec and parts of the metadata  metadata:  labels:  foo: bar  spec:  # shoot.name is not specified since it's filled automatically by the controller  seedTemplate: # Either a seed or a gardenlet section must be specified, see above  metadata:  labels:  foo: bar  provider:  type: gcp  region: europe-west1  taints:  - key: seed.gardener.cloud/protected  ...  shootTemplate: # Shoot template, including spec and parts of the metadata  metadata:  labels:  foo: bar  spec:  cloudProfileName: gcp  secretBindingName: shoot-operator-gcp  region: europe-west1  provider:  type: gcp  ... ManagedSeedSet Controller ManagedSeedSets are reconciled by a new managed seed set controller in gardener-controller-manager. During the reconciliation this controller creates and deletes ManagedSeeds and Shoots in response to changes to the replicas and selector fields.\nNote: The introduction of the ManagedSeedSet resource would not require any changes to gardenlet or to existing gardener-controller-manager controllers.\nManaging ManagedSeed Updates To manage ManagedSeed updates, we considered two possible approaches:\n A ManagedSeedSet, similarly to a ReplicaSet, does not manage updates to its replicas in any way. In the future, we might introduce ManagedSeedDeployments, a higher-level concept that manages ManagedSeedSets and provides declarative updates to ManagedSeeds along with other useful features, similarly to a Deployment. Such a mechanism would involve creating new ManagedSeedSets, and therefore new seeds, behind the scenes, and moving existing shoots to them. A ManagedSeedSet does manage updates to its replicas, similarly to a StatefulSet. Updates are performed “in-place”, without creating new seeds and moving existing shoots to them. Such a mechanism could also take advantage of other StatefulSet features, such as ordered rolling updates and phased rollouts.  There is an important difference between seeds and pods or nodes in that seeds are more “heavyweight” and therefore updating a set of seeds by introducing new seeds and moving shoots to them tends to be much more complex, time-consuming, and prone to failures compared to updating the seeds “in place”. Furthermore, updating seeds in this way depends on a mature implementation of GEP-7: Shoot Control Plane Migration, which is not available right now. Due to these considerations, we favor the second approach over the first one.\nManagedSeed Identity and Order A StatefulSet manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods. It maintains a stable identity (including network identity) for each of their Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.\nA StatefulSet achieves the above by associating each replica with an ordinal number. With n replicas, these ordinal numbers range from 0 to n-1. When scaling out, newly added replicas always have ordinal numbers larger than those of previously existing replicas. When scaling in, it is the replicas with the largest original numbers that are removed.\nBesides stable identity and persistent storage, these ordinal numbers are also used to implement the following StatefulSet features:\n Ordered, graceful deployment and scaling. Ordered, automated rolling updates. Such rolling updates can be partitioned (limited to replicas with ordinal numbers greater than or equal to the “partition”) to achieve phased rollouts.  A ManagedSeedSet, unlike a StatefulSet, does not need to maintain a stable identity for its ManagedSeeds. Furthermore, it would not be practical to always remove the replicas with the largest ordinal numbers when scaling in, since the corresponding seeds may have shoots scheduled onto them, while other seeds, with lower ordinals, may have fewer shoots (or none), and therefore be much better candidates for being removed.\nOn the other hand, it would be beneficial if a ManagedSeedSet, like a StatefulSet, provides ordered deployment and scaling, ordered rolling updates, and phased rollouts. The main advantage of these features is that a deployment or update failure would affect fewer replicas (ideally just one), containing any potential damage and making the situation easier to handle, thus achieving some of the goals stated in Issue #87. They could also help to contain seed rolling updates outside business hours.\nBased on the above considerations, we propose the following mechanism for handling ManagedSeed identity and order:\n A ManagedSeedSet uses ordinal numbers generated by an increasing sequence to identify ManagedSeeds and Shoots it creates and manages. These numbers always start from 0 and are incremented by 1 for each newly added replica. Replicas (both ManagedSeeds and Shoots) are named after the ManagedSeedSet with the ordinal number appended. For example, for a ManagedSeedSet named test its replicas are named test-0, test-1, etc. Gaps in the sequence created by removing replicas with ordinal numbers in the middle of the range are never filled in. A newly added replica always receives a number that is not only free, but also unique to itself. For example, if there are 2 replicas named test-0 and test-1 and any one of them is removed, a newly added replica will still be named test-2.  Although such ordinal numbers can also provide some form of stable identity, in this case it is much more important that they can provide a predictable ordering for deployments and updates, and can also be used to partition rolling updates similarly to StatefulSet ordinal numbers.\nUpdate Strategies The ManagedSeedSet’s .spec.updateStrategy field allows configuring automated rolling updates for the ManagedSeeds and Shoots in a ManagedSeedSet.\nRolling Updates\nThe RollingUpdate update strategy implements automated, rolling update for the ManagedSeeds and Shoots in a ManagedSeedSet. With this strategy, the ManagedSeedSet controller will update each ManagedSeed and Shoot in the ManagedSeedSet. It will proceed from the largest number to the smallest, updating each ManagedSeed and its corresponding Shoot one at a time. It will wait until both the Shoot and the Seed of an updated ManagedSeed are Ready prior to updating its predecessor.\nAs a further improvement upon the above, the controller could check not only the ManagedSeeds and their corresponding Shoots for readiness, but also the Shoots scheduled onto these ManagedSeeds. The rollout would then only continue if no more than X percent of these Shoots are not reconciled and Ready. Since checking all these additional conditions might require some complex logic, it should be performed by an independent managed seed care controller that updates the ManagedSeed resource with the readiness of its Seed and all Shoots scheduled onto the Seed.\nNote that unlike a StatefulSet, an OnDelete update strategy is not supported.\nPartitions\nThe RollingUpdate update strategy can be partitioned, by specifying a .spec.updateStrategy.rollingUpdate.partition. If a partition is specified, only ManagedSeeds and Shoots with ordinals greater than or equal to the partition will be updated when any of the ManagedSeedSet’s templates is updated. All remaining ManagedSeeds and Shoots will not be updated. If a ManagedSeedSet’s .spec.updateStrategy.rollingUpdate.partition is greater than the largest ordinal number in use by a replica, updates to its templates will not be propagated to its replicas (but newly added replicas may still use the updated templates depending on the partition value).\nKeeping Track of Revision History and Performing Rollbacks Similarly to a StatefulSet, the ManagedSeedSet controller uses ControllerRevisions to keep track of the revision history, and controller-revision-hash labels to maintain an association between a ManagedSeed or a Shoot and the concrete template revisions based on which they were created or last updated. These are used for the following purposes:\n During an update, determine which replicas are still not on the latest revision and therefore should be updated. Display the revision history of a ManagedSeedSet via kubectl rollout history. Roll back all ManagedSeedSet replicas to a specific revision via kubectl rollout undo  Note: The above kubectl rollout commands will not work with custom resources such as ManagedSeedSets out of the box (the documentation says explicitly that valid resource types are only deployments, daemonsets, and statefulsets), but it should be possible to eventually support such commands for ManagedSeedSets via a kubectl plugin.\nScaling-in ManagedSeedSets Deleting ManagedSeeds in response to decreasing the replicas of a ManagedSeedSet deserves special attention for two reasons:\n A seed that is already in use by shoots cannot be deleted, unless the shoots are either deleted or moved to other seeds first. When there are more empty seeds than requested for deletion, determining which seeds to delete might not be as straightforward as with pods or nodes.  The above challenges could be addressed as follows:\n In order to scale in a ManagedSeedSet successfully, there should be at least as many empty ManagedSeeds as the difference between the old and the new replicas. In some cases, the user might need to ensure that this is the case by draining some seeds manually before decreasing the replicas field. It should be possible to protect ManagedSeeds from deletion even if they are empty, perhaps via an annotation such as seedmanagement.gardener.cloud/protect-from-deletion. Such seeds are not taken into account when determining whether the scale in operation can succeed. The decision which seeds to delete among the ManagedSeeds that are empty and not protected should be based on hints, perhaps again in the form of annotations, that could be added manually by the user, as well as other factors, see Prioritizing ManagedSeed Deletion.  Prioritizing ManagedSeed Deletion To help the controller decide which empty ManagedSeeds are to be deleted first, the user could manually annotate ManagedSeeds with a seed priority annotation such as seedmanagement.gardener.cloud/priority. ManagedSeeds with lower priority are more likely to be deleted first. If not specified, a certain default value is assumed, for example 3.\nBesides this annotation, the controller should take into account also other factors, such as the current seed conditions (NotReady should be preferred for deletion over Ready), as well as its age (older should be preferred for deletion over newer).\nAuto-scaling Seeds The most interesting and advanced automated seed management feature is making sure that a Garden cluster has enough seeds registered to schedule new shoots (and, in the future, reschedule shoots from drained seeds) without exceeding the seeds capacity for shoots, but not more than actually needed at any given moment. This would involve introducing an auto-scaling mechanism for seeds in Garden clusters.\nThe proposed solution builds upon the ideas introduced earlier. The ManagedSeedSet resource (and in the future, also the ManagedSeedDeployment resource) could have a scale subresource that changes the replicas field. This would allow a new “seed autoscaler” controller to scale these resources via a special “autoscaler” resource (for example SeedAutoscaler), similarly to how the Kubernetes Horizontal Pod Autoscaler controller scales pods, as described in Horizontal Pod Autoscaler Walkthrough.\nThe primary metric used for scaling should be the number of shoots already scheduled onto that seed either as a direct value or as a percentage of the seed’s capacity for shoots introduced in Ensuring Seeds Capacity for Shoots Is Not Exceeded (utilization). Later, custom metrics based on other resources, including provider-specific resources, could be considered as well.\nNote: Even if the controller is called Horizontal Pod Autoscaler, it is capable of scaling any resource with a scale subresource, using any custom metric. Therefore, initially it was proposed to use this controller directly. However, a number of important drawbacks were identified with this approach, and so it is no longer proposed here.\nSeedAutoscaler Resource The SeedAutoscaler automatically scales the number of ManagedSeeds in a ManagedSeedSet based on observed resource utilization. The resource could be any resource that is tracked via the capacity and allocatable fields in the Seed status, including in particular the number of shoots already scheduled onto the seed.\nThe SeedAutoscaler is implemented as a custom resource and a new controller. The resource determines the behavior of the controller. The SeedAutoscaler resource has a scaleTargetRef that specifies the target resource to be scaled, the minimum and maximum number of replicas, as well as a list of metrics. The only supported metric type initially is Resource for resources that are tracked via the capacity and allocatable fields in the Seed status. The resource target can be of type Utilization or AverageValue.\nExample SeedAutoscaler resource:\napiVersion: seedmanagement.gardener.cloud/v1alpha1 kind: SeedAutoscaler metadata:  name: crazy-botany  namespace: garden spec:  scaleTargetRef:  apiVersion: seedmanagement.gardener.cloud/v1alpha1  kind: ManagedSeedSet  name: crazy-botany  minReplicas: 1  maxReplicas: 10  metrics:  - type: Resource # Only Resource is supported  resource:  name: shoots  target:  type: Utilization # Utilization or AverageValue  averageUtilization: 50 SeedAutoscaler Controller SeedAutoscaler resources are reconciled by a new seed autoscaler controller, either in gardener-controller-manager or out-of-tree, similarly to cluster-autoscaler. The controller periodically adjusts the number of replicas in a ManagedSeedSet to match the observed average resource utilization to the target specified by user.\nNote: The SeedAutoscaler controller should perhaps not be limited to evaluating only metrics, it could also take into account also taints, label selectors, etc. This is not yet reflected in the example SeedAutoscaler resource above. Such details are intentionally not specified in this GEP, they should be further explored in the issues created to track the actual implementation.\nEvaluating Metrics for Autoscaling The metrics used by the controller, for example the shoots metric above, could be evaluated in one of the following ways:\n Directly, by looking at the capacity and allocatable fields in the Seed status and comparing to the actual resource consumption calculated by simply counting all shoots that meet a certain criteria (e.g. shoots that are scheduled onto the seed), then taking an average over all seeds in the set. By sampling existing metrics exported for example by gardener-metrics-exporter.  The second approach decouples the seed autoscaler controller from the actual metrics evaluation, and therefore allows plugging in new metrics more easily. It also has the advantage that the exported metrics could also be used for other purposes, e.g. for triggering Prometheus alerts or building Grafana dashboards. It has the disadvantage that the seed autoscaler controller would depend on the metrics exporter to do its job properly.\n","categories":"","description":"","excerpt":"Automated Seed Management Automated seed management involves …","ref":"/docs/gardener/proposals/13-automated-seed-management/","tags":"","title":"13 Automated Seed Management"},{"body":"Shoot Control Plane Migration “Bad Case” Scenario The migration flow described as part of GEP-7 can only be executed if both the Garden cluster and source seed cluster are healthy, and gardenlet in the source seed cluster can connect to the Garden cluster. In this case, gardenlet can directly scale down the shoot’s control plane in the source seed, after checking the spec.seedName field.\nHowever, there might be situations in which gardenlet in the source seed cluster can’t connect to the Garden cluster and determine that spec.seedName has changed. Similarly, the connection to the seed kube-apiserver could also be broken. This might be caused by issues with the seed cluster itself. In other situations, the migration flow steps in the source seed might have started but might not be able to finish successfully. In all such cases, it should still be possible to migrate a shoot’s control plane to a different seed, even though executing the migration flow steps in the source seed might not be possible. The potential “split brain” situation caused by having the shoot’s control plane components attempting to reconcile the shoot resources in two different seeds must still be avoided, by ensuring that the shoot’s control plane in the source seed is deactivated before it is activated in the destination seed.\nThe mechanisms and adaptations described below have been tested as part of a PoC prior to describing them here.\nOwner Election / Copying Snapshots To achieve the goals outlined above, an “owner election” (or rather, “ownership passing”) mechanism is introduced to ensure that the source and destination seeds are able to successfully negotiate a single “owner” during the migration. This mechanism is based on special owner DNS records that uniquely identify the seed that currently hosts the shoot’s control plane (“owns” the shoot).\nFor example, for a shoot named i500152-gcp in project dev that uses an internal domain suffix internal.dev.k8s.ondemand.com and is scheduled on a seed with an identity shoot--i500152--gcp2-0841c87f-8db9-4d04-a603-35570da6341f-sap-landscape-dev, the owner DNS record is a TXT record with a domain name owner.i500152-gcp.dev.internal.dev.k8s.ondemand.com and a single value shoot--i500152--gcp2-0841c87f-8db9-4d04-a603-35570da6341f-sap-landscape-dev. The owner DNS record is created and maintained by reconciling an owner DNSRecord resource.\nUnlike other extension resources, the owner DNSRecord resource is not reconciled every time the shoot is reconciled, but only when the resource is created. Therefore, the owner DNS record value (the owner ID) is updated only when the shoot is migrated to a different seed. For more information, see Add handling of owner DNSRecord resources.\nThe owner DNS record domain name and owner ID are passed to components that need to perform ownership checks, such as the backup-restore container of the etcd-main StatefulSet, and all extension controllers. These components then check regularly whether the actual owner ID (the value of the record) matches the passed ID. If they don’t, the ownership check is considered failed, which causes the special behavior described below.\nNote: A previous revision of this document proposed using “sync objects” written to and read from the backup container of the source seed as JSON files by the etcd-backup-restore processes in both seeds. With the introduction of owner DNS records such sync objects are no longer needed.\nFor the destination seed to actually become the owner, it needs to acquire the shoot’s etcd data by copying the final full snapshot (and potentially also older snapshots) from the backup container of the source seed.\nThe mechanism to copy the snapshots and pass the ownership from the source to the destination seed consists of the following steps:\n  The reconciliation flow (“restore” phase) is triggered in the destination seed without first executing the migration flow in the source seed (or perhaps it was executed, but it failed, and its state is currently unknown).\n  The owner DNSRecord resource is created in the destination seed. As a result, the actual owner DNS record is updated with the destination seed ID. From this point, ownership checks by the etcd-backup-restore process and extension controller watchdogs in the source seed will fail, which will cause the special behavior described below.\n  An additional “source” backup entry referencing the source seed backup bucket is deployed to the Garden cluster and the destination seed and reconciled by the backup entry controller. As a result, a secret with the appropriate credentials for accessing the source seed backup container named source-etcd-backup is created in the destination seed. The normal backup entry (referencing the destination seed backup container) is also deployed and reconciled, as usual, resulting in the usual etcd-backup secret being created.\n  A special “copy” version of the etcd-main Etcd resource is deployed to the destination seed. In its backup section, this resource contains a sourceStore in addition to the usual store, which contains the parameters needed to use the source seed backup container, such as its name and the secret created in the previous step.\nspec:  backup:  ...  store:  container: 408740b8-6491-415e-98e6-76e92e5956ac  secretRef:  name: etcd-backup  ...  sourceStore:  container: d1435fea-cd5e-4d5b-a198-81f4025454ff  secretRef:  name: source-etcd-backup  ...   The etcd-druid in the destination seed reconciles the above resource by deploying a etcd-copy Job that contains a single backup-restore container. It executes the newly introduced copy command of etcd-backup-restore that copies the snapshots from the source to the destination backup container.\n  Before starting the copy itself, the etcd-backup-restore process in the destination seed checks if a final full snapshot (a full snapshot marked as final=true) exists in the backup container. If such a snapshot is not found, it waits for it to appear in order to proceed. This waiting is up to a certain timeout that should be sufficient for a full snapshot to be taken; after this timeout has elapsed, it proceeds anyway, and the reconciliation flow continues from step 9. As described in Handling Inability to Access the Backup Container below, this is safe to do.\n  The etcd-backup-restore process in the source seed detects that the owner ID in the owner DNS record is different from the expected owner ID (because it was updated in step 2) and switches to a special “final snapshot” mode. In this mode the regular snapshotter is stopped, the readiness probe of the main etcd container starts returning 503, and one final full snapshot is taken. This snapshot is marked as final=true in order to ensure that it’s only taken once, and in order to enable the etcd-backup-restore process in the destination seed to find it (see step 6).\nNote: While testing our PoC, we noticed that simply making the readiness probe of the main etcd container fail doesn’t terminate the existing open connections from kube-apiserver to etcd. For this to happen, either the kube-apiserver or the etcd process has to be restarted at least once. Therefore, when the snapshotter is stopped because an ownership change has been detected, the main etcd process is killed (using SIGTERM to allow graceful termination) to ensure that any open connections from kube-apiserver are terminated. For this to work, the 2 containers must share the process namespace.\n  Since the kube-apiserver process in the source seed is no longer able to connect to etcd, all shoot control plane controllers (kube-controller-manager, kube-scheduler, machine-controller-manager, etc.) and extension controllers reconciling shoot resources in the source seed that require a connection to the shoot in order to work start failing. All remaining extension controllers are prevented from reconciling shoot resources via the watchdogs mechanism. At this point, the source seed has effectively lost its ownership of the shoot, and it is safe for the destination seed to assume the ownership.\n  After the etcd-backup-restore process in the destination seed detects that a final full snapshot exists, it copies all snapshots (or a subset of all snapshots) from the source to the destination backup container. When this is done, the Job finishes successfully which signals to the reconciliation flow that the snapshots have been copied.\nNote: To save time, only the final full snapshot taken in step 6, or a subset defined by some criteria, could be copied, instead of all snapshots.\n  The special “copy” version of the etcd-main Etcd resource is deleted from the source seed, and as a result the etcd-copy Job is also deleted by etcd-druid.\n  The additional “source” backup entry referencing the source seed backup container is deleted from the Garden cluster and the destination seed. As a result, its corresponding source-etcd-backup secret is also deleted from the destination seed.\n  From this point, the reconciliation flow proceeds as already described in GEP-7. This is safe, since the source seed cluster is no longer able to interfere with the shoot.\n  Handling Inability to Access the Backup Container The mechanism described above assumes that the etcd-backup-restore process in the source seed is able to access its backup container in order to take snapshots. If this is not the case, but an ownership change was detected, the etcd-backup-restore process still sets the readiness probe status of the main etcd container to 503, and kills the main etcd process as described above to ensure that any open connections from kube-apiserver are terminated. This effectively deactivates the source seed control plane to ensure that the ownership of the shoot can be passed to a different seed.\nBecause of this, etcd-backup-restore process in the destination seed responsible for copying the snapshots can avoid waiting forever for a final full snapshot to appear. Instead, after a certain timeout has elapsed, it can proceed with the copying. In this situation, whatever latest snapshot is found in the source backup container will be restored in the destination seed. The shoot is still migrated to a healthy seed at the cost of losing the etcd data that accumulated between the point in time when the connection to the source backup container was lost, and the point in time when the source seed cluster was deactivated.\nWhen the connection to the backup container is restored in the source seed, a final full snapshot will be eventually taken. Depending on the stage of the restoration flow in the destination seed, this snapshot may be copied to the destination seed and restored, or it may simply be ignored since the snapshots have already been copied.\nHandling Inability to Resolve the Owner DNS Record The situation when the owner DNS record cannot be resolved is treated similarly to a failed ownership check: the etcd-backup-restore process sets the readiness probe status of the main etcd container to 503, and kills the main etcd process as described above to ensure that any open connections from kube-apiserver are terminated, effectively deactivating the source seed control plane. The final full snapshot is not taken in this case to ensure that the control plane can be re-activated if needed.\nWhen the owner DNS record can be resolved again, the following 2 situations are possible:\n If the source seed is still the owner of the shoot, the etcd-backup-restore process will set the readiness probe status of the main etcd container to 200, so kube-apiserver will be able to connect to etcd and the source seed control plane will be activated again. If the source seed is no longer the owner of the shoot, the etcd readiness probe will continue to fail, and the source seed control plane will remain inactive. In addition, the final full snapshot will be taken at this time, for the same reason as described in Handling Inability to Access the Backup Container.  Note: We expect that actual DNS outages are extremely unlikely. A more likely reason for an inability to resolve a DNS record could be network issues with the underlying infrastructure. In such cases, the shoot would usually not be usable / reachable anyway, so deactivating its control plane would not cause a worse outage.\nMigration Flow Adaptations Certain changes to the migration flow are needed in order to ensure that it is compatible with the owner election mechanism described above. Instead of taking a full snapshot of the source seed etcd, the flow deletes the owner DNS record by deleting the owner DNSRecord resource. This causes the ownership check by etcd-backup-restore to fail, and the final full snapshot to be eventually taken, so the migration flow waits for a final full snapshot to appear as the last step before deleting the shoot namespace in the source seed. This ensures that the reconciliation flow described above will find a final full snapshot waiting to be copied at step 6.\nChecking for the final full snapshot is performed by calling the already existing etcd-backup-restore endpoint snapshot/latest. This is possible, since the backup-restore container is always running at this point.\nAfter the final full snapshot has been taken, the readiness probe of the main etcd container starts failing, which means that if the migration flow is retried due to an error it must skip the step that waits for etcd-main to become ready. To determine if this is the case, a check whether the final full snapshot has been taken or not is performed by calling the same etcd-backup-restore endpoint, e.g. snapshot/latest. This is possible if the etcd-main Etcd resource exists with non-zero replicas. Otherwise:\n If the resource doesn’t exist, it must have been already deleted, so the final full snapshot n must have been already taken. If it exists with zero replicas, the shoot must be hibernated, and the migration flow must have never been executed (since it scales up etcd as one of its first steps), so the final full snapshot must not have been taken yet.  Extension Controller Watchdogs Some extension controllers will stop reconciling shoot resources after the connection to the shoot’s kube-apiserver is lost. Others, most notably the infrastructure controller, will not be affected. Even though new shoot reconciliations won’t be performed by gardenlet, such extension controllers might be stuck in a retry loop triggered by a previous reconciliation, which may cause them to reconcile their resources after gardenlet has already stopped reconciling the shoot. In addition, a reconciliation started when the seed still owned the shoot might take some time and therefore might still be running after the ownership has changed. To ensure that the source seed is completely deactivated, an additional safety mechanism is needed.\nThis mechanism should handle the following interesting cases:\n gardenlet cannot connect to the Garden kube-apiserver. In this case it cannot fetch shoots and therefore does not know if control plane migration has been triggered. Even though gardenlet will not trigger new reconciliations, extension controllers could still attempt to reconcile their resources if they are stuck a retry loop from a previous reconciliation, and already running reconciliations will not be stopped. gardenlet cannot connect to the seed’s kube-apiserver. In this case gardenlet knows if migration has been triggered, but it will not start shoot migration or reconciliation as it will first check the seed conditions and try to update the Cluster resource, both of which will fail. Extension controllers could still be able to connect to the seed’s kube-apiserver (if they are not running where gardenlet is running), and similarly to the previous case, they could still attempt to reconcile their resources. The seed components (etcd-druid, extension controllers, etc) cannot connect to the seed’s kube-apiserver. In this case extension controllers would not be able to reconcile their resources as they cannot fetch them from the seed’s kube-apiserver. When the connection to the kube-apiserver comes back, the controllers might be stuck in a retry loop from a previous reconciliation, or the resources could still be annotated with gardener.cloud/operation=reconcile. This could lead to a race condition depending on who manages to update or get the resources first. If gardenlet manages to update the resources before they are read by the extension controllers, they would be properly updated with gardener.cloud/operation=migrate. Otherwise, they would be reconciled as usual.  Note: A previous revision of this document proposed using “cluster leases” as such an additional safety mechanism. With the introduction of owner DNS records cluster leases are no longer needed.\nThe safety mechanism is based on extension controller watchdogs. These are simply additional goroutines that are started when a reconciliation is started by an extension controller. These goroutines perform an ownership check on a regular basis using the owner DNS record, similar to the check performed by the etcd-backup-restore process described above. If the check fails, the watchdog cancels the reconciliation context, which immediately aborts the reconciliation.\nNote: The dns-external extension controller is the only extension controller that neither needs the shoot’s kube-apiserver, nor uses the watchdog mechanism described here. Therefore, this controller will continue reconciling DNSEntry resources even after the source seed has lost the ownership of the shoot. With the PoC, we manually delete the DNSOwner resources from the source seed cluster to prevent this from happening. Eventually, the dns-external controller should be adapted to use the owner DNS records to ensure that it disables itself after the seed has lost the ownership of the shoot. Changes in this direction have already been agreed and relevant PRs proposed.\n","categories":"","description":"","excerpt":"Shoot Control Plane Migration “Bad Case” Scenario The migration flow …","ref":"/docs/gardener/proposals/17-shoot-control-plane-migration-bad-case/","tags":"","title":"17 Shoot Control Plane Migration Bad Case"},{"body":"Question You deployed an application with a web UI or an internal endpoint in your Kubernetes (K8s) cluster. How can I access this endpoint without an external load balancer (e.g. Ingress)? This tutorial presents two options:\n Using Kubernetes port forward Using Kubernetes apiserver proxy  Please note that the options described here are mostly for quick testing or troubleshooting your application. For enabling access to your application for productive environment, please refer to Service and Ingress.\nSolution 1: Using Kubernetes port forward You could use the port forwarding functionality of kubectl to access the pods from your local host without involving a service.\nTo access any pod follow these steps:\n Run kubectl get pods Note down the name of the pod in question as \u003cyour-pod-name\u003e Run kubectl port-forward \u003cyour-pod-name\u003e \u003clocal-port\u003e:\u003cyour-app-port\u003e Run a web browser or curl locally and enter the URL http(s)://localhost:\u003clocal-port\u003e  In addition, kubectl port-forward allows to use a resource name. such as a deployment or service name, to select a matching pod to port forward. Find more details in the Kubernetes documentation.\nThe main drawback of this approach is that the pod’s name will change as soon as it is restarted. Moreover, you need to have a web browser on your client and you need to make sure that the local port is not already used by an application running on your system. Finally, sometimes port forwarding is canceled due to non obvious reasons. This leads to a kind of shaky approach. A more robust approach is to access the application using kube-proxy.\nSolution 2: Using apiserver proxy There are several different proxies used with Kubernetes, the official documentation provides a good overview.\nIn this tutorial we are using apiserver proxy to enable access to services running in Kubernetes without using an Ingress. Different from the first solution, a service is required for this solution .\nUse the following URL to access a service via apiserver proxy. For details about apiserver proxy URLs read Discovering builtin services.\nhttps://\u003ccluster-master\u003e/api/v1/namespace/\u003cnamespace\u003e/services/\u003cservice\u003e:\u003cservice-port\u003e/proxy/\u003cservice-endpoint\u003e\nExample:\n   cluster-master namespace service service-port service-endpoint url to access service     api.testclstr.cpet.k8s.sapcloud.io default nginx-svc 80 / http://api.testclstr.cpet.k8s.sapcloud.io/api/v1/namespaces/default/services/nginx-svc:80/proxy/   api.testclstr.cpet.k8s.sapcloud.io default docker-nodejs-svc 4500 /cpu?baseNumber=4 https://api.testclstr.cpet.k8s.sapcloud.io/api/v1/namespaces/default/services/docker-nodejs-svc:4500/proxy/cpu?baseNumber=4    There are applications, which do not yet support relative URLs like Prometheus (as of end of November, 2017). This typically leads to missing JavaScript objects when trying to open the URL in a browser. In this case use the port-forward approach described above.\n","categories":"","description":"","excerpt":"Question You deployed an application with a web UI or an internal …","ref":"/docs/guides/applications/access_pod_from_local/","tags":"","title":"Access a Port of a Pod Locally"},{"body":"Access Restrictions The dashboard can be configured with access restrictions.\nAccess restrictions are shown for regions that have a matching label in the CloudProfile\n regions:  - name: pangaea-north-1  zones:  - name: pangaea-north-1a  - name: pangaea-north-1b  - name: pangaea-north-1c  labels:  seed.gardener.cloud/eu-access: \"true\"  If the user selects the access restriction, spec.seedSelector.matchLabels[key] will be set. When selecting an option, metadata.annotations[optionKey] will be set.  The value that is set depends on the configuration. See 2. under Configuration section below.\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata:  annotations:  support.gardener.cloud/eu-access-for-cluster-addons: \"true\"  support.gardener.cloud/eu-access-for-cluster-nodes: \"true\"  ... spec:  seedSelector:  matchLabels:  seed.gardener.cloud/eu-access: \"true\" In order for the shoot (with enabled access restriction) to be scheduled on a seed, the seed needs to have the label set. E.g.\napiVersion: core.gardener.cloud/v1beta1 kind: Seed metadata:  labels:  seed.gardener.cloud/eu-access: \"true\" ... Configuration As gardener administrator:\n you can control the visibility of the chips with the accessRestriction.items[].display.visibleIf and accessRestriction.items[].options[].display.visibleIf property. E.g. in this example the access restriction chip is shown if the value is true and the option is shown if the value is false. you can control the value of the input field (switch / checkbox) with the accessRestriction.items[].input.inverted and accessRestriction.items[].options[].input.inverted property. Setting the inverted property to true will invert the value. That means that when selecting the input field the value will be'false' instead of 'true'. you can configure the text that is displayed when no access restriction options are available by setting accessRestriction.noItemsText example values.yaml:  accessRestriction:  noItemsText: No access restriction options available for region {region} and cloud profile {cloudProfile}  items:  - key: seed.gardener.cloud/eu-access  display:  visibleIf: true  # title: foo # optional title, if not defined key will be used  # description: bar # optional description displayed in a tooltip  input:  title: EU Access  description: | This service is offered to you with our regular SLAs and 24x7 support for the control plane of the cluster. 24x7 support for cluster add-ons and nodes is only available if you meet the following conditions:  options:  - key: support.gardener.cloud/eu-access-for-cluster-addons  display:  visibleIf: false  # title: bar # optional title, if not defined key will be used  # description: baz # optional description displayed in a tooltip  input:  title: No personal data is used as name or in the content of Gardener or Kubernetes resources (e.g. Gardener project name or Kubernetes namespace, configMap or secret in Gardener or Kubernetes)  description: | If you can't comply, only third-level/dev support at usual 8x5 working hours in EEA will be available to you for all cluster add-ons such as DNS and certificates, Calico overlay network and network policies, kube-proxy and services, and everything else that would require direct inspection of your cluster through its API server  inverted: true  - key: support.gardener.cloud/eu-access-for-cluster-nodes  display:  visibleIf: false  input:  title: No personal data is stored in any Kubernetes volume except for container file system, emptyDirs, and persistentVolumes (in particular, not on hostPath volumes)  description: | If you can't comply, only third-level/dev support at usual 8x5 working hours in EEA will be available to you for all node-related components such as Docker and Kubelet, the operating system, and everything else that would require direct inspection of your nodes through a privileged pod or SSH  inverted: true ","categories":"","description":"","excerpt":"Access Restrictions The dashboard can be configured with access …","ref":"/docs/dashboard/deployment/access-restrictions/","tags":"","title":"Access Restrictions"},{"body":"Adding support for a new provider Steps to be followed while implementing a new (hyperscale) provider are mentioned below. This is the easiest way to add new provider support using a blueprint code.\nHowever, you may also develop your machine controller from scratch, which would provide you with more flexibility. First, however, make sure that your custom machine controller adheres to the Machine.Status struct defined in the MachineAPIs. This will make sure the MCM can act with higher-level controllers like MachineSet and MachineDeployment controller. The key is the Machine.Status.CurrentStatus.Phase key that indicates the status of the machine object.\nOur strong recommendation would be to follow the steps below. This provides the most flexibility required to support machine management for adding new providers. And if you feel to extend the functionality, feel free to update our machine controller libraries.\nSetting up your repository  Create a new empty repository named machine-controller-manager-provider-{provider-name} on GitHub username/project. Do not initialize this repository with a README. Copy the remote repository URL (HTTPS/SSH) to this repository displayed once you create this repository. Now, on your local system, create directories as required. {your-github-username} given below could also be {github-project} depending on where you have created the new repository. mkdir -p $GOPATH/src/github.com/{your-github-username}  Navigate to this created directory. cd $GOPATH/src/github.com/{your-github-username}  Clone this repository on your local machine. git clone git@github.com:gardener/machine-controller-manager-provider-sampleprovider.git  Rename the directory from machine-controller-manager-provider-sampleprovider to machine-controller-manager-provider-{provider-name}. mv machine-controller-manager-provider-sampleprovider machine-controller-manager-provider-{provider-name}  Navigate into the newly-created directory. cd machine-controller-manager-provider-{provider-name}  Update the remote origin URL to the newly created repository’s URL you had copied above. git remote set-url origin git@github.com:{your-github-username}/machine-controller-manager-provider-{provider-name}.git  Rename GitHub project from gardener to {github-org/your-github-username} wherever you have cloned the repository above. Also, edit all occurrences of the word sampleprovider to {provider-name} in the code. Then, use the hack script given below to do the same. make rename-project PROJECT_NAME={github-org/your-github-username} PROVIDER_NAME={provider-name} eg:  make rename-project PROJECT_NAME=gardener PROVIDER_NAME=AmazonWebServices (or)  make rename-project PROJECT_NAME=githubusername PROVIDER_NAME=AWS  Now, commit your changes and push them upstream. git add -A git commit -m \"Renamed SampleProvide to {provider-name}\" git push origin master   Code changes required The contract between the Machine Controller Manager (MCM) and the Machine Controller (MC) AKA driver has been documented here and the machine error codes can be found here. You may refer to them for any queries.\n⚠️\n Keep in mind that there should be a unique way to map between machine objects and VMs. This can be done by mapping machine object names with VM-Name/ tags/ other metadata. Optionally, there should also be a unique way to map a VM to its machine class object. This can be done by tagging VM objects with tags/resource groups associated with the machine class.  Steps to integrate  Update the pkg/provider/apis/provider_spec.go specification file to reflect the structure of the ProviderSpec blob. It typically contains the machine template details in the MachineClass object. Follow the sample spec provided already in the file. A sample provider specification can be found here. Fill in the methods described at pkg/provider/core.go to manage VMs on your cloud provider. Comments are provided above each method to help you fill them up with desired REQUEST and RESPONSE parameters.  A sample provider implementation for these methods can be found here. Fill in the required methods CreateMachine(), and DeleteMachine() methods. Optionally fill in methods like GetMachineStatus(), ListMachines(), and GetVolumeIDs(). You may choose to fill these once the working of the required methods seems to be working. GetVolumeIDs() expects VolumeIDs to be decoded from the volumeSpec based on the cloud provider. There is also an OPTIONAL method GenerateMachineClassForMigration() that helps in migration of {ProviderSpecific}MachineClass to MachineClass CR (custom resource). This only makes sense if you have an existing implementation (in-tree) acting on different CRD types. You would like to migrate this. If not, you MUST return an error (machine error UNIMPLEMENTED) to avoid processing this step.   Perform validation of APIs that you have described and make it a part of your methods as required at each request. Write unit tests to make it work with your implementation by running make test. make test  Re-generate the vendors to update any new vendors imported. make revendor  Update the sample YAML files on the kubernetes/ directory to provide sample files through which the working of the machine controller can be tested. Update README.md to reflect any additional changes  Testing your code changes Make sure $TARGET_KUBECONFIG points to the cluster where you wish to manage machines. Likewise, $CONTROL_NAMESPACE represents the namespaces where MCM is looking for machine CR objects, and $CONTROL_KUBECONFIG points to the cluster that holds these machine CRs.\n On the first terminal running at $GOPATH/src/github.com/{github-org/your-github-username}/machine-controller-manager-provider-{provider-name},  Run the machine controller (driver) using the command below. make start    On the second terminal pointing to $GOPATH/src/github.com/gardener,  Clone the latest MCM code git clone git@github.com:gardener/machine-controller-manager.git  Navigate to the newly-created directory. cd machine-controller-manager  Deploy the required CRDs from the machine-controller-manager repo, kubectl apply -f kubernetes/crds  Run the machine-controller-manager in the master branch make start    On the third terminal pointing to $GOPATH/src/github.com/{github-org/your-github-username}/machine-controller-manager-provider-{provider-name}  Fill in the object files given below and deploy them as described below. Deploy the machine-class kubectl apply -f kubernetes/machine-class.yaml  Deploy the kubernetes secret if required. kubectl apply -f kubernetes/secret.yaml  Deploy the machine object and make sure it joins the cluster successfully. kubectl apply -f kubernetes/machine.yaml  Once the machine joins, you can test by deploying a machine-deployment. Deploy the machine-deployment object and make sure it joins the cluster successfully. kubectl apply -f kubernetes/machine-deployment.yaml  Make sure to delete both the machine and machine-deployment objects after use. kubectl delete -f kubernetes/machine.yaml kubectl delete -f kubernetes/machine-deployment.yaml     Releasing your docker image  Make sure you have logged into gcloud/docker using the CLI. To release your docker image, run the following.   make release IMAGE_REPOSITORY=\u003clink-to-image-repo\u003e A sample kubernetes deploy file can be found at kubernetes/deployment.yaml. Update the same (with your desired MCM and MC images) to deploy your MCM pod.  ","categories":"","description":"","excerpt":"Adding support for a new provider Steps to be followed while …","ref":"/docs/other-components/machine-controller-manager/docs/development/cp_support_new/","tags":"","title":"Adding Support for a Cloud Provider"},{"body":"Adding support for a new cloud provider For adding support for a new cloud provider in the Machine Controller Manager, follow the steps described below. Replace provider with your provider-name.\n Add a ProviderMachineClass CRD similar to existing AWSMachineClass into kubernetes/crds.yaml. Add ProviderMachineClass structs similar to existing AWSMachineClass into the machine APIs into pkg/apis/machine/types.go and pkg/apis/machine/v1alpha1/types.go. This would be the machineClass template used to describe provider specific configurations. Add the Go structures of your machine class (list) to pkg/apis/machine/register.go and pkg/apis/machine/v1alpha1/register.go to allow reporting events on these objects. Regenerate the machine API clients by running ./hack/generate-code Add validation for the new provider machine class at pkg/apis/machine/validation/providermachineclass.go similar to pkg/apis/machine/validation/awsmachineclass.go Update pkg/controller/machine_util.go to allow validation of the new provider. Add a new driver into pkg/driver/driver_provider.go similar to pkg/driver/driver_aws.go to implement the driver interface. Update pkg/driver/driver.go to add a new switch case to support the new provider driver. Add a new method in pkg/controller/machine_safety.go called checkProviderMachineClass similar to the existing method called checkAWSMachineClass present in the same file. Now invoke this method as a go-routine in the method checkVMObjects. Extend the StartControllers() function in cmd/machine-controller-manager/app/controllermanager.go to only start if your new machine class is under the available resources. Update pkg/controller/controller.go to add new providerMachineClassLister, providerMachineClassQueue, awsMachineClassSynced into the controller struct. Also initialize them in NewController() method. Add a new file pkg/controller/providermachineclass.go that allows re-queuing of machines which refer to an modified providerMachineClass. Update pkg/controller/controller.go to extend WaitForCacheSync and .Shutdown() similar to other cloud providers. Update the example ClusterRole in kubernetes/deployment/in-tree/clusterrole.yaml to allow operations on your new machine class. Update pkg/controller/controller.go, pkg/controller/secret.go, pkg/controller/secret_util.go to add event handlers to add/remove finalizers referenced by your machine Class. Refer this commit.  ","categories":"","description":"","excerpt":"Adding support for a new cloud provider For adding support for a new …","ref":"/docs/other-components/machine-controller-manager/docs/development/cp_support_old/","tags":"","title":"Adding Support for a Cloud Provider (Legacy)"},{"body":"Extension Admission The extensions are expected to validate their respective resources for their extension specific configurations, when the resources are newly created or updated. For example, provider extensions would validate spec.provider.infrastructureConfig and spec.provider.controlPlaneConfig in the Shoot resource and spec.providerConfig in the CloudProfile resource, networking extensions would validate spec.networking.providerConfig in the Shoot resource. As best practice, the validation should be performed only if there is a change in the spec of the resource. Please find an exemplary implementation here.\nWhen a resource is newly created or updated, Gardener adds an extension label for all the extension types referenced in the spec of the resource. This label is of the form \u003cextension-type\u003e.extensions.gardener.cloud/\u003cextension-name\u003e : \"true\". For example, an extension label for provider extension type aws, looks like provider.extensions.gardener.cloud/aws : \"true\". The extensions should add object selectors in their admission webhooks for these labels, to filter out the objects they are responsible for. At present, these labels are added to BackupEntrys, BackupBuckets, CloudProfiles, Seeds, SecretBindings and Shoots. Please see this for the full list of extension labels.\n","categories":"","description":"","excerpt":"Extension Admission The extensions are expected to validate their …","ref":"/docs/gardener/extensions/admission/","tags":"","title":"Admission"},{"body":"Gardener Admission Controller While the Gardener API server works with admission plugins to validate and mutate resources belonging to Gardener related API groups, e.g. core.gardener.cloud, the same is needed for resources belonging to non-Gardener API groups as well, e.g. Secrets in the core API group. Therefore, the Gardener Admission Controller runs a http(s) server with the following handlers which serve as validating/mutating endpoints for admission webhooks. It is also used to serve http(s) handlers for authorization webhooks.\nAdmission Webhook Handlers This section describes the admission webhook handlers that are currently served.\nKubeconfig Secret Validator Malicious Kubeconfigs applied by end users may cause a leakage of sensitive data. This handler checks if the incoming request contains a Kubernetes secret with a .data.kubeconfig field and denies the request if the Kubeconfig structure violates Gardener’s security standards.\nNamespace Validator Namespaces are the backing entities of Gardener projects in which shoot clusters objects reside. This validation handler protects active namespaces against premature deletion requests. Therefore, it denies deletion requests if a namespace still contains shoot clusters or if it belongs to a non-deleting Gardener project (w/o .metadata.deletionTimestamp).\nResource Size Validator Since users directly apply Kubernetes native objects to the Garden cluster, it also involves the risk of being vulnerable to DoS attacks because these resources are read continuously watched and read by controllers. One example is the creation of Shoot resources with large annotation values (up to 256 kB per value) which can cause severe out-of-memory issues for the Gardenlet component. Vertical autoscaling can help to mitigate such situations, but we cannot expect to scale infinitely, and thus need means to block the attack itself.\nThe Resource Size Validator checks arbitrary incoming admission requests against a configured maximum size for the resource’s group-version-kind combination and denies the request if the contained object exceeds the quota.\nExample for Gardener Admission Controller configuration:\nserver:  resourceAdmissionConfiguration:  limits:  - apiGroups: [\"core.gardener.cloud\"]  apiVersions: [\"*\"]  resources: [\"shoots\"]  size: 100k  - apiGroups: [\"\"]  apiVersions: [\"v1\"]  resources: [\"secrets\"]  size: 100k  unrestrictedSubjects:  - kind: Group  name: gardener.cloud:system:seeds  apiGroup: rbac.authorization.k8s.io  # - kind: User  # name: admin  # apiGroup: rbac.authorization.k8s.io  # - kind: ServiceAccount  # name: \"*\"  # namespace: garden  # apiGroup: \"\"  operationMode: block #log With the configuration above, the Resource Size Validator denies requests for shoots with Gardener’s core API group which exceed a size of 100 kB. The same is done for Kubernetes secrets.\nAs this feature is meant to protect the system from malicious requests sent by users, it is recommended to exclude trusted groups, users or service accounts from the size restriction via resourceAdmissionConfiguration.unrestrictedSubjects. For example, the backing user for the Gardenlet should always be capable of changing the shoot resource instead of being blocked due to size restrictions. This is because the Gardenlet itself occasionally changes the shoot specification, labels or annotations, and might violate the quota if the existing resource is already close to the quota boundary. Also, operators are supposed to be trusted users and subjecting them to a size limitation can inhibit important operational tasks. Wildcard (\"*\") in subject name is supported.\nSize limitations depend on the individual Gardener setup and choosing the wrong values can affect the availability of your Gardener service. resourceAdmissionConfiguration.operationMode allows to control if a violating request is actually denied (default) or only logged. It’s recommended to start with log, check the logs for exceeding requests, adjust the limits if necessary and finally switch to block.\nSeedRestriction Please refer to this document for more information.\nAuthorization Webhook Handlers This section describes the authorization webhook handlers that are currently served.\nSeedAuthorization Please refer to this document for more information.\n","categories":"","description":"","excerpt":"Gardener Admission Controller While the Gardener API server works with …","ref":"/docs/gardener/concepts/admission-controller/","tags":"","title":"Admission Controller"},{"body":"See who is using Gardener Gardener adopters in production environments that have publicly shared details of their usage.           SAP uses Gardener to deploy and manage Kubernetes clusters at scale in a uniform way across infrastructures (AWS, Azure, GCP, Alicloud, as well as generic interfaces to OpenStack and vSphere). Workloads include Databases (SAP HANA Cloud), Big Data (SAP Data Intelligence), Kyma, many other cloud native applications, and diverse business workloads.    Gardener can now be run by customers on the Public Cloud Platform of the leading European Cloud Provider OVHcloud.    ScaleUp Technologies runs Gardener within their public Openstack Clouds (Hamburg, Berlin, Düsseldorf). Their clients run all kinds of workloads on top of Gardener maintained Kubernetes clusters ranging from databases to Software-as-a-Service applications.    Finanz Informatik Technologie Services GmbH uses Gardener to offer k8s as a service for customers in the financial industry in Germany. It is built on top of a “metal as a service” infrastructure implemented from scratch for k8s workloads in mind. The result is k8s on top of bare metal in minutes.    PingCAP TiDB, is a cloud-native distributed SQL database with MySQL compatibility, and one of the most popular open-source database projects - with 23.5K+ stars and 400+ contributors. Its sister project TiKV is a Cloud Native Interactive Landscape project. PingCAP envisioned their managed TiDB service, known as TiDB Cloud, to be multi-tenant, secure, cost-efficient, and to be compatible with different cloud providers and they chose Gardener.    Beezlabs uses Gardener to deliver Intelligent Process Automation platform, on multiple cloud providers and reduce costs and lock-in risks.    b’nerd uses Gardener as the core technology for its own managed Kubernetes as a Service solution and operates multiple Gardener installations for several cloud hosting service providers.    STACKIT is a digital brand of Europes’ biggest retailer, the Schwarz Group, which includes Lidl, Kaufland, but also production and recycling companies. It uses Gardener to offer public and private Kubernetes as a service in own data centers in Europe and targets to become the cloud provider for German and European small and mid-sized companies.    Supporting and managing multiple application landscapes on-premises and across different hyperscaler infrastructures can be painful. At T-Systems we use Gardener both for internal usage and to manage clusters for our customers. We love the openness of the project, the flexibility and the architecture that allows us to manage clusters around the world with only one team from one single pane of glass and to meet industry specific certification standards. The sovereignty by design is another great value, the technology implicitly brings along.    The German-based company 23 Technologies uses Gardener to offer an enterprise-class Kubernetes engine for industrial use cases as well as cloud service providers and offers managed and professional services for it. 23T is also the team behind okeanos.dev, a public service that can be used by anyone to try out gardener.    B1 Systems GmbH is a international provider of Linux \u0026 Open Source consulting, training, managed service \u0026 support. We are founded in 2004 and based in Germany. Our team of 140 Linux experts offers tailor-made solutions based on cloud \u0026 container technologies, virtualization \u0026 high availability as well as monitoring, system \u0026 configuration management. B1 is using Gardener internally and also set up solutions/environments for customers.    finleap connect GmbH is the leading independent Open Banking platform provider in Europe. It enables companies across a multitude of industries to provide the next generation of financial services by understanding how customers transact and interact. With its “full-stack” platform of solutions, finleap connect makes it possible for its clients to compliantly access the financial transactions data of customers, enrich said data with analytics tools, provide digital banking services and deliver high-quality, digital financial services products and services to customers. Gardener uniquly enables us to deploy our platform in Europe and across the globe in a uniform way on the providers preferred by our customers.    Codesphere is a Cloud IDE with integrated and automated deployment of web apps. It uses Gardener internally to manage clusters that host customer deployments and internal systems all over the world.    plusserver combines its own cloud offerings with hyperscaler platforms to provide individually tailored multi-cloud solutions. The plusserver Kubernetes Engine (PSKE) based on Gardener reduces the complexity in managing multi-cloud environments and enables companies to orchestrate their containers and cloud-native applications across a variety of platforms such as plusserver’s pluscloud open or hyperscalers such as AWS, either by mouseclick or via an API. With PSKE, companies remain vendor-independent and profit from guaranteed data sovereignty and data security due to GDPR-compliant cloud platforms in the certified plusserver data centers in Germany.    If you’re using Gardener and you aren’t on this list, submit a pull request! ","categories":"","description":"","excerpt":"See who is using Gardener Gardener adopters in production environments …","ref":"/adopter/","tags":"","title":"Adopters"},{"body":"Alerting Gardener uses Prometheus to gather metrics from each component. A Prometheus is deployed in each shoot control plane (on the seed) which is responsible for gathering control plane and cluster metrics. Prometheus can be configured to fire alerts based on these metrics and send them to an alertmanager. The alertmanager is responsible for sending the alerts to users and operators. This document describes how to setup alerting for:\n end-users/stakeholders/customers operators/administrators  Alerting for Users To receive email alerts as a user set the following values in the shoot spec:\nspec:  monitoring:  alerting:  emailReceivers:  - john.doe@example.com emailReceivers is a list of emails that will receive alerts if something is wrong with the shoot cluster. A list of alerts for users can be found here.\nAlerting for Operators Currently, Gardener supports two options for alerting:\n Email Alerting Sending Alerts to an external alertmanager  A list of operator alerts can be found here.\nEmail Alerting Gardener provides the option to deploy an alertmanager into each seed. This alertmanager is responsible for sending out alerts to operators for each shoot cluster in the seed. Only email alerts are supported by the alertmanager managed by Gardener. This is configurable by setting the Gardener controller manager configuration values alerting. See this on how to configure the Gardener’s SMTP secret. If the values are set, a secret with the label gardener.cloud/role: alerting will be created in the garden namespace of the garden cluster. This secret will be used by each alertmanager in each seed.\nExternal Alertmanager The alertmanager supports different kinds of alerting configurations. The alertmanager provided by Gardener only supports email alerts. If email is not sufficient, then alerts can be sent to an external alertmanager. Prometheus will send alerts to a URL and then alerts will be handled by the external alertmanager. This external alertmanager is operated and configured by the operator (i.e. Gardener does not configure or deploy this alertmanager). To configure sending alerts to an external alertmanager, create a secret in the virtual garden cluster in the garden namespace with the label: gardener.cloud/role: alerting. This secret needs to contain a URL to the external alertmanager and information regarding authentication. Supported authentication types are:\n No Authentication (none) Basic Authentication (basic) Mutual TLS (certificate)  Remote Alertmanager Examples Note: the url value cannot be prepended with http or https.\n# No Authentication apiVersion: v1 kind: Secret metadata:  labels:  gardener.cloud/role: alerting  name: alerting-auth  namespace: garden data:  # No Authentication  auth_type: base64(none)  url: base64(external.alertmanager.foo)   # Basic Auth  auth_type: base64(basic)  url: base64(extenal.alertmanager.foo)  username: base64(admin)  password: base64(password)   # Mutual TLS  auth_type: base64(certificate)  url: base64(external.alertmanager.foo)  ca.crt: base64(ca)  tls.crt: base64(certificate)  tls.key: base64(key)  insecure_skip_verify: base64(false)   # Email Alerts (internal alertmanager)  auth_type: base64(smtp)  auth_identity: base64(internal.alertmanager.auth_identity)  auth_password: base64(internal.alertmanager.auth_password)  auth_username: base64(internal.alertmanager.auth_username)  from: base64(internal.alertmanager.from)  smarthost: base64(internal.alertmanager.smarthost)  to: base64(internal.alertmanager.to) type: Opaque Configuring your External Alertmanager Please refer to the alertmanager documentation on how to configure an alertmanager.\nWe recommend you use at least the following inhibition rules in your alertmanager configuration to prevent excessive alerts:\ninhibit_rules: # Apply inhibition if the alert name is the same. - source_match:  severity: critical  target_match:  severity: warning  equal: ['alertname', 'service', 'cluster']  # Stop all alerts for type=shoot if there are VPN problems. - source_match:  service: vpn  target_match_re:  type: shoot  equal: ['type', 'cluster']  # Stop warning and critical alerts if there is a blocker - source_match:  severity: blocker  target_match_re:  severity: ^(critical|warning)$  equal: ['cluster']  # If the API server is down inhibit no worker nodes alert. No worker nodes depends on kube-state-metrics which depends on the API server. - source_match:  service: kube-apiserver  target_match_re:  service: nodes  equal: ['cluster']  # If API server is down inhibit kube-state-metrics alerts. - source_match:  service: kube-apiserver  target_match_re:  severity: info  equal: ['cluster']  # No Worker nodes depends on kube-state-metrics. Inhibit no worker nodes if kube-state-metrics is down. - source_match:  service: kube-state-metrics-shoot  target_match_re:  service: nodes  equal: ['cluster'] Below is a graph visualizing the inhibition rules:\n","categories":"","description":"","excerpt":"Alerting Gardener uses Prometheus to gather metrics from each …","ref":"/docs/gardener/monitoring/alerting/","tags":"","title":"Alerting"},{"body":"Gardener API server The Gardener API server is a Kubernetes-native extension based on its aggregation layer. It is registered via an APIService object and designed to run inside a Kubernetes cluster whose API it wants to extend.\nAfter registration, it exposes the following resources:\nCloudProfiles CloudProfiles are resources that describe a specific environment of an underlying infrastructure provider, e.g. AWS, Azure, etc. Each shoot has to reference a CloudProfile to declare the environment it should be created in. In a CloudProfile the gardener operator specifies certain constraints like available machine types, regions, which Kubernetes versions they want to offer, etc. End-users can read CloudProfiles to see these values, but only operators can change the content or create/delete them. When a shoot is created or updated then an admission plugin checks that only values are used that are allowed via the referenced CloudProfile.\nAdditionally, a CloudProfile may contain a providerConfig which is a special configuration dedicated for the infrastructure provider. Gardener does not evaluate or understand this config, but extension controllers might need for declaration of provider-specific constraints, or global settings.\nPlease see this example manifest and consult the documentation of your provider extension controller to get information about its providerConfig.\nSeeds Seeds are resources that represent seed clusters. Gardener does not care about how a seed cluster got created - the only requirement is that it is of at least Kubernetes v1.17 and passes the Kubernetes conformance tests. The Gardener operator has to either deploy the Gardenlet into the cluster they want to use as seed (recommended, then the Gardenlet will create the Seed object itself after bootstrapping), or they provide the kubeconfig to the cluster inside a secret (that is referenced by the Seed resource) and create the Seed resource themselves.\nPlease see this, this(, and optionally this) example manifests.\nShootQuotas In order to allow end-users not having their own dedicated infrastructure account to try out Gardener the operator can register an account owned by them that they allow to be used for trial clusters. Trial clusters can be put under quota such that they don’t consume too many resources (resulting in costs), and so that one user cannot consume all resources on their own. These clusters are automatically terminated after a specified time, but end-users may extend the lifetime manually if needed.\nPlease see this example manifest.\nProjects The first thing before creating a shoot cluster is to create a Project. A project is used to group multiple shoot clusters together. End-users can invite colleagues to the project to enable collaboration, and they can either make them admin or viewer. After an end-user has created a project they will get a dedicated namespace in the garden cluster for all their shoots.\nPlease see this example manifest.\nSecretBindings Now that the end-user has a namespace the next step is registering their infrastructure provider account.\nPlease see this example manifest and consult the documentation of the extension controller for the respective infrastructure provider to get information about which keys are required in this secret.\nAfter the secret has been created the end-user has to create a special SecretBinding resource that binds this secret. Later when creating shoot clusters they will reference such a binding.\nPlease see this example manifest.\nShoots Shoot cluster contain various settings that influence how end-user Kubernetes clusters will look like in the end. As Gardener heavily relies on extension controllers for operating system configuration, networking, and infrastructure specifics, the end-user has the possibility (and responsibility) to provide these provider-specific configurations as well. Such configurations are not evaluated by Gardener (because it doesn’t know/understand them), but they are only transported to the respective extension controller.\n⚠️ This means that any configuration issues/mistake on the end-user side that relates to a provider-specific flag or setting cannot be caught during the update request itself but only later during the reconciliation (unless a validator webhook has been registered in the garden cluster by an operator).\nPlease see this example manifest and consult the documentation of the provider extension controller to get information about its spec.provider.controlPlaneConfig, .spec.provider.infrastructureConfig, and .spec.provider.workers[].providerConfig.\n(Cluster)OpenIDConnectPresets Please see this separate documentation file.\nOverview Data Model ","categories":"","description":"","excerpt":"Gardener API server The Gardener API server is a Kubernetes-native …","ref":"/docs/gardener/concepts/apiserver/","tags":"","title":"API Server"},{"body":"Specification ProviderSpec Schema AWSMachineClass   AWSMachineClass TODO\n   Field Type Description      apiVersion   string    machine.sapcloud.io.v1alpha1      kind   string   AWSMachineClass     metadata    Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec    AWSMachineClassSpec     (Optional)      ami    string        region    string        blockDevices    []AWSBlockDeviceMappingSpec         ebsOptimized    bool        iam    AWSIAMProfileSpec         machineType    string        keyName    string        monitoring    bool        networkInterfaces    []AWSNetworkInterfaceSpec         tags    map[string]string        spotPrice    *string        secretRef    Kubernetes core/v1.SecretReference         credentialsSecretRef    Kubernetes core/v1.SecretReference            AlicloudMachineClass   AlicloudMachineClass TODO\n   Field Type Description      apiVersion   string    machine.sapcloud.io.v1alpha1      kind   string   AlicloudMachineClass     metadata    Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec    AlicloudMachineClassSpec     (Optional)      imageID    string        instanceType    string        region    string        zoneID    string        securityGroupID    string        vSwitchID    string        privateIPAddress    string        systemDisk    AlicloudSystemDisk         dataDisks    []AlicloudDataDisk         instanceChargeType    string        internetChargeType    string        internetMaxBandwidthIn    *int        internetMaxBandwidthOut    *int        spotStrategy    string        IoOptimized    string        tags    map[string]string        keyPairName    string        secretRef    Kubernetes core/v1.SecretReference         credentialsSecretRef    Kubernetes core/v1.SecretReference            AzureMachineClass   AzureMachineClass TODO\n   Field Type Description      apiVersion   string    machine.sapcloud.io.v1alpha1      kind   string   AzureMachineClass     metadata    Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec    AzureMachineClassSpec     (Optional)      location    string        tags    map[string]string        properties    AzureVirtualMachineProperties         resourceGroup    string        subnetInfo    AzureSubnetInfo         secretRef    Kubernetes core/v1.SecretReference         credentialsSecretRef    Kubernetes core/v1.SecretReference            GCPMachineClass   GCPMachineClass TODO\n   Field Type Description      apiVersion   string    machine.sapcloud.io.v1alpha1      kind   string   GCPMachineClass     metadata    Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec    GCPMachineClassSpec     (Optional)      canIpForward    bool        deletionProtection    bool        description    *string        disks    []*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.GCPDisk         labels    map[string]string        machineType    string        metadata    []*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.GCPMetadata         networkInterfaces    []*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.GCPNetworkInterface         scheduling    GCPScheduling         secretRef    Kubernetes core/v1.SecretReference         credentialsSecretRef    Kubernetes core/v1.SecretReference         serviceAccounts    []GCPServiceAccount         tags    []string        region    string        zone    string           Machine   Machine is the representation of a physical or virtual machine.\n   Field Type Description      apiVersion   string    machine.sapcloud.io.v1alpha1      kind   string   Machine     metadata    Kubernetes meta/v1.ObjectMeta     ObjectMeta for machine object\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec    MachineSpec     Spec contains the specification of the machine\n     class    ClassSpec     (Optional) Class contains the machineclass attributes of a machine\n    providerID    string    (Optional) ProviderID represents the provider’s unique ID given to a machine\n    nodeTemplate    NodeTemplateSpec     (Optional) NodeTemplateSpec describes the data a node should have when created from a template\n    MachineConfiguration    MachineConfiguration      (Members of MachineConfiguration are embedded into this type.) (Optional) Configuration for the machine-controller.\n       status    MachineStatus     Status contains fields depicting the status\n    MachineClass   MachineClass can be used to templatize and re-use provider configuration across multiple Machines / MachineSets / MachineDeployments.\n   Field Type Description      apiVersion   string    machine.sapcloud.io.v1alpha1      kind   string   MachineClass     metadata    Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     nodeTemplate    NodeTemplate     (Optional) NodeTemplate contains subfields to track all node resources and other node info required to scale nodegroup from zero\n    credentialsSecretRef    Kubernetes core/v1.SecretReference     CredentialsSecretRef can optionally store the credentials (in this case the SecretRef does not need to store them). This might be useful if multiple machine classes with the same credentials but different user-datas are used.\n    providerSpec    k8s.io/apimachinery/pkg/runtime.RawExtension     Provider-specific configuration to use during node creation.\n    provider    string    Provider is the combination of name and location of cloud-specific drivers.\n    secretRef    Kubernetes core/v1.SecretReference     SecretRef stores the necessary secrets such as credentials or userdata.\n    MachineDeployment   MachineDeployment enables declarative updates for machines and MachineSets.\n   Field Type Description      apiVersion   string    machine.sapcloud.io.v1alpha1      kind   string   MachineDeployment     metadata    Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec    MachineDeploymentSpec     (Optional) Specification of the desired behavior of the MachineDeployment.\n     replicas    int32    (Optional) Number of desired machines. This is a pointer to distinguish between explicit zero and not specified. Defaults to 0.\n    selector    Kubernetes meta/v1.LabelSelector     (Optional) Label selector for machines. Existing MachineSets whose machines are selected by this will be the ones affected by this MachineDeployment.\n    template    MachineTemplateSpec     Template describes the machines that will be created.\n    strategy    MachineDeploymentStrategy     (Optional) The MachineDeployment strategy to use to replace existing machines with new ones.\n    minReadySeconds    int32    (Optional) Minimum number of seconds for which a newly created machine should be ready without any of its container crashing, for it to be considered available. Defaults to 0 (machine will be considered available as soon as it is ready)\n    revisionHistoryLimit    *int32    (Optional) The number of old MachineSets to retain to allow rollback. This is a pointer to distinguish between explicit zero and not specified.\n    paused    bool    (Optional) Indicates that the MachineDeployment is paused and will not be processed by the MachineDeployment controller.\n    rollbackTo    RollbackConfig     (Optional) DEPRECATED. The config this MachineDeployment is rolling back to. Will be cleared after rollback is done.\n    progressDeadlineSeconds    *int32    (Optional) The maximum time in seconds for a MachineDeployment to make progress before it is considered to be failed. The MachineDeployment controller will continue to process failed MachineDeployments and a condition with a ProgressDeadlineExceeded reason will be surfaced in the MachineDeployment status. Note that progress will not be estimated during the time a MachineDeployment is paused. This is not set by default.\n       status    MachineDeploymentStatus     (Optional) Most recently observed status of the MachineDeployment.\n    MachineSet   MachineSet TODO\n   Field Type Description      apiVersion   string    machine.sapcloud.io.v1alpha1      kind   string   MachineSet     metadata    Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec    MachineSetSpec     (Optional)      replicas    int32    (Optional)     selector    Kubernetes meta/v1.LabelSelector     (Optional)     machineClass    ClassSpec     (Optional)     template    MachineTemplateSpec     (Optional)     minReadySeconds    int32    (Optional)        status    MachineSetStatus     (Optional)     PacketMachineClass   PacketMachineClass TODO\n   Field Type Description      apiVersion   string    machine.sapcloud.io.v1alpha1      kind   string   PacketMachineClass     metadata    Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec    PacketMachineClassSpec     (Optional)      facility    []string        machineType    string        billingCycle    string        OS    string        projectID    string        tags    []string        sshKeys    []string        userdata    string        secretRef    Kubernetes core/v1.SecretReference         credentialsSecretRef    Kubernetes core/v1.SecretReference            AWSBlockDeviceMappingSpec   (Appears on: AWSMachineClassSpec)     Field Type Description      deviceName    string    The device name exposed to the machine (for example, /dev/sdh or xvdh).\n    ebs    AWSEbsBlockDeviceSpec     Parameters used to automatically set up EBS volumes when the machine is launched.\n    noDevice    string    Suppresses the specified device included in the block device mapping of the AMI.\n    virtualName    string    The virtual device name (ephemeralN). Machine store volumes are numbered starting from 0. An machine type with 2 available machine store volumes can specify mappings for ephemeral0 and ephemeral1.The number of available machine store volumes depends on the machine type. After you connect to the machine, you must mount the volume.\nConstraints: For M3 machines, you must specify machine store volumes in the block device mapping for the machine. When you launch an M3 machine, we ignore any machine store volumes specified in the block device mapping for the AMI.\n    AWSEbsBlockDeviceSpec   (Appears on: AWSBlockDeviceMappingSpec)  Describes a block device for an EBS volume. Please also see https://docs.aws.amazon.com/goto/WebAPI/ec2-2016-11-15/EbsBlockDevice\n   Field Type Description      deleteOnTermination    *bool    Indicates whether the EBS volume is deleted on machine termination.\n    encrypted    bool    Indicates whether the EBS volume is encrypted. Encrypted Amazon EBS volumes may only be attached to machines that support Amazon EBS encryption.\n    iops    int64    The number of I/O operations per second (IOPS) that the volume supports. For io1, this represents the number of IOPS that are provisioned for the volume. For gp2, this represents the baseline performance of the volume and the rate at which the volume accumulates I/O credits for bursting. For more information about General Purpose SSD baseline performance, I/O credits, and bursting, see Amazon EBS Volume Types (http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html) in the Amazon Elastic Compute Cloud User Guide.\nConstraint: Range is 100-20000 IOPS for io1 volumes and 100-10000 IOPS for gp2 volumes.\nCondition: This parameter is required for requests to create io1 volumes; it is not used in requests to create gp2, st1, sc1, or standard volumes.\n    kmsKeyID    *string    Identifier (key ID, key alias, ID ARN, or alias ARN) for a customer managed CMK under which the EBS volume is encrypted.\nThis parameter is only supported on BlockDeviceMapping objects called by RunInstances (https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html), RequestSpotFleet (https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RequestSpotFleet.html), and RequestSpotInstances (https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RequestSpotInstances.html).\n    snapshotID    *string    The ID of the snapshot.\n    volumeSize    int64    The size of the volume, in GiB.\nConstraints: 1-16384 for General Purpose SSD (gp2), 4-16384 for Provisioned IOPS SSD (io1), 500-16384 for Throughput Optimized HDD (st1), 500-16384 for Cold HDD (sc1), and 1-1024 for Magnetic (standard) volumes. If you specify a snapshot, the volume size must be equal to or larger than the snapshot size.\nDefault: If you’re creating the volume from a snapshot and don’t specify a volume size, the default is the snapshot size.\n    volumeType    string    The volume type: gp2, io1, st1, sc1, or standard.\nDefault: standard\n    AWSIAMProfileSpec   (Appears on: AWSMachineClassSpec)  Describes an IAM machine profile.\n   Field Type Description      arn    string    The Amazon Resource Name (ARN) of the machine profile.\n    name    string    The name of the machine profile.\n    AWSMachineClassSpec   (Appears on: AWSMachineClass)  AWSMachineClassSpec is the specification of a AWSMachineClass.\n   Field Type Description      ami    string        region    string        blockDevices    []AWSBlockDeviceMappingSpec         ebsOptimized    bool        iam    AWSIAMProfileSpec         machineType    string        keyName    string        monitoring    bool        networkInterfaces    []AWSNetworkInterfaceSpec         tags    map[string]string        spotPrice    *string        secretRef    Kubernetes core/v1.SecretReference         credentialsSecretRef    Kubernetes core/v1.SecretReference         AWSNetworkInterfaceSpec   (Appears on: AWSMachineClassSpec)  Describes a network interface. Please also see https://docs.aws.amazon.com/goto/WebAPI/ec2-2016-11-15/MachineAWSNetworkInterfaceSpecification\n   Field Type Description      associatePublicIPAddress    *bool    Indicates whether to assign a public IPv4 address to an machine you launch in a VPC. The public IP address can only be assigned to a network interface for eth0, and can only be assigned to a new network interface, not an existing one. You cannot specify more than one network interface in the request. If launching into a default subnet, the default value is true.\n    deleteOnTermination    *bool    If set to true, the interface is deleted when the machine is terminated. You can specify true only if creating a new network interface when launching an machine.\n    description    *string    The description of the network interface. Applies only if creating a network interface when launching an machine.\n    securityGroupIDs    []string    The IDs of the security groups for the network interface. Applies only if creating a network interface when launching an machine.\n    subnetID    string    The ID of the subnet associated with the network string. Applies only if creating a network interface when launching an machine.\n    AlicloudDataDisk   (Appears on: AlicloudMachineClassSpec)     Field Type Description      name    string        category    string        description    string    (Optional)     encrypted    bool        deleteWithInstance    *bool        size    int        AlicloudMachineClassSpec   (Appears on: AlicloudMachineClass)  AlicloudMachineClassSpec is the specification of a AlicloudMachineClass.\n   Field Type Description      imageID    string        instanceType    string        region    string        zoneID    string        securityGroupID    string        vSwitchID    string        privateIPAddress    string        systemDisk    AlicloudSystemDisk         dataDisks    []AlicloudDataDisk         instanceChargeType    string        internetChargeType    string        internetMaxBandwidthIn    *int        internetMaxBandwidthOut    *int        spotStrategy    string        IoOptimized    string        tags    map[string]string        keyPairName    string        secretRef    Kubernetes core/v1.SecretReference         credentialsSecretRef    Kubernetes core/v1.SecretReference         AlicloudSystemDisk   (Appears on: AlicloudMachineClassSpec)  AlicloudSystemDisk describes SystemDisk for Alicloud.\n   Field Type Description      category    string        size    int        AzureDataDisk   (Appears on: AzureStorageProfile)     Field Type Description      name    string        lun    *int32        caching    string        storageAccountType    string        diskSizeGB    int32        AzureHardwareProfile   (Appears on: AzureVirtualMachineProperties)  AzureHardwareProfile is specifies the hardware settings for the virtual machine. Refer github.com/Azure/azure-sdk-for-go/arm/compute/models.go for VMSizes\n   Field Type Description      vmSize    string        AzureImageReference   (Appears on: AzureStorageProfile)  AzureImageReference is specifies information about the image to use. You can specify information about platform images, marketplace images, or virtual machine images. This element is required when you want to use a platform image, marketplace image, or virtual machine image, but is not used in other creation operations.\n   Field Type Description      id    string        urn    *string    Uniform Resource Name of the OS image to be used , it has the format ‘publisher:offer:sku:version’\n    AzureLinuxConfiguration   (Appears on: AzureOSProfile)  AzureLinuxConfiguration is specifies the Linux operating system settings on the virtual machine. For a list of supported Linux distributions, see Linux on Azure-Endorsed Distributions For running non-endorsed distributions, see Information for Non-Endorsed Distributions.\n   Field Type Description      disablePasswordAuthentication    bool        ssh    AzureSSHConfiguration         AzureMachineClassSpec   (Appears on: AzureMachineClass)  AzureMachineClassSpec is the specification of a AzureMachineClass.\n   Field Type Description      location    string        tags    map[string]string        properties    AzureVirtualMachineProperties         resourceGroup    string        subnetInfo    AzureSubnetInfo         secretRef    Kubernetes core/v1.SecretReference         credentialsSecretRef    Kubernetes core/v1.SecretReference         AzureMachineSetConfig   (Appears on: AzureVirtualMachineProperties)  AzureMachineSetConfig contains the information about the machine set\n   Field Type Description      id    string        kind    string        AzureManagedDiskParameters   (Appears on: AzureOSDisk)  AzureManagedDiskParameters is the parameters of a managed disk.\n   Field Type Description      id    string        storageAccountType    string        AzureNetworkInterfaceReference   (Appears on: AzureNetworkProfile)  AzureNetworkInterfaceReference is describes a network interface reference.\n   Field Type Description      id    string        properties    AzureNetworkInterfaceReferenceProperties         AzureNetworkInterfaceReferenceProperties   (Appears on: AzureNetworkInterfaceReference)  AzureNetworkInterfaceReferenceProperties is describes a network interface reference properties.\n   Field Type Description      primary    bool        AzureNetworkProfile   (Appears on: AzureVirtualMachineProperties)  AzureNetworkProfile is specifies the network interfaces of the virtual machine.\n   Field Type Description      networkInterfaces    AzureNetworkInterfaceReference         acceleratedNetworking    *bool        AzureOSDisk   (Appears on: AzureStorageProfile)  AzureOSDisk is specifies information about the operating system disk used by the virtual machine. For more information about disks, see About disks and VHDs for Azure virtual machines.\n   Field Type Description      name    string        caching    string        managedDisk    AzureManagedDiskParameters         diskSizeGB    int32        createOption    string        AzureOSProfile   (Appears on: AzureVirtualMachineProperties)  AzureOSProfile is specifies the operating system settings for the virtual machine.\n   Field Type Description      computerName    string        adminUsername    string        adminPassword    string        customData    string        linuxConfiguration    AzureLinuxConfiguration         AzureSSHConfiguration   (Appears on: AzureLinuxConfiguration)  AzureSSHConfiguration is SSH configuration for Linux based VMs running on Azure\n   Field Type Description      publicKeys    AzureSSHPublicKey         AzureSSHPublicKey   (Appears on: AzureSSHConfiguration)  AzureSSHPublicKey is contains information about SSH certificate public key and the path on the Linux VM where the public key is placed.\n   Field Type Description      path    string        keyData    string        AzureStorageProfile   (Appears on: AzureVirtualMachineProperties)  AzureStorageProfile is specifies the storage settings for the virtual machine disks.\n   Field Type Description      imageReference    AzureImageReference         osDisk    AzureOSDisk         dataDisks    []AzureDataDisk         AzureSubResource   (Appears on: AzureVirtualMachineProperties)  AzureSubResource is the Sub Resource definition.\n   Field Type Description      id    string        AzureSubnetInfo   (Appears on: AzureMachineClassSpec)  AzureSubnetInfo is the information containing the subnet details\n   Field Type Description      vnetName    string        vnetResourceGroup    *string        subnetName    string        AzureVirtualMachineProperties   (Appears on: AzureMachineClassSpec)  AzureVirtualMachineProperties is describes the properties of a Virtual Machine.\n   Field Type Description      hardwareProfile    AzureHardwareProfile         storageProfile    AzureStorageProfile         osProfile    AzureOSProfile         networkProfile    AzureNetworkProfile         availabilitySet    AzureSubResource         identityID    *string        zone    *int        machineSet    AzureMachineSetConfig         ClassSpec   (Appears on: MachineSetSpec, MachineSpec)  ClassSpec is the class specification of machine\n   Field Type Description      apiGroup    string    API group to which it belongs\n    kind    string    Kind for machine class\n    name    string    Name of machine class\n    ConditionStatus (string alias)\n  (Appears on: MachineDeploymentCondition, MachineSetCondition)  CurrentStatus   (Appears on: MachineStatus)  CurrentStatus contains information about the current status of Machine.\n   Field Type Description      phase    MachinePhase         timeoutActive    bool        lastUpdateTime    Kubernetes meta/v1.Time     Last update time of current status\n    GCPDisk   GCPDisk describes disks for GCP.\n   Field Type Description      autoDelete    *bool        boot    bool        sizeGb    int64        type    string        interface    string        image    string        labels    map[string]string        GCPMachineClassSpec   (Appears on: GCPMachineClass)  GCPMachineClassSpec is the specification of a GCPMachineClass.\n   Field Type Description      canIpForward    bool        deletionProtection    bool        description    *string        disks    []*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.GCPDisk         labels    map[string]string        machineType    string        metadata    []*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.GCPMetadata         networkInterfaces    []*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.GCPNetworkInterface         scheduling    GCPScheduling         secretRef    Kubernetes core/v1.SecretReference         credentialsSecretRef    Kubernetes core/v1.SecretReference         serviceAccounts    []GCPServiceAccount         tags    []string        region    string        zone    string        GCPMetadata   GCPMetadata describes metadata for GCP.\n   Field Type Description      key    string        value    *string        GCPNetworkInterface   GCPNetworkInterface describes network interfaces for GCP\n   Field Type Description      disableExternalIP    bool        network    string        subnetwork    string        GCPScheduling   (Appears on: GCPMachineClassSpec)  GCPScheduling describes scheduling configuration for GCP.\n   Field Type Description      automaticRestart    bool        onHostMaintenance    string        preemptible    bool        GCPServiceAccount   (Appears on: GCPMachineClassSpec)  GCPServiceAccount describes service accounts for GCP.\n   Field Type Description      email    string        scopes    []string        LastOperation   (Appears on: MachineSetStatus, MachineStatus, MachineSummary)  LastOperation suggests the last operation performed on the object\n   Field Type Description      description    string    Description of the current operation\n    lastUpdateTime    Kubernetes meta/v1.Time     Last update time of current operation\n    state    MachineState     State of operation\n    type    MachineOperationType     Type of operation\n    MachineConfiguration   (Appears on: MachineSpec)  MachineConfiguration describes the configurations useful for the machine-controller.\n   Field Type Description      drainTimeout    Kubernetes meta/v1.Duration     (Optional) MachineDraintimeout is the timeout after which machine is forcefully deleted.\n    healthTimeout    Kubernetes meta/v1.Duration     (Optional) MachineHealthTimeout is the timeout after which machine is declared unhealhty/failed.\n    creationTimeout    Kubernetes meta/v1.Duration     (Optional) MachineCreationTimeout is the timeout after which machinie creation is declared failed.\n    maxEvictRetries    *int32    (Optional) MaxEvictRetries is the number of retries that will be attempted while draining the node.\n    nodeConditions    *string    (Optional) NodeConditions are the set of conditions if set to true for MachineHealthTimeOut, machine will be declared failed.\n    MachineDeploymentCondition   (Appears on: MachineDeploymentStatus)  MachineDeploymentCondition describes the state of a MachineDeployment at a certain point.\n   Field Type Description      type    MachineDeploymentConditionType     Type of MachineDeployment condition.\n    status    ConditionStatus     Status of the condition, one of True, False, Unknown.\n    lastUpdateTime    Kubernetes meta/v1.Time     The last time this condition was updated.\n    lastTransitionTime    Kubernetes meta/v1.Time     Last time the condition transitioned from one status to another.\n    reason    string    The reason for the condition’s last transition.\n    message    string    A human readable message indicating details about the transition.\n    MachineDeploymentConditionType (string alias)\n  (Appears on: MachineDeploymentCondition)  MachineDeploymentSpec   (Appears on: MachineDeployment)  MachineDeploymentSpec is the specification of the desired behavior of the MachineDeployment.\n   Field Type Description      replicas    int32    (Optional) Number of desired machines. This is a pointer to distinguish between explicit zero and not specified. Defaults to 0.\n    selector    Kubernetes meta/v1.LabelSelector     (Optional) Label selector for machines. Existing MachineSets whose machines are selected by this will be the ones affected by this MachineDeployment.\n    template    MachineTemplateSpec     Template describes the machines that will be created.\n    strategy    MachineDeploymentStrategy     (Optional) The MachineDeployment strategy to use to replace existing machines with new ones.\n    minReadySeconds    int32    (Optional) Minimum number of seconds for which a newly created machine should be ready without any of its container crashing, for it to be considered available. Defaults to 0 (machine will be considered available as soon as it is ready)\n    revisionHistoryLimit    *int32    (Optional) The number of old MachineSets to retain to allow rollback. This is a pointer to distinguish between explicit zero and not specified.\n    paused    bool    (Optional) Indicates that the MachineDeployment is paused and will not be processed by the MachineDeployment controller.\n    rollbackTo    RollbackConfig     (Optional) DEPRECATED. The config this MachineDeployment is rolling back to. Will be cleared after rollback is done.\n    progressDeadlineSeconds    *int32    (Optional) The maximum time in seconds for a MachineDeployment to make progress before it is considered to be failed. The MachineDeployment controller will continue to process failed MachineDeployments and a condition with a ProgressDeadlineExceeded reason will be surfaced in the MachineDeployment status. Note that progress will not be estimated during the time a MachineDeployment is paused. This is not set by default.\n    MachineDeploymentStatus   (Appears on: MachineDeployment)  MachineDeploymentStatus is the most recently observed status of the MachineDeployment.\n   Field Type Description      observedGeneration    int64    (Optional) The generation observed by the MachineDeployment controller.\n    replicas    int32    (Optional) Total number of non-terminated machines targeted by this MachineDeployment (their labels match the selector).\n    updatedReplicas    int32    (Optional) Total number of non-terminated machines targeted by this MachineDeployment that have the desired template spec.\n    readyReplicas    int32    (Optional) Total number of ready machines targeted by this MachineDeployment.\n    availableReplicas    int32    (Optional) Total number of available machines (ready for at least minReadySeconds) targeted by this MachineDeployment.\n    unavailableReplicas    int32    (Optional) Total number of unavailable machines targeted by this MachineDeployment. This is the total number of machines that are still required for the MachineDeployment to have 100% available capacity. They may either be machines that are running but not yet available or machines that still have not been created.\n    conditions    []MachineDeploymentCondition     Represents the latest available observations of a MachineDeployment’s current state.\n    collisionCount    *int32    (Optional) Count of hash collisions for the MachineDeployment. The MachineDeployment controller uses this field as a collision avoidance mechanism when it needs to create the name for the newest MachineSet.\n    failedMachines    []*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.MachineSummary     (Optional) FailedMachines has summary of machines on which lastOperation Failed\n    MachineDeploymentStrategy   (Appears on: MachineDeploymentSpec)  MachineDeploymentStrategy describes how to replace existing machines with new ones.\n   Field Type Description      type    MachineDeploymentStrategyType     (Optional) Type of MachineDeployment. Can be “Recreate” or “RollingUpdate”. Default is RollingUpdate.\n    rollingUpdate    RollingUpdateMachineDeployment     (Optional) Rolling update config params. Present only if MachineDeploymentStrategyType =\nRollingUpdate. TODO: Update this to follow our convention for oneOf, whatever we decide it to be.\n    MachineDeploymentStrategyType (string alias)\n  (Appears on: MachineDeploymentStrategy)  MachineOperationType (string alias)\n  (Appears on: LastOperation)  MachineOperationType is a label for the operation performed on a machine object.\nMachinePhase (string alias)\n  (Appears on: CurrentStatus)  MachinePhase is a label for the condition of a machines at the current time.\nMachineSetCondition   (Appears on: MachineSetStatus)  MachineSetCondition describes the state of a machine set at a certain point.\n   Field Type Description      type    MachineSetConditionType     Type of machine set condition.\n    status    ConditionStatus     Status of the condition, one of True, False, Unknown.\n    lastTransitionTime    Kubernetes meta/v1.Time     (Optional) The last time the condition transitioned from one status to another.\n    reason    string    (Optional) The reason for the condition’s last transition.\n    message    string    (Optional) A human readable message indicating details about the transition.\n    MachineSetConditionType (string alias)\n  (Appears on: MachineSetCondition)  MachineSetConditionType is the condition on machineset object\nMachineSetSpec   (Appears on: MachineSet)  MachineSetSpec is the specification of a MachineSet.\n   Field Type Description      replicas    int32    (Optional)     selector    Kubernetes meta/v1.LabelSelector     (Optional)     machineClass    ClassSpec     (Optional)     template    MachineTemplateSpec     (Optional)     minReadySeconds    int32    (Optional)     MachineSetStatus   (Appears on: MachineSet)  MachineSetStatus holds the most recently observed status of MachineSet.\n   Field Type Description      replicas    int32    Replicas is the number of actual replicas.\n    fullyLabeledReplicas    int32    (Optional) The number of pods that have labels matching the labels of the pod template of the replicaset.\n    readyReplicas    int32    (Optional) The number of ready replicas for this replica set.\n    availableReplicas    int32    (Optional) The number of available replicas (ready for at least minReadySeconds) for this replica set.\n    observedGeneration    int64    (Optional) ObservedGeneration is the most recent generation observed by the controller.\n    machineSetCondition    []MachineSetCondition     (Optional) Represents the latest available observations of a replica set’s current state.\n    lastOperation    LastOperation     LastOperation performed\n    failedMachines    []github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.MachineSummary     (Optional) FailedMachines has summary of machines on which lastOperation Failed\n    MachineSpec   (Appears on: Machine, MachineTemplateSpec)  MachineSpec is the specification of a Machine.\n   Field Type Description      class    ClassSpec     (Optional) Class contains the machineclass attributes of a machine\n    providerID    string    (Optional) ProviderID represents the provider’s unique ID given to a machine\n    nodeTemplate    NodeTemplateSpec     (Optional) NodeTemplateSpec describes the data a node should have when created from a template\n    MachineConfiguration    MachineConfiguration      (Members of MachineConfiguration are embedded into this type.) (Optional) Configuration for the machine-controller.\n    MachineState (string alias)\n  (Appears on: LastOperation)  MachineState is a current state of the machine.\nMachineStatus   (Appears on: Machine)  MachineStatus holds the most recently observed status of Machine.\n   Field Type Description      node    string    Node string\n    conditions    []Kubernetes core/v1.NodeCondition     Conditions of this machine, same as node\n    lastOperation    LastOperation     Last operation refers to the status of the last operation performed\n    currentStatus    CurrentStatus     Current status of the machine object\n    lastKnownState    string    (Optional) LastKnownState can store details of the last known state of the VM by the plugins. It can be used by future operation calls to determine current infrastucture state\n    MachineSummary   MachineSummary store the summary of machine.\n   Field Type Description      name    string    Name of the machine object\n    providerID    string    ProviderID represents the provider’s unique ID given to a machine\n    lastOperation    LastOperation     Last operation refers to the status of the last operation performed\n    ownerRef    string    OwnerRef\n    MachineTemplateSpec   (Appears on: MachineDeploymentSpec, MachineSetSpec)  MachineTemplateSpec describes the data a machine should have when created from a template\n   Field Type Description      metadata    Kubernetes meta/v1.ObjectMeta     (Optional) Standard object’s metadata. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec    MachineSpec     (Optional) Specification of the desired behavior of the machine. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status\n     class    ClassSpec     (Optional) Class contains the machineclass attributes of a machine\n    providerID    string    (Optional) ProviderID represents the provider’s unique ID given to a machine\n    nodeTemplate    NodeTemplateSpec     (Optional) NodeTemplateSpec describes the data a node should have when created from a template\n    MachineConfiguration    MachineConfiguration      (Members of MachineConfiguration are embedded into this type.) (Optional) Configuration for the machine-controller.\n       NodeTemplate   (Appears on: MachineClass)  NodeTemplate contains subfields to track all node resources and other node info required to scale nodegroup from zero\n   Field Type Description      capacity    Kubernetes core/v1.ResourceList     Capacity contains subfields to track all node resources required to scale nodegroup from zero\n    instanceType    string    Instance type of the node belonging to nodeGroup\n    region    string    Region of the expected node belonging to nodeGroup\n    zone    string    Zone of the expected node belonging to nodeGroup\n    NodeTemplateSpec   (Appears on: MachineSpec)  NodeTemplateSpec describes the data a node should have when created from a template\n   Field Type Description      metadata    Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec    Kubernetes core/v1.NodeSpec     (Optional) NodeSpec describes the attributes that a node is created with.\n     podCIDR    string    (Optional) PodCIDR represents the pod IP range assigned to the node.\n    podCIDRs    []string    (Optional) podCIDRs represents the IP ranges assigned to the node for usage by Pods on that node. If this field is specified, the 0th entry must match the podCIDR field. It may contain at most 1 value for each of IPv4 and IPv6.\n    providerID    string    (Optional) ID of the node assigned by the cloud provider in the format: ://\n    unschedulable    bool    (Optional) Unschedulable controls node schedulability of new pods. By default, node is schedulable. More info: https://kubernetes.io/docs/concepts/nodes/node/#manual-node-administration\n    taints    []Kubernetes core/v1.Taint     (Optional) If specified, the node’s taints.\n    configSource    Kubernetes core/v1.NodeConfigSource     (Optional) Deprecated. If specified, the source of the node’s configuration. The DynamicKubeletConfig feature gate must be enabled for the Kubelet to use this field. This field is deprecated as of 1.22: https://git.k8s.io/enhancements/keps/sig-node/281-dynamic-kubelet-configuration\n    externalID    string    (Optional) Deprecated. Not all kubelets will set this field. Remove field after 1.13. see: https://issues.k8s.io/61966\n       OpenStackMachineClass   OpenStackMachineClass TODO\n   Field Type Description      metadata    Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec    OpenStackMachineClassSpec     (Optional)      imageID    string        imageName    string        region    string        availabilityZone    string        flavorName    string        keyName    string        securityGroups    []string        tags    map[string]string        networkID    string        networks    []OpenStackNetwork         subnetID    *string        secretRef    Kubernetes core/v1.SecretReference         credentialsSecretRef    Kubernetes core/v1.SecretReference         podNetworkCidr    string        rootDiskSize    int        useConfigDrive    *bool    in GB\n    serverGroupID    *string           OpenStackMachineClassSpec   (Appears on: OpenStackMachineClass)  OpenStackMachineClassSpec is the specification of a OpenStackMachineClass.\n   Field Type Description      imageID    string        imageName    string        region    string        availabilityZone    string        flavorName    string        keyName    string        securityGroups    []string        tags    map[string]string        networkID    string        networks    []OpenStackNetwork         subnetID    *string        secretRef    Kubernetes core/v1.SecretReference         credentialsSecretRef    Kubernetes core/v1.SecretReference         podNetworkCidr    string        rootDiskSize    int        useConfigDrive    *bool    in GB\n    serverGroupID    *string        OpenStackNetwork   (Appears on: OpenStackMachineClassSpec)     Field Type Description      id    string        name    string    takes priority before name\n    podNetwork    bool        PacketMachineClassSpec   (Appears on: PacketMachineClass)  PacketMachineClassSpec is the specification of a PacketMachineClass.\n   Field Type Description      facility    []string        machineType    string        billingCycle    string        OS    string        projectID    string        tags    []string        sshKeys    []string        userdata    string        secretRef    Kubernetes core/v1.SecretReference         credentialsSecretRef    Kubernetes core/v1.SecretReference         RollbackConfig   (Appears on: MachineDeploymentSpec)     Field Type Description      revision    int64    (Optional) The revision to rollback to. If set to 0, rollback to the last revision.\n    RollingUpdateMachineDeployment   (Appears on: MachineDeploymentStrategy)  Spec to control the desired behavior of rolling update.\n   Field Type Description      maxUnavailable    k8s.io/apimachinery/pkg/util/intstr.IntOrString     (Optional) The maximum number of machines that can be unavailable during the update. Value can be an absolute number (ex: 5) or a percentage of desired machines (ex: 10%). Absolute number is calculated from percentage by rounding down. This can not be 0 if MaxSurge is 0. By default, a fixed value of 1 is used. Example: when this is set to 30%, the old MC can be scaled down to 70% of desired machines immediately when the rolling update starts. Once new machines are ready, old MC can be scaled down further, followed by scaling up the new MC, ensuring that the total number of machines available at all times during the update is at least 70% of desired machines.\n    maxSurge    k8s.io/apimachinery/pkg/util/intstr.IntOrString     (Optional) The maximum number of machines that can be scheduled above the desired number of machines. Value can be an absolute number (ex: 5) or a percentage of desired machines (ex: 10%). This can not be 0 if MaxUnavailable is 0. Absolute number is calculated from percentage by rounding up. By default, a value of 1 is used. Example: when this is set to 30%, the new MC can be scaled up immediately when the rolling update starts, such that the total number of old and new machines do not exceed 130% of desired machines. Once old machines have been killed, new MC can be scaled up further, ensuring that total number of machines running at any time during the update is atmost 130% of desired machines.\n      Generated with gen-crd-api-reference-docs \n","categories":"","description":"","excerpt":"Specification ProviderSpec Schema AWSMachineClass   AWSMachineClass …","ref":"/docs/other-components/machine-controller-manager/docs/documents/apis/","tags":"","title":"Apis"},{"body":"Admission Plugins Similar to the kube-apiserver, the gardener-apiserver comes with a few in-tree managed admission plugins. If you want to get an overview of the what and why of admission plugins then this document might be a good start.\nThis document lists all existing admission plugins with a short explanation of what it is responsible for.\nClusterOpenIDConnectPreset, OpenIDConnectPreset (both enabled by default)\nThese admission controllers react on CREATE operations for Shoots. If the Shoot does not specify any OIDC configuration (.spec.kubernetes.kubeAPIServer.oidcConfig=nil) then it tries to find a matching ClusterOpenIDConnectPreset or OpenIDConnectPreset, respectively. If there are multiples that match then the one with the highest weight “wins”. In this case, the admission controller will default the OIDC configuration in the Shoot.\nControllerRegistrationResources (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for ControllerRegistrations. It validates that there exists only one ControllerRegistration in the system that is primarily responsible for a given kind/type resource combination. This prevents misconfiguration by the Gardener administrator/operator.\nCustomVerbAuthorizer (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Projects. It validates whether the user is bound to a RBAC role with the modify-spec-tolerations-whitelist verb in case the user tries to change the .spec.tolerations.whitelist field of the respective Project resource. Usually, regular project members are not bound to this custom verb, allowing the Gardener administrator to manage certain toleration whitelists on Project basis.\nDeletionConfirmation (enabled by default)\nThis admission controller reacts on DELETE operations for Projects and Shoots and ShootStates. It validates that the respective resource is annotated with a deletion confirmation annotation, namely confirmation.gardener.cloud/deletion=true. Only if this annotation is present it allows the DELETE operation to pass. This prevents users from accidental/undesired deletions.\nExposureClass (enabled by default)\nThis admission controller reacts on Create operations for Shootss. It mutates Shoot resources which has an ExposureClass referenced by merging their both shootSelectors and/or tolerations into the Shoot resource.\nExtensionValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for BackupEntrys, BackupBuckets, Seeds, and Shoots. For all the various extension types in the specifications of these objects, it validates whether there exists a ControllerRegistration in the system that is primarily responsible for the stated extension type(s). This prevents misconfigurations that would otherwise allow users to create such resources with extension types that don’t exist in the cluster, effectively leading to failing reconciliation loops.\nExtensionLabels (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for BackupBuckets, BackupEntrys, CloudProfiles, Seeds, SecretBindings and Shoots. For all the various extension types in the specifications of these objects, it adds a corresponding label in the resource. This would allow extension admission webhooks to filter out the resources they are responsible for and ignore all others. This label is of the form \u003cextension-type\u003e.extensions.gardener.cloud/\u003cextension-name\u003e : \"true\". For example, an extension label for provider extension type aws, looks like provider.extensions.gardener.cloud/aws : \"true\".\nProjectValidator (enabled by default)\nThis admission controller reacts on CREATE operations for Projects. It prevents creating Projects with a non-empty .spec.namespace if the value in .spec.namespace does not start with garden-.\n⚠️ This admission plugin will be removed in a future release and its business logic will be incorporated into the static validation of the gardener-apiserver.\nResourceQuota (enabled by default)\nThis admission controller enables object count ResourceQuotas for Gardener resources, e.g. Shoots, SecretBindings, Projects, etc..\n ⚠️ In addition to this admission plugin, the ResourceQuota controller must be enabled for the Kube-Controller-Manager of your Garden cluster.\n ResourceReferenceManager (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for CloudProfiles, Projects, SecretBindings, Seeds, and Shoots. Generally, it checks whether referred resources stated in the specifications of these objects exist in the system (e.g., if a referenced Secret exists). However, it also has some special behaviours for certain resources:\n CloudProfiles: It rejects removing Kubernetes or machine image versions if there is at least one Shoot that refers to them. Projects: It sets the .spec.createdBy field for newly created Project resources, and defaults the .spec.owner field in case it is empty (to the same value of .spec.createdBy). Seeds: It rejects changing the .spec.settings.shootDNS.enabled value if there is at least one Shoot that refers to this seed. Shoots: It sets the gardener.cloud/created-by=\u003cusername\u003e annotation for newly created Shoot resources.  SeedValidator (enabled by default)\nThis admission controller reacts on DELETE operations for Seeds. Rejects the deletion if Shoot(s) reference the seed cluster.\nShootDNS (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It tries to assign a default domain to the Shoot if it gets scheduled to a seed that enables DNS for shoots (.spec.settings.shootDNS.enabled=true). It also validates that the DNS configuration (.spec.dns) is not set if the seed disables DNS for shoots.\nShootNodeLocalDNSEnabledByDefault (disabled by default)\nThis admission controller reacts on CREATE operations for Shoots. If enabled, it will enable node local dns within the shoot cluster (see this doc) by setting spec.systemComponents.nodeLocalDNS.enabled=true for newly created Shoots. Already existing Shoots and new Shoots that explicitly disable node local dns (spec.systemComponents.nodeLocalDNS.enabled=false) will not be affected by this admission plugin.\nShootQuotaValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It validates the resource consumption declared in the specification against applicable Quota resources. Only if the applicable Quota resources admit the configured resources in the Shoot then it allows the request. Applicable Quotas are referred in the SecretBinding that is used by the Shoot.\nShootVPAEnabledByDefault (disabled by default)\nThis admission controller reacts on CREATE operations for Shoots. If enabled, it will enable the managed VerticalPodAutoscaler components (see this doc) by setting spec.kubernetes.verticalPodAutoscaler.enabled=true for newly created Shoots. Already existing Shoots and new Shoots that explicitly disable VPA (spec.kubernetes.verticalPodAutoscaler.enabled=false) will not be affected by this admission plugin.\nShootTolerationRestriction (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It validates the .spec.tolerations used in Shoots against the whitelist of its Project, or against the whitelist configured in the admission controller’s configuration, respectively. Additionally, it defaults the .spec.tolerations in Shoots with those configured in its Project, and those configured in the admission controller’s configuration, respectively.\nShootValidator (enabled by default)\nThis admission controller reacts on CREATE, UPDATE and DELETE operations for Shoots. It validates certain configurations in the specification against the referred CloudProfile (e.g., machine images, machine types, used Kubernetes version, …). Generally, it performs validations that cannot be handled by the static API validation due to their dynamic nature (e.g., when something needs to be checked against referred resources). Additionally, it takes over certain defaulting tasks (e.g., default machine image for worker pools).\nShootManagedSeed (enabled by default)\nThis admission controller reacts on UPDATE and DELETE operations for Shoots. It validates certain configuration values in the specification that are specific to ManagedSeeds (e.g. the nginx-addon of the Shoot has to be disabled, the Shoot VPA has to be enabled). It rejects the deletion if the Shoot is referred to by a ManagedSeed.\nManagedSeedValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for ManagedSeedss. It validates certain configuration values in the specification against the referred Shoot, for example Seed provider, network ranges, DNS domain, etc. Similarly to ShootValidator, it performs validations that cannot be handled by the static API validation due to their dynamic nature. Additionally, it performs certain defaulting tasks, making sure that configuration values that are not specified are defaulted to the values of the referred Shoot, for example Seed provider, network ranges, DNS domain, etc.\nManagedSeedShoot (enabled by default)\nThis admission controller reacts on DELETE operations for ManagedSeeds. It rejects the deletion if there are Shoots that are scheduled onto the Seed that is registered by the ManagedSeed.\nShootDNSRewriting (disabled by default)\nThis admission controller reacts on CREATE operations for Shoots. If enabled, it adds a set of common suffixes configured in its admission plugin configuration to the Shoot (spec.systemComponents.coreDNS.rewriting.commonSuffixes) (see this doc). Already existing Shoots will not be affected by this admission plugin.\n","categories":"","description":"","excerpt":"Admission Plugins Similar to the kube-apiserver, the …","ref":"/docs/gardener/concepts/apiserver_admission_plugins/","tags":"","title":"APIServer Admission Plugins"},{"body":"APIServerSNI environment variable injection If the Gardener administrator has enabled APIServerSNI feature gate for a particular Seed cluster, then in each Shoot cluster’s kube-system namespace a DaemonSet called apiserver-proxy is deployed. It routes traffic to the upstream Shoot Kube APIServer. See the APIServer SNI GEP for more details.\nTo skip this extra network hop, a mutating webhook called apiserver-proxy.networking.gardener.cloud is deployed next to the API server in the Seed. It adds KUBERNETES_SERVICE_HOST environment variable to each container and init container that do not specify it. See the webhook repository for more information.\nOpt-out of pod injection In some cases it’s desirable to opt-out of Pod injection:\n DNS is disabled on that individual Pod, but it still needs to talk to the kube-apiserver. Want to test the kube-proxy and kubelet in-cluster discovery.  Opt-out of pod injection for specific pods To opt out of the injection, the Pod should be labeled with apiserver-proxy.networking.gardener.cloud/inject: disable e.g.:\napiVersion: apps/v1 kind: Deployment metadata:  name: nginx  labels:  app: nginx spec:  replicas: 1  selector:  matchLabels:  app: nginx  template:  metadata:  labels:  app: nginx  apiserver-proxy.networking.gardener.cloud/inject: disable  spec:  containers:  - name: nginx  image: nginx:1.14.2  ports:  - containerPort: 80 Opt-out of pod injection on namespace level To opt out of the injection of all Pods in a namespace, you should label your namespace with apiserver-proxy.networking.gardener.cloud/inject: disable e.g.:\napiVersion: v1 kind: Namespace metadata:  labels:  apiserver-proxy.networking.gardener.cloud/inject: disable  name: my-namespace or via kubectl for existing namespace:\nkubectl label namespace my-namespace apiserver-proxy.networking.gardener.cloud/inject=disable  NOTE: Please be aware that it’s not possible to disable injection on namespace level and enable it for individual pods in it.\n Opt-out of pod injection for the entire cluster If the injection is causing problems for different workloads and ignoring individual pods or namespaces is not possible, then the feature could be disabled for the entire cluster with the alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector annotation with value disable on the Shoot resource itself:\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata:  annotations:  alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector: 'disable'  name: my-cluster or via kubectl for existing shoot cluster:\nkubectl label shoot my-cluster alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector=disable  NOTE: Please be aware that it’s not possible to disable injection on cluster level and enable it for individual pods in it.\n ","categories":"","description":"","excerpt":"APIServerSNI environment variable injection If the Gardener …","ref":"/docs/gardener/usage/apiserver-sni-injection/","tags":"","title":"APIServer SNI Injection"},{"body":"Official Definition - What is Kubernetes?  “Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.”\n Introduction - Basic Principle The foundation of the Gardener (providing Kubernetes Clusters as a Service) is Kubernetes itself, because Kubernetes is the go-to solution to manage software in the Cloud, even when it’s Kubernetes itself (see also OpenStack which is provisioned more and more on top of Kubernetes as well).\nWhile self-hosting, meaning to run Kubernetes components inside Kubernetes, is a popular topic in the community, we apply a special pattern catering to the needs of our cloud platform to provision hundreds or even thousands of clusters. We take a so-called “seed” cluster and seed the control plane (such as the API server, scheduler, controllers, etcd persistence and others) of an end-user cluster, which we call “shoot” cluster, as pods into the “seed” cluster. That means one “seed” cluster, of which we will have one per IaaS and region, hosts the control planes of multiple “shoot” clusters. That allows us to avoid dedicated hardware/virtual machines for the “shoot” cluster control planes. We simply put the control plane into pods/containers and since the “seed” cluster watches them, they can be deployed with a replica count of 1 and only need to be scaled out when the control plane gets under pressure, but no longer for HA reasons. At the same time, the deployments get simpler (standard Kubernetes deployment) and easier to update (standard Kubernetes rolling update). The actual “shoot” cluster consists only out of the worker nodes (no control plane) and therefore the users may get full administrative access to their clusters.\nSetting The Scene - Components and Procedure We provide a central operator UI, which we call the “Gardener Dashboard”. It talks to a dedicated cluster, which we call the “Garden” cluster and uses custom resources managed by an aggregated API server, one of the general extension concepts of Kubernetes) to represent “shoot” clusters. In this “Garden” cluster runs the “Gardener”, which is basically a Kubernetes controller that watches the custom resources and acts upon them, i.e. creates, updates/modifies, or deletes “shoot” clusters. The creation follows basically these steps:\n Create a namespace in the “seed” cluster for the “shoot” cluster which will host the “shoot” cluster control plane Generate secrets and credentials which the worker nodes will need to talk to the control plane Create the infrastructure (using Terraform), which basically consists out of the network setup) Deploy the “shoot” cluster control plane into the “shoot” namespace in the “seed” cluster, containing the “machine-controller-manager” pod Create machine CRDs in the “seed” cluster, describing the configuration and the number of worker machines for the “shoot” (the machine-controller-manager watches the CRDs and creates virtual machines out of it) Wait for the “shoot” cluster API server to become responsive (pods will be scheduled, persistent volumes and load balancers are created by Kubernetes via the respective cloud provider) Finally we deploy kube-system daemons like kube-proxy and further add-ons like the dashboard into the “shoot” cluster and the cluster becomes active  Overview Architecture Diagram Detailed Architecture Diagram Note: The kubelet as well as the pods inside the “shoot” cluster talk through the front-door (load balancer IP; public Internet) to its “shoot” cluster API server running in the “seed” cluster. The reverse communication from the API server to the pod, service, and node networks happens through a VPN connection that we deploy into “seed” and “shoot” clusters.\n","categories":"","description":"","excerpt":"Official Definition - What is Kubernetes?  “Kubernetes is an …","ref":"/docs/gardener/concepts/architecture/","tags":"","title":"Architecture"},{"body":"Auditing Kubernetes for Secure Setup Increasing the Security of all Gardener Stakeholders In summer 2018, the Gardener project team asked Kinvolk to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work is to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a Control-Plane-as-a-Service with a network air gap.\nAlong the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.\nMajor Findings From this experience, we’d like to share a few examples of security issues that could happen on a Kubernetes installation and how to fix them.\nAlban Crequy (Kinvolk) and Dirk Marwinski (SAP SE) gave a presentation entitled Hardening Multi-Cloud Kubernetes Clusters as a Service at KubeCon 2018 in Shanghai presenting some of the findings.\nHere is a summary of the findings:\n  Privilege escalation due to insecure configuration of the Kubernetes API server\n Root cause: Same certificate authority (CA) is used for both the API server and the proxy that allows accessing the API server. Risk: Users can get access to the API server. Recommendation: Always use different CAs.    Exploration of the control plane network with malicious HTTP-redirects\n  Root cause: See detailed description below.\n  Risk: Provoked error message contains full HTTP payload from an existing endpoint which can be exploited. The contents of the payload depends on your setup, but can potentially be user data, configuration data, and credentials.\n  Recommendation:\n Use the latest version of Gardener Ensure the seed cluster’s container network supports network policies. Clusters that have been created with Kubify are not protected as Flannel is used there which doesn’t support network policies.      Reading private AWS metadata via Grafana\n Root cause: It is possible to configuring a new custom data source in Grafana, we could send HTTP requests to target the control Risk: Users can get the “user-data” for the seed cluster from the metadata service and retrieve a kubeconfig for that Kubernetes cluster Recommendation: Lockdown Grafana features to only what’s necessary in this setup, block all unnecessary outgoing traffic, move Grafana to a different network, lockdown unauthenticated endpoints    Scenario 1: Privilege Escalation with Insecure API Server In most configurations, different components connect directly to the Kubernetes API server, often using a kubeconfig with a client certificate. The API server is started with the flag:\n/hyperkube apiserver --client-ca-file=/srv/kubernetes/ca/ca.crt ... The API server will check whether the client certificate presented by kubectl, kubelet, scheduler or another component is really signed by the configured certificate authority for clients.\nThe API server can have many clients of various kinds\nHowever, it is possible to configure the API server differently for use with an intermediate authenticating proxy. The proxy will authenticate the client with its own custom method and then issue HTTP requests to the API server with additional HTTP headers specifying the user name and group name. The API server should only accept HTTP requests with HTTP headers from a legitimate proxy. To allow the API server to check incoming requests, you need pass on a list of certificate authorities (CAs) to it. Requests coming from a proxy are only accepted if they use a client certificate that is signed by one of the CAs of that list.\n--requestheader-client-ca-file=/srv/kubernetes/ca/ca-proxy.crt --requestheader-username-headers=X-Remote-User --requestheader-group-headers=X-Remote-Group API server clients can reach the API server through an authenticating proxy\nSo far, so good. But what happens if malicious user “Mallory” tries to connect directly to the API server and reuses the HTTP headers to pretend to be someone else?\nWhat happens when a client bypasses the proxy, connecting directly to the API server?\nWith a correct configuration, Mallory’s kubeconfig will have a certificate signed by the API server certificate authority but not signed by the proxy certificate authority. So the API server will not accept the extra HTTP header “X-Remote-Group: system:masters”.\nYou only run into an issue when the same certificate authority is used for both the API server and the proxy. Then, any Kubernetes client certificate can be used to take the role of different user or group as the API server will accept the user header and group header.\nThe kubectl tool does not normally add those HTTP headers but it’s pretty easy to generate the corresponding HTTP requests manually.\nWe worked on improving the Kubernetes documentation to make clearer that this configuration should be avoided.\nScenario 2: Exploration of the Control Plane Network with Malicious HTTP-Redirects The API server is a central component of Kubernetes and many components initiate connections to it, including the Kubelet running on worker nodes. Most of the requests from those clients will end up updating Kubernetes objects (pods, services, deployments, and so on) in the etcd database but the API server usually does not need to initiate TCP connections itself.\nThe API server is mostly a component that receives requests\nHowever, there are exceptions. Some kubectl commands will trigger the API server to open a new connection to the Kubelet. Kubectl exec is one of those commands. In order to get the standard I/Os from the pod, the API server will start an HTTP connection to the Kubelet on the worker node where the pod is running. Depending on the container runtime used, it can be done in different ways, but one way to do it is for the Kubelet to reply with a HTTP-302 redirection to the Container Runtime Interface (CRI). Basically, the Kubelet is telling the API server to get the streams from CRI itself directly instead of forwarding. The redirection from the Kubelet will only change the port and path from the URL; the IP address will not be changed because the Kubelet and the CRI component run on the same worker node.\nBut the API server also initiates some connections, for example, to worker nodes\nIt’s often quite easy for users of a Kubernetes cluster to get access to worker nodes and tamper with the Kubelet. They could be given explicit SSH access or they could be given a kubeconfig with enough privileges to create privileged pods or even just pods with “host” volumes.\nIn contrast, users — even those with “system:masters” permissions or “root” rights — are often not given access to the control plane. On setups like for example GKE or Gardener, the control plane is running on separate nodes, with a different administrative access. It could be hosted on a different cloud provider account. So users are not free to explore the internal network in the control plane.\nWhat would happen if a user was tampering with the Kubelet to make it maliciously redirect kubectl exec requests to a different random endpoint? Most likely the given endpoint would not speak the streaming server protocol, so there would be an error. However, the full HTTP payload from the endpoint is included in the error message printed by kubectl exec.\nThe API server is tricked to connect to other components\nThe impact of this issue depends on the specific setup. But in many configurations, we could find a metadata service (such as the AWS metadata service) containing user data, configurations and credentials. The setup we explored had a different AWS account and a different EC2 instance profile for the worker nodes and the control plane. This issue allowed users to get access to the AWS metadata service in the context of the control plane, which they should not have access to.\nWe have reported this issue to the Kubernetes Security mailing list and the public pull request that addresses the issue has been merged PR#66516. It provides a way to enforce HTTP redirect validation (disabled by default).\nBut there are several other ways that users could trigger the API server to generate HTTP requests and get the reply payload back, so it is advised to isolate the API server and other components from the network as additional precautious measures. Depending on where the API server runs, it could be with Kubernetes Network Policies, EC2 Security Groups or just iptables directly. Following the defense in depth principle, it is a good idea to apply the API server HTTP redirect validation when it is available as well as firewall rules.\nIn Gardener, this has been fixed with Kubernetes network policies along with changes to ensure the API server does not need to contact the metadata service. You can see more details in the announcements on the Gardener mailing list. This is tracked in CVE-2018-2475.\nTo be protected from this issue, stakeholders should:\n Use the latest version of Gardener Ensure the seed cluster’s container network supports network policies. Clusters that have been created with Kubify are not protected as Flannel is used there which doesn’t support network policies.  Scenario 3: Reading Private AWS Metadata via Grafana For our tests, we had access to a Kubernetes setup where users are not only given access to the API server in the control plane, but also to a Grafana instance that is used to gather data from their Kubernetes clusters via Prometheus. The control plane is managed and users don’t have access to the nodes that it runs. They can only access the API server and Grafana via a load balancer. The internal network of the control plane is therefore hidden to users.\nPrometheus and Grafana can be used to monitor worker nodes\nUnfortunately, that setup was not protecting the control plane network from nosy users. By configuring a new custom data source in Grafana, we could send HTTP requests to target the control plane network, for example the AWS metadata service. The reply payload is not displayed on the Grafana Web UI but it is possible to access it from the debugging console of the Chrome browser.\nCredentials can be retrieved from the debugging console of Chrome\nAdding a Grafana data source is a way to issue HTTP requests to arbitrary targets\nIn that installation, users could get the “user-data” for the seed cluster from the metadata service and retrieve a kubeconfig for that Kubernetes cluster.\nThere are many possible measures to avoid this situation: lockdown Grafana features to only what’s necessary in this setup, block all unnecessary outgoing traffic, move Grafana to a different network, lockdown unauthenticated endpoints.\nConclusion The three scenarios above show pitfalls with a Kubernetes setup. A lot of them were specific to the Kubernetes installation: different cloud providers or different configurations will show different weakness. Users should no longer be given access to Grafana.\n","categories":"","description":"A few insecure configurations in Kubernetes","excerpt":"A few insecure configurations in Kubernetes","ref":"/docs/guides/applications/insecure-configuration/","tags":"","title":"Auditing Kubernetes for Secure Setup"},{"body":"Packages:\n  authentication.gardener.cloud/v1alpha1   authentication.gardener.cloud/v1alpha1  Package v1alpha1 is a version of the API.\nResource Types:  AdminKubeconfigRequest  AdminKubeconfigRequest   AdminKubeconfigRequest can be used to request a kubeconfig with admin credentials for a Shoot cluster.\n   Field Description      apiVersion string   authentication.gardener.cloud/v1alpha1      kind string  AdminKubeconfigRequest    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  AdminKubeconfigRequestSpec     Spec is the specification of the AdminKubeconfigRequest.\n     expirationSeconds  int64    (Optional) ExpirationSeconds is the requested validity duration of the credential. The credential issuer may return a credential with a different validity duration so a client needs to check the ‘expirationTimestamp’ field in a response. Defaults to 1 hour.\n       status  AdminKubeconfigRequestStatus     Status is the status of the AdminKubeconfigRequest.\n    AdminKubeconfigRequestSpec   (Appears on: AdminKubeconfigRequest)  AdminKubeconfigRequestSpec contains the expiration time of the kubeconfig.\n   Field Description      expirationSeconds  int64    (Optional) ExpirationSeconds is the requested validity duration of the credential. The credential issuer may return a credential with a different validity duration so a client needs to check the ‘expirationTimestamp’ field in a response. Defaults to 1 hour.\n    AdminKubeconfigRequestStatus   (Appears on: AdminKubeconfigRequest)  AdminKubeconfigRequestStatus is the status of the AdminKubeconfigRequest containing the kubeconfig and expiration of the credential.\n   Field Description      kubeconfig  []byte    Kubeconfig contains the kubeconfig with cluster-admin privileges for the shoot cluster.\n    expirationTimestamp  Kubernetes meta/v1.Time     ExpirationTimestamp is the expiration timestamp of the returned credential.\n      Generated with gen-crd-api-reference-docs \n","categories":"","description":"","excerpt":"Packages:\n  authentication.gardener.cloud/v1alpha1 …","ref":"/docs/gardener/api-reference/authentication/","tags":"","title":"Authentication"},{"body":"Authentication of Gardener control plane components against the Garden cluster Note: This document refers to Gardener’s API server, admission controller, controller manager and scheduler components. Any reference to the term Gardener control plane component can be replaced with any of the mentioned above.\nThere are several authentication possibilities depending on whether or not the concept of Virtual Garden is used.\nVirtual Garden is not used, i.e., the runtime Garden cluster is also the target Garden cluster. Automounted Service Account Token The easiest way to deploy a Gardener control plane component will be to not provide kubeconfig at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.\nService Account Token Volume Projection Another solution will be to use Service Account Token Volume Projection combined with a kubeconfig referencing a token file (see example below).\napiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://default.kubernetes.svc.cluster.local  name: garden contexts: - context:  cluster: garden  user: garden  name: garden current-context: garden users: - name: garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token This will allow for automatic rotation of the service account token by the kubelet. The configuration can be achieved by setting both .Values.global.\u003cGardenerControlPlaneComponent\u003e.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.\u003cGardenerControlPlaneComponent\u003e.kubeconfig in the respective chart’s values.yaml file.\nVirtual Garden is used, i.e., the runtime Garden cluster is different from the target Garden cluster. Service Account The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the target cluster. Then use the generated service account token and craft a kubeconfig which will be used by the workload in the runtime cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting .Values.global.deployment.virtualGarden.enabled: true and following these steps:\n Deploy the application part of the charts in the target cluster. Get the service account token and craft the kubeconfig. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Client Certificate Another solution will be to bind the roles in the target cluster to a User subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both .Values.global.deployment.virtualGarden.enabled: true and .Values.global.deployment.virtualGarden.\u003cGardenerControlPlaneComponent\u003e.user.name, then following these steps:\n Generate a client certificate for the target cluster for the respective user. Deploy the application part of the charts in the target cluster. Craft a kubeconfig using the already generated client certificate. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Projected Service Account Token This approach requires an already deployed and configured oidc-webhook-authenticator for the target cluster. Also the runtime cluster should be registered as a trusted identity provider in the target cluster. Then projected service accounts tokens from the runtime cluster can be used to authenticate against the target cluster. The needed steps are as follows:\n Deploy OWA and establish the needed trust. Set .Values.global.deployment.virtualGarden.enabled: true and .Values.global.deployment.virtualGarden.\u003cGardenerControlPlaneComponent\u003e.user.name. Note: username value will depend on the trust configuration, e.g., \u003cprefix\u003e:system:serviceaccount:\u003cnamespace\u003e:\u003cserviceaccount\u003e Set .Values.global.\u003cGardenerControlPlaneComponent\u003e.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.\u003cGardenerControlPlaneComponent\u003e.serviceAccountTokenVolumeProjection.audience. Note: audience value will depend on the trust configuration, e.g., \u003ccliend-id-from-trust-config\u003e. Craft a kubeconfig (see example below). Deploy the application part of the charts in the target cluster. Deploy the runtime part of the charts in the runtime cluster.  apiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://virtual-garden.api  name: virtual-garden contexts: - context:  cluster: virtual-garden  user: virtual-garden  name: virtual-garden current-context: virtual-garden users: - name: virtual-garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token ","categories":"","description":"","excerpt":"Authentication of Gardener control plane components against the Garden …","ref":"/docs/gardener/deployment/authentication_gardener_control_plane/","tags":"","title":"Authentication Gardener Control Plane"},{"body":"Introduction With kubectl you can easily deploy an image from your local environment.\nHowever, what if you want to use a automated deployment script on a CI server (e.g. Jenkins), but don’t want to store the KUBECONFIG on that server?\nYou can use kubectl and connect to the API-server of your cluster.\nPrerequisites   Create a service account user\nkubectl create serviceaccount deploy-user -n default   Bind a role to the newly created serviceuser\n !!! Warning !!! In this example the preconfigured role edit and the namespace default is being used, please adjust the role to a more strict scope! see https://kubernetes.io/docs/admin/authorization/rbac/\n kubectl create rolebinding deploy-default-role --clusterrole=edit --serviceaccount=default:deploy-user --namespace=default   Get the URL of your API-server\nAPISERVER=$(kubectl config view | grep server | cut -f 2- -d \":\" | tr -d \" \")   Get the service account\nSERVICEACCOUNT=$(kubectl get serviceaccount deploy-user -n default -o=jsonpath={.secrets[0].name})   Generate a token for the serviceaccount\nTOKEN=$(kubectl get secret -n default $SERVICEACCOUNT -o=jsonpath={.data.token} | base64 -D)   Usage You can deploy your app without setting the kubeconfig locally, you just need to pass the environment variables (e.g. store them in the Jenkins credentials store)\nkubectl --server=${APIServer} --token=${TOKEN} --insecure-skip-tls-verify=true apply --filename myapp.yaml ","categories":"","description":"Automated deployment with kubectl","excerpt":"Automated deployment with kubectl","ref":"/docs/guides/client_tools/kubectl-apiserver/","tags":"","title":"Automated Deployment"},{"body":"Azure Permissions The following document describes the required Azure actions manage a Shoot cluster on Azure split by the different Azure provider/services.\nBe aware some actions are just required if particilar deployment sceanrios or features e.g. bring your own vNet, use Azure-file, let the Shoot act as Seed etc. should be used.\nMicrosoft.Compute # Required if a non zonal cluster based on Availability Set should be used. Microsoft.Compute/availabilitySets/delete Microsoft.Compute/availabilitySets/read Microsoft.Compute/availabilitySets/write # Required to let Kubernetes manage Azure disks. Microsoft.Compute/disks/delete Microsoft.Compute/disks/read Microsoft.Compute/disks/write # Required for to fetch meta information about disk and virtual machines sizes. Microsoft.Compute/locations/diskOperations/read Microsoft.Compute/locations/operations/read Microsoft.Compute/locations/vmSizes/read # Required if csi snapshot capabilities should be used and/or the Shoot should act as a Seed. Microsoft.Compute/snapshots/delete Microsoft.Compute/snapshots/read Microsoft.Compute/snapshots/write # Required to let Gardener/Machine-Controller-Manager manage the cluster nodes/machines. Microsoft.Compute/virtualMachines/delete Microsoft.Compute/virtualMachines/read Microsoft.Compute/virtualMachines/start/action Microsoft.Compute/virtualMachines/write # Required if a non zonal cluster based on VMSS Flex (VMO) should be used. Microsoft.Compute/virtualMachineScaleSets/delete Microsoft.Compute/virtualMachineScaleSets/read Microsoft.Compute/virtualMachineScaleSets/write Microsoft.ManagedIdentity # Required if a user provided Azure managed identity should attached to the cluster nodes. Microsoft.ManagedIdentity/userAssignedIdentities/assign/action Microsoft.ManagedIdentity/userAssignedIdentities/read Microsoft.MarketplaceOrdering # Required if nodes/machines should be created with images hosted on the Azure Marketplace. Microsoft.MarketplaceOrdering/offertypes/publishers/offers/plans/agreements/read Microsoft.MarketplaceOrdering/offertypes/publishers/offers/plans/agreements/write Microsoft.Network # Required to let Kubernetes manage services of type 'LoadBalancer'. Microsoft.Network/loadBalancers/backendAddressPools/join/action Microsoft.Network/loadBalancers/delete Microsoft.Network/loadBalancers/read Microsoft.Network/loadBalancers/write # Required in case the Shoot should use NatGateway(s). Microsoft.Network/natGateways/delete Microsoft.Network/natGateways/join/action Microsoft.Network/natGateways/read Microsoft.Network/natGateways/write # Required to let Gardener/Machine-Controller-Manager manage the cluster nodes/machines. Microsoft.Network/networkInterfaces/delete Microsoft.Network/networkInterfaces/ipconfigurations/join/action Microsoft.Network/networkInterfaces/ipconfigurations/read Microsoft.Network/networkInterfaces/join/action Microsoft.Network/networkInterfaces/read Microsoft.Network/networkInterfaces/write # Required to let Gardener maintain the basic infrastructure of the Shoot cluster and maintaing LoadBalancer services. Microsoft.Network/networkSecurityGroups/delete Microsoft.Network/networkSecurityGroups/join/action Microsoft.Network/networkSecurityGroups/read Microsoft.Network/networkSecurityGroups/write # Required for managing LoadBalancers and NatGateways. Microsoft.Network/publicIPAddresses/delete Microsoft.Network/publicIPAddresses/join/action Microsoft.Network/publicIPAddresses/read Microsoft.Network/publicIPAddresses/write # Required for managing the basic infrastructure of a cluster and maintaing LoadBalancer services. Microsoft.Network/routeTables/delete Microsoft.Network/routeTables/join/action Microsoft.Network/routeTables/read Microsoft.Network/routeTables/routes/delete Microsoft.Network/routeTables/routes/read Microsoft.Network/routeTables/routes/write Microsoft.Network/routeTables/write # Required to let Gardener maintain the basic infrastructure of the Shoot cluster. # Only a subset is required for the bring your own vNet scenario. Microsoft.Network/virtualNetworks/delete # not required for bring your own vnet Microsoft.Network/virtualNetworks/read Microsoft.Network/virtualNetworks/subnets/delete Microsoft.Network/virtualNetworks/subnets/join/action Microsoft.Network/virtualNetworks/subnets/read Microsoft.Network/virtualNetworks/subnets/write Microsoft.Network/virtualNetworks/write # not required for bring your own vnet Microsoft.Resources # Required to let Gardener maintain the basic infrastructure of the Shoot cluster. Microsoft.Resources/subscriptions/resourceGroups/delete Microsoft.Resources/subscriptions/resourceGroups/read Microsoft.Resources/subscriptions/resourceGroups/write Microsoft.Storage # Required if Azure File should be used and/or if the Shoot should act as Seed. Microsoft.Storage/operations/read Microsoft.Storage/storageAccounts/blobServices/containers/delete Microsoft.Storage/storageAccounts/blobServices/containers/read Microsoft.Storage/storageAccounts/blobServices/containers/write Microsoft.Storage/storageAccounts/blobServices/read Microsoft.Storage/storageAccounts/delete Microsoft.Storage/storageAccounts/listkeys/action Microsoft.Storage/storageAccounts/read Microsoft.Storage/storageAccounts/write ","categories":"","description":"","excerpt":"Azure Permissions The following document describes the required Azure …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-azure/docs/azure-permissions/","tags":"","title":"Azure Permissions"},{"body":"Backup and restore Kubernetes uses Etcd as the key-value store for its resource definitions. Gardener supports the backup and restore of etcd. It is the responsibility of the shoot owners to backup the workload data.\nGardener uses etcd-backup-restore component to backup the etcd backing the Shoot cluster regularly and restore in case of disaster. It is deployed as sidecar via etcd-druid. This doc mainly focuses on the backup and restore configuration used by Gardener when deploying these components. For more details on the design and internal implementation details, please refer GEP-06 and documentation on individual repository.\nBucket provisioning Refer the backup bucket extension document to know details about configuring backup bucket.\nBackup Policy etcd-backup-restore supports full snapshot and delta snapshots over full snapshot. In Gardener, this configuration is currently hard-coded to following parameters:\n Full Snapshot Schedule:  Daily, 24hr interval. For each Shoot, the schedule time in a day is randomized based on the configured Shoot maintenance window.   Delta Snapshot schedule:  At 5min interval. If aggregated events size since last snapshot goes beyond 100Mib.   Backup History / Garbage backup deletion policy:  Gardener configure backup restore to have Exponential garbage collection policy. As per policy, following backups are retained. All full backups and delta backups for the previous hour. Latest full snapshot of each previous hour for the day. Latest full snapshot of each previous day for 7 days. Latest full snapshot of the previous 4 weeks. Garbage Collection is configured at 12hr interval.   Listing:  Gardener don’t have any API to list out the backups. To find the backup list, admin can checkout the BackupEntry resource associated with Shoot which holds the bucket and prefix details on object store.    Restoration Restoration process of etcd is automated through the etcd-backup-restore component from latest snapshot. Gardener dosen’t support Point-In-Time-Recovery (PITR) of etcd. In case of etcd disaster, the etcd is recovered from latest backup automatically. For further details, please refer the doc. Post restoration of etcd, the Shoot reconciliation loop brings back the cluster to same state.\nAgain, Shoot owner is responsible for maintaining the backup/restore of his workload. Gardener does only take care of the cluster’s etcd.\n","categories":"","description":"","excerpt":"Backup and restore Kubernetes uses Etcd as the key-value store for its …","ref":"/docs/gardener/concepts/backup-restore/","tags":"","title":"Backup Restore"},{"body":"Contract: BackupBucket Resource The Gardener project features a sub-project called etcd-backup-restore to take periodic backups of etcd backing Shoot clusters. It demands the bucket (or its equivalent in different object store providers) to be created and configured externally with appropriate credentials. The BackupBucket resource takes this responsibility in Gardener.\nBefore introducing the BackupBucket extension resource, Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see AWS Backup). Now, Gardener commissions an external, provider-specific controller to take over this task. You can also refer to backupInfra proposal documentation to get idea about how the transition was done and understand the resource in broader scope.\nWhat is the Scope of Bucket? A bucket will be provisioned per Seed. So, a backup of every Shoot created on that Seed will be stored under a different shoot specific prefix under the bucket. For the backup of the Shoot rescheduled on different Seed, it will continue to use the same bucket.\nWhat is the Lifespan of BackupBucket? The bucket associated with BackupBucket will be created at the creation of the Seed. And as per current implementation, it will also be deleted on deletion of the Seed, if there isn’t any BackupEntry resource associated with it.\nIn the future, we plan to introduce a schedule for BackupBucket - the deletion logic for the BackupBucket resource, which will reschedule it on different available Seeds on deletion or failure of a health check for the currently associated seed. In that case, the BackupBucket will be deleted only if there isn’t any schedulable Seed available and there isn’t any associated BackupEntry resource.\nWhat Needs to be Implemented to Support a New Infrastructure Provider? As part of the seed flow, Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: BackupBucket metadata:  name: foo spec:  type: azure  providerConfig:  \u003csome-optional-provider-specific-backupbucket-configuration\u003e  region: eu-west-1  secretRef:  name: backupprovider  namespace: shoot--foo--bar The .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. This provider secret will be configured by the Gardener operator in the Seed resource and propagated over there by the seed controller.\nAfter your controller has created the required bucket, if required, it generates the secret to access the objects in the bucket and put a reference to it in status. This secret is supposed to be used by Gardener or eventually a BackupEntry resource and etcd-backup-restore component to backup the etcd.\nIn order to support a new infrastructure provider, you need to write a controller that watches all BackupBuckets with .spec.type=\u003cmy-provider-name\u003e. You can take a look at the below referenced example implementation for the Azure provider.\nReferences and Additional Resources  BackupBucket API Reference Exemplary implementation for the Azure provider BackupEntry resource documentation Shared bucket proposal  ","categories":"","description":"","excerpt":"Contract: BackupBucket Resource The Gardener project features a …","ref":"/docs/gardener/extensions/backupbucket/","tags":"","title":"BackupBucket"},{"body":"Contract: BackupEntry Resource The Gardener project features a sub-project called etcd-backup-restore to take periodic backups of etcd backing Shoot clusters. It demands the bucket (or its equivalent in different object store providers) access credentials to be created and configured externally with appropriate credentials. The BackupEntry resource takes this responsibility in Gardener to provide this information by creating a secret specific to the component.\nThat being said, the core motivation for introducing this resource was to support retention of backups post deletion of Shoot. The etcd-backup-restore components take responsibility of garbage collecting old backups out of the defined period. Once a shoot is deleted, we need to persist the backups for few days. Hence, Gardener uses the BackupEntry resource for this housekeeping work post deletion of a Shoot. The BackupEntry resource is responsible for shoot specific prefix under referred bucket.\nBefore introducing the BackupEntry extension resource, Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see AWS Backup). Now, Gardener commissions an external, provider-specific controller to take over this task. You can also refer to backupInfra proposal documentation to get idea about how the transition was done and understand the resource in broader scope.\nWhat is the Lifespan of BackupEntry? The bucket associated with BackupEntry will be created by using a BackupBucket resource. The BackupEntry resource will be created as a part of the Shoot creation. But resources might continue to exist post deletion of a Shoot (see Gardenlet for more details).\nWhat Needs to be Implemented to Support a New Infrastructure Provider? As part of the shoot flow, Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: BackupEntry metadata:  name: shoot--foo--bar spec:  type: azure  providerConfig:  \u003csome-optional-provider-specific-backup-bucket-configuration\u003e  backupBucketProviderStatus:  \u003csome-optional-provider-specific-backup-bucket-status\u003e  region: eu-west-1  bucketName: foo  secretRef:  name: backupprovider  namespace: shoot--foo--bar The .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. This provider secret will be propagated from the BackupBucket resource by the shoot controller.\nYour controller is supposed to create the etcd-backup secret in the control plane namespace of a shoot. This secret is supposed to be used by Gardener or eventually by the etcd-backup-restore component to backup the etcd. The controller implementation should clean up the objects created under the shoot specific prefix in the bucket equivalent to the name of the BackupEntry resource.\nIn order to support a new infrastructure provider, you need to write a controller that watches all the BackupBuckets with .spec.type=\u003cmy-provider-name\u003e. You can take a look at the below referenced example implementation for the Azure provider.\nReferences and Additional Resources  BackupEntry API Reference Exemplary implementation for the Azure provider BackupBucket resource documentation Shared bucket proposal Gardener-controller-manager-component-config API specification  ","categories":"","description":"","excerpt":"Contract: BackupEntry Resource The Gardener project features a …","ref":"/docs/gardener/extensions/backupentry/","tags":"","title":"BackupEntry"},{"body":"Contract: Bastion resource The Gardener project allows users to connect to Shoot worker nodes via SSH. As nodes are usually firewalled and not directly accessible from the public internet, GEP-15 introduced the concept of “Bastions”. A bastion is a dedicated server that only serves to allow SSH ingress to the worker nodes.\nBastion resources contain the user’s public SSH key and IP address, in order to provision the server accordingly: The public key is put onto the Bastion and SSH ingress is only authorized for the given IP address (in fact, it’s not a single IP address, but a set of IP ranges, however for most purposes a single IP is be used).\nWhat is the lifespan of Bastion? Once a Bastion has been created in the garden, it will be replicated to the appropriate seed cluster, where a controller then reconciles a server and firewall rules etc. on the cloud provider used by the target Shoot. When the Bastion is ready (i.e. has a public IP), that IP is stored in the Bastion’s status and from there is picked up by the garden cluster and gardenctl eventually.\nTo make multiple SSH sessions possible, the existence of the Bastion is not directly tied to the execution of gardenctl: users can exit out of gardenctl and use ssh manually to connect to the bastion and worker nodes.\nHowever, Bastions have an expiry date, after which they will be garbage collected.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: Bastion metadata:  name: mybastion  namespace: shoot--foo--bar spec:  type: aws  # userData is base64-encoded cloud provider user data; this contains the  # user's SSH key  userData: IyEvYmluL2Jhc2ggL....Nlcgo=  ingress:  - ipBlock:  cidr: 192.88.99.0/32 # this is most likely the user's IP address Your controller is supposed to create a new instance at the given cloud provider, firewall it to only allow SSH (TCP port 22) from the given IP blocks, and then to configure the firewall for the worker nodes to allow SSH from the bastion instance. When a Bastion is deleted, all these changes need to be reverted.\nImplementation details ConfigValidator interface For bastion controllers, the generic Reconciler also delegates to a ConfigValidator interface that contains a single Validate method. This method is called by the generic Reconciler at the beginning of every reconciliation, and can be implemented by the extension to validate the .spec.providerConfig part of the Bastion resource with the respective cloud provider, typically the existence and validity of cloud provider resources such as VPCs, Images.\nThe Validate method returns a list of errors. If this list is non-empty, the generic Reconciler will fail with an error. This error will have the error code ERR_CONFIGURATION_PROBLEM, unless there is at least one error in the list that has its ErrorType field set to field.ErrorTypeInternal.\nReferences and additional resources  Bastion API Reference Exemplary implementation for the AWS provider GEP-15  ","categories":"","description":"","excerpt":"Contract: Bastion resource The Gardener project allows users to …","ref":"/docs/gardener/extensions/bastion/","tags":"","title":"Bastion"},{"body":"GEP-15: Bastion Management and SSH Key Pair Rotation Table of Contents  Motivation  Goals Non-Goals   Proposal  Involved Components SSH Flow Resource Example   SSH Key Pair Rotation  Rotation Proposal    Motivation gardenctl (v1) has the functionality to setup ssh sessions to the targeted shoot cluster (nodes). To this end, infrastructure resources like VMs, public IPs, firewall rules, etc. have to be created. gardenctl will clean up the resources after termination of the ssh session (or rather when the operator is done with her work). However, there were issues in the past where these infrastructure resources were not properly cleaned up afterwards, e.g. due to some error (no retries either). Hence, the proposal is to have a dedicated controller (for each infrastructure) that manages the infrastructure resources and their cleanup. The current gardenctl also re-used the ssh node credentials for the bastion host. While that’s possible, it would be safer to rather use personal or generated ssh key pairs to access the bastion host. The static shoot-specific ssh key pair should be rotated regularly, e.g. once in the maintenance time window. This also means that we cannot create the node VMs anymore with infrastructure public keys as these cannot be revoked or rotated (e.g. in AWS) without terminating the VM itself.\nChanges to the Bastion resource should only be allowed for controllers on seeds that are responsible for it. This cannot be restricted when using custom resources. The proposal, as outlined below, suggests to implement the necessary changes in the gardener core components and to adapt the SeedAuthorizer to consider Bastion resources that the Gardener API Server serves.\nGoals  Operators can request and will be granted time-limited ssh access to shoot cluster nodes via bastion hosts. To that end, requestors must present their public ssh key and only this will be installed into sshd on the bastion hosts. The bastion hosts will be firewalled and ingress traffic will be permitted only from the client IP of the requestor. Except for traffic on port 22 to the cluster worker nodes, no egress from the bastion is allowed. The actual node ssh private key (resp. key pair) will be rotated by Gardener and access to the nodes is only possible with this constantly rotated key pair and not with the personal one that is used only for the bastion host. Bastion host and access is granted only for the extent of this operator request (of course multiple ssh sessions are possible, in parallel or repeatedly, but after “the time is up”, access is no longer possible). By these means (personal public key and allow-listed client IP) nobody else can use (a.k.a. impersonate) the requestor (not even other operators). Necessary infrastructure resources for ssh access (such as VMs, public IPs, firewall rules, etc.) are automatically created and also terminated after usage, but at the latest after the above mentioned time span is up.  Non-Goals  Node-specific access Auditability on operating system level (not only auditing the ssh login, but everything that is done on a node and other respective resources, e.g. by using dedicated operating system users) Reuse of temporarily created necessary infrastructure resources by different users  Proposal Involved Components The following is a list of involved components, that either need to be newly introduced or extended if already existing\n Gardener API Server (GAPI)  New operations.gardener.cloud API Group New resource type Bastion, see resource example below New Admission Webhooks for Bastion resource SeedAuthorizer: The SeedAuthorizer and dependency graph needs to be extended to consider the Bastion resource https://github.com/gardener/gardener/tree/master/pkg/admissioncontroller/webhooks/auth/seed/graph Is configured with timeToLive, the time to add to the current time on each heartbeat   gardenlet  Deploys Bastion CRD under the extensions.gardener.cloud API Group to the Seed, see resource example below Similar to BackupBuckets or BackupEntry, the gardenlet watches the Bastion resource in the garden cluster and creates a seed-local Bastion resource, on which the provider specific bastion controller acts upon   gardenctlv2 (or any other client)  Creates Bastion resource in the garden cluster Establishes an ssh connection to a shoot node, using a bastion host as proxy Heartbeats / keeps alive the Bastion resource during ssh connection   Gardener extension provider   Provider specific bastion controller Should be added to gardener-extension-provider- repos, e.g. https://github.com/gardener/gardener-extension-provider-aws/tree/master/pkg/controller Has the permission to update the Bastion/status subresource on the seed cluster Runs on seed (of course)   Gardener Controller Manager (GCM)  Bastion heartbeat controller  Cleans up Bastion resource on missing heartbeat. Is configured with a maxLifetime for the Bastion resource     Gardener (RBAC)  The project admin role should be extended to allow CRUD operations on the Bastion resource. The gardener.cloud:system:project-member-aggregation ClusterRole needs to be updated accordingly (https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/charts/application/templates/rbac-user.yaml)    SSH Flow Users should only get the RBAC permission to create / update Bastion resources for a namespace, if they should be allowed to ssh onto the shoot nodes in this namespace. A project member with admin role will have these permissions. User/gardenctlv2 creates Bastion resource in garden cluster (see resource example below)  First, gardenctl would figure out the own public IP of the user’s machine. Either by calling an external service (gardenctl (v1) uses https://github.com/gardener/gardenctl/blob/master/pkg/cmd/miscellaneous.go#L226) or by calling a binary that prints the public IP(s) to stdout. The binary should be configurable. The result is set under spec.ingress[].ipBlock.cidr Creates new ssh key pair. The newly created key pair is used only once for each bastion host, so it has a 1:1 relationship to it. It is cleaned up after it is not used anymore, e.g. if the Bastion resource was deleted. The public ssh key is set under spec.sshPublicKey The targeted shoot is set under spec.shootRef   GAPI Admission Plugin for the Bastion resource in the garden cluster  on creation, sets metadata.annotations[\"gardener.cloud/created-by\"] according to the user that created the resource when gardener.cloud/operation: keepalive is set it will be removed by GAPI from the annotations and status.lastHeartbeatTimestamp will be set with the current timestamp. The status.expirationTimestamp will be calculated by taking the last heartbeat timestamp and adding x minutes (configurable, default 60 Minutes). validates that only the creator of the bastion (see gardener.cloud/created-by annotation) can update spec.ingress validates that a Bastion can only be created for a Shoot if that Shoot is already assigned to a Seed sets spec.seedName and spec.providerType based on the spec.shootRef   gardenlet  Watches Bastion resource for own seed under api group operations.gardener.cloud in the garden cluster Creates Bastion custom resource under api group extensions.gardener.cloud/v1alpha1 in the seed cluster  Populates bastion user data under field under spec.userData similar to https://github.com/gardener/gardenctl/blob/1e3e5fa1d5603e2161f45046ba7c6b5b4107369e/pkg/cmd/ssh.go#L160-L171. By this means the spec.sshPublicKey from the Bastion resource in the garden cluster will end up in the authorized_keys file on the bastion host.     Gardener extension provider  / Bastion Controller on Seed:  With own Bastion Custom Resource Definition in the seed under the api group extensions.gardener.cloud/v1alpha1 Watches Bastion custom resources that are created by the gardenlet in the seed Controller reads cloudprovider credentials from seed-shoot namespace Deploy infrastructure resources  Bastion VM. Uses user data from spec.userData attaches public IP, creates security group, firewall rules, etc.   Updates status of Bastion resource:  With bastion IP under status.ingress.ip or hostname under status.ingress.hostname Updates the status.lastOperation with the status of the last reconcile operation     gardenlet  Syncs back the status.ingress and status.conditions of the Bastion resource in the seed to the garden cluster in case it changed   gardenctl  initiates ssh session once status.conditions['BastionReady'] is true of the Bastion resource in the garden cluster  locates private ssh key matching spec[\"sshPublicKey\"] which was configured beforehand by the user reads bastion IP (status.ingress.ip) or hostname (status.ingress.hostname) reads the private key from the ssh key pair for the shoot node opens ssh connection to the bastion and from there to the respective shoot node   runs heartbeat in parallel as long as the ssh session is open by annotating the Bastion resource with gardener.cloud/operation: keepalive   GCM:  Once status.expirationTimestamp is reached, the Bastion will be marked for deletion   gardenlet:  Once the Bastion resource in the garden cluster is marked for deletion, it marks the Bastion resource in the seed for deletion   Gardener extension provider  / Bastion Controller on Seed:  all created resources will be cleaned up On succes, removes finalizer on Bastion resource in seed   gardenlet:  removes finalizer on Bastion resource in garden cluster    Resource Example Bastion resource in the garden cluster\napiVersion: operations.gardener.cloud/v1alpha1 kind: Bastion metadata:  generateName: cli-  name: cli-abcdef  namespace: garden-myproject  annotations:  gardener.cloud/created-by: foo # immutable, set by the GAPI Admission Plugin  # gardener.cloud/operation: keepalive # this annotation is removed by the GAPI and the status.lastHeartbeatTimestamp and status.expirationTimestamp will be updated accordingly spec:  shootRef: # namespace cannot be set / it's the same as .metadata.namespace  name: my-cluster # immutable   # the following fields are set by the GAPI  seedName: aws-eu2  providerType: aws   sshPublicKey: c3NoLXJzYSAuLi4K # immutable, public `ssh` key of the user   ingress: # can only be updated by the creator of the bastion  - ipBlock:  cidr: 1.2.3.4/32 # public IP of the user. CIDR is a string representing the IP Block. Valid examples are \"192.168.1.1/24\" or \"2001:db9::/64\"  status:  observedGeneration: 1   # the following fields are managed by the controller in the seed and synced by gardenlet  ingress: # IP or hostname of the bastion  ip: 1.2.3.5  # hostname: foo.bar   conditions:  - type: BastionReady # when the `status` is true of condition type `BastionReady`, the client can initiate the `ssh` connection  status: 'True'  lastTransitionTime: \"2021-03-19T11:59:00Z\"  lastUpdateTime: \"2021-03-19T11:59:00Z\"  reason: BastionReady  message: Bastion for the cluster is ready.   # the following fields are only set by the GAPI  lastHeartbeatTimestamp: \"2021-03-19T11:58:00Z\" # will be set when setting the annotation gardener.cloud/operation: keepalive  expirationTimestamp: \"2021-03-19T12:58:00Z\" # extended on each keepalive Bastion custom resource in the seed cluster\napiVersion: extensions.gardener.cloud/v1alpha1 kind: Bastion metadata:  name: cli-abcdef  namespace: shoot--myproject--mycluster spec:  userData: |- # this is normally base64-encoded, but decoded for the example. Contains spec.sshPublicKey from Bastion resource in garden cluster  #!/bin/bash  # create user  # add ssh public key to authorized_keys  # ...   ingress:  - ipBlock:  cidr: 1.2.3.4/32   type: aws # from extensionsv1alpha1.DefaultSpec  status:  observedGeneration: 1  ingress:  ip: 1.2.3.5  # hostname: foo.bar  conditions:  - type: BastionReady  status: 'True'  lastTransitionTime: \"2021-03-19T11:59:00Z\"  lastUpdateTime: \"2021-03-19T11:59:00Z\"  reason: BastionReady  message: Bastion for the cluster is ready. SSH Key Pair Rotation Currently, the ssh key pair for the shoot nodes are created once during shoot cluster creation. These key pairs should be rotated on a regular basis.\nRotation Proposal  gardeneruser original user data component:  The gardeneruser create script should be changed into a reconcile script, and renamed accordingly. It needs to be adapted so that the authorized_keys file will be updated / overwritten with the current and old ssh public key from the cloud-config user data.   Rotation trigger:  Once in the maintenance time window On demand, by annotating the shoot with gardener.cloud/operation: rotate-ssh-keypair   On rotation trigger:  gardenlet  Prerequisite of ssh key pair rotation: all nodes of all the worker pools have successfully applied the desired version of their cloud-config user data Creates or updates the secret ssh-keypair.old with the content of ssh-keypair in the seed-shoot namespace. The old private key can be used by clients as fallback, in case the new ssh public key is not yet applied on the node Generates new ssh-keypair secret The OperatingSystemConfig needs to be re-generated and deployed with the new and old ssh public key   As usual (for more details, see here):  Once the cloud-config-\u003cX\u003e secret in the kube-system namespace of the shoot cluster is updated, it will be picked up by the downloader script (checks every 30s for updates) The downloader runs the “execution” script from the cloud-config-\u003cX\u003e secret The “execution” script includes also the original user data script, which it writes to PATH_CLOUDCONFIG, compares it against the previous cloud config and runs the script in case it has changed Running the original user data script will also run the gardeneruser component, where the authorized_keys file will be updated After the most recent cloud-config user data was applied, the “execution” script annotates the node with checksum/cloud-config-data: \u003ccloud-config-checksum\u003e to indicate the success      Limitations Each operating system has its own default user (e.g. core, admin, ec2-user etc). These users get their SSH keys during VM creation (however there is a different handling on Google Cloud Platform as stated below). These keys currently do not get rotated respectively are not removed from the authorized_keys file. This means that the initial ssh key will still be valid for the default operating system user.\nOn Google Cloud Platform, the VMs do not have any static users (i.e. no gardener user) and there is an agent on the nodes that syncs the users with their SSH keypairs from the GCP IAM service.\n","categories":"","description":"","excerpt":"GEP-15: Bastion Management and SSH Key Pair Rotation Table of Contents …","ref":"/docs/gardener/proposals/15-manage-bastions-and-ssh-key-pair-rotation/","tags":"","title":"Bastion Management and SSH Key Pair Rotation"},{"body":"CA Rotation in Extensions GEP-18 proposes adding support for automated rotation of Shoot cluster certificate authorities (CAs). This document outlines all requirements that Gardener extensions need to fulfill in order to support the CA rotation feature.\nRequirements for Shoot Cluster CA Rotation  Extensions must not rely on static CA Secret names managed by gardenlet, because their names are changing during CA rotation. Extensions cannot issue or use client certificates for authenticating against shoot API servers. Instead, they should use short-lived auto-rotated ServiceAccount tokens via gardener-resource-manager’s TokenRequestor. Also see Conventions and TokenRequestor documents. Extensions need to generate dedicated CAs for signing server certificates (e.g. cloud-controller-manager). There should be one CA per controller and purpose in order to bind the lifecycle to the reconciliation cycle of the respective object for which it is created. CAs managed by extensions should be rotated in lock-step with the shoot cluster CA. When the user triggers a rotation, gardenlet writes phase and initiation time to Shoot.status.credentials.rotation.certificateAuthorities.{phase,lastInitiationTime}. See GEP-18 for a detailed description on what needs to happen in each phase. Extensions can retrieve this information from Cluster.shoot.status.  Utilities for Secrets Management In order to fulfill the requirements listed above, extension controllers can reuse the SecretsManager that gardenlet uses to manage all shoot cluster CAs, certificates, and other secrets as well. It implements the core logic for managing secrets that need to be rotated, auto-renewed etc.\nAdditionally, there are utilities for reusing SecretsManager in extension controllers. They already implement above requirements based on the Cluster resource and allow focusing on the extension controllers’ business logic.\nFor example, a simple SecretsManager usage in an extension controller could look like this:\nconst (  // identity for SecretsManager instance in ControlPlane controller  identity = \"provider-foo-controlplane\"  // secret config name of the dedicated CA  caControlPlaneName = \"ca-provider-foo-controlplane\" )  func Reconcile() {  var (  cluster *extensionscontroller.Cluster  client client.Client   // define wanted secrets with options  secretConfigs = []extensionssecretsmanager.SecretConfigWithOptions{  {  // dedicated CA for ControlPlane controller  Config: \u0026secretutils.CertificateSecretConfig{  Name: caControlPlaneName,  CommonName: \"ca-provider-foo-controlplane\",  CertType: secretutils.CACert,  },  // persist CA so that it gets restored on control plane migration  Options: []secretsmanager.GenerateOption{secretsmanager.Persist()},  },  {  // server cert for control plane component  Config: \u0026secretutils.CertificateSecretConfig{  Name: \"cloud-controller-manager\",  CommonName: \"cloud-controller-manager\",  DNSNames: kutil.DNSNamesForService(\"cloud-controller-manager\", namespace),  CertType: secretutils.ServerCert,  },  // sign with our dedicated CA  Options: []secretsmanager.GenerateOption{secretsmanager.SignedByCA(caControlPlaneName)},  },  }  )   // initialize SecretsManager based on Cluster object  sm, err := extensionssecretsmanager.SecretsManagerForCluster(ctx, logger.WithName(\"secretsmanager\"), clock.RealClock{}, client, cluster, identity, secretConfigs)   // generate all wanted secrets (first CAs, then the rest)  secrets, err := extensionssecretsmanager.GenerateAllSecrets(ctx, sm, secretConfigs)   // cleanup any secrets that are not needed any more (e.g. after rotation)  err = sm.Cleanup(ctx) } Please pay attention to the following points:\n There should be one SecretsManager identity per controller (and purpose if applicable) in order to prevent conflicts between different instances. E.g., there should be different identities for Infrastructrue, Worker controller etc. and the ControlPlane controller should use dedicated SecretsManager identities per purpose (e.g. provider-foo-controlplane and provider-foo-controlplane-exposure). All other points in Reusing the SecretsManager in Other Components  ","categories":"","description":"","excerpt":"CA Rotation in Extensions GEP-18 proposes adding support for automated …","ref":"/docs/gardener/extensions/ca-rotation/","tags":"","title":"CA Rotation"},{"body":"Gardener Extension for Calico Networking  \nThis controller operates on the Network resource in the extensions.gardener.cloud/v1alpha1 API group. It manages those objects that are requesting Calico Networking configuration (.spec.type=calico):\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: Network metadata:  name: calico-network  namespace: shoot--core--test-01 spec:  type: calico  clusterCIDR: 192.168.0.0/24  serviceCIDR: 10.96.0.0/24  providerConfig:  apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1  kind: NetworkConfig  ipam:  type: host-local  cidr: usePodCIDR  ebpfDataplane:  enabled: false  ipv4:  mode: Always  backend: bird  typha:  enabled: true Please find a concrete example in the example folder. All the Calico specific configuration should be configured in the providerConfig section. If additional configuration is required, it should be added to the networking-calico chart in controllers/networking-calico/charts/internal/calico/values.yaml and corresponding code parts should be adapted (for example in controllers/networking-calico/pkg/charts/utils.go).\nOnce the network resource is applied, the networking-calico controller would then create all the necessary managed-resources which should be picked up by the gardener-resource-manager which will then apply all the network extensions resources to the shoot cluster.\nFinally after successful reconciliation an output similar to the one below should be expected.\n status:  lastOperation:  description: Successfully reconciled network  lastUpdateTime: \"...\"  progress: 100  state: Succeeded  type: Reconcile  observedGeneration: 1  providerStatus:  apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1  kind: NetworkStatus  How to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start. Please make sure to have the kubeconfig pointed to the cluster you want to connect to. Static code checks and tests can be executed by running make verify. We are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io Gardener Extensions Golang library GEP-1 (Gardener Enhancement Proposal) on extensibility Extensibility API documentation  ","categories":"","description":"Gardener extension controller for the Calico CNI network plugin","excerpt":"Gardener extension controller for the Calico CNI network plugin","ref":"/docs/extensions/network-extensions/gardener-extension-networking-calico/","tags":"","title":"Calico CNI"},{"body":"While it is possible, we highly recommend not to use privileged containers in your productive environment.\n","categories":"","description":"","excerpt":"While it is possible, we highly recommend not to use privileged …","ref":"/docs/faq/privileged-containers/","tags":"","title":"Can I run privileged containers?"},{"body":"There is no automatic migration of major/minor versions of Kubernetes. You need to update your clusters manually or press the Upgrade button in the Dashboard.\nBefore updating a cluster you should be aware of the potential errors this might cause. The following video will dive into a Kubernetes outage in production that Monzo experienced, its causes and effects, and the architectural and operational lessons learned.\n It is therefore recommended to first update your test cluster and validate it before performing changes on a productive environment.\n","categories":"","description":"","excerpt":"There is no automatic migration of major/minor versions of Kubernetes. …","ref":"/docs/faq/automatic-upgrade/","tags":"","title":"Can Kubernetes upgrade automatically?"},{"body":"Backing up your Kubernetes cluster is possible through the use of specialized software like Velero. Velero consists of a server side component and a client tool that allow you to backup or restore all objects in your cluster, as well as the cluster resources and persistent volumes.\n","categories":"","description":"","excerpt":"Backing up your Kubernetes cluster is possible through the use of …","ref":"/docs/faq/backup/","tags":"","title":"Can you backup your Kubernetes cluster resources?"},{"body":"The migration of clusters or content from one cluster to another is out of scope for the Gardener project. For such scenarios you may consider using tools like Velero.\n","categories":"","description":"","excerpt":"The migration of clusters or content from one cluster to another is …","ref":"/docs/faq/automatic-migrate/","tags":"","title":"Can you migrate the content of one cluster to another cluster?"},{"body":"Extending the API This document describes the steps that need to be performed when changing the API. It provides guidance for API changes to both (Gardener system in general or component configurations).\nGenerally, as Gardener is a Kubernetes-native extension, it follows the same API conventions and guidelines like Kubernetes itself. This document as well as this document already provide a good overview and general explanation of the basic concepts behind it. We are following the same approaches.\nGardener API The Gardener API is defined in pkg/apis/{core,extensions,settings} directories and is the main point of interaction with the system. It must be ensured that the API is always backwards-compatible. If fields shall be removed permanently from the API then a proper deprecation period must be adhered to so that end-users have enough time adapt their clients.\nChecklist when changing the API:\n Modify the field(s) in the respective Golang files of all external and the internal version.  Make sure new fields are being added as “optional” fields, i.e., they are of pointer types, they have the // +optional comment, and they have the omitempty JSON tag. Make sure that the existing field numbers in the protobuf tags are not changed.   If necessary then implement/adapt the conversion logic defined in the versioned APIs (e.g., pkg/apis/core/v1beta1/conversions*.go). If necessary then implement/adapt defaulting logic defined in the versioned APIs (e.g., pkg/apis/core/v1beta1/defaults*.go). Run the code generation: make generate If necessary then implement/adapt validation logic defined in the internal API (e.g., pkg/apis/core/validation/validation*.go). If necessary then adapt the exemplary YAML manifests of the Gardener resources defined in example/*.yaml. In most cases it makes sense to add/adapt the documentation for administrators/operators and/or end-users in the docs folder to provide information on purpose and usage of the added/changed fields. When opening the pull request then always add a release note so that end-users are becoming aware of the changes.  Component configuration APIs Most Gardener components have a component configuration that follows similar principles to the Gardener API. Those component configurations are defined in pkg/{controllermanager,gardenlet,scheduler},pkg/apis/config. Hence, the above checklist also applies for changes to those APIs. However, since these APIs are only used internally and only during the deployment of Gardener the guidelines with respect to changes and backwards-compatibility are slightly relaxed. If necessary then it is allowed to remove fields without a proper deprecation period if the release note uses the breaking operator keywords.\nIn addition to the above checklist:\n If necessary then adapt the Helm chart of Gardener defined in charts/gardener. Adapt the values.yaml file as well as the manifest templates.  ","categories":"","description":"","excerpt":"Extending the API This document describes the steps that need to be …","ref":"/docs/gardener/development/changing-the-api/","tags":"","title":"Changing the APIs"},{"body":"CI/CD As an execution environment for CI/CD workloads, we use Concourse. We however abstract from the underlying “build executor” and instead offer a Pipeline Definition Contract, through which components declare their build pipelines as required.\nIn order to run continuous delivery workloads for all components contributing to the Gardener project, we operate a central service.\nTypical workloads encompass the execution of tests and builds of a variety of technologies, as well as building and publishing container images, typically containing build results.\nWe are building our CI/CD offering around some principles:\n container-native - each workload is executed within a container environment. Components may customise used container images automation - pipelines are generated without manual interaction self-service - components customise their pipelines by changing their sources standardisation  Learn more on our: Build Pipeline Reference Manual\n","categories":"","description":"","excerpt":"CI/CD As an execution environment for CI/CD workloads, we use …","ref":"/docs/contribute/10_code/14_cicd/","tags":"","title":"CI/CD"},{"body":"Gardener Extension for cilium Networking  \nThis controller operates on the Network resource in the extensions.gardener.cloud/v1alpha1 API group. It manages those objects that are requesting cilium Networking configuration (.spec.type=cilium):\napiVersion: extensions.gardener.cloud/v1alpha1 kind: Network metadata:  name: cilium-network  namespace: shoot--foo--bar spec:  type: cilium  podCIDR: 10.244.0.0/16  serviceCIDR: 10.96.0.0/24  providerConfig:  apiVersion: cilium.networking.extensions.gardener.cloud/v1alpha1  kind: NetworkConfig # hubble: # enabled: true # store: kubernetes Please find a concrete example in the example folder. All the cilium specific configuration should be configured in the providerConfig section. If additional configuration is required, it should be added to the networking-cilium chart in controllers/networking-cilium/charts/internal/cilium/values.yaml and corresponding code parts should be adapted (for example in controllers/networking-cilium/pkg/charts/utils.go).\nOnce the network resource is applied, the networking-cilium controller would then create all the necessary managed-resources which should be picked up by the gardener-resource-manager which will then apply all the network extensions resources to the shoot cluster.\nFinally after successful reconciliation an output similar to the one below should be expected.\n status:  lastOperation:  description: Successfully reconciled network  lastUpdateTime: \"...\"  progress: 100  state: Succeeded  type: Reconcile  observedGeneration: 1  How to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start. Please make sure to have the kubeconfig pointed to the cluster you want to connect to. Static code checks and tests can be executed by running make verify. We are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io Gardener Extensions Golang library GEP-1 (Gardener Enhancement Proposal) on extensibility Extensibility API documentation Docs for cilium user  ","categories":"","description":"Gardener extension controller for the Cilium CNI network plugin","excerpt":"Gardener extension controller for the Cilium CNI network plugin","ref":"/docs/extensions/network-extensions/gardener-extension-networking-cilium/","tags":"","title":"Cilium CNI"},{"body":"Cluster resource As part of the extensibility epic a lot of responsibility that was previously taken over by Gardener directly has now been shifted to extension controllers running in the seed clusters. These extensions often serve a well-defined purpose, e.g. the management of DNS records, infrastructure, etc. We have introduced a couple of extension CRDs in the seeds whose specification is written by Gardener, and which are acted up by the extensions.\nHowever, the extensions sometimes require more information that is not directly part of the specification. One example of that is the GCP infrastructure controller which needs to know the shoot’s pod and service network. Another example is the Azure infrastructure controller which requires some information out of the CloudProfile resource. The problem is that Gardener does not know which extension requires which information so that it can write it into their specific CRDs.\nIn order to deal with this problem we have introduced the Cluster extension resource. This CRD is written into the seeds, however, it does not contain a status, so it is not expected that something acts upon it. Instead, you can treat it like a ConfigMap which contains data that might be interesting for you. In the context of Gardener, seeds and shoots, and extensibility the Cluster resource contains the CloudProfile, Seed, and Shoot manifest. Extension controllers can take whatever information they want out of it that might help completing their individual tasks.\n---  apiVersion: extensions.gardener.cloud/v1alpha1 kind: Cluster metadata:  name: shoot--foo--bar spec:  cloudProfile:  apiVersion: core.gardener.cloud/v1beta1  kind: CloudProfile  ...  seed:  apiVersion: core.gardener.cloud/v1beta1  kind: Seed  ...  shoot:  apiVersion: core.gardener.cloud/v1beta1  kind: Shoot  ... The resource is written by Gardener before it starts the reconciliation flow of the shoot.\n⚠️ All Gardener components use the core.gardener.cloud/v1beta1 version, i.e., the Cluster resource will contain the objects in this version.\nImportant information that should be taken into account There are some fields in the Shoot specification that might be interesting to take into account.\n .spec.hibernation.enabled={true,false}: Extension controllers might want to behave differently if the shoot is hibernated or not (probably they might want to scale down their control plane components, for example). .status.lastOperation.state=Failed: If Gardener sets the shoot’s last operation state to Failed it means that Gardener won’t automatically retry to finish the reconciliation/deletion flow because an error occurred that could not be resolved within the last 24h (default). In this case end-users are expected to manually re-trigger the reconciliation flow in case they want Gardener to try again. Extension controllers are expected to follow the same principle. This means they have to read the shoot state out of the Cluster resource.  Extension resources not associated with a shoot In some cases, Gardener may create extension resources that are not associated with a shoot, but are needed to support some functionality internal to Gardener. Such resources will be created in the garden namespace of a seed cluster.\nFor example, if the managed ingress controller is active on the seed, Gardener will create a DNSRecord resource(s) in the garden namespace of the seed cluster for the ingress DNS record.\nExtension controllers that may be expected to reconcile extension resources in the garden namespace should make sure that they can tolerate the absence of a cluster resource. This means that they should not attempt to read the cluster resource in such cases, or if they do they should ignore the “not found” error.\nReferences and additional resources  Cluster API (Golang specification)  ","categories":"","description":"","excerpt":"Cluster resource As part of the extensibility epic a lot of …","ref":"/docs/gardener/extensions/cluster/","tags":"","title":"Cluster"},{"body":"Relation between Gardener API and Cluster API (SIG Cluster Lifecycle) In essence, the Cluster API harmonizes how to get to clusters, while Gardener goes one step further and also harmonizes the clusters themselves. The Cluster API delegates the specifics to so-called providers for infrastructures or control planes via specific CR(D)s while Gardener only has one cluster CR(D). Different Cluster API providers, e.g. for AWS, Azure, GCP, etc. give you vastly different Kubernetes clusters. In contrast, Gardener gives you the exact same clusters with the exact same K8s version, operating system, control plane configuration like for API server or kubelet, add-ons like overlay network, HPA/VPA, DNS and certificate controllers, ingress and network policy controllers, control plane monitoring and logging stacks, down to the behavior of update procedures, auto-scaling, self-healing, etc. on all supported infrastructures. These homogeneous clusters are an essential goal for Gardener as its main purpose is to simplify operations for teams that need to develop and ship software on Kubernetes clusters on a plethora of infrastructures (a.k.a. multi-cloud).\nIncidentally, Gardener influenced the Machine API in the Cluster API with its Machine Controller Manager and was the first to adopt it, see also joint SIG Cluster Lifecycle KubeCon talk where @hardikdr from our Gardener team in India spoke.\nThat means, we follow the Cluster API with great interest and are active members. It was completely overhauled from v1alpha1 to v1alpha2. But because v1alpha2 made too many assumptions about the bring-up of masters and was enforcing master machine operations (see here: “As of v1alpha2, Machine-Based is the only control plane type that Cluster API supports”), services that managed their control planes differently like GKE or Gardener couldn’t adopt it (e.g. Google only supports v1alpha1). In 2020 v1alpha3 was introduced and made it possible (again) to integrate managed services like GKE or Gardener. The mapping from the Gardener API to the Cluster API is mostly syntactic.\nTo wrap it up, while the Cluster API knows about clusters, it doesn’t know about their make-up. With Gardener, we wanted to go beyond that and harmonize the make-up of the clusters themselves and make them homogeneous across all supported infrastructures. Gardener can therefore deliver homogeneous clusters with exactly the same configuration and behavior on all infrastructures (see also Gardener’s coverage in the official conformance test grid).\nWith Cluster API v1alpha3 and the support for declarative control plane management, it became now possible (again) to enable Kubernetes managed services like GKE or Gardener. We would be more than happy, if the community would be interested, to contribute a Gardener control plane provider.\n","categories":"","description":"","excerpt":"Relation between Gardener API and Cluster API (SIG Cluster Lifecycle) …","ref":"/docs/gardener/concepts/cluster-api/","tags":"","title":"Cluster API"},{"body":"You are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions:\n Contributions must be licensed under the Apache 2.0 License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.  ","categories":"","description":"","excerpt":"You are welcome to contribute code to Gardener in order to fix a bug …","ref":"/contribute/code/","tags":"","title":"Code"},{"body":"Community Calls Join our community calls to connect with other Gardener enthusiasts and watch cool presentations.\nWhat content can you expect?\n Gardener core developers roll out new information, share knowledge with the members and demonstrate new service capabilities. Adopters and contributors share their use-cases, experience and exchange on future requirements.  If you want to receive updates, sign up here:\n Gardener Google Group  The recordings are published on the Gardener Project YouTube channel.     Topic Speaker Date and Time Link     Get more computing power in Gardener by overcoming Kubelet limitations with CRI-resource-manager\nWhat problems CRI-resource-manager solves?Extension’s quality of life “must have” features that makes it “ready to production”Demo of how to dynamically configure Gardener extension (and debug propagation when it fails) Pawel Palucki October 20, 2022 10:00-11:00 AM CEST Join Zoom Meeting Meeting ID: 982 7843 1856 Passcode: 495335   Cilium / Isovalent Presentation Raymond de Jong October 6, 2022 Recording Summary   Gardener Extension Development - From scratch to the gardener-extension-shoot-flux Jens Schneider, Lothar Gesslein June 9, 2022 Recording Summary   Deploying and Developing Gardener Locally (Without Any External Infrastructure!) Tim Ebert, Rafael Franzke March 17, 2022 Recording Summary   Gardenctl-v2 Holger Kosser, Lukas Gross, Peter Sutter February 17, 2022 Recording Summary    Google Calendar\n ","categories":"","description":"","excerpt":"Community Calls Join our community calls to connect with other …","ref":"/community/","tags":"","title":"Community"},{"body":"Concept Title Overview This section provides an overview of the topic and the information provided in it.\nRelevant heading 1 This section gives the user all the information needed in order to understand the topic.\nRelevant subheading This adds information that belongs to the topic discussed in the parent heading\nRelevant heading 2 This section gives the user all the information needed in order to understand the topic.\nRelated Links  Link 1 Link 2  ","categories":"","description":"Describes the contents of a concept topic","excerpt":"Describes the contents of a concept topic","ref":"/docs/contribute/20_documentation/40_style_guide/concept_template/","tags":"","title":"Concept Topic Structure"},{"body":"Gardener Configuration and Usage Gardener automates the full lifecycle of Kubernetes clusters as a service. Additionally, it has several extension points allowing external controllers to plug-in to the lifecycle. As a consequence, there are several configuration options for the various custom resources that are partially required.\nThis document describes the\n configuration and usage of Gardener as operator/administrator. configuration and usage of Gardener as end-user/stakeholder/customer.  Configuration and Usage of Gardener as Operator/Administrator When we use the terms “operator/administrator” we refer to both the people deploying and operating Gardener. Gardener consists of the following components:\n gardener-apiserver, a Kubernetes-native API extension that serves custom resources in the Kubernetes-style (like Seeds and Shoots), and a component that contains multiple admission plugins. gardener-admission-controller, an HTTP(S) server with several handlers to be used in a ValidatingWebhookConfiguration. gardener-controller-manager, a component consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for maintaining Shoots, reconciling Projects, etc.). gardener-scheduler, a component that assigns newly created Shoot clusters to appropriate Seed clusters. gardenlet, a component running in seed clusters and consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for reconciliation and deletion of Shoots).  Each of these components have various configuration options. The gardener-apiserver uses the standard API server library maintained by the Kubernetes community, and as such it mainly supports command line flags. Other components use so-called componentconfig files that describe their configuration in a Kubernetes-style versioned object.\nConfiguration file for Gardener admission controller The Gardener admission controller does only support one command line flag which should be a path to a valid admission-controller configuration file. Please take a look at this example configuration.\nConfiguration file for Gardener controller manager The Gardener controller manager does only support one command line flag which should be a path to a valid controller-manager configuration file. Please take a look at this example configuration.\nConfiguration file for Gardener scheduler The Gardener scheduler also only supports one command line flag which should be a path to a valid scheduler configuration file. Please take a look at this example configuration. Information about the concepts of the Gardener scheduler can be found here\nConfiguration file for Gardenlet The Gardenlet also only supports one command line flag which should be a path to a valid gardenlet configuration file. Please take a look at this example configuration. Information about the concepts of the Gardenlet can be found here\nSystem configuration After successful deployment of the four components you need to setup the system. Let’s first focus on some “static” configuration. When the gardenlet starts it scans the garden namespace of the garden cluster for Secrets that have influence on its reconciliation loops, mainly the Shoot reconciliation:\n  Internal domain secret, contains the DNS provider credentials (having appropriate privileges) which will be used to create/delete so-called “internal” DNS records for the Shoot clusters, please see this for an example.\n This secret is used in order to establish a stable endpoint for shoot clusters which is used internally by all control plane components. The DNS records are normal DNS records but called “internal” in our scenario because only the kubeconfigs for the control plane components use this endpoint when talking to the shoot clusters. It is forbidden to change the internal domain secret if there are existing shoot clusters.    Default domain secrets (optional), contain the DNS provider credentials (having appropriate privileges) which will be used to create/delete DNS records for a default domain for shoots (e.g., example.com), please see this for an example.\n Not every end-user/stakeholder/customer has its own domain, however, Gardener needs to create a DNS record for every shoot cluster. As landscape operator you might want to define a default domain owned and controlled by you that is used for all shoot clusters that don’t specify their own domain. If you have multiple default domain secrets defined you can add a priority as an annotation (dns.gardener.cloud/domain-default-priority) to select which domain should be used for new shoots while creation. The domain with the highest priority is selected while shoot creation. If there is no annotation defined the default priority is 0, also all non integer values are considered as priority 0.    ⚠️ Please note that the mentioned domain secrets are only needed if you have at least one seed cluster that is not specifing .spec.settings.shootDNS.enabled=false. Seeds with this taint don’t create any DNS records for shoots scheduled on it, hence, if you only have such seeds, you don’t need to create the domain secrets.\n  Alerting secrets (optional), contain the alerting configuration and credentials for the AlertManager to send email alerts. It is also possible to configure the monitoring stack to send alerts to an AlertManager not deployed by Gardener to handle alerting. Please see this for an example.\n If email alerting is configured:  An AlertManager is deployed into each seed cluster that handles the alerting for all shoots on the seed cluster. Gardener will inject the SMTP credentials into the configuration of the AlertManager. The AlertManager will send emails to the configured email address in case any alerts are firing.   If an external AlertManager is configured:  Each shoot has a Prometheus responsible for monitoring components and sending out alerts. The alerts will be sent to a URL configured in the alerting secret. This external AlertManager is not managed by Gardener and can be configured however the operator sees fit. Supported authentication types are no authentication, basic, or mutual TLS.      OpenVPN Diffie-Hellmann Key secret (optional), contains the self-generated Diffie-Hellmann key used by OpenVPN in your landscape, please see this for an example.\n If you don’t specify a custom key then a default key is used, but for productive landscapes it’s recommend to create a landscape-specific key and define it.    Global monitoring secrets (optional), contains basic authentication credentials for the Prometheus aggregating metrics for all clusters.\n These secrets are synced to each seed cluster and used to gain access to the aggregate monitoring components.    Apart from this “static” configuration there are several custom resources extending the Kubernetes API and used by Gardener. As an operator/administrator you have to configure some of them to make the system work.\nConfiguration and Usage of Gardener as End-User/Stakeholder/Customer As an end-user/stakeholder/customer you are using a Gardener landscape that has been setup for you by another team. You don’t need to care about how Gardener itself has to be configured or how it has to be deployed. Take a look at this document - it describes which resources are offered by Gardener. You may want to have a more detailed look for Projects, SecretBindings, Shoots, and (Cluster)OpenIDConnectPresets.\n","categories":"","description":"","excerpt":"Gardener Configuration and Usage Gardener automates the full lifecycle …","ref":"/docs/gardener/usage/configuration/","tags":"","title":"Configuration"},{"body":"Configuring the Logging stack via Gardenlet configurations Enable the Logging In order to install the Gardener logging stack the logging.enabled configuration option has to be enabled in the Gardenlet configuration:\nlogging:  enabled: true From now on each Seed is going to have a logging stack which will collect logs from all pods and some systemd services. Logs related to Shoots with testing purpose are dropped in the fluent-bit output plugin. Shoots with a purpose different than testing have the same type of log aggregator (but different instance) as the Seed. The logs can be viewed in the Grafana in the garden namespace for the Seed components and in the respective shoot control plane namespaces.\nEnable logs from the Shoot’s node systemd services. The logs from the systemd services on each node can be retrieved by enabling the logging.shootNodeLogging option in the Gardenlet configuration:\nlogging:  enabled: true  shootNodeLogging:  shootPurposes:  - \"evaluation\"  - \"deployment\" Under the shootPurpose section just list all the shoot purposes for which the Shoot node logging feature will be enabled. Specifying the testing purpose has no effect because this purpose prevents the logging stack installation. Logs can be viewed in the operator Grafana! The dedicated labels are unit, syslog_identifier and nodename in the Explore menu.\nConfiguring the log processor Under logging.fluentBit there is three optional sections.\n input: This overwrite the input configuration of the fluent-bit log processor. output: This overwrite the output configuration of the fluent-bit log processor. service: This overwrite the service configuration of the fluent-bit log processor.  logging:  enabled: true  fluentBit:  output: |-[Output] ...  input: |-[Input] ...  service: |-[Service] ... additional egress IPBlock for allow-fluentbit NetworkPolicy The optional setting under logging.fluentBit.networkPolicy.additionalEgressIPBlocks add additional egress IPBlock to allow-fluentbit NetworkPolicy to forward logs to a central system.\nlogging:  enabled: true  fluentBit:  additionalEgressIpBlock:  - 123.123.123.123/32 Configure central logging For central logging, the output configuration of the fluent-bit log processor can be overwritten (logging.fluentBit.output) and the Loki instances deployments in Garden and Shoot namespace can be enabled/disabled (logging.loki.enabled), by default Loki is enabled.\nlogging:  enabled: true  fluentBit:  output: |-[Output] ...  loki:  enabled: false Configuring central Loki storage capacity By default, the central Loki has 100Gi of storage capacity. To overwrite the current central Loki storage capacity, the logging.loki.garden.storage setting in the gardenlet’s component configuration should be altered. If you need to increase it you can do so without losing the current data by specifying higher capacity. Doing so, the Loki’s PersistentVolume capacity will be increased instead of deleting the current PV. However, if you specify less capacity then the PersistentVolume will be deleted and with it the logs, too.\nlogging:  enabled: true  fluentBit:  output: |-[Output] ...  loki:  garden:  storage: \"200Gi\" ","categories":"","description":"","excerpt":"Configuring the Logging stack via Gardenlet configurations Enable the …","ref":"/docs/gardener/deployment/configuring_logging/","tags":"","title":"Configuring Logging"},{"body":"Connect kubectl In Kubernetes, the configuration for access to your cluster is a format known as kubeconfig that is normally stored as a file. It contains details such as cluster API server addresses and user access credentials. Treat it as sensitive data. Tools like kubectl use kubeconfig to connect and authenticate to a cluster and perform operations on it. Learn more about kubeconfig and kubectl on kubernetes.io.\nPrerequisites  You are logged on to the Gardener Dashboard. You have created a cluster and its status is operational.  On this page:\n Downloading kubeconfig for a cluster Connecting to the cluster Exporting KUBECONFIG environment variable   Downloading kubeconfig for a cluster   Select your project from the dropdown on the left, then choose CLUSTERS and locate your cluster in the list. Choose the key icon to bring up a dialog with the access options.\nIn the Kubeconfig section the options are to download, copy or view the kubeconfig for the cluster. The same options are available also in the Access section in the cluster details screen. To find it, choose a cluster from the list.\n  Choose the download icon to download kubeconfig as file on your local system.\n  Connecting to the cluster In the following command, change \u003cpath-to-kubeconfig\u003e with the actual path to the file where you stored the kubeconfig downloaded in the previous steps.\n$ kubectl --kubeconfig=\u003cpath-to-kubeconfig\u003e get namespaces The command connects to the cluster and list its namespaces.\nExporting KUBECONFIG environment variable Since many kubectl commands will be used, it’s a good idea to take advantage of every opportunity to shorten the expressions. The kubectl tool has a fallback strategy for looking up a kubeconfig to work with. For example, it looks for the KUBECONFIG environment variable with value that is the path to the kubeconfig file meant to be used. Export the variable:\n$ export KUBECONFIG=\u003cpath-to-file\u003e In the previous snippet make sure to change the \u003cpath-to-file\u003e with the path to the kubeconfig for the cluster that you want to connect to on your system.\nWhat’s next?  Using Dashboard Terminal  ","categories":"","description":"","excerpt":"Connect kubectl In Kubernetes, the configuration for access to your …","ref":"/docs/dashboard/usage/connect-kubectl/","tags":"","title":"Connect Kubectl"},{"body":"Connectivity Shoot Connectivity We measure the connectivity from the shoot to the API Server. This is done via the blackbox exporter which is deployed in the shoot’s kube-system namespace. Prometheus will scrape the blackbox exporter and then the exporter will try to access the API Server. Metrics are exposed if the connection was successful or not. This can be seen in the dashboard Kubernetes Control Plane Status dashboard under the API Server Connectivity panel. The shoot line represents the connectivity from the shoot.\nSeed Connectivity In addition to the shoot connectivity, we also measure the seed connectivity. This means trying to reach the API Server from the seed via the external fully qualified domain name of the API server. The connectivity is also displayed in the above panel as the seed line. Both seed and shoot connectivity are shown below.\n","categories":"","description":"","excerpt":"Connectivity Shoot Connectivity We measure the connectivity from the …","ref":"/docs/gardener/monitoring/connectivity/","tags":"","title":"Connectivity"},{"body":"Problem Two of the most common problems are specifying the wrong container image or trying to use private images without providing registry credentials.\nNote: There is no observable difference in Pod status between a missing image and incorrect registry permissions. In either case, Kubernetes will report an ErrImagePull status for the Pods. For this reason, this article deals with both scenarios.\nExample Let’s see an example. We’ll create a pod named fail referencing a non-existent Docker image:\nkubectl run -i --tty fail --image=tutum/curl:1.123456 the command prompt doesn’t return and you can press ctrl+c\nError analysis We can then inspect our Pods and see that we have one Pod with a status of ErrImagePull or ImagePullBackOff.\n$ (minikube) kubectl get pods NAME READY STATUS RESTARTS AGE client-5b65b6c866-cs4ch 1/1 Running 1 1m fail-6667d7685d-7v6w8 0/1 ErrImagePull 0 \u003cinvalid\u003e vuejs-578574b75f-5x98z 1/1 Running 0 1d $ (minikube) For some additional information, we can describe the failing Pod.\nkubectl describe pod fail-6667d7685d-7v6w8 As you can see in the events section, your image can’t be pulled\nName:\tfail-6667d7685d-7v6w8 Namespace:\tdefault Node:\tminikube/192.168.64.10 Start Time:\tWed, 22 Nov 2017 10:01:59 +0100 Labels:\tpod-template-hash=2223832418 run=fail Annotations:\tkubernetes.io/created-by={\"kind\":\"SerializedReference\",\"apiVersion\":\"v1\",\"reference\":{\"kind\":\"ReplicaSet\",\"namespace\":\"default\",\"name\":\"fail-6667d7685d\",\"uid\":\"cc4ccb3f-cf63-11e7-afca-4a7a1fa05b3f\",\"a... . . . . Events: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 1m\t1m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned fail-6667d7685d-7v6w8 to minikube 1m\t1m\t1\tkubelet, minikube\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \"default-token-9fr6r\" 1m\t6s\t4\tkubelet, minikube\tspec.containers{fail}\tNormal\tPulling\tpulling image \"tutum/curl:1.123456\" 1m\t5s\t4\tkubelet, minikube\tspec.containers{fail}\tWarning\tFailed\tFailed to pull image \"tutum/curl:1.123456\": rpc error: code = Unknown desc = Error response from daemon: manifest for tutum/curl:1.123456 not found 1m\t\u003cinvalid\u003e\t10\tkubelet, minikube\tWarning\tFailedSync\tError syncing pod 1m\t\u003cinvalid\u003e\t6\tkubelet, minikube\tspec.containers{fail}\tNormal\tBackOff\tBack-off pulling image \"tutum/curl:1.123456\" Why couldn’t Kubernetes pull the image? There are three primary candidates besides network connectivity issues:\n The image tag is incorrect The image doesn’t exist Kubernetes doesn’t have permissions to pull that image  If you don’t notice a typo in your image tag, then it’s time to test using your local machine. I usually start by running docker pull on my local development machine with the exact same image tag. In this case, I would run docker pull tutum/curl:1.123456\nIf this succeeds, then it probably means that Kubernetes doesn’t have correct permissions to pull that image.\nAdd the docker registry user/pwd to your cluster\nkubectl create secret docker-registry dockersecret --docker-server=https://index.docker.io/v1/ --docker-username=\u003cusername\u003e --docker-password=\u003cpassword\u003e --docker-email=\u003cemail\u003e If the exact image tag fails, then I will test without an explicit image tag - docker pull tutum/curl - which will attempt to pull the latest tag. If this succeeds, then that means the originally specified tag doesn’t exist. Go to the Docker registry and check which tags are available for this image.\nIf docker pull tutum/curl (without an exact tag) fails, then we have a bigger problem - that image does not exist at all in our image registry.\n","categories":"","description":"Wrong Container Image or Invalid Registry Permissions","excerpt":"Wrong Container Image or Invalid Registry Permissions","ref":"/docs/guides/applications/missing-registry-permission/","tags":"","title":"Container Image not Pulled"},{"body":"Preface A container image should use a fixed tag or the content hash of the image. It should not use the tags latest, head, canary, or other tags that are designed to be floating.\nProblem Many Kubernetes users have run into this problem. The story goes something like this:\n Deploy any image using an image tag (e.g. cp-enablement/awesomeapp:1.0) Fix a bug in awesomeapp Build a new image and push it with the same tag (cp-enablement/awesomeapp:1.0) Update your deployment Realize that the bug is still present Rinse and repeat steps 3 to 5 until you recognize this doesn’t work  The problem relates to how Kubernetes decides whether to do a docker pull when starting a container. Since we tagged our image as :1.0, the default pull policy is IfNotPresent. The Kubelet already has a local copy of cp-enablement/awesomeapp:1.0, hence it doesn’t attempt to do a docker pull. When the new Pods come up, they still use the old broken Docker image.\nThere are three ways to resolve this:\n Switch to using the tag :latest (DO NOT DO THIS!) Specify ImagePullPolicy: always (not recomended). Use unique tags (best practice)  Solution In the quest to automate myself out of a job, I created a bash script that runs anytime to create a new tag and push the build result to the registry.\n#!/usr/bin/env bash  # Set the docker image name and the corresponding repository # Ensure that you change them in the deployment.yml as well. # You must be logged in with docker login… # # CHANGE THIS TO YOUR Docker.io SETTINGS # PROJECT=awesomeapp REPOSITORY=cp-enablement  # exit if any subcommand or pipeline returns a non-zero status. set -e  # set debug mode #set -x  # build my nodeJS app # npm run build  # get latest version IDs from the Docker.io registry and increment them # VERSION=$(curl https://registry.hub.docker.com/v1/repositories/$REPOSITORY/$PROJECT/tags | sed -e 's/[][]//g' -e 's/\"//g' -e 's/ //g' | tr '}' '\\n' | awk -F: '{print $3}' | grep v| tail -n 1) VERSION=${VERSION:1} ((VERSION++)) VERSION=\"v$VERSION\"   # build a new docker image # echo '\u003e\u003e\u003e Building new image' # Due to a bug in Docker we need to analyse the log to find out if build passed (see https://github.com/dotcloud/docker/issues/1875) docker build --no-cache=true -t $REPOSITORY/$PROJECT:$VERSION . | tee /tmp/docker_build_result.log RESULT=$(cat /tmp/docker_build_result.log | tail -n 1) if [[ \"$RESULT\" != *Successfully* ]]; then  exit -1 fi   echo '\u003e\u003e\u003e Push new image' docker push $REPOSITORY/$PROJECT:$VERSION ","categories":"","description":"Updating Images in your cluster during development","excerpt":"Updating Images in your cluster during development","ref":"/docs/guides/applications/image-pull-policy/","tags":"","title":"Container Image not Updating"},{"body":"Gardener Container Runtime Extension At the lowest layers of a Kubernetes node is the software that, among other things, starts and stops containers. It is called “Container Runtime”. The most widely known container runtime is Docker, but it is not alone in this space. In fact, the container runtime space has been rapidly evolving.\nKubernetes supports different container runtimes using Container Runtime Interface (CRI) – a plugin interface which enables kubelet to use a wide variety of container runtimes.\nGardener supports creation of Worker machines using CRI, more information can be found here: CRI Support.\nMotivation Prior to the Container Runtime Extensibility concept, Gardener used Docker as the only container runtime to use in shoot worker machines. Because of the wide variety of different container runtimes offers multiple important features (for example enhanced security concepts) it is important to enable end users to use other container runtimes as well.\nThe ContainerRuntime Extension Resource Here is what a typical ContainerRuntime resource would look-like:\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: ContainerRuntime metadata:  name: my-container-runtime spec:  binaryPath: /var/bin/containerruntimes  type: gvisor  workerPool:  name: worker-ubuntu  selector:  matchLabels:  worker.gardener.cloud/pool: worker-ubuntu Gardener deploys one ContainerRuntime resource per worker pool per CRI. To exemplify this, consider a Shoot having two worker pools (worker-one, worker-two) using containerd as the CRI as well as gvisor and kata as enabled container runtimes. Gardener would deploy four ContainerRuntime resources. For worker-one: one ContainerRuntime for type gvisor and one for type kata. The same resource are being deployed for worker-two.\nSupporting a new Container Runtime Provider To add support for another container runtime (e.g., gvisor, kata-containers, etc.) a container runtime extension controller needs to be implemented. It should support Gardener’s supported CRI plugins.\nThe container runtime extension should install the necessary resources into the shoot cluster (e.g., RuntimeClasses), and it should copy the runtime binaries to the relevant worker machines in path: spec.binaryPath. Gardener labels the shoot nodes according to the CRI configured: worker.gardener.cloud/cri-name=\u003cvalue\u003e (e.g worker.gardener.cloud/cri-name=containerd) and multiple labels for each of the container runtimes configured for the shoot Worker machine: containerruntime.worker.gardener.cloud/\u003ccontainer-runtime-type-value\u003e=true (e.g containerruntime.worker.gardener.cloud/gvisor=true). The way to install the binaries is by creating a daemon set which copies the binaries from an image in a docker registry to the relevant labeled Worker’s nodes (avoid downloading binaries from internet to also cater with isolated environments).\nFor additional reference, please have a look at the runtime-gvsior provider extension, which provides more information on how to configure the necessary charts as well as the actuators required to reconcile container runtime inside the Shoot cluster to the desired state.\n","categories":"","description":"","excerpt":"Gardener Container Runtime Extension At the lowest layers of a …","ref":"/docs/gardener/extensions/containerruntime/","tags":"","title":"ContainerRuntime"},{"body":"Control Plane Migration Preconditions To be able to use this feature, the SeedChange feature gate has to be enabled on your gardener-apiserver.\nAlso, the involved Seeds need to have enabled BackupBuckets.\nShootState ShootState is an API resource which stores non-reconstructible state and data required to completely recreate a Shoot’s control plane on a new Seed. The ShootState resource is created on Shoot creation in its Project namespace and the required state/data is persisted during Shoot creation or reconciliation.\nShoot Control Plane Migration Triggering the migration is done by changing the Shoot’s .spec.seedName to a Seed that differs from the .status.seedName, we call this Seed \"Destination Seed\". This action can only be performed by an operator with necessary RBAC. If the Destination Seed does not have a backup and restore configuration, the change to spec.seedName is rejected. Additionally, this Seed must not be set for deletion and must be healthy.\nIf the Shoot has different .spec.seedName and .status.seedName a process is started to prepare the Control Plane for migration:\n .status.lastOperation is changed to Migrate. Kubernetes API Server is stopped and the extension resources are annotated with gardener.cloud/operation=migrate. Full snapshot of the ETCD is created and terminating of the Control Plane in the Source Seed is initiated.  If the process is successful, we update the status of the Shoot by setting the .status.seedName to the null value. That way, a restoration is triggered in the Destination Seed and .status.lastOperation is changed to Restore. The control plane migration is completed when the Restore operation has completed successfully.\nWhen the CopyEtcdBackupsDuringControlPlaneMigration feature gate is enabled on the gardenlet, the etcd backups will be copied over to the BackupBucket of the Destination Seed during control plane migration and any future backups will be uploaded there. Otherwise, backups will continue to be uploaded to the BackupBucket of the Source Seed,\nTriggering the migration For controlplane migration, operators with necessary RBAC can use the shoots/binding subresource to change the .spec.seedName, with the following commands:\nexport NAMESPACE=my-namespace export SHOOT_NAME=my-shoot kubectl -n ${NAMESPACE} get shoot ${SHOOT_NAME} -o json | jq '.spec.seedName = \"\u003cdestination-seed\u003e\"' | kubectl replace --raw /apis/core.gardener.cloud/v1beta1/namespaces/${NAMESPACE}/shoots/${SHOOT_NAME}/binding -f - | jq -r '.spec.seedName' ","categories":"","description":"","excerpt":"Control Plane Migration Preconditions To be able to use this feature, …","ref":"/docs/gardener/usage/control_plane_migration/","tags":"","title":"Control Plane Migration"},{"body":"Gardener Controller Manager The gardener-controller-manager (often refered to as “GCM”) is a component that runs next to the Gardener API server, similar to the Kubernetes Controller Manager. It runs several control loops that do not require talking to any seed or shoot cluster. Also, as of today it exposes an HTTP server that is serving several health check endpoints and metrics.\nThis document explains the various functionalities of the gardener-controller-manager and their purpose.\nControl Loops Bastion Controller Bastion resources have a limited lifetime which can be extended up to a certain amount by performing a heartbeat on them. The Bastion controller is responsible for deleting expired or rotten Bastions.\n “expired” means a Bastion has exceeded its status.expirationTimestamp. “rotten” means a Bastion is older than the configured maxLifetime.  The maxLifetime defaults to 24 hours and is an option in the BastionControllerConfiguration which is part of gardener-controller-managers ControllerManagerControllerConfiguration, see the example config file for details.\nThe controller also deletes Bastions in case the referenced Shoot\n does no longer exist is marked for deletion (i.e., have a non-nil .metadata.deletionTimestamp) was migrated to another seed (i.e., Shoot.spec.seedName is different than Bastion.spec.seedName).  The deletion of Bastions triggers the gardenlet to perform the necessary cleanups in the Seed cluster, so some time can pass between deletion and the Bastion actually disappearing. Clients like gardenctl are advised to not re-use Bastions whose deletion timestamp has been set already.\nRefer to GEP-15 for more information on the lifecycle of Bastion resources.\nCertificateSigningRequest Controller After the gardenlet gets deployed on the Seed cluster it needs to establish itself as a trusted party to communicate with the Gardener API server. It runs through a bootstrap flow similar to the kubelet bootstrap process.\nOn startup the gardenlet uses a kubeconfig with a bootstrap token which authenticates it as being part of the system:bootstrappers group. This kubeconfig is used to create a CertificateSigningRequest (CSR) against the Gardener API server.\nThe controller in gardener-controller-manager checks whether the CertificateSigningRequest has the expected organisation, common name and usages which the gardenlet would request.\nIt only auto-approves the CSR if the client making the request is allowed to “create” the certificatesigningrequests/seedclient subresource. Clients with the system:bootstrappers group are bound to the gardener.cloud:system:seed-bootstrapper ClusterRole, hence, they have such privileges. As the bootstrap kubeconfig for the gardenlet contains a bootstrap token which is authenticated as being part of the systems:bootstrappers group, its created CSR gets auto-approved.\nCloudProfile Controller CloudProfiles are essential when it comes to reconciling Shoots since they contain constraints (like valid machine types, Kubernetes versions, or machine images) and sometimes also some global configuration for the respective environment (typically via provider-specific configuration in .spec.providerConfig).\nConsequently, to ensure that CloudProfiles in-use are always present in the system until the last referring Shoot gets deleted, the controller adds a finalizer which is only released when there is no Shoot referencing the CloudProfile anymore.\nControllerDeployment Controller Extensions are registered in garden cluster via ControllerRegistration and deployment of respective extensions are specified via ControllerDeployment. For more info refer here.\nThis controller ensures that ControllerDeployment in-use always exists until the last ControllerRegistration referencing them gets deleted. The controller adds a finalizer which is only released when there is no ControllerRegistration referencing the ControllerDeployment anymore.\nControllerRegistration Controller The ControllerRegistration controller makes sure that the required Gardener Extensions specified by the ControllerRegistration resources are present in the seed clusters. It also takes care of the creation and deletion of ControllerInstallation objects for a given seed cluster. The controller has three reconciliation loops.\n“Main” Reconciler This reconciliation loop watches the Seed objects and determines which ControllerRegistrations are required for them and reconciles the corresponding ControllerInstallation resources to reach the determined state. To begin with, it computes the kind/type combinations of extensions required for the seed. For this, the controller examines a live list of ControllerRegistrations, ControllerInstallations, BackupBuckets, BackupEntrys, Shoots, and Secrets from the garden cluster. For example, it examines the shoots running on the seed and deducts kind/type like Infrastructure/gcp. It also decides whether they should always be deployed based on the .spec.deployment.policy. For the configuration options, please see this section.\nBased on these required combinations, each of them are mapped to ControllerRegistration objects and then to their corresponding ControllerInstallation objects (if existing). The controller then creates or updates the required ControllerInstallation objects for the given seed. It also deletes every existing ControllerInstallation whose referenced ControllerRegistration is not part of the required list. For example, if the shoots in the seed are no longer using the DNS provider aws-route53, then the controller proceeds to delete the respective ControllerInstallation object.\n“ControllerRegistration” Reconciler This reconciliation loop watches the ControllerRegistration resource and adds finalizers to it when they are created. In case a deletion request comes in for the resource, i.e., if a .metadata.deletionTimestamp is set, it actively scans for a ControllerInstallation resource using this ControllerRegistration, and decides whether the deletion can be allowed. In case no related ControllerInstallation is present, it removes the finalizer and marks it for deletion.\n“Seed” Reconciler This loop also watches the Seed object and adds finalizers to it at creation. If a .metadata.deletionTimestamp is set for the seed then the controller checks for existing ControllerInstallation objects which reference this seed. If no such objects exist then it removes the finalizer and allows the deletion.\nEvent Controller With the Gardener Event Controller you can prolong the lifespan of events related to Shoot clusters. This is an optional controller which will become active once you provide the below mentioned configuration.\nAll events in K8s are deleted after a configurable time-to-live (controlled via a kube-apiserver argument called --event-ttl (defaulting to 1 hour)). The need to prolong the time-to-live for Shoot cluster events frequently arises when debugging customer issues on live systems. This controller leaves events involving Shoots untouched while deleting all other events after a configured time. In order to activate it, provide the following configuration:\n concurrentSyncs: The amount of goroutines scheduled for reconciling events. ttlNonShootEvents: When an event reaches this time-to-live it gets deleted unless it is a Shoot-related event (defaults to 1h, equivalent to the event-ttl default).   ⚠️ In addition, you should also configure the --event-ttl for the kube-apiserver to define an upper-limit of how long Shoot-related events should be stored. The --event-ttl should be larger than the ttlNonShootEvents or this controller will have no effect.\n ExposureClass Controller ExposureClass abstracts the ability to expose a Shoot clusters control plane in certain network environments (e.g. corporate networks, DMZ, internet) on all Seeds or a subset of the Seeds. For more refer.\nConsequently, to ensure that ExposureClasss in-use are always present in the system until the last referring Shoot gets deleted, the controller adds a finalizer which is only released when there is no Shoot referencing the ExposureClass anymore.\nQuota Controller Quota object limits the resources consumed by shoot clusters either per provider secret or per project/namespace.\nConsequently, to ensure that Quotas in-use are always present in the system until the last SecretBinding that references them gets deleted, the controller adds a finalizer which is only released when there is no SecretBinding referencing the Quota anymore.\nProject Controller There are multiple controllers responsible for different aspects of Project objects. Please also refer to the Project documentation.\n“Main” Reconciler This reconciler manages a dedicated Namespace for each Project. The namespace name can either be specified explicitly in .spec.namespace (must be prefixed with garden-) or it will be determined by the controller. If .spec.namespace is set, it tries to create it. If it already exists, it tries to adopt it. This will only succeed if the Namespace was previously labeled with gardener.cloud/role=project and project.gardener.cloud/name=\u003cproject-name\u003e. This is to prevent that end-users can adopt arbitrary namespaces and escalate their privileges, e.g. the kube-system namespace.\nAfter the namespace was created/adopted the controller creates several ClusterRoles and ClusterRoleBindings that allow the project members to access related resources based on their roles. These RBAC resources are prefixed with gardener.cloud:system:project{-member,-viewer}:\u003cproject-name\u003e. Gardener administrators and extension developers can define their own roles, see this document for more information.\nIn addition, operators can configure the Project controller to maintain a default ResourceQuota for project namespaces. Quotas can especially limit the creation of user facing resources, e.g. Shoots, SecretBindings, Secrets and thus protect the Garden cluster from massive resource exhaustion but also enable operators to align quotas with respective enterprise policies.\n ⚠️ Gardener itself is not exempted from configured quotas. For example, Gardener creates Secrets for every shoot cluster in the project namespace and at the same time increases the available quota count. Please mind this additional resource consumption.\n The controller configuration provides a template section controllers.project.quotas where such a ResourceQuota (see example below) can be deposited.\ncontrollers:  project:  quotas:  - config:  apiVersion: v1  kind: ResourceQuota  spec:  hard:  count/shoots.core.gardener.cloud: \"100\"  count/secretbindings.core.gardener.cloud: \"10\"  count/secrets: \"800\"  projectSelector: {} The Project controller takes the specified config and creates a ResourceQuota with the name gardener in the project namespace. If a ResourceQuota resource with the name gardener already exists, the controller will only update fields in spec.hard which are unavailable at that time. This is done to configure a default Quota in all projects but to allow manual quota increases as the projects’ demands increase. spec.hard fields in the ResourceQuota object that are not present in the configuration are removed from the object. Labels and annotations on the ResourceQuota config get merged with the respective fields on existing ResourceQuotas. An optional projectSelector narrows down the amount of projects that are equipped with the given config. If multiple configs match for a project, then only the first match in the list is applied to the project namespace.\nThe .status.phase of the Project resources is set to Ready or Failed by the reconciler to indicate whether the reconciliation loop was performed successfully. Also, it generates Events to provide further information about its operations.\nWhen a Project is marked for deletion, the controller ensures that there are no Shoots left in the project namespace. Once all Shoots are gone, the Namespace and Project is released.\n“Stale Projects” Reconciler As Gardener is a large-scale Kubernetes as a Service it is designed for being used by a large amount of end-users. Over time, it is likely to happen that some of the hundreds or thousands of Project resources are no longer actively used.\nGardener offers the “stale projects” reconciler which will take care of identifying such stale projects, marking them with a “warning”, and eventually deleting them after a certain time period. This reconciler is enabled by default and works as following:\n Projects are considered as “stale”/not actively used when all of the following conditions apply: The namespace associated with the Project does not have any…  Shoot resources. BackupEntry resources. Secret resources that are referenced by a SecretBinding that is in use by a Shoot (not necessarily in the same namespace). Quota resources that are referenced by a SecretBinding that is in use by a Shoot (not necessarily in the same namespace). The time period when the project was used for the last time (status.lastActivityTimestamp) is longer than the configured minimumLifetimeDays    If a project is considered “stale” then its .status.staleSinceTimestamp will be set to the time when it was first detected to be stale. If it gets actively used again this timestamp will be removed. After some time the .status.staleAutoDeleteTimestamp will be set to a timestamp after which Gardener will auto-delete the Project resource if it still is not actively used.\nThe component configuration of the gardener-controller-manager offers to configure the following options:\n minimumLifetimeDays: Don’t consider newly created Projects as “stale” too early to give people/end-users some time to onboard and get familiar with the system. The “stale project” reconciler won’t set any timestamp for Projects younger than minimumLifetimeDays. When you change this value then projects marked as “stale” may be no longer marked as “stale” in case they are young enough, or vice versa. staleGracePeriodDays: Don’t compute auto-delete timestamps for stale Projects that are unused for only less than staleGracePeriodDays. This is to not unnecessarily make people/end-users nervous “just because” they haven’t actively used their Project for a given amount of time. When you change this value then already assigned auto-delete timestamps may be removed again if the new grace period is not yet exceeded. staleExpirationTimeDays: Expiration time after which stale Projects are finally auto-deleted (after .status.staleSinceTimestamp). If this value is changed and an auto-delete timestamp got already assigned to the projects then the new value will only take effect if it’s increased. Hence, decreasing the staleExpirationTimeDays will not decrease already assigned auto-delete timestamps.   Gardener administrators/operators can exclude specific Projects from the stale check by annotating the related Namespace resource with project.gardener.cloud/skip-stale-check=true.\n “Activity” Reconciler Since the other two reconcilers are unable to actively monitor the relevant objects that are used in a Project (Shoot, Secret, etc.), there could be a situation where the user creates and deletes objects in a short period of time. In that case the Stale Project Reconciler could not see that there was any activity on that project and it will still mark it as a Stale, even though it is actively used.\nThe Project Activity Reconciler is implemented to take care of such cases. An event handler will notify the reconciler for any acitivity and then it will update the status.lastActivityTimestamp. This update will also trigger the Stale Project Reconciler.\nSecretBinding Controller SecretBindings reference Secrets and Quotas and are themselves referenced by Shoots. The controller adds finalizers to the referenced objects to ensure they don’t get deleted while still being referenced. Similarly, to ensure that SecretBindings in-use are always present in the system until the last referring Shoot gets deleted, the controller adds a finalizer which is only released when there is no Shoot referencing the SecretBinding anymore.\nReferenced Secrets will also be labeled with provider.shoot.gardener.cloud/\u003ctype\u003e=true where \u003ctype\u003e is the value of the .provider.type of the SecretBinding. Also, all referenced Secrets as well as Quotas will be labeled with reference.gardener.cloud/secretbinding=true to allow easily filtering for objects referenced by SecretBindings.\nSeed Controller The Seed controller in the gardener-controller-manager reconciles Seed objects with the help of the following reconcilers.\n“Main” Reconciler This reconciliation loop takes care about seed related operations in the Garden cluster. When a new Seed object is created the reconciler creates a new Namespace in the garden cluster seed-\u003cseed-name\u003e. Namespaces dedicated to single seed clusters allow us to segregate access permissions i.e., a gardenlet must not have permissions to access objects in all Namespaces in the Garden cluster. There are objects in a Garden environment which are created once by the operator e.g., default domain secret, alerting credentials, and required for operations happening in the gardenlet. Therefore, we not only need a seed specific Namespace but also a copy of these “shared” objects.\nThe “main” reconciler takes care about this replication:\n   Kind Namespace Label Selector     Secret garden gardener.cloud/role    “Backup Buckets Check” Reconciler Every time a BackupBucket object is created or updated, the referenced Seed object is enqueued for reconciliation. It’s the reconciler’s task to check the status subresource of all existing BackupBuckets that reference this Seed. If at least one BackupBucket has .status.lastError != nil, the BackupBucketsReady condition on the Seed will be set to False, and consequently the Seed is considered as NotReady. If the SeedBackupBucketsCheckControllerConfiguration (which is part of gardener-controller-managers component configuration) contains a conditionThreshold for the BackupBucketsReady, the condition will instead first be set to Progressing and eventually to False once the conditionThreshold expires, see the example config file for details. Once the BackupBucket is healthy again, the seed will be re-queued and the condition will turn true.\n“Extensions Check” Reconciler This reconciler reconciles Seed objects and checks whether all ControllerInstallations referencing them are in a healthy state. Concretely, all three conditions Valid, Installed, and Healthy must have status True and the Progressing condition must have status False. Based on this check, it maintains the ExtensionsReady condition in the respective Seed’s .status.conditions list.\n“Lifecycle” Reconciler The “Lifecycle” reconciler processes Seed objects which are enqueued every 10 seconds in order to check if the responsible gardenlet is still responding and operable. Therefore, it checks renewals via Lease objects of the seed in the garden cluster which are renewed regularly by the gardenlet.\nIn case a Lease is not renewed for the configured amount in config.controllers.seed.monitorPeriod.duration:\n The reconciler assumes that the gardenlet stopped operating and updates the GardenletReady condition to Unknown. Additionally, conditions and constraints of all Shoot resources scheduled on the affected seed are set to Unknown as well because a striking gardenlet won’t be able to maintain these conditions any more. If the gardenlet’s client certificate has expired (identified based on the .status.clientCertificateExpirationTimestamp field in the Seed resource) and if it is managed by a ManagedSeed then this will be triggered for a reconciliation. This will trigger the bootstrapping process again and allows gardenlets to obtain a fresh client certificate.  Shoot Controller “Conditions” Reconciler In case the reconciled Shoot is registered via a ManagedSeed as a seed cluster, this reconciler merges the conditions in the respective Seed’s .status.conditions into the .status.conditions of the Shoot. This is to provide a holistic view on the status of the registered seed cluster by just looking at the Shoot resource.\n“Hibernation” Reconciler This reconciler is responsible for hibernating or awakening shoot clusters based on the schedules defined in their .spec.hibernation.schedules. It ignores failed Shoots and those marked for deletion.\n“Maintenance” Reconciler This reconciler is responsible for maintaining shoot clusters based on the time window defined in their .spec.maintenance.timeWindow. It might auto-update the Kubernetes version or the operating system versions specified in the worker pools (.spec.provider.workers). It could also add some operation or task annotations, read more here.\n“Quota” Reconciler This reconciler might auto-delete shoot clusters in case their referenced SecretBinding is itself referencing a Quota with .spec.clusterLifetimeDays != nil. If the shoot cluster is older than the configured lifetime then it gets deleted. It maintains the expiration time of the Shoot in the value of the shoot.gardener.cloud/expiration-timestamp annotation. This annotation might be overridden, however only by at most twice the value of the .spec.clusterLifetimeDays.\n“Reference” Reconciler Shoot objects may specify references to other objects in the Garden cluster which are required for certain features. For example, users can configure various DNS providers via .spec.dns.providers and usually need to refer to a corresponding Secret with valid DNS provider credentials inside. Such objects need a special protection against deletion requests as long as they are still being referenced by one or multiple shoots.\nTherefore, this reconciler checks Shoots for referenced objects and adds the finalizer gardener.cloud/reference-protection to their .metadata.finalizers list. The reconciled Shoot also gets this finalizer to enable a proper garbage collection in case the gardener-controller-manager is offline at the moment of an incoming deletion request. When an object is not actively referenced anymore because the Shoot specification has changed or all related shoots were deleted (are in deletion), the controller will remove the added finalizer again so that the object can safely be deleted or garbage collected.\nThis reconciler inspects the following references:\n DNS provider secrets (.spec.dns.provider) Audit policy configmaps (.spec.kubernetes.kubeAPIServer.auditConfig.auditPolicy.configMapRef)  Further checks might be added in the future.\n“Retry” Reconciler This reconciler is responsible for retrying certain failed Shoots. Currently, the reconciler retries only failed Shoots with error code ERR_INFRA_RATE_LIMITS_EXCEEDED, see this document for more details.\n“Status Label” Reconciler This reconciler is responsible for maintaining the shoot.gardener.cloud/status label on Shoots, see this document for more details.\n","categories":"","description":"","excerpt":"Gardener Controller Manager The gardener-controller-manager (often …","ref":"/docs/gardener/concepts/controller-manager/","tags":"","title":"Controller Manager"},{"body":"Registering Extension Controllers Extensions are registered in the garden cluster via ControllerRegistration resources. Deployment for respective extensions are specified via ControllerDeployment resources. Gardener evaluates the registrations and deployments and creates ControllerInstallation resources which describe the request “please install this controller X to this seed Y”.\nSimilar to how CloudProfile or Seed resources get into the system, the Gardener administrator must deploy the ControllerRegistration and ControllerDeployment resources (this does not happen automatically in any way - the administrator decides which extensions shall be enabled).\nThe specification mainly describes which of Gardener’s extension CRDs are managed, for example:\napiVersion: core.gardener.cloud/v1beta1 kind: ControllerDeployment metadata:  name: os-gardenlinux type: helm providerConfig:  chart: H4sIFAAAAAAA/yk... # \u003cbase64-gzip-chart\u003e  values:  foo: bar --- apiVersion: core.gardener.cloud/v1beta1 kind: ControllerRegistration metadata:  name: os-gardenlinux spec:  deployment:  deploymentRefs:  - name: os-gardenlinux  resources:  - kind: OperatingSystemConfig  type: gardenlinux  primary: true This information tells Gardener that there is an extension controller that can handle OperatingSystemConfig resources of type gardenlinux. A reference to the shown ControllerDeployment specifies how the deployment of the extension controller is accomplished.\nAlso, it specifies that this controller is the primary one responsible for the lifecycle of the OperatingSystemConfig resource. Setting primary to false would allow to register additional, secondary controllers that may also watch/react on the OperatingSystemConfig/coreos resources, however, only the primary controller may change/update the main status of the extension object (that are used to “communicate” with the Gardenlet). Particularly, only the primary controller may set .status.lastOperation, .status.lastError, .status.observedGeneration, and .status.state. Secondary controllers may contribute to the .status.conditions[] if they like, of course.\nSecondary controllers might be helpful in scenarios where additional tasks need to be completed which are not part of the reconciliation logic of the primary controller but separated out into a dedicated extension.\n⚠️ There must be exactly one primary controller for every registered kind/type combination. Also, please note that the primary field cannot be changed after creation of the ControllerRegistration.\nDeploying Extension Controllers Submitting above ControllerDeployment and ControllerRegistration will create a ControllerInstallation resource:\napiVersion: core.gardener.cloud/v1beta1 kind: ControllerInstallation metadata:  name: os-gardenlinux spec:  deploymentRef:  name: os-gardenlinux  registrationRef:  name: os-gardenlinux  seedRef:  name: aws-eu1 This resource expresses that Gardener requires the os-gardenlinux extension controller to run on the aws-eu1 seed cluster.\nThe Gardener Controller Manager does automatically determine which extension is required on which seed cluster and will only create ControllerInstallation objects for those. Also, it will automatically delete ControllerInstallations referencing extension controllers that are no longer required on a seed (e.g., because all shoots on it have been deleted). There are additional configuration options, please see this section.\nHow do extension controllers get deployed to seeds? After Gardener has written the ControllerInstallation resource some component must satisfy this request and start deploying the extension controller to the seed. Depending on the complexity of the controller’s lifecycle management, configuration, etc. there are two possible scenarios:\nScenario 1: Deployed by Gardener In many cases the extension controllers are easy to deploy and configure. It is sufficient to simply create a Helm chart (standardized way of packaging software in the Kubernetes context) and deploy it together with some static configuration values. Gardener supports this scenario and allows to provide arbitrary deployment information in the ControllerDeployment resource’s .providerConfig section:\n... type: helm providerConfig:  chart: H4sIFAAAAAAA/yk...  values:  foo: bar If .type=helm then Gardener itself will take over the responsibility the deployment. It base64-decodes the provided Helm chart (.providerConfig.chart) and deploys it with the provided static configuration (.providerConfig.values). The chart and the values can be updated at any time - Gardener will recognize and re-trigger the deployment process.\nIn order to allow extensions to get information about the garden and the seed cluster Gardener does mix-in certain properties into the values (root level) of every deployed Helm chart:\ngardener:  garden:  identifier: \u003cuuid-of-gardener-installation\u003e  seed:  identifier: \u003cseed-name\u003e  region: europe  spec: \u003ccomplete-seed-spec\u003e Extensions can use this information in their Helm chart in case they require knowledge about the garden and the seed environment. The list might be extended in the future.\nℹ️ Gardener uses the UUID of the garden Namespace object in the .gardener.garden.identifier property.\nScenario 2: Deployed by a (non-human) Kubernetes operator Some extension controllers might be more complex and require additional domain-specific knowledge wrt. lifecycle or configuration. In this case, we encourage to follow the Kubernetes operator pattern and deploy a dedicated operator for this extension into the garden cluster. The ControllerDeployments’s .type field would then not be helm, and no Helm chart or values need to be provided there. Instead, the operator itself knows how to deploy the extension into the seed. It must watch ControllerInstallation resources and act one those referencing a ControllerRegistration the operator is responsible for.\nIn order to let Gardener know that the extension controller is ready and running in the seed the ControllerInstallation’s .status field supports two conditions: RegistrationValid and InstallationSuccessful - both must be provided by the responsible operator:\n... status:  conditions:  - lastTransitionTime: \"2019-01-22T11:51:11Z\"  lastUpdateTime: \"2019-01-22T11:51:11Z\"  message: Chart could be rendered successfully.  reason: RegistrationValid  status: \"True\"  type: Valid  - lastTransitionTime: \"2019-01-22T11:51:12Z\"  lastUpdateTime: \"2019-01-22T11:51:12Z\"  message: Installation of new resources succeeded.  reason: InstallationSuccessful  status: \"True\"  type: Installed Additionally, the .status field has a providerStatus section into which the operator can (optionally) put any arbitrary data associated with this installation.\nExtensions in the garden cluster itself The Shoot resource itself will contain some provider-specific data blobs. As a result, some extensions might also want to run in the garden cluster, e.g., to provide ValidatingWebhookConfigurations for validating the correctness of their provider-specific blobs:\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata:  name: johndoe-aws  namespace: garden-dev spec:  ...  cloud:  type: aws  region: eu-west-1  providerConfig:  apiVersion: aws.cloud.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  vpc: # specify either 'id' or 'cidr'  # id: vpc-123456  cidr: 10.250.0.0/16  internal:  - 10.250.112.0/22  public:  - 10.250.96.0/22  workers:  - 10.250.0.0/19  zones:  - eu-west-1a ... In the above example, Gardener itself does not understand the AWS-specific provider configuration for the infrastructure. However, if this part of the Shoot resource should be validated then you should run an AWS-specific component in the garden cluster that registers a webhook. You can do it similarly if you want to default some fields of a resource (by using a MutatingWebhookConfiguration).\nAgain, similar to how Gardener is deployed to the garden cluster, these components must be deployed and managed by the Gardener administrator.\nExtension resource configurations The Extension resource allows injecting arbitrary steps into the shoot reconciliation flow that are unknown to Gardener. Hence, it is slightly special and allows further configuration when registering it:\napiVersion: core.gardener.cloud/v1beta1 kind: ControllerRegistration metadata:  name: extension-foo spec:  resources:  - kind: Extension  type: foo  primary: true  globallyEnabled: true  reconcileTimeout: 30s The globallyEnabled=true option specifies that the Extension/foo object shall be created by default for all shoots (unless they opted out by setting .spec.extensions[].enabled=false in the Shoot spec).\nThe reconcileTimeout tells Gardener how long it should wait during its shoot reconciliation flow for the Extension/foo’s reconciliation to finish.\nDeployment configuration options The .spec.deployment resource allows to configure a deployment policy. There are the following policies:\n OnDemand (default): Gardener will demand the deployment and deletion of the extension controller to/from seed clusters dynamically. It will automatically determine (based on other resources like Shoots) whether it is required and decide accordingly. Always: Gardener will demand the deployment of the extension controller to seed clusters independent of whether it is actually required or not. This might be helpful if you want to add a new component/controller to all seed clusters by default. Another use-case is to minimize the durations until extension controllers get deployed and ready in case you have highly fluctuating seed clusters. AlwaysExceptNoShoots: Similar to Always, but if the seed does not have any shoots then the extension is not being deployed. It will be deleted from a seed after the last shoot has been removed from it.  Also, the .spec.deployment.seedSelector allows to specify a label selector for seed clusters. Only if it matches the labels of a seed then it will be deployed to it. Please note that a seed selector can only be specified for secondary controllers (primary=false for all .spec.resources[]).\n","categories":"","description":"","excerpt":"Registering Extension Controllers Extensions are registered in the …","ref":"/docs/gardener/extensions/controllerregistration/","tags":"","title":"ControllerRegistration"},{"body":"Contract: ControlPlane resource Most Kubernetes clusters require a cloud-controller-manager or CSI drivers in order to work properly. Before introducing the ControlPlane extension resource Gardener was having several different Helm charts for the cloud-controller-manager deployments for the various providers. Now, Gardener commissions an external, provider-specific controller to take over this task.\nWhich control plane resources are required? As mentioned in the controlplane customization webhooks document Gardener shall not deploy any cloud-controller-manager or any other provider-specific component. Instead, it creates a ControlPlane CRD that should be picked up by provider extensions. Its purpose is to trigger the deployment of such provider-specific components in the shoot namespace in the seed cluster.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: ControlPlane metadata:  name: control-plane  namespace: shoot--foo--bar spec:  type: openstack  region: europe-west1  secretRef:  name: cloudprovider  namespace: shoot--foo--bar  providerConfig:  apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1  kind: ControlPlaneConfig  loadBalancerProvider: provider  zone: eu-1a  cloudControllerManager:  featureGates:  CustomResourceValidation: true  infrastructureProviderStatus:  apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureStatus  networks:  floatingPool:  id: vpc-1234  subnets:  - purpose: nodes  id: subnetid The .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used for the shoot cluster. However, the most important section is the .spec.providerConfig and the .spec.infrastructureProviderStatus. The first one contains an embedded declaration of the provider specific configuration for the control plane (that cannot be known by Gardener itself). You are responsible for designing how this configuration looks like. Gardener does not evaluate it but just copies this part from what has been provided by the end-user in the Shoot resource. The second one contains the output of the Infrastructure resource (that might be relevant for the CCM config).\nIn order to support a new control plane provider you need to write a controller that watches all ControlPlanes with .spec.type=\u003cmy-provider-name\u003e. You can take a look at the below referenced example implementation for the Alicloud provider.\nThe control plane controller as part of the ControlPlane reconciliation, often deploys resources (e.g. pods/deployments) into the Shoot namespace in the Seed as part of its ControlPlane reconciliation loop. Because the namespace contains network policies that per default deny all ingress and egress traffic, the pods may need to have proper labels matching to the selectors of the network policies in order to allow the required network traffic. Otherwise, they won’t be allowed to talk to certain other components (e.g., the kube-apiserver of the shoot). Please see this document for more information.\nNon-provider specific information required for infrastructure creation Most providers might require further information that is not provider specific but already part of the shoot resource. One example for this is the GCP control plane controller which needs the Kubernetes version of the shoot cluster (because it already uses the in-tree Kubernetes cloud-controller-manager). As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information that is not part of the Infrastructure resource itself.\nReferences and additional resources  ControlPlane API (Golang specification) Exemplary implementation for the Alicloud provider  ","categories":"","description":"","excerpt":"Contract: ControlPlane resource Most Kubernetes clusters require a …","ref":"/docs/gardener/extensions/controlplane/","tags":"","title":"ControlPlane"},{"body":"Contract: ControlPlane resource with purpose exposure Some Kubernetes clusters require an additional deployments required by the seed cloud provider in order to work properly, e.g. AWS Load Balancer Readvertiser. Before using ControlPlane resources with purpose exposure Gardener was having different Helm charts for the deployments for the various providers. Now, Gardener commissions an external, provider-specific controller to take over this task.\nWhich control plane resources are required? As mentioned in the controlplane document Gardener shall not deploy any other provider-specific component. Instead, it creates a ControlPlane CRD with purpose exposure that should be picked up by provider extensions. Its purpose is to trigger the deployment of such provider-specific components in the shoot namespace in the seed cluster that are needed to expose the kube-apiserver.\nThe shoot cluster’s kube-apiserver are exposed via a Service of type LoadBalancer from the shoot provider (you may run the control plane of an Azure shoot in a GCP seed) it’s the seed provider extension controller that should act on the ControlPlane resources with purpose exposure.\nIf SNI is enabled, then the Service from above is of type ClusterIP and Gardner will not create ControlPlane resources with purpose exposure.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\napiVersion: extensions.gardener.cloud/v1alpha1 kind: ControlPlane metadata:  name: control-plane-exposure  namespace: shoot--foo--bar spec:  type: aws  purpose: exposure  region: europe-west1  secretRef:  name: cloudprovider  namespace: shoot--foo--bar The .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used for the shoot cluster. It is most likely not needed, however, still added for some potential corner cases. If you don’t need it then just ignore it. The .spec.region contains the region of the seed cluster.\nIn order to support a control plane provider with purpose exposure you need to write a controller or expand the existing controlplane controller that watches all ControlPlanes with .spec.type=\u003cmy-provider-name\u003e and purpose exposure. You can take a look at the below referenced example implementation for the AWS provider.\nNon-provider specific information required for infrastructure creation Most providers might require further information that is not provider specific but already part of the shoot resource. As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information.\nReferences and additional resources  ControlPlane API (Golang specification) Exemplary implementation for the AWS provider AWS Load Balancer Readvertiser  ","categories":"","description":"","excerpt":"Contract: ControlPlane resource with purpose exposure Some Kubernetes …","ref":"/docs/gardener/extensions/controlplane-exposure/","tags":"","title":"ControlPlane Exposure"},{"body":"Controlplane customization webhooks Gardener creates the Shoot controlplane in several steps of the Shoot flow. At different point of this flow, it:\n deploys standard controlplane components such as kube-apiserver, kube-controller-manager, and kube-scheduler by creating the corresponding deployments, services, and other resources in the Shoot namespace. initiates the deployment of custom controlplane components by ControlPlane controllers by creating a ControlPlane resource in the Shoot namespace.  In order to apply any provider-specific changes to the configuration provided by Gardener for the standard controlplane components, cloud extension providers can install mutating admission webhooks for the resources created by Gardener in the Shoot namespace.\nWhat needs to be implemented to support a new cloud provider? In order to support a new cloud provider you should install “controlplane” mutating webhooks for any of the following resources:\n Deployment with name kube-apiserver, kube-controller-manager, or kube-scheduler Service with name kube-apiserver OperatingSystemConfig with any name and purpose reconcile  See Contract Specification for more details on the contract that Gardener and webhooks should adhere to regarding the content of the above resources.\nYou can install 3 different kinds of controlplane webhooks:\n Shoot, or controlplane webhooks apply changes needed by the Shoot cloud provider, for example the --cloud-provider command line flag of kube-apiserver and kube-controller-manager. Such webhooks should only operate on Shoot namespaces labeled with shoot.gardener.cloud/provider=\u003cprovider\u003e. Seed, or controlplaneexposure webhooks apply changes needed by the Seed cloud provider, for example annotations on the kube-apiserver service to ensure cloud-specific load balancers are correctly provisioned for a service of type LoadBalancer. Such webhooks should only operate on Shoot namespaces labeled with seed.gardener.cloud/provider=\u003cprovider\u003e.  The labels shoot.gardener.cloud/provider and seed.gardener.cloud/provider are added by Gardener when it creates the Shoot namespace.\nContract Specification This section specifies the contract that Gardener and webhooks should adhere to in order to ensure smooth interoperability. Note that this contract can’t be specified formally and is therefore easy to violate, especially by Gardener. The Gardener team will nevertheless do its best to adhere to this contract in the future and to ensure via additional measures (tests, validations) that it’s not unintentionally broken. If it needs to be changed intentionally, this can only happen after proper communication has taken place to ensure that the affected provider webhooks could be adapted to work with the new version of the contract.\nNote: The contract described below may not necessarily be what Gardener does currently (as of May 2019). Rather, it reflects the target state after changes for Gardener extensibility have been introduced.\nkube-apiserver To deploy kube-apiserver, Gardener shall create a deployment and a service both named kube-apiserver in the Shoot namespace. They can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of the kube-apiserver deployment shall contain a container named kube-apiserver.\nThe command field of the kube-apiserver container shall contain the kube-apiserver command line. It shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n admission plugins (--enable-admission-plugins, --disable-admission-plugins) secure communications (--etcd-cafile, --etcd-certfile, --etcd-keyfile, …) audit log (--audit-log-*) ports (--secure-port)  The kube-apiserver command line shall not contain any provider-specific flags, such as:\n --cloud-provider --cloud-config  These flags can be added by webhooks if needed.\nThe kube-apiserver command line may contain a number of additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-apiserver behavior for the specific provider. Among the flags to be considered are:\n --endpoint-reconciler-type --advertise-address --feature-gates  Gardener may use SNI to expose the apiserver (APIServerSNI feature gate). In this case, Gardener shall label the kube-apiserver’s Deployment with core.gardener.cloud/apiserver-exposure: gardener-managed label and expects that the --endpoint-reconciler-type and --advertise-address flags are not modified.\nThe --enable-admission-plugins flag may contain admission plugins that are not compatible with CSI plugins such as PersistentVolumeLabel. Webhooks should therefore ensure that such admission plugins are either explicitly enabled (if CSI plugins are not used) or disabled (otherwise).\nThe env field of the kube-apiserver container shall not contain any provider-specific environment variables (so it will be empty). If any provider-specific environment variables are needed, they should be added by webhooks.\nThe volumes field of the pod template of the kube-apiserver deployment, and respectively the volumeMounts field of the kube-apiserver container shall not contain any provider-specific Secret or ConfigMap resources. If such resources should be mounted as volumes, this should be done by webhooks.\nThe kube-apiserver Service may be of type LoadBalancer, but shall not contain any provider-specific annotations that may be needed to actually provision a load balancer resource in the Seed provider’s cloud. If any such annotations are needed, they should be added by webhooks (typically controlplaneexposure webhooks).\nThe kube-apiserver Service shall be of type ClusterIP, if Gardener is using SNI to expose the apiserver (APIServerSNI feature gate). In this case, Gardener shall label this Service with core.gardener.cloud/apiserver-exposure: gardener-managed label and expects that no mutations happen.\nkube-controller-manager To deploy kube-controller-manager, Gardener shall create a deployment named kube-controller-manager in the Shoot namespace. It can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of the kube-controller-manager deployment shall contain a container named kube-controller-manager.\nThe command field of the kube-controller-manager container shall contain the kube-controller-manager command line. It shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n --kubeconfig, --authentication-kubeconfig, --authorization-kubeconfig --leader-elect secure communications (--tls-cert-file, --tls-private-key-file, …) cluster CIDR and identity (--cluster-cidr, --cluster-name) sync settings (--concurrent-deployment-syncs, --concurrent-replicaset-syncs) horizontal pod autoscaler (--horizontal-pod-autoscaler-*) ports (--port, --secure-port)  The kube-controller-manager command line shall not contain any provider-specific flags, such as:\n --cloud-provider --cloud-config --configure-cloud-routes --external-cloud-volume-plugin  These flags can be added by webhooks if needed.\nThe kube-controller-manager command line may contain a number of additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-controller-manager behavior for the specific provider. Among the flags to be considered are:\n --feature-gates  The env field of the kube-controller-manager container shall not contain any provider-specific environment variables (so it will be empty). If any provider-specific environment variables are needed, they should be added by webhooks.\nThe volumes field of the pod template of the kube-controller-manager deployment, and respectively the volumeMounts field of the kube-controller-manager container shall not contain any provider-specific Secret or ConfigMap resources. If such resources should be mounted as volumes, this should be done by webhooks.\nkube-scheduler To deploy kube-scheduler, Gardener shall create a deployment named kube-scheduler in the Shoot namespace. It can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of the kube-scheduler deployment shall contain a container named kube-scheduler.\nThe command field of the kube-scheduler container shall contain the kube-scheduler command line. It shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n --config --authentication-kubeconfig, --authorization-kubeconfig secure communications (--tls-cert-file, --tls-private-key-file, …) ports (--port, --secure-port)  The kube-scheduler command line may contain additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-controller-manager behavior for the specific provider. Among the flags to be considered are:\n --feature-gates  The kube-scheduler command line can’t contain provider-specific flags, and it makes no sense to specify provider-specific environment variables or mount provider-specific Secret or ConfigMap resources as volumes.\netcd-main and etcd-events To deploy etcd, Gardener shall create 2 Etcd named etcd-main and etcd-events in the Shoot namespace. They can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nGardener shall configure the Etcd resource completely to set up an etcd cluster which uses the default storage class of the seed cluster.\ncloud-controller-manager Gardener shall not deploy a cloud-controller-manager. If it is needed, it should be added by a ControlPlane controller\nCSI controllers Gardener shall not deploy a CSI controller. If it is needed, it should be added by a ControlPlane controller\nkubelet To specify the kubelet configuration, Gardener shall create a OperatingSystemConfig resource with any name and purpose reconcile in the Shoot namespace. It can therefore also be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener. Gardener may write multiple such resources with different type to the same Shoot namespaces if multiple OSs are used.\nThe OSC resource shall contain a unit named kubelet.service, containing the corresponding systemd unit configuration file. The [Service] section of this file shall contain a single ExecStart option having the kubelet command line as its value.\nThe OSC resource shall contain a file with path /var/lib/kubelet/config/kubelet, which contains a KubeletConfiguration resource in YAML format. Most of the flags that can be specified in the kubelet command line can alternatively be specified as options in this configuration as well.\nThe kubelet command line shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n --config --bootstrap-kubeconfig, --kubeconfig --network-plugin (and, if it equals cni, also --cni-bin-dir and --cni-conf-dir) --node-labels  The kubelet command line shall not contain any provider-specific flags, such as:\n --cloud-provider --cloud-config --provider-id  These flags can be added by webhooks if needed.\nThe kubelet command line / configuration may contain a number of additional provider-independent flags / options. In general, webhooks should ignore these unless they are known to interfere with the desired kubelet behavior for the specific provider. Among the flags / options to be considered are:\n --enable-controller-attach-detach (enableControllerAttachDetach) - should be set to true if CSI plugins are used, but in general can also be ignored since its default value is also true, and this should work both with and without CSI plugins. --feature-gates (featureGates) - should contain a list of specific feature gates if CSI plugins are used. If CSI plugins are not used, the corresponding feature gates can be ignored since enabling them should not harm in any way.  ","categories":"","description":"","excerpt":"Controlplane customization webhooks Gardener creates the Shoot …","ref":"/docs/gardener/extensions/controlplane-webhooks/","tags":"","title":"ControlPlane Webhooks"},{"body":"General conventions All the extensions that are registered to Gardener are deployed to the seed clusters, on which they are required (also see ControllerRegistration).\nSome of these extensions might need to create global resources in the seed (e.g., ClusterRoles), i.e., it’s important to have a naming scheme to avoid conflicts as it cannot be checked or validated upfront that two extensions don’t use the same names.\nConsequently, this page should help answering some general questions that might come up when it comes to developing an extension.\nPriorityClasses Extensions are not supposed to create and use self-defined PriorityClasses. Instead, they can and should rely on well-known PriorityClasses managed by gardenlet.\nIs there a naming scheme for (global) resources? As there is no formal process to validate non-existence of conflicts between two extensions please follow these naming schemes when creating resources (especially, when creating global resources, but it’s in general a good idea for most created resources):\nThe resource name should be prefixed with extensions.gardener.cloud:\u003cextension-type\u003e-\u003cextension-name\u003e:\u003cresource-name\u003e, for example:\n extensions.gardener.cloud:provider-aws:machine-controller-manager extensions.gardener.cloud:extension-certificate-service:cert-broker  How to create resources in the shoot cluster? Some extensions might not only create resources in the seed cluster itself but also in the shoot cluster. Usually, every extension comes with a ServiceAccount and the required RBAC permissions when it gets installed to the seed. However, there are no credentials for the shoot for every extension.\nExtensions are supposed to use ManagedResources to manage resources in shoot clusters. gardenlet deploys gardener-resource-manager instances into all shoot control planes, that will reconcile ManagedResources without a specified class (spec.class=null) in shoot clusters.\nIf you need to deploy a non-DaemonSet resource you need to ensure that it only runs on nodes that are allowed to host system components and extensions. To do that you need to configure a nodeSelector as following:\nnodeSelector:  worker.gardener.cloud/system-components: \"true\" How to create kubeconfigs for the shoot cluster? Historically, Gardener extensions used to generate kubeconfigs with client certificates for components they deploy into the shoot control plane. For this, they reused the shoot cluster CA secret (ca) to issue new client certificates. With gardener/gardener#4661 we moved away from using client certificates in favor of short-lived, auto-rotated ServiceAccount tokens. These tokens are managed by gardener-resource-manager’s TokenRequestor. Extensions are supposed to reuse this mechanism for requesting tokens and a generic-token-kubeconfig for authenticating against shoot clusters.\nWith GEP-18 (Shoot cluster CA rotation), a dedicated CA will be used for signing client certificates (gardener/gardener#5779) which will be rotated when triggered by the shoot owner. With this, extensions cannot reuse the ca secret anymore to issue client certificates. Hence, extensions must switch to short-lived ServiceAccount tokens in order to support the CA rotation feature.\nThe generic-token-kubeconfig secret contains the CA bundle for establishing trust to shoot API servers. However, as the secret is immutable its name changes with the rotation of the cluster CA. Extensions need to look up the generic-token-kubeconfig.secret.gardener.cloud/name annotation on the respective Cluster object in order to determine which secret contains the current CA bundle. The helper function extensionscontroller.GenericTokenKubeconfigSecretNameFromCluster can be used for this task.\nYou can take a look at CA Rotation in Extensions for more details on the CA rotation feature in regard to extensions.\nHow to create certificates for the shoot cluster? Gardener creates several certificate authorities (CA) that are used to create server certificates for various components. For example, the shoot’s etcd has its own CA, the kube-aggregator has its own CA as well, and both are different to the actual cluster’s CA.\nWith GEP-18 (Shoot cluster CA rotation), extensions are required to do the same and generate dedicated CAs for their components (e.g. for signing a server certificate for cloud-controller-manager). They must not depend on the CA secrets managed by gardenlet.\nPlease see CA Rotation in Extensions for the exact requirements, that extensions need to fulfill in order to support the CA rotation feature.\n","categories":"","description":"","excerpt":"General conventions All the extensions that are registered to Gardener …","ref":"/docs/gardener/extensions/conventions/","tags":"","title":"Conventions"},{"body":"Packages:\n  core.gardener.cloud/v1beta1   core.gardener.cloud/v1beta1  Package v1beta1 is a version of the API.\nResource Types:  BackupBucket  BackupEntry  CloudProfile  ControllerDeployment  ControllerInstallation  ControllerRegistration  Project  Quota  SecretBinding  Seed  Shoot  BackupBucket   BackupBucket holds details about backup bucket\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  BackupBucket    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupBucketSpec     Specification of the Backup Bucket.\n     provider  BackupBucketProvider     Provider holds the details of cloud provider of the object store. This field is immutable.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to BackupBucket resource.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupBucket for running controller. This field is immutable.\n       status  BackupBucketStatus     Most recently observed status of the Backup Bucket.\n    BackupEntry   BackupEntry holds details about shoot backup.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  BackupEntry    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupEntrySpec     (Optional) Spec contains the specification of the Backup Entry.\n     bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    seedName  string    (Optional) SeedName holds the name of the seed to which this BackupEntry is scheduled\n       status  BackupEntryStatus     (Optional) Status contains the most recently observed status of the Backup Entry.\n    CloudProfile   CloudProfile represents certain properties about a provider environment.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  CloudProfile    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  CloudProfileSpec     (Optional) Spec defines the provider environment properties.\n     caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every host machine of shoot cluster targeting this profile.\n    kubernetes  KubernetesSettings     Kubernetes contains constraints regarding allowed values of the ‘kubernetes’ block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []MachineType     MachineTypes contains constraints regarding allowed values for machine types in the ‘workers’ block in the Shoot specification.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains provider-specific configuration for the profile.\n    regions  []Region     Regions contains constraints regarding allowed values for regions and zones.\n    seedSelector  SeedSelector     (Optional) SeedSelector contains an optional list of labels on Seed resources that marks those seeds whose shoots may use this provider profile. An empty list means that all seeds of the same provider type are supported. This is useful for environments that are of the same type (like openstack) but may have different “instances”/landscapes. Optionally a list of possible providers can be added to enable cross-provider scheduling. By default, the provider type of the seed must match the shoot’s provider.\n    type  string    Type is the name of the provider.\n    volumeTypes  []VolumeType     (Optional) VolumeTypes contains constraints regarding allowed values for volume types in the ‘workers’ block in the Shoot specification.\n       ControllerDeployment   ControllerDeployment contains information about how this controller is deployed.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  ControllerDeployment    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     type  string    Type is the deployment type.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     ProviderConfig contains type-specific configuration. It contains assets that deploy the controller.\n    ControllerInstallation   ControllerInstallation represents an installation request for an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  ControllerInstallation    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControllerInstallationSpec     Spec contains the specification of this installation. If the object’s deletion timestamp is set, this field is immutable.\n     registrationRef  Kubernetes core/v1.ObjectReference     RegistrationRef is used to reference a ControllerRegistration resource. The name field of the RegistrationRef is immutable.\n    seedRef  Kubernetes core/v1.ObjectReference     SeedRef is used to reference a Seed resource. The name field of the SeedRef is immutable.\n    deploymentRef  Kubernetes core/v1.ObjectReference     (Optional) DeploymentRef is used to reference a ControllerDeployment resource.\n       status  ControllerInstallationStatus     Status contains the status of this installation.\n    ControllerRegistration   ControllerRegistration represents a registration of an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  ControllerRegistration    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControllerRegistrationSpec     Spec contains the specification of this registration. If the object’s deletion timestamp is set, this field is immutable.\n     resources  []ControllerResource     (Optional) Resources is a list of combinations of kinds (DNSProvider, Infrastructure, Generic, …) and their actual types (aws-route53, gcp, auditlog, …).\n    deployment  ControllerRegistrationDeployment     (Optional) Deployment contains information for how this controller is deployed.\n       Project   Project holds certain properties about a Gardener project.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Project    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ProjectSpec     (Optional) Spec defines the project properties.\n     createdBy  Kubernetes rbac/v1.Subject     (Optional) CreatedBy is a subject representing a user name, an email address, or any other identifier of a user who created the project. This field is immutable.\n    description  string    (Optional) Description is a human-readable description of what the project is used for.\n    owner  Kubernetes rbac/v1.Subject     (Optional) Owner is a subject representing a user name, an email address, or any other identifier of a user owning the project. IMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the owner role. The only way to change the owner will be by moving the owner role. In this API version the only way to change the owner is to use this field. TODO: Remove this field in favor of the owner role in v1.\n    purpose  string    (Optional) Purpose is a human-readable explanation of the project’s purpose.\n    members  []ProjectMember     (Optional) Members is a list of subjects representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    namespace  string    (Optional) Namespace is the name of the namespace that has been created for the Project object. A nil value means that Gardener will determine the name of the namespace. This field is immutable.\n    tolerations  ProjectTolerations     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n       status  ProjectStatus     (Optional) Most recently observed status of the Project.\n    Quota   Quota represents a quota on resources consumed by shoot clusters either per project or per provider secret.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Quota    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  QuotaSpec     (Optional) Spec defines the Quota constraints.\n     clusterLifetimeDays  int32    (Optional) ClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.\n    metrics  Kubernetes core/v1.ResourceList     Metrics is a list of resources which will be put under constraints.\n    scope  Kubernetes core/v1.ObjectReference     Scope is the scope of the Quota object, either ‘project’ or ‘secret’. This field is immutable.\n       SecretBinding   SecretBinding represents a binding to a secret in the same or another namespace.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  SecretBinding    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret object in the same or another namespace. This field is immutable.\n    quotas  []Kubernetes core/v1.ObjectReference     (Optional) Quotas is a list of references to Quota objects in the same or another namespace. This field is immutable.\n    provider  SecretBindingProvider     (Optional) Provider defines the provider type of the SecretBinding. This field is immutable.\n    Seed   Seed represents an installation request for an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Seed    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  SeedSpec     Spec contains the specification of this installation.\n     backup  SeedBackup     (Optional) Backup holds the object store configuration for the backups of shoot (currently only etcd). If it is not specified, then there won’t be any backups taken for shoots associated with this seed. If backup field is present in seed, then backups of the etcd from shoot control plane will be stored under the configured object store.\n    dns  SeedDNS     DNS contains DNS-relevant information about this seed cluster.\n    networks  SeedNetworks     Networks defines the pod, service and worker network of the Seed cluster.\n    provider  SeedProvider     Provider defines the provider type and region for this Seed cluster.\n    secretRef  Kubernetes core/v1.SecretReference     (Optional) SecretRef is a reference to a Secret object containing the Kubeconfig of the Kubernetes cluster to be registered as Seed.\n    taints  []SeedTaint     (Optional) Taints describes taints on the seed.\n    volume  SeedVolume     (Optional) Volume contains settings for persistentvolumes created in the seed cluster.\n    settings  SeedSettings     (Optional) Settings contains certain settings for this seed cluster.\n    ingress  Ingress     (Optional) Ingress configures Ingress specific settings of the Seed cluster. This field is immutable.\n    highAvailability  HighAvailability     (Optional) HighAvailability describes the high availability configuration for seed system components. A highly available seed will need at least 3 nodes or 3 availability zones (depending on the configured FailureTolerance of node or zone), allowing spreading of system components across the configured failure domain.\n       status  SeedStatus     Status contains the status of this installation.\n    Shoot   Shoot represents a Shoot cluster created and managed by Gardener.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Shoot    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ShootSpec     (Optional) Specification of the Shoot cluster. If the object’s deletion timestamp is set, this field is immutable.\n     addons  Addons     (Optional) Addons contains information about enabled/disabled addons and their configuration.\n    cloudProfileName  string    CloudProfileName is a name of a CloudProfile object. This field is immutable.\n    dns  DNS     (Optional) DNS contains information about the DNS settings of the Shoot.\n    extensions  []Extension     (Optional) Extensions contain type and provider information for Shoot extensions.\n    hibernation  Hibernation     (Optional) Hibernation contains information whether the Shoot is suspended or not.\n    kubernetes  Kubernetes     Kubernetes contains the version and configuration settings of the control plane components.\n    networking  Networking     Networking contains information about cluster networking such as CNI Plugin type, CIDRs, …etc.\n    maintenance  Maintenance     (Optional) Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n    monitoring  Monitoring     (Optional) Monitoring contains information about custom monitoring configurations for the shoot.\n    provider  Provider     Provider contains all provider-specific and provider-relevant information.\n    purpose  ShootPurpose     (Optional) Purpose is the purpose class for this cluster.\n    region  string    Region is a name of a region. This field is immutable.\n    secretBindingName  string    SecretBindingName is the name of the a SecretBinding that has a reference to the provider secret. The credentials inside the provider secret will be used to create the shoot in the respective account. This field is immutable.\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot. This field is immutable when the SeedChange feature gate is disabled.\n    seedSelector  SeedSelector     (Optional) SeedSelector is an optional selector which must match a seed’s labels for the shoot to be scheduled on that seed.\n    resources  []NamedResourceReference     (Optional) Resources holds a list of named resource references that can be referred to in extension configs by their names.\n    tolerations  []Toleration     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n    exposureClassName  string    (Optional) ExposureClassName is the optional name of an exposure class to apply a control plane endpoint exposure strategy. This field is immutable.\n    systemComponents  SystemComponents     (Optional) SystemComponents contains the settings of system components in the control or data plane of the Shoot cluster.\n    controlPlane  ControlPlane     (Optional) ControlPlane contains general settings for the control plane of the shoot.\n       status  ShootStatus     (Optional) Most recently observed status of the Shoot cluster.\n    Addon   (Appears on: KubernetesDashboard, NginxIngress)  Addon allows enabling or disabling a specific addon and is used to derive from.\n   Field Description      enabled  bool    Enabled indicates whether the addon is enabled or not.\n    Addons   (Appears on: ShootSpec)  Addons is a collection of configuration for specific addons which are managed by the Gardener.\n   Field Description      kubernetesDashboard  KubernetesDashboard     (Optional) KubernetesDashboard holds configuration settings for the kubernetes dashboard addon.\n    nginxIngress  NginxIngress     (Optional) NginxIngress holds configuration settings for the nginx-ingress addon.\n    AdmissionPlugin   (Appears on: KubeAPIServerConfig)  AdmissionPlugin contains information about a specific admission plugin and its corresponding configuration.\n   Field Description      name  string    Name is the name of the plugin.\n    config  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) Config is the configuration of the plugin.\n    disabled  bool    (Optional) Disabled specifies whether this plugin should be disabled.\n    Alerting   (Appears on: Monitoring)  Alerting contains information about how alerting will be done (i.e. who will receive alerts and how).\n   Field Description      emailReceivers  []string    (Optional) MonitoringEmailReceivers is a list of recipients for alerts\n    AuditConfig   (Appears on: KubeAPIServerConfig)  AuditConfig contains settings for audit of the api server\n   Field Description      auditPolicy  AuditPolicy     (Optional) AuditPolicy contains configuration settings for audit policy of the kube-apiserver.\n    AuditPolicy   (Appears on: AuditConfig)  AuditPolicy contains audit policy for kube-apiserver\n   Field Description      configMapRef  Kubernetes core/v1.ObjectReference     (Optional) ConfigMapRef is a reference to a ConfigMap object in the same namespace, which contains the audit policy for the kube-apiserver.\n    AvailabilityZone   (Appears on: Region)  AvailabilityZone is an availability zone.\n   Field Description      name  string    Name is an an availability zone name.\n    unavailableMachineTypes  []string    (Optional) UnavailableMachineTypes is a list of machine type names that are not availability in this zone.\n    unavailableVolumeTypes  []string    (Optional) UnavailableVolumeTypes is a list of volume type names that are not availability in this zone.\n    BackupBucketProvider   (Appears on: BackupBucketSpec)  BackupBucketProvider holds the details of cloud provider of the object store.\n   Field Description      type  string    Type is the type of provider.\n    region  string    Region is the region of the bucket.\n    BackupBucketSpec   (Appears on: BackupBucket)  BackupBucketSpec is the specification of a Backup Bucket.\n   Field Description      provider  BackupBucketProvider     Provider holds the details of cloud provider of the object store. This field is immutable.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to BackupBucket resource.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupBucket for running controller. This field is immutable.\n    BackupBucketStatus   (Appears on: BackupBucket)  BackupBucketStatus holds the most recently observed status of the Backup Bucket.\n   Field Description      providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus is the configuration passed to BackupBucket resource.\n    lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the BackupBucket.\n    lastError  LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this BackupBucket. It corresponds to the BackupBucket’s generation, which is updated on mutation by the API Server.\n    generatedSecretRef  Kubernetes core/v1.SecretReference     (Optional) GeneratedSecretRef is reference to the secret generated by backup bucket, which will have object store specific credentials.\n    BackupEntrySpec   (Appears on: BackupEntry)  BackupEntrySpec is the specification of a Backup Entry.\n   Field Description      bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    seedName  string    (Optional) SeedName holds the name of the seed to which this BackupEntry is scheduled\n    BackupEntryStatus   (Appears on: BackupEntry)  BackupEntryStatus holds the most recently observed status of the Backup Entry.\n   Field Description      lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the BackupEntry.\n    lastError  LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this BackupEntry. It corresponds to the BackupEntry’s generation, which is updated on mutation by the API Server.\n    seedName  string    (Optional) SeedName is the name of the seed to which this BackupEntry is currently scheduled. This field is populated at the beginning of a create/reconcile operation. It is used when moving the BackupEntry between seeds.\n    migrationStartTime  Kubernetes meta/v1.Time     (Optional) MigrationStartTime is the time when a migration to a different seed was initiated.\n    CRI   (Appears on: MachineImageVersion, Worker)  CRI contains information about the Container Runtimes.\n   Field Description      name  CRIName     The name of the CRI library. Supported values are docker and containerd.\n    containerRuntimes  []ContainerRuntime     (Optional) ContainerRuntimes is the list of the required container runtimes supported for a worker pool.\n    CRIName (string alias)\n  (Appears on: CRI)  CRIName is a type alias for the CRI name string.\nCloudProfileSpec   (Appears on: CloudProfile)  CloudProfileSpec is the specification of a CloudProfile. It must contain exactly one of its defined keys.\n   Field Description      caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every host machine of shoot cluster targeting this profile.\n    kubernetes  KubernetesSettings     Kubernetes contains constraints regarding allowed values of the ‘kubernetes’ block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []MachineType     MachineTypes contains constraints regarding allowed values for machine types in the ‘workers’ block in the Shoot specification.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains provider-specific configuration for the profile.\n    regions  []Region     Regions contains constraints regarding allowed values for regions and zones.\n    seedSelector  SeedSelector     (Optional) SeedSelector contains an optional list of labels on Seed resources that marks those seeds whose shoots may use this provider profile. An empty list means that all seeds of the same provider type are supported. This is useful for environments that are of the same type (like openstack) but may have different “instances”/landscapes. Optionally a list of possible providers can be added to enable cross-provider scheduling. By default, the provider type of the seed must match the shoot’s provider.\n    type  string    Type is the name of the provider.\n    volumeTypes  []VolumeType     (Optional) VolumeTypes contains constraints regarding allowed values for volume types in the ‘workers’ block in the Shoot specification.\n    ClusterAutoscaler   (Appears on: Kubernetes)  ClusterAutoscaler contains the configuration flags for the Kubernetes cluster autoscaler.\n   Field Description      scaleDownDelayAfterAdd  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterAdd defines how long after scale up that scale down evaluation resumes (default: 1 hour).\n    scaleDownDelayAfterDelete  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterDelete how long after node deletion that scale down evaluation resumes, defaults to scanInterval (default: 0 secs).\n    scaleDownDelayAfterFailure  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterFailure how long after scale down failure that scale down evaluation resumes (default: 3 mins).\n    scaleDownUnneededTime  Kubernetes meta/v1.Duration     (Optional) ScaleDownUnneededTime defines how long a node should be unneeded before it is eligible for scale down (default: 30 mins).\n    scaleDownUtilizationThreshold  float64    (Optional) ScaleDownUtilizationThreshold defines the threshold in fraction (0.0 - 1.0) under which a node is being removed (default: 0.5).\n    scanInterval  Kubernetes meta/v1.Duration     (Optional) ScanInterval how often cluster is reevaluated for scale up or down (default: 10 secs).\n    expander  ExpanderMode     (Optional) Expander defines the algorithm to use during scale up (default: least-waste). See: https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders.\n    maxNodeProvisionTime  Kubernetes meta/v1.Duration     (Optional) MaxNodeProvisionTime defines how long CA waits for node to be provisioned (default: 20 mins).\n    maxGracefulTerminationSeconds  int32    (Optional) MaxGracefulTerminationSeconds is the number of seconds CA waits for pod termination when trying to scale down a node (default: 600).\n    ignoreTaints  []string    (Optional) IgnoreTaints specifies a list of taint keys to ignore in node templates when considering to scale a node group.\n    Condition   (Appears on: ControllerInstallationStatus, SeedStatus, ShootStatus)  Condition holds the information about the state of a resource.\n   Field Description      type  ConditionType     Type of the condition.\n    status  ConditionStatus     Status of the condition, one of True, False, Unknown.\n    lastTransitionTime  Kubernetes meta/v1.Time     Last time the condition transitioned from one status to another.\n    lastUpdateTime  Kubernetes meta/v1.Time     Last time the condition was updated.\n    reason  string    The reason for the condition’s last transition.\n    message  string    A human readable message indicating details about the transition.\n    codes  []ErrorCode     (Optional) Well-defined error codes in case the condition reports a problem.\n    ConditionStatus (string alias)\n  (Appears on: Condition)  ConditionStatus is the status of a condition.\nConditionType (string alias)\n  (Appears on: Condition)  ConditionType is a string alias.\nContainerRuntime   (Appears on: CRI)  ContainerRuntime contains information about worker’s available container runtime\n   Field Description      type  string    Type is the type of the Container Runtime.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to container runtime resource.\n    ControlPlane   (Appears on: ShootSpec)  ControlPlane holds information about the general settings for the control plane of a shoot.\n   Field Description      highAvailability  HighAvailability     (Optional) HighAvailability holds the configuration settings for high availability of the control plane of a shoot.\n    ControllerDeploymentPolicy (string alias)\n  (Appears on: ControllerRegistrationDeployment)  ControllerDeploymentPolicy is a string alias.\nControllerInstallationSpec   (Appears on: ControllerInstallation)  ControllerInstallationSpec is the specification of a ControllerInstallation.\n   Field Description      registrationRef  Kubernetes core/v1.ObjectReference     RegistrationRef is used to reference a ControllerRegistration resource. The name field of the RegistrationRef is immutable.\n    seedRef  Kubernetes core/v1.ObjectReference     SeedRef is used to reference a Seed resource. The name field of the SeedRef is immutable.\n    deploymentRef  Kubernetes core/v1.ObjectReference     (Optional) DeploymentRef is used to reference a ControllerDeployment resource.\n    ControllerInstallationStatus   (Appears on: ControllerInstallation)  ControllerInstallationStatus is the status of a ControllerInstallation.\n   Field Description      conditions  []Condition     (Optional) Conditions represents the latest available observations of a ControllerInstallations’s current state.\n    providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus contains type-specific status.\n    ControllerRegistrationDeployment   (Appears on: ControllerRegistrationSpec)  ControllerRegistrationDeployment contains information for how this controller is deployed.\n   Field Description      policy  ControllerDeploymentPolicy     (Optional) Policy controls how the controller is deployed. It defaults to ‘OnDemand’.\n    seedSelector  Kubernetes meta/v1.LabelSelector     (Optional) SeedSelector contains an optional label selector for seeds. Only if the labels match then this controller will be considered for a deployment. An empty list means that all seeds are selected.\n    deploymentRefs  []DeploymentRef     (Optional) DeploymentRefs holds references to ControllerDeployments. Only one element is support now.\n    ControllerRegistrationSpec   (Appears on: ControllerRegistration)  ControllerRegistrationSpec is the specification of a ControllerRegistration.\n   Field Description      resources  []ControllerResource     (Optional) Resources is a list of combinations of kinds (DNSProvider, Infrastructure, Generic, …) and their actual types (aws-route53, gcp, auditlog, …).\n    deployment  ControllerRegistrationDeployment     (Optional) Deployment contains information for how this controller is deployed.\n    ControllerResource   (Appears on: ControllerRegistrationSpec)  ControllerResource is a combination of a kind (DNSProvider, Infrastructure, Generic, …) and the actual type for this kind (aws-route53, gcp, auditlog, …).\n   Field Description      kind  string    Kind is the resource kind, for example “OperatingSystemConfig”.\n    type  string    Type is the resource type, for example “coreos” or “ubuntu”.\n    globallyEnabled  bool    (Optional) GloballyEnabled determines if this ControllerResource is required by all Shoot clusters.\n    reconcileTimeout  Kubernetes meta/v1.Duration     (Optional) ReconcileTimeout defines how long Gardener should wait for the resource reconciliation.\n    primary  bool    (Optional) Primary determines if the controller backed by this ControllerRegistration is responsible for the extension resource’s lifecycle. This field defaults to true. There must be exactly one primary controller for this kind/type combination. This field is immutable.\n    CoreDNS   (Appears on: SystemComponents)  CoreDNS contains the settings of the Core DNS components running in the data plane of the Shoot cluster.\n   Field Description      autoscaling  CoreDNSAutoscaling     (Optional) Autoscaling contains the settings related to autoscaling of the Core DNS components running in the data plane of the Shoot cluster.\n    rewriting  CoreDNSRewriting     (Optional) Rewriting contains the setting related to rewriting of requests, which are obviously incorrect due to the unnecessary application of the search path.\n    CoreDNSAutoscaling   (Appears on: CoreDNS)  CoreDNSAutoscaling contains the settings related to autoscaling of the Core DNS components running in the data plane of the Shoot cluster.\n   Field Description      mode  CoreDNSAutoscalingMode     The mode of the autoscaling to be used for the Core DNS components running in the data plane of the Shoot cluster. Supported values are horizontal and cluster-proportional.\n    CoreDNSAutoscalingMode (string alias)\n  (Appears on: CoreDNSAutoscaling)  CoreDNSAutoscalingMode is a type alias for the Core DNS autoscaling mode string.\nCoreDNSRewriting   (Appears on: CoreDNS)  CoreDNSRewriting contains the setting related to rewriting requests, which are obviously incorrect due to the unnecessary application of the search path.\n   Field Description      commonSuffixes  []string    (Optional) CommonSuffixes are expected to be the suffix of a fully qualified domain name. Each suffix should contain at least one or two dots (‘.’) to prevent accidental clashes.\n    DNS   (Appears on: ShootSpec)  DNS holds information about the provider, the hosted zone id and the domain.\n   Field Description      domain  string    (Optional) Domain is the external available domain of the Shoot cluster. This domain will be written into the kubeconfig that is handed out to end-users. This field is immutable.\n    providers  []DNSProvider     (Optional) Providers is a list of DNS providers that shall be enabled for this shoot cluster. Only relevant if not a default domain is used.\n    DNSIncludeExclude   (Appears on: DNSProvider, SeedDNSProvider)  DNSIncludeExclude contains information about which domains shall be included/excluded.\n   Field Description      include  []string    (Optional) Include is a list of domains that shall be included.\n    exclude  []string    (Optional) Exclude is a list of domains that shall be excluded.\n    DNSProvider   (Appears on: DNS)  DNSProvider contains information about a DNS provider.\n   Field Description      domains  DNSIncludeExclude     (Optional) Domains contains information about which domains shall be included/excluded for this provider.\n    primary  bool    (Optional) Primary indicates that this DNSProvider is used for shoot related domains.\n    secretName  string    (Optional) SecretName is a name of a secret containing credentials for the stated domain and the provider. When not specified, the Gardener will use the cloud provider credentials referenced by the Shoot and try to find respective credentials there (primary provider only). Specifying this field may override this behavior, i.e. forcing the Gardener to only look into the given secret.\n    type  string    (Optional) Type is the DNS provider type.\n    zones  DNSIncludeExclude     (Optional) Zones contains information about which hosted zones shall be included/excluded for this provider.\n    DataVolume   (Appears on: Worker)  DataVolume contains information about a data volume.\n   Field Description      name  string    Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    VolumeSize is the size of the volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    DeploymentRef   (Appears on: ControllerRegistrationDeployment)  DeploymentRef contains information about ControllerDeployment references.\n   Field Description      name  string    Name is the name of the ControllerDeployment that is being referred to.\n    ErrorCode (string alias)\n  (Appears on: Condition, LastError)  ErrorCode is a string alias.\nExpanderMode (string alias)\n  (Appears on: ClusterAutoscaler)  ExpanderMode is type used for Expander values\nExpirableVersion   (Appears on: KubernetesSettings, MachineImageVersion)  ExpirableVersion contains a version and an expiration date.\n   Field Description      version  string    Version is the version identifier.\n    expirationDate  Kubernetes meta/v1.Time     (Optional) ExpirationDate defines the time at which this version expires.\n    classification  VersionClassification     (Optional) Classification defines the state of a version (preview, supported, deprecated)\n    Extension   (Appears on: ShootSpec)  Extension contains type and provider information for Shoot extensions.\n   Field Description      type  string    Type is the type of the extension resource.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to extension resource.\n    disabled  bool    (Optional) Disabled allows to disable extensions that were marked as ‘globally enabled’ by Gardener administrators.\n    FailureTolerance   (Appears on: HighAvailability)  FailureTolerance describes information about failure tolerance level of a highly available resource.\n   Field Description      type  FailureToleranceType     Type specifies the type of failure that the highly available resource can tolerate\n    FailureToleranceType (string alias)\n  (Appears on: FailureTolerance)  FailureToleranceType specifies the type of failure that a highly available shoot control plane that can tolerate.\nGardener   (Appears on: SeedStatus, ShootStatus)  Gardener holds the information about the Gardener version that operated a resource.\n   Field Description      id  string    ID is the Docker container id of the Gardener which last acted on a resource.\n    name  string    Name is the hostname (pod name) of the Gardener which last acted on a resource.\n    version  string    Version is the version of the Gardener which last acted on a resource.\n    Hibernation   (Appears on: ShootSpec)  Hibernation contains information whether the Shoot is suspended or not.\n   Field Description      enabled  bool    (Optional) Enabled specifies whether the Shoot needs to be hibernated or not. If it is true, the Shoot’s desired state is to be hibernated. If it is false or nil, the Shoot’s desired state is to be awakened.\n    schedules  []HibernationSchedule     (Optional) Schedules determine the hibernation schedules.\n    HibernationSchedule   (Appears on: Hibernation)  HibernationSchedule determines the hibernation schedule of a Shoot. A Shoot will be regularly hibernated at each start time and will be woken up at each end time. Start or End can be omitted, though at least one of each has to be specified.\n   Field Description      start  string    (Optional) Start is a Cron spec at which time a Shoot will be hibernated.\n    end  string    (Optional) End is a Cron spec at which time a Shoot will be woken up.\n    location  string    (Optional) Location is the time location in which both start and and shall be evaluated.\n    HighAvailability   (Appears on: ControlPlane, SeedSpec)  HighAvailability specifies the configuration settings for high availability for a resource. Typical usages could be to configure HA for shoot control plane or for seed system components.\n   Field Description      failureTolerance  FailureTolerance     FailureTolerance holds information about failure tolerance level of a highly available resource.\n    HorizontalPodAutoscalerConfig   (Appears on: KubeControllerManagerConfig)  HorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      cpuInitializationPeriod  Kubernetes meta/v1.Duration     (Optional) The period after which a ready pod transition is considered to be the first.\n    downscaleStabilization  Kubernetes meta/v1.Duration     (Optional) The configurable window at which the controller will choose the highest recommendation for autoscaling.\n    initialReadinessDelay  Kubernetes meta/v1.Duration     (Optional) The configurable period at which the horizontal pod autoscaler considers a Pod “not yet ready” given that it’s unready and it has transitioned to unready during that time.\n    syncPeriod  Kubernetes meta/v1.Duration     (Optional) The period for syncing the number of pods in horizontal pod autoscaler.\n    tolerance  float64    (Optional) The minimum change (from 1.0) in the desired-to-actual metrics ratio for the horizontal pod autoscaler to consider scaling.\n    Ingress   (Appears on: SeedSpec)  Ingress configures the Ingress specific settings of the Seed cluster\n   Field Description      domain  string    Domain specifies the IngressDomain of the Seed cluster pointing to the ingress controller endpoint. It will be used to construct ingress URLs for system applications running in Shoot clusters. Once set this field is immutable.\n    controller  IngressController     Controller configures a Gardener managed Ingress Controller listening on the ingressDomain\n    IngressController   (Appears on: Ingress)  IngressController enables a Gardener managed Ingress Controller listening on the ingressDomain\n   Field Description      kind  string    Kind defines which kind of IngressController to use, for example nginx\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig specifies infrastructure specific configuration for the ingressController\n    KubeAPIServerConfig   (Appears on: Kubernetes)  KubeAPIServerConfig contains configuration settings for the kube-apiserver.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     admissionPlugins  []AdmissionPlugin     (Optional) AdmissionPlugins contains the list of user-defined admission plugins (additional to those managed by Gardener), and, if desired, the corresponding configuration.\n    apiAudiences  []string    (Optional) APIAudiences are the identifiers of the API. The service account token authenticator will validate that tokens used against the API are bound to at least one of these audiences. Defaults to [“kubernetes”].\n    auditConfig  AuditConfig     (Optional) AuditConfig contains configuration settings for the audit of the kube-apiserver.\n    enableBasicAuthentication  bool    (Optional) EnableBasicAuthentication defines whether basic authentication should be enabled for this cluster or not.\n    oidcConfig  OIDCConfig     (Optional) OIDCConfig contains configuration settings for the OIDC provider.\n    runtimeConfig  map[string]bool    (Optional) RuntimeConfig contains information about enabled or disabled APIs.\n    serviceAccountConfig  ServiceAccountConfig     (Optional) ServiceAccountConfig contains configuration settings for the service account handling of the kube-apiserver.\n    watchCacheSizes  WatchCacheSizes     (Optional) WatchCacheSizes contains configuration of the API server’s watch cache sizes. Configuring these flags might be useful for large-scale Shoot clusters with a lot of parallel update requests and a lot of watching controllers (e.g. large ManagedSeed clusters). When the API server’s watch cache’s capacity is too small to cope with the amount of update requests and watchers for a particular resource, it might happen that controller watches are permanently stopped with too old resource version errors. Starting from kubernetes v1.19, the API server’s watch cache size is adapted dynamically and setting the watch cache size flags will have no effect, except when setting it to 0 (which disables the watch cache).\n    requests  KubeAPIServerRequests     (Optional) Requests contains configuration for request-specific settings for the kube-apiserver.\n    enableAnonymousAuthentication  bool    (Optional) EnableAnonymousAuthentication defines whether anonymous requests to the secure port of the API server should be allowed (flag --anonymous-auth). See: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/\n    eventTTL  Kubernetes meta/v1.Duration     (Optional) EventTTL controls the amount of time to retain events. Defaults to 1h.\n    KubeAPIServerRequests   (Appears on: KubeAPIServerConfig)  KubeAPIServerRequests contains configuration for request-specific settings for the kube-apiserver.\n   Field Description      maxNonMutatingInflight  int32    (Optional) MaxNonMutatingInflight is the maximum number of non-mutating requests in flight at a given time. When the server exceeds this, it rejects requests.\n    maxMutatingInflight  int32    (Optional) MaxMutatingInflight is the maximum number of mutating requests in flight at a given time. When the server exceeds this, it rejects requests.\n    KubeControllerManagerConfig   (Appears on: Kubernetes)  KubeControllerManagerConfig contains configuration settings for the kube-controller-manager.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     horizontalPodAutoscaler  HorizontalPodAutoscalerConfig     (Optional) HorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager.\n    nodeCIDRMaskSize  int32    (Optional) NodeCIDRMaskSize defines the mask size for node cidr in cluster (default is 24). This field is immutable.\n    podEvictionTimeout  Kubernetes meta/v1.Duration     (Optional) PodEvictionTimeout defines the grace period for deleting pods on failed nodes. Defaults to 2m.\n    nodeMonitorGracePeriod  Kubernetes meta/v1.Duration     (Optional) NodeMonitorGracePeriod defines the grace period before an unresponsive node is marked unhealthy.\n    KubeProxyConfig   (Appears on: Kubernetes)  KubeProxyConfig contains configuration settings for the kube-proxy.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     mode  ProxyMode     (Optional) Mode specifies which proxy mode to use. defaults to IPTables.\n    enabled  bool    (Optional) Enabled indicates whether kube-proxy should be deployed or not. Depending on the networking extensions switching kube-proxy off might be rejected. Consulting the respective documentation of the used networking extension is recommended before using this field. defaults to true if not specified.\n    KubeSchedulerConfig   (Appears on: Kubernetes)  KubeSchedulerConfig contains configuration settings for the kube-scheduler.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     kubeMaxPDVols  string    (Optional) KubeMaxPDVols allows to configure the KUBE_MAX_PD_VOLS environment variable for the kube-scheduler. Please find more information here: https://kubernetes.io/docs/concepts/storage/storage-limits/#custom-limits Note that using this field is considered alpha-/experimental-level and is on your own risk. You should be aware of all the side-effects and consequences when changing it.\n    profile  SchedulingProfile     (Optional) Profile configures the scheduling profile for the cluster. If not specified, the used profile is “balanced” (provides the default kube-scheduler behavior).\n    KubeletConfig   (Appears on: Kubernetes, WorkerKubernetes)  KubeletConfig contains configuration settings for the kubelet.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     cpuCFSQuota  bool    (Optional) CPUCFSQuota allows you to disable/enable CPU throttling for Pods.\n    cpuManagerPolicy  string    (Optional) CPUManagerPolicy allows to set alternative CPU management policies (default: none).\n    evictionHard  KubeletConfigEviction     (Optional) EvictionHard describes a set of eviction thresholds (e.g. memory.available   evictionMaxPodGracePeriod  int32    (Optional) EvictionMaxPodGracePeriod describes the maximum allowed grace period (in seconds) to use when terminating pods in response to a soft eviction threshold being met. Default: 90\n    evictionMinimumReclaim  KubeletConfigEvictionMinimumReclaim     (Optional) EvictionMinimumReclaim configures the amount of resources below the configured eviction threshold that the kubelet attempts to reclaim whenever the kubelet observes resource pressure. Default: 0 for each resource\n    evictionPressureTransitionPeriod  Kubernetes meta/v1.Duration     (Optional) EvictionPressureTransitionPeriod is the duration for which the kubelet has to wait before transitioning out of an eviction pressure condition. Default: 4m0s\n    evictionSoft  KubeletConfigEviction     (Optional) EvictionSoft describes a set of eviction thresholds (e.g. memory.available   evictionSoftGracePeriod  KubeletConfigEvictionSoftGracePeriod     (Optional) EvictionSoftGracePeriod describes a set of eviction grace periods (e.g. memory.available=1m30s) that correspond to how long a soft eviction threshold must hold before triggering a Pod eviction. Default: memory.available: 1m30s nodefs.available: 1m30s nodefs.inodesFree: 1m30s imagefs.available: 1m30s imagefs.inodesFree: 1m30s\n    maxPods  int32    (Optional) MaxPods is the maximum number of Pods that are allowed by the Kubelet. Default: 110\n    podPidsLimit  int64    (Optional) PodPIDsLimit is the maximum number of process IDs per pod allowed by the kubelet.\n    imagePullProgressDeadline  Kubernetes meta/v1.Duration     (Optional) ImagePullProgressDeadline describes the time limit under which if no pulling progress is made, the image pulling will be cancelled. Default: 1m\n    failSwapOn  bool    (Optional) FailSwapOn makes the Kubelet fail to start if swap is enabled on the node. (default true).\n    kubeReserved  KubeletConfigReserved     (Optional) KubeReserved is the configuration for resources reserved for kubernetes node components (mainly kubelet and container runtime). When updating these values, be aware that cgroup resizes may not succeed on active worker nodes. Look for the NodeAllocatableEnforced event to determine if the configuration was applied. Default: cpu=80m,memory=1Gi,pid=20k\n    systemReserved  KubeletConfigReserved     (Optional) SystemReserved is the configuration for resources reserved for system processes not managed by kubernetes (e.g. journald). When updating these values, be aware that cgroup resizes may not succeed on active worker nodes. Look for the NodeAllocatableEnforced event to determine if the configuration was applied.\n    imageGCHighThresholdPercent  int32    (Optional) ImageGCHighThresholdPercent describes the percent of the disk usage which triggers image garbage collection. Default: 50\n    imageGCLowThresholdPercent  int32    (Optional) ImageGCLowThresholdPercent describes the percent of the disk to which garbage collection attempts to free. Default: 40\n    serializeImagePulls  bool    (Optional) SerializeImagePulls describes whether the images are pulled one at a time. Default: true\n    registryPullQPS  int32    (Optional) RegistryPullQPS is the limit of registry pulls per second. The value must not be a negative number. Setting it to 0 means no limit. Default: 5\n    registryBurst  int32    (Optional) RegistryBurst is the maximum size of bursty pulls, temporarily allows pulls to burst to this number, while still not exceeding registryPullQPS. The value must not be a negative number. Only used if registryPullQPS is greater than 0. Default: 10\n    seccompDefault  bool    (Optional) SeccompDefault enables the use of RuntimeDefault as the default seccomp profile for all workloads. This requires the corresponding SeccompDefault feature gate to be enabled as well. This field is only available for Kubernetes v1.25 or later.\n    KubeletConfigEviction   (Appears on: KubeletConfig)  KubeletConfigEviction contains kubelet eviction thresholds supporting either a resource.Quantity or a percentage based value.\n   Field Description      memoryAvailable  string    (Optional) MemoryAvailable is the threshold for the free memory on the host server.\n    imageFSAvailable  string    (Optional) ImageFSAvailable is the threshold for the free disk space in the imagefs filesystem (docker images and container writable layers).\n    imageFSInodesFree  string    (Optional) ImageFSInodesFree is the threshold for the available inodes in the imagefs filesystem.\n    nodeFSAvailable  string    (Optional) NodeFSAvailable is the threshold for the free disk space in the nodefs filesystem (docker volumes, logs, etc).\n    nodeFSInodesFree  string    (Optional) NodeFSInodesFree is the threshold for the available inodes in the nodefs filesystem.\n    KubeletConfigEvictionMinimumReclaim   (Appears on: KubeletConfig)  KubeletConfigEvictionMinimumReclaim contains configuration for the kubelet eviction minimum reclaim.\n   Field Description      memoryAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) MemoryAvailable is the threshold for the memory reclaim on the host server.\n    imageFSAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) ImageFSAvailable is the threshold for the disk space reclaim in the imagefs filesystem (docker images and container writable layers).\n    imageFSInodesFree  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) ImageFSInodesFree is the threshold for the inodes reclaim in the imagefs filesystem.\n    nodeFSAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) NodeFSAvailable is the threshold for the disk space reclaim in the nodefs filesystem (docker volumes, logs, etc).\n    nodeFSInodesFree  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) NodeFSInodesFree is the threshold for the inodes reclaim in the nodefs filesystem.\n    KubeletConfigEvictionSoftGracePeriod   (Appears on: KubeletConfig)  KubeletConfigEvictionSoftGracePeriod contains grace periods for kubelet eviction thresholds.\n   Field Description      memoryAvailable  Kubernetes meta/v1.Duration     (Optional) MemoryAvailable is the grace period for the MemoryAvailable eviction threshold.\n    imageFSAvailable  Kubernetes meta/v1.Duration     (Optional) ImageFSAvailable is the grace period for the ImageFSAvailable eviction threshold.\n    imageFSInodesFree  Kubernetes meta/v1.Duration     (Optional) ImageFSInodesFree is the grace period for the ImageFSInodesFree eviction threshold.\n    nodeFSAvailable  Kubernetes meta/v1.Duration     (Optional) NodeFSAvailable is the grace period for the NodeFSAvailable eviction threshold.\n    nodeFSInodesFree  Kubernetes meta/v1.Duration     (Optional) NodeFSInodesFree is the grace period for the NodeFSInodesFree eviction threshold.\n    KubeletConfigReserved   (Appears on: KubeletConfig)  KubeletConfigReserved contains reserved resources for daemons\n   Field Description      cpu  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) CPU is the reserved cpu.\n    memory  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) Memory is the reserved memory.\n    ephemeralStorage  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) EphemeralStorage is the reserved ephemeral-storage.\n    pid  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) PID is the reserved process-ids.\n    Kubernetes   (Appears on: ShootSpec)  Kubernetes contains the version and configuration variables for the Shoot control plane.\n   Field Description      allowPrivilegedContainers  bool    (Optional) AllowPrivilegedContainers indicates whether privileged containers are allowed in the Shoot. Defaults to true for Kubernetes versions below v1.25. Unusable for Kubernetes versions v1.25 and higher.\n    clusterAutoscaler  ClusterAutoscaler     (Optional) ClusterAutoscaler contains the configuration flags for the Kubernetes cluster autoscaler.\n    kubeAPIServer  KubeAPIServerConfig     (Optional) KubeAPIServer contains configuration settings for the kube-apiserver.\n    kubeControllerManager  KubeControllerManagerConfig     (Optional) KubeControllerManager contains configuration settings for the kube-controller-manager.\n    kubeScheduler  KubeSchedulerConfig     (Optional) KubeScheduler contains configuration settings for the kube-scheduler.\n    kubeProxy  KubeProxyConfig     (Optional) KubeProxy contains configuration settings for the kube-proxy.\n    kubelet  KubeletConfig     (Optional) Kubelet contains configuration settings for the kubelet.\n    version  string    Version is the semantic Kubernetes version to use for the Shoot cluster.\n    verticalPodAutoscaler  VerticalPodAutoscaler     (Optional) VerticalPodAutoscaler contains the configuration flags for the Kubernetes vertical pod autoscaler.\n    enableStaticTokenKubeconfig  bool    (Optional) EnableStaticTokenKubeconfig indicates whether static token kubeconfig secret should be present in garden cluster (default: true).\n    KubernetesConfig   (Appears on: KubeAPIServerConfig, KubeControllerManagerConfig, KubeProxyConfig, KubeSchedulerConfig, KubeletConfig)  KubernetesConfig contains common configuration fields for the control plane components.\n   Field Description      featureGates  map[string]bool    (Optional) FeatureGates contains information about enabled feature gates.\n    KubernetesDashboard   (Appears on: Addons)  KubernetesDashboard describes configuration values for the kubernetes-dashboard addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     authenticationMode  string    (Optional) AuthenticationMode defines the authentication mode for the kubernetes-dashboard.\n    KubernetesSettings   (Appears on: CloudProfileSpec)  KubernetesSettings contains constraints regarding allowed values of the ‘kubernetes’ block in the Shoot specification.\n   Field Description      versions  []ExpirableVersion     (Optional) Versions is the list of allowed Kubernetes versions with optional expiration dates for Shoot clusters.\n    LastError   (Appears on: BackupBucketStatus, BackupEntryStatus, ShootStatus)  LastError indicates the last occurred error for an operation on a resource.\n   Field Description      description  string    A human readable message indicating details about the last error.\n    taskID  string    (Optional) ID of the task which caused this last error\n    codes  []ErrorCode     (Optional) Well-defined error codes of the last error(s).\n    lastUpdateTime  Kubernetes meta/v1.Time     (Optional) Last time the error was reported\n    LastMaintenance   (Appears on: ShootStatus)  LastMaintenance holds information about a maintenance operation on the Shoot.\n   Field Description      operations  []string    A human-readable message containing details about the operations performed in the last maintenance.\n    triggeredTime  Kubernetes meta/v1.Time     TriggeredTime is the time when maintenance was triggered.\n    LastOperation   (Appears on: BackupBucketStatus, BackupEntryStatus, ShootStatus)  LastOperation indicates the type and the state of the last operation, along with a description message and a progress indicator.\n   Field Description      description  string    A human readable message indicating details about the last operation.\n    lastUpdateTime  Kubernetes meta/v1.Time     Last time the operation state transitioned from one to another.\n    progress  int32    The progress in percentage (0-100) of the last operation.\n    state  LastOperationState     Status of the last operation, one of Aborted, Processing, Succeeded, Error, Failed.\n    type  LastOperationType     Type of the last operation, one of Create, Reconcile, Delete.\n    LastOperationState (string alias)\n  (Appears on: LastOperation)  LastOperationState is a string alias.\nLastOperationType (string alias)\n  (Appears on: LastOperation)  LastOperationType is a string alias.\nMachine   (Appears on: Worker)  Machine contains information about the machine type and image.\n   Field Description      type  string    Type is the machine type of the worker group.\n    image  ShootMachineImage     (Optional) Image holds information about the machine image to use for all nodes of this pool. It will default to the latest version of the first image stated in the referenced CloudProfile if no value has been provided.\n    architecture  string    (Optional) Architecture is CPU architecture of machines in this worker pool.\n    MachineControllerManagerSettings   (Appears on: Worker)  MachineControllerManagerSettings contains configurations for different worker-pools. Eg. MachineDrainTimeout, MachineHealthTimeout.\n   Field Description      machineDrainTimeout  Kubernetes meta/v1.Duration     (Optional) MachineDrainTimeout is the period after which machine is forcefully deleted.\n    machineHealthTimeout  Kubernetes meta/v1.Duration     (Optional) MachineHealthTimeout is the period after which machine is declared failed.\n    machineCreationTimeout  Kubernetes meta/v1.Duration     (Optional) MachineCreationTimeout is the period after which creation of the machine is declared failed.\n    maxEvictRetries  int32    (Optional) MaxEvictRetries are the number of eviction retries on a pod after which drain is declared failed, and forceful deletion is triggered.\n    nodeConditions  []string    (Optional) NodeConditions are the set of conditions if set to true for the period of MachineHealthTimeout, machine will be declared failed.\n    MachineImage   (Appears on: CloudProfileSpec)  MachineImage defines the name and multiple versions of the machine image in any environment.\n   Field Description      name  string    Name is the name of the image.\n    versions  []MachineImageVersion     Versions contains versions, expiration dates and container runtimes of the machine image\n    MachineImageVersion   (Appears on: MachineImage)  MachineImageVersion is an expirable version with list of supported container runtimes and interfaces\n   Field Description      ExpirableVersion  ExpirableVersion      (Members of ExpirableVersion are embedded into this type.)     cri  []CRI     (Optional) CRI list of supported container runtime and interfaces supported by this version\n    architectures  []string    (Optional) Architectures is the list of CPU architectures of the machine image in this version.\n    MachineType   (Appears on: CloudProfileSpec)  MachineType contains certain properties of a machine type.\n   Field Description      cpu  k8s.io/apimachinery/pkg/api/resource.Quantity     CPU is the number of CPUs for this machine type.\n    gpu  k8s.io/apimachinery/pkg/api/resource.Quantity     GPU is the number of GPUs for this machine type.\n    memory  k8s.io/apimachinery/pkg/api/resource.Quantity     Memory is the amount of memory for this machine type.\n    name  string    Name is the name of the machine type.\n    storage  MachineTypeStorage     (Optional) Storage is the amount of storage associated with the root volume of this machine type.\n    usable  bool    (Optional) Usable defines if the machine type can be used for shoot clusters.\n    architecture  string    (Optional) Architecture is the CPU architecture of this machine type.\n    MachineTypeStorage   (Appears on: MachineType)  MachineTypeStorage is the amount of storage associated with the root volume of this machine type.\n   Field Description      class  string    Class is the class of the storage type.\n    size  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) StorageSize is the storage size.\n    type  string    Type is the type of the storage.\n    minSize  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) MinSize is the minimal supported storage size. This overrides any other common minimum size configuration from spec.volumeTypes[*].minSize.\n    Maintenance   (Appears on: ShootSpec)  Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n   Field Description      autoUpdate  MaintenanceAutoUpdate     (Optional) AutoUpdate contains information about which constraints should be automatically updated.\n    timeWindow  MaintenanceTimeWindow     (Optional) TimeWindow contains information about the time window for maintenance operations.\n    confineSpecUpdateRollout  bool    (Optional) ConfineSpecUpdateRollout prevents that changes/updates to the shoot specification will be rolled out immediately. Instead, they are rolled out during the shoot’s maintenance time window. There is one exception that will trigger an immediate roll out which is changes to the Spec.Hibernation.Enabled field.\n    MaintenanceAutoUpdate   (Appears on: Maintenance)  MaintenanceAutoUpdate contains information about which constraints should be automatically updated.\n   Field Description      kubernetesVersion  bool    KubernetesVersion indicates whether the patch Kubernetes version may be automatically updated (default: true).\n    machineImageVersion  bool    MachineImageVersion indicates whether the machine image version may be automatically updated (default: true).\n    MaintenanceTimeWindow   (Appears on: Maintenance)  MaintenanceTimeWindow contains information about the time window for maintenance operations.\n   Field Description      begin  string    Begin is the beginning of the time window in the format HHMMSS+ZONE, e.g. “220000+0100”. If not present, a random value will be computed.\n    end  string    End is the end of the time window in the format HHMMSS+ZONE, e.g. “220000+0100”. If not present, the value will be computed based on the “Begin” value.\n    Monitoring   (Appears on: ShootSpec)  Monitoring contains information about the monitoring configuration for the shoot.\n   Field Description      alerting  Alerting     (Optional) Alerting contains information about the alerting configuration for the shoot cluster.\n    NamedResourceReference   (Appears on: ShootSpec)  NamedResourceReference is a named reference to a resource.\n   Field Description      name  string    Name of the resource reference.\n    resourceRef  Kubernetes autoscaling/v1.CrossVersionObjectReference     ResourceRef is a reference to a resource.\n    Networking   (Appears on: ShootSpec)  Networking defines networking parameters for the shoot cluster.\n   Field Description      type  string    Type identifies the type of the networking plugin. This field is immutable.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to network resource.\n    pods  string    (Optional) Pods is the CIDR of the pod network. This field is immutable.\n    nodes  string    (Optional) Nodes is the CIDR of the entire node network. This field is immutable.\n    services  string    (Optional) Services is the CIDR of the service network. This field is immutable.\n    NginxIngress   (Appears on: Addons)  NginxIngress describes configuration values for the nginx-ingress addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     loadBalancerSourceRanges  []string    (Optional) LoadBalancerSourceRanges is list of allowed IP sources for NginxIngress\n    config  map[string]string    (Optional) Config contains custom configuration for the nginx-ingress-controller configuration. See https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/configmap.md#configuration-options\n    externalTrafficPolicy  Kubernetes core/v1.ServiceExternalTrafficPolicyType     (Optional) ExternalTrafficPolicy controls the .spec.externalTrafficPolicy value of the load balancer Service exposing the nginx-ingress. Defaults to Cluster.\n    NodeLocalDNS   (Appears on: SystemComponents)  NodeLocalDNS contains the settings of the node local DNS components running in the data plane of the Shoot cluster.\n   Field Description      enabled  bool    Enabled indicates whether node local DNS is enabled or not.\n    forceTCPToClusterDNS  bool    (Optional) ForceTCPToClusterDNS indicates whether the connection from the node local DNS to the cluster DNS (Core DNS) will be forced to TCP or not. Default, if unspecified, is to enforce TCP.\n    forceTCPToUpstreamDNS  bool    (Optional) ForceTCPToUpstreamDNS indicates whether the connection from the node local DNS to the upstream DNS (infrastructure DNS) will be forced to TCP or not. Default, if unspecified, is to enforce TCP.\n    OIDCConfig   (Appears on: KubeAPIServerConfig)  OIDCConfig contains configuration settings for the OIDC provider. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      caBundle  string    (Optional) If set, the OpenID server’s certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host’s root CA set will be used.\n    clientAuthentication  OpenIDConnectClientAuthentication     (Optional) ClientAuthentication can optionally contain client configuration used for kubeconfig generation.\n    clientID  string    (Optional) The client ID for the OpenID Connect client, must be set if oidc-issuer-url is set.\n    groupsClaim  string    (Optional) If provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This flag is experimental, please see the authentication documentation for further details.\n    groupsPrefix  string    (Optional) If provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies.\n    issuerURL  string    (Optional) The URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT).\n    requiredClaims  map[string]string    (Optional) key=value pairs that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value.\n    signingAlgs  []string    (Optional) List of allowed JOSE asymmetric signing algorithms. JWTs with a ‘alg’ header value not in this list will be rejected. Values are defined by RFC 7518 https://tools.ietf.org/html/rfc7518#section-3.1\n    usernameClaim  string    (Optional) The OpenID claim to use as the user name. Note that claims other than the default (‘sub’) is not guaranteed to be unique and immutable. This flag is experimental, please see the authentication documentation for further details. (default “sub”)\n    usernamePrefix  string    (Optional) If provided, all usernames will be prefixed with this value. If not provided, username claims other than ‘email’ are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value ‘-’.\n    OpenIDConnectClientAuthentication   (Appears on: OIDCConfig)  OpenIDConnectClientAuthentication contains configuration for OIDC clients.\n   Field Description      extraConfig  map[string]string    (Optional) Extra configuration added to kubeconfig’s auth-provider. Must not be any of idp-issuer-url, client-id, client-secret, idp-certificate-authority, idp-certificate-authority-data, id-token or refresh-token\n    secret  string    (Optional) The client Secret for the OpenID Connect client.\n    ProjectMember   (Appears on: ProjectSpec)  ProjectMember is a member of a project.\n   Field Description      Subject  Kubernetes rbac/v1.Subject      (Members of Subject are embedded into this type.) Subject is representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    role  string    Role represents the role of this member. IMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the roles list. TODO: Remove this field in favor of the roles list in v1.\n    roles  []string    (Optional) Roles represents the list of roles of this member.\n    ProjectPhase (string alias)\n  (Appears on: ProjectStatus)  ProjectPhase is a label for the condition of a project at the current time.\nProjectSpec   (Appears on: Project)  ProjectSpec is the specification of a Project.\n   Field Description      createdBy  Kubernetes rbac/v1.Subject     (Optional) CreatedBy is a subject representing a user name, an email address, or any other identifier of a user who created the project. This field is immutable.\n    description  string    (Optional) Description is a human-readable description of what the project is used for.\n    owner  Kubernetes rbac/v1.Subject     (Optional) Owner is a subject representing a user name, an email address, or any other identifier of a user owning the project. IMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the owner role. The only way to change the owner will be by moving the owner role. In this API version the only way to change the owner is to use this field. TODO: Remove this field in favor of the owner role in v1.\n    purpose  string    (Optional) Purpose is a human-readable explanation of the project’s purpose.\n    members  []ProjectMember     (Optional) Members is a list of subjects representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    namespace  string    (Optional) Namespace is the name of the namespace that has been created for the Project object. A nil value means that Gardener will determine the name of the namespace. This field is immutable.\n    tolerations  ProjectTolerations     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n    ProjectStatus   (Appears on: Project)  ProjectStatus holds the most recently observed status of the project.\n   Field Description      observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this project.\n    phase  ProjectPhase     Phase is the current phase of the project.\n    staleSinceTimestamp  Kubernetes meta/v1.Time     (Optional) StaleSinceTimestamp contains the timestamp when the project was first discovered to be stale/unused.\n    staleAutoDeleteTimestamp  Kubernetes meta/v1.Time     (Optional) StaleAutoDeleteTimestamp contains the timestamp when the project will be garbage-collected/automatically deleted because it’s stale/unused.\n    lastActivityTimestamp  Kubernetes meta/v1.Time     (Optional) LastActivityTimestamp contains the timestamp from the last activity performed in this project.\n    ProjectTolerations   (Appears on: ProjectSpec)  ProjectTolerations contains the tolerations for taints on seed clusters.\n   Field Description      defaults  []Toleration     (Optional) Defaults contains a list of tolerations that are added to the shoots in this project by default.\n    whitelist  []Toleration     (Optional) Whitelist contains a list of tolerations that are allowed to be added to the shoots in this project. Please note that this list may only be added by users having the spec-tolerations-whitelist verb for project resources.\n    Provider   (Appears on: ShootSpec)  Provider contains provider-specific information that are handed-over to the provider-specific extension controller.\n   Field Description      type  string    Type is the type of the provider. This field is immutable.\n    controlPlaneConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ControlPlaneConfig contains the provider-specific control plane config blob. Please look up the concrete definition in the documentation of your provider extension.\n    infrastructureConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureConfig contains the provider-specific infrastructure config blob. Please look up the concrete definition in the documentation of your provider extension.\n    workers  []Worker     Workers is a list of worker groups.\n    ProxyMode (string alias)\n  (Appears on: KubeProxyConfig)  ProxyMode available in Linux platform: ‘userspace’ (older, going to be EOL), ‘iptables’ (newer, faster), ‘ipvs’ (newest, better in performance and scalability). As of now only ‘iptables’ and ‘ipvs’ is supported by Gardener. In Linux platform, if the iptables proxy is selected, regardless of how, but the system’s kernel or iptables versions are insufficient, this always falls back to the userspace proxy. IPVS mode will be enabled when proxy mode is set to ‘ipvs’, and the fall back path is firstly iptables and then userspace.\nQuotaSpec   (Appears on: Quota)  QuotaSpec is the specification of a Quota.\n   Field Description      clusterLifetimeDays  int32    (Optional) ClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.\n    metrics  Kubernetes core/v1.ResourceList     Metrics is a list of resources which will be put under constraints.\n    scope  Kubernetes core/v1.ObjectReference     Scope is the scope of the Quota object, either ‘project’ or ‘secret’. This field is immutable.\n    Region   (Appears on: CloudProfileSpec)  Region contains certain properties of a region.\n   Field Description      name  string    Name is a region name.\n    zones  []AvailabilityZone     (Optional) Zones is a list of availability zones in this region.\n    labels  map[string]string    (Optional) Labels is an optional set of key-value pairs that contain certain administrator-controlled labels for this region. It can be used by Gardener administrators/operators to provide additional information about a region, e.g. wrt quality, reliability, access restrictions, etc.\n    ResourceWatchCacheSize   (Appears on: WatchCacheSizes)  ResourceWatchCacheSize contains configuration of the API server’s watch cache size for one specific resource.\n   Field Description      apiGroup  string    (Optional) APIGroup is the API group of the resource for which the watch cache size should be configured. An unset value is used to specify the legacy core API (e.g. for secrets).\n    resource  string    Resource is the name of the resource for which the watch cache size should be configured (in lowercase plural form, e.g. secrets).\n    size  int32    CacheSize specifies the watch cache size that should be configured for the specified resource.\n    SchedulingProfile (string alias)\n  (Appears on: KubeSchedulerConfig)  SchedulingProfile is a string alias used for scheduling profile values.\nSecretBindingProvider   (Appears on: SecretBinding)  SecretBindingProvider defines the provider type of the SecretBinding.\n   Field Description      type  string    Type is the type of the provider.\nFor backwards compatibility, the field can contain multiple providers separated by a comma. However the usage of single SecretBinding (hence Secret) for different cloud providers is strongly discouraged.\n    SeedBackup   (Appears on: SeedSpec)  SeedBackup contains the object store configuration for backups for shoot (currently only etcd).\n   Field Description      provider  string    Provider is a provider name. This field is immutable.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to BackupBucket resource.\n    region  string    (Optional) Region is a region name. This field is immutable.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a Secret object containing the cloud provider credentials for the object store where backups should be stored. It should have enough privileges to manipulate the objects as well as buckets.\n    SeedDNS   (Appears on: SeedSpec)  SeedDNS contains DNS-relevant information about this seed cluster.\n   Field Description      ingressDomain  string    (Optional) IngressDomain is the domain of the Seed cluster pointing to the ingress controller endpoint. It will be used to construct ingress URLs for system applications running in Shoot clusters. This field is immutable. This will be removed in the next API version and replaced by spec.ingress.domain.\n    provider  SeedDNSProvider     (Optional) Provider configures a DNSProvider\n    SeedDNSProvider   (Appears on: SeedDNS)  SeedDNSProvider configures a DNSProvider for Seeds\n   Field Description      type  string    Type describes the type of the dns-provider, for example aws-route53\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a Secret object containing cloud provider credentials used for registering external domains.\n    domains  DNSIncludeExclude     (Optional) Domains contains information about which domains shall be included/excluded for this provider.\n    zones  DNSIncludeExclude     (Optional) Zones contains information about which hosted zones shall be included/excluded for this provider.\n    SeedNetworks   (Appears on: SeedSpec)  SeedNetworks contains CIDRs for the pod, service and node networks of a Kubernetes cluster.\n   Field Description      nodes  string    (Optional) Nodes is the CIDR of the node network. This field is immutable.\n    pods  string    Pods is the CIDR of the pod network. This field is immutable.\n    services  string    Services is the CIDR of the service network. This field is immutable.\n    shootDefaults  ShootNetworks     (Optional) ShootDefaults contains the default networks CIDRs for shoots.\n    blockCIDRs  []string    (Optional) BlockCIDRs is a list of network addresses that should be blocked for shoot control plane components running in the seed cluster.\n    SeedProvider   (Appears on: SeedSpec)  SeedProvider defines the provider type and region for this Seed cluster.\n   Field Description      type  string    Type is the name of the provider.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to Seed resource.\n    region  string    Region is a name of a region.\n    SeedSelector   (Appears on: CloudProfileSpec, ShootSpec)  SeedSelector contains constraints for selecting seed to be usable for shoots using a profile\n   Field Description      LabelSelector  Kubernetes meta/v1.LabelSelector      (Members of LabelSelector are embedded into this type.) (Optional) LabelSelector is optional and can be used to select seeds by their label settings\n    providerTypes  []string    (Optional) Providers is optional and can be used by restricting seeds by their provider type. ‘*’ can be used to enable seeds regardless of their provider type.\n    SeedSettingDependencyWatchdog   (Appears on: SeedSettings)  SeedSettingDependencyWatchdog controls the dependency-watchdog settings for the seed.\n   Field Description      endpoint  SeedSettingDependencyWatchdogEndpoint     (Optional) Endpoint controls the endpoint settings for the dependency-watchdog for the seed.\n    probe  SeedSettingDependencyWatchdogProbe     (Optional) Probe controls the probe settings for the dependency-watchdog for the seed.\n    SeedSettingDependencyWatchdogEndpoint   (Appears on: SeedSettingDependencyWatchdog)  SeedSettingDependencyWatchdogEndpoint controls the endpoint settings for the dependency-watchdog for the seed.\n   Field Description      enabled  bool    Enabled controls whether the endpoint controller of the dependency-watchdog should be enabled. This controller helps to alleviate the delay where control plane components remain unavailable by finding the respective pods in CrashLoopBackoff status and restarting them once their dependants become ready and available again.\n    SeedSettingDependencyWatchdogProbe   (Appears on: SeedSettingDependencyWatchdog)  SeedSettingDependencyWatchdogProbe controls the probe settings for the dependency-watchdog for the seed.\n   Field Description      enabled  bool    Enabled controls whether the probe controller of the dependency-watchdog should be enabled. This controller scales down the kube-controller-manager of shoot clusters in case their respective kube-apiserver is not reachable via its external ingress in order to avoid melt-down situations.\n    SeedSettingExcessCapacityReservation   (Appears on: SeedSettings)  SeedSettingExcessCapacityReservation controls the excess capacity reservation for shoot control planes in the seed.\n   Field Description      enabled  bool    Enabled controls whether the excess capacity reservation should be enabled.\n    SeedSettingLoadBalancerServices   (Appears on: SeedSettings)  SeedSettingLoadBalancerServices controls certain settings for services of type load balancer that are created in the seed.\n   Field Description      annotations  map[string]string    (Optional) Annotations is a map of annotations that will be injected/merged into every load balancer service object.\n    SeedSettingOwnerChecks   (Appears on: SeedSettings)  SeedSettingOwnerChecks controls certain owner checks settings for shoots scheduled on this seed.\n   Field Description      enabled  bool    Enabled controls whether owner checks are enabled for shoots scheduled on this seed. It is enabled by default because it is a prerequisite for control plane migration.\n    SeedSettingScheduling   (Appears on: SeedSettings)  SeedSettingScheduling controls settings for scheduling decisions for the seed.\n   Field Description      visible  bool    Visible controls whether the gardener-scheduler shall consider this seed when scheduling shoots. Invisible seeds are not considered by the scheduler.\n    SeedSettingShootDNS   (Appears on: SeedSettings)  SeedSettingShootDNS controls the shoot DNS settings for the seed.\n   Field Description      enabled  bool    Enabled controls whether the DNS for shoot clusters should be enabled. When disabled then all shoots using the seed won’t get any DNS providers, DNS records, and no DNS extension controller is required to be installed here. This is useful for environments where DNS is not required.\n    SeedSettingVerticalPodAutoscaler   (Appears on: SeedSettings)  SeedSettingVerticalPodAutoscaler controls certain settings for the vertical pod autoscaler components deployed in the seed.\n   Field Description      enabled  bool    Enabled controls whether the VPA components shall be deployed into the garden namespace in the seed cluster. It is enabled by default because Gardener heavily relies on a VPA being deployed. You should only disable this if your seed cluster already has another, manually/custom managed VPA deployment.\n    SeedSettings   (Appears on: SeedSpec)  SeedSettings contains certain settings for this seed cluster.\n   Field Description      excessCapacityReservation  SeedSettingExcessCapacityReservation     (Optional) ExcessCapacityReservation controls the excess capacity reservation for shoot control planes in the seed.\n    scheduling  SeedSettingScheduling     (Optional) Scheduling controls settings for scheduling decisions for the seed.\n    shootDNS  SeedSettingShootDNS     (Optional) ShootDNS controls the shoot DNS settings for the seed. Deprecated: This field is deprecated and will be removed in a future version of Gardener. Do not use it.\n    loadBalancerServices  SeedSettingLoadBalancerServices     (Optional) LoadBalancerServices controls certain settings for services of type load balancer that are created in the seed.\n    verticalPodAutoscaler  SeedSettingVerticalPodAutoscaler     (Optional) VerticalPodAutoscaler controls certain settings for the vertical pod autoscaler components deployed in the seed.\n    ownerChecks  SeedSettingOwnerChecks     (Optional) SeedSettingOwnerChecks controls certain owner checks settings for shoots scheduled on this seed.\n    dependencyWatchdog  SeedSettingDependencyWatchdog     (Optional) DependencyWatchdog controls certain settings for the dependency-watchdog components deployed in the seed.\n    SeedSpec   (Appears on: Seed, SeedTemplate)  SeedSpec is the specification of a Seed.\n   Field Description      backup  SeedBackup     (Optional) Backup holds the object store configuration for the backups of shoot (currently only etcd). If it is not specified, then there won’t be any backups taken for shoots associated with this seed. If backup field is present in seed, then backups of the etcd from shoot control plane will be stored under the configured object store.\n    dns  SeedDNS     DNS contains DNS-relevant information about this seed cluster.\n    networks  SeedNetworks     Networks defines the pod, service and worker network of the Seed cluster.\n    provider  SeedProvider     Provider defines the provider type and region for this Seed cluster.\n    secretRef  Kubernetes core/v1.SecretReference     (Optional) SecretRef is a reference to a Secret object containing the Kubeconfig of the Kubernetes cluster to be registered as Seed.\n    taints  []SeedTaint     (Optional) Taints describes taints on the seed.\n    volume  SeedVolume     (Optional) Volume contains settings for persistentvolumes created in the seed cluster.\n    settings  SeedSettings     (Optional) Settings contains certain settings for this seed cluster.\n    ingress  Ingress     (Optional) Ingress configures Ingress specific settings of the Seed cluster. This field is immutable.\n    highAvailability  HighAvailability     (Optional) HighAvailability describes the high availability configuration for seed system components. A highly available seed will need at least 3 nodes or 3 availability zones (depending on the configured FailureTolerance of node or zone), allowing spreading of system components across the configured failure domain.\n    SeedStatus   (Appears on: Seed)  SeedStatus is the status of a Seed.\n   Field Description      gardener  Gardener     (Optional) Gardener holds information about the Gardener which last acted on the Shoot.\n    kubernetesVersion  string    (Optional) KubernetesVersion is the Kubernetes version of the seed cluster.\n    conditions  []Condition     (Optional) Conditions represents the latest available observations of a Seed’s current state.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Seed. It corresponds to the Seed’s generation, which is updated on mutation by the API Server.\n    clusterIdentity  string    (Optional) ClusterIdentity is the identity of the Seed cluster. This field is immutable.\n    capacity  Kubernetes core/v1.ResourceList     (Optional) Capacity represents the total resources of a seed.\n    allocatable  Kubernetes core/v1.ResourceList     (Optional) Allocatable represents the resources of a seed that are available for scheduling. Defaults to Capacity.\n    clientCertificateExpirationTimestamp  Kubernetes meta/v1.Time     (Optional) ClientCertificateExpirationTimestamp is the timestamp at which gardenlet’s client certificate expires.\n    SeedTaint   (Appears on: SeedSpec)  SeedTaint describes a taint on a seed.\n   Field Description      key  string    Key is the taint key to be applied to a seed.\n    value  string    (Optional) Value is the taint value corresponding to the taint key.\n    SeedTemplate   SeedTemplate is a template for creating a Seed object.\n   Field Description      metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  SeedSpec     (Optional) Specification of the desired behavior of the Seed.\n     backup  SeedBackup     (Optional) Backup holds the object store configuration for the backups of shoot (currently only etcd). If it is not specified, then there won’t be any backups taken for shoots associated with this seed. If backup field is present in seed, then backups of the etcd from shoot control plane will be stored under the configured object store.\n    dns  SeedDNS     DNS contains DNS-relevant information about this seed cluster.\n    networks  SeedNetworks     Networks defines the pod, service and worker network of the Seed cluster.\n    provider  SeedProvider     Provider defines the provider type and region for this Seed cluster.\n    secretRef  Kubernetes core/v1.SecretReference     (Optional) SecretRef is a reference to a Secret object containing the Kubeconfig of the Kubernetes cluster to be registered as Seed.\n    taints  []SeedTaint     (Optional) Taints describes taints on the seed.\n    volume  SeedVolume     (Optional) Volume contains settings for persistentvolumes created in the seed cluster.\n    settings  SeedSettings     (Optional) Settings contains certain settings for this seed cluster.\n    ingress  Ingress     (Optional) Ingress configures Ingress specific settings of the Seed cluster. This field is immutable.\n    highAvailability  HighAvailability     (Optional) HighAvailability describes the high availability configuration for seed system components. A highly available seed will need at least 3 nodes or 3 availability zones (depending on the configured FailureTolerance of node or zone), allowing spreading of system components across the configured failure domain.\n       SeedVolume   (Appears on: SeedSpec)  SeedVolume contains settings for persistentvolumes created in the seed cluster.\n   Field Description      minimumSize  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) MinimumSize defines the minimum size that should be used for PVCs in the seed.\n    providers  []SeedVolumeProvider     (Optional) Providers is a list of storage class provisioner types for the seed.\n    SeedVolumeProvider   (Appears on: SeedVolume)  SeedVolumeProvider is a storage class provisioner type.\n   Field Description      purpose  string    Purpose is the purpose of this provider.\n    name  string    Name is the name of the storage class provisioner type.\n    ServiceAccountConfig   (Appears on: KubeAPIServerConfig)  ServiceAccountConfig is the kube-apiserver configuration for service accounts.\n   Field Description      issuer  string    (Optional) Issuer is the identifier of the service account token issuer. The issuer will assert this identifier in “iss” claim of issued tokens. This value is used to generate new service account tokens. This value is a string or URI. Defaults to URI of the API server.\n    signingKeySecretName  Kubernetes core/v1.LocalObjectReference     (Optional) SigningKeySecret is a reference to a secret that contains an optional private key of the service account token issuer. The issuer will sign issued ID tokens with this private key. Only useful if service account tokens are also issued by another external system. Deprecated: This field is deprecated and will be removed in a future version of Gardener. Do not use it.\n    extendTokenExpiration  bool    (Optional) ExtendTokenExpiration turns on projected service account expiration extension during token generation, which helps safe transition from legacy token to bound service account token feature. If this flag is enabled, admission injected tokens would be extended up to 1 year to prevent unexpected failure during transition, ignoring value of service-account-max-token-expiration.\n    maxTokenExpiration  Kubernetes meta/v1.Duration     (Optional) MaxTokenExpiration is the maximum validity duration of a token created by the service account token issuer. If an otherwise valid TokenRequest with a validity duration larger than this value is requested, a token will be issued with a validity duration of this value. This field must be within [30d,90d].\n    acceptedIssuers  []string    (Optional) AcceptedIssuers is an additional set of issuers that are used to determine which service account tokens are accepted. These values are not used to generate new service account tokens. Only useful when service account tokens are also issued by another external system or a change of the current issuer that is used for generating tokens is being performed. This field is only available for Kubernetes v1.22 or later.\n    ShootAdvertisedAddress   (Appears on: ShootStatus)  ShootAdvertisedAddress contains information for the shoot’s Kube API server.\n   Field Description      name  string    Name of the advertised address. e.g. external\n    url  string    The URL of the API Server. e.g. https://api.foo.bar or https://1.2.3.4\n    ShootCARotation   (Appears on: ShootCredentialsRotation)  ShootCARotation contains information about the certificate authority credential rotation.\n   Field Description      phase  ShootCredentialsRotationPhase     Phase describes the phase of the certificate authority credential rotation.\n    lastInitiationTime  Kubernetes meta/v1.Time     (Optional) LastInitiationTime is the most recent time when the certificate authority credential rotation was initiated.\n    lastCompletionTime  Kubernetes meta/v1.Time     (Optional) LastCompletionTime is the most recent time when the certificate authority credential rotation was successfully completed.\n    ShootCredentials   (Appears on: ShootStatus)  ShootCredentials contains information about the shoot credentials.\n   Field Description      rotation  ShootCredentialsRotation     (Optional) Rotation contains information about the credential rotations.\n    ShootCredentialsRotation   (Appears on: ShootCredentials)  ShootCredentialsRotation contains information about the rotation of credentials.\n   Field Description      certificateAuthorities  ShootCARotation     (Optional) CertificateAuthorities contains information about the certificate authority credential rotation.\n    kubeconfig  ShootKubeconfigRotation     (Optional) Kubeconfig contains information about the kubeconfig credential rotation.\n    sshKeypair  ShootSSHKeypairRotation     (Optional) SSHKeypair contains information about the ssh-keypair credential rotation.\n    observability  ShootObservabilityRotation     (Optional) Observability contains information about the observability credential rotation.\n    serviceAccountKey  ShootServiceAccountKeyRotation     (Optional) ServiceAccountKey contains information about the service account key credential rotation.\n    etcdEncryptionKey  ShootETCDEncryptionKeyRotation     (Optional) ETCDEncryptionKey contains information about the ETCD encryption key credential rotation.\n    ShootCredentialsRotationPhase (string alias)\n  (Appears on: ShootCARotation, ShootETCDEncryptionKeyRotation, ShootServiceAccountKeyRotation)  ShootCredentialsRotationPhase is a string alias.\nShootETCDEncryptionKeyRotation   (Appears on: ShootCredentialsRotation)  ShootETCDEncryptionKeyRotation contains information about the ETCD encryption key credential rotation.\n   Field Description      phase  ShootCredentialsRotationPhase     Phase describes the phase of the ETCD encryption key credential rotation.\n    lastInitiationTime  Kubernetes meta/v1.Time     (Optional) LastInitiationTime is the most recent time when the ETCD encryption key credential rotation was initiated.\n    lastCompletionTime  Kubernetes meta/v1.Time     (Optional) LastCompletionTime is the most recent time when the ETCD encryption key credential rotation was successfully completed.\n    ShootKubeconfigRotation   (Appears on: ShootCredentialsRotation)  ShootKubeconfigRotation contains information about the kubeconfig credential rotation.\n   Field Description      lastInitiationTime  Kubernetes meta/v1.Time     (Optional) LastInitiationTime is the most recent time when the kubeconfig credential rotation was initiated.\n    lastCompletionTime  Kubernetes meta/v1.Time     (Optional) LastCompletionTime is the most recent time when the kubeconfig credential rotation was successfully completed.\n    ShootMachineImage   (Appears on: Machine)  ShootMachineImage defines the name and the version of the shoot’s machine image in any environment. Has to be defined in the respective CloudProfile.\n   Field Description      name  string    Name is the name of the image.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the shoot’s individual configuration passed to an extension resource.\n    version  string    (Optional) Version is the version of the shoot’s image. If version is not provided, it will be defaulted to the latest version from the CloudProfile.\n    ShootNetworks   (Appears on: SeedNetworks)  ShootNetworks contains the default networks CIDRs for shoots.\n   Field Description      pods  string    (Optional) Pods is the CIDR of the pod network.\n    services  string    (Optional) Services is the CIDR of the service network.\n    ShootObservabilityRotation   (Appears on: ShootCredentialsRotation)  ShootObservabilityRotation contains information about the observability credential rotation.\n   Field Description      lastInitiationTime  Kubernetes meta/v1.Time     (Optional) LastInitiationTime is the most recent time when the observability credential rotation was initiated.\n    lastCompletionTime  Kubernetes meta/v1.Time     (Optional) LastCompletionTime is the most recent time when the observability credential rotation was successfully completed.\n    ShootPurpose (string alias)\n  (Appears on: ShootSpec)  ShootPurpose is a type alias for string.\nShootSSHKeypairRotation   (Appears on: ShootCredentialsRotation)  ShootSSHKeypairRotation contains information about the ssh-keypair credential rotation.\n   Field Description      lastInitiationTime  Kubernetes meta/v1.Time     (Optional) LastInitiationTime is the most recent time when the ssh-keypair credential rotation was initiated.\n    lastCompletionTime  Kubernetes meta/v1.Time     (Optional) LastCompletionTime is the most recent time when the ssh-keypair credential rotation was successfully completed.\n    ShootServiceAccountKeyRotation   (Appears on: ShootCredentialsRotation)  ShootServiceAccountKeyRotation contains information about the service account key credential rotation.\n   Field Description      phase  ShootCredentialsRotationPhase     Phase describes the phase of the service account key credential rotation.\n    lastInitiationTime  Kubernetes meta/v1.Time     (Optional) LastInitiationTime is the most recent time when the service account key credential rotation was initiated.\n    lastCompletionTime  Kubernetes meta/v1.Time     (Optional) LastCompletionTime is the most recent time when the service account key credential rotation was successfully completed.\n    ShootSpec   (Appears on: Shoot, ShootTemplate)  ShootSpec is the specification of a Shoot.\n   Field Description      addons  Addons     (Optional) Addons contains information about enabled/disabled addons and their configuration.\n    cloudProfileName  string    CloudProfileName is a name of a CloudProfile object. This field is immutable.\n    dns  DNS     (Optional) DNS contains information about the DNS settings of the Shoot.\n    extensions  []Extension     (Optional) Extensions contain type and provider information for Shoot extensions.\n    hibernation  Hibernation     (Optional) Hibernation contains information whether the Shoot is suspended or not.\n    kubernetes  Kubernetes     Kubernetes contains the version and configuration settings of the control plane components.\n    networking  Networking     Networking contains information about cluster networking such as CNI Plugin type, CIDRs, …etc.\n    maintenance  Maintenance     (Optional) Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n    monitoring  Monitoring     (Optional) Monitoring contains information about custom monitoring configurations for the shoot.\n    provider  Provider     Provider contains all provider-specific and provider-relevant information.\n    purpose  ShootPurpose     (Optional) Purpose is the purpose class for this cluster.\n    region  string    Region is a name of a region. This field is immutable.\n    secretBindingName  string    SecretBindingName is the name of the a SecretBinding that has a reference to the provider secret. The credentials inside the provider secret will be used to create the shoot in the respective account. This field is immutable.\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot. This field is immutable when the SeedChange feature gate is disabled.\n    seedSelector  SeedSelector     (Optional) SeedSelector is an optional selector which must match a seed’s labels for the shoot to be scheduled on that seed.\n    resources  []NamedResourceReference     (Optional) Resources holds a list of named resource references that can be referred to in extension configs by their names.\n    tolerations  []Toleration     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n    exposureClassName  string    (Optional) ExposureClassName is the optional name of an exposure class to apply a control plane endpoint exposure strategy. This field is immutable.\n    systemComponents  SystemComponents     (Optional) SystemComponents contains the settings of system components in the control or data plane of the Shoot cluster.\n    controlPlane  ControlPlane     (Optional) ControlPlane contains general settings for the control plane of the shoot.\n    ShootStatus   (Appears on: Shoot)  ShootStatus holds the most recently observed status of the Shoot cluster.\n   Field Description      conditions  []Condition     (Optional) Conditions represents the latest available observations of a Shoots’s current state.\n    constraints  []Condition     (Optional) Constraints represents conditions of a Shoot’s current state that constraint some operations on it.\n    gardener  Gardener     Gardener holds information about the Gardener which last acted on the Shoot.\n    hibernated  bool    IsHibernated indicates whether the Shoot is currently hibernated.\n    lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the Shoot.\n    lastErrors  []LastError     (Optional) LastErrors holds information about the last occurred error(s) during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Shoot. It corresponds to the Shoot’s generation, which is updated on mutation by the API Server.\n    retryCycleStartTime  Kubernetes meta/v1.Time     (Optional) RetryCycleStartTime is the start time of the last retry cycle (used to determine how often an operation must be retried until we give up).\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot. This value is only written after a successful create/reconcile operation. It will be used when control planes are moved between Seeds.\n    technicalID  string    TechnicalID is the name that is used for creating the Seed namespace, the infrastructure resources, and basically everything that is related to this particular Shoot. This field is immutable.\n    uid  k8s.io/apimachinery/pkg/types.UID     UID is a unique identifier for the Shoot cluster to avoid portability between Kubernetes clusters. It is used to compute unique hashes. This field is immutable.\n    clusterIdentity  string    (Optional) ClusterIdentity is the identity of the Shoot cluster. This field is immutable.\n    advertisedAddresses  []ShootAdvertisedAddress     (Optional) List of addresses on which the Kube API server can be reached.\n    migrationStartTime  Kubernetes meta/v1.Time     (Optional) MigrationStartTime is the time when a migration to a different seed was initiated.\n    credentials  ShootCredentials     (Optional) Credentials contains information about the shoot credentials.\n    lastHibernationTriggerTime  Kubernetes meta/v1.Time     (Optional) LastHibernationTriggerTime indicates the last time when the hibernation controller managed to change the hibernation settings of the cluster\n    lastMaintenances  []LastMaintenance     (Optional) LastMaintenances holds information about the last maintenance operations on the Shoot.\n    ShootTemplate   ShootTemplate is a template for creating a Shoot object.\n   Field Description      metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ShootSpec     (Optional) Specification of the desired behavior of the Shoot.\n     addons  Addons     (Optional) Addons contains information about enabled/disabled addons and their configuration.\n    cloudProfileName  string    CloudProfileName is a name of a CloudProfile object. This field is immutable.\n    dns  DNS     (Optional) DNS contains information about the DNS settings of the Shoot.\n    extensions  []Extension     (Optional) Extensions contain type and provider information for Shoot extensions.\n    hibernation  Hibernation     (Optional) Hibernation contains information whether the Shoot is suspended or not.\n    kubernetes  Kubernetes     Kubernetes contains the version and configuration settings of the control plane components.\n    networking  Networking     Networking contains information about cluster networking such as CNI Plugin type, CIDRs, …etc.\n    maintenance  Maintenance     (Optional) Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n    monitoring  Monitoring     (Optional) Monitoring contains information about custom monitoring configurations for the shoot.\n    provider  Provider     Provider contains all provider-specific and provider-relevant information.\n    purpose  ShootPurpose     (Optional) Purpose is the purpose class for this cluster.\n    region  string    Region is a name of a region. This field is immutable.\n    secretBindingName  string    SecretBindingName is the name of the a SecretBinding that has a reference to the provider secret. The credentials inside the provider secret will be used to create the shoot in the respective account. This field is immutable.\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot. This field is immutable when the SeedChange feature gate is disabled.\n    seedSelector  SeedSelector     (Optional) SeedSelector is an optional selector which must match a seed’s labels for the shoot to be scheduled on that seed.\n    resources  []NamedResourceReference     (Optional) Resources holds a list of named resource references that can be referred to in extension configs by their names.\n    tolerations  []Toleration     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n    exposureClassName  string    (Optional) ExposureClassName is the optional name of an exposure class to apply a control plane endpoint exposure strategy. This field is immutable.\n    systemComponents  SystemComponents     (Optional) SystemComponents contains the settings of system components in the control or data plane of the Shoot cluster.\n    controlPlane  ControlPlane     (Optional) ControlPlane contains general settings for the control plane of the shoot.\n       SystemComponents   (Appears on: ShootSpec)  SystemComponents contains the settings of system components in the control or data plane of the Shoot cluster.\n   Field Description      coreDNS  CoreDNS     (Optional) CoreDNS contains the settings of the Core DNS components running in the data plane of the Shoot cluster.\n    nodeLocalDNS  NodeLocalDNS     (Optional) NodeLocalDNS contains the settings of the node local DNS components running in the data plane of the Shoot cluster.\n    Toleration   (Appears on: ProjectTolerations, ShootSpec)  Toleration is a toleration for a seed taint.\n   Field Description      key  string    Key is the toleration key to be applied to a project or shoot.\n    value  string    (Optional) Value is the toleration value corresponding to the toleration key.\n    VersionClassification (string alias)\n  (Appears on: ExpirableVersion)  VersionClassification is the logical state of a version.\nVerticalPodAutoscaler   (Appears on: Kubernetes)  VerticalPodAutoscaler contains the configuration flags for the Kubernetes vertical pod autoscaler.\n   Field Description      enabled  bool    Enabled specifies whether the Kubernetes VPA shall be enabled for the shoot cluster.\n    evictAfterOOMThreshold  Kubernetes meta/v1.Duration     (Optional) EvictAfterOOMThreshold defines the threshold that will lead to pod eviction in case it OOMed in less than the given threshold since its start and if it has only one container (default: 10m0s).\n    evictionRateBurst  int32    (Optional) EvictionRateBurst defines the burst of pods that can be evicted (default: 1)\n    evictionRateLimit  float64    (Optional) EvictionRateLimit defines the number of pods that can be evicted per second. A rate limit set to 0 or -1 will disable the rate limiter (default: -1).\n    evictionTolerance  float64    (Optional) EvictionTolerance defines the fraction of replica count that can be evicted for update in case more than one pod can be evicted (default: 0.5).\n    recommendationMarginFraction  float64    (Optional) RecommendationMarginFraction is the fraction of usage added as the safety margin to the recommended request (default: 0.15).\n    updaterInterval  Kubernetes meta/v1.Duration     (Optional) UpdaterInterval is the interval how often the updater should run (default: 1m0s).\n    recommenderInterval  Kubernetes meta/v1.Duration     (Optional) RecommenderInterval is the interval how often metrics should be fetched (default: 1m0s).\n    Volume   (Appears on: Worker)  Volume contains information about the volume type, size, and encryption.\n   Field Description      name  string    (Optional) Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    VolumeSize is the size of the volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    VolumeType   (Appears on: CloudProfileSpec)  VolumeType contains certain properties of a volume type.\n   Field Description      class  string    Class is the class of the volume type.\n    name  string    Name is the name of the volume type.\n    usable  bool    (Optional) Usable defines if the volume type can be used for shoot clusters.\n    minSize  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) MinSize is the minimal supported storage size.\n    WatchCacheSizes   (Appears on: KubeAPIServerConfig)  WatchCacheSizes contains configuration of the API server’s watch cache sizes.\n   Field Description      default  int32    (Optional) Default configures the default watch cache size of the kube-apiserver (flag --default-watch-cache-size, defaults to 100). See: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/\n    resources  []ResourceWatchCacheSize     (Optional) Resources configures the watch cache size of the kube-apiserver per resource (flag --watch-cache-sizes). See: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/\n    Worker   (Appears on: Provider)  Worker is the base definition of a worker group.\n   Field Description      annotations  map[string]string    (Optional) Annotations is a map of key/value pairs for annotations for all the Node objects in this worker pool.\n    caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every machine of this worker pool.\n    cri  CRI     (Optional) CRI contains configurations of CRI support of every machine in the worker pool. Defaults to a CRI with name containerd when the Kubernetes version of the Shoot is \u003e= 1.22.\n    kubernetes  WorkerKubernetes     (Optional) Kubernetes contains configuration for Kubernetes components related to this worker pool.\n    labels  map[string]string    (Optional) Labels is a map of key/value pairs for labels for all the Node objects in this worker pool.\n    name  string    Name is the name of the worker group.\n    machine  Machine     Machine contains information about the machine type and image.\n    maximum  int32    Maximum is the maximum number of VMs to create.\n    minimum  int32    Minimum is the minimum number of VMs to create.\n    maxSurge  k8s.io/apimachinery/pkg/util/intstr.IntOrString     (Optional) MaxSurge is maximum number of VMs that are created during an update.\n    maxUnavailable  k8s.io/apimachinery/pkg/util/intstr.IntOrString     (Optional) MaxUnavailable is the maximum number of VMs that can be unavailable during an update.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the provider-specific configuration for this worker pool.\n    taints  []Kubernetes core/v1.Taint     (Optional) Taints is a list of taints for all the Node objects in this worker pool.\n    volume  Volume     (Optional) Volume contains information about the volume type and size.\n    dataVolumes  []DataVolume     (Optional) DataVolumes contains a list of additional worker volumes.\n    kubeletDataVolumeName  string    (Optional) KubeletDataVolumeName contains the name of a dataVolume that should be used for storing kubelet state.\n    zones  []string    (Optional) Zones is a list of availability zones that are used to evenly distribute this worker pool. Optional as not every provider may support availability zones.\n    systemComponents  WorkerSystemComponents     (Optional) SystemComponents contains configuration for system components related to this worker pool\n    machineControllerManager  MachineControllerManagerSettings     (Optional) MachineControllerManagerSettings contains configurations for different worker-pools. Eg. MachineDrainTimeout, MachineHealthTimeout.\n    WorkerKubernetes   (Appears on: Worker)  WorkerKubernetes contains configuration for Kubernetes components related to this worker pool.\n   Field Description      kubelet  KubeletConfig     (Optional) Kubelet contains configuration settings for all kubelets of this worker pool. If set, all spec.kubernetes.kubelet settings will be overwritten for this worker pool (no merge of settings).\n    version  string    (Optional) Version is the semantic Kubernetes version to use for the Kubelet in this Worker Group. If not specified the kubelet version is derived from the global shoot cluster kubernetes version. version must be equal or lower than the version of the shoot kubernetes version. Only one minor version difference to other worker groups and global kubernetes version is allowed.\n    WorkerSystemComponents   (Appears on: Worker)  WorkerSystemComponents contains configuration for system components related to this worker pool\n   Field Description      allow  bool    Allow determines whether the pool should be allowed to host system components or not (defaults to true)\n      Generated with gen-crd-api-reference-docs \n","categories":"","description":"","excerpt":"Packages:\n  core.gardener.cloud/v1beta1   core.gardener.cloud/v1beta1 …","ref":"/docs/gardener/api-reference/core/","tags":"","title":"Core"},{"body":"Gardener Extension for CoreOS Container Linux  \nProject Gardener implements the automated management and operation of Kubernetes clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.\nRecently, most of the vendor specific logic has been developed in-tree. However, the project has grown to a size where it is very hard to extend, maintain, and test. With GEP-1 we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.\nThis controller operates on the OperatingSystemConfig resource in the extensions.gardener.cloud/v1alpha1 API group. It supports CoreOS Container Linux and Flatcar Container Linux (“a friendly fork of CoreOS Container Linux”).\nThe controller manages those objects that are requesting CoreOS Container Linux configuration (.spec.type=coreos) or Flatcar Container Linux configuration (.spec.type=flatcar):\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: OperatingSystemConfig metadata:  name: pool-01-original  namespace: default spec:  type: coreos  units:  ...  files:  ... Please find a concrete example in the example folder.\nAfter reconciliation the resulting data will be stored in a secret within the same namespace (as the config itself might contain confidential data). The name of the secret will be written into the resource’s .status field:\n... status:  ...  cloudConfig:  secretRef:  name: osc-result-pool-01-original  namespace: default  command: /usr/bin/coreos-cloudinit -from-file=\u003cpath\u003e  units:  - docker-monitor.service  - kubelet-monitor.service  - kubelet.service The secret has one data key cloud_config that stores the generation.\nAn example for a ControllerRegistration resource that can be used to register this controller to Gardener can be found here.\nPlease find more information regarding the extensibility concepts and a detailed proposal here.\n How to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the ./dev/kubeconfig file.\nStatic code checks and tests can be executed by running make verify. We are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io Gardener Extensions Golang library GEP-1 (Gardener Enhancement Proposal) on extensibility Extensibility API documentation  ","categories":"","description":"Gardener extension controller for the CoreOS/FlatCar Container Linux operating system","excerpt":"Gardener extension controller for the CoreOS/FlatCar Container Linux …","ref":"/docs/extensions/os-extensions/gardener-extension-os-coreos/","tags":"","title":"CoreOS/FlatCar OS"},{"body":"Create a Shoot Cluster As you have already prepared an example Shoot manifest in the steps described in the development documentation, please open another Terminal pane/window with the KUBECONFIG environment variable pointing to the Garden development cluster and send the manifest to the Kubernetes API server:\n$ kubectl apply -f your-shoot-aws.yaml You should see that the Gardener has immediately picked up your manifest and started to deploy the Shoot cluster.\nIn order to investigate what is happening in the Seed cluster, please download its proper Kubeconfig yourself (see next paragraph). The namespace of the Shoot cluster in the Seed cluster will look like that: shoot-johndoe-johndoe-1, whereas the first johndoe is your namespace in the Garden cluster (also called “project”) and the johndoe-1 suffix is the actual name of the Shoot cluster.\nTo connect to the newly created Shoot cluster, you must download its Kubeconfig as well. Please connect to the proper Seed cluster, navigate to the Shoot namespace, and download the Kubeconfig from the kubecfg secret in that namespace.\nDelete a Shoot Cluster In order to delete your cluster, you have to set an annotation confirming the deletion first, and trigger the deletion after that. You can use the prepared delete shoot script which takes the Shoot name as first parameter. The namespace can be specified by the second parameter, but it is optional. If you don’t state it, it defaults to your namespace (the username you are logged in with to your machine).\n$ ./hack/usage/delete shoot johndoe-1 johndoe ( hack bash script can be found here https://github.com/gardener/gardener/blob/master/hack/usage/delete)\nConfigure a Shoot cluster alert receiver The receiver of the Shoot alerts can be configured from the .spec.monitoring.alerting.emailReceivers section in the Shoot specification. The value of the field has to be a list of valid mail addresses.\nThe alerting for the Shoot clusters is handled by the Prometheus Alertmanager. The Alertmanager will be deployed next to the control plane when the Shoot resource specifies .spec.monitoring.alerting.emailReceivers and if a SMTP secret exists.\nIf the field gets removed then the Alertmanager will be also removed during the next reconcilation of the cluster. The opposite is also valid if the field is added to an existing cluster.\n","categories":"","description":"","excerpt":"Create a Shoot Cluster As you have already prepared an example Shoot …","ref":"/docs/guides/administer_shoots/create-delete-shoot/","tags":"","title":"Create / Delete a Shoot cluster"},{"body":"Overview Gardener allows you to create a Kubernetes cluster on different infrastructure providers. This tutorial will guide you through the process of creating a cluster on Alibaba Cloud.\nPrerequisites  You have created an Alibaba Cloud account. You have access to the Gardener dashboard and have permissions to create projects.  Steps   Go to the Gardener dashboard and create a project.\n To be able to add shoot clusters to this project, you must first create a technical user on Alibaba Cloud with sufficient permissions.\n   Choose Secrets, then the plus icon and select AliCloud.\n  To copy the policy for Alibaba Cloud from the Gardener dashboard, click on the help icon for Alibaba Cloud secrets, and choose copy .\n  Create a custom policy in Alibaba Cloud:\n  Log on to your Alibaba account and choose RAM \u003e Permissions \u003e Policies.\n  Enter the name of your policy.\n  Select Script.\n  Paste the policy that you copied from the Gardener dashboard to this custom policy.\n  Choose OK.\n    In the Alibaba Cloud console, create a new technical user:\n  Choose RAM \u003e Users.\n  Choose Create User.\n  Enter a logon and display name for your user.\n  Select Open API Access.\n  Choose OK.\n   After the user is created, AccessKeyId and AccessKeySecret are generated and displayed. Remember to save them. The AccessKey is used later to create secrets for Gardener.\n   Assign the policy you created to the technical user:\n  Choose RAM \u003e Permissions \u003e Grants.\n  Choose Grant Permission.\n  Select Alibaba Cloud Account.\n  Assign the policy you’ve created before to the technical user.\n    Create your secret.\n Type the name of your secret. Copy and paste the Access Key ID and Secret Access Key you saved when you created the technical user on Alibaba Cloud. Choose Add secret.    After completing these steps, you should see your newly created secret in the Infrastructure Secrets section.\n   To create a new cluster, choose Clusters and then the plus sign in the upper right corner.\n  In the Create Cluster section:\n  Select AliCloud in the Infrastructure tab.\n  Type the name of your cluster in the Cluster Details tab.\n  Choose the secret you created before in the Infrastructure Details tab.\n  Choose Create.\n    Wait for your cluster to get created.\n  Result After completing the steps in this tutorial, you will be able to see and download the kubeconfig of your cluster. With it you can create shoot clusters on Alibaba Cloud.  The size of persistent volumes in your shoot cluster must at least be 20 GiB large. If you choose smaller sizes in your Kubernetes PV definition, the allocation of cloud disk space on Alibaba Cloud fails.\n ","categories":"","description":"","excerpt":"Overview Gardener allows you to create a Kubernetes cluster on …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-alicloud/docs/tutorials/kubernetes-cluster-on-alicloud-with-gardener/kubernetes-cluster-on-alicloud-with-gardener/","tags":"","title":"Create a Kubernetes Cluster on Alibaba Cloud with Gardener"},{"body":"Overview Gardener allows you to create a Kubernetes cluster on different infrastructure providers. This tutorial will guide you through the process of creating a cluster on AWS.\nPrerequisites  You have created an AWS account. You have access to the Gardener dashboard and have permissions to create projects.  Steps   Go to the Gardener dashboard and create a Project.\n  Choose Secrets, then the plus icon and select AWS.\n  To copy the policy for AWS from the Gardener dashboard, click on the help icon for AWS secrets, and choose copy .\n  Create a new policy in AWS:\n  Choose Create policy.\n  Paste the policy that you copied from the Gardener dashboard to this custom policy.\n  Choose Next until you reach the Review section.\n  Fill in the name and description, then choose Create policy.\n    Create a new technical user in AWS:\n  Type in a username and select the access key credential type.\n  Choose Attach an existing policy.\n  Select GardenerAccess from the policy list.\n  Choose Next until you reach the Review section.\n   Note: After the user is created, Access key ID and Secret access key are generated and displayed. Remember to save them. The Access key ID is used later to create secrets for Gardener.\n   On the Gardener dashboard, choose Secrets and then the plus sign . Select AWS from the drop down menu to add a new AWS secret.\n  Create your secret.\n Type the name of your secret. Copy and paste the Access Key ID and Secret Access Key you saved when you created the technical user on AWS. Choose Add secret.    After completing these steps, you should see your newly created secret in the Infrastructure Secrets section.\n   To create a new cluster, choose Clusters and then the plus sign in the upper right corner.\n  In the Create Cluster section:\n Select AWS in the Infrastructure tab. Type the name of your cluster in the Cluster Details tab. Choose the secret you created before in the Infrastructure Details tab. Choose Create.    Wait for your cluster to get created.\n  Result After completing the steps in this tutorial, you will be able to see and download the kubeconfig of your cluster.\n","categories":"","description":"","excerpt":"Overview Gardener allows you to create a Kubernetes cluster on …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/tutorials/kubernetes-cluster-on-aws-with-gardener/kubernetes-cluster-on-aws-with-gardener/","tags":"","title":"Create a Kubernetes cluster on AWS with Gardener"},{"body":"Overview Gardener allows you to create a Kubernetes cluster on different infrastructure providers. This tutorial will guide you through the process of creating a cluster on Azure.\nPrerequisites  You have created an Azure account. You have access to the Gardener dashboard and have permissions to create projects. You have an Azure Service Principal assigned to your subscription.  Steps   Go to the Gardener dashboard and create a Project.\n  Get the properties of your Azure AD tenant, Subscription and Service Principal.\nBefore you can provision and access a Kubernetes cluster on Azure, you need to add the Azure service principal, AD tenant and subscription credentials in Gardener. Gardener needs the credentials to provision and operate the Azure infrastructure for your Kubernetes cluster.\nEnsure that the Azure service principal has the actions defined here within your Subscription assigned. If no fine-grained permission/actions are required then simply the buildin Contributor role can be assigned.\n  Tenant ID\nTo find your TenantID, follow this guide.\n  SubscriptionID\nTo find your SubscriptionID, search for and select Subscriptions. After that, copy the SubscriptionID from your subscription of choice.   Service Principal (SPN)\nA service principal consist of a ClientID (also called ApplicationID) and a Client Secret. For more information, see here. You need to obtain the:\n  Client ID\nAccess the Azure Portal and navigate to the Active Directory service. Within the service navigate to App registrations and select your service principal. Copy the ClientID you see there.\n  Client Secret\nSecrets for the Azure Account/Service Principal can be generated/rotated via the Azure Portal. After copying your ClientID, in the Detail view of your Service Principal navigate to Certificates \u0026 secrets. In the section, you can generate a new secret.\n      Choose Secrets, then the plus icon and select Azure.\n  Create your secret.\n Type the name of your secret. Copy and paste the TenantID, SubscriptionID and the Service Principal credentials (ClientID and ClientSecret). Choose Add secret.    After completing these steps, you should see your newly created secret in the Infrastructure Secrets section.\n   Register resource providers for your subscription.\n go to your azure dashboard go to subscriptions -\u003e \u003cyour_subscription\u003e pick resource providers from the sidebar register microsoft.Network register microsoft.Compute    To create a new cluster, choose Clusters and then the plus sign in the upper right corner.\n  In the Create Cluster section:\n Select Azure in the Infrastructure tab. Type the name of your cluster in the Cluster Details tab. Choose the secret you created before in the Infrastructure Details tab. Choose Create.    Wait for your cluster to get created.\n  Result After completing the steps in this tutorial, you will be able to see and download the kubeconfig of your cluster.\n","categories":"","description":"","excerpt":"Overview Gardener allows you to create a Kubernetes cluster on …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-azure/docs/tutorials/kubernetes-cluster-on-azure-with-gardener/kubernetes-cluster-on-azure-with-gardener/","tags":"","title":"Create a Kubernetes cluster on Azure with Gardener"},{"body":"Create a Shoot cluster into existing AWS VPC Gardener can create a new VPC, or use an existing one for your Shoot cluster. Depending on your needs you may want to create Shoot(s) into already created VPC. The tutorial describes how to create a Shoot cluster into existing AWS VPC. The steps are identical for Alicloud, Azure, and GCP. Please note that the existing VPC must be in the same region like the shoot cluster that you want to deploy into the VPC.\nTL;DR If .spec.provider.infrastructureConfig.networks.vpc.cidr is specified, Gardener will create a new VPC with the given CIDR block and respectively will delete it on Shoot deletion.\nIf .spec.provider.infrastructureConfig.networks.vpc.id is specified, Gardener will use the existing VPC and respectively won’t delete it on Shoot deletion.\n It’s not recommended to create a Shoot cluster into VPC that is managed by Gardener (that is created for another Shoot cluster). In this case the deletion of the initial Shoot cluster will fail to delete the VPC because there will be resources attached to it.\nGardener won’t delete any manually created (unmanaged) resources in your cloud provider account.\n 1. Configure AWS CLI The aws configure command is a convenient way to setup your AWS CLI. It will prompt you for your credentials and settings which will be used in the following AWS CLI invocations.\n$ aws configure AWS Access Key ID [None]: \u003cACCESS_KEY_ID\u003e AWS Secret Access Key [None]: \u003cSECRET_ACCESS_KEY\u003e Default region name [None]: \u003cDEFAULT_REGION\u003e Default output format [None]: \u003cDEFAULT_OUTPUT_FORMAT\u003e 2. Create VPC Create the VPC by running the following command:\n$ aws ec2 create-vpc --cidr-block \u003ccidr-block\u003e {  \"Vpc\": {  \"VpcId\": \"vpc-ff7bbf86\",  \"InstanceTenancy\": \"default\",  \"Tags\": [],  \"CidrBlockAssociations\": [  {  \"AssociationId\": \"vpc-cidr-assoc-6e42b505\",  \"CidrBlock\": \"10.0.0.0/16\",  \"CidrBlockState\": {  \"State\": \"associated\"  }  }  ],  \"Ipv6CidrBlockAssociationSet\": [],  \"State\": \"pending\",  \"DhcpOptionsId\": \"dopt-38f7a057\",  \"CidrBlock\": \"10.0.0.0/16\",  \"IsDefault\": false  } } Gardener requires the VPC to have enabled DNS support, i.e the attributes enableDnsSupport and enableDnsHostnames must be set to true. enableDnsSupport attribute is enabled by default, enableDnsHostnames - not. Set the enableDnsHostnames attribute to true:\n$ aws ec2 modify-vpc-attribute --vpc-id vpc-ff7bbf86 --enable-dns-hostnames 3. Create Internet Gateway Gardener also requires that an internet gateway is attached to the VPC. You can create one using:\n$ aws ec2 create-internet-gateway {  \"InternetGateway\": {  \"Tags\": [],  \"InternetGatewayId\": \"igw-c0a643a9\",  \"Attachments\": []  } } and attach it to the VPC using:\n$ aws ec2 attach-internet-gateway --internet-gateway-id igw-c0a643a9 --vpc-id vpc-ff7bbf86 4. Create the Shoot Prepare your Shoot manifest (you could check the example manifests). Please make sure that you choose the region in which you had created the VPC earlier (step 2). Also, put your VPC ID in the .spec.provider.infrastructureConfig.networks.vpc.id field:\nspec:  region: \u003caws-region-of-vpc\u003e  provider:  type: aws  infrastructureConfig:  apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  vpc:  id: vpc-ff7bbf86  # ... Apply your Shoot manifest.\n$ kubectl apply -f your-shoot-aws.yaml Ensure that the Shoot cluster is properly created.\n$ kubectl get shoot $SHOOT_NAME -n $SHOOT_NAMESPACE NAME CLOUDPROFILE VERSION SEED DOMAIN OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE \u003cSHOOT_NAME\u003e aws 1.15.0 aws \u003cSHOOT_DOMAIN\u003e Succeeded 100 True True True True 20m ","categories":"","description":"","excerpt":"Create a Shoot cluster into existing AWS VPC Gardener can create a new …","ref":"/docs/guides/administer_shoots/create-shoot-into-existing-aws-vpc/","tags":"","title":"Create a Shoot Cluster into Existing AWS VPC"},{"body":"Overview Gardener allows you to create a Kubernetes cluster on different infrastructure providers. This tutorial will guide you through the process of creating a cluster on GCP.\nPrerequisites  You have created a GCP account. You have access to the Gardener dashboard and have permissions to create projects.  Steps   Go to the Gardener dashboard and create a Project.\n  Check which roles are required by Gardener.\n  Choose Secrets, then the plus icon and select GCP.\n  Click on the help button .\n    Create a service account with the correct roles in GCP:\n  Create a new service account in GCP.\n  Enter the name and description of your service account.\n  Assign the roles required by Gardener.\n  Choose Done.\n    Create a key for your service:\n  Locate your service account, then choose Actions and Manage keys.\n  Choose Add Key, then Create new key.\n  Save the private key of the service account in JSON format.\n Note: Save the key of the user, it’s used later to create secrets for Gardener.\n     Enable the Google Compute API by following these steps.\n When you are finished, you should see the following page:\n   Enable the Google IAM API by following these steps.\n When you are finished, you should see the following page:\n   On the Gardener dashboard, choose Secrets and then the plus sign . Select GCP from the drop down menu to add a new GCP secret.\n  Create your secret.\n Type the name of your secret. Select your Cloud Profile. Copy and paste the contents of the .JSON file you saved when you created the secret key on GCP. Choose Add secret.    After completing these steps, you should see your newly created secret in the Infrastructure Secrets section.\n   To create a new cluster, choose Clusters and then the plus sign in the upper right corner.\n  In the Create Cluster section:\n Select GCP in the Infrastructure tab. Type the name of your cluster in the Cluster Details tab. Choose the secret you created before in the Infrastructure Details tab. Choose Create.    Wait for your cluster to get created.\n  Result After completing the steps in this tutorial, you will be able to see and download the kubeconfig of your cluster.\n","categories":"","description":"","excerpt":"Overview Gardener allows you to create a Kubernetes cluster on …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-gcp/docs/tutorials/kubernetes-cluster-on-gcp-with-gardener/kubernetes-cluster-on-gcp-with-gardener/","tags":"","title":"Create a Кubernetes cluster on GCP with Gardener"},{"body":"(Custom) CSI Components Some provider extensions for Gardener are using CSI components to manage persistent volumes in the shoot clusters. Additionally, most of the provider extensions are deploying controllers for taking volume snapshots (CSI snapshotter).\nEnd-users can deploy their own CSI components and controllers into shoot clusters. In such situations, there are multiple controllers acting on the VolumeSnapshot custom resources (each responsible for those instances associated with their respective driver provisioner types).\nHowever, this might lead to operational conflicts that cannot be overcome by Gardener alone. Concretely, Gardener cannot know which custom CSI components were installed by end-users which can lead to issues, especially during shoot cluster deletion. You can add a label to your custom CSI components indicating that Gardener should not try to remove them during shoot cluster deletion. This means you have to take care of the lifecycle for these components yourself!\nRecommendations Custom CSI components are typically regular Deployments running in the shoot clusters.\nPlease label them with the shoot.gardener.cloud/no-cleanup=true label.\nBackground Information When a shoot cluster is deleted, Gardener deletes most Kubernetes resources (Deployments, DaemonSets, StatefulSets, etc.). Gardener will also try to delete CSI components if they are not marked with the above mentioned label.\nThis can result in VolumeSnapshot resources still having finalizers that will never be cleaned up. Consequently, manual intervention is required to clean them up before the cluster deletion can continue.\n","categories":"","description":"","excerpt":"(Custom) CSI Components Some provider extensions for Gardener are …","ref":"/docs/gardener/usage/csi_components/","tags":"","title":"CSI Components"},{"body":"Custom containerd Configuration In case a Shoot cluster uses containerd (see this document) for more information), it is possible to make the containerd process load custom configuration files. Gardener initializes contaienerd with the following statement:\nimports = [\"/etc/containerd/conf.d/*.toml\"] This means that all *.toml files in the /etc/containerd/conf.d directory will be imported and merged with the default configuration. Please consult the upstream containerd documentation for more information.\n ⚠️ Note that this only applies to nodes which were newly created after gardener/gardener@v1.51 was deployed. Existing nodes are not affected.\n ","categories":"","description":"","excerpt":"Custom containerd Configuration In case a Shoot cluster uses …","ref":"/docs/gardener/usage/custom-containerd-config/","tags":"","title":"Custom containerd Configuration"},{"body":"Custom DNS Configuration Gardener provides Kubernetes-Clusters-As-A-Service where all the system components (e.g., kube-proxy, networking, dns, …) are managed. As a result, Gardener needs to ensure and auto-correct additional configuration to those system components to avoid unnecessary down-time.\nIn some cases, auto-correcting system components can prevent users from deploying applications on top of the cluster that requires bits of customization, DNS configuration can be a good example.\nTo allow for customizations for DNS configuration (that could potentially lead to downtime) while having the option to “undo”, we utilize the import plugin from CoreDNS [1]. which enables in-line configuration changes.\nHow to use To customize your CoreDNS cluster config, you can simply edit a ConfigMap named coredns-custom in the kube-system namespace. By editing, this ConfigMap, you are modifying CoreDNS configuration, therefore care is advised.\nFor example, to apply new config to CoreDNS that would point all .global DNS requests to another DNS pod, simply edit the configuration as follows:\napiVersion: v1 kind: ConfigMap metadata:  name: coredns-custom  namespace: kube-system data:  istio.server: |global:8053 { errors cache 30 forward . 1.2.3.4 }  corefile.override: |# \u003csome-plugin\u003e \u003csome-plugin-config\u003e debug whoami It is important to have the ConfigMap keys ending with *.server (if you would like to add a new server) or *.override if you want to customize the current server configuration (it is optional setting both).\n[Optional] Reload CoreDNS As Gardener is configuring the reload plugin of CoreDNS a restart of the CoreDNS components is typically not necessary to propagate ConfigMap changes. However, if you don’t want to wait for the default (30s) to kick in, you can roll-out your CoreDNS deployment using:\nkubectl -n kube-system rollout restart deploy coredns This will reload the config into CoreDNS.\nThe approach we follow here was inspired by AKS’s approach [2].\nAnti-Pattern Applying a configuration that is in-compatible with the running version of CoreDNS is an anti-pattern (sometimes plugin configuration changes, simply applying a configuration can break DNS).\nIf incompatible changes are applied by mistake, simply delete the content of the ConfigMap and re-apply. This should bring the cluster DNS back to functioning state.\nReferences [1] Import plugin [2] AKS Custom DNS\n","categories":"","description":"","excerpt":"Custom DNS Configuration Gardener provides …","ref":"/docs/gardener/usage/custom-dns-config/","tags":"","title":"Custom DNS Configuration"},{"body":"Custom Shoot Fields The Dashboard supports custom shoot fields, that can be defined per project by specifying metadata.annotations[\"dashboard.gardener.cloud/shootCustomFields\"]. The fields can be configured to be displayed on the cluster list and cluster details page. Custom fields do not show up on the ALL_PROJECTS page.\n   Property Type Default Required Description     name String  ✔️ Name of the custom field   path String  ✔️ Path in shoot resource, of which the value must be of primitive type (no object / array). Use lodash get path syntax, e.g. metadata.labels[\"shoot.gardener.cloud/status\"] or spec.networking.type   icon String   MDI icon for field on the cluster details page. See https://materialdesignicons.com/ for available icons. Must be in the format: mdi-\u003cicon-name\u003e.   tooltip String   Tooltip for the custom field that appears when hovering with the mouse over the value   defaultValue String/Number   Default value, in case there is no value for the given path   showColumn Bool true  Field shall appear as column in the cluster list   columnSelectedByDefault Bool true  Indicates if field shall be selected by default on the cluster list (not hidden by default)   weight Number 0  Defines the order of the column. The standard columns start with weight 100 and continue in 100 increments (200, 300, ..)   sortable Bool true  Indicates if column is sortable on the cluster list.   searchable Bool true  Field shall appear in a dedicated card (Custom Fields) on the cluster details page   showDetails Bool true  Indicates if field shall appear in a dedicated card (Custom Fields) on the cluster details page    As there is currently no way to configure the custom shoot fields for a project in the gardener dashboard, you have to use kubectl to update the project resource. See /docs/dashboard/usage/project-operations/#download-kubeconfig-for-a-user on how to get a kubeconfig for the garden cluster in order to edit the project.\nThe following is an example project yaml:\napiVersion: core.gardener.cloud/v1beta1 kind: Project metadata:  annotations:  dashboard.gardener.cloud/shootCustomFields: |{ \"shootStatus\": { \"name\": \"Shoot Status\", \"path\": \"metadata.labels[\\\"shoot.gardener.cloud/status\\\"]\", \"icon\": \"mdi-heart-pulse\", \"tooltip\": \"Indicates the health status of the cluster\", \"defaultValue\": \"unknown\", \"showColumn\": true, \"columnSelectedByDefault\": true, \"weight\": 950, \"searchable\": true, \"sortable\": true, \"showDetails\": true }, \"networking\": { \"name\": \"Networking Type\", \"path\": \"spec.networking.type\", \"icon\": \"mdi-table-network\", \"showColumn\": false } } ","categories":"","description":"","excerpt":"Custom Shoot Fields The Dashboard supports custom shoot fields, that …","ref":"/docs/dashboard/usage/custom-fields/","tags":"","title":"Custom Fields"},{"body":"Custom Seccomp profile Context Seccomp (secure computing mode) is a security facility in the Linux kernel for restricting the set of system calls applications can make.\nStarting from Kubernetes v1.3.0 the Seccomp feature is in Alpha. To configure it on a Pod, the following annotations can be used:\n seccomp.security.alpha.kubernetes.io/pod: \u003cseccomp-profile\u003e where \u003cseccomp-profile\u003e is the seccomp profile to apply to all containers in a Pod. container.seccomp.security.alpha.kubernetes.io/\u003ccontainer-name\u003e: \u003cseccomp-profile\u003e where \u003cseccomp-profile\u003e is the seccomp profile to apply to \u003ccontainer-name\u003e in a Pod.  More details can be found in the PodSecurityPolicy documentation.\nInstallation of custom profile By default, kubelet loads custom Seccomp profiles from /var/lib/kubelet/seccomp/. There are two ways in which Seccomp profiles can be added to a Node:\n to be baked in the machine image to be added at runtime.  This guide focuses on creating those profiles via a DaemonSet.\nCreate a file called seccomp-profile.yaml with the following content:\napiVersion: v1 kind: ConfigMap metadata:  name: seccomp-profile  namespace: kube-system data:  my-profile.json: |{ \"defaultAction\": \"SCMP_ACT_ALLOW\", \"syscalls\": [ { \"name\": \"chmod\", \"action\": \"SCMP_ACT_ERRNO\" } ] }  The policy above is a very simple one and not siutable for complex applications. The default docker profile can be used a reference. Feel free to modify it to your needs.\n Apply the ConfigMap in your cluster:\n$ kubectl apply -f seccomp-profile.yaml configmap/seccomp-profile created The next steps is to create the DaemonSet seccomp installer. It’s going to copy the policy from above in /var/lib/kubelet/seccomp/my-profile.json.\nCreate a file called seccomp-installer.yaml with the following content:\napiVersion: apps/v1 kind: DaemonSet metadata:  name: seccomp  namespace: kube-system  labels:  security: seccomp spec:  selector:  matchLabels:  security: seccomp  template:  metadata:  labels:  security: seccomp  spec:  initContainers:  - name: installer  image: alpine:3.10.0  command: [\"/bin/sh\", \"-c\", \"cp -r -L /seccomp/*.json /host/seccomp/\"]  volumeMounts:  - name: profiles  mountPath: /seccomp  - name: hostseccomp  mountPath: /host/seccomp  readOnly: false  containers:  - name: pause  image: k8s.gcr.io/pause:3.1  terminationGracePeriodSeconds: 5  volumes:  - name: hostseccomp  hostPath:  path: /var/lib/kubelet/seccomp  - name: profiles  configMap:  name: seccomp-profile Create the installer and wait until it’s ready on all Nodes:\n$ kubectl apply -f seccomp-installer.yaml daemonset.apps/seccomp-installer created $ kubectl -n kube-system get pods -l security=seccomp NAME READY STATUS RESTARTS AGE seccomp-installer-wjbxq 1/1 Running 0 21s Create a Pod using custom Seccomp profile Finally we want to create a profile which uses our new Seccomp profile my-profile.json.\nCreate a file called my-seccomp-pod.yaml with the following content:\napiVersion: v1 kind: Pod metadata:  name: seccomp-app  namespace: default  annotations:  seccomp.security.alpha.kubernetes.io/pod: \"localhost/my-profile.json\"  # you can specify seccomp profile per container. If you add another profile you can configure  # it for a specific container - 'pause' in this case.  # container.seccomp.security.alpha.kubernetes.io/pause: \"localhost/some-other-profile.json\" spec:  containers:  - name: pause  image: k8s.gcr.io/pause:3.1 Create the Pod and see that’s running:\n$ kubectl apply -f my-seccomp-pod.yaml pod/seccomp-app created $ kubectl get pod seccomp-app NAME READY STATUS RESTARTS AGE seccomp-app 1/1 Running 0 42s Throubleshooting If an invalid or not existing profile is used then the Pod will be stuck in ContainerCreating phase:\nbroken-seccomp-pod.yaml:\napiVersion: v1 kind: Pod metadata:  name: broken-seccomp  namespace: default  annotations:  seccomp.security.alpha.kubernetes.io/pod: \"localhost/not-existing-profile.json\" spec:  containers:  - name: pause  image: k8s.gcr.io/pause:3.1 $ kubectl apply -f broken-seccomp-pod.yaml pod/broken-seccomp created $ kubectl get pod broken-seccomp NAME READY STATUS RESTARTS AGE broken-seccomp 1/1 ContainerCreating 0 2m $ kubectl describe pod broken-seccomp Name: broken-seccomp Namespace: default .... Events:  Type Reason Age From Message  ---- ------ ---- ---- -------  Normal Scheduled 18s default-scheduler Successfully assigned kube-system/broken-seccomp to docker-desktop  Warning FailedCreatePodSandBox 4s (x2 over 18s) kubelet, docker-desktop Failed create pod sandbox: rpc error: code = Unknown desc = failed to make sandbox docker config for pod \"broken-seccomp\": failed to generate sandbox security options for sandbox \"broken-seccomp\": failed to generate seccomp security options for container: cannot load seccomp profile \"/var/lib/kubelet/seccomp/not-existing-profile.json\": open /var/lib/kubelet/seccomp/not-existing-profile.json: no such file or directory Further reading  https://en.wikipedia.org/wiki/Seccomp https://docs.docker.com/engine/security/seccomp https://lwn.net/Articles/656307/ http://man7.org/conf/lpc2015/limiting_kernel_attack_surface_with_seccomp-LPC_2015-Kerrisk.pdf  ","categories":"","description":"","excerpt":"Custom Seccomp profile Context Seccomp (secure computing mode) is a …","ref":"/docs/guides/applications/secure-seccomp/","tags":"","title":"Custom Seccomp Profile"},{"body":"Default Seccomp Profile and Configuration This is a short guide describing how to enable the defaulting of seccomp profiles for Gardener managed workloads in the seed.\nDefault Kubernetes Behavior The state of Kubernetes in versions \u003c 1.25 is such that all workloads by default run in Unconfined (seccomp disabled) mode. This is undesirable since this is the least restrictive profile. Also mind that any privileged container will always run as Unconfined. More information about seccomp can be found in this Kubernetes tutorial.\nSetting the Seccomp Profile to RuntimeDefault for seed clusters To address the above issue, Gardener provides a webhook that is capable of mutating pods in the seed clusters, explicitly providing them with a seccomp profile type of RuntimeDefault. This profile is defined by the container runtime and represents a set of default syscalls that are allowed or not.\nspec:  securityContext:  seccompProfile:  type: RuntimeDefault A Pod is mutated when all of the following preconditions are fulfilled:\n The Pod is created in Gardener managed namespace. The Pod is NOT labeled with seccompprofile.resources.gardener.cloud/skip. The Pod does NOT explicitly specify .spec.securityContext.seccompProfile.type.  How to Configure To enable this feature, the Gardenlet DefaultSeccompProfile feature gate must be set to true.\nfeatureGates:  DefaultSeccompProfile: true Please refer to the examples here for more information.\nOnce the feature gate is enabled, the webhook will be registered and configured for the seed cluster. Newly created pods will be mutated to have their seccomp profile set to RuntimeDefault.\n Please note that this feature is still in Alpha, so you might see instabilities every now and then.\n Setting the Seccomp Profile to RuntimeDefault for shoot clusters For kubernetes shoot versions \u003e= 1.25 you can enable the use of RuntimeDefault as the default seccomp profile for all workloads. If enabled, the kubelet will use the RuntimeDefault seccomp profile by default, which is defined by the container runtime, instead of using the Unconfined mode. More information for this feature can be found in the Kubernetes documentation\nTo use seccomp profile defaulting, you must run the kubelet with the SeccompDefault feature gate enabled (this is the default for k8s versions \u003e= 1.25).\nHow to Configure To enable this feature, the kubelet seccompDefault configuration parameter must be set to true in the shoot’s spec.\nspec:  kubernetes:  version: 1.25.0  kubelet:  seccompDefault: true Please refer to the examples here for more information.\n","categories":"","description":"","excerpt":"Default Seccomp Profile and Configuration This is a short guide …","ref":"/docs/gardener/usage/default_seccomp_profile/","tags":"","title":"Default Seccomp Profile"},{"body":"Testing We follow the BDD-style testing principles and are leveraging the Ginkgo framework along with Gomega as matcher library. In order to execute the existing tests, you can use\nmake test # runs tests make verify # runs static code checks and test There is an additional command for analyzing the code coverage of the tests. Ginkgo will generate standard Golang cover profiles which will be translated into a HTML file by the Go Cover Tool. Another command helps you to clean up the filesystem from the temporary cover profile files and the HTML report:\nmake test-cov open gardener.coverage.html make test-cov-clean sigs.k8s.io/controller-runtime env test Some of the integration tests in Gardener are using the sigs.k8s.io/controller-runtime/pkg/envtest package. It sets up a temporary control plane (etcd + kube-apiserver) against the integration tests can run. The test and test-cov rules in the Makefile prepare this env test automatically by downloading the respective binaries (if not yet present) and set the necessary environment variables.\nYou can also run go test or ginkgo without the test/test-cov rules. In this case you have to set the KUBEBUILDER_ASSETS environment variable to the path that contains the etcd + kube-apiserver binaries or you need to have the binaries pre-installed under /usr/local/kubebuilder/bin.\nDependency Management We are using go modules for depedency management. In order to add a new package dependency to the project, you can perform go get \u003cPACKAGE\u003e@\u003cVERSION\u003e or edit the go.mod file and append the package along with the version you want to use.\nUpdating Dependencies The Makefile contains a rule called revendor which performs go mod vendor and go mod tidy. go mod vendor resets the main module’s vendor directory to include all packages needed to build and test all the main module’s packages. It does not include test code for vendored packages. go mod tidy makes sure go.mod matches the source code in the module. It adds any missing modules necessary to build the current module’s packages and dependencies, and it removes unused modules that don’t provide any relevant packages.\nmake revendor The dependencies are installed into the vendor folder which should be added to the VCS.\nWarning Make sure that you test the code after you have updated the dependencies!  ","categories":"","description":"","excerpt":"Testing We follow the BDD-style testing principles and are leveraging …","ref":"/docs/contribute/10_code/20_dependencies/","tags":"","title":"Dependencies"},{"body":"Dependency Management We are using go modules for depedency management. In order to add a new package dependency to the project, you can perform go get \u003cPACKAGE\u003e@\u003cVERSION\u003e or edit the go.mod file and append the package along with the version you want to use.\nUpdating Dependencies The Makefile contains a rule called revendor which performs go mod tidy and go mod vendor. go mod tidy makes sure go.mod matches the source code in the module. It adds any missing modules necessary to build the current module’s packages and dependencies, and it removes unused modules that don’t provide any relevant packages. go mod vendor resets the main module’s vendor directory to include all packages needed to build and test all the main module’s packages. It does not include test code for vendored packages.\nmake revendor The dependencies are installed into the vendor folder which should be added to the VCS.\n⚠️ Make sure that you test the code after you have updated the dependencies!\nExported Packages This repository contains several packages that could be considered “exported packages”, in a sense that they are supposed to be reused in other Go projects. For example:\n Gardener’s API packages: pkg/apis Library for building Gardener extensions: extensions Gardener’s Test Framework: test/framework  There are a few more folders in this repository (non-Go sources) that are reused across projects in the gardener organization:\n GitHub templates: .github Concourse / cc-utils related helpers: hack/.ci Development, build and testing helpers: hack  These packages feature a dummy doc.go file to allow other Go projects to pull them in as go mod dependencies.\nThese packages are explicitly not supposed to be used in other projects (consider them as “non-exported”):\n API validation packages: pkg/apis/*/*/validation Operation package (main Gardener business logic regarding Seed and Shoot clusters): pkg/operation Third party code: third_party  Currently, we don’t have a mechanism yet for selectively syncing out these exported packages into dedicated repositories like kube’s staging mechanism (publishing-bot).\nImport Restrictions We want to make sure, that other projects can depend on this repository’s “exported” packages without pulling in the entire repository (including “non-exported” packages) or a high number of other unwanted dependencies. Hence, we have to be careful when adding new imports or references between our packages.\n ℹ️ General rule of thumb: the mentioned “exported” packages should be as self-contained as possible and depend on as few other packages in the repository and other projects as possible.\n In order to support that rule and automatically check compliance with that goal, we leverage import-boss. The tool checks all imports of the given packages (including transitive imports) against rules defined in .import-restrictions files in each directory. An import is allowed if it matches at least one allowed prefix and does not match any forbidden prefixes. Note: '' (the empty string) is a prefix of everything. For more details, see: https://github.com/kubernetes/code-generator/tree/master/cmd/import-boss\nimport-boss is executed on every pull request and blocks the PR if it doesn’t comply with the defined import restrictions. You can also run it locally using make check.\nImport restrictions should be changed in the following situations:\n We spot a new pattern of imports across our packages that was not restricted before but makes it more difficult for other projects to depend on our “exported” packages. In that case, the imports should be further restricted to disallow such problematic imports, and the code/package structure should be reworked to comply with the newly given restrictions. We want to share code between packages, but existing import restrictions prevent us from doing so. In that case, please consider what additional dependencies it will pull in, when loosening existing restrictions. Also consider possible alternatives, like code restructurings or extracting shared code into dedicated packages for minimal impact on dependent projects.  ","categories":"","description":"","excerpt":"Dependency Management We are using go modules for depedency …","ref":"/docs/gardener/development/dependencies/","tags":"","title":"Dependencies"},{"body":"Deploying Gardenlets Gardenlets act as decentral “agents” to manage shoot clusters of a seed cluster.\nTo support scaleability in an automated way, gardenlets are deployed automatically. However, you can still deploy gardenlets manually to be more flexible, for example, when shoot clusters that need to be managed by Gardener are behind a firewall. The gardenlet only requires network connectivity from the gardenlet to the Garden cluster (not the other way round), so it can be used to register Kubernetes clusters with no public endpoint.\nProcedure   First, an initial gardenlet needs to be deployed:\n Deploy it manually if you have special requirements. More information: Deploy a Gardenlet Manually Let the Gardener installer deploy it automatically otherwise. More information: Automatic Deployment of Gardenlets    To add additional seed clusters, it is recommended to use regular shoot clusters. You can do this by creating a ManagedSeed resource with a gardenlet section as described in Register Shoot as Seed.\n  ","categories":"","description":"","excerpt":"Deploying Gardenlets Gardenlets act as decentral “agents” to manage …","ref":"/docs/gardener/deployment/deploy_gardenlet/","tags":"","title":"Deploy Gardenlet"},{"body":"Automatic Deployment of Gardenlets The gardenlet can automatically deploy itself into shoot clusters, and register this cluster as a seed cluster. These clusters are called “managed seeds” (aka “shooted seeds”). This procedure is the preferred way to add additional seed clusters, because shoot clusters already come with production-grade qualities that are also demanded for seed clusters.\nPrerequisites The only prerequisite is to register an initial cluster as a seed cluster that has already a gardenlet deployed:\n This gardenlet was either deployed as part of a Gardener installation using a setup tool (for example, gardener/garden-setup) or the gardenlet was deployed manually  for a step-by-step manual installation Guide see: Deploy a Gardenlet Manually)     The initial cluster can be the garden cluster itself.\n Self-Deployment of Gardenlets in Additional Managed Seed Clusters For a better scalability, you usually need more seed clusters that you can create as follows:\n Use the initial cluster as the seed cluster for other managed seed clusters. It hosts the control planes of the other seed clusters. The gardenlet deployed in the initial cluster deploys itself automatically into the managed seed clusters.  The advantage of this approach is that there’s only one initial gardenlet installation required. Every other managed seed cluster has a gardenlet deployed automatically.\nRelated Links Register Shoot as Seed\ngarden-setup\n","categories":"","description":"","excerpt":"Automatic Deployment of Gardenlets The gardenlet can automatically …","ref":"/docs/gardener/deployment/deploy_gardenlet_automatically/","tags":"","title":"Deploy Gardenlet Automatically"},{"body":"Deploy a Gardenlet Manually Manually deploying a gardenlet is required in the following cases:\n  The Kubernetes cluster to be registered as a seed cluster has no public endpoint, because it is behind a firewall. The gardenlet must then be deployed into the cluster itself.\n  The Kubernetes cluster to be registered as a seed cluster is managed externally (the Kubernetes cluster is not a shoot cluster, so Automatic Deployment of Gardenlets cannot be used).\n  The gardenlet runs outside of the Kubernetes cluster that should be registered as a seed cluster. (The gardenlet is not restricted to run in the seed cluster or to be deployed into a Kubernetes cluster at all).\n   Once you’ve deployed a gardenlet manually, for example, behind a firewall, you can deploy new gardenlets automatically. The manually deployed gardenlet is then used as a template for the new gardenlets. More information: Automatic Deployment of Gardenlets.\n Prerequisites Kubernetes cluster that should be registered as a seed cluster   Verify that the cluster has a supported Kubernetes version.\n  Determine the nodes, pods, and services CIDR of the cluster. You need to configure this information in the Seed configuration. Gardener uses this information to check that the shoot cluster isn’t created with overlapping CIDR ranges.\n  Every Seed cluster needs an Ingress controller which distributes external requests to internal components like grafana and prometheus. Gardener supports two approaches to achieve this:\n  a. Gardener managed Ingress controller and DNS records. For this configure the following lines in your Seed resource:\nspec:  dns:  provider:  type: aws-route53  secretRef:  name: ingress-secret  namespace: garden  ingress:  domain: ingress.my-seed.example.com  controller:  kind: nginx  providerConfig:  \u003csome-optional-provider-specific-config-for-the-ingressController\u003e ⚠ Please note that if you set .spec.ingress then .spec.dns.ingressDomain must be nil.\nb. Self-managed DNS record and Ingress controller:\n⚠️ There should exist a DNS record *.ingress.\u003cSEED-CLUSTER-DOMAIN\u003e where \u003cSEED-CLUSTER-DOMAIN\u003e is the value of the .dns.ingressDomain field of a Seed cluster resource (or the respective Gardenlet configuration).\nThis is how it could be done for the Nginx ingress controller\nDeploy nginx into the kube-system namespace in the Kubernetes cluster that should be registered as a Seed.\nNginx will on most cloud providers create the service with type LoadBalancer with an external ip.\nNAME TYPE CLUSTER-IP EXTERNAL-IP nginx-ingress-controller LoadBalancer 10.0.15.46 34.200.30.30 Create a wildcard A record (e.g *.ingress.sweet-seed.. IN A 34.200.30.30) with your DNS provider and point it to the external ip of the ingress service. This ingress domain is later required to register the Seed cluster.\nPlease configure the ingress domain in the Seed specification as follows:\nspec:  dns:  ingressDomain: ingress.sweet-seed.\u003cmy-domain\u003e ⚠ Please note that if you set .spec.dns.ingressDomain then .spec.ingress must be nil.\nkubeconfig for the Seed Cluster The kubeconfig is required to deploy the gardenlet Helm chart to the seed cluster. The gardenlet requires certain privileges to be able to operate. These privileges are described in RBAC resources in the gardenlet Helm chart (see charts/gardener/gardenlet/charts/runtime/templates). The Helm chart contains a service account gardenlet that the gardenlet deployment uses by default to talk to the Seed API server.\n If the gardenlet isn’t deployed in the seed cluster, the gardenlet can be configured to use a kubeconfig, which also requires the above-mentioned privileges, from a mounted directory. The kubeconfig is specified in section seedClientConnection.kubeconfig of the Gardenlet configuration. This configuration option isn’t used in the following, as this procedure only describes the recommended setup option where the gardenlet is running in the seed cluster itself.\n Procedure Overview   Prepare the garden cluster:\n Create a bootstrap token secret in the kube-system namespace of the garden cluster Create RBAC roles for the gardenlet to allow bootstrapping in the garden cluster    Prepare the gardenlet Helm chart.\n  Automatically register shoot cluster as a seed cluster.\n  Deploy the gardenlet\n  Check that the gardenlet is successfully deployed\n  Create a bootstrap token secret in the kube-system namespace of the garden cluster The gardenlet needs to talk to the Gardener API server residing in the garden cluster.\nThe gardenlet can be configured with an already existing garden cluster kubeconfig in one of the following ways:\n  Either by specifying gardenClientConnection.kubeconfig in the Gardenlet configuration or\n  by supplying the environment variable GARDEN_KUBECONFIG pointing to a mounted kubeconfig file).\n  The preferred way however, is to use the gardenlets ability to request a signed certificate for the garden cluster by leveraging Kubernetes Certificate Signing Requests. The gardenlet performs a TLS bootstrapping process that is similar to the Kubelet TLS Bootstrapping. Make sure that the API server of the garden cluster has bootstrap token authentication enabled.\nThe client credentials required for the gardenlets TLS bootstrapping process, need to be either token or certificate (OIDC isn’t supported) and have permissions to create a Certificate Signing Request (CSR). It’s recommended to use bootstrap tokens due to their desirable security properties (such as a limited token lifetime).\nTherefore, first create a bootstrap token secret for the garden cluster:\napiVersion: v1 kind: Secret metadata:  # Name MUST be of form \"bootstrap-token-\u003ctoken id\u003e\"  name: bootstrap-token-07401b  namespace: kube-system  # Type MUST be 'bootstrap.kubernetes.io/token' type: bootstrap.kubernetes.io/token stringData:  # Human readable description. Optional.  description: \"Token to be used by the gardenlet for Seed `sweet-seed`.\"   # Token ID and secret. Required.  token-id: 07401b # 6 characters  token-secret: f395accd246ae52d # 16 characters   # Expiration. Optional.  # expiration: 2017-03-10T03:22:11Z   # Allowed usages.  usage-bootstrap-authentication: \"true\"  usage-bootstrap-signing: \"true\" When you later prepare the gardenlet Helm chart, a kubeconfig based on this token is shared with the gardenlet upon deployment.\nCreate RBAC roles for the gardenlet to allow bootstrapping in the garden cluster This step is only required if the gardenlet you deploy is the first gardenlet in the Gardener installation. Additionally, when using the control plane chart, the following resources are already contained in the Helm chart, that is, if you use it you can skip these steps as the needed RBAC roles already exist.\nThe gardenlet uses the configured bootstrap kubeconfig in gardenClientConnection.bootstrapKubeconfig to request a signed certificate for the user gardener.cloud:system:seed:\u003cseed-name\u003e in the group gardener.cloud:system:seeds.\nCreate a ClusterRole and ClusterRoleBinding that grant full admin permissions to authenticated gardenlets.\nCreate the following resources in the garden cluster:\n--- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata:  name: gardener.cloud:system:seeds rules:  - apiGroups:  - '*'  resources:  - '*'  verbs:  - '*' --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata:  name: gardener.cloud:system:seeds roleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: gardener.cloud:system:seeds subjects:  - kind: Group  name: gardener.cloud:system:seeds  apiGroup: rbac.authorization.k8s.io --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata:  name: gardener.cloud:system:seed-bootstrapper rules:  - apiGroups:  - certificates.k8s.io  resources:  - certificatesigningrequests  verbs:  - create  - get  - apiGroups:  - certificates.k8s.io  resources:  - certificatesigningrequests/seedclient  verbs:  - create --- # A kubelet/gardenlet authenticating using bootstrap tokens is authenticated as a user in the group system:bootstrappers # Allows the Gardenlet to create a CSR apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata:  name: gardener.cloud:system:seed-bootstrapper roleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: gardener.cloud:system:seed-bootstrapper subjects:  - kind: Group  name: system:bootstrappers  apiGroup: rbac.authorization.k8s.io ℹ️ After bootstrapping, the gardenlet has full administrative access to the garden cluster. You might be interested to harden this and limit its permissions to only resources related to the seed cluster it is responsible for. Please take a look into this document.\nPrepare the gardenlet Helm chart This section only describes the minimal configuration, using the global configuration values of the gardenlet Helm chart. For an overview over all values, see the configuration values. We refer to the global configuration values as gardenlet configuration in the remaining procedure.\n  Create a gardenlet configuration gardenlet-values.yaml based on this template.\n  Create a bootstrap kubeconfig based on the bootstrap token created in the garden cluster.\nReplace the \u003cbootstrap-token\u003e with token-id.token-secret (from our previous example: 07401b.f395accd246ae52d) from the bootstrap token secret.\napiVersion: v1 kind: Config current-context: gardenlet-bootstrap@default clusters: - cluster:  certificate-authority-data: \u003cca-of-garden-cluster\u003e  server: https://\u003cendpoint-of-garden-cluster\u003e  name: default contexts: - context:  cluster: default  user: gardenlet-bootstrap  name: gardenlet-bootstrap@default users: - name: gardenlet-bootstrap  user:  token: \u003cbootstrap-token\u003e   In section gardenClientConnection.bootstrapKubeconfig of your gardenlet configuration, provide the bootstrap kubeconfig together with a name and namespace to the gardenlet Helm chart.\ngardenClientConnection:  bootstrapKubeconfig:  name: gardenlet-kubeconfig-bootstrap  namespace: garden  kubeconfig: | \u003cbootstrap-kubeconfig\u003e # will be base64 encoded by helm The bootstrap kubeconfig is stored in the specified secret.\n  In section gardenClientConnection.kubeconfigSecret of your gardenlet configuration, define a name and a namespace where the gardenlet stores the real kubeconfig that it creates during the bootstrap process. If the secret doesn’t exist, the gardenlet creates it for you.\ngardenClientConnection:  kubeconfigSecret:  name: gardenlet-kubeconfig  namespace: garden   Updating the garden cluster CA The kubeconfig created by the gardenlet in step 4 will not be recreated as long as it exists, even if a new bootstrap kubeconfig is provided. To enable rotation of the garden cluster CA certificate, a new bundle can be provided via the gardenClientConnection.gardenClusterCACert field. If the provided bundle differs from the one currently in the gardenlet’s kubeconfig secret then it will be updated. To remove the CA completely (e.g. when switching to a publicly trusted endpoint) this field can be set to either none or null.\nAutomatically register shoot cluster as a seed cluster A seed cluster can either be registered by manually creating the Seed resource or automatically by the gardenlet. This functionality is useful for managed seed clusters, as the gardenlet in the garden cluster deploys a copy of itself into the cluster with automatic registration of the Seed configured. However, it can also be used to have a streamlined seed cluster registration process when manually deploying the gardenlet.\n This procedure doesn’t describe all the possible configurations for the Seed resource. For more information, see:\n Example Seed resource Configurable Seed settings.   Adjust the gardenlet component configuration   Supply the Seed resource in section seedConfig of your gardenlet configuration gardenlet-values.yaml.\n  Add the seedConfig to your gardenlet configuration gardenlet-values.yaml. The field seedConfig.spec.provider.type specifies the infrastructure provider type (for example, aws) of the seed cluster. For all supported infrastructure providers, see Known Extension Implementations.\n.... seedConfig:  metadata:  name: sweet-seed  spec:  dns:  ingressDomain: ingress.sweet-seed.\u003cmy-domain\u003e # see prerequisites  networks: # see prerequisites  nodes: 10.240.0.0/16  pods: 100.244.0.0/16  services: 100.32.0.0/13  shootDefaults: # optional: non-overlapping default CIDRs for shoot clusters of that Seed  pods: 100.96.0.0/11  services: 100.64.0.0/13  provider:  region: eu-west-1  type: \u003cprovider\u003e   Optional: Enable HA mode You may consider running gardenlet with multiple replicas, especially if the seed cluster is configured to host HA shoot control planes. Therefore, the following Helm chart values define the degree of high availability you want to achieve for the gardenlet deployment.\nglobal:  gardenlet:  replicaCount: 2 # or more if a higher failure tolerance is required.  failureToleranceType: zone # One of `zone` or `node` - defines how replicas are spread. Optional: Enable backup and restore The seed cluster can be set up with backup and restore for the main etcds of shoot clusters.\nGardener uses etcd-backup-restore that integrates with different storage providers to store the shoot cluster’s main etcd backups. Make sure to obtain client credentials that have sufficient permissions with the chosen storage provider.\nCreate a secret in the garden cluster with client credentials for the storage provider. The format of the secret is cloud provider specific and can be found in the repository of the respective Gardener extension. For example, the secret for AWS S3 can be found in the AWS provider extension (30-etcd-backup-secret.yaml).\napiVersion: v1 kind: Secret metadata:  name: sweet-seed-backup  namespace: garden type: Opaque data:  # client credentials format is provider specific Configure the Seed resource in section seedConfig of your gardenlet configuration to use backup and restore:\n... seedConfig:  metadata:  name: sweet-seed  spec:  backup:  provider: \u003cprovider\u003e  secretRef:  name: sweet-seed-backup  namespace: garden Deploy the gardenlet  The gardenlet doesn’t have to run in the same Kubernetes cluster as the seed cluster it’s registering and reconciling, but it is in most cases advantageous to use in-cluster communication to talk to the Seed API server. Running a gardenlet outside of the cluster is mostly used for local development.\n The gardenlet-values.yaml looks something like this (with automatic Seed registration and backup for shoot clusters enabled):\nglobal:  # Gardenlet configuration values  gardenlet:  enabled: true  ...  \u003cdefault config\u003e  ...  config:  gardenClientConnection:  ...  bootstrapKubeconfig:  name: gardenlet-bootstrap-kubeconfig  namespace: garden  kubeconfig: |apiVersion: v1 clusters: - cluster: certificate-authority-data: \u003cdummy\u003e server: \u003cmy-garden-cluster-endpoint\u003e name: my-kubernetes-cluster ....   kubeconfigSecret:  name: gardenlet-kubeconfig  namespace: garden  ...  \u003cdefault config\u003e  ...  seedConfig:  metadata:  name: sweet-seed  spec:  dns:  ingressDomain: ingress.sweet-seed.\u003cmy-domain\u003e  networks:  nodes: 10.240.0.0/16  pods: 100.244.0.0/16  services: 100.32.0.0/13  shootDefaults:  pods: 100.96.0.0/11  services: 100.64.0.0/13  provider:  region: eu-west-1  type: \u003cprovider\u003e  backup:  provider: \u003cprovider\u003e  secretRef:  name: sweet-seed-backup  namespace: garden Deploy the gardenlet Helm chart to the Kubernetes cluster.\nhelm install gardenlet charts/gardener/gardenlet \\  --namespace garden \\  -f gardenlet-values.yaml \\  --wait This helm chart creates:\n A service account gardenlet that the gardenlet can use to talk to the Seed API server. RBAC roles for the service account (full admin rights at the moment). The secret (garden/gardenlet-bootstrap-kubeconfig) containing the bootstrap kubeconfig. The gardenlet deployment in the garden namespace.  Check that the gardenlet is successfully deployed   Check that the gardenlets certificate bootstrap was successful.\nCheck if the secret gardenlet-kubeconfig in the namespace garden in the seed cluster is created and contains a kubeconfig with a valid certificate.\n  Get the kubeconfig from the created secret.\n$ kubectl -n garden get secret gardenlet-kubeconfig -o json | jq -r .data.kubeconfig | base64 -d   Test against the garden cluster and verify it’s working.\n  Extract the client-certificate-data from the user gardenlet.\n  View the certificate:\n$ openssl x509 -in ./gardenlet-cert -noout -text Check that the certificate is valid for a year (that is the lifetime of new certificates).\n    Check that the bootstrap secret gardenlet-bootstrap-kubeconfig has been deleted from the seed cluster in namespace garden.\n  Check that the seed cluster is registered and READY in the garden cluster.\nCheck that the seed cluster sweet-seed exists and all conditions indicate that it’s available. If so, the Gardenlet is sending regular heartbeats and the seed bootstrapping was successful.\nCheck that the conditions on the Seed resource look similar to the following:\n$ kubectl get seed sweet-seed -o json | jq .status.conditions [  {  \"lastTransitionTime\": \"2020-07-17T09:17:29Z\",  \"lastUpdateTime\": \"2020-07-17T09:17:29Z\",  \"message\": \"Gardenlet is posting ready status.\",  \"reason\": \"GardenletReady\",  \"status\": \"True\",  \"type\": \"GardenletReady\"  },  {  \"lastTransitionTime\": \"2020-07-17T09:17:49Z\",  \"lastUpdateTime\": \"2020-07-17T09:53:17Z\",  \"message\": \"Seed cluster has been bootstrapped successfully.\",  \"reason\": \"BootstrappingSucceeded\",  \"status\": \"True\",  \"type\": \"Bootstrapped\"  },  {  \"lastTransitionTime\": \"2020-07-17T09:17:49Z\",  \"lastUpdateTime\": \"2020-07-17T09:53:17Z\",  \"message\": \"Backup Buckets are available.\",  \"reason\": \"BackupBucketsAvailable\",  \"status\": \"True\",  \"type\": \"BackupBucketsReady\"  } ]   Related Links Issue #1724: Harden Gardenlet RBAC privileges.\nBackup and Restore.\n","categories":"","description":"","excerpt":"Deploy a Gardenlet Manually Manually deploying a gardenlet is required …","ref":"/docs/gardener/deployment/deploy_gardenlet_manually/","tags":"","title":"Deploy Gardenlet Manually"},{"body":"Deployment of the AliCloud provider extension Disclaimer: This document is NOT a step by step installation guide for the AliCloud provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the AliCloud provider extension repository.\ngardener-extension-admission-alicloud Authentication against the Garden cluster There are several authentication possibilities depending on whether or not the concept of Virtual Garden is used.\nVirtual Garden is not used, i.e., the runtime Garden cluster is also the target Garden cluster. Automounted Service Account Token The easiest way to deploy the gardener-extension-admission-alicloud component will be to not provide kubeconfig at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.\nService Account Token Volume Projection Another solution will be to use Service Account Token Volume Projection combined with a kubeconfig referencing a token file (see example below).\napiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://default.kubernetes.svc.cluster.local  name: garden contexts: - context:  cluster: garden  user: garden  name: garden current-context: garden users: - name: garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token This will allow for automatic rotation of the service account token by the kubelet. The configuration can be achieved by setting both .Values.global.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.kubeconfig in the respective chart’s values.yaml file.\nVirtual Garden is used, i.e., the runtime Garden cluster is different from the target Garden cluster. Service Account The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the target cluster. Then use the generated service account token and craft a kubeconfig which will be used by the workload in the runtime cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting .Values.global.virtualGarden.enabled: true and following these steps:\n Deploy the application part of the charts in the target cluster. Get the service account token and craft the kubeconfig. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Client Certificate Another solution will be to bind the roles in the target cluster to a User subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both .Values.global.virtualGarden.enabled: true and .Values.global.virtualGarden.user.name, then following these steps:\n Generate a client certificate for the target cluster for the respective user. Deploy the application part of the charts in the target cluster. Craft a kubeconfig using the already generated client certificate. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Projected Service Account Token This approach requires an already deployed and configured oidc-webhook-authenticator for the target cluster. Also the runtime cluster should be registered as a trusted identity provider in the target cluster. Then projected service accounts tokens from the runtime cluster can be used to authenticate against the target cluster. The needed steps are as follows:\n Deploy OWA and establish the needed trust. Set .Values.global.virtualGarden.enabled: true and .Values.global.virtualGarden.user.name. Note: username value will depend on the trust configuration, e.g., \u003cprefix\u003e:system:serviceaccount:\u003cnamespace\u003e:\u003cserviceaccount\u003e Set .Values.global.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.serviceAccountTokenVolumeProjection.audience. Note: audience value will depend on the trust configuration, e.g., \u003ccliend-id-from-trust-config\u003e. Craft a kubeconfig (see example below). Deploy the application part of the charts in the target cluster. Deploy the runtime part of the charts in the runtime cluster.  apiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://virtual-garden.api  name: virtual-garden contexts: - context:  cluster: virtual-garden  user: virtual-garden  name: virtual-garden current-context: virtual-garden users: - name: virtual-garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token ","categories":"","description":"","excerpt":"Deployment of the AliCloud provider extension Disclaimer: This …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-alicloud/docs/deployment/","tags":"","title":"Deployment"},{"body":"Deployment of the AWS provider extension Disclaimer: This document is NOT a step by step installation guide for the AWS provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the AWS provider extension repository.\ngardener-extension-admission-aws Authentication against the Garden cluster There are several authentication possibilities depending on whether or not the concept of Virtual Garden is used.\nVirtual Garden is not used, i.e., the runtime Garden cluster is also the target Garden cluster. Automounted Service Account Token The easiest way to deploy the gardener-extension-admission-aws component will be to not provide kubeconfig at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.\nService Account Token Volume Projection Another solution will be to use Service Account Token Volume Projection combined with a kubeconfig referencing a token file (see example below).\napiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://default.kubernetes.svc.cluster.local  name: garden contexts: - context:  cluster: garden  user: garden  name: garden current-context: garden users: - name: garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token This will allow for automatic rotation of the service account token by the kubelet. The configuration can be achieved by setting both .Values.global.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.kubeconfig in the respective chart’s values.yaml file.\nVirtual Garden is used, i.e., the runtime Garden cluster is different from the target Garden cluster. Service Account The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the target cluster. Then use the generated service account token and craft a kubeconfig which will be used by the workload in the runtime cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting .Values.global.virtualGarden.enabled: true and following these steps:\n Deploy the application part of the charts in the target cluster. Get the service account token and craft the kubeconfig. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Client Certificate Another solution will be to bind the roles in the target cluster to a User subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both .Values.global.virtualGarden.enabled: true and .Values.global.virtualGarden.user.name, then following these steps:\n Generate a client certificate for the target cluster for the respective user. Deploy the application part of the charts in the target cluster. Craft a kubeconfig using the already generated client certificate. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Projected Service Account Token This approach requires an already deployed and configured oidc-webhook-authenticator for the target cluster. Also the runtime cluster should be registered as a trusted identity provider in the target cluster. Then projected service accounts tokens from the runtime cluster can be used to authenticate against the target cluster. The needed steps are as follows:\n Deploy OWA and establish the needed trust. Set .Values.global.virtualGarden.enabled: true and .Values.global.virtualGarden.user.name. Note: username value will depend on the trust configuration, e.g., \u003cprefix\u003e:system:serviceaccount:\u003cnamespace\u003e:\u003cserviceaccount\u003e Set .Values.global.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.serviceAccountTokenVolumeProjection.audience. Note: audience value will depend on the trust configuration, e.g., \u003ccliend-id-from-trust-config\u003e. Craft a kubeconfig (see example below). Deploy the application part of the charts in the target cluster. Deploy the runtime part of the charts in the runtime cluster.  apiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://virtual-garden.api  name: virtual-garden contexts: - context:  cluster: virtual-garden  user: virtual-garden  name: virtual-garden current-context: virtual-garden users: - name: virtual-garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token ","categories":"","description":"","excerpt":"Deployment of the AWS provider extension Disclaimer: This document is …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/deployment/","tags":"","title":"Deployment"},{"body":"Deployment of the Azure provider extension Disclaimer: This document is NOT a step by step installation guide for the Azure provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the Azure provider extension repository.\ngardener-extension-admission-azure Authentication against the Garden cluster There are several authentication possibilities depending on whether or not the concept of Virtual Garden is used.\nVirtual Garden is not used, i.e., the runtime Garden cluster is also the target Garden cluster. Automounted Service Account Token The easiest way to deploy the gardener-extension-admission-azure component will be to not provide kubeconfig at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.\nService Account Token Volume Projection Another solution will be to use Service Account Token Volume Projection combined with a kubeconfig referencing a token file (see example below).\napiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://default.kubernetes.svc.cluster.local  name: garden contexts: - context:  cluster: garden  user: garden  name: garden current-context: garden users: - name: garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token This will allow for automatic rotation of the service account token by the kubelet. The configuration can be achieved by setting both .Values.global.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.kubeconfig in the respective chart’s values.yaml file.\nVirtual Garden is used, i.e., the runtime Garden cluster is different from the target Garden cluster. Service Account The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the target cluster. Then use the generated service account token and craft a kubeconfig which will be used by the workload in the runtime cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting .Values.global.virtualGarden.enabled: true and following these steps:\n Deploy the application part of the charts in the target cluster. Get the service account token and craft the kubeconfig. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Client Certificate Another solution will be to bind the roles in the target cluster to a User subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both .Values.global.virtualGarden.enabled: true and .Values.global.virtualGarden.user.name, then following these steps:\n Generate a client certificate for the target cluster for the respective user. Deploy the application part of the charts in the target cluster. Craft a kubeconfig using the already generated client certificate. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Projected Service Account Token This approach requires an already deployed and configured oidc-webhook-authenticator for the target cluster. Also the runtime cluster should be registered as a trusted identity provider in the target cluster. Then projected service accounts tokens from the runtime cluster can be used to authenticate against the target cluster. The needed steps are as follows:\n Deploy OWA and establish the needed trust. Set .Values.global.virtualGarden.enabled: true and .Values.global.virtualGarden.user.name. Note: username value will depend on the trust configuration, e.g., \u003cprefix\u003e:system:serviceaccount:\u003cnamespace\u003e:\u003cserviceaccount\u003e Set .Values.global.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.serviceAccountTokenVolumeProjection.audience. Note: audience value will depend on the trust configuration, e.g., \u003ccliend-id-from-trust-config\u003e. Craft a kubeconfig (see example below). Deploy the application part of the charts in the target cluster. Deploy the runtime part of the charts in the runtime cluster.  apiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://virtual-garden.api  name: virtual-garden contexts: - context:  cluster: virtual-garden  user: virtual-garden  name: virtual-garden current-context: virtual-garden users: - name: virtual-garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token ","categories":"","description":"","excerpt":"Deployment of the Azure provider extension Disclaimer: This document …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-azure/docs/deployment/","tags":"","title":"Deployment"},{"body":"Deployment of the GCP provider extension Disclaimer: This document is NOT a step by step installation guide for the GCP provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the GCP provider extension repository.\ngardener-extension-admission-gcp Authentication against the Garden cluster There are several authentication possibilities depending on whether or not the concept of Virtual Garden is used.\nVirtual Garden is not used, i.e., the runtime Garden cluster is also the target Garden cluster. Automounted Service Account Token The easiest way to deploy the gardener-extension-admission-gcp component will be to not provide kubeconfig at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.\nService Account Token Volume Projection Another solution will be to use Service Account Token Volume Projection combined with a kubeconfig referencing a token file (see example below).\napiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://default.kubernetes.svc.cluster.local  name: garden contexts: - context:  cluster: garden  user: garden  name: garden current-context: garden users: - name: garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token This will allow for automatic rotation of the service account token by the kubelet. The configuration can be achieved by setting both .Values.global.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.kubeconfig in the respective chart’s values.yaml file.\nVirtual Garden is used, i.e., the runtime Garden cluster is different from the target Garden cluster. Service Account The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the target cluster. Then use the generated service account token and craft a kubeconfig which will be used by the workload in the runtime cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting .Values.global.virtualGarden.enabled: true and following these steps:\n Deploy the application part of the charts in the target cluster. Get the service account token and craft the kubeconfig. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Client Certificate Another solution will be to bind the roles in the target cluster to a User subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both .Values.global.virtualGarden.enabled: true and .Values.global.virtualGarden.user.name, then following these steps:\n Generate a client certificate for the target cluster for the respective user. Deploy the application part of the charts in the target cluster. Craft a kubeconfig using the already generated client certificate. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Projected Service Account Token This approach requires an already deployed and configured oidc-webhook-authenticator for the target cluster. Also the runtime cluster should be registered as a trusted identity provider in the target cluster. Then projected service accounts tokens from the runtime cluster can be used to authenticate against the target cluster. The needed steps are as follows:\n Deploy OWA and establish the needed trust. Set .Values.global.virtualGarden.enabled: true and .Values.global.virtualGarden.user.name. Note: username value will depend on the trust configuration, e.g., \u003cprefix\u003e:system:serviceaccount:\u003cnamespace\u003e:\u003cserviceaccount\u003e Set .Values.global.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.serviceAccountTokenVolumeProjection.audience. Note: audience value will depend on the trust configuration, e.g., \u003ccliend-id-from-trust-config\u003e. Craft a kubeconfig (see example below). Deploy the application part of the charts in the target cluster. Deploy the runtime part of the charts in the runtime cluster.  apiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://virtual-garden.api  name: virtual-garden contexts: - context:  cluster: virtual-garden  user: virtual-garden  name: virtual-garden current-context: virtual-garden users: - name: virtual-garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token ","categories":"","description":"","excerpt":"Deployment of the GCP provider extension Disclaimer: This document is …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-gcp/docs/deployment/","tags":"","title":"Deployment"},{"body":"Deployment of the OpenStack provider extension Disclaimer: This document is NOT a step by step installation guide for the OpenStack provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the OpenStack provider extension repository.\ngardener-extension-admission-openstack Authentication against the Garden cluster There are several authentication possibilities depending on whether or not the concept of Virtual Garden is used.\nVirtual Garden is not used, i.e., the runtime Garden cluster is also the target Garden cluster. Automounted Service Account Token The easiest way to deploy the gardener-extension-admission-openstack component will be to not provide kubeconfig at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.\nService Account Token Volume Projection Another solution will be to use Service Account Token Volume Projection combined with a kubeconfig referencing a token file (see example below).\napiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://default.kubernetes.svc.cluster.local  name: garden contexts: - context:  cluster: garden  user: garden  name: garden current-context: garden users: - name: garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token This will allow for automatic rotation of the service account token by the kubelet. The configuration can be achieved by setting both .Values.global.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.kubeconfig in the respective chart’s values.yaml file.\nVirtual Garden is used, i.e., the runtime Garden cluster is different from the target Garden cluster. Service Account The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the target cluster. Then use the generated service account token and craft a kubeconfig which will be used by the workload in the runtime cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting .Values.global.virtualGarden.enabled: true and following these steps:\n Deploy the application part of the charts in the target cluster. Get the service account token and craft the kubeconfig. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Client Certificate Another solution will be to bind the roles in the target cluster to a User subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both .Values.global.virtualGarden.enabled: true and .Values.global.virtualGarden.user.name, then following these steps:\n Generate a client certificate for the target cluster for the respective user. Deploy the application part of the charts in the target cluster. Craft a kubeconfig using the already generated client certificate. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Projected Service Account Token This approach requires an already deployed and configured oidc-webhook-authenticator for the target cluster. Also the runtime cluster should be registered as a trusted identity provider in the target cluster. Then projected service accounts tokens from the runtime cluster can be used to authenticate against the target cluster. The needed steps are as follows:\n Deploy OWA and establish the needed trust. Set .Values.global.virtualGarden.enabled: true and .Values.global.virtualGarden.user.name. Note: username value will depend on the trust configuration, e.g., \u003cprefix\u003e:system:serviceaccount:\u003cnamespace\u003e:\u003cserviceaccount\u003e Set .Values.global.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.serviceAccountTokenVolumeProjection.audience. Note: audience value will depend on the trust configuration, e.g., \u003ccliend-id-from-trust-config\u003e. Craft a kubeconfig (see example below). Deploy the application part of the charts in the target cluster. Deploy the runtime part of the charts in the runtime cluster.  apiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://virtual-garden.api  name: virtual-garden contexts: - context:  cluster: virtual-garden  user: virtual-garden  name: virtual-garden current-context: virtual-garden users: - name: virtual-garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token ","categories":"","description":"","excerpt":"Deployment of the OpenStack provider extension Disclaimer: This …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-openstack/docs/deployment/","tags":"","title":"Deployment"},{"body":"Deployment of the vSphere provider extension Disclaimer: This document is NOT a step by step installation guide for the vSphere provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the vSphere provider extension repository.\ngardener-extension-validator-vsphere Authentication against the Garden cluster There are several authentication possibilities depending on whether or not the concept of Virtual Garden is used.\nVirtual Garden is not used, i.e., the runtime Garden cluster is also the target Garden cluster. Automounted Service Account Token The easiest way to deploy the gardener-extension-validator-vsphere component will be to not provide kubeconfig at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.\nService Account Token Volume Projection Another solution will be to use Service Account Token Volume Projection combined with a kubeconfig referencing a token file (see example below).\napiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://default.kubernetes.svc.cluster.local  name: garden contexts: - context:  cluster: garden  user: garden  name: garden current-context: garden users: - name: garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token This will allow for automatic rotation of the service account token by the kubelet. The configuration can be achieved by setting both .Values.global.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.kubeconfig in the respective chart’s values.yaml file.\nVirtual Garden is used, i.e., the runtime Garden cluster is different from the target Garden cluster. Service Account The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the target cluster. Then use the generated service account token and craft a kubeconfig which will be used by the workload in the runtime cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting .Values.global.virtualGarden.enabled: true and following these steps:\n Deploy the application part of the charts in the target cluster. Get the service account token and craft the kubeconfig. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Client Certificate Another solution will be to bind the roles in the target cluster to a User subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both .Values.global.virtualGarden.enabled: true and .Values.global.virtualGarden.user.name, then following these steps:\n Generate a client certificate for the target cluster for the respective user. Deploy the application part of the charts in the target cluster. Craft a kubeconfig using the already generated client certificate. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Projected Service Account Token This approach requires an already deployed and configured oidc-webhook-authenticator for the target cluster. Also the runtime cluster should be registered as a trusted identity provider in the target cluster. Then projected service accounts tokens from the runtime cluster can be used to authenticate against the target cluster. The needed steps are as follows:\n Deploy OWA and establish the needed trust. Set .Values.global.virtualGarden.enabled: true and .Values.global.virtualGarden.user.name. Note: username value will depend on the trust configuration, e.g., \u003cprefix\u003e:system:serviceaccount:\u003cnamespace\u003e:\u003cserviceaccount\u003e Set .Values.global.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.serviceAccountTokenVolumeProjection.audience. Note: audience value will depend on the trust configuration, e.g., \u003ccliend-id-from-trust-config\u003e. Craft a kubeconfig (see example below). Deploy the application part of the charts in the target cluster. Deploy the runtime part of the charts in the runtime cluster.  apiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://virtual-garden.api  name: virtual-garden contexts: - context:  cluster: virtual-garden  user: virtual-garden  name: virtual-garden current-context: virtual-garden users: - name: virtual-garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token ","categories":"","description":"","excerpt":"Deployment of the vSphere provider extension Disclaimer: This document …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/docs/deployment/","tags":"","title":"Deployment"},{"body":"Deployment of the networking Calico extension Disclaimer: This document is NOT a step by step deployment guide for the networking Calico extension and only contains some configuration specifics regarding the deployment of different components via the helm charts residing in the networking Calico extension repository.\ngardener-extension-admission-calico Authentication against the Garden cluster There are several authentication possibilities depending on whether or not the concept of Virtual Garden is used.\nVirtual Garden is not used, i.e., the runtime Garden cluster is also the target Garden cluster. Automounted Service Account Token The easiest way to deploy the gardener-extension-admission-calico component will be to not provide kubeconfig at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.\nService Account Token Volume Projection Another solution will be to use Service Account Token Volume Projection combined with a kubeconfig referencing a token file (see example below).\napiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://default.kubernetes.svc.cluster.local  name: garden contexts: - context:  cluster: garden  user: garden  name: garden current-context: garden users: - name: garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token This will allow for automatic rotation of the service account token by the kubelet. The configuration can be achieved by setting both .Values.global.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.kubeconfig in the respective chart’s values.yaml file.\nVirtual Garden is used, i.e., the runtime Garden cluster is different from the target Garden cluster. Service Account The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the target cluster. Then use the generated service account token and craft a kubeconfig which will be used by the workload in the runtime cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting .Values.global.virtualGarden.enabled: true and following these steps:\n Deploy the application part of the charts in the target cluster. Get the service account token and craft the kubeconfig. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Client Certificate Another solution will be to bind the roles in the target cluster to a User subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both .Values.global.virtualGarden.enabled: true and .Values.global.virtualGarden.user.name, then following these steps:\n Generate a client certificate for the target cluster for the respective user. Deploy the application part of the charts in the target cluster. Craft a kubeconfig using the already generated client certificate. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Projected Service Account Token This approach requires an already deployed and configured oidc-webhook-authenticator for the target cluster. Also the runtime cluster should be registered as a trusted identity provider in the target cluster. Then projected service accounts tokens from the runtime cluster can be used to authenticate against the target cluster. The needed steps are as follows:\n Deploy OWA and establish the needed trust. Set .Values.global.virtualGarden.enabled: true and .Values.global.virtualGarden.user.name. Note: username value will depend on the trust configuration, e.g., \u003cprefix\u003e:system:serviceaccount:\u003cnamespace\u003e:\u003cserviceaccount\u003e Set .Values.global.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.serviceAccountTokenVolumeProjection.audience. Note: audience value will depend on the trust configuration, e.g., \u003ccliend-id-from-trust-config\u003e. Craft a kubeconfig (see example below). Deploy the application part of the charts in the target cluster. Deploy the runtime part of the charts in the runtime cluster.  apiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://virtual-garden.api  name: virtual-garden contexts: - context:  cluster: virtual-garden  user: virtual-garden  name: virtual-garden current-context: virtual-garden users: - name: virtual-garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token ","categories":"","description":"","excerpt":"Deployment of the networking Calico extension Disclaimer: This …","ref":"/docs/extensions/network-extensions/gardener-extension-networking-calico/docs/deployment/","tags":"","title":"Deployment"},{"body":"Deployment of the shoot DNS service extension Disclaimer: This document is NOT a step by step deployment guide for the shoot DNS service extension and only contains some configuration specifics regarding the deployment of different components via the helm charts residing in the shoot DNS service extension repository.\ngardener-extension-admission-shoot-dns-service Authentication against the Garden cluster There are several authentication possibilities depending on whether or not the concept of Virtual Garden is used.\nVirtual Garden is not used, i.e., the runtime Garden cluster is also the target Garden cluster. Automounted Service Account Token The easiest way to deploy the gardener-extension-admission-shoot-dns-service component will be to not provide kubeconfig at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.\nService Account Token Volume Projection Another solution will be to use Service Account Token Volume Projection combined with a kubeconfig referencing a token file (see example below).\napiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://default.kubernetes.svc.cluster.local  name: garden contexts: - context:  cluster: garden  user: garden  name: garden current-context: garden users: - name: garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token This will allow for automatic rotation of the service account token by the kubelet. The configuration can be achieved by setting both .Values.global.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.kubeconfig in the respective chart’s values.yaml file.\nVirtual Garden is used, i.e., the runtime Garden cluster is different from the target Garden cluster. Service Account The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the target cluster. Then use the generated service account token and craft a kubeconfig which will be used by the workload in the runtime cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting .Values.global.virtualGarden.enabled: true and following these steps:\n Deploy the application part of the charts in the target cluster. Get the service account token and craft the kubeconfig. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Client Certificate Another solution will be to bind the roles in the target cluster to a User subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both .Values.global.virtualGarden.enabled: true and .Values.global.virtualGarden.user.name, then following these steps:\n Generate a client certificate for the target cluster for the respective user. Deploy the application part of the charts in the target cluster. Craft a kubeconfig using the already generated client certificate. Set the crafted kubeconfig and deploy the runtime part of the charts in the runtime cluster.  Projected Service Account Token This approach requires an already deployed and configured oidc-webhook-authenticator for the target cluster. Also the runtime cluster should be registered as a trusted identity provider in the target cluster. Then projected service accounts tokens from the runtime cluster can be used to authenticate against the target cluster. The needed steps are as follows:\n Deploy OWA and establish the needed trust. Set .Values.global.virtualGarden.enabled: true and .Values.global.virtualGarden.user.name. Note: username value will depend on the trust configuration, e.g., \u003cprefix\u003e:system:serviceaccount:\u003cnamespace\u003e:\u003cserviceaccount\u003e Set .Values.global.serviceAccountTokenVolumeProjection.enabled: true and .Values.global.serviceAccountTokenVolumeProjection.audience. Note: audience value will depend on the trust configuration, e.g., \u003ccliend-id-from-trust-config\u003e. Craft a kubeconfig (see example below). Deploy the application part of the charts in the target cluster. Deploy the runtime part of the charts in the runtime cluster.  apiVersion: v1 kind: Config clusters: - cluster:  certificate-authority-data: \u003cCA-DATA\u003e  server: https://virtual-garden.api  name: virtual-garden contexts: - context:  cluster: virtual-garden  user: virtual-garden  name: virtual-garden current-context: virtual-garden users: - name: virtual-garden  user:  tokenFile: /var/run/secrets/projected/serviceaccount/token ","categories":"","description":"","excerpt":"Deployment of the shoot DNS service extension Disclaimer: This …","ref":"/docs/extensions/others/gardener-extension-shoot-dns-service/docs/installation/deployment/","tags":"","title":"Deployment"},{"body":"DNS Autoscaling This is a short guide describing different options how to automatically scale CoreDNS in the shoot cluster.\nBackground Currently, Gardener uses CoreDNS as DNS server. Per default, it is installed as a deployment into the shoot cluster that is auto-scaled horizontally to cover for QPS-intensive applications. However, doing so does not seem to be enough to completely circumvent DNS bottlenecks such as:\n Cloud provider limits for DNS lookups. Unreliable UDP connections that forces a period of timeout in case packets are dropped. Unnecessary node hopping since CoreDNS is not deployed on all nodes, and as a result DNS queries end-up traversing multiple nodes before reaching the destination server. Inefficient load-balancing of services (e.g., round-robin might not be enough when using IPTables mode). Overload of the CoreDNS replicas as the maximum amount of replicas is fixed. and more …  As an alternative with extended configuration options, Gardener provides cluster-proportional autoscaling of CoreDNS. This guide focuses on the configuration of cluster-proportional autoscaling of CoreDNS and its advantages/disadvantages compared to the horizontal autoscaling. Please note that there is also the option to use a node-local DNS cache, which helps mitigate potential DNS bottlenecks (see Trade-offs in conjunction with NodeLocalDNS for considerations regarding using NodeLocalDNS together with one of the CoreDNS autoscaling approaches).\nConfiguring cluster-proportional DNS Autoscaling All that needs to be done to enable the usage of cluster-proportional autoscaling of CoreDNS is to set the corresponding option (spec.systemComponents.coreDNS.autoscaling.mode) in the Shoot resource to cluster-proportional:\n... spec:  ...  systemComponents:  coreDNS:  autoscaling:  mode: cluster-proportional ... To switch back to horizontal DNS autoscaling you can set the spec.systemComponents.coreDNS.autoscaling.mode to horizontal (or remove the coreDNS section).\nOnce the cluster-proportional autoscaling of CoreDNS has been enabled and the Shoot cluster has been reconciled afterwards, a ConfigMap called coredns-autoscaler will be created in the kube-system namespace with the default settings. The content will be similar to the following:\nlinear: '{\"coresPerReplica\":256,\"min\":2,\"nodesPerReplica\":16}' It is possible to adapt the ConfigMap according to your needs in case the defaults do not work as desired. The number of CoreDNS replicas is calculated according to the following formula:\nreplicas = max( ceil( cores × 1 / coresPerReplica ) , ceil( nodes × 1 / nodesPerReplica ) ) Depending on your needs, you can adjust coresPerReplica or nodesPerReplica, but it is also possible to override min if required.\nTrade-offs of horizontal and cluster-proportional DNS Autoscaling The horizontal autoscaling of CoreDNS as implemented by Gardener is fully managed, i.e. you do not need to perform any configuration changes. It scales according to the CPU usage of CoreDNS replicas meaning that it will create new replicas if the existing ones are under heavy load. This approach scales between 2 and 5 instances, which is sufficient for most workloads. In case this is not enough, the cluster-proportional autoscaling approach can be used instead with its more flexible configuration options.\nThe cluster-proportional autoscaling of CoreDNS as implemented by Gardener is fully managed, but allows more configuration options to adjust the default settings to your individual needs. It scales according to the cluster size, i.e. if your cluster grows in terms of cores/nodes so will the amount of CoreDNS replicas. However, it does not take the actual workload, e.g. CPU consumption, into account.\nExperience shows that the horizontal autoscaling of CoreDNS works for a variety of workloads. It does reach its limits if a cluster has a high amount of DNS requests, though. The cluster-proportional autoscaling approach allows to fine-tune the amount of CoreDNS replicas. It helps to scale in clusters of changing size. However, please keep in mind that you need to cater for the maximum amount of DNS requests as the replicas will not be adapted according to the workload, but only according to the cluster size (cores/nodes).\nTrade-offs in conjunction with NodeLocalDNS Using a node-local DNS cache can mitigate a lot of the potential DNS related problems. It works fine with a DNS workload that can be handle through the cache and reduces the inter-node DNS communication. As node-local DNS cache reduces the amount of traffic being sent to the cluster’s CoreDNS replicas, it usually works fine with horizontally scaled CoreDNS. Nevertheless, it also works with CoreDNS scaled in a cluster-proportional approach. In this mode, though, it might make sense to adapt the default settings as the CoreDNS workload is likely significantly reduced.\nOverall, you can view the DNS options on a scale. Horizontally scaled DNS provides a small amount of DNS servers. Especially for bigger clusters, a cluster-proportional approach will yield more CoreDNS instances and hence may yield a more balanced DNS solution. By adapting the settings you can further increase the amount of CoreDNS replicas. On the other end of the spectrum, a node-local DNS cache provides DNS on every node and allows to reduce the amount of (backend) CoreDNS instances regardless if they are horizontally or cluster-proportionally scaled.\n","categories":"","description":"","excerpt":"DNS Autoscaling This is a short guide describing different options how …","ref":"/docs/gardener/usage/dns-autoscaling/","tags":"","title":"DNS Autoscaling"},{"body":"Request DNS Names in Shoot Clusters Introduction Within a shoot cluster, it is possible to request DNS records via the following resource types:\n Ingress Service DNSEntry  It is necessary that the Gardener installation your shoot cluster runs in is equipped with a shoot-dns-service extension. This extension uses the seed’s dns management infrastructure to maintain DNS names for shoot clusters. Please ask your Gardener operator if the extension is available in your environment.\nShoot Feature Gate In some Gardener setups the shoot-dns-service extension is not enabled globally and thus must be configured per shoot cluster. Please adapt the shoot specification by the configuration shown below to activate the extension individually.\nkind: Shoot ... spec:  extensions:  - type: shoot-dns-service ... Before you start You should :\n Have created a shoot cluster Have created and correctly configured a DNS Provider (Please consult this page for more information) Have a basic understanding of DNS (see link under References)  There are 2 types of DNS that you can use within Kubernetes :\n internal (usually managed by coreDNS) external (managed by a public DNS provider).  This page, and the extension, exclusively works for external DNS handling.\nGardener allows 2 way of managing your external DNS:\n Manually, which means you are in charge of creating / maintaining your Kubernetes related DNS entries Via the Gardener DNS extension  Gardener DNS extension The managed external DNS records feature of the Gardener clusters makes all this easier. You do not need DNS service provider specific knowledge, and in fact you do not need to leave your cluster at all to achieve that. You simply annotate the Ingress / Service that needs its DNS records managed and it will be automatically created / managed by Gardener.\nManaged external DNS records are supported with the following DNS provider types:\n aws-route53 azure-dns azure-private-dns google-clouddns openstack-designate alicloud-dns cloudflare-dns  Request DNS records for Ingress resources To request a DNS name for an Ingress or Service object in the shoot cluster it must be annotated with the DNS class garden and an annotation denoting the desired DNS names.\nExample for an annotated Ingress resource:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata:  name: amazing-ingress  annotations:  # Let Gardener manage external DNS records for this Ingress.  dns.gardener.cloud/dnsnames: special.example.com # Use \"*\" to collects domains names from .spec.rules[].host  dns.gardener.cloud/ttl: \"600\"  dns.gardener.cloud/class: garden  # If you are delegating the certificate management to Gardener, uncomment the following line  #cert.gardener.cloud/purpose: managed spec:  rules:  - host: special.example.com  http:  paths:  - pathType: Prefix  path: \"/\"  backend:  service:  name: amazing-svc  port:  number: 8080  # Uncomment the following part if you are delegating the certificate management to Gardener  #tls:  # - hosts:  # - special.example.com  # secretName: my-cert-secret-name For an Ingress, the DNS names are already declared in the specification. Nevertheless the dnsnames annotation must be present. Here a subset of the DNS names of the ingress can be specified. If DNS names for all names are desired, the value all can be used.\nKeep in mind that ingress resources are ignored unless an ingress controller is set up. Gardener does not provide an ingress controller by default. For more details, see Ingress Controllers and Service in the Kubernetes documentation.\nRequest DNS records for service type LoadBalancer Example for an annotated Service (it must have the type LoadBalancer) resource:\napiVersion: v1 kind: Service metadata:  name: amazing-svc  annotations:  # Let Gardener manage external DNS records for this Service.  dns.gardener.cloud/dnsnames: special.example.com  dns.gardener.cloud/ttl: \"600\"  dns.gardener.cloud/class: garden spec:  selector:  app: amazing-app  ports:  - protocol: TCP  port: 80  targetPort: 8080  type: LoadBalancer Creating a DNSEntry resource explicitly It is also possible to create a DNS entry via the Kubernetes resource called DNSEntry:\napiVersion: dns.gardener.cloud/v1alpha1 kind: DNSEntry metadata:  annotations:  # Let Gardener manage this DNS entry.  dns.gardener.cloud/class: garden  name: special-dnsentry  namespace: default spec:  dnsName: special.example.com  ttl: 600  targets:  - 1.2.3.4 If one of the accepted DNS names is a direct subname of the shoot’s ingress domain, this is already handled by the standard wildcard entry for the ingress domain. Therefore this name should be excluded from the dnsnames list in the annotation. If only this DNS name is configured in the ingress, no explicit DNS entry is required, and the DNS annotations should be omitted at all.\nYou can check the status of the DNSEntry with\n$ kubectl get dnsentry NAME DNS TYPE PROVIDER STATUS AGE mydnsentry special.example.com aws-route53 default/aws Ready 24s As soon as the status of the entry is Ready, the provider has accepted the new DNS record. Depending on the provider and your DNS settings and cache, it may take up to 24 hours for the new entry to be propagated over all internet.\nMore examples can be found here\nRequest DNS records for Service/Ingress resources using a DNSAnnotation resource In rare cases it may not be possible to add annotations to a Service or Ingress resource object.\nE.g.: the helm chart used to deploy the resource may not be adaptable for some reasons or some automation is used, which always restores the original content of the resource object by dropping any additional annotations.\nIn these cases, it is recommended to use an additional DNSAnnotation resource in order to have more flexibility that DNSentry resources. The DNSAnnotation resource makes the DNS shoot service behave as if annotations have been added to the referenced resource.\nFor the Ingress example shown above, you can create a DNSAnnotation resource alternatively to provide the annotations.\napiVersion: dns.gardener.cloud/v1alpha1 kind: DNSAnnotation metadata:  annotations:  dns.gardener.cloud/class: garden  name: test-ingress-annotation  namespace: default spec:  resourceRef:  kind: Ingress  apiVersion: networking.k8s.io/v1  name: test-ingress  namespace: default  annotations:  dns.gardener.cloud/dnsnames: '*'  dns.gardener.cloud/class: garden Note that the DNSAnnotation resource itself needs the dns.gardener.cloud/class=garden annotation. This also only works for annotations known to the DNS shoot service (see Accepted External DNS Records Annotations).\nFor more details, see also DNSAnnotation objects\nAccepted External DNS Records Annotations Here are all of the accepted annotation related to the DNS extension:\n- dns.gardener.cloud/dnsnames # Mandatory, accepts a comma-separated list of DNS names if multiple names are required - dns.gardener.cloud/class # Mandatory, DNS extension class name (usually \"garden\") - dns.gardener.cloud/ttl # Recommended, Time-To-Live of the DNS record - dns.gardener.cloud/cname-lookup-interval # Optional, lookup interval for CNAMEs that must be resolved to IP (in seconds) - dns.gardener.cloud/realms # Optional, for restricting provider access for shoot DNS entries If one of the accepted DNS names is a direct subdomain of the shoot’s ingress domain, this is already handled by the standard wildcard entry for the ingress domain. Therefore, this name should be excluded from the dnsnames list in the annotation. If only this DNS name is configured in the ingress, no explicit DNS entry is required, and the DNS annotations should be omitted at all.\nTroubleshooting General DNS tools To check the DNS resolution, use the nslookup or dig command.\n$ nslookup special.your-domain.com or with dig\n$ dig +short special.example.com Depending on your network settings, you may get a successful response faster using a public DNS server (e.g. 8.8.8.8, 8.8.4.4, or 1.1.1.1)  dig @8.8.8.8 +short special.example.com DNS record events The DNS controller publishes Kubernetes events for the resource which requested the DNS record (Ingress, Service, DNSEntry). These events reveal more information about the DNS requests being processed and are especially useful to check any kind of misconfiguration, e.g. requests for a domain you don’t own.\nEvents for a successfully created DNS record:\n$ kubectl describe service my-service Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal dns-annotation 19s dns-controller-manager special.example.com: dns entry is pending Normal dns-annotation 19s (x3 over 19s) dns-controller-manager special.example.com: dns entry pending: waiting for dns reconciliation Normal dns-annotation 9s (x3 over 10s) dns-controller-manager special.example.com: dns entry active Please note, events vanish after their retention period (usually 1h).\nDNSEntry status DNSEntry resources offer a .status sub-resource which can be used to check the current state of the object.\nStatus of a erroneous DNSEntry.\n status: message: No responsible provider found observedGeneration: 3 provider: remote state: Error References  Understanding DNS Kubernetes Internal DNS DNSEntry API (Golang) Managing Certificates with Gardener  ","categories":"","description":"","excerpt":"Request DNS Names in Shoot Clusters Introduction Within a shoot …","ref":"/docs/extensions/others/gardener-extension-shoot-dns-service/docs/usage/dns_names/","tags":"","title":"DNS Names"},{"body":"DNS Providers Introduction Gardener can manage DNS records on your behalf, so that you can request them via different resource types (see here) within the shoot cluster. The domains for which you are permitted to request records, are however restricted and depend on the DNS provider configuration.\nShoot provider By default, every shoot cluster is equipped with a default provider. It is the very same provider that manages the shoot cluster’s kube-apiserver public DNS record (DNS address in your Kubeconfig).\nkind: Shoot ... dns: domain: shoot.project.default-domain.gardener.cloud You are permitted to request any sub-domain of .dns.domain that is not already taken (e.g. api.shoot.project.default-domain.gardener.cloud, *.ingress.shoot.project.default-domain.gardener.cloud) with this provider.\nAdditional providers If you need to request DNS records for domains not managed by the default provider, additional providers can be configured in the shoot specification. Alternatively, if it is enabled, it can be added as DNSProvider resources to the shoot cluster.\nAdditional providers in the shoot specification To add a providers in the shoot spec, you need set them in the spec.dns.providers list.\nFor example:\nkind: Shoot ... spec:  dns:  domain: shoot.project.default-domain.gardener.cloud  providers:  - secretName: my-aws-account  type: aws-route53  - secretName: my-gcp-account  type: google-clouddns  Please consult the API-Reference to get a complete list of supported fields and configuration options.\n Referenced secrets should exist in the project namespace in the Garden cluster and must comply with the provider specific credentials format. The External-DNS-Management project provides corresponding examples (20-secret-\u003cprovider-name\u003e-credentials.yaml) for known providers.\nAdditional providers as resources in the shoot cluster If it is not enabled globally, you have to enable the feature in the shoot manifest:\nKind: Shoot ... spec:  extensions:  - type: shoot-dns-service  providerConfig:  apiVersion: service.dns.extensions.gardener.cloud/v1alpha1  kind: DNSConfig  dnsProviderReplication:  enabled: true ... To add a provider directly in the shoot cluster, provide a DNSProvider in any namespace together with Secret containing the credentials.\nFor example if the domain is hosted with AWS Route 53 (provider type aws-route53):\napiVersion: dns.gardener.cloud/v1alpha1 kind: DNSProvider metadata:  annotations:  dns.gardener.cloud/class: garden  name: my-own-domain  namespace: my-namespace spec:  type: aws-route53  secretRef:  name: my-own-domain-credentials  domains:  include:  - my.own.domain.com --- apiVersion: v1 kind: Secret metadata:  name: my-own-domain-credentials  namespace: my-namespace type: Opaque data:  # replace '...' with values encoded as base64  AWS_ACCESS_KEY_ID: ...  AWS_SECRET_ACCESS_KEY: ... The External-DNS-Management project provides examples with more details for DNSProviders (30-provider-\u003cprovider-name\u003e.yaml) and credential Secrets (20-secret-\u003cprovider-name\u003e.yaml) at https://github.com/gardener/external-dns-management//examples for all supported provider types.\n","categories":"","description":"","excerpt":"DNS Providers Introduction Gardener can manage DNS records on your …","ref":"/docs/extensions/others/gardener-extension-shoot-dns-service/docs/usage/dns_providers/","tags":"","title":"DNS Providers"},{"body":"Contract: DNSRecord resources Every shoot cluster requires external DNS records that are publicly resolvable. The management of these DNS records requires provider-specific knowledge which is to be developed outside the Gardener’s core repository.\nCurrently, Gardener uses DNSProvider and DNSEntry resources. However, this introduces undesired coupling of Gardener to a controller that does not adhere to the Gardener extension contracts. Because of this, we plan to stop using DNSProvider and DNSEntry resources for Gardener DNS records in the future and use the DNSRecord resources described here instead.\nWhat does Gardener create DNS records for? Internal domain name Every shoot cluster’s kube-apiserver running in the seed is exposed via a load balancer that has a public endpoint (IP or hostname). This endpoint is used by end-users and also by system components (that are running in another network, e.g., the kubelet or kube-proxy) to talk to the cluster. In order to be robust against changes of this endpoint (e.g., caused due to re-creation of the load balancer or move of the DNS record to another seed cluster) Gardener creates a so-called internal domain name for every shoot cluster. The internal domain name is a publicly resolvable DNS record that points to the load balancer of the kube-apiserver. Gardener uses this domain name in the kubeconfigs of all system components, instead of using directly the load balancer endpoint. This way Gardener does not need to recreate all kubeconfigs if the endpoint changes - it just needs to update the DNS record.\nExternal domain name The internal domain name is not configurable by end-users directly but configured by the Gardener administrator. However, end-users usually prefer to have another DNS name, maybe even using their own domain sometimes to access their Kubernetes clusters. Gardener supports that by creating another DNS record, named external domain name, that actually points to the internal domain name. The kubeconfig handed out to end-users does contain this external domain name, i.e., users can access their clusters with the DNS name they like to.\nAs not every end-user has an own domain it is possible for Gardener administrators to configure so-called default domains. If configured, shoots that do not specify a domain explicitly get an external domain name based on a default domain (unless explicitly stated that this shoot should not get an external domain name (.spec.dns.provider=unmanaged).\nIngress domain name (deprecated) Gardener allows to deploy a nginx-ingress-controller into a shoot cluster (deprecated). This controller is exposed via a public load balancer (again, either IP or hostname). Gardener creates a wildcard DNS record pointing to this load balancer. Ingress resources can later use this wildcard DNS record to expose underlying applications.\nWhat needs to be implemented to support a new DNS provider? As part of the shoot flow Gardener will create a number of DNSRecord resources in the seed cluster (one for each of the DNS records mentioned above) that need to be reconciled by an extension controller. This resource contains the following information:\n The DNS provider type (e.g., aws-route53, google-clouddns, …) A reference to a Secret object that contains the provider-specific credentials used to communicate with the provider’s API. The fully qualified domain name (FQDN) of the DNS record, e.g. “api.\u003cshoot domain\u003e”. The DNS record type, one of A, CNAME, or TXT. The DNS record values, that is a list of IP addresses for A records, a single hostname for CNAME records, or a list of texts for TXT records.  Optionally, the DNSRecord resource may contain also the following information:\n The region of the DNS record. If not specified, the region specified in the referenced Secret shall be used. If that is also not specified, the extension controller shall use a certain default region. The DNS hosted zone of the DNS record. If not specified, it shall be determined automatically by the extension controller by getting all hosted zones of the account and searching for the longest zone name that is a suffix of the fully qualified domain name (FQDN) mentioned above. The TTL of the DNS record in seconds. If not specified, it shall be set by the extension controller to 120.  Example DNSRecord:\n--- apiVersion: v1 kind: Secret metadata:  name: dnsrecord-bar-external  namespace: shoot--foo--bar type: Opaque data:  # aws-route53 specific credentials here --- apiVersion: extensions.gardener.cloud/v1alpha1 kind: DNSRecord metadata:  name: dnsrecord-external  namespace: default spec:  type: aws-route53  secretRef:  name: dnsrecord-bar-external  namespace: shoot--foo--bar # region: eu-west-1 # zone: ZFOO  name: api.bar.foo.my-fancy-domain.com  recordType: A  values:  - 1.2.3.4 # ttl: 600 In order to support a new DNS record provider you need to write a controller that watches all DNSRecords with .spec.type=\u003cmy-provider-name\u003e. You can take a look at the below referenced example implementation for the AWS route53 provider.\nKey names in secrets containing provider-specific credentials For compatibility with existing setups, extension controllers shall support two different namings of keys in secrets containing provider-specific credentials:\n The naming used by the external-dns-management DNS controller. For example on AWS, the key names are AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_REGION. The naming used by other provider-specific extension controllers, e.g. for infrastructure. For example on AWS, the key names are accessKeyId, secretAccessKey, and region.  Avoiding reading the DNS hosted zones If the DNS hosted zone is not specified in the DNSRecord resource, during the first reconciliation the extension controller shall determine the correct DNS hosted zone for the specified FQDN and write it to the status of the resource:\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: DNSRecord metadata:  name: dnsrecord-external  namespace: shoot--foo--bar spec:  ... status:  lastOperation: ...  zone: ZFOO On subsequent reconciliations, the extension controller shall use the zone from the status and avoid reading the DNS hosted zones from the provider. If the DNSRecord resource specifies a zone in .spec.zone and the extension controller has written a value to .status.zone, the first one shall be considered with higher priority by the extension controller.\nNon-provider specific information required for DNS record creation Some providers might require further information that is not provider specific but already part of the shoot resource. As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information that is not part of the DNSRecord resource itself.\nUsing DNSRecord resources gardenlet manages DNSRecord resources for all three DNS records mentioned above (internal, external, and ingress). In order to successfully reconcile a shoot with the feature gate enabled, extension controllers for DNSRecord resources for types used in the default, internal and custom domain secrets should be registered via ControllerRegistration resources.\nNote: For compatibility reasons, the spec.dns.providers section is still used to specify additional providers. Only the one marked as primary: true will be used for DNSRecord. All others are considered by the shoot-dns-service extension only (if deployed).\nSupport for DNSRecord resources in the provider extensions The following table contains information about the provider extension version that adds support for DNSRecord resources:\n   Extension Version     provider-alicloud v1.26.0   provider-aws v1.27.0   provider-azure v1.21.0   provider-gcp v1.18.0   provider-openstack v1.21.0   provider-vsphere N/A   provider-equinix-metal N/A   provider-kubevirt N/A   provider-openshift N/A    References and additional resources  DNSRecord API (Golang specification) Sample implementation for the AWS route53 provider  ","categories":"","description":"","excerpt":"Contract: DNSRecord resources Every shoot cluster requires external …","ref":"/docs/gardener/extensions/dnsrecord/","tags":"","title":"DNS Record"},{"body":"DNS Search Path Optimization DNS Search Path Using fully qualified names has some downsides, e.g. it may become harder to move deployments from one landscape to the next. It is far easier and simple to rely on short/local names, which may have different meaning depending on the context they are used in.\nThe DNS search path allows the usage of short/local names. It is an ordered list of DNS suffixes to append to short/local names to create a fully qualified name.\nIf a short/local name should be resolved each entry is appended to it one by one to check whether it can be resolved. The process stops when either the name could be resolved or the DNS search path ends. As the last step after trying the search path, the short/local name is attempted to be resolved on it own.\nDNS Option ndots As explained in the section above, the DNS search path is used for short/local names to create fully qualified names. The DNS option ndots specifies how many dots (.) a name needs to have to be considered fully qualified. For names with less than ndots dots (.), the DNS search path will be applied.\nDNS Search Path, ndots and Kubernetes Kubernetes tries to make it easy/convenient for developers to use name resolution. It provides several means to address a service, most notably by its name directly, using the namespace as suffix, utilizing \u003cnamespace\u003e.svc as suffix or as a fully qualified name as \u003cservice\u003e.\u003cnamespace\u003e.svc.cluster.local (assuming cluster.local to be the cluster domain).\nThis is why the DNS search path is fairly long in Kubernetes, usually consisting of \u003cnamespace\u003e.svc.cluster.local, svc.cluster.local, cluster.local and potentially some additional entries coming from the local network of the cluster. For various reasons, the default ndots value in the context of Kubernetes is with 5 also fairly large. See this comment for a more detailed description.\nDNS Search Path/ndots Problem in Kubernetes As the DNS search path is long and ndots is large, a lot of DNS queries might traverse the DNS search path. This results in an explosion of DNS requests.\nFor example, consider the name resolution of the default kubernetes service kubernetes.default.svc.cluster.local. As this name has only four dots it is not considered a fully qualified name according to the default ndots=5 setting. Therefore, the DNS search path is applied resulting in the following queries being created\n kubernetes.default.svc.cluster.local.some-namespace.svc.cluster.local kubernetes.default.svc.cluster.local.svc.cluster.local kubernetes.default.svc.cluster.local.cluster.local kubernetes.default.svc.cluster.local.network-domain …  In IPv4/IPv6 dual stack systems, the amount of DNS requests may even double as each name is resolved for IPv4 and IPv6.\nGeneral Workarounds/Mitigations Kubernetes provides the capability to set the DNS options for each pod (see Pod DNS config for details). However, this has to be applied for every pod (doing name resolution) to resolve the problem. A mutating webhook may be useful in this regard. Unfortunately, the DNS requirements may be different depending on the workload. Therefore, a general solution may difficult to impossible.\nAnother approach is to use always fully qualified names and append a dot (.) to the name to prevent the name resolution system from using the DNS search path. This might be somewhat counterintuitive as most developers are not used to the trailing dot (.). Furthermore, it makes moving to different landscapes more difficult/error-prone.\nGardener specific Workarounds/Mitigations Gardener allows users to customize their DNS configuration. CoreDNS allows several approaches to deal with the requests generated by the DNS search path. Caching is possible as well as query rewriting. There are also several other plugins available, which may mitigate the situation.\nGardener DNS Query Rewriting As explained above, the application of the DNS search path may lead to the undesired creation of DNS requests. Especially with the default setting of ndots=5, seemingly fully qualified names pointing to services in the cluster may trigger the DNS search path application.\nGardener allows to automatically rewrite some obviously incorrect DNS names, which stem from application of the DNS search path, to the most likely desired name. The feature can be enabled by setting the Gardenlet feature gate CoreDNSQueryRewriting to true:\nfeatureGates:  CoreDNSQueryRewriting: true In case the feature is enabled in the Gardenlet it can be disabled per shoot cluster by setting the annotation alpha.featuregates.shoot.gardener.cloud/core-dns-rewriting-disabled to any value.\nThis will automatically rewrite requests like service.namespace.svc.cluster.local.other-namespace.svc.cluster.local to service.namespace.svc.cluster.local. The same holds true for service.namespace.svc.other-namespace.svc.cluster.local, which will also be rewritten to service.namespace.svc.cluster.local.\nIn case applications also target services for name resolution, which are outside of the cluster and have less than ndots dots, it might be helpful to prevent search path application for them as well. One way to achieve it is by adding them to the commonSuffixes:\n... spec:  ...  systemComponents:  coreDNS:  rewriting:  commonSuffixes:  - gardener.cloud  - github.com ... DNS requests containing a common suffix and ending in \u003cnamespace\u003e.svc.cluster.local are assumed to be incorrect application of the DNS search path. Therefore, they are rewritten to everything ending in the common suffix. For example, www.gardener.cloud.namespace.svc.cluster.local would be rewritten to www.gardener.cloud.\nPlease note that the common suffixes should be long enough and include enough dots (.) to prevent random overlap with other DNS queries. For example, it would be a bad idea to simply put com on the list of common suffixes as there may be services/namespaces, which have com as part of their name. The effect would be seemingly random DNS requests. Gardener enforces at least two dots (.) in the common suffixes.\n","categories":"","description":"","excerpt":"DNS Search Path Optimization DNS Search Path Using fully qualified …","ref":"/docs/gardener/usage/dns-search-path-optimization/","tags":"","title":"DNS Search Path Optimization"},{"body":"Gardener Extension for DNS services  \nProject Gardener implements the automated management and operation of Kubernetes clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.\nRecently, most of the vendor specific logic has been developed in-tree. However, the project has grown to a size where it is very hard to extend, maintain, and test. With GEP-1 we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.\nExtension-Resources Example extension resource:\napiVersion: extensions.gardener.cloud/v1alpha1 kind: Extension metadata:  name: \"extension-dns-service\"  namespace: shoot--project--abc spec:  type: shoot-dns-service How to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the ./dev/kubeconfig file. Static code checks and tests can be executed by running make verify. We are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io Gardener Extensions Golang library GEP-1 (Gardener Enhancement Proposal) on extensibility Extensibility API documentation  ","categories":"","description":"Gardener extension controller for DNS services for shoot clusters","excerpt":"Gardener extension controller for DNS services for shoot clusters","ref":"/docs/extensions/others/gardener-extension-shoot-dns-service/","tags":"","title":"DNS services"},{"body":"Kubernetes dockershim removal What’s happening? With Kubernetes v1.20 the built-in dockershim was deprecated and is scheduled to be removed with v1.24. Don’t Panic! The Kubernetes community has published a blogpost and an FAQ with more information.\nGardener also needs to switch from using the built-in dockershim to containerd. Gardener will not change running Shoot clusters. But changes to the container runtime will be coupled to the K8s version selected by the Shoot:\n starting with K8s version 1.22 Shoots not explicitly selecting a container runtime will get containerd instead of docker. Shoots can still select docker explicitly if needed. starting with K8s version 1.23 docker can no longer be selected.  At this point in time, we have no plans to support other container runtimes, such as cri-o.\nWhat should I do? As a gardener operator:\n add containerd and docker to .spec.machineImages[].versions[].cri.name in your CloudProfile to allow users selecting a container runtime for their Shoots (see below). Note: Please take a look at our detailed information regarding container runtime support in Gardener Operating System Extensions update your cloud provider extensions to avoid a node rollout when a Shoot is configured from cri: nil to cri.name: docker. Note: Please take a look at our detailed information regarding stable Worker node hash support in Gardener Provider Extensions  As a shoot owner:\n check if you have dependencies to the docker container runtime. Note: This is not only about your actual workload, but also concerns ops tooling as well as logging, monitoring and metric agents installed on the nodes test with containerd:  create a new Shoot or add a Worker Pool to an existing one set .spec.provider.workers[].cri.name: containerd for your Shoot   once testing is successful, switch to containerd with your production workload. You don’t need to wait for kubernetes v1.22, containerd is considered production ready as of today if you find dependencies to docker, set .spec.provider.workers[].cri.name: docker explicitly to avoid defaulting to containerd once you update your Shoot to kubernetes v1.22  Timeline  2021-08-04: Kubernetes v1.22 released. Shoots using this version get containerd as default container runtime. Shoots can still select docker explicitly if needed. 2021-12-07: Kubernetes v1.23 released. Shoots using this version can no longer select docker as container runtime. 2022-06-28: Kubernetes v1.21 goes out of maintenance. This is the last version not affected by these changes. Make sure you have tested thoroughly and set the correct configuration for your Shoots! 2022-10-28: Kubernetes v1.22 goes out of maintenance. This is the last version that you can use with docker as container runtime. Make sure you have removed any dependencies to docker as container runtime!  See the official kubernetes documentation for the exact dates for all releases.\nContainer Runtime support in Gardener Operating System Extensions    Operating System docker support containerd support     GardenLinux ✅ \u003e= v0.3.0   Ubuntu ✅ \u003e= v1.4.0   SuSE CHost ✅ \u003e= v1.14.0   CoreOS/FlatCar ✅ \u003e= v1.8.0    Note: If you’re using a different Operating System Extension, start evaluating now if it provides support for containerd. Please refer to our documentation of the operatingsystemconfig contract to understand how to support containerd for an Operating System Extension.\nStable Worker node hash support in Gardener Provider Extensions Upgrade to these versions to avoid a node rollout when a Shoot is configured from cri: nil to cri.name: docker.\n   Provider Extension Stable worker hash support     Alicloud \u003e= 1.26.0   AWS \u003e= 1.27.0   Azure \u003e= 1.21.0   GCP \u003e= 1.18.0   OpenStack \u003e= 1.21.0   vSphere \u003e= 0.11.0    Note: If you’re using a different Provider Extension, start evaluating now if it keeps the worker hash stable when switching from .spec.provider.workers[].cri: nil to .spec.provider.workers[].cri.name: docker. This doesn’t impact functional correctness, however, a node rollout will be triggered when users decide to configure docker for their shoots.\n","categories":"","description":"","excerpt":"Kubernetes dockershim removal What’s happening? With Kubernetes v1.20 …","ref":"/docs/gardener/usage/docker-shim-removal/","tags":"","title":"Docker Shim Removal"},{"body":"Using latest tag for an image Many Dockerfiles use the FROM package:latest pattern at the top of their Dockerfiles to pull the latest image from a Docker registry.\nBad Dockerfile FROMalpineWhile simple, using the latest tag for an image means that your build can suddenly break if that image gets updated. This can lead to problems where everything builds fine locally (because your local cache thinks it is the latest) while a build server may fail, because some Pipelines makes a clean pull on every build. Additionally, troubleshooting can prove to be difficult, since the maintainer of the Dockerfile didn’t actually make any changes.\nGood Dockerfile A digest takes the place of the tag when pulling an image. This will ensure your Dockerfile remains immutable.\nFROMalpine@sha256:7043076348bf5040220df6ad703798fd8593a0918d06d3ce30c6c93be117e430Running apt/apk/yum update Running apt-get install is one of those things virtually every Debian-based Dockerfile will have to satiate some external package requirements your code needs to run. But, using apt-get as an example, comes with its own problems.\napt-get upgrade\nThis will update all your packages to their latests versions, which can be bad because it prevents your Dockerfile from creating consistent, immutable builds.\napt-get update in a different line than running your apt-get install command.\nRunning apt-get update as a single line entry will get cached by the build and won’t actually run every time you need to run apt-get install. Instead, make sure you run apt-get update in the same line with all the packages to ensure all are updated correctly.\nAvoid big container images Building small container image will reduce the time needed to start or restart pods. An image based on the popular Alpine Linux project is much smaller than most distribution based images (~5MB). For most popular languages and products, there are usually an official Alpine Linux image, e.g. golang, nodejs and postgres.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE postgres 9.6.9-alpine 6583932564f8 13 days ago 39.26 MB postgres 9.6 d92dad241eff 13 days ago 235.4 MB postgres 10.4-alpine 93797b0f31f4 13 days ago 39.56 MB In addition, for compiled languages such as Go or C++ which does not requires build time tooling during runtime, it is recommended to avoid build time tooling in the final images. With Docker’s support for multi-stages builds this can be easily achieved with minimal effort. Such an example can be found here.\nGoogle’s distroless image is also a good base image.\n","categories":"","description":"Common Dockerfile pitfalls","excerpt":"Common Dockerfile pitfalls","ref":"/docs/guides/applications/dockerfile_pitfall/","tags":"","title":"Dockerfile Pitfalls"},{"body":"#Documentation\n##Table of Contents\nInstallation  prepare-vsphere.md deployment.md tanzu-vsphere.md  Usage  usage-as-end-user.md usage-as-operator.md  Development  local-setup.md  ","categories":"","description":"","excerpt":"#Documentation\n##Table of Contents\nInstallation  prepare-vsphere.md …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/docs/","tags":"","title":"Docs"},{"body":"Contributing Documentation You are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions:\n Contributions must be licensed under the Creative Commons Attribution 4.0 International License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.  ","categories":"","description":"","excerpt":"Contributing Documentation You are welcome to contribute documentation …","ref":"/contribute/docs/","tags":"","title":"Documentation"},{"body":"Documentation Index Using Out-of-Tree (External) provider support (Recommended) Development  Adding support for a new cloud provider  Using In-Tree provider support (⚠️ DEPRECATED!) Development  Adding support for a new cloud provider Setting up a local development environment Testing and Dependency Management  Usage  Setting up your usage environment Creating/Deleting machines (VM) Maintaining machine replicas using machines-sets Updating machines using machines-deployments  Deployment  Deploying the MCM into a Kubernetes cluster using IN-TREE providers  ","categories":"","description":"","excerpt":"Documentation Index Using Out-of-Tree (External) provider support …","ref":"/docs/other-components/machine-controller-manager/docs/","tags":"","title":"Documentation"},{"body":"GEP-16: Dynamic kubeconfig generation for Shoot clusters Table of Contents  GEP-16: Dynamic kubeconfig generation for Shoot clusters  Table of Contents Summary Motivation  Goals Non-Goals   Proposal Alternatives    Summary This GEP introduces new Shoot subresource called AdminKubeconfigRequest allowing for users to dynamically generate a short-lived kubeconfig that can be used to access the Shoot cluster as cluster-admin.\nMotivation Today, when access to the created Shoot clusters is needed, a kubeconfig with static token credentials is used. This static token is in the system:masters group, granting it cluster-admin privileges. The kubeconfig is generated when the cluster is reconciled, stored in ShootState and replicated in the Project’s namespace in a Secret. End-users can fetch the secret and use the kubeconfig inside it.\nThere are several problems with this approach:\n The token in the kubeconfig does not have any expiration, so end-users have to request a kubeconfig credential rotation if they want revoke the token. There is no user identity in the token. e.g. if user Joe gets the kubeconfig from the Secret, user in that token would be system:cluster-admin and not Joe when accessing the Shoot cluster with it. This makes auditing events in the cluster almost impossible.  Goals   Add a Shoot subresource called adminkubeconfig that would produce a kubeconfig used to access that Shoot cluster.\n  The kubeconfig is not stored in the API Server, but generated for each request.\n  In the AdminKubeconfigRequest send to that subresource, end-users can specify the expiration time of the credential.\n  The identity (user) in the Gardener cluster would be part of the identity (x509 client certificate). E.g if Joe authenticates against the Gardener API server, the generated certificate for Shoot authentication would have the following subject:\n Common Name: Joe Organisation: system:masters    The maximum validity of the certificate can be enforced by setting a flag on the gardener-apiserver.\n  Deprecate and remove the old {shoot-name}.kubeconfig secrets in each Project namespace.\n  Non-Goals  Generate OpenID Connect kubeconfigs  Proposal The gardener-apiserver would serve a new shoots/adminkubeconfig resource. It can only accept CREATE calls and accept AdminKubeconfigRequest. A AdminKubeconfigRequest would have the following structure:\napiVersion: authentication.gardener.cloud/v1alpha1 kind: AdminKubeconfigRequest spec:  expirationSeconds: 3600 Where expirationSeconds is the validity of the certificate in seconds. In this case it would be 1 hour. The maximum validity of a AdminKubeconfigRequest is configured by --shoot-admin-kubeconfig-max-expiration flag in the gardener-apiserver.\nWhen such request is received, the API server would find the ShootState associated with that cluster and generate a kubeconfig. The x509 client certificate would be signed by the Shoot cluster’s CA and the user used in the subject’s common name would be from the User.Info used to make the request.\napiVersion: authentication.gardener.cloud/v1alpha1 kind: AdminKubeconfigRequest spec:  expirationSeconds: 3600 status:  expirationTimestamp: \"2021-02-22T09:06:51Z\"  kubeConfig: # this is normally base64-encoded, but decoded for the example  apiVersion: v1  clusters:  - cluster:  certificate-authority-data: LS0tLS1....  server: https://api.shoot-cluster  name: shoot-cluster-a  contexts:  - context:  cluster: shoot-cluster-a  user: shoot-cluster-a  name: shoot-cluster-a  current-context: shoot-cluster-a  kind: Config  preferences: {}  users:  - name: shoot-cluster-a  user:  client-certificate-data: LS0tLS1CRUd...  client-key-data: LS0tLS1CRUd... New feature gate called AdminKubeconfigRequest enables the above mentioned API in the gardener-apiserver. The old {shoot-name}.kubeconfig is kept, but deprecated and will be removed in the future.\nIn order to get the server’s address used in the kubeconfig, the Shoot’s status should be updated with new entries:\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata:  name: crazy-botany  namespace: garden-dev spec: {} status:  advertisedAddresses:  - name: external  url: https://api.shoot-cluster.external.foo  - name: internal  url: https://api.shoot-cluster.internal.foo  - name: ip  url: https://1.2.3.4 This is needed, because the Gardener API server might not know on which IP address the API server is advertised on (e.g. DNS is disabled).\nIf there are multiple entries, each would be added in a separate cluster in the kubeconfig and a context with the same name would be added as well. The current context would be selected as the first entry in the advertisedAddresses list (.status.advertisedAddresses[0]).\nAlternatives  Dynamic OpenID Connect Webhook Authenticator can be used instead. Ideally cluster admins can enable either or both.  ","categories":"","description":"","excerpt":"GEP-16: Dynamic kubeconfig generation for Shoot clusters Table of …","ref":"/docs/gardener/proposals/16-adminkubeconfig-subresource/","tags":"","title":"Dynamic kubeconfig generation for Shoot clusters"},{"body":"Introduction The example shows how to run a postgres database on Kubernetes and how to dynamically provision and mount the storage volumes needed by the database\nRun postgres database Define the following Kubernetes resources in a yaml file\n PersistentVolumeClaim (PVC) Deployment  PersistentVolumeClaim apiVersion: v1 kind: PersistentVolumeClaim metadata:  name: postgresdb-pvc spec:  accessModes:  - ReadWriteOnce  resources:  requests:  storage: 9Gi  storageClassName: 'default' This defines a PVC using storage class default. Storage classes abstract from the underlying storage provider as well as other parameters, like disk-type (e.g.; solid-state vs standard disks).\nThe default storage class has annotation {“storageclass.kubernetes.io/is-default-class”:“true”}.\n $ kubectl describe sc default Name: default IsDefaultClass: Yes Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"storage.k8s.io/v1beta1\",\"kind\":\"StorageClass\",\"metadata\":{\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"},\"labels\":{\"addonmanager.kubernetes.io/mode\":\"Exists\"},\"name\":\"default\",\"namespace\":\"\"},\"parameters\":{\"type\":\"gp2\"},\"provisioner\":\"kubernetes.io/aws-ebs\"} ,storageclass.kubernetes.io/is-default-class=true Provisioner: kubernetes.io/aws-ebs Parameters: type=gp2 AllowVolumeExpansion: \u003cunset\u003e MountOptions: \u003cnone\u003e ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: \u003cnone\u003e A Persistent Volume is automatically created when it is dynamically provisioned. In following example, the PVC is defined as “postgresdb-pvc”, and a corresponding PV “pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb” is created and associated with pvc automatically.\n$ kubectl create -f .\\postgres_deployment.yaml persistentvolumeclaim \"postgresdb-pvc\" created  $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 3s  $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE postgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 8s Notice that the RECLAIM POLICY is Delete (default value), which is one of the two reclaim policies, the other one is Retain. (A third policy Recycle has been deprecated). In case of Delete, the PV is deleted automatically when the PVC is removed, and the data on the PVC will also be lost.\nOn the other hand, PV with Retain policy will not be deleted when the PVC is removed, and moved to Release status, so that data can be recovered by Administrators later.\nYou can use the kubectl patch command to change the reclaim policy as described here here or use kubectl edit pv \u003cpv-name\u003e to edit online as below:\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 44m  # change the relcaim policy from \"Delete\" to \"Retain\" $ kubectl edit pv pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb persistentvolume \"pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb\" edited  # check the reclaim policy afterwards $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 45m Deployment Once a PVC is created, you can use it in your container via volumes.persistentVolumeClaim.claimName. In below example, pvc postgresdb-pvc is mounted as readable and writable, and in volumeMounts two paths in the container are mounted to subfolders in the volume.\napiVersion: apps/v1 kind: Deployment metadata:  name: postgres  namespace: default  labels:  app: postgres  annotations:  deployment.kubernetes.io/revision: \"1\" spec:  replicas: 1  strategy:  type: RollingUpdate  rollingUpdate:  maxUnavailable: 1  maxSurge: 1  selector:  matchLabels:  app: postgres  template:  metadata:  name: postgres  labels:  app: postgres  spec:  containers:  - name: postgres  image: \"cpettech.docker.repositories.sap.ondemand.com/jtrack_postgres:howto\"  env:  - name: POSTGRES_USER  value: postgres  - name: POSTGRES_PASSWORD  value: p5FVqfuJFrM42cVX9muQXxrC3r8S9yn0zqWnFR6xCoPqxqVQ  - name: POSTGRES_INITDB_XLOGDIR  value: \"/var/log/postgresql/logs\"  ports:  - containerPort: 5432  volumeMounts:  - mountPath: /var/lib/postgresql/data  name: postgre-db  subPath: data # https://github.com/kubernetes/website/pull/2292. Solve the issue of crashing initdb due to non-empty directory (i.e. lost+found)  - mountPath: /var/log/postgresql/logs  name: postgre-db  subPath: logs  volumes:  - name: postgre-db  persistentVolumeClaim:  claimName: postgresdb-pvc  readOnly: false  imagePullSecrets:  - name: cpettechregistry To check the mount points in the container:\n$ kubectl get po NAME READY STATUS RESTARTS AGE postgres-7f485fd768-c5jf9 1/1 Running 0 32m  $ kubectl exec -it postgres-7f485fd768-c5jf9 bash  root@postgres-7f485fd768-c5jf9:/# ls /var/lib/postgresql/data/ base pg_clog pg_dynshmem pg_ident.conf pg_multixact pg_replslot pg_snapshots pg_stat_tmp pg_tblspc PG_VERSION postgresql.auto.conf postmaster.opts global pg_commit_ts pg_hba.conf pg_logical pg_notify pg_serial pg_stat pg_subtrans pg_twophase pg_xlog postgresql.conf postmaster.pid  root@postgres-7f485fd768-c5jf9:/# ls /var/log/postgresql/logs/ 000000010000000000000001 archive_status Deleting a PersistentVolumeClaim In case of “Delete” policy, deleting a PVC will also delete its associated PV. If “Retain” is the reclaim policy, the PV will change status from Bound to Released when PVC is deleted.\n# Check pvc and pv before deletion $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE postgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 50m  $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 50m  # delete pvc $ kubectl delete pvc postgresdb-pvc persistentvolumeclaim \"postgresdb-pvc\" deleted  # pv changed to status \"Released\" $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Released default/postgresdb-pvc default 51m ","categories":"","description":"Running a Postgres database on Kubernetes and dynamically provision and mount the storage volumes needed by the database","excerpt":"Running a Postgres database on Kubernetes and dynamically provision …","ref":"/docs/tutorials/dynamic-pvc/","tags":"","title":"Dynamic Volume Provisioning"},{"body":"Gardener Extension for Networking Filter \nProject Gardener implements the automated management and operation of Kubernetes clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.\nRecently, most of the vendor specific logic has been developed in-tree. However, the project has grown to a size where it is very hard to extend, maintain, and test. With GEP-1 we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.\nThis controller implements Gardener’s extension contract for the shoot-networking-filter extension.\nAn example for a ControllerRegistration resource that can be used to register this controller to Gardener can be found here.\nPlease find more information regarding the extensibility concepts and a detailed proposal here.\nExtension Resources Currently there is nothing to specify in the extension spec.\nExample extension resource:\napiVersion: extensions.gardener.cloud/v1alpha1 kind: Extension metadata:  name: extension-shoot-networking-filter  namespace: shoot--project--abc spec: When an extension resource is reconciled, the extension controller will create a daemonset egress-filter-applier on the shoot containing either a blackholer or firewaller container.\nPlease note, this extension controller relies on the Gardener-Resource-Manager to deploy k8s resources to seed and shoot clusters.\nHow to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start.\nWe are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io GEP-1 (Gardener Enhancement Proposal) on extensibility Extensibility API documentation Gardener Extensions Golang library Gardener API Reference  ","categories":"","description":"Gardener extension controller for egress filtering for shoot clusters","excerpt":"Gardener extension controller for egress filtering for shoot clusters","ref":"/docs/extensions/others/gardener-extension-shoot-networking-filter/","tags":"","title":"Egress filtering"},{"body":"etcd - Key-Value Store for Kubernetes etcd is a strongly consistent key-value store and the most prevalent choice for the Kubernetes persistence layer. All API cluster objects like Pods, Deployments, Secrets, etc. are stored in etcd which makes it an essential part of a Kubernetes control plane.\nShoot cluster persistence Each shoot cluster gets its very own persistence for the control plane. It runs in the shoot namespace on the respective seed cluster. Concretely, there are two etcd instances per shoot cluster which the Kube-Apiserver is configured to use in the following way:\n etcd-main  A store that contains all “cluster critical” or “long-term” objects. These object kinds are typically considered for a backup to prevent any data loss.\n etcd-events  A store that contains all Event objects (events.k8s.io) of a cluster. Events have usually a short retention period, occur frequently but are not essential for a disaster recovery.\nThe setup above prevents both, the critical etcd-main is not flooded by Kubernetes Events as well as backup space is not occupied by non-critical data. This segmentation saves time and resources.\netcd Operator Configuring, maintaining and health-checking etcd is outsourced to a dedicated operator called ETCD Druid. When Gardenlet reconciles a Shoot resource, it creates or updates an Etcd resources in the seed cluster, containing necessary information (backup information, defragmentation schedule, resources, etc.) etcd-druid needs to manage the lifecycle of the desired etcd instance (today main or events). Likewise, when the shoot is deleted, Gardenlet deletes the Etcd resource and ETCD Druid takes care about cleaning up all related objects, e.g. the backing StatefulSet.\nAutoscaling Gardenlet maintains HVPA objects for etcd StatefulSets if the corresponding feature gate is enabled. This enables a vertical scaling for etcd. Downscaling is handled more pessimistic to prevent many subsequent etcd restarts. Thus, for production and infrastructure clusters downscaling is deactivated and for all other clusters lower advertised requests/limits are only applied during a shoot’s maintenance time window.\nBackup If Seeds specify backups for etcd (example), then Gardener and the respective provider extensions are responsible for creating a bucket on the cloud provider’s side (modelled through BackupBucket resource). The bucket stores backups of shoots scheduled on that seed. Furthermore, Gardener creates a BackupEntry which subdivides the bucket and thus makes it possible to store backups of multiple shoot clusters.\nThe etcd-main instance itself is configured to run with a special backup-restore sidecar. It takes care about regularly backing up etcd data and restoring it in case of data loss. More information can be found on the component’s GitHub page https://github.com/gardener/etcd-backup-restore.\nHow long backups are stored in the bucket after a shoot has been deleted, depends on the configured retention period in the Seed resource. Please see this example configuration for more information.\nHousekeeping etcd maintenance tasks must be performed from time to time in order to re-gain database storage and to ensure the system’s reliability. The backup-restore sidecar takes care about this job as well. Gardener chooses a random time within the shoot’s maintenance time to schedule these tasks.\n","categories":"","description":"","excerpt":"etcd - Key-Value Store for Kubernetes etcd is a strongly consistent …","ref":"/docs/gardener/concepts/etcd/","tags":"","title":"Etcd"},{"body":"ETCD Druid  \nBackground Etcd in the control plane of Kubernetes clusters which are managed by Gardener is deployed as a StatefulSet. The statefulset has replica of a pod containing two containers namely, etcd and backup-restore. The etcd container calls components in etcd-backup-restore via REST api to perform data validation before etcd is started. If this validation fails etcd data is restored from the latest snapshot stored in the cloud-provider’s object store. Once etcd has started, the etcd-backup-restore periodically creates full and delta snapshots. It also performs defragmentation of etcd data periodically.\nThe etcd-backup-restore needs as input the cloud-provider information comprising of security credentials to access the object store, the object store bucket name and prefix for the directory used to store snapshots. Currently, for operations like migration and validation, the bash script has to be updated to initiate the operation.\nGoals  Deploy etcd and etcd-backup-restore using an etcd CRD. Support more than one etcd replica. Perform scheduled snapshots. Support operations such as restores, defragmentation and scaling with zero-downtime. Handle cloud-provider specific operation logic. Trigger a full backup on request before volume deletion. Offline compaction of full and delta snapshots stored in object store.  Proposal The existing method of deploying etcd and backup-sidecar as a StatefulSet alleviates the pain of ensuring the pods are live and ready after node crashes. However, deploying etcd as a Statefulset introduces a plethora of challenges. The etcd controller should be smart enough to handle etcd statefulsets taking into account limitations imposed by statefulsets. The controller shall update the status regarding how to target the K8s objects it has created. This field in the status can be leveraged by HVPA to scale etcd resources eventually.\nCRD specification The etcd CRD should contain the information required to create the etcd and backup-restore sidecar in a pod/statefulset.\n---  apiVersion: druid.gardener.cloud/v1alpha1 kind: Etcd metadata:  finalizers:  - druid.gardener.cloud/etcd  name: test  namespace: demo spec:  annotations:  app: etcd-statefulset  garden.sapcloud.io/role: controlplane  networking.gardener.cloud/to-dns: allowed  networking.gardener.cloud/to-private-networks: allowed  networking.gardener.cloud/to-public-networks: allowed  role: test  backup:  deltaSnapshotMemoryLimit: 1Gi  deltaSnapshotPeriod: 300s  fullSnapshotSchedule: 0 */24 * * *  garbageCollectionPeriod: 43200s  garbageCollectionPolicy: Exponential  imageRepository: eu.gcr.io/gardener-project/gardener/etcdbrctl  imageVersion: v0.12.0  port: 8080  resources:  limits:  cpu: 500m  memory: 2Gi  requests:  cpu: 23m  memory: 128Mi  snapstoreTempDir: /var/etcd/data/temp  etcd:  Quota: 8Gi  clientPort: 2379  defragmentationSchedule: 0 */24 * * *  enableTLS: false  imageRepository: eu.gcr.io/gardener-project/gardener/etcd  imageVersion: v3.4.13-bootstrap  initialClusterState: new  initialClusterToken: new  metrics: basic  pullPolicy: IfNotPresent  resources:  limits:  cpu: 2500m  memory: 4Gi  requests:  cpu: 500m  memory: 1000Mi  serverPort: 2380  storageCapacity: 80Gi  storageClass: gardener.cloud-fast  sharedConfig:  autoCompactionMode: periodic  autoCompactionRetention: 30m  labels:  app: etcd-statefulset  garden.sapcloud.io/role: controlplane  networking.gardener.cloud/to-dns: allowed  networking.gardener.cloud/to-private-networks: allowed  networking.gardener.cloud/to-public-networks: allowed  role: test  pvcRetentionPolicy: DeleteAll  replicas: 1  storageCapacity: 80Gi  storageClass: gardener.cloud-fast  store:  storageContainer: test  storageProvider: S3  storePrefix: etcd-test  storeSecret: etcd-backup  tlsClientSecret: etcd-client-tls  tlsServerSecret: etcd-server-tls status:  etcd:  apiVersion: apps/v1  kind: StatefulSet  name: etcd-test Implementation Agenda As first step implement defragmentation during maintenance windows. Subsequently, we will add zero-downtime upgrades and defragmentation.\nWorkflow Deployment workflow Defragmentation workflow ","categories":"","description":"A druid for etcd management in Gardener","excerpt":"A druid for etcd management in Gardener","ref":"/docs/other-components/etcd-druid/","tags":"","title":"etcd Druid"},{"body":"Excess Reserve Capacity  Excess Reserve Capacity  Goal Note Possible Approaches  Approach 1: Enhance Machine-controller-manager to also entertain the excess machines Approach 2: Enhance Cluster-autoscaler by simulating fake pods in it Approach 3: Enhance cluster-autoscaler to support pluggable scaling-events Approach 4: Make intelligent use of Low-priority pods      Goal Currently, autoscaler optimizes the number of machines for a given application-workload. Along with effective resource utilization, this feature brings concern where, many times, when new application instances are created - they don’t find space in existing cluster. This leads the cluster-autoscaler to create new machines via MachineDeployment, which can take from 3-4 minutes to ~10 minutes, for the machine to really come-up and join the cluster. In turn, application-instances have to wait till new machines join the cluster.\nOne of the promising solutions to this issue is Excess Reserve Capacity. Idea is to keep a certain number of machines or percent of resources[cpu/memory] always available, so that new workload, in general, can be scheduled immediately unless huge spike in the workload. Also, the user should be given enough flexibility to choose how many resources or how many machines should be kept alive and non-utilized as this affects the Cost directly.\nNote  We decided to go with Approach-4 which is based on low priority pods. Please find more details here: https://github.com/gardener/gardener/issues/254 Approach-3 looks more promising in long term, we may decide to adopt that in future based on developments/contributions in autoscaler-community.  Possible Approaches Following are the possible approaches, we could think of so far.\nApproach 1: Enhance Machine-controller-manager to also entertain the excess machines   Machine-controller-manager currently takes care of the machines in the shoot cluster starting from creation-deletion-health check to efficient rolling-update of the machines. From the architecture point of view, MachineSet makes sure that X number of machines are always running and healthy. MachineDeployment controller smartly uses this facility to perform rolling-updates.\n  We can expand the scope of MachineDeployment controller to maintain excess number of machines by introducing new parallel independent controller named MachineTaint controller. This will result in MCM to include Machine, MachineSet, MachineDeployment, MachineSafety, MachineTaint controllers. MachineTaint controller does not need to introduce any new CRD - analogy fits where taint-controller also resides into kube-controller-manager.\n  Only Job of MachineTaint controller will be:\n List all the Machines under each MachineDeployment. Maintain taints of noSchedule and noExecute on X latest MachineObjects. There should be an event-based informer mechanism where MachineTaintController gets to know about any Update/Delete/Create event of MachineObjects - in turn, maintains the noSchedule and noExecute taints on all the latest machines. - Why latest machines? - Whenever autoscaler decides to add new machines - essentially ScaleUp event - taints from the older machines are removed and newer machines get the taints. This way X number of Machines immediately becomes free for new pods to be scheduled. - While ScaleDown event, autoscaler specifically mentions which machines should be deleted, and that should not bring any concerns. Though we will have to put proper label/annotation defined by autoscaler on taintedMachines, so that autoscaler does not consider the taintedMachines for deletion while scale-down. * Annotation on tainted node: \"cluster-autoscaler.kubernetes.io/scale-down-disabled\": \"true\"    Implementation Details:\n Expect new optional field ExcessReplicas in MachineDeployment.Spec. MachineDeployment controller now adds both Spec.Replicas and Spec.ExcessReplicas[if provided], and considers that as a standard desiredReplicas. - Current working of MCM will not be affected if ExcessReplicas field is kept nil. MachineController currently reads the NodeObject and sets the MachineConditions in MachineObject. Machine-controller will now also read the taints/labels from the MachineObject - and maintains it on the NodeObject.    We expect cluster-autoscaler to intelligently make use of the provided feature from MCM.\n CA gets the input of min:max:excess from Gardener. CA continues to set the MachineDeployment.Spec.Replicas as usual based on the application-workload. In addition, CA also sets the MachieDeployment.Spec.ExcessReplicas . Corner-case: * CA should decrement the excessReplicas field accordingly when desiredReplicas+excessReplicas on MachineDeployment goes beyond max.    Approach 2: Enhance Cluster-autoscaler by simulating fake pods in it  There was already an attempt by community to support this feature.  Refer for details to: https://github.com/kubernetes/autoscaler/pull/77/files    Approach 3: Enhance cluster-autoscaler to support pluggable scaling-events  Forked version of cluster-autoscaler could be improved to plug-in the algorithm for excess-reserve capacity. Needs further discussion around upstream support. Create golang channel to separate the algorithms to trigger scaling (hard-coded in cluster-autoscaler, currently) from the algorithms about how to to achieve the scaling (already pluggable in cluster-autoscaler). This kind of separation can help us introduce/plug-in new algorithms (such as based node resource utilisation) without affecting existing code-base too much while almost completely re-using the code-base for the actual scaling. Also this approach is not specific to our fork of cluster-autoscaler. It can be made upstream eventually as well.  Approach 4: Make intelligent use of Low-priority pods  Refer to: pod-priority-preemption TL; DR:  High priority pods can preempt the low-priority pods which are already scheduled. Pre-create bunch[equivivalent of X shoot-control-planes] of low-priority pods with priority of zero, then start creating the workload pods with better priority which will reschedule the low-priority pods or otherwise keep them in pending state if the limit for max-machines has reached. This is still alpha feature.    ","categories":"","description":"","excerpt":"Excess Reserve Capacity  Excess Reserve Capacity  Goal Note Possible …","ref":"/docs/other-components/machine-controller-manager/docs/proposals/excess_reserve_capacity/","tags":"","title":"Excess Reserve Capacity"},{"body":"ExposureClasses The Gardener API server provides a cluster-scoped ExposureClass resource. This resource is used to allow exposing the control plane of a Shoot cluster in various network environments like restricted corporate networks, DMZ etc.\nBackground The ExposureClass resource is based on the concept for the RuntimeClass resource in Kubernetes.\nA RuntimeClass abstracts the installation of a certain container runtime (e.g. gVisor, Kata Containers) on all nodes or a subset of the nodes in a Kubernetes cluster. See here.\nIn contrast, an ExposureClass abstracts the ability to expose a Shoot clusters control plane in certain network environments (e.g. corporate networks, DMZ, internet) on all Seeds or a subset of the Seeds.\nExample: RuntimeClass and ExposureClass\napiVersion: node.k8s.io/v1 kind: RuntimeClass metadata:  name: gvisor handler: gvisorconfig # scheduling: # nodeSelector: # env: prod --- kind: ExposureClass metadata:  name: internet handler: internet-config # scheduling: # seedSelector: # matchLabels: # network/env: internet Similar to RuntimeClasses, ExposureClasses also define a .handler field reflecting the name reference for the corresponding CRI configuration of the RuntimeClass and the control plane exposure configuration for the ExposureClass.\nThe CRI handler for RuntimeClasses is usually installed by an administrator (e.g. via a DaemonSet which installs the corresponding container runtime on the nodes). The control plane exposure configuration for ExposureClasses will be also provided by an administrator. This exposure configuration is part of the Gardenlet configuration as this component is responsible to configure the control plane accordingly. See here.\nThe RuntimeClass also supports the selection of a node subset (which have the respective controller runtime binaries installed) for pod scheduling via its .scheduling section. The ExposureClass also supports the selection of a subset of available Seed clusters whose Gardenlet is capable of applying the exposure configuration for the Shoot control plane accordingly via its .scheduling section.\nUsage by a Shoot A Shoot can reference an ExposureClass via the .spec.exposureClassName field.\n⚠️ When creating a Shoot resource, the Gardener scheduler will try to assign the Shoot to a Seed which will host its control plane. The scheduling behaviour can be influenced via the .spec.seedSelectors and/or .spec.tolerations fields in the Shoot. ExposureClasses can contain also scheduling instructions. If a Shoot is referencing an ExposureClass then the scheduling instructions of both will be merged into the Shoot. Those unions of scheduling instructions might lead to a selection of a Seed which is not able to deal with the handler of the ExposureClass and the Shoot creation might end up in an error. In such case, the Shoot scheduling instructions should be revisited to check that they are not interfere with the ones from the ExposureClass. If this is not feasible then the combination with the ExposureClass is might not possible and you need to contact your Gardener administrator.\n Example: Shoot and ExposureClass scheduling instructions merge flow  Assuming there is the following Shoot which is referencing the ExposureClass below:  apiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata:  name: abc  namespace: garden-dev spec:  exposureClassName: abc  seedSelectors:  matchLabels:  env: prod --- apiVersion: core.gardener.cloud/v1alpha1 kind: ExposureClass metadata:  name: abc handler: abc scheduling:  seedSelector:  matchLabels:  network: internal Both seedSelectors would be merged into the Shoot. The result would be the following:  apiVersion: core.gardener.cloud/v1alpha1 kind: Shoot metadata:  name: abc  namespace: garden-dev spec:  exposureClassName: abc  seedSelectors:  matchLabels:  env: prod  network: internal Now the Gardener Scheduler would try to find a Seed with those labels.   If there are no Seeds with matching labels for the seed selector then the Shoot will be unschedulable If there are Seeds with matching labels for the seed selector then the Shoot will be assigned to the best candidate after the scheduling strategy is applied, see here  If the Seed is not able to serve the ExposureClass handler abc then the Shoot will end up in error state If the Seed is able to serve the ExposureClass handler abc then the Shoot will be created     Gardenlet Configuration ExposureClass handlers The Gardenlet is responsible to realize the control plane exposure strategy defined in the referenced ExposureClass of a Shoot.\nTherefore, the GardenletConfiguration can contain an .exposureClassHandlers list with the respective configuration.\nExample of the GardenletConfiguration:\nexposureClassHandlers: - name: internet-config  loadBalancerService:  annotations:  loadbalancer/network: internet - name: internal-config  loadBalancerService:  annotations:  loadbalancer/network: internal  sni:  ingress:  namespace: ingress-internal  labels:  network: internal Each Gardenlet can define how the handler of a certain ExposureClass needs to be implemented for the Seed(s) where it is responsible for.\nThe .name is the name of the handler config and it must match to the .handler in the ExposureClass.\nAll control planes on a Seed are exposed via a load balancer. Either a dedicated one or a central shared one. The load balancer service needs to be configured in a way that it is reachable from the target network environment. Therefore, the configuration of load balancer service need to be specified which can be done via the .loadBalancerService section. The common way to influence load balancer service behaviour is via annotations where the respective cloud-controller-manager will react on and configure the infrastructure load balancer accordingly.\nIn case the Gardenlet runs with activated APIServerSNI feature flag (default), the control planes on a Seed will be exposed via a central load balancer and with Envoy via TLS SNI passthrough proxy. In this case, the Gardenlet will install a dedicated ingress gateway (Envoy + load balancer + respective configuration) for each handler on the Seed. The configuration of the ingress gateways can be controlled via the .sni section in the same way like for the default ingress gateways.\n","categories":"","description":"","excerpt":"ExposureClasses The Gardener API server provides a cluster-scoped …","ref":"/docs/gardener/usage/exposureclasses/","tags":"","title":"ExposureClasses"},{"body":"Contract: Extension resource Gardener defines common procedures which must be passed to create a functioning shoot cluster. Well known steps are represented by special resources like Infrastructure, OperatingSystemConfig or DNS. These resources are typically reconciled by dedicated controllers setting up the infrastructure on the hyperscaler or managing DNS entries, etc..\nBut, some requirements don’t match with those special resources or don’t depend on being proceeded at a specific step in the creation / deletion flow of the shoot. They require a more generic hook. Therefore, Gardener offers the Extension resource.\nWhat is required to register and support an Extension type? Gardener creates one Extension resource per registered extension type in ControllerRegistration per shoot.\napiVersion: core.gardener.cloud/v1beta1 kind: ControllerRegistration metadata:  name: extension-example spec:  resources:  - kind: Extension  type: example  globallyEnabled: true If spec.resources[].globallyEnabled is true then the Extension resources of the given type is created for every shoot cluster. Set to false, the Extension resource is only created if configured in the Shoot manifest.\nThe Extension resources are created in the shoot namespace of the seed cluster.\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: Extension metadata:  name: example  namespace: shoot--foo--bar spec:  type: example  providerConfig: {} Your controller needs to reconcile extensions.extensions.gardener.cloud. Since there can exist multiple Extension resources per shoot, each one holds a spec.type field to let controllers check their responsibility (similar to all other extension resources of Gardener).\nProviderConfig It is possible to provide data in the Shoot resource which is copied to spec.providerConfig of the Extension resource.\n--- apiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata:  name: bar  namespace: garden-foo spec:  extensions:  - type: example  providerConfig:  foo: bar ... results in\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: Extension metadata:  name: example  namespace: shoot--foo--bar spec:  type: example  providerConfig:  foo: bar Shoot reconciliation flow and Extension status Gardener creates Extension resources as part of the Shoot reconciliation. Moreover, it is guaranteed that the Cluster resource exists before the Extension resource is created.\nFor an Extension controller it is crucial to maintain the Extension’s status correctly. At the end Gardener checks the status of each Extension and only reports a successful shoot reconciliation if the state of the last operation is Succeeded.\napiVersion: extensions.gardener.cloud/v1alpha1 kind: Extension metadata:  generation: 1  name: example  namespace: shoot--foo--bar spec:  type: example status:  lastOperation:  state: Succeeded  observedGeneration: 1 ","categories":"","description":"","excerpt":"Contract: Extension resource Gardener defines common procedures which …","ref":"/docs/gardener/extensions/extension/","tags":"","title":"Extension"},{"body":"Packages:\n  extensions.gardener.cloud/v1alpha1   extensions.gardener.cloud/v1alpha1  Package v1alpha1 is the v1alpha1 version of the API.\nResource Types:  BackupBucket  BackupEntry  Bastion  Cluster  ContainerRuntime  ControlPlane  DNSRecord  Extension  Infrastructure  Network  OperatingSystemConfig  Worker  BackupBucket   BackupBucket is a specification for backup bucket.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  BackupBucket    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupBucketSpec     Specification of the BackupBucket. If the object’s deletion timestamp is set, this field is immutable.\n     DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this bucket. This field is immutable.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n       status  BackupBucketStatus     (Optional)     BackupEntry   BackupEntry is a specification for backup Entry.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  BackupEntry    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupEntrySpec     Specification of the BackupEntry. If the object’s deletion timestamp is set, this field is immutable.\n     DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    backupBucketProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) BackupBucketProviderStatus contains the provider status that has been generated by the controller responsible for the BackupBucket resource.\n    region  string    Region is the region of this Entry. This field is immutable.\n    bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n       status  BackupEntryStatus     (Optional)     Bastion   Bastion is a bastion or jump host that is dynamically created to provide SSH access to shoot nodes.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Bastion    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  BastionSpec     Spec is the specification of this Bastion. If the object’s deletion timestamp is set, this field is immutable.\n     DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    userData  []byte    UserData is the base64-encoded user data for the bastion instance. This should contain code to provision the SSH key on the bastion instance. This field is immutable.\n    ingress  []BastionIngressPolicy     Ingress controls from where the created bastion host should be reachable.\n       status  BastionStatus     (Optional) Status is the bastion’s status.\n    Cluster   Cluster is a specification for a Cluster resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Cluster    metadata  Kubernetes meta/v1.ObjectMeta     Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ClusterSpec          cloudProfile  k8s.io/apimachinery/pkg/runtime.RawExtension     CloudProfile is a raw extension field that contains the cloudprofile resource referenced by the shoot that has to be reconciled.\n    seed  k8s.io/apimachinery/pkg/runtime.RawExtension     Seed is a raw extension field that contains the seed resource referenced by the shoot that has to be reconciled.\n    shoot  k8s.io/apimachinery/pkg/runtime.RawExtension     Shoot is a raw extension field that contains the shoot resource that has to be reconciled.\n       ContainerRuntime   ContainerRuntime is a specification for a container runtime resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  ContainerRuntime    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ContainerRuntimeSpec     Specification of the ContainerRuntime. If the object’s deletion timestamp is set, this field is immutable.\n     binaryPath  string    BinaryPath is the Worker’s machine path where container runtime extensions should copy the binaries to.\n    workerPool  ContainerRuntimeWorkerPool     WorkerPool identifies the worker pool of the Shoot. For each worker pool and type, Gardener deploys a ContainerRuntime CRD.\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n       status  ContainerRuntimeStatus     (Optional)     ControlPlane   ControlPlane is a specification for a ControlPlane resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  ControlPlane    metadata  Kubernetes meta/v1.ObjectMeta     Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControlPlaneSpec     Specification of the ControlPlane. If the object’s deletion timestamp is set, this field is immutable.\n     DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  Purpose     (Optional) Purpose contains the data if a cloud provider needs additional components in order to expose the control plane. This field is immutable.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the region of this control plane. This field is immutable.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n       status  ControlPlaneStatus     (Optional)     DNSRecord   DNSRecord is a specification for a DNSRecord resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  DNSRecord    metadata  Kubernetes meta/v1.ObjectMeta     Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  DNSRecordSpec     Specification of the DNSRecord. If the object’s deletion timestamp is set, this field is immutable.\n     DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    region  string    (Optional) Region is the region of this DNS record. If not specified, the region specified in SecretRef will be used. If that is also not specified, the extension controller will use its default region.\n    zone  string    (Optional) Zone is the DNS hosted zone of this DNS record. If not specified, it will be determined automatically by getting all hosted zones of the account and searching for the longest zone name that is a suffix of Name.\n    name  string    Name is the fully qualified domain name, e.g. “api.”. This field is immutable.\n    recordType  DNSRecordType     RecordType is the DNS record type. Only A, CNAME, and TXT records are currently supported. This field is immutable.\n    values  []string    Values is a list of IP addresses for A records, a single hostname for CNAME records, or a list of texts for TXT records.\n    ttl  int64    (Optional) TTL is the time to live in seconds. Defaults to 120.\n       status  DNSRecordStatus     (Optional)     Extension   Extension is a specification for a Extension resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Extension    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ExtensionSpec     Specification of the Extension. If the object’s deletion timestamp is set, this field is immutable.\n     DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n       status  ExtensionStatus     (Optional)     Infrastructure   Infrastructure is a specification for cloud provider infrastructure.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Infrastructure    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  InfrastructureSpec     Specification of the Infrastructure. If the object’s deletion timestamp is set, this field is immutable.\n     DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this infrastructure. This field is immutable.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with this infrastructure.\n       status  InfrastructureStatus     (Optional)     Network   Network is the specification for cluster networking.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Network    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  NetworkSpec     Specification of the Network. If the object’s deletion timestamp is set, this field is immutable.\n     DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    podCIDR  string    PodCIDR defines the CIDR that will be used for pods. This field is immutable.\n    serviceCIDR  string    ServiceCIDR defines the CIDR that will be used for services. This field is immutable.\n       status  NetworkStatus     (Optional)     OperatingSystemConfig   OperatingSystemConfig is a specification for a OperatingSystemConfig resource\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  OperatingSystemConfig    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  OperatingSystemConfigSpec     Specification of the OperatingSystemConfig. If the object’s deletion timestamp is set, this field is immutable.\n     criConfig  CRIConfig     (Optional) CRI config is a structure contains configurations of the CRI library\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  OperatingSystemConfigPurpose     Purpose describes how the result of this OperatingSystemConfig is used by Gardener. Either it gets sent to the Worker extension controller to bootstrap a VM, or it is downloaded by the cloud-config-downloader script already running on a bootstrapped VM. This field is immutable.\n    reloadConfigFilePath  string    (Optional) ReloadConfigFilePath is the path to the generated operating system configuration. If set, controllers are asked to use it when determining the .status.command of this resource. For example, if for CoreOS the reload-path might be “/var/lib/config”; then the controller shall set .status.command to “/usr/bin/coreos-cloudinit –from-file=/var/lib/config”.\n    units  []Unit     (Optional) Units is a list of unit for the operating system configuration (usually, a systemd unit).\n    files  []File     (Optional) Files is a list of files that should get written to the host’s file system.\n       status  OperatingSystemConfigStatus     (Optional)     Worker   Worker is a specification for a Worker resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Worker    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  WorkerSpec     Specification of the Worker. If the object’s deletion timestamp is set, this field is immutable.\n     DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus is a raw extension field that contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the name of the region where the worker pool should be deployed to. This field is immutable.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with these workers.\n    pools  []WorkerPool     Pools is a list of worker pools.\n       status  WorkerStatus     (Optional)     BackupBucketSpec   (Appears on: BackupBucket)  BackupBucketSpec is the spec for an BackupBucket resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this bucket. This field is immutable.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    BackupBucketStatus   (Appears on: BackupBucket)  BackupBucketStatus is the status for an BackupBucket resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    generatedSecretRef  Kubernetes core/v1.SecretReference     (Optional) GeneratedSecretRef is reference to the secret generated by backup bucket, which will have object store specific credentials.\n    BackupEntrySpec   (Appears on: BackupEntry)  BackupEntrySpec is the spec for an BackupEntry resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    backupBucketProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) BackupBucketProviderStatus contains the provider status that has been generated by the controller responsible for the BackupBucket resource.\n    region  string    Region is the region of this Entry. This field is immutable.\n    bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    BackupEntryStatus   (Appears on: BackupEntry)  BackupEntryStatus is the status for an BackupEntry resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    BastionIngressPolicy   (Appears on: BastionSpec)  BastionIngressPolicy represents an ingress policy for SSH bastion hosts.\n   Field Description      ipBlock  Kubernetes networking/v1.IPBlock     IPBlock defines an IP block that is allowed to access the bastion.\n    BastionSpec   (Appears on: Bastion)  BastionSpec contains the specification for an SSH bastion host.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    userData  []byte    UserData is the base64-encoded user data for the bastion instance. This should contain code to provision the SSH key on the bastion instance. This field is immutable.\n    ingress  []BastionIngressPolicy     Ingress controls from where the created bastion host should be reachable.\n    BastionStatus   (Appears on: Bastion)  BastionStatus holds the most recently observed status of the Bastion.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    ingress  Kubernetes core/v1.LoadBalancerIngress     (Optional) Ingress is the external IP and/or hostname of the bastion host.\n    CRIConfig   (Appears on: OperatingSystemConfigSpec)  CRIConfig contains configurations of the CRI library.\n   Field Description      name  CRIName     Name is a mandatory string containing the name of the CRI library. Supported values are docker and containerd.\n    CRIName (string alias)\n  (Appears on: CRIConfig)  CRIName is a type alias for the CRI name string.\nCloudConfig   (Appears on: OperatingSystemConfigStatus)  CloudConfig contains the generated output for the given operating system config spec. It contains a reference to a secret as the result may contain confidential data.\n   Field Description      secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    ClusterSpec   (Appears on: Cluster)  ClusterSpec is the spec for a Cluster resource.\n   Field Description      cloudProfile  k8s.io/apimachinery/pkg/runtime.RawExtension     CloudProfile is a raw extension field that contains the cloudprofile resource referenced by the shoot that has to be reconciled.\n    seed  k8s.io/apimachinery/pkg/runtime.RawExtension     Seed is a raw extension field that contains the seed resource referenced by the shoot that has to be reconciled.\n    shoot  k8s.io/apimachinery/pkg/runtime.RawExtension     Shoot is a raw extension field that contains the shoot resource that has to be reconciled.\n    ContainerRuntimeSpec   (Appears on: ContainerRuntime)  ContainerRuntimeSpec is the spec for a ContainerRuntime resource.\n   Field Description      binaryPath  string    BinaryPath is the Worker’s machine path where container runtime extensions should copy the binaries to.\n    workerPool  ContainerRuntimeWorkerPool     WorkerPool identifies the worker pool of the Shoot. For each worker pool and type, Gardener deploys a ContainerRuntime CRD.\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    ContainerRuntimeStatus   (Appears on: ContainerRuntime)  ContainerRuntimeStatus is the status for a ContainerRuntime resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    ContainerRuntimeWorkerPool   (Appears on: ContainerRuntimeSpec)  ContainerRuntimeWorkerPool identifies a Shoot worker pool by its name and selector.\n   Field Description      name  string    Name specifies the name of the worker pool the container runtime should be available for. This field is immutable.\n    selector  Kubernetes meta/v1.LabelSelector     Selector is the label selector used by the extension to match the nodes belonging to the worker pool.\n    ControlPlaneSpec   (Appears on: ControlPlane)  ControlPlaneSpec is the spec of a ControlPlane resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  Purpose     (Optional) Purpose contains the data if a cloud provider needs additional components in order to expose the control plane. This field is immutable.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the region of this control plane. This field is immutable.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    ControlPlaneStatus   (Appears on: ControlPlane)  ControlPlaneStatus is the status of a ControlPlane resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    DNSRecordSpec   (Appears on: DNSRecord)  DNSRecordSpec is the spec of a DNSRecord resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    region  string    (Optional) Region is the region of this DNS record. If not specified, the region specified in SecretRef will be used. If that is also not specified, the extension controller will use its default region.\n    zone  string    (Optional) Zone is the DNS hosted zone of this DNS record. If not specified, it will be determined automatically by getting all hosted zones of the account and searching for the longest zone name that is a suffix of Name.\n    name  string    Name is the fully qualified domain name, e.g. “api.”. This field is immutable.\n    recordType  DNSRecordType     RecordType is the DNS record type. Only A, CNAME, and TXT records are currently supported. This field is immutable.\n    values  []string    Values is a list of IP addresses for A records, a single hostname for CNAME records, or a list of texts for TXT records.\n    ttl  int64    (Optional) TTL is the time to live in seconds. Defaults to 120.\n    DNSRecordStatus   (Appears on: DNSRecord)  DNSRecordStatus is the status of a DNSRecord resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    zone  string    (Optional) Zone is the DNS hosted zone of this DNS record.\n    DNSRecordType (string alias)\n  (Appears on: DNSRecordSpec)  DNSRecordType is a string alias.\nDataVolume   (Appears on: WorkerPool)  DataVolume contains information about a data volume.\n   Field Description      name  string    Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    Size is the of the root volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    DefaultSpec   (Appears on: BackupBucketSpec, BackupEntrySpec, BastionSpec, ContainerRuntimeSpec, ControlPlaneSpec, DNSRecordSpec, ExtensionSpec, InfrastructureSpec, NetworkSpec, OperatingSystemConfigSpec, WorkerSpec)  DefaultSpec contains common status fields for every extension resource.\n   Field Description      type  string    Type contains the instance of the resource’s kind.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the provider specific configuration.\n    DefaultStatus   (Appears on: BackupBucketStatus, BackupEntryStatus, BastionStatus, ContainerRuntimeStatus, ControlPlaneStatus, DNSRecordStatus, ExtensionStatus, InfrastructureStatus, NetworkStatus, OperatingSystemConfigStatus, WorkerStatus)  DefaultStatus contains common status fields for every extension resource.\n   Field Description      providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus contains provider-specific status.\n    conditions  []github.com/gardener/gardener/pkg/apis/core/v1beta1.Condition     (Optional) Conditions represents the latest available observations of a Seed’s current state.\n    lastError  github.com/gardener/gardener/pkg/apis/core/v1beta1.LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    lastOperation  github.com/gardener/gardener/pkg/apis/core/v1beta1.LastOperation     (Optional) LastOperation holds information about the last operation on the resource.\n    observedGeneration  int64    ObservedGeneration is the most recent generation observed for this resource.\n    state  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) State can be filled by the operating controller with what ever data it needs.\n    resources  []github.com/gardener/gardener/pkg/apis/core/v1beta1.NamedResourceReference     (Optional) Resources holds a list of named resource references that can be referred to in the state by their names.\n    DropIn   (Appears on: Unit)  DropIn is a drop-in configuration for a systemd unit.\n   Field Description      name  string    Name is the name of the drop-in.\n    content  string    Content is the content of the drop-in.\n    ExtensionSpec   (Appears on: Extension)  ExtensionSpec is the spec for a Extension resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    ExtensionStatus   (Appears on: Extension)  ExtensionStatus is the status for a Extension resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    File   (Appears on: OperatingSystemConfigSpec)  File is a file that should get written to the host’s file system. The content can either be inlined or referenced from a secret in the same namespace.\n   Field Description      path  string    Path is the path of the file system where the file should get written to.\n    permissions  int32    (Optional) Permissions describes with which permissions the file should get written to the file system. Should be defaulted to octal 0644.\n    content  FileContent     Content describe the file’s content.\n    FileCodecID (string alias)\n  FileCodecID is the id of a FileCodec for cloud-init scripts.\nFileContent   (Appears on: File)  FileContent can either reference a secret or contain inline configuration.\n   Field Description      secretRef  FileContentSecretRef     (Optional) SecretRef is a struct that contains information about the referenced secret.\n    inline  FileContentInline     (Optional) Inline is a struct that contains information about the inlined data.\n    transmitUnencoded  bool    (Optional) TransmitUnencoded set to true will ensure that the os-extension does not encode the file content when sent to the node. This for example can be used to manipulate the clear-text content before it reaches the node.\n    FileContentInline   (Appears on: FileContent)  FileContentInline contains keys for inlining a file content’s data and encoding.\n   Field Description      encoding  string    Encoding is the file’s encoding (e.g. base64).\n    data  string    Data is the file’s data.\n    FileContentSecretRef   (Appears on: FileContent)  FileContentSecretRef contains keys for referencing a file content’s data from a secret in the same namespace.\n   Field Description      name  string    Name is the name of the secret.\n    dataKey  string    DataKey is the key in the secret’s .data field that should be read.\n    InfrastructureSpec   (Appears on: Infrastructure)  InfrastructureSpec is the spec for an Infrastructure resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this infrastructure. This field is immutable.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with this infrastructure.\n    InfrastructureStatus   (Appears on: Infrastructure)  InfrastructureStatus is the status for an Infrastructure resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    nodesCIDR  string    (Optional) NodesCIDR is the CIDR of the node network that was optionally created by the acting extension controller. This might be needed in environments in which the CIDR for the network for the shoot worker node cannot be statically defined in the Shoot resource but must be computed dynamically.\n    MachineDeployment   (Appears on: WorkerStatus)  MachineDeployment is a created machine deployment.\n   Field Description      name  string    Name is the name of the MachineDeployment resource.\n    minimum  int32    Minimum is the minimum number for this machine deployment.\n    maximum  int32    Maximum is the maximum number for this machine deployment.\n    MachineImage   (Appears on: WorkerPool)  MachineImage contains logical information about the name and the version of the machie image that should be used. The logical information must be mapped to the provider-specific information (e.g., AMIs, …) by the provider itself.\n   Field Description      name  string    Name is the logical name of the machine image.\n    version  string    Version is the version of the machine image.\n    NetworkSpec   (Appears on: Network)  NetworkSpec is the spec for an Network resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    podCIDR  string    PodCIDR defines the CIDR that will be used for pods. This field is immutable.\n    serviceCIDR  string    ServiceCIDR defines the CIDR that will be used for services. This field is immutable.\n    NetworkStatus   (Appears on: Network)  NetworkStatus is the status for an Network resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    NodeTemplate   (Appears on: WorkerPool)  NodeTemplate contains information about the expected node properties.\n   Field Description      capacity  Kubernetes core/v1.ResourceList     Capacity represents the expected Node capacity.\n    Object   Object is an extension object resource.\nOperatingSystemConfigPurpose (string alias)\n  (Appears on: OperatingSystemConfigSpec)  OperatingSystemConfigPurpose is a string alias.\nOperatingSystemConfigSpec   (Appears on: OperatingSystemConfig)  OperatingSystemConfigSpec is the spec for a OperatingSystemConfig resource.\n   Field Description      criConfig  CRIConfig     (Optional) CRI config is a structure contains configurations of the CRI library\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  OperatingSystemConfigPurpose     Purpose describes how the result of this OperatingSystemConfig is used by Gardener. Either it gets sent to the Worker extension controller to bootstrap a VM, or it is downloaded by the cloud-config-downloader script already running on a bootstrapped VM. This field is immutable.\n    reloadConfigFilePath  string    (Optional) ReloadConfigFilePath is the path to the generated operating system configuration. If set, controllers are asked to use it when determining the .status.command of this resource. For example, if for CoreOS the reload-path might be “/var/lib/config”; then the controller shall set .status.command to “/usr/bin/coreos-cloudinit –from-file=/var/lib/config”.\n    units  []Unit     (Optional) Units is a list of unit for the operating system configuration (usually, a systemd unit).\n    files  []File     (Optional) Files is a list of files that should get written to the host’s file system.\n    OperatingSystemConfigStatus   (Appears on: OperatingSystemConfig)  OperatingSystemConfigStatus is the status for a OperatingSystemConfig resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    cloudConfig  CloudConfig     (Optional) CloudConfig is a structure for containing the generated output for the given operating system config spec. It contains a reference to a secret as the result may contain confidential data.\n    command  string    (Optional) Command is the command whose execution renews/reloads the cloud config on an existing VM, e.g. “/usr/bin/reload-cloud-config -from-file=”. The  is optionally provided by Gardener in the .spec.reloadConfigFilePath field.\n    units  []string    (Optional) Units is a list of systemd unit names that are part of the generated Cloud Config and shall be restarted when a new version has been downloaded.\n    Purpose (string alias)\n  (Appears on: ControlPlaneSpec)  Purpose is a string alias.\nSpec   Spec is the spec section of an Object.\nStatus   Status is the status of an Object.\nUnit   (Appears on: OperatingSystemConfigSpec)  Unit is a unit for the operating system configuration (usually, a systemd unit).\n   Field Description      name  string    Name is the name of a unit.\n    command  string    (Optional) Command is the unit’s command.\n    enable  bool    (Optional) Enable describes whether the unit is enabled or not.\n    content  string    (Optional) Content is the unit’s content.\n    dropIns  []DropIn     (Optional) DropIns is a list of drop-ins for this unit.\n    Volume   (Appears on: WorkerPool)  Volume contains information about the root disks that should be used for worker pools.\n   Field Description      name  string    (Optional) Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    Size is the of the root volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    WorkerPool   (Appears on: WorkerSpec)  WorkerPool is the definition of a specific worker pool.\n   Field Description      machineType  string    MachineType contains information about the machine type that should be used for this worker pool.\n    maximum  int32    Maximum is the maximum size of the worker pool.\n    maxSurge  k8s.io/apimachinery/pkg/util/intstr.IntOrString     MaxSurge is maximum number of VMs that are created during an update.\n    maxUnavailable  k8s.io/apimachinery/pkg/util/intstr.IntOrString     MaxUnavailable is the maximum number of VMs that can be unavailable during an update.\n    annotations  map[string]string    (Optional) Annotations is a map of key/value pairs for annotations for all the Node objects in this worker pool.\n    labels  map[string]string    (Optional) Labels is a map of key/value pairs for labels for all the Node objects in this worker pool.\n    taints  []Kubernetes core/v1.Taint     (Optional) Taints is a list of taints for all the Node objects in this worker pool.\n    machineImage  MachineImage     MachineImage contains logical information about the name and the version of the machie image that should be used. The logical information must be mapped to the provider-specific information (e.g., AMIs, …) by the provider itself.\n    minimum  int32    Minimum is the minimum size of the worker pool.\n    name  string    Name is the name of this worker pool.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is a provider specific configuration for the worker pool.\n    userData  []byte    UserData is a base64-encoded string that contains the data that is sent to the provider’s APIs when a new machine/VM that is part of this worker pool shall be spawned.\n    volume  Volume     (Optional) Volume contains information about the root disks that should be used for this worker pool.\n    dataVolumes  []DataVolume     (Optional) DataVolumes contains a list of additional worker volumes.\n    kubeletDataVolumeName  string    (Optional) KubeletDataVolumeName contains the name of a dataVolume that should be used for storing kubelet state.\n    zones  []string    (Optional) Zones contains information about availability zones for this worker pool.\n    machineControllerManager  github.com/gardener/gardener/pkg/apis/core/v1beta1.MachineControllerManagerSettings     (Optional) MachineControllerManagerSettings contains configurations for different worker-pools. Eg. MachineDrainTimeout, MachineHealthTimeout.\n    kubernetesVersion  string    (Optional) KubernetesVersion is the kubernetes version in this worker pool\n    nodeTemplate  NodeTemplate     (Optional) NodeTemplate contains resource information of the machine which is used by Cluster Autoscaler to generate nodeTemplate during scaling a nodeGroup from zero\n    architecture  string    (Optional) Architecture is the CPU architecture of the worker pool machines and machine image.\n    WorkerSpec   (Appears on: Worker)  WorkerSpec is the spec for a Worker resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus is a raw extension field that contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the name of the region where the worker pool should be deployed to. This field is immutable.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with these workers.\n    pools  []WorkerPool     Pools is a list of worker pools.\n    WorkerStatus   (Appears on: Worker)  WorkerStatus is the status for a Worker resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    machineDeployments  []MachineDeployment     MachineDeployments is a list of created machine deployments. It will be used to e.g. configure the cluster-autoscaler properly.\n      Generated with gen-crd-api-reference-docs \n","categories":"","description":"","excerpt":"Packages:\n  extensions.gardener.cloud/v1alpha1 …","ref":"/docs/gardener/api-reference/extensions/","tags":"","title":"Extensions"},{"body":"Feature Gates in Gardener This page contains an overview of the various feature gates an administrator can specify on different Gardener components.\nOverview Feature gates are a set of key=value pairs that describe Gardener features. You can turn these features on or off using the a component configuration file for a specific component.\nEach Gardener component lets you enable or disable a set of feature gates that are relevant to that component. For example this is the configuration of the gardenlet component.\nThe following tables are a summary of the feature gates that you can set on different Gardener components.\n The “Since” column contains the Gardener release when a feature is introduced or its release stage is changed. The “Until” column, if not empty, contains the last Gardener release in which you can still use a feature gate. If a feature is in the Alpha or Beta state, you can find the feature listed in the Alpha/Beta feature gate table. If a feature is stable you can find all stages for that feature listed in the Graduated/Deprecated feature gate table. The Graduated/Deprecated feature gate table also lists deprecated and withdrawn features.  Feature gates for Alpha or Beta features    Feature Default Stage Since Until     HVPA false Alpha 0.31    HVPAForShootedSeed false Alpha 0.32    ManagedIstio false Alpha 1.5 1.18   ManagedIstio true Beta 1.19    ManagedIstio (deprecated) true Beta 1.48    APIServerSNI false Alpha 1.7 1.18   APIServerSNI true Beta 1.19    APIServerSNI (deprecated) true Beta 1.48    SeedChange false Alpha 1.12 1.52   SeedChange true Beta 1.53    ReversedVPN false Alpha 1.22 1.41   ReversedVPN true Beta 1.42    CopyEtcdBackupsDuringControlPlaneMigration false Alpha 1.37 1.52   CopyEtcdBackupsDuringControlPlaneMigration true Beta 1.53    ForceRestore false Alpha 1.39    HAControlPlanes false Alpha 1.49    DefaultSeccompProfile false Alpha 1.54    CoreDNSQueryRewriting false Alpha 1.55     Feature gates for graduated or deprecated features    Feature Default Stage Since Until     NodeLocalDNS false Alpha 1.7    NodeLocalDNS  Removed 1.26    KonnectivityTunnel false Alpha 1.6    KonnectivityTunnel  Removed 1.27    MountHostCADirectories false Alpha 1.11 1.25   MountHostCADirectories true Beta 1.26 1.27   MountHostCADirectories true GA 1.27    MountHostCADirectories  Removed 1.30    DisallowKubeconfigRotationForShootInDeletion false Alpha 1.28 1.31   DisallowKubeconfigRotationForShootInDeletion true Beta 1.32 1.35   DisallowKubeconfigRotationForShootInDeletion true GA 1.36    DisallowKubeconfigRotationForShootInDeletion  Removed 1.38    Logging false Alpha 0.13 1.40   Logging  Removed 1.41    AdminKubeconfigRequest false Alpha 1.24 1.38   AdminKubeconfigRequest true Beta 1.39 1.41   AdminKubeconfigRequest true GA 1.42 1.49   AdminKubeconfigRequest  Removed 1.50    UseDNSRecords false Alpha 1.27 1.38   UseDNSRecords true Beta 1.39 1.43   UseDNSRecords true GA 1.44 1.49   UseDNSRecords  Removed 1.50    CachedRuntimeClients false Alpha 1.7 1.33   CachedRuntimeClients true Beta 1.34 1.44   CachedRuntimeClients true GA 1.45 1.49   CachedRuntimeClients  Removed 1.50    DenyInvalidExtensionResources false Alpha 1.31 1.41   DenyInvalidExtensionResources true Beta 1.42 1.44   DenyInvalidExtensionResources true GA 1.45 1.49   DenyInvalidExtensionResources  Removed 1.50    RotateSSHKeypairOnMaintenance false Alpha 1.28 1.44   RotateSSHKeypairOnMaintenance true Beta 1.45 1.47   RotateSSHKeypairOnMaintenance (deprecated) false Beta 1.48 1.50   RotateSSHKeypairOnMaintenance (deprecated)  Removed 1.51    ShootMaxTokenExpirationOverwrite false Alpha 1.43 1.44   ShootMaxTokenExpirationOverwrite true Beta 1.45 1.47   ShootMaxTokenExpirationOverwrite true GA 1.48 1.50   ShootMaxTokenExpirationOverwrite  Removed 1.51    ShootMaxTokenExpirationValidation false Alpha 1.43 1.45   ShootMaxTokenExpirationValidation true Beta 1.46 1.47   ShootMaxTokenExpirationValidation true GA 1.48 1.50   ShootMaxTokenExpirationValidation  Removed 1.51    WorkerPoolKubernetesVersion false Alpha 1.35 1.45   WorkerPoolKubernetesVersion true Beta 1.46 1.49   WorkerPoolKubernetesVersion true GA 1.50 1.51   WorkerPoolKubernetesVersion  Removed 1.52    DisableDNSProviderManagement false Alpha 1.41 1.49   DisableDNSProviderManagement true Beta 1.50 1.51   DisableDNSProviderManagement true GA 1.52    SecretBindingProviderValidation false Alpha 1.38 1.50   SecretBindingProviderValidation true Beta 1.51 1.52   SecretBindingProviderValidation true GA 1.53 1.54   SecretBindingProviderValidation  Removed 1.55    SeedKubeScheduler false Alpha 1.15 1.54   SeedKubeScheduler false Deprecated 1.55    ShootCARotation false Alpha 1.42 1.50   ShootCARotation true Beta 1.51 1.56   ShootCARotation true GA 1.57    ShootSARotation false Alpha 1.48 1.50   ShootSARotation true Beta 1.51 1.56   ShootSARotation true GA 1.57     Using a feature A feature can be in Alpha, Beta or GA stage. An Alpha feature means:\n Disabled by default. Might be buggy. Enabling the feature may expose bugs. Support for feature may be dropped at any time without notice. The API may change in incompatible ways in a later software release without notice. Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support.  A Beta feature means:\n Enabled by default. The feature is well tested. Enabling the feature is considered safe. Support for the overall feature will not be dropped, though details may change. The schema and/or semantics of objects may change in incompatible ways in a subsequent beta or stable release. When this happens, we will provide instructions for migrating to the next version. This may require deleting, editing, and re-creating API objects. The editing process may require some thought. This may require downtime for applications that rely on the feature. Recommended for only non-critical uses because of potential for incompatible changes in subsequent releases.   Please do try Beta features and give feedback on them! After they exit beta, it may not be practical for us to make more changes.\n A General Availability (GA) feature is also referred to as a stable feature. It means:\n The feature is always enabled; you cannot disable it. The corresponding feature gate is no longer needed. Stable versions of features will appear in released software for many subsequent versions.  List of Feature Gates    Feature Relevant Components Description     HVPA gardenlet Enables simultaneous horizontal and vertical scaling in Seed Clusters.   HVPAForShootedSeed gardenlet Enables simultaneous horizontal and vertical scaling in managed seed (aka “shooted seed”) clusters.   ManagedIstio (deprecated) gardenlet Enables a Gardener-tailored Istio in each Seed cluster. Disable this feature if Istio is already installed in the cluster. Istio is not automatically removed if this feature is disabled. See the detailed documentation for more information.   APIServerSNI (deprecated) gardenlet Enables only one LoadBalancer to be used for every Shoot cluster API server in a Seed. Enable this feature when ManagedIstio is enabled or Istio is manually deployed in Seed cluster. See GEP-8 for more details.   SeedChange gardener-apiserver Enables updating the spec.seedName field during shoot validation from a non-empty value in order to trigger shoot control plane migration.   SeedKubeScheduler gardenlet Adds custom kube-scheduler in gardener-kube-scheduler namespace. It schedules pods with scheduler name gardener-kube-scheduler on Nodes with higher resource utilization.   ReversedVPN gardenlet Reverses the connection setup of the vpn tunnel between the Seed and the Shoot cluster(s). It allows Seed and Shoot clusters to be in different networks with only direct access in one direction (Shoot -\u003e Seed). In addition to that, it reduces the amount of load balancers required, i.e. no load balancers are required for the vpn tunnel anymore. It requires APIServerSNI and kubernetes version 1.18 or higher to work. Details can be found in GEP-14.   CopyEtcdBackupsDuringControlPlaneMigration gardenlet Enables the copy of etcd backups from the object store of the source seed to the object store of the destination seed during control plane migration.   SecretBindingProviderValidation gardener-apiserver Enables validations on Gardener API server that:\n- requires the provider type of a SecretBinding to be set (on SecretBinding creation)\n- requires the SecretBinding provider type to match the Shoot provider type (on Shoot creation)\n- enforces immutability on the provider type of a SecretBinding   ForceRestore gardenlet Enables forcing the shoot’s restoration to the destination seed during control plane migration if the preparation for migration in the source seed is not finished after a certain grace period and is considered unlikely to succeed (falling back to the control plane migration “bad case” scenario). If you enable this feature gate, make sure to also enable CopyEtcdBackupsDuringControlPlaneMigration.   DisableDNSProviderManagement gardenlet Disables management of dns.gardener.cloud/v1alpha1.DNSProvider resources. In this case, the shoot-dns-service extension will take this over if it is installed.   ShootCARotation gardener-apiserver, gardenlet Enables the feature to trigger automated CA rotation for shoot clusters.   ShootSARotation gardener-apiserver, gardenlet Enables the feature to trigger automated service account signing key rotation for shoot clusters.   HAControlPlanes gardener-scheduler, gardenlet HAControlPlanes allows shoot control planes to be run in high availability mode.   WorkerPoolKubernetesVersion gardener-apiserver Allows to overwrite the Kubernetes version used for shoot clusters per worker pool (see this document)   DefaultSeccompProfile gardenlet Enables the defaulting of the seccomp profile for Gardener managed workload in the seed to RuntimeDefault.   CoreDNSQueryRewriting gardenlet Enables automatic DNS query rewriting in shoot cluster’s CoreDNS to shortcut name resolution of fully qualified in-cluster and out-of-cluster names, which follow a user-defined pattern. Details can be found in DNS Search Path Optimization.    ","categories":"","description":"","excerpt":"Feature Gates in Gardener This page contains an overview of the …","ref":"/docs/gardener/deployment/feature_gates/","tags":"","title":"Feature Gates"},{"body":"This page gives writing formatting guidelines for the Gardener documentation. For style guidelines, see the Style Guide.\nThese are guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.\n Formatting of Inline Elements Code Snippet Formatting  Formatting of Inline Elements    Type of Text Formatting Markdown Syntax     User Interface Elements italics Choose *CLUSTERS*.   New Terms and Emphasis bold Do **not** stop it.   Technical Names code Open file `root.yaml`.   API Objects and Technical Components code Deploy a `Pod`.   Inline Code and Inline Commands code For declarative management, use `kubectl apply`.   Object Field Names and Field Values code Set the value of `image` to `nginx:1.8`.   Links and References link Visit the [Gardener website](https://gardener.cloud/)   Headers various # API Server    User Interface Elements When referring to UI elements, refrain from using verbs like “Click” or “Select with right mouse button”. This level of detail is hardly ever needed and also invalidates a procedure if other devices are used. For example, for a tablet you’d say “Tap on”.\nUse italics when you refer to UI elements.\n   UI Element Standard Formulation Markdown Syntax     Button, Menu path Choose UI Element. Choose *UI Element*.   Menu path, context menu, navigation path Choose System \u003e User Profile \u003e Own Data. Choose *System* \\\u003e *User Profile* \\\u003e *Own Data*.   Entry fields Enter your password. Enter your password.   Checkbox, radio button Select Filter. Select *Filter*.   Expandable screen elements Expand User Settings.\nCollapse User Settings. Expand *User Settings*.\nCollapse *User Settings*.    New Terms and Emphasis Use bold to emphasize something or to introduce a new term.\n   Do Don’t     A cluster is a set of nodes … A “cluster” is a set of nodes …   The system does not delete your objects. The system does not(!) delete your objects.    Technical Names Use code style (using backticks) for filenames, technical componentes, directories, and paths.\n   Do Don’t     Open file envars.yaml. Open the envars.yaml file.   Go to directory /docs/tutorials. Go to the /docs/tutorials directory.   Open file /_data/concepts.yaml. Open the /_data/concepts.yaml file.    API Objects and Technical Components When you refer to an API object, use the same uppercase and lowercase letters that are used in the actual object name, and use backticks to format them. Typically, the names of API objects use camel case.\nDon’t split the API object name into separate words. For example, use PodTemplateList, not Pod Template List.\nRefer to API objects without saying “object,” unless omitting “object” leads to an awkward construction.\n   Do Don’t     The Pod has two containers. The pod has two containers.   The Deployment is responsible for … The Deployment object is responsible for …   A PodList is a list of Pods. A Pod List is a list of pods.   The gardener-control-manager has control loops… The gardener-control-manager has control loops…   The gardenlet starts up with a bootstrap kubeconfig having a bootstrap token that allows to create CertificateSigningRequest (CSR) resources. The gardenlet starts up with a bootstrap kubeconfig having a bootstrap token that allows to create CertificateSigningRequest (CSR) resources.    Inline Code and Inline Commands Use backticks (`) for inline code.\n   Do Don’t     The kubectl run command creates a Deployment. The “kubectl run” command creates a Deployment.   For declarative management, use kubectl apply. For declarative management, use “kubectl apply”.    Object Field Names and Field Values Use backticks (`) for field names, and field values.\n   Do Don’t     Set the value of the replicas field in the configuration file. Set the value of the “replicas” field in the configuration file.   The value of the exec field is an ExecAction object. The value of the “exec” field is an ExecAction object.   Set the value of imagePullPolicy to Always. Set the value of imagePullPolicy to “Always”.   Set the value of image to nginx:1.8. the value of image to nginx:1.8.    Links and References    Do Don’t     Use a descriptor of the link’s destination: “For more information, visit Gardener’s website.” Use a generic placeholder: “For more information, go here.”    Another thing to keep in mind is that markdown links do not work in shortcodes. To circumvent this problem, you can use HTML links.\nHeaders  Use H1 for the title of the topic. Use H2 for each main section. Use H3 for any sub-section in the main sections. Avoid using H4-H6. Try moving the additional information to a new topic instead.  Code Snippet Formatting Don’t include the command prompt    Do Don’t     kubectl get pods $ kubectl get pods    Separate commands from output  Verify that the pod is running on your chosen node: kubectl get pods --output=wide The output is similar to:\nNAME READY STATUS RESTARTS AGE IP NODE nginx 1/1 Running 0 13s 10.200.0.4 worker0  Placeholders Use angle brackets for placeholders. Tell the reader what a placeholder represents, for example:\n Display information about a pod:\nkubectl describe pod \u003cpod-name\u003e \u003cpod-name\u003e is the name of one of your pods.\n Versioning Kubernetes examples Make code examples and configuration examples that include version information consistent with the accompanying text. Identify the Kubernetes version in the Prerequisites section.\n","categories":"","description":"","excerpt":"This page gives writing formatting guidelines for the Gardener …","ref":"/docs/contribute/20_documentation/20_formatting_guide/","tags":"","title":"Formatting Guide"},{"body":"Gardener Extension for Garden Linux OS  \nThis controller operates on the OperatingSystemConfig resource in the extensions.gardener.cloud/v1alpha1 API group. It manages those objects that are requesting Garden Linux OS configuration (.spec.type=gardenlinux):\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: OperatingSystemConfig metadata:  name: pool-01-original  namespace: default spec:  type: gardenlinux  units:  ...  files:  ... Please find a concrete example in the example folder.\nAfter reconciliation the resulting data will be stored in a secret within the same namespace (as the config itself might contain confidential data). The name of the secret will be written into the resource’s .status field:\n... status:  ...  cloudConfig:  secretRef:  name: osc-result-pool-01-original  namespace: default  command: /usr/bin/env bash \u003cpath\u003e  units:  - docker-monitor.service  - kubelet-monitor.service  - kubelet.service The secret has one data key cloud_config that stores the generation.\nAn example for a ControllerRegistration resource that can be used to register this controller to Gardener can be found here.\nThis controller is based on revision b5ba8164 of gardener-extension-os-ubuntu-alicloud. Its implementation is using the oscommon library for operating system configuration controllers.\nPlease find more information regarding the extensibility concepts and a detailed proposal here.\n How to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the ./dev/kubeconfig file. Static code checks and tests can be executed by running make verify. We are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io Gardener Extensions Golang library GEP-1 (Gardener Enhancement Proposal) on extensibility Extensibility API documentation  ","categories":"","description":"Gardener extension controller for the Garden Linux operating system","excerpt":"Gardener extension controller for the Garden Linux operating system","ref":"/docs/extensions/os-extensions/gardener-extension-os-gardenlinux/","tags":"","title":"Garden Linux OS"},{"body":"As we ramp up more and more friends of Gardener, I thought it worthwile to explore and write a tutorial about how to simply\n create a Gardener managed Kubernetes Cluster (Shoot) via kubectl, install Istio as a preferred, production ready Ingress/Service Mesh (instead of the Nginx Ingress addon), attach your own custom domain to be managed by Gardener, combine everything with certificates from Let’s Encrypt.  Here are some pre-pointers that you will need to go deeper:\n CRUD Gardener Shoot DNS Management Certificate Management Tutorial Domain Names Tutorial Certificates  Tip If you try my instructions and fail, then read the alternative title of this tutorial as \"Shoot yourself in the foot with Gardener, custom Domains, Istio and Certificates\".  First Things First Login to your Gardener landscape, setup a project with adequate infrastructure credentials and then navigate to your account. Note down the name of your secret. I chose the GCP infrastructure from the vast possible options that my Gardener provides me with, so i had named the secret as shoot-operator-gcp.\nFrom the Access widget (leave the default settings) download your personalized kubeconfig into ~/.kube/kubeconfig-garden-myproject. Follow the instructions to setup kubelogin:\nFor convinience, let us set an alias command with\nalias kgarden=\"kubectl --kubeconfig ~/.kube/kubeconfig-garden-myproject.yaml\" kgarden now gives you all botanical powers and connects you directly with your Gardener.\nYou should now be able to run kgarden get shoots, automatically get an oidc token, and list already running clusters/shoots.\nPrepare your Custom Domain I am going to use Cloud Flare as programmatic DNS of my custom domain mydomain.io. Please follow detailed instructions from Cloud Flare on how to delegate your domain (the free account does not support delegating subdomains). Alternatively, AWS Route53 (and most others) support delegating subdomains.\nI needed to follow these instructions and created the following secret:\napiVersion: v1 kind: Secret metadata:  name: cloudflare-mydomain-io type: Opaque data:  CLOUDFLARE_API_TOKEN: useYOURownDAMITzNDU2Nzg5MDEyMzQ1Njc4OQ== Apply this secret into your project with kgarden create -f cloudflare-mydomain-io.yaml.\nOur External DNS Manager also supports Amazon Route53, Google CloudDNS, AliCloud DNS, Azure DNS, or OpenStack Designate. Check it out.\nPrepare Gardener Extensions I now need to prepare the Gardener extensions shoot-dns-service and shoot-cert-service and set the parameters accordingly.\nPlease note, that the availability of Gardener Extensions depends on how your administrator has configured the Gardener landscape. Please contact your Gardener administrator in case you experience any issues during activation.  The following snipplet allows Gardener to manage my entire custom domain, whereas with the include: attribute I restrict all dynamic entries under the subdomain gsicdc.mydomain.io:\n dns:  providers:  - domains:  include:  - gsicdc.mydomain.io  primary: false  secretName: cloudflare-mydomain-io  type: cloudflare-dns  extensions:  - type: shoot-dns-service The next snipplet allows Gardener to manage certificates automatically from Let’s Encrypt on mydomain.io for me:\n extensions:  - type: shoot-cert-service  providerConfig:  apiVersion: service.cert.extensions.gardener.cloud/v1alpha1  issuers:  - email: me@mail.com  name: mydomain  server: 'https://acme-v02.api.letsencrypt.org/directory'  - email: me@mail.com  name: mydomain-staging  server: 'https://acme-staging-v02.api.letsencrypt.org/directory'  Adjust the snipplets with your parameters (don't forget your email). And please use the mydomain-staging issuer while you are testing and learning. Otherwise, Let's Encrypt will rate limit your frequent requests and you can wait a week until you can continue.  References for Let’s Encrypt:\n Rate limit Staging environment Challenge Types Wildcard Certificates  Create the Gardener Shoot Cluster Remember I chose to create the Shoot on GCP, so below is the simplest declarative shoot or cluster order document. Notice that I am referring to the infrastructure credentials with shoot-operator-gcp and I combined the above snipplets into the yaml file:\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata:  name: gsicdc spec:  dns:  providers:  - domains:  include:  - gsicdc.mydomain.io  primary: false  secretName: cloudflare-mydomain-io  type: cloudflare-dns  extensions:  - type: shoot-dns-service  - type: shoot-cert-service  providerConfig:  apiVersion: service.cert.extensions.gardener.cloud/v1alpha1  issuers:  - email: me@mail.com  name: mydomain  server: 'https://acme-v02.api.letsencrypt.org/directory'  - email: me@mail.com  name: mydomain-staging  server: 'https://acme-staging-v02.api.letsencrypt.org/directory'  cloudProfileName: gcp  kubernetes:  allowPrivilegedContainers: true  version: 1.18.2  maintenance:  autoUpdate:  kubernetesVersion: true  machineImageVersion: true  networking:  nodes: 10.250.0.0/16  pods: 100.96.0.0/11  services: 100.64.0.0/13  type: calico  provider:  controlPlaneConfig:  apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1  kind: ControlPlaneConfig  zone: europe-west1-d  infrastructureConfig:  apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  workers: 10.250.0.0/16  type: gcp  workers:  - machine:  image:  name: gardenlinux  version: 11.29.2  type: n1-standard-2  maxSurge: 1  maxUnavailable: 0  maximum: 2  minimum: 1  name: my-workerpool  volume:  size: 50Gi  type: pd-standard  zones:  - europe-west1-d  purpose: testing  region: europe-west1  secretBindingName: shoot-operator-gcp Create your cluster and wait for it to be ready (about 5 to 7min).\n$ kgarden create -f gsicdc.yaml shoot.core.gardener.cloud/gsicdc created  $ kgarden get shoot gsicdc --watch NAME CLOUDPROFILE VERSION SEED DOMAIN HIBERNATION OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE gsicdc gcp 1.18.2 gcp gsicdc.myproject.shoot.devgarden.cloud Awake Processing 38 Progressing Progressing Unknown Unknown 83s ... gsicdc gcp 1.18.2 gcp gsicdc.myproject.shoot.devgarden.cloud Awake Succeeded 100 True True True False 6m7s Get access to your freshly baked cluster and set your KUBECONFIG:\n$ kgarden get secrets gsicdc.kubeconfig -o jsonpath={.data.kubeconfig} | base64 -d \u003ekubeconfig-gsicdc.yaml  $ export KUBECONFIG=$(pwd)/kubeconfig-gsicdc.yaml $ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 100.64.0.1 \u003cnone\u003e 443/TCP 89m Install Istio Please follow the Istio installation instructions and download istioctl. If you are on a Mac, I recommend\n$ brew install istioctl I want to install Istio with a default profile and SDS enabled. Furthermore I pass the following annotations to the service object istio-ingressgateway in the istio-system namespace.\n annotations:  cert.gardener.cloud/issuer: mydomain-staging  cert.gardener.cloud/secretname: wildcard-tls  dns.gardener.cloud/class: garden  dns.gardener.cloud/dnsnames: \"*.gsicdc.mydomain.io\"  dns.gardener.cloud/ttl: \"120\" With these annotations three things now happen automagically:\n The External DNS Manager, provided to you as a service (dns.gardener.cloud/class: garden), picks up the request and creates the wildcard DNS entry *.gsicdc.mydomain.io with a time to live of 120sec at your DNS provider. My provider Cloud Flare is very very quick (as opposed to some other services). You should be able to verify the entry with dig lovemygardener.gsicdc.mydomain.io within seconds. The Certificate Management picks up the request as well and initates a DNS01 protocol exchange with Let’s Encrypt; using the staging environment referred to with the issuer behind mydomain-staging. After aproximately 70sec (give and take) you will receive the wildcard certificate in the wildcard-tls secret in the namespace istio-system.  Notice, that the namespace for the certificate secret is often the cause of many troubeshooting sessions: the secret must reside in the same namespace of the gateway.  Here is the istio-install script:\n$ export domainname=\"*.gsicdc.mydomain.io\" $ export issuer=\"mydomain-staging\"  $ cat \u003c\u003cEOF | istioctl install -y -f - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: profile: default components: ingressGateways: - name: istio-ingressgateway enabled: true k8s: serviceAnnotations: cert.gardener.cloud/issuer: \"${issuer}\" cert.gardener.cloud/secretname: wildcard-tls dns.gardener.cloud/class: garden dns.gardener.cloud/dnsnames: \"${domainname}\" dns.gardener.cloud/ttl: \"120\" EOF Verify that setup is working and that DNS and certificates have been created/delivered:\n$ kubectl -n istio-system describe service istio-ingressgateway \u003csnip\u003e Events:  Type Reason Age From Message  ---- ------ ---- ---- -------  Normal EnsuringLoadBalancer 58s service-controller Ensuring load balancer  Normal reconcile 58s cert-controller-manager created certificate object istio-system/istio-ingressgateway-service-pwqdm  Normal cert-annotation 58s cert-controller-manager wildcard-tls: cert request is pending  Normal cert-annotation 54s cert-controller-manager wildcard-tls: certificate pending: certificate requested, preparing/waiting for successful DNS01 challenge  Normal cert-annotation 28s cert-controller-manager wildcard-tls: certificate ready  Normal EnsuredLoadBalancer 26s service-controller Ensured load balancer  Normal reconcile 26s dns-controller-manager created dns entry object shoot--core--gsicdc/istio-ingressgateway-service-p9qqb  Normal dns-annotation 26s dns-controller-manager *.gsicdc.mydomain.io: dns entry is pending  Normal dns-annotation 21s (x3 over 21s) dns-controller-manager *.gsicdc.mydomain.io: dns entry active  $ dig lovemygardener.gsicdc.mydomain.io  ; \u003c\u003c\u003e\u003e DiG 9.10.6 \u003c\u003c\u003e\u003e lovemygardener.gsicdc.mydomain.io \u003csnip\u003e ;; ANSWER SECTION: lovemygardener.gsicdc.mydomain.io. 120 IN A\t35.195.120.62 \u003csnip\u003e There you have it, the wildcard-tls certificate is ready and the *.gsicdc.mydomain.io dns entry is active. Traffic will be going your way.\nHandy tools to install Another set of fine tools to use are kapp (formerly known as k14s), k9s and HTTPie. While we are at it, let’s install them all. If you are on a Mac, I recommend:\nbrew tap vmware-tanzu/carvel brew install ytt kbld kapp kwt imgpkg vendir brew install derailed/k9s/k9s brew install httpie Ingress to your service Networking is a central part of Kubernetes, but it can be challenging to understand exactly how it is expected to work. You should learn about Kubernetes networking, and first try to debug problems yourself. With a solid managed cluster from Gardener, it is always PEBCAK!  Kubernetes Ingress is a subject that is evolving to much broader standard. Please watch Evolving the Kubernetes Ingress APIs to GA and Beyond for a good introduction. In this example, I did not want to use the Kubernetes Ingress compatibility option of Istio. Instead, I used VirtualService and Gateway from the Istio’s API group networking.istio.io/v1beta1 directly, and enabled istio-injection generically for the namespace.\nI use httpbin as service that I want to expose to the internet, or where my ingress should be routed to (depends on your point of view, I guess).\napiVersion: v1 kind: Namespace metadata:  name: production  labels:  istio-injection: enabled --- apiVersion: v1 kind: Service metadata:  name: httpbin  namespace: production  labels:  app: httpbin spec:  ports:  - name: http  port: 8000  targetPort: 80  selector:  app: httpbin --- apiVersion: apps/v1 kind: Deployment metadata:  name: httpbin  namespace: production spec:  replicas: 1  selector:  matchLabels:  app: httpbin  template:  metadata:  labels:  app: httpbin  spec:  containers:  - image: docker.io/kennethreitz/httpbin  imagePullPolicy: IfNotPresent  name: httpbin  ports:  - containerPort: 80 --- apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata:  name: httpbin-gw  namespace: production spec:  selector:  istio: ingressgateway #! use istio default ingress gateway  servers:  - port:  number: 80  name: http  protocol: HTTP  tls:  httpsRedirect: true  hosts:  - \"httpbin.gsicdc.mydomain.io\"  - port:  number: 443  name: https  protocol: HTTPS  tls:  mode: SIMPLE  credentialName: wildcard-tls  hosts:  - \"httpbin.gsicdc.mydomain.io\" --- apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata:  name: httpbin-vs  namespace: production spec:  hosts:  - \"httpbin.gsicdc.mydomain.io\"  gateways:  - httpbin-gw  http:  - match:  - uri:  regex: /.*  route:  - destination:  port:  number: 8000  host: httpbin --- Let us now deploy the whole package of Kubernetes primitives using kapp:\n$ kapp deploy -a httpbin -f httpbin-kapp.yaml Target cluster 'https://api.gsicdc.myproject.shoot.devgarden.cloud' (nodes: shoot--myproject--gsicdc-my-workerpool-z1-6586c8f6cb-x24kh)  Changes  Namespace Name Kind Conds. Age Op Wait to Rs Ri (cluster) production Namespace - - create reconcile - - production httpbin Deployment - - create reconcile - - ^ httpbin Service - - create reconcile - - ^ httpbin-gw Gateway - - create reconcile - - ^ httpbin-vs VirtualService - - create reconcile - -  Op: 5 create, 0 delete, 0 update, 0 noop Wait to: 5 reconcile, 0 delete, 0 noop  Continue? [yN]: y  5:36:31PM: ---- applying 1 changes [0/5 done] ---- \u003csnip\u003e 5:37:00PM: ok: reconcile deployment/httpbin (apps/v1) namespace: production 5:37:00PM: ---- applying complete [5/5 done] ---- 5:37:00PM: ---- waiting complete [5/5 done] ----  Succeeded Let’s finaly test the service (Of course you can use the browser as well):\n$ http httpbin.gsicdc.mydomain.io HTTP/1.1 301 Moved Permanently content-length: 0 date: Wed, 13 May 2020 21:29:13 GMT location: https://httpbin.gsicdc.mydomain.io/ server: istio-envoy  $ curl -k https://httpbin.gsicdc.mydomain.io/ip {  \"origin\": \"10.250.0.2\" } Quod erat demonstrandum. The proof of exchanging the issuer is now left to the reader.\nTip Remember that the certificate is actually not valid because it is issued from the Let's encrypt staging environment. Thus, we needed \"curl -k\" or \"http --verify no\".  Hint: use the interactive k9s tool. Cleanup Remove the cloud native application:\n$ kapp ls Apps in namespace 'default'  Name Namespaces Lcs Lca httpbin (cluster),production true 17m  $ kapp delete -a httpbin ... Continue? [yN]: y ... 11:47:47PM: ---- waiting complete [8/8 done] ----  Succeeded Remove Istio:\n$ istioctl x uninstall --purge clusterrole.rbac.authorization.k8s.io \"prometheus-istio-system\" deleted clusterrolebinding.rbac.authorization.k8s.io \"prometheus-istio-system\" deleted ... Delete your Shoot:\nkgarden annotate shoot gsicdc confirmation.gardener.cloud/deletion=true --overwrite kgarden delete shoot gsicdc --wait=false ","categories":"","description":"","excerpt":"As we ramp up more and more friends of Gardener, I thought it …","ref":"/docs/extensions/others/gardener-extension-shoot-cert-service/docs/tutorial-custom-domain-with-istio/","tags":"","title":"Gardener yourself a Shoot with Istio, custom Domains, and Certificates"},{"body":"Gardenlet Gardener is implemented using the operator pattern: It uses custom controllers that act on our own custom resources, and apply Kubernetes principles to manage clusters instead of containers. Following this analogy, you can recognize components of the Gardener architecture as well-known Kubernetes components, for example, shoot clusters can be compared with pods, and seed clusters can be seen as worker nodes.\nThe following Gardener components play a similar role as the corresponding components in the Kubernetes architecture:\n   Gardener Component Kubernetes Component     gardener-apiserver kube-apiserver   gardener-controller-manager kube-controller-manager   gardener-scheduler kube-scheduler   gardenlet kubelet    Similar to how the kube-scheduler of Kubernetes finds an appropriate node for newly created pods, the gardener-scheduler of Gardener finds an appropriate seed cluster to host the control plane for newly ordered clusters. By providing multiple seed clusters for a region or provider, and distributing the workload, Gardener also reduces the blast radius of potential issues.\nKubernetes runs a primary “agent” on every node, the kubelet, which is responsible for managing pods and containers on its particular node. Decentralizing the responsibility to the kubelet has the advantage that the overall system is scalable. Gardener achieves the same for cluster management by using a gardenlet as primary “agent” on every seed cluster, and is only responsible for shoot clusters located in its particular seed cluster:\nThe gardener-controller-manager has control loops to manage resources of the Gardener API. However, instead of letting the gardener-controller-manager talk directly to seed clusters or shoot clusters, the responsibility isn’t only delegated to the gardenlet, but also managed using a reversed control flow: It’s up to the gardenlet to contact the Gardener API server, for example, to share a status for its managed seed clusters.\nReversing the control flow allows placing seed clusters or shoot clusters behind firewalls without the necessity of direct access via VPN tunnels anymore.\nTLS Bootstrapping Kubernetes doesn’t manage worker nodes itself, and it’s also not responsible for the lifecycle of the kubelet running on the workers. Similarly, Gardener doesn’t manage seed clusters itself, so Gardener is also not responsible for the lifecycle of the gardenlet running on the seeds. As a consequence, both the gardenlet and the kubelet need to prepare a trusted connection to the Gardener API server and the Kubernetes API server correspondingly.\nTo prepare a trusted connection between the gardenlet and the Gardener API server, the gardenlet initializes a bootstrapping process after you deployed it into your seed clusters:\n  The gardenlet starts up with a bootstrap kubeconfig having a bootstrap token that allows to create CertificateSigningRequest (CSR) resources.\n  After the CSR is signed, the gardenlet downloads the created client certificate, creates a new kubeconfig with it, and stores it inside a Secret in the seed cluster.\n  The gardenlet deletes the bootstrap kubeconfig secret, and starts up with its new kubeconfig.\n  The gardenlet starts normal operation.\n  The gardener-controller-manager runs a control loop that automatically signs CSRs created by gardenlets.\n The gardenlet bootstrapping process is based on the kubelet bootstrapping process. More information: Kubelet’s TLS bootstrapping.\n If you don’t want to run this bootstrap process you can create a kubeconfig pointing to the garden cluster for the gardenlet yourself, and use field gardenClientConnection.kubeconfig in the gardenlet configuration to share it with the gardenlet.\nGardenlet Certificate Rotation The certificate used to authenticate the gardenlet against the API server has a certain validity based on the configuration of the garden cluster (--cluster-signing-duration flag of the kube-controller-manager (default 1y)).\n If your garden cluster is of at least Kubernetes v1.22 then you can also configure the validity for the client certificate by specifying .gardenClientConnection.kubeconfigValidity.validity in the gardenlet’s component configuration. Note that changing this value will only take effect when the kubeconfig is rotated again (it is not picked up immediately). The minimum validity is 10m (that’s what is enforced by the CertificateSigningRequest API in Kubernetes which is used by gardenlet).\n By default, after about 70-90% of the validity expired, the gardenlet tries to automatically replace the current certificate with a new one (certificate rotation).\n You can change these boundaries by specifying .gardenClientConnection.kubeconfigValidity.autoRotationJitterPercentage{Min,Max} in the gardenlet’s component configuration.\n To use certificate rotation, you need to specify the secret to store the kubeconfig with the rotated certificate in field .gardenClientConnection.kubeconfigSecret of the gardenlet component configuration.\nRotate certificates using bootstrap kubeconfig If the gardenlet created the certificate during the initial TLS Bootstrapping using the Bootstrap kubeconfig, certificates can be rotated automatically. The same control loop in the gardener-controller-manager that signs the CSRs during the initial TLS Bootstrapping also automatically signs the CSR during a certificate rotation.\nℹ️ You can trigger an immediate renewal by annotating the Secret in the seed cluster stated in the .gardenClientConnection.kubeconfigSecret field with gardener.cloud/operation=renew and restarting the gardenlet. After it booted up again, gardenlet will issue a new certificate independent of the remaining validity of the existing one.\nRotate Certificate Using Custom kubeconfig When trying to rotate a custom certificate that wasn’t created by gardenlet as part of the TLS Bootstrap, the x509 certificate’s Subject field needs to conform to the following:\n the Common Name (CN) is prefixed with gardener.cloud:system:seed: the Organization (O) equals gardener.cloud:system:seeds  Otherwise, the gardener-controller-manager doesn’t automatically sign the CSR. In this case, an external component or user needs to approve the CSR manually, for example, using command kubectl certificate approve seed-csr-\u003c...\u003e). If that doesn’t happen within 15 minutes, the gardenlet repeats the process and creates another CSR.\nConfiguring the Seed to work with The Gardenlet works with a single seed, which must be configured in the GardenletConfiguration under .seedConfig. This must be a copy of the Seed resource, for example (see example/20-componentconfig-gardenlet.yaml for a more complete example):\napiVersion: gardenlet.config.gardener.cloud/v1alpha1 kind: GardenletConfiguration seedConfig:  metadata:  name: my-seed  spec:  provider:  type: aws  # ...  secretRef:  name: my-seed-secret  namespace: garden When using make start-gardenlet, the corresponding script will automatically fetch the seed cluster’s kubeconfig based on the seedConfig.spec.secretRef and set the environment accordingly.\nOn startup, gardenlet registers a Seed resource using the given template in seedConfig if it’s not present already.\nComponent Configuration In the component configuration for the gardenlet, it’s possible to define:\n settings for the Kubernetes clients interacting with the various clusters settings for the control loops inside the gardenlet settings for leader election and log levels, feature gates, and seed selection or seed configuration.  More information: Example Gardenlet Component Configuration.\nHeartbeats Similar to how Kubernetes uses Lease objects for node heart beats (see KEP), the gardenlet is using Lease objects for heart beats of the seed cluster. Every two seconds, the gardenlet checks that the seed cluster’s /healthz endpoint returns HTTP status code 200. If that is the case, the gardenlet renews the lease in the Garden cluster in the gardener-system-seed-lease namespace and updates the GardenletReady condition in the status.conditions field of the Seed resource(s).\nSimilarly to the node-lifecycle-controller inside the kube-controller-manager, the gardener-controller-manager features a seed-lifecycle-controller that sets the GardenletReady condition to Unknown in case the gardenlet fails to renew the lease. As a consequence, the gardener-scheduler doesn’t consider this seed cluster for newly created shoot clusters anymore.\n/healthz Endpoint The gardenlet includes an HTTP server that serves a /healthz endpoint. It’s used as a liveness probe in the Deployment of the gardenlet. If the gardenlet fails to renew its lease then the endpoint returns 500 Internal Server Error, otherwise it returns 200 OK.\nPlease note that the /healthz only indicates whether the gardenlet could successfully probe the Seed’s API server and renew the lease with the Garden cluster. It does not show that the Gardener extension API server (with the Gardener resource groups) is available. However, the Gardenlet is designed to withstand such connection outages and retries until the connection is reestablished.\nControl Loops The gardenlet consists out of several controllers which are now described in more detail.\nBackupEntry Controller The BackupEntry controller reconciles those core.gardener.cloud/v1beta1.BackupEntry resources whose .spec.seedName value is equal to the name of a Seed the respective gardenlet is responsible for. Those resources are created by the Shoot controller (only if backup is enabled for the respective Seed) and there is exactly one BackupEntry per Shoot.\nThe controller creates an extensions.gardener.cloud/v1alpha1.BackupEntry resource (non-namespaced) in the seed cluster and waits until the responsible extension controller reconciled it (see this for more details). The status is populated in the .status.lastOperation field.\nThe core.gardener.cloud/v1beta1.BackupEntry resource has an owner reference pointing to the corresponding Shoot. Hence, if the Shoot is deleted, also the BackupEntry resource gets deleted. In this case, the controller deletes the extensions.gardener.cloud/v1alpha1.BackupEntry resource in the seed cluster and waits until the responsible extension controller has deleted it. Afterwards, the finalizer of the core.gardener.cloud/v1beta1.BackupEntry resource is released so that it finally disappears from the system.\nKeep Backup for Deleted Shoots In some scenarios it might be beneficial to not immediately delete the BackupEntrys (and with them, the etcd backup) for deleted Shoots.\nIn this case you can configure the .controllers.backupEntry.deletionGracePeriodHours field in the component configuration of the gardenlet. For example, if you set it to 48, then the BackupEntrys for deleted Shoots will only be deleted 48 hours after the Shoot was deleted.\nAdditionally, you can limit the shoot purposes for which this applies by setting .controllers.backupEntry.deletionGracePeriodShootPurposes[]. For example, if you set it to [production] then only the BackupEntrys for Shoots with .spec.purpose=production will be deleted after the configured grace period. All others will be deleted immediately after the Shoot deletion.\nControllerInstallation Controller The ControllerInstallation controller in the gardenlet reconciles ControllerInstallation objects with the help of the following reconcilers.\n“Main” Reconciler This reconciler is responsible for ControllerInstallations referencing a ControllerDeployment whose type=helm. It is responsible for unpacking the Helm chart tarball in the ControllerDeployments .providerConfig.chart field and deploying the rendered resources to the seed cluster. The Helm chart values in .providerConfig.values will be used and extended with some information about the Gardener environment and the seed cluster:\ngardener:  version: \u003cgardenlet-version\u003e  garden:  clusterIdentity: \u003cidentity-of-garden-cluster\u003e  seed:  identity: \u003cseed-name\u003e  clusterIdentity: \u003cidentity-of-seed-cluster\u003e  annotations: \u003cseed-annotations\u003e  labels: \u003cseed-labels\u003e  spec: \u003cseed-specification\u003e As of today, there are a few more fields in .gardener.seed, but it is recommended to use the .gardener.seed.spec if the Helm chart needs more information about the seed configuration.\nThe rendered chart will be deployed via a ManagedResource created in the garden namespace of the seed cluster. It is labeled with controllerinstallation-name=\u003cname\u003e so that one can easily find the owning ControllerInstallation for an existing ManagedResource.\nThe reconciler maintains the Installed condition of the ControllerInstallation and sets it to False if the rendering or deployment fails.\n“Care” Reconciler This reconciler reconciles ControllerInstallation objects and checks whether they are in a healthy state. It checks the .status.conditions of the backing ManagedResource created in the garden namespace of the seed cluster.\n If the ResourcesApplied condition of the ManagedResource is True then the Installed condition of the ControllerInstallation will be set to True. If the ResourcesHealthy condition of the ManagedResource is True then the Healthy condition of the ControllerInstallation will be set to True. If the ResourcesProgressing condition of the ManagedResource is True then the Progressing condition of the ControllerInstallation will be set to True.  A ControllerInstallation is considered “healthy” if Applied=Healthy=true and Progressing=False.\n“Required” Reconciler This reconciler watches all resources in the extensions.gardener.cloud API group in the seed cluster. It is responsible for maintaining the Required condition on ControllerInstallations. Concretely, when there is at least one extension resource in the seed cluster a ControllerInstallation is responsible for, then the status of the Required condition will be True. If there are no extension resources anymore, its status will be False.\nThis condition is taken into account by the ControllerRegistration controller part of gardener-controller-manager when it computes which extensions have to deployed to which seed cluster, see this document for more details.\nShootState Controller The ShootState controller in the gardenlet reconciles resources containing information that have to be synced to the ShootState. This information is used when a control plane migration is performed.\n“Extensions” Reconciler This reconciler watches resources in the extensions.gardener.cloud API group in the seed cluster which contain Shoot-specific state or data. Those are BackupEntrys, ContainerRuntimes, ControlPlanes, DNSRecords, Extensions, Infrastructures, Networks, OperatingSystemConfigs, and Workers.\nWhen there is a change in the .status.state or .status.resources[] fields of these resources then this information is synced into the ShootState resource in the garden cluster.\n“Secret” Reconciler This reconciler reconciles Secrets having labels managed-by=secrets-manager and persist=true in the shoot namespaces in the seed cluster. It syncs them to the ShootState so that the secrets can be restored from there in case a shoot control plane has to be restored to another seed cluster (in case of migration).\nManaged Seeds Gardener users can use shoot clusters as seed clusters, so-called “managed seeds” (aka “shooted seeds”), by creating ManagedSeed resources. By default, the gardenlet that manages this shoot cluster then automatically creates a clone of itself with the same version and the same configuration that it currently has. Then it deploys the gardenlet clone into the managed seed cluster.\nIf you want to prevent the automatic gardenlet deployment, specify the seedTemplate section in the ManagedSeed resource, and don’t specify the gardenlet section. In this case, you have to deploy the gardenlet on your own into the seed cluster.\nMore information: Register Shoot as Seed\nMigrating from Previous Gardener Versions If your Gardener version doesn’t support gardenlets yet, no special migration is required, but the following prerequisites must be met:\n Your Gardener version is at least 0.31 before upgrading to v1. You have to make sure that your garden cluster is exposed in a way that it’s reachable from all your seed clusters.  With previous Gardener versions, you had deployed the Gardener Helm chart (incorporating the API server, controller-manager, and scheduler). With v1, this stays the same, but you now have to deploy the gardenlet Helm chart as well into all of your seeds (if they aren’t managed, as mentioned earlier).\nMore information: Deploy a Gardenlet for all instructions.\nRelated Links  Gardener Architecture #356: Implement Gardener Scheduler #2309: Add /healthz endpoint for Gardenlet  ","categories":"","description":"","excerpt":"Gardenlet Gardener is implemented using the operator pattern: It uses …","ref":"/docs/gardener/concepts/gardenlet/","tags":"","title":"Gardenlet"},{"body":"Scoped API Access for Gardenlets By default, gardenlets have administrative access in the garden cluster. They are able to execute any API request on any object independent of whether the object is related to the seed cluster the gardenlet is responsible for. As RBAC is not powerful enough for fine-grained checks and for the sake of security, Gardener provides two optional but recommended configurations for your environments that scope the API access for gardenlets.\nSimilar to the Node authorization mode in Kubernetes, Gardener features a SeedAuthorizer plugin. It is a special-purpose authorization plugin that specifically authorizes API requests made by the gardenlets.\nLikewise, similar to the NodeRestriction admission plugin in Kubernetes, Gardener features a SeedRestriction plugin. It is a special-purpose admission plugin that specifically limits the Kubernetes objects gardenlets can modify.\n📚 You might be interested to look into the design proposal for scoped Kubelet API access from the Kubernetes community. It can be translated to Gardener and Gardenlets with their Seed and Shoot resources.\nFlow Diagram The following diagram shows how the two plugins are included in the request flow of a gardenlet. When they are not enabled then the kube-apiserver is internally authorizing the request via RBAC before forwarding the request directly to the gardener-apiserver, i.e., the gardener-admission-controller would not be consulted (this is not entirely correct because it also serves other admission webhook handlers, but for simplicity reasons this document focuses on the API access scope only).\nWhen enabling the plugins, there is one additional step for each before the gardener-apiserver responds to the request.\nPlease note that the example shows a request to an object (Shoot) residing in one of the API groups served by gardener-apiserver. However, the gardenlet is also interacting with objects in API groups served by the kube-apiserver (e.g., Secret,ConfigMap, etc.). In this case, the consultation of the SeedRestriction admission plugin is performed by the kube-apiserver itself before it forwards the request to the gardener-apiserver.\nToday, the following rules are implemented:\n   Resource Verbs Path(s) Description     BackupBucket get, list, watch, create, update, patch, delete BackupBucket -\u003e Seed Allow get, list, watch requests for all BackupBuckets. Allow only create, update, patch, delete requests for BackupBuckets assigned to the gardenlet’s Seed.   BackupEntry get, list, watch, create, update, patch BackupEntry -\u003e Seed Allow get, list, watch requests for all BackupEntrys. Allow only create, update, patch requests for BackupEntrys assigned to the gardenlet’s Seed and referencing BackupBuckets assigned to the gardenlet’s Seed.   Bastion get, list, watch, create, update, patch Bastion -\u003e Seed Allow get, list, watch requests for all Bastions. Allow only create, update, patch requests for Bastions assigned to the gardenlet’s Seed.   CertificateSigningRequest get, create CertificateSigningRequest -\u003e Seed Allow only get, create requests for CertificateSigningRequests related to the gardenlet’s Seed.   CloudProfile get CloudProfile -\u003e Shoot -\u003e Seed Allow only get requests for CloudProfiles referenced by Shoots that are assigned to the gardenlet’s Seed.   ClusterRoleBinding create, get, update, patch, delete ClusterRoleBinding -\u003e ManagedSeed -\u003e Shoot -\u003e Seed Allow create, get, update, patch requests for ManagedSeeds in the bootstrapping phase assigned to the gardenlet’s Seeds. Allow delete requests from gardenlets bootstrapped via ManagedSeeds.   ConfigMap get ConfigMap -\u003e Shoot -\u003e Seed Allow only get requests for ConfigMaps referenced by Shoots that are assigned to the gardenlet’s Seed. Allows reading the kube-system/cluster-identity ConfigMap.   ControllerRegistration get, list, watch ControllerRegistration -\u003e ControllerInstallation -\u003e Seed Allow get, list, watch requests for all ControllerRegistrations.   ControllerDeployment get ControllerDeployment -\u003e ControllerInstallation -\u003e Seed Allow get requests for ControllerDeploymentss referenced by ControllerInstallations assigned to the gardenlet’s Seed.   ControllerInstallation get, list, watch, update, patch ControllerInstallation -\u003e Seed Allow get, list, watch requests for all ControllerInstallations. Allow only update, patch requests for ControllerInstallations assigned to the gardenlet’s Seed.   Event create, patch none Allow to create or patch all kinds of Events.   ExposureClass get ExposureClass -\u003e Shoot -\u003e Seed Allow get requests for ExposureClasses referenced by Shoots that are assigned to the gardenlet’s Seed. Deny get requests to other ExposureClasses.   Lease create, get, watch, update Lease -\u003e Seed Allow create, get, update, and delete requests for Leases of the gardenlet’s Seed.   ManagedSeed get, list, watch, update, patch ManagedSeed -\u003e Shoot -\u003e Seed Allow get, list, watch requests for all ManagedSeeds. Allow only update, patch requests for ManagedSeeds referencing a Shoot assigned to the gardenlet’s Seed.   Namespace get Namespace -\u003e Shoot -\u003e Seed Allow get requests for Namespaces of Shoots that are assigned to the gardenlet’s Seed. Always allow get requests for the garden Namespace.   Project get Project -\u003e Namespace -\u003e Shoot -\u003e Seed Allow get requests for Projects referenced by the Namespace of Shoots that are assigned to the gardenlet’s Seed.   SecretBinding get SecretBinding -\u003e Shoot -\u003e Seed Allow only get requests for SecretBindings referenced by Shoots that are assigned to the gardenlet’s Seed.   Secret create, get, update, patch, delete(, list, watch) Secret -\u003e Seed, Secret -\u003e Shoot -\u003e Seed, Secret -\u003e SecretBinding -\u003e Shoot -\u003e Seed, BackupBucket -\u003e Seed Allow get, list, watch requests for all Secrets in the seed-\u003cname\u003e namespace. Allow only create, get, update, patch, delete requests for the Secrets related to resources assigned to the gardenlet's Seed`s.   Seed get, list, watch, create, update, patch, delete Seed Allow get, list, watch requests for all Seeds. Allow only create, update, patch, delete requests for the gardenlet’s Seeds. [1]   ServiceAccount create, get, update, patch, delete ServiceAccount -\u003e ManagedSeed -\u003e Shoot -\u003e Seed Allow create, get, update, patch requests for ManagedSeeds in the bootstrapping phase assigned to the gardenlet’s Seeds. Allow delete requests from gardenlets bootstrapped via ManagedSeeds.   Shoot get, list, watch, update, patch Shoot -\u003e Seed Allow get, list, watch requests for all Shoots. Allow only update, patch requests for Shoots assigned to the gardenlet’s Seed.   ShootState get, create, update, patch ShootState -\u003e Shoot -\u003e Seed Allow only get, create, update, patch requests for ShootStates belonging by Shoots that are assigned to the gardenlet’s Seed.    [1] If you use ManagedSeed resources then the gardenlet reconciling them (“parent gardenlet”) may be allowed to submit certain requests for the Seed resources resulting out of such ManagedSeed reconciliations (even if the “parent gardenlet” is not responsible for them):\n ℹ️ It is allowed to delete the Seed resources if the corresponding ManagedSeed objects already have a deletionTimestamp (this is secure as gardenlets themselves don’t have permissions for deleting ManagedSeeds). ⚠ It is allowed to create or update Seed resources if the corresponding ManagedSeed objects use a seed template, i.e., .spec.seedTemplate != nil. In this case, there is at least one gardenlet in your system which is responsible for two or more Seeds. Please keep in mind that this use case is not recommended for production scenarios (you should only have one dedicated gardenlet per seed cluster), hence, the security improvements discussed in this document might be limited.  SeedAuthorizer Authorization Webhook Enablement The SeedAuthorizer is implemented as Kubernetes authorization webhook and part of the gardener-admission-controller component running in the garden cluster.\n🎛 In order to activate it, you have to follow these steps:\n  Set the following flags for the kube-apiserver of the garden cluster (i.e., the kube-apiserver whose API is extended by Gardener):\n --authorization-mode=RBAC,Node,Webhook (please note that Webhook should appear after RBAC in the list [1]; Node might not be needed if you use a virtual garden cluster) --authorization-webhook-config-file=\u003cpath-to-the-webhook-config-file\u003e --authorization-webhook-cache-authorized-ttl=0 --authorization-webhook-cache-unauthorized-ttl=0    The webhook config file (stored at \u003cpath-to-the-webhook-config-file\u003e) should look as follows:\napiVersion: v1 kind: Config clusters: - name: garden  cluster:  certificate-authority-data: base64(CA-CERT-OF-GARDENER-ADMISSION-CONTROLLER)  server: https://gardener-admission-controller.garden/webhooks/auth/seed users: - name: kube-apiserver  user: {} contexts: - name: auth-webhook  context:  cluster: garden  user: kube-apiserver current-context: auth-webhook   When deploying the Gardener controlplane Helm chart, set .global.rbac.seedAuthorizer.enabled=true. This will prevent that the RBAC resources granting global access for all gardenlets will be deployed.\n  Delete the existing RBAC resources granting global access for all gardenlets by running:\nkubectl delete \\  clusterrole.rbac.authorization.k8s.io/gardener.cloud:system:seeds \\  clusterrolebinding.rbac.authorization.k8s.io/gardener.cloud:system:seeds \\  --ignore-not-found   Please note that you should activate the SeedRestriction admission handler as well.\n [1] The reason for the fact that Webhook authorization plugin should appear after RBAC is that the kube-apiserver will be depending on the gardener-admission-controller (serving the webhook). However, the gardener-admission-controller can only start when gardener-apiserver runs, but gardener-apiserver itself can only start when kube-apiserver runs. If Webhook is before RBAC then gardener-apiserver might not be able to start, leading to a deadlock.\n Authorizer Decisions As mentioned earlier, it’s the authorizer’s job to evaluate API requests and return one of the following decisions:\n DecisionAllow: The request is allowed, further configured authorizers won’t be consulted. DecisionDeny: The request is denied, further configured authorizers won’t be consulted. DecisionNoOpinion: A decision cannot be made, further configured authorizers will be consulted.  For backwards compatibility, no requests are denied at the moment, so that they are still deferred to a subsequent authorizer like RBAC. Though, this might change in the future.\nFirst, the SeedAuthorizer extracts the Seed name from the API request. This requires a proper TLS certificate the gardenlet uses to contact the API server and is automatically given if TLS bootstrapping is used. Concretely, the authorizer checks the certificate for name gardener.cloud:system:seed:\u003cseed-name\u003e and group gardener.cloud:system:seeds. In cases where this information is missing e.g., when a custom Kubeconfig is used, the authorizer cannot make any decision. Thus, RBAC is still a considerable option to restrict the gardenlet’s access permission if the above explained preconditions are not given.\nWith the Seed name at hand, the authorizer checks for an existing path from the resource that a request is being made for to the Seed belonging to the gardenlet. Take a look at the Implementation Details section for more information.\nImplementation Details Internally, the SeedAuthorizer uses a directed, acyclic graph data structure in order to efficiently respond to authorization requests for gardenlets:\n A vertex in this graph represents a Kubernetes resource with its kind, namespace, and name (e.g., Shoot:garden-my-project/my-shoot). An edge from vertex u to vertex v in this graph exists when  (1) v is referred by u and v is a Seed, or when (2) u is referred by v, or when (3) u is strictly associated with v.    For example, a Shoot refers to a Seed, a CloudProfile, a SecretBinding, etc., so it has an outgoing edge to the Seed (1) and incoming edges from the CloudProfile and SecretBinding vertices (2). However, there might also be a ShootState or a BackupEntry resource strictly associated with this Shoot, hence, it has incoming edges from these vertices (3).\nIn above picture the resources that are actively watched have are shaded. Gardener resources are green while Kubernetes resources are blue. It shows the dependencies between the resources and how the graph is built based on above rules.\nℹ️ Above picture shows all resources that may be accessed by gardenlets, except for the Quota resource which is only included for completeness.\nNow, when a gardenlet wants to access certain resources then the SeedAuthorizer uses a Depth-First traversal starting from the vertex representing the resource in question, e.g., from a Project vertex. If there is a path from the Project vertex to the vertex representing the Seed the gardenlet is responsible for then it allows the request.\nMetrics The SeedAuthorizer registers the following metrics related to the mentioned graph implementation:\n   Metric Description     gardener_admission_controller_seed_authorizer_graph_update_duration_seconds Histogram of duration of resource dependency graph updates in seed authorizer, i.e., how long does it take to update the graph’s vertices/edges when a resource is created, changed, or deleted.   gardener_admission_controller_seed_authorizer_graph_path_check_duration_seconds Histogram of duration of checks whether a path exists in the resource dependency graph in seed authorizer.    Debug Handler When the .server.enableDebugHandlers field in the gardener-admission-controller’s component configuration is set to true then it serves a handler that can be used for debugging the resource dependency graph under /debug/resource-dependency-graph.\n🚨 Only use this setting for development purposes as it enables unauthenticated users to view all data if they have access to the gardener-admission-controller component.\nThe handler renders an HTML page displaying the current graph with a list of vertices and its associated incoming and outgoing edges to other vertices. Depending on the size of the Gardener landscape (and consequently, the size of the graph), it might not be possible to render it in its entirety. If there are more than 2000 vertices then the default filtering will selected for kind=Seed to prevent overloading the output.\nExample output:\n------------------------------------------------------------------------------- | | # Seed:my-seed | \u003c- (11) | BackupBucket:73972fe2-3d7e-4f61-a406-b8f9e670e6b7 | BackupEntry:garden-my-project/shoot--dev--my-shoot--4656a460-1a69-4f00-9372-7452cbd38ee3 | ControllerInstallation:dns-external-mxt8m | ControllerInstallation:extension-shoot-cert-service-4qw5j | ControllerInstallation:networking-calico-bgrb2 | ControllerInstallation:os-gardenlinux-qvb5z | ControllerInstallation:provider-gcp-w4mvf | Secret:garden/backup | Shoot:garden-my-project/my-shoot | ------------------------------------------------------------------------------- | | # Shoot:garden-my-project/my-shoot | \u003c- (5) | CloudProfile:gcp | Namespace:garden-my-project | Secret:garden-my-project/my-dns-secret | SecretBinding:garden-my-project/my-credentials | ShootState:garden-my-project/my-shoot | -\u003e (1) | Seed:my-seed | ------------------------------------------------------------------------------- | | # ShootState:garden-my-project/my-shoot | -\u003e (1) | Shoot:garden-my-project/my-shoot | -------------------------------------------------------------------------------  ... (etc., similarly for the other resources) There are anchor links to easily jump from one resource to another, and the page provides means for filtering the results based on the kind, namespace, and/or name.\nPitfalls When there is a relevant update to an existing resource, i.e., when a reference to another resource is changed, then the corresponding vertex (along with all associated edges) is first deleted from the graph before it gets added again with the up-to-date edges. However, this does only work for vertices belonging to resources that are only created in exactly one “watch handler”. For example, the vertex for a SecretBinding can either be created in the SecretBinding handler itself or in the Shoot handler. In such cases, deleting the vertex before (re-)computing the edges might lead to race conditions and potentially renders the graph invalid. Consequently, instead of deleting the vertex, only the edges the respective handler is responsible for are deleted. If the vertex ends up with no remaining edges then it also gets deleted automatically. Afterwards, the vertex can either be added again or the updated edges can be created.\nSeedRestriction Admission Webhook Enablement The SeedRestriction is implemented as Kubernetes admission webhook and part of the gardener-admission-controller component running in the garden cluster.\n🎛 In order to activate it, you have to set .global.admission.seedRestriction.enabled=true when using the Gardener controlplane Helm chart. This will add an additional webhook in the existing ValidatingWebhookConfiguration of the gardener-admission-controller which contains the configuration for the SeedRestriction handler. Please note that it should only be activated when the SeedAuthorizer is active as well.\nAdmission Decisions The admission’s purpose is to perform extended validation on requests which require the body of the object in question. Additionally, it handles CREATE requests of gardenlets (above discussed resource dependency graph cannot be used in such cases because there won’t be any vertex/edge for non-existing resources).\nGardenlets are restricted to only create new resources which are somehow related to the seed clusters they are responsible for.\n","categories":"","description":"","excerpt":"Scoped API Access for Gardenlets By default, gardenlets have …","ref":"/docs/gardener/deployment/gardenlet_api_access/","tags":"","title":"Gardenlet API Access"},{"body":"GEP-NNNN: Your short, descriptive title Table of Contents  Summary Motivation  Goals Non-Goals   Proposal Alternatives  Summary Motivation Goals Non-Goals Proposal Alternatives ","categories":"","description":"","excerpt":"GEP-NNNN: Your short, descriptive title Table of Contents  Summary …","ref":"/docs/gardener/proposals/00-template/","tags":"","title":"GEP Title"},{"body":"Get a Shell to a Kubernetes Node To troubleshoot certain problems in a Kubernetes cluster, operators need access to the host of the Kubernetes node to troubleshoot problems. This can be required if a node misbehaves or fails to join the cluster in the first place.\nWith access to the host, it is for instance possible to check the kubelet logs and interact with common tools such as systemctland journalctl.\nThe first section of this guide explores options to get a shell to the node of a Gardener Kubernetes cluster. The options described in the second section do not rely on Kubernetes capabilities to get shell access to a node and thus can also be used if an instance failed to join the cluster.\nThis guide only covers how to get access to the host, but does not cover troubleshooting methods.\n Get a Shell to a Kubernetes Node Get a Shell to an operational cluster node  Gardener Dashboard gardenctl shell Gardener Ops Toolbelt Custom root pod   SSH access to a node that failed to join the cluster  Identifying the problematic instance gardenctl ssh SSH with manually created Bastion on AWS  Create the Bastion Security Group Create the bastion instance   Connecting to the target instance Cleanup    Get a Shell to an operational cluster node The following describes four different approaches to get a shell to an operational Shoot worker node. As a prerequisite to troubleshooting a Kubernetes node, the node must have joined the cluster successfully and be able to run a pod. All of the described approaches involve scheduling a pod with root permissions and mounting the root filesystem.\nGardener Dashboard Prerequisite: the terminal feature is configured for the Gardener dashboard.\nNavigate to the cluster overview page and find the Terminal in the Access tile.\nSelect the target Cluster (Garden, Seed / Control Plane, Shoot cluster) depending on the requirements and access rights (only certain users have access to the Seed Control Plane).\nTo open the terminal configuration, click on the top right-hand corner of the screen.\nSet the Terminal Runtime to “Privileged. Also specify the target node from the drop-down menu.\nThe dashboard then schedules a pod and opens a shell session to the node.\nTo get access to common binaries installed on the host, prefix the command with chroot /hostroot. Note that the path depends on where the root path is mounted in the container. In the default image used by the Dashboard, it is under /hostroot.\ngardenctl shell Prerequisite: kubectl and gardenctl are available and configured.\nFirst, target a Garden cluster containing all the Shoot definitions.\n$ gardenctl target garden \u003ctarget-garden\u003e Target an available Shoot by name. This sets up the context and configures the kubeconfig file of the Shoot cluster. Subsequent commands will execute in this context.\n$ gardenctl target shoot \u003ctarget-shoot\u003e Get the nodes of the Shoot cluster.\n$ gardenctl kubectl get nodes Pick a node name from the list above and get a root shell access to it.\n$ gardenctl shell \u003ctarget-node\u003e Gardener Ops Toolbelt Prerequisite: kubectl is available.\nThe Gardener ops-toolbelt can be used as a convenient way to deploy a root pod to a node. The pod uses an image that is bundled with a bunch of useful troubleshooting tools. This is also the same image that is used by default when using the Gardener Dashboard terminal feature as described in the previous section.\nThe easiest way to use the Gardener ops-toolbelt is to execute the ops-pod script in the hacks folder. To get root shell access to a node, execute the aforementioned script by supplying the target node name as an argument:\n$ \u003cpath-to-ops-toolbelt-repo\u003e/hacks/ops-pod \u003ctarget-node\u003e Custom root pod Alternatively, a pod can be assigned to a target node and a shell can be opened via standard Kubernetes means. To enable root access to the node, the pod specification requires proper securityContext and volume properties.\nFor instance you can use the following pod manifest, after changing  with the name of the node you want this pod attached to:\napiVersion: v1 kind: Pod metadata:  name: privileged-pod  namespace: default spec:  nodeSelector:  kubernetes.io/hostname: \u003ctarget-node-name\u003e  containers:  - name: busybox  image: busybox  stdin: true  securityContext:  privileged: true  volumeMounts:  - name: host-root-volume  mountPath: /host  readOnly: true  volumes:  - name: host-root-volume  hostPath:  path: /  hostNetwork: true  hostPID: true  restartPolicy: Never SSH access to a node that failed to join the cluster This section explores two options that can be used to get SSH access to a node that failed to join the cluster. As it is not possible to schedule a pod on the node, the Kubernetes-based methods explored so far cannot be used in this scenario.\nAdditionally, Gardener typically provisions worker instances in a private subnet of the VPC, hence - there is no public IP address that could be used for direct SSH access.\nFor this scenario, cloud providers typically have extensive documentation (e.g AWS \u0026 GCP and in some cases tooling support). However, these approaches are mostly cloud provider specific, require interaction via their CLI and API or sometimes the installation of a cloud provider specific agent one the node.\nAlternatively, gardenctl can be used providing a cloud provider agnostic and out-of-the-box support to get ssh access to an instance in a private subnet. Currently gardenctl supports AWS, GCP, Openstack, Azure and Alibaba Cloud.\nIdentifying the problematic instance First, the problematic instance has to be identified. In Gardener, worker pools can be created in different cloud provider regions, zones and accounts.\nThe instance would typically show up as successfully started / running in the cloud provider dashboard or API and it is not immediately obvious which one has a problem. Instead, we can use the Gardener API / CRDs to obtain the faulty instance identifier in a cloud-agnostic way.\nGardener uses the Machine Controller Manager to create the Shoot worker nodes. For each worker node, the Machine Controller Manager creates a Machine CRD in the Shoot namespace in the respective Seed cluster. Usually the problematic instance can be identified as the respective Machine CRD has status pending.\nThe instance / node name can be obtained from the Machine .status field:\n$ kubectl get machine \u003cmachine-name\u003e -o json | jq -r .status.node This is all the information needed to go ahead and use gardenctl ssh to get a shell to the node. In addition, the used cloud provider, the specific identifier of the instance and the instance region can be identified from the Machine CRD.\nGet the identifier of the instance via:\n$ kubectl get machine \u003cmachine-name\u003e -o json | jq -r .spec.providerID // e.g aws:///eu-north-1/i-069733c435bdb4640 The identifier shows that the instance belongs to the cloud provider aws with the ec2 instance-id i-069733c435bdb4640 in region eu-north-1.\nTo get more information about the instance, check out the MachineClass (e.g AWSMachineClass) that is associated with each Machine CRD in the Shoot namespace of the Seed cluster. The AWSMachineClass contains the machine image (ami), machine-type, iam information, network-interfaces, subnets, security groups and attached volumes.\nOf course, the information can also be used to get the instance with the cloud provider CLI / API.\ngardenctl ssh Using the node name of the problematic instance, we can use the gardenctl ssh command to get SSH access to the cloud provider instance via an automatically set up bastion host. gardenctl takes care of spinning up the bastion instance, setting up the SSH keys, ports and security groups and opens a root shell on the target instance. After the SSH session has ended, gardenctl deletes the created cloud provider resources.\nUse the following commands:\nFirst, target a Garden cluster containing all the Shoot definitions.\n$ gardenctl target garden \u003ctarget-garden\u003e Target an available Shoot by name. This sets up the context, configures the kubeconfig file of the Shoot cluster and downloads the cloud provider credentials. Subsequent commands will execute in this context.\n$ gardenctl target shoot \u003ctarget-shoot\u003e This uses the cloud provider credentials to spin up the bastion and to open a shell on the target instance.\n$ gardenctl ssh \u003ctarget-node\u003e SSH with manually created Bastion on AWS In case you are not using gardenctl or want to control the bastion instance yourself, you can also manually set it up. The steps described here are generally the same as those used by gardenctl internally. Despite some cloud provider specifics they can be generalized to the following list:\n Open port 22 on the target instance. Create an instance / VM in a public subnet (bastion instance needs to have public ip address). Set-up security groups, roles and open port 22 for the bastion instance.  The following diagram shows an overview how the SSH access to the target instance works:\nThis guide demonstrates the setup of a bastion on AWS.\nPrerequisites:\n The AWS CLI is set up. Obtain target instance-id (see here). Obtain the VPC ID the Shoot resources are created in. This can be found in the Infrastructure CRD in the Shoot namespace in the Seed. Make sure that port 22 on the target instance is open (default for Gardener deployed instances).  Extract security group via  $ aws ec2 describe-instances --instance-ids \u003cinstance-id\u003e  Check for rule that allows inbound connections on port 22:  $ aws ec2 describe-security-groups --group-ids=\u003csecurity-group-id\u003e  If not available, create the rule with the following comamnd:  $ aws ec2 authorize-security-group-ingress --group-id \u003csecurity-group-id\u003e --protocol tcp --port 22 --cidr 0.0.0.0/0   Create the Bastion Security Group   The common name of the security group is \u003cshoot-name\u003e-bsg. Create the security group:\n$ aws ec2 create-security-group --group-name \u003cbastion-security-group-name\u003e --description ssh-access --vpc-id \u003cVPC-ID\u003e   Optionally, create identifying tags for the security group:\n$ aws ec2 create-tags --resources \u003cbastion-security-group-id\u003e --tags Key=component,Value=\u003ctag\u003e   Create permission in the bastion security group that allows ssh access on port 22.\n$ aws ec2 authorize-security-group-ingress --group-id \u003cbastion-security-group-id\u003e --protocol tcp --port 22 --cidr 0.0.0.0/0   Create an IAM role for the bastion instance with the name \u003cshoot-name\u003e-bastions:\n$ aws iam create-role --role-name \u003cshoot-name\u003e-bastions The content should be:\n  { \"Version\": \"2012-10-17\", \"Statement\": [  {  \"Effect\": \"Allow\",  \"Action\": [  \"ec2:DescribeRegions\"  ],  \"Resource\": [  \"*\"  ]  } ] }   Create the instance profile with name \u003cshoot-name\u003e-bastions:\n$ aws iam create-instance-profile --instance-profile-name \u003cname\u003e   Add the created role to the instance profile:\n$ aws iam add-role-to-instance-profile --instance-profile-name \u003cinstance-profile-name\u003e --role-name \u003crole-name\u003e   Create the bastion instance Next, in order to be able to ssh into the bastion instance, the instance has to be set up with a user with a public ssh key. Create a user gardener that has the same Gardener-generated public ssh key as the target instance.\n  First, we need to get the public part of the Shoot ssh-key. The ssh-key is stored in a secret in the the project namespace in the Garden cluster. The name is: \u003cshoot-name\u003e-ssh-publickey. Get the key via:\n$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\\\"id_rsa.pub\\\"   A script handed over as user-data to the bastion ec2 instance, can be used to create the gardener user and add the ssh-key. For your convenience, you can use the following script to generate the user-data.\n  #!/bin/bash -eu saveUserDataFile () {  ssh_key=$1  cat \u003e gardener-bastion-userdata.sh \u003c\u003cEOF #!/bin/bash -eu id gardener || useradd gardener -mU mkdir -p /home/gardener/.ssh echo \"$ssh_key\" \u003e /home/gardener/.ssh/authorized_keys chown gardener:gardener /home/gardener/.ssh/authorized_keys echo \"gardener ALL=(ALL) NOPASSWD:ALL\" \u003e/etc/sudoers.d/99-gardener-user EOF }   if [ -p /dev/stdin ]; then  read -r input  cat | saveUserDataFile \"$input\" else  pbpaste | saveUserDataFile \"$input\" fi   Use the script by handing-over the public ssh-key of the Shoot cluster:\n$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\\\"id_rsa.pub\\\" | ./generate-userdata.sh This generates a file called gardener-bastion-userdata.sh in the same directory containing the user-data.\n  The following information is needed to create the bastion instance:\nbastion-IAM-instance-profile-name\n Use the created instance profile with name \u003cshoot-name\u003e-bastions  image-id\n Possible use the same image-id as for the target instance (or any other image). Has cloud provider specific format (AWS: ami).  ssh-public-key-name\n This is the ssh key pair already created in the Shoot’s cloud provider account by Gardener during the Infrastructure CRD reconciliation. The name is usually: \u003cshoot-name\u003e-ssh-publickey  subnet-id\n Choose a subnet that is attached to an Internet Gateway and NAT Gateway (bastion instance must have a public IP). The Gardener created public subnet with the name \u003cshoot-name\u003e-public-utility-\u003cxy\u003e can be used. Please check the created subnets with the cloud provider.  bastion-security-group-id\n Use the id of the created bastion security group.  file-path-to-userdata\n  Use the filepath to user-data file generated in the previous step.\n  bastion-instance-name\n Optional to tag the instance. Usually \u003cshoot-name\u003e-bastions      Create the bastion instance via:\n  $ ec2 run-instances --iam-instance-profile Name=\u003cbastion-IAM-instance-profile-name\u003e --image-id \u003cimage-id\u003e --count 1 --instance-type t3.nano --key-name \u003cssh-public-key-name\u003e --security-group-ids \u003cbastion-security-group-id\u003e --subnet-id \u003csubnet-id\u003e --associate-public-ip-address --user-data \u003cfile-path-to-userdata\u003e --tag-specifications ResourceType=instance,Tags=[{Key=Name,Value=\u003cbastion-instance-name\u003e},{Key=component,Value=\u003cmytag\u003e}] ResourceType=volume,Tags=[{Key=component,Value=\u003cmytag\u003e}]\" Capture the instance-id from the reponse and wait until the ec2 instance is running and has a public ip address.\nConnecting to the target instance Save the private key of the ssh-key-pair in a temporary local file for later use.\n$ umask 077 $ kubectl get secret \u003cshoot-name\u003e.ssh-keypair -o json | jq -r .data.\\\"id_rsa\\\" | base64 -d \u003e id_rsa.key Use the private ssh key to ssh into the bastion instance.\n$ ssh -i \u003cpath-to-private-key\u003e gardener@\u003cpublic-bastion-instance-ip\u003e If that works, connect from your local terminal to the target instance via the bastion.\n$ ssh -i \u003cpath-to-private-key\u003e -o ProxyCommand=\"ssh -W %h:%p -i \u003cprivate-key\u003e -o IdentitiesOnly=yes -o StrictHostKeyChecking=no gardener@\u003cpublic-ip-bastion\u003e\" gardener@\u003cprivate-ip-target-instance\u003e -o IdentitiesOnly=yes -o StrictHostKeyChecking=no Cleanup Do not forget to cleanup the created resources. Otherwise Gardener will eventually fail to delete the Shoot.\n","categories":"","description":"Describes the methods for getting shell access to worker nodes.","excerpt":"Describes the methods for getting shell access to worker nodes.","ref":"/docs/guides/monitoring_and_troubleshooting/shell-to-node/","tags":"","title":"Get a Shell to a Gardener Shoot Worker Node"},{"body":"Deploying Gardener locally This document will walk you through deploying Gardener on your local machine. If you encounter difficulties, please open an issue so that we can make this process easier.\nGardener runs in any Kubernetes cluster. In this guide, we will start a KinD cluster which is used as both garden and seed cluster (please refer to the architecture overview) for simplicity.\nBased on Skaffold, the container images for all required components will be built and deployed into the cluster (via their Helm charts).\nPrerequisites  Make sure that you have followed the Local Setup guide up until the Get the sources step. Make sure your Docker daemon is up-to-date, up and running and has enough resources (at least 8 CPUs and 8Gi memory; see here how to configure the resources for Docker for Mac).  Please note that 8 CPU / 8Gi memory might not be enough for more than two Shoot clusters, i.e., you might need to increase these values if you want to run additional Shoots. If you plan on following the optional steps to create a second seed cluster, the required resources will be more - at least 10 CPUs and 18Gi memory. Additionally, please configure at least 120Gi of disk size for the Docker daemon. Tip: With docker system df and docker system prune -a you can cleanup unused data.\n   Setting up the KinD cluster (garden and seed) make kind-up This command sets up a new KinD cluster named gardener-local and stores the kubeconfig in the ./example/gardener-local/kind/kubeconfig file.\n It might be helpful to copy this file to $HOME/.kube/config since you will need to target this KinD cluster multiple times. Alternatively, make sure to set your KUBECONFIG environment variable to ./example/gardener-local/kind/kubeconfig for all future steps via export KUBECONFIG=example/gardener-local/kind/kubeconfig.\n All following steps assume that you are using this kubeconfig.\nAdditionally, this command also deploys a local container registry to the cluster as well as a few registry mirrors, that are set up as a pull-through cache for all upstream registries Gardener uses by default. This is done to speed up image pulls across local clusters. The local registry can be accessed as localhost:5001 for pushing and pulling. The storage directories of the registries are mounted to the host machine under dev/local-registry. With this, mirrored images don’t have to be pulled again after recreating the cluster.\nThe command also deploys a default calico installation as the cluster’s CNI implementation with NetworkPolicy support (the default kindnet CNI doesn’t provide NetworkPolicy support). Furthermore, it deploys the metrics-server in order to support HPA and VPA on the seed cluster.\nSetting up Gardener make gardener-up This will first build the images based (which might take a bit if you do it for the first time). Afterwards, the Gardener resources will be deployed into the cluster.\nCreating a Shoot cluster You can wait for the Seed to be ready by running\nkubectl wait --for=condition=gardenletready --for=condition=extensionsready --for=condition=bootstrapped seed local --timeout=5m Alternatively, you can run kubectl get seed local and wait for the STATUS to indicate readiness:\nNAME STATUS PROVIDER REGION AGE VERSION K8S VERSION local Ready local local 4m42s vX.Y.Z-dev v1.21.1 In order to create a first shoot cluster, just run\nkubectl apply -f example/provider-local/shoot.yaml You can wait for the Shoot to be ready by running\nkubectl wait --for=condition=apiserveravailable --for=condition=controlplanehealthy --for=condition=everynodeready --for=condition=systemcomponentshealthy shoot local -n garden-local --timeout=10m Alternatively, you can run kubectl -n garden-local get shoot local and wait for the LAST OPERATION to reach 100%:\nNAME CLOUDPROFILE PROVIDER REGION K8S VERSION HIBERNATION LAST OPERATION STATUS AGE local local local local 1.21.0 Awake Create Processing (43%) healthy 94s (Optional): You could also execute a simple e2e test (creating and deleting a shoot) by running\nmake test-e2e-local-simple KUBECONFIG=\"$PWD/example/gardener-local/kind/kubeconfig\" Accessing the Shoot cluster ⚠️ Please note that in this setup shoot clusters are not accessible by default when you download the kubeconfig and try to communicate with them. The reason is that your host most probably cannot resolve the DNS names of the clusters since provider-local extension runs inside the KinD cluster (see this for more details). Hence, if you want to access the shoot cluster, you have to run the following command which will extend your /etc/hosts file with the required information to make the DNS names resolvable:\ncat \u003c\u003cEOF | sudo tee -a /etc/hosts # Manually created to access local Gardener shoot clusters. # TODO: Remove this again when the shoot cluster access is no longer required. 127.0.0.1 api.local.local.external.local.gardener.cloud 127.0.0.1 api.local.local.internal.local.gardener.cloud 127.0.0.1 api.e2e-managedseed.garden.external.local.gardener.cloud 127.0.0.1 api.e2e-managedseed.garden.internal.local.gardener.cloud 127.0.0.1 api.e2e-hibernated.local.external.local.gardener.cloud 127.0.0.1 api.e2e-hibernated.local.internal.local.gardener.cloud 127.0.0.1 api.e2e-unpriv.local.external.local.gardener.cloud 127.0.0.1 api.e2e-unpriv.local.internal.local.gardener.cloud 127.0.0.1 api.e2e-wake-up.local.external.local.gardener.cloud 127.0.0.1 api.e2e-wake-up.local.internal.local.gardener.cloud 127.0.0.1 api.e2e-migrate.local.external.local.gardener.cloud 127.0.0.1 api.e2e-migrate.local.internal.local.gardener.cloud 127.0.0.1 api.e2e-rotate.local.external.local.gardener.cloud 127.0.0.1 api.e2e-rotate.local.internal.local.gardener.cloud 127.0.0.1 api.e2e-default.local.external.local.gardener.cloud 127.0.0.1 api.e2e-default.local.internal.local.gardener.cloud EOF Now you can access it by running\nkubectl -n garden-local get secret local.kubeconfig -o jsonpath={.data.kubeconfig} | base64 -d \u003e /tmp/kubeconfig-shoot-local.yaml kubectl --kubeconfig=/tmp/kubeconfig-shoot-local.yaml get nodes (Optional): Setting up a second seed cluster There are cases where you would want to create a second cluster seed in your local setup. For example, if you want to test the control plane migration feature. The following steps describe how to do that.\nmake kind2-up This command sets up a new KinD cluster named gardener-local2 and stores its kubeconfig in the ./example/gardener-local/kind2/kubeconfig file.\nIn order to deploy required resources in the KinD cluster that you just created, run:\nmake gardenlet-kind2-up The following steps assume that you are using the kubeconfig that points to the gardener-local cluster (first KinD cluster): export KUBECONFIG=example/gardener-local/kind/kubeconfig.\nYou can wait for the local2 Seed to be ready by running:\nkubectl wait --for=condition=gardenletready --for=condition=extensionsready --for=condition=bootstrapped seed local2 --timeout=5m Alternatively, you can run kubectl get seed local2 and wait for the STATUS to indicate readiness:\nNAME STATUS PROVIDER REGION AGE VERSION K8S VERSION local2 Ready local local 4m42s vX.Y.Z-dev v1.21.1 If you want to perform control plane migration you can follow the steps outlined here to migrate the shoot cluster to the second seed you just created.\nDeleting the Shoot cluster ./hack/usage/delete shoot local garden-local (Optional): Tear down the second seed cluster make kind2-down Tear down the Gardener environment make kind-down Remote local setup Just like Prow is executing the KinD based integration tests in a K8s pod, it is possible to interactively run this KinD based Gardener development environment aka “local setup” in a “remote” K8s pod.\nk apply -f docs/deployment/content/remote-local-setup.yaml k exec -it deployment/remote-local-setup -- sh  tmux -u a Caveats Please refer to the TMUX documentation for working effectively inside the remote-local-setup pod.\nTo access Grafana, Prometheus or other components in a browser, two port forwards are needed:\nThe port forward from the laptop to the pod:\nk port-forward deployment/remote-local-setup 3000 The port forward in the remote-local-setup pod to the respective component:\nk port-forward -n shoot--local--local deployment/grafana-operators 3000 Further reading This setup makes use of the local provider extension. You can read more about it in this document.\n","categories":"","description":"","excerpt":"Deploying Gardener locally This document will walk you through …","ref":"/docs/gardener/deployment/getting_started_locally/","tags":"","title":"Getting Started Locally"},{"body":"Running Gardener locally This document will walk you through running Gardener on your local machine for development purposes. If you encounter difficulties, please open an issue so that we can make this process easier.\nGardener runs in any Kubernetes cluster. In this guide, we will start a KinD cluster which is used as both garden and seed cluster (please refer to the architecture overview) for simplicity.\nThe Gardener components, however, will be run as regular processes on your machine (hence, no container images are being built).\nPrerequisites   Make sure that you have followed the Local Setup guide up until the Get the sources step.\n  Make sure your Docker daemon is up-to-date, up and running and has enough resources (at least 4 CPUs and 4Gi memory; see here how to configure the resources for Docker for Mac).\n Please note that 4 CPU / 4Gi memory might not be enough for more than one Shoot cluster, i.e., you might need to increase these values if you want to run additional Shoots. If you plan on following the optional steps to create a second seed cluster, the required resources will be more - at least 10 CPUs and 16Gi memory.\n Additionally, please configure at least 120Gi of disk size for the Docker daemon.\n Tip: With docker system df and docker system prune -a you can cleanup unused data.\n   Make sure that you increase the maximum number of open files on your host:\n  On Mac, run sudo launchctl limit maxfiles 65536 200000\n  On Linux, extend the /etc/security/limits.conf file with\n* hard nofile 97816 * soft nofile 97816 and reload the terminal.\n    Setting up the KinD cluster (garden and seed) make kind-up KIND_ENV=local This command sets up a new KinD cluster named gardener-local and stores the kubeconfig in the ./example/gardener-local/kind/kubeconfig file.\n It might be helpful to copy this file to $HOME/.kube/config since you will need to target this KinD cluster multiple times. Alternatively, make sure to set your KUBECONFIG environment variable to ./example/gardener-local/kind/kubeconfig for all future steps via export KUBECONFIG=example/gardener-local/kind/kubeconfig.\n All following steps assume that you are using this kubeconfig.\nAdditionally, this command also deploys a local container registry to the cluster as well as a few registry mirrors, that are set up as a pull-through cache for all upstream registries Gardener uses by default. This is done to speed up image pulls across local clusters. The local registry can be accessed as localhost:5001 for pushing and pulling. The storage directories of the registries are mounted to the host machine under dev/local-registry. With this, mirrored images don’t have to be pulled again after recreating the cluster.\nThe command also deploys a default calico installation as the cluster’s CNI implementation with NetworkPolicy support (the default kindnet CNI doesn’t provide NetworkPolicy support). Furthermore, it deploys the metrics-server in order to support HPA and VPA on the seed cluster.\nSetting up Gardener make dev-setup # preparing the environment (without webhooks for now) kubectl wait --for=condition=ready pod -l run=etcd -n garden --timeout 2m # wait for etcd to be ready make start-apiserver # starting gardener-apiserver In a new terminal pane, run\nkubectl wait --for=condition=available apiservice v1beta1.core.gardener.cloud # wait for gardener-apiserver to be ready make start-admission-controller # starting gardener-admission-controller In a new terminal pane, run\nmake dev-setup DEV_SETUP_WITH_WEBHOOKS=true # preparing the environment with webhooks make start-controller-manager # starting gardener-controller-manager (Optional): In a new terminal pane, run\nmake start-scheduler # starting gardener-scheduler In a new terminal pane, run\nmake register-local-env # registering the local environment (CloudProfile, Seed, etc.) make start-gardenlet SEED_NAME=local # starting gardenlet In a new terminal pane, run\nmake start-extension-provider-local # starting gardener-extension-provider-local ℹ️ The provider-local is started with elevated privileges since it needs to manipulate your /etc/hosts file to enable you accessing the created shoot clusters from your local machine, see this for more details.\nCreating a Shoot cluster You can wait for the Seed to be ready by running\nkubectl wait --for=condition=gardenletready --for=condition=extensionsready --for=condition=bootstrapped seed local --timeout=5m Alternatively, you can run kubectl get seed local and wait for the STATUS to indicate readiness:\nNAME STATUS PROVIDER REGION AGE VERSION K8S VERSION local Ready local local 4m42s vX.Y.Z-dev v1.21.1 In order to create a first shoot cluster, just run\nkubectl apply -f example/provider-local/shoot.yaml You can wait for the Shoot to be ready by running\nkubectl wait --for=condition=apiserveravailable --for=condition=controlplanehealthy --for=condition=everynodeready --for=condition=systemcomponentshealthy shoot local -n garden-local --timeout=10m Alternatively, you can run kubectl -n garden-local get shoot local and wait for the LAST OPERATION to reach 100%:\nNAME CLOUDPROFILE PROVIDER REGION K8S VERSION HIBERNATION LAST OPERATION STATUS AGE local local local local 1.21.0 Awake Create Processing (43%) healthy 94s (Optional): You could also execute a simple e2e test (creating and deleting a shoot) by running\nmake test-e2e-local-simple KUBECONFIG=\"$PWD/example/gardener-local/kind/kubeconfig\" When the shoot got successfully created you can access it as follows:\nkubectl -n garden-local get secret local.kubeconfig -o jsonpath={.data.kubeconfig} | base64 -d \u003e /tmp/kubeconfig-shoot-local.yaml kubectl --kubeconfig=/tmp/kubeconfig-shoot-local.yaml get nodes (Optional): Setting up a second seed cluster There are cases where you would want to create a second seed cluster in your local setup. For example, if you want to test the control plane migration feature. The following steps describe how to do that.\nAdd a new IP address on your loopback device which will be necessary for the new KinD cluster that you will create. On Mac, the default loopback device is lo0.\nsudo ip addr add 127.0.0.2 dev lo0 # adding 127.0.0.2 ip to the loopback interface Next, setup the second KinD cluster:\nmake kind2-up KIND_ENV=local This command sets up a new KinD cluster named gardener-local2 and stores its kubeconfig in the ./example/gardener-local/kind2/kubeconfig file. You will need this file when starting the provider-local extension controller for the second seed cluster.\nmake register-kind2-env # registering the local2 seed make start-gardenlet SEED_NAME=local2 # starting gardenlet for the local2 seed In a new terminal pane, run\nexport KUBECONFIG=./example/gardener-local/kind2/kubeconfig # setting KUBECONFIG to point to second kind cluster make start-extension-provider-local \\  WEBHOOK_SERVER_PORT=9444 \\  WEBHOOK_CERT_DIR=/tmp/gardener-extension-provider-local2 \\  SERVICE_HOST_IP=127.0.0.2 \\  METRICS_BIND_ADDRESS=:8082 \\  HEALTH_BIND_ADDRESS=:8083 # starting gardener-extension-provider-local If you want to perform a control plane migration you can follow the steps outlined here to migrate the shoot cluster to the second seed you just created.\nDeleting the Shoot cluster ./hack/usage/delete shoot local garden-local (Optional): Tear down the second seed cluster make tear-down-kind2-env make kind2-down Tear down the Gardener environment make tear-down-local-env make kind-down Remote local setup Just like Prow is executing the KinD based integration tests in a K8s pod, it is possible to interactively run this KinD based Gardener development environment aka “local setup” in a “remote” K8s pod.\nk apply -f docs/development/content/remote-local-setup.yaml k exec -it deployment/remote-local-setup -- sh  tmux -u a Caveats Please refer to the TMUX documentation for working effectively inside the remote-local-setup pod.\nTo access Grafana, Prometheus or other components in a browser, two port forwards are needed:\nThe port forward from the laptop to the pod:\nk port-forward deployment/remote-local-setup 3000 The port forward in the remote-local-setup pod to the respective component:\nk port-forward -n shoot--local--local deployment/grafana-operators 3000 Further reading This setup makes use of the local provider extension. You can read more about it in this document.\n","categories":"","description":"","excerpt":"Running Gardener locally This document will walk you through running …","ref":"/docs/gardener/development/getting_started_locally/","tags":"","title":"Getting Started Locally"},{"body":"Intro Be aware, that the following sections might be opinionated. Kubernetes, and the GPU support in particular, are rapidly evolving, which means that this guide is likely to be outdated sometime soon. For this reason, contributions are highly appreciated to update this guide.\nCreate a Cluster First thing first, let’s create a k8s cluster with GPU accelerated nodes. In this example we will use AWS p2.xlarge EC2 instance because it’s the cheapest available option at the moment. Use such cheap instances for learning to limit your resource costs. This costs around 1€/hour per GPU\nInstall NVidia Driver as Daemonset apiVersion: apps/v1 kind: DaemonSet metadata:  name: nvidia-driver-installer  namespace: kube-system  labels:  k8s-app: nvidia-driver-installer spec:  selector:  matchLabels:  name: nvidia-driver-installer  k8s-app: nvidia-driver-installer  template:  metadata:  labels:  name: nvidia-driver-installer  k8s-app: nvidia-driver-installer  spec:  hostPID: true  initContainers:  - image: squat/modulus:4a1799e7aa0143bcbb70d354bab3e419b1f54972  name: modulus  args:  - compile  - nvidia  - \"410.104\"  securityContext:  privileged: true  env:  - name: MODULUS_CHROOT  value: \"true\"  - name: MODULUS_INSTALL  value: \"true\"  - name: MODULUS_INSTALL_DIR  value: /opt/drivers  - name: MODULUS_CACHE_DIR  value: /opt/modulus/cache  - name: MODULUS_LD_ROOT  value: /root  - name: IGNORE_MISSING_MODULE_SYMVERS  value: \"1\"  volumeMounts:  - name: etc-coreos  mountPath: /etc/coreos  readOnly: true  - name: usr-share-coreos  mountPath: /usr/share/coreos  readOnly: true  - name: ld-root  mountPath: /root  - name: module-cache  mountPath: /opt/modulus/cache  - name: module-install-dir-base  mountPath: /opt/drivers  - name: dev  mountPath: /dev  containers:  - image: \"gcr.io/google-containers/pause:3.1\"  name: pause  tolerations:  - key: \"nvidia.com/gpu\"  effect: \"NoSchedule\"  operator: \"Exists\"  volumes:  - name: etc-coreos  hostPath:  path: /etc/coreos  - name: usr-share-coreos  hostPath:  path: /usr/share/coreos  - name: ld-root  hostPath:  path: /  - name: module-cache  hostPath:  path: /opt/modulus/cache  - name: dev  hostPath:  path: /dev  - name: module-install-dir-base  hostPath:  path: /opt/drivers Install Device Plugin apiVersion: apps/v1 kind: DaemonSet metadata:  name: nvidia-gpu-device-plugin  namespace: kube-system  labels:  k8s-app: nvidia-gpu-device-plugin  #addonmanager.kubernetes.io/mode: Reconcile spec:  selector:  matchLabels:  k8s-app: nvidia-gpu-device-plugin  template:  metadata:  labels:  k8s-app: nvidia-gpu-device-plugin  annotations:  scheduler.alpha.kubernetes.io/critical-pod: ''  spec:  priorityClassName: system-node-critical  volumes:  - name: device-plugin  hostPath:  path: /var/lib/kubelet/device-plugins  - name: dev  hostPath:  path: /dev  containers:  - image: \"k8s.gcr.io/nvidia-gpu-device-plugin@sha256:08509a36233c5096bb273a492251a9a5ca28558ab36d74007ca2a9d3f0b61e1d\"  command: [\"/usr/bin/nvidia-gpu-device-plugin\", \"-logtostderr\", \"-host-path=/opt/drivers/nvidia\"]  name: nvidia-gpu-device-plugin  resources:  requests:  cpu: 50m  memory: 10Mi  limits:  cpu: 50m  memory: 10Mi  securityContext:  privileged: true  volumeMounts:  - name: device-plugin  mountPath: /device-plugin  - name: dev  mountPath: /dev  updateStrategy:  type: RollingUpdate Test To run an example training on a GPU node, start first a base image with Tensorflow with GPU support \u0026 Keras\napiVersion: apps/v1 kind: Deployment metadata:  name: deeplearning-workbench  namespace: default spec:  replicas: 1  selector:  matchLabels:  app: deeplearning-workbench  template:  metadata:  labels:  app: deeplearning-workbench  spec:  containers:  - name: deeplearning-workbench  image: afritzler/deeplearning-workbench  resources:  limits:  nvidia.com/gpu: 1  tolerations:  - key: \"nvidia.com/gpu\"  effect: \"NoSchedule\"  operator: \"Exists\" Note: the tolerations section above is not required if you deploy the ExtendedResourceToleration admission controller to your cluster. You can do this in the kubernetes section of your Gardener cluster shoot.yaml as follows:\n kubernetes: kubeAPIServer: admissionPlugins: - name: ExtendedResourceToleration Now exec into the container and start an example Keras training\nkubectl exec -it deeplearning-workbench-8676458f5d-p4d2v -- /bin/bash cd /keras/example python imdb_cnn.py Acknowledgments \u0026 References  Andreas Fritzler from the Gardener Core team for the R\u0026D and providing this setup. Build and install NVIDIA driver on CoreOS Nvidia Device Plugin  ","categories":"","description":"Setting up a GPU Enabled Cluster for Deep Learning","excerpt":"Setting up a GPU Enabled Cluster for Deep Learning","ref":"/docs/tutorials/gpu/","tags":"","title":"GPU Enabled Cluster"},{"body":"GRPC based implementation of Cloud Providers - WIP Goal: Currently the Cloud Providers’ (CP) functionalities ( Create(), Delete(), List() ) are part of the Machine Controller Manager’s (MCM)repository. Because of this, adding support for new CPs into MCM requires merging code into MCM which may not be required for core functionalities of MCM itself. Also, for various reasons it may not be feasible for all CPs to merge their code with MCM which is an Open Source project.\nBecause of these reasons, it was decided that the CP’s code will be moved out in separate repositories so that they can be maintained separately by the respective teams. Idea is to make MCM act as a GRPC server, and CPs as GRPC clients. The CP can register themselves with the MCM using a GRPC service exposed by the MCM. Details of this approach is discussed below.\nHow it works: MCM acts as GRPC server and listens on a pre-defined port 5000. It implements below GRPC services. Details of each of these services are mentioned in next section.\n Register() GetMachineClass() GetSecret()  GRPC services exposed by MCM: Register() rpc Register(stream DriverSide) returns (stream MCMside) {}\nThe CP GRPC client calls this service to register itself with the MCM. The CP passes the kind and the APIVersion which it implements, and MCM maintains an internal map for all the registered clients. A GRPC stream is returned in response which is kept open througout the life of both the processes. MCM uses this stream to communicate with the client for machine operations: Create(), Delete() or List(). The CP client is responsible for reading the incoming messages continuously, and based on the operationType parameter embedded in the message, it is supposed to take the required action. This part is already handled in the package grpc/infraclient. To add a new CP client, import the package, and implement the ExternalDriverProvider interface:\ntype ExternalDriverProvider interface { Create(machineclass *MachineClassMeta, credentials, machineID, machineName string) (string, string, error) Delete(machineclass *MachineClassMeta, credentials, machineID string) error List(machineclass *MachineClassMeta, credentials, machineID string) (map[string]string, error) } GetMachineClass() rpc GetMachineClass(MachineClassMeta) returns (MachineClass) {}\nAs part of the message from MCM for various machine operations, the name of the machine class is sent instead of the full machine class spec. The CP client is expected to use this GRPC service to get the full spec of the machine class. This optionally enables the client to cache the machine class spec, and make the call only if the machine calass spec is not already cached.\nGetSecret() rpc GetSecret(SecretMeta) returns (Secret) {}\nAs part of the message from MCM for various machine operations, the Cloud Config (CC) and CP credentials are not sent. The CP client is expected to use this GRPC service to get the secret which has CC and CP’s credentials from MCM. This enables the client to cache the CC and credentials, and to make the call only if the data is not already cached.\nHow to add a new Cloud Provider’s support Import the package grpc/infraclient and grpc/infrapb from MCM (currently in MCM’s “grpc-driver” branch)\n Implement the interface ExternalDriverProvider  Create(): Creates a new machine Delete(): Deletes a machine List(): Lists machines   Use the interface MachineClassDataProvider  GetMachineClass(): Makes the call to MCM to get machine class spec GetSecret(): Makes the call to MCM to get secret containing Cloud Config and CP’s credentials    Example implementation: Refer GRPC based implementation for AWS client: https://github.com/ggaurav10/aws-driver-grpc\n","categories":"","description":"","excerpt":"GRPC based implementation of Cloud Providers - WIP Goal: Currently the …","ref":"/docs/other-components/machine-controller-manager/docs/proposals/external_providers_grpc/","tags":"","title":"GRPC Based Implementation of Cloud Providers"},{"body":"Gardener Extension for the gVisor Container Runtime Sandbox  \nProject Gardener implements the automated management and operation of Kubernetes clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.\nRecently, most of the vendor specific logic has been developed in-tree. However, the project has grown to a size where it is very hard to extend, maintain, and test. With GEP-1 we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.\n How to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the ./dev/kubeconfig file.\nStatic code checks and tests can be executed by running make verify. We are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io GEP-1 (Gardener Enhancement Proposal) on extensibility GEP-10 (Additional Container Runtimes) Extensibility API documentation  ","categories":"","description":"Gardener extension controller for the gVisor container runtime sandbox","excerpt":"Gardener extension controller for the gVisor container runtime sandbox","ref":"/docs/extensions/container-runtime-extensions/gardener-extension-runtime-gvisor/","tags":"","title":"GVisor container runtime"},{"body":"Hardening the Gardener Community Setup Context Gardener stakeholders in the Open Source community usually use the Gardener Setup Scripts, to create a Garden cluster based on Kubernetes v1.9 which then can be used to create Shoot clusters based on Kubernetes v1.10, v1.11 and v1.12. Shoot clusters can play the following roles in a Gardener landscape:\n Seed cluster Shoot cluster  As Alban Crequy from Kinvolk has recommended in his recent Gardener blog Auditing Kubernetes for Secure Setup the Gardener Team at SAP has applied several means to harden the Gardener landscapes at SAP.\nRecommendations Mitigation for Gardener CVE-2018-2475 The following recommendations describe how you can harden your Gardener Community Setup by adding a Seed cluster hardened with network policies.\n Use the Gardener Setup Scripts to create a Garden cluster in a dedicated IaaS account Create a Shoot cluster in a different IaaS account As a precaution you should not deploy the Kubernetes dashboard on this Shoot cluster Register this newly created Shoot cluster as a Seed cluster in the Gardener End user Shoot clusters can then be created using this newly created Seed cluster (which in turn is a Shoot cluster).  A tutorial on how to create a shooted seed cluster can be found here.\nThe rational behind this activity is, that Calico network policies harden this Seed cluster but the community installer uses Flannel which does not offer these features for the Garden cluster.\nWhen you have added a hardened Seed cluster you are expected not be vulnerable to the Gardener CVE-2018-2475 anymore.\nMitigation for Kubernetes CVE-2018-1002105 In addition when you follow the recommendations in the recent Gardener Security Announcement you are expected not be vulnerable to the Kubernetes CVE-2018-1002105 with your hardened Gardener Community Setup.\nAlternative Approach For this alternative approach there is no Gardener blog available, it is not part of the Gardener Setup Scripts, but it was tested by the Gardener Team at SAP. Use GKE to host a Garden cluster based on Kubernetes v1.10, v1.11 and v1.12 (without the Kubernetes dashboard) in a dedicated GCP account. If you do this by your own, please ensure that the network policies are turned on, which might not be the case by default. Then you can apply the security configuration which Alban Crequy from Kinvolk has recommended in his blog directly in the Garden cluster and create Shoot clusters from there in a different IaaS account.\n","categories":"","description":"","excerpt":"Hardening the Gardener Community Setup Context Gardener stakeholders …","ref":"/docs/guides/install_gardener/secure-setup/","tags":"","title":"Hardening the Gardener Community Setup"},{"body":"Health Check Library Goal Typically an extension reconciles a specific resource (Custom Resource Definitions (CRDs)) and creates/modifies resources in the cluster (via helm, managed resources, kubectl, …). We call these API Objects ‘dependent objects’ - as they are bound to the lifecycle of the extension.\nThe goal of this library is to enable extensions to setup health checks for their ‘dependent objects’ with minimal effort.\nUsage The library provides a generic controller with the ability to register any resource that satisfies the extension object interface. An example is the Worker CRD.\nHealth check functions for commonly used dependent objects can be reused and registered with the controller, such as:\n Deployment DaemonSet StatefulSet ManagedResource (Gardener specific)  See below example taken from the provider-aws.\nhealth.DefaultRegisterExtensionForHealthCheck(  aws.Type,  extensionsv1alpha1.SchemeGroupVersion.WithKind(extensionsv1alpha1.WorkerResource),  func() runtime.Object { return \u0026extensionsv1alpha1.Worker{} },  mgr, // controller runtime manager  opts, // options for the health check controller  nil, // custom predicates  map[extensionshealthcheckcontroller.HealthCheck]string{  general.CheckManagedResource(genericactuator.McmShootResourceName): string(gardencorev1beta1.ShootSystemComponentsHealthy),  general.CheckSeedDeployment(aws.MachineControllerManagerName): string(gardencorev1beta1.ShootEveryNodeReady),  worker.SufficientNodesAvailable(): string(gardencorev1beta1.ShootEveryNodeReady),  }) This creates a health check controller that reconciles the extensions.gardener.cloud/v1alpha1.Worker resource with the spec.type ‘aws’. Three health check functions are registered that are executed during reconciliation. Each health check is mapped to a single HealthConditionType that results in conditions with the same condition.type (see below). To contribute to the Shoot’s health, the following can be used: SystemComponentsHealthy, EveryNodeReady, ControlPlaneHealthy. The Gardener/Gardenlet checks each extension for conditions matching these types. However extensions are free to choose any HealthConditionType. More information can be found here.\nA health check has to satisfy below interface. You can find implementation examples here.\ntype HealthCheck interface {  // Check is the function that executes the actual health check  Check(context.Context, types.NamespacedName) (*SingleCheckResult, error)  // InjectSeedClient injects the seed client  InjectSeedClient(client.Client)  // InjectShootClient injects the shoot client  InjectShootClient(client.Client)  // SetLoggerSuffix injects the logger  SetLoggerSuffix(string, string)  // DeepCopy clones the healthCheck  DeepCopy() HealthCheck } The health check controller regularly (default: 30s) reconciles the extension resource and executes the registered health checks for the dependent objects. As a result, the controller writes condition(s) to the status of the extension containing the health check result. In our example, two checks are mapped to ShootEveryNodeReady and one to ShootSystemComponentsHealthy, leading to conditions with two distinct HealthConditionTypes (condition.type)\nstatus:  conditions:  - lastTransitionTime: \"20XX-10-28T08:17:21Z\"  lastUpdateTime: \"20XX-11-28T08:17:21Z\"  message: (1/1) Health checks successful  reason: HealthCheckSuccessful  status: \"True\"  type: SystemComponentsHealthy  - lastTransitionTime: \"20XX-10-28T08:17:21Z\"  lastUpdateTime: \"20XX-11-28T08:17:21Z\"  message: (2/2) Health checks successful  reason: HealthCheckSuccessful  status: \"True\"  type: EveryNodeReady Please note that there are four statuses: True, False, Unknown, and Progressing.\n True should be used for successful health checks. False should be used for unsuccessful/failing health checks. Unknown should be used when there was an error trying to determine the health status. Progressing should be used to indicate that the health status did not succeed but for expected reasons (e.g., a cluster scale up/down could make the standard health check fail because something is wrong with the Machines, however, it’s actually an expected situation and known to be completed within a few minutes.)  Health checks that report Progressing should also provide a timeout after which this “progressing situation” is expected to be completed. The health check library will automatically transition the status to False if the timeout was exceeded.\nAdditional Considerations It is up to the extension to decide how to conduct health checks, though it is recommended to make use of the build-in health check functionality of managed-resources for trivial checks. By deploying the depending resources via managed resources, the gardener resource manager conducts basic checks for different API objects out-of-the-box (e.g Deployments, DaemonSets, …) - and writes health conditions. In turn, the library contains a health check function to gather the health information from managed resources.\nMore sophisticated health checks should be implemented by the extension controller itself (implementing the HealthCheck interface).\n","categories":"","description":"","excerpt":"Health Check Library Goal Typically an extension reconciles a specific …","ref":"/docs/gardener/extensions/healthcheck-library/","tags":"","title":"Healthcheck Library"},{"body":"Heartbeat Controller The heartbeat controller renews a dedicated Lease object named gardener-extension-heartbeat at regular 30 second intervals by default. This Lease is used for heartbeats similar to how gardenlet uses Lease objects for seed heartbeats (see gardenlet heartbeats).\nThe gardener-extension-heartbeat Lease can be checked by other controllers to verify that the corresponding extension controller is still running. Currently, gardenlet checks this Lease when performing shoot health checks and expects to find the Lease inside the namespace where the extension controller is deployed by the corresponding ControllerInstallation. For each extension resource deployed in the Shoot control plane, gardenlet finds the corresponding gardener-extension-heartbeat Lease resource and checks whether the Lease’s .spec.renewTime is older than the allowed threshold for stale extension health checks - in this case, gardenlet considers the health check report for an extension resource as “outdated” and reflects this in the Shoot status.\n","categories":"","description":"","excerpt":"Heartbeat Controller The heartbeat controller renews a dedicated Lease …","ref":"/docs/gardener/extensions/heartbeat/","tags":"","title":"Heartbeat"},{"body":"GEP20 - Highly Available Shoot Control Planes Table of Contents  GEP20 - Highly Available Shoot Control Planes  Table of Contents Summary Motivation Goals Non-Goals High Availablity  Topologies Recommended number of nodes and zones Recommended number of replicas   Gardener Shoot API  Proposed changes   Gardener Scheduler  Case #1: HA shoot with no seed assigned Case #2: HA shoot with assigned seed and updated failure tolerance   Setting up a Seed for HA  Hosting a HA shoot control plane with node failure tolerance Hosting a HA shoot control plane with zone failure tolerance Compute Seed Usage   Scheduling control plane components  Zone pinning Single-Zone Multi-Zone (#replicas \u003c= #zones) Multi-Zone (#replicas \u003e #zones)   Disruptions and zero downtime maintenance Seed System Components Shoot Control Plane Components  Kube Apiserver Gardener Resource Manager Etcd  Gardener etcd component changes   Other critical components having single replica   Handling Outages  Node failures  Impact of Node failure   What is Zone outage?  Impact of a Zone Outage   Identify a Zone outage Identify zone recovery Recovery  Current Recovery Mechanisms Recovery from Node failure Recovery from Zone failure   Option #1: Leverage existing recovery options - Preferred Option #2: Redundencies for all critical control plane components Option #3: Auto-rebalance pods in the event of AZ failure   Cost Implications on hosting HA control plane  Compute \u0026 Storage Network latency Cross-Zonal traffic  Ingress/Egress traffic analysis Optimizing Cost: Topology Aware Hint     References Appendix  ETCD Active-Passive Options Topology Spread Constraints evaluation and findings Availability Zone Outage simulation Ingress/Egress Traffic Analysis Details      Summary Gardener today only offers highly available control planes for some of its components (like Kubernetes API Server and Gardener Resource Manager) which are deployed with multiple replicas and allow a distribution across nodes. Many of the other critical control plane components including etcd are only offered with a single replica, making them susceptible to both node failure as well as zone failure causing downtimes.\nThis GEP extends the failure domain tolerance for shoot control plane components as well as seed components to survive extensive node or availability zone (AZ) outages.\nMotivation High availability (HA) of Kubernetes control planes is desired to ensure continued operation, even in the case of partial failures of nodes or availability zones. Tolerance to common failure domains ranges from hardware (e.g. utility power sources and backup power sources, network switches, disk/data, racks, cooling systems etc.) to software.\nEach consumer therefore needs to decide on the degree of failure isolation that is desired for the control plane of their respective shoot clusters.\nGoals  Provision shoot clusters with highly available control planes (HA shoots) and a failure tolerance on node or AZ level. Consumers may enable/disable high-availability and choose failure tolerance between multiple nodes within a single zone or multiple nodes spread across multiple zones. Migrating non-HA to HA shoots. For failure tolerance on zone level only if shoot is already scheduled to a multi-zonal seed. Scheduling HA shoots to adequate seeds.  Non-Goals  Setting up a high available Gardener service. Upgrading from a single-zone shoot control plane to a multi-zonal shoot control plane. Failure domains on region level, i.e. multi-region control-planes. Downgrading HA shoots to non-HA shoots. In the current scope, three control plane components - Kube Apiserver, etcd and Gardener Resource Manager will be highly available. In the future, other components could be set up in HA mode. To achieve HA we consider components to have at least three replicas. Greater failure tolerance is not targeted by this GEP.  High Availablity Topologies Many shoot control plane (etcd, kube-apiserver, gardener-resource-manager, …) and seed system components (gardenlet, istio, etcd-druid, …) provide means to achieve high availability. Commonly these either run in an Active-Active or in an Active-Passive mode. Active-Active means that each component replica serves incoming requests (primarily intended for load balancing) whereas Active-Passive means that only one replica is active while others remain on stand-by.\nRecommended number of nodes and zones It is recommended that for high-availability setup an odd number of nodes (node tolerance) or zones (zone tolerance) must be used. This also follows the recommendations on the etcd cluster size. The recommendations for number of zones will be largely influenced by quorum-based etcd cluster setup recommendations as other shoot control plane components are either stateless or non-quorum-based stateful components.\nLet’s take the following example to explain this recommendation further:\n Seed clusters’ worker nodes are spread across two zones Gardener would distribute a three member etcd cluster - AZ-1: 2 replicas, AZ-2: 1 replica If AZ-1 goes down then, quorum is lost and the only remaining etcd member enters into a read-only state. If AZ-2 goes down then:  If the leader is in AZ-2, then it will force a re-election and the quorum will be restored with 2 etcd members in AZ-1. If the leader is not in AZ-2, then etcd cluster will still be operational without any downtime as the quorum is not lost.    Result:\n There seems to be no clear benefit to spreading an etcd cluster across 2 zones as there is an additional cost of cross-zonal traffic that will be incurred due to communication amongst the etcd members and also due to API server communication with an etcd member across zones. There is no significant gain in availability as compared to an etcd cluster provisioned within a single zone. Therefore it is a recommendation that for regions having 2 availability zones, etcd cluster should only be spread across nodes in a single AZ.  Validation\nEnforcing that a highly available ManagedSeed is setup with odd number of zones, additional checks needs to be introduced in admission plugin.\nRecommended number of replicas The minimum number of replicas required to achieve HA depends on the topology and the requirement of each component that run in an active-active mode.\nActive-Active\n If application needs a quorum to operate (e.g. etcd), at least three replicas are required (ref). Non-quorum based components are also supposed to run with a minimum count of three replicas to survive node/zone outages and support load balancing.  Active-Passive\n Components running in a active-passive mode are expected to have at least two replicas, so that there is always one replica on stand-by.  Gardener Shoot API Proposed changes The following changes to the shoot API are suggested to enable the HA feature for a shoot cluster:\nkind: Shoot apiVersion: core.gardener.cloud/v1beta1 spec:  controlPlane:  highAvailability:  failureTolerance:  type: \u003cnode | zone\u003e The consumer can optionally specify highAvailability in the shoot spec. Failure tolerance of node (aka single-zone shoot clusters) signifies that the HA shoot control plane can tolerate a single node failure, whereas zone (aka multi-zone shoots clusters) signifies that the HA shoot control plane can withstand an outage of a single zone.\nGardener Scheduler A scheduling request could be for a HA shoot with failure tolerance of node or zone. It is therefore required to appropriately select a seed.\nCase #1: HA shoot with no seed assigned Proposed Changes\nFitering candidate seeds\nA new filter step needs to be introduced in the reconciler which selects candidate seeds. It ensures that shoots with zone tolerance are only scheduled to seeds which have worker nodes across multiple availability tones (aka multi-zonal seeds).\nScoring of candidate seeds\nToday, after Gardener Scheduler filtered candidates and applied the configured strategy, it chooses the seed with least scheduled shoots ref.\nThis GEP intends to enhance this very last step by also taking the requested failure tolerance into consideration: If there are potential single- and multi-zonal candidates remaining, a single-zonal seed is always preferred for a shoot requesting no or node tolerance, independent from the utilization of the seed (also see this draft PR). A multi-zonal seed is only chosen if no single-zonal one is suitable after filtering was done and the strategy has been applied.\nMotivation: It is expected that operators will prepare their landscapes for HA control-planes by changing worker nodes of existing seeds but also by adding completely new multi-zonal seed clusters. For the latter, multi-zonal seeds should primarily be reserved for multi-zonal shoots.\nCase #2: HA shoot with assigned seed and updated failure tolerance A shoot has a pre-defined non-HA seed. A change has been made to the shoot spec, setting control HA to zone.\nProposed Change\n If the shoot is not already scheduled on a multi-zonal seed, then the shoot admission plugin must deny the request. Either shoot owner creates shoot from scratch or needs to align with Gardener operator who has to move the shoot to a proper seed first via control plane migration (editing the shoots/binding resource). An automated control plane migration is deliberately not performed as it involves a considerable downtime and the feature itself is not stable by the time this GEP was written.  Setting up a Seed for HA As mentioned in High-Availablity, certain aspects need to be considered for a seed cluster to host HA shoots. The following sections explain the requirements for a seed cluster to host a single or multi zonal HA shoot cluster.\nHosting a HA shoot control plane with node failure tolerance To host an HA shoot control plane within a single zone, it should be ensured that each worker pool that potentially runs seed system or shoot control plane components should at least have three nodes. This is also the minium size that is required by an HA etcd cluster with a failure tolerance of a single node. Furthermore, the nodes must run in a single zone only (see Recommended number of nodes and zones).\nHosting a HA shoot control plane with zone failure tolerance To host an HA shoot control plane across availability zones, worker pools should have a minimum of three nodes spread across an odd number of availability zones (min. 3).\nAn additional label seed.gardener.cloud/multi-zonal: true should be added to the seed indicating that this seed is capable of hosting multi-zonal HA shoot control planes, which in turn will help gardener scheduler to short-list the seeds as candidates.\nIn case of a ManagedSeed Gardener can add this label automatically to seed clusters if at least one worker pool fulfills the requirements mentioned above and doesn’t enforce Taints on its nodes. Gardener may in addition validate if the ManagedSeed is properly set up for the seed.gardener.cloud/multi-zonal: true label when it is added manually.\nCompute Seed Usage At present seed usage is computed by counting the number of shoot control planes that are hosted in a seed. Every seed has a number of shoots it can host status.allocatable.shoots (configurable via ResourceConfiguration. Operators need to rethink this value for multi-zonal seed clusters.\nWhich parameters could be considered?\n Number of available machines of a type as requested as part of the shoot spec. Sufficient capacity should be available to also allow rolling updates which will also be governed by maxSurge configuration at the worker pool level. Node CIDR range must grant enough space to schedule additional replicas that the HA feature requires. (For instance, for etcd the requirement for nodes will be 3 times as compared to the current single node). If additional zones are added to an existing non-multi-zonal seed cluster to make it multi-zonal then care should be taken that zone specific CIDRs are appropriately checked and changed if required. Number of volumes that will be required to host a multi-node/multi-zone etcd cluster will increase by (n-1) where n is the total number of members in the etcd cluster.  The above list is not an exhaustive list and is just indicative that the currently set limit of 250 will have to be revisited.\nScheduling control plane components Zone pinning HA shoot clusters with failure tolerance of node as well as non-HA shoot clusters can be scheduled on single-zonal and multi-zonal seeds alike. On a multi-zonal seed it’s desireable to place components of the same control plane in one zone only to reduce cost and latency effects due to cross network traffic. Thus, it’s essential to add Pod affinity rules to control plane component with multiple replicas:\nspec:  affinity:  podAffinity:  requiredDuringSchedulingIgnoredDuringExecution:  - labelSelector:  matchLabels:  gardener.cloud/shoot: \u003ctechnical-id\u003e  \u003clabels\u003e  topologyKey: \"topology.kubernetes.io/zone\" A special challenge is to select the entire set of control plane pods belonging to a single control plane. Today, Gardener and extensions don’t put a common label to the affected pods. We propose to introduce a new label gardener.cloud/shoot: \u003ctechnical-id\u003e where technical-id is the shoot namespace. A mutating webhoook in the Gardener Resource Manager should apply this label and the affinity rules to every pod in the control plane. This label and the pod affinity rule will ensure that all the pods in the control plane are pinned to a specific zone for HA shoot cluster having failure tolerance of node.\nSingle-Zone There are control plane components (like etcd) which requires one etcd member pod per node. Following anti-affinity rule guarantees that each pod of etcd is scheduled onto a different node.\nspec:  affinity:  podAntiAffinity:  requiredDuringSchedulingIgnoredDuringExecution:  - labelSelector:  matchLabels:  \u003clabels\u003e  topologyKey: \"kubernetes.io/hostname\" For other control plane components which do not have a stricter requirements to have one replica per node, a more optimal scheduling strategy should be used. Following topology spread constraint provides better utilization of node resources, allowing cluster autoscaler to downsize node groups if certain nodes are under-utilized.\nspec:  topologySpreadConstraints:  - maxSkew: 1  topologyKey: kubernetes.io/hostname  whenUnsatisfiable: DoNotSchedule  labelSelector:  matchLabels:  \u003clabels\u003e Using topology spread constraints (as described above) would still ensure that if there are more than one replica defined for a control plane component then it will be distributed across more than one node ensuring failure tolerance of at least one node.\nMulti-Zone (#replicas \u003c= #zones) If the replica count is equal to the number of available zones, then we can enforce the zone spread during scheduling.\nspec:  affinity:  podAntiAffinity:  requiredDuringSchedulingIgnoredDuringExecution:  - labelSelector:  matchLabels:  \u003clabels\u003e  topologyKey: \"topology.kubernetes.io/zone\" Multi-Zone (#replicas \u003e #zones) Enforcing a zone spread for components with a replica count higher than the total amount of zones is not possible. In this case we plan to rather use the following Pod Topology Spread Constraints which allows a distribution over zones and nodes. The maxSkew value determines how big a imbalance of the pod distribution can be and thus it allows to schedule replicas with a count beyond the number of availability zones (e.g. Kube-Apiserver).\n NOTE:\n During testing we found a few inconsistencies and some quirks (more information) which is why we rely on Topology Spread Constraints (TSC) only for this case. In addition to circumvent issue kubernetes/kubernetes#98215, Gardener is supposed to add a gardener.cloud/rolloutVersion label and incrementing the version every time the .spec of the component is changed (see workaround).  Update: kubernetes/kubernetes#98215 has been very recently closed. A new feature-gate has been created which is only available from kubernetes 1.5 onwards.\n spec:  topologySpreadConstraints:  - maxSkew: 2  topologyKey: topology.kubernetes.io/zone  whenUnsatisfiable: DoNotSchedule  labelSelector:  matchLabels:  \u003clabels\u003e  gardener.cloud/rolloutVersion: \u003cversion\u003e  - maxSkew: 1  topologyKey: kubernetes.io/hostname  whenUnsatisfiable: DoNotSchedule  labelSelector:  matchLabels:  \u003clabels\u003e  gardener.cloud/rolloutVersion: \u003cversion\u003e Disruptions and zero downtime maintenance A secondary effect of provisioning affected seed system and shoot control plane components in an HA fashion is the support for zero downtime maintenance, i.e. a certain amount of replicas always serves requests while an update is being rolled out. Therefore, proper PodDisruptionBudgets are required. With this GEP it’s planned to set spec.maxUnavailable: 1 for every involved and further mentioned component.\nSeed System Components The following seed system components already run or are planned[*] to be configured with a minimum of two replicas:\n Etcd-Druid* (active-passive) Gardenlet* (active-passive) Istio Ingress Gateway* (active-active) Istio Control Plane* (active-passive) Gardener Resource Manager (controllers: active-passive, webhooks: active-active) Gardener Seed Admission Controller (active-active) Nginx Ingress Controller (active-active) Reversed VPN Auth Server (active-active)  The reason to run controller in active-passive mode is that in case of an outage a stand-by instance can quickly take over the leadership which reduces the overall downtime of that component in comparison to a single replica instance that would need to be evicted and re-scheduled first (see Current-Recovery-Mechanisms).\nIn addition, the pods of above mentioned components will be configured with the discussed anti-affinity rules (see Scheduling control plane components). The Single-Zone case will be the default while Multi-Zone anti-affinity rules apply to seed system components, if the seed is labelled with seed.gardener.cloud/multi-zonal: true (see Hosting a multi-zonal HA shoot control plane).\nShoot Control Plane Components Similar to the Seed System Components the following shoot control plane components are considered critical so that Gardener ought to avoid any downtime. Thus, current recovery mechanisms are considered insufficient if only one replica is involved.\nKube Apiserver The Kube Apiserver’s HVPA resource needs to be adjusted in case of an HA shoot request:\nspec:  hpa:  template:  spec:  minReplicas: 3 The discussed TSC in Scheduling control plane components applies as well.\nGardener Resource Manager The Gardener Resource Manager is already set up with spec.replicas: 3 today. Only the Affinity and anti-affinity rules must be configured on top.\nEtcd In contrast to other components, it’s not trivial to run multiple replicas for etcd because different rules and considerations apply to form a quorum-based cluster ref. Most of the complexity (e.g. cluster bootstrap, scale-up) is already outsourced to Etcd-Druid and efforts have been made to support may use-cases already (see gardener/etcd-druid#107 and Multi-Node etcd GEP). Please note, that especially for etcd an active-passive alternative was evaluated here. Due to the complexity and implementation effort it was decided to proceed with the active-active built-in support, but to keep this as a reference in case we’ll see blockers in the future.\nGardener etcd component changes With most of the complexity being handled by Etcd-Druid, Gardener still needs to implement the following requirements if HA is enabled.:\n Set etcd.spec.replicas: 3. Set etcd.spec.etcd.schedulingConstraints to the matching anti-affinity rule. Deploy NetworkPolicies that a allow a peer-to-peer communication between the etcd pods. Create and pass a peer CA and server/client certificate to etcd.spec.etcd.peerUrlTls  The groundwork for this was already done by gardener/gardener#5741.\nOther critical components having single replica Following shoot control plane components are currently setup with a single replica and are planned to run with a minimum of 2 replicas:\n Cluster Autoscaler (if enabled) Cloud Controller Manager (CCM) Kube Controller Manager (KCM) Kube Scheduler Machine Controller Manager (MCM) CSI driver controller   NOTE: MCM, CCM and CSI driver controller are components deployed by provider extensions. HA specific configuration should be configured there.\n Additionally Affinity and anti-affinity rules must be configured.\nHandling Outages Node failures It is possible that node(s) hosting the control plane component are no longer available/reachable. Some of the reasons could be crashing of a node, kubelet running on the node is unable to renew its lease, network partition etc. The topology of control plane components and recovery mechanisms will determine the duration of the downtime that will result when a node is no longer reachable/available.\nImpact of Node failure Case #1\nHA Failure Tolerance: node\nFor control plane components having multiple replicas, each replica will be provisioned on a different node (one per node) as per the scheduling constraints.\nSince there are lesser than the desired replicas for shoot control plane pods, kube-scheduler will attempt to look for another node while respecting pod scheduling constraints. If a node satisfying scheduling constraints is found then it will be chosen to schedule control plane pods. If there are no nodes that satisfy the scheduling constraints, then it must wait for Cluster-Autoscaler to scale the node group and then for the Machine-Controller-Manager to provision a new node in the scaled node group. In the event Machine-Controller-Manager is unable to create a new machine, then the replica that was evicted from the failed node, will be stuck in Pending state.\nImpact on etcd\nAssuming that default etcd cluster size of 3 members, unavailability of one node is tolerated. If more than 1 node hosting the control plane components goes down or is unavailable, then etcd cluster will lose quorum till new nodes are provisioned.\nCase #2\nHA Failure Tolerance: zone\nFor control plane components having multiple replicas, each replica will be spread across zones as per the scheduling constraints.\netcd\nkube-scheduler will attempt to look for another node in the same zone since the pod scheduling constraints will prevent it from scheduling the pod onto another zone. If a node is found where there are no control plane components deployed, then it will choose that node to schedule the control plane pods. If there are no nodes that satisfy the scheduling constraints, then it must wait for Machine-Controller-Manager to provision a new node. Reference to PVC will also prevent an etcd member from getting scheduled in another zone since persistent volumes are not shared across availability zones.\nKube Apiserver and Gardener Resource Manager\nThese control plane components will use these rules which allows the replica deployed on the now deleted machine to be brought up on another node within the same zone. However, if there are no nodes available in that zone, then for Gardener Resource Manager which uses requiredDuringSchedulingIgnoredDuringExecution no replacement replica will be scheduled in another zone. Kube ApiServer on the other hand uses topology spread constrains with maxSkew=2 on the topologyKey: topology.kubernetes.io/zone which allows it to schedule a replacement pod in another zone.\nOther control plane components having single replica\nCurrently there are no pod scheduling constraints on such control plane components. Current recovery mechanisms as described above will come into play and recover these pods.\nWhat is Zone outage? No clear definition of a zone outage emerges. However, we can look at different reasons for a zone outage across providers that have been listed in the past and derive a definition out of it.\nSome of the most common failures for zone outages have been due to:\n Network congestion, failure of network devices etc., resulting in loss of connectivity to the nodes within a zone. Infrastructure power down due to cooling systems failure/general temperature threshold breach. Loss of power due to extreme weather conditions and failure of primary and backup generators resulting in partial or complete shutdown of infrastructure. Operator mistakes leading to cascading DNS issues, over-provisioning of servers resulting in massive increase in system memory. Stuck volumes or volumes with severely degraded performance which are unable to service read and write requests which can potentially have cascading effects on other critical services like load balancers, database services etc.  The above list is not comprehensive but a general pattern emerges. The outages range from:\n Shutdown of machines in a specific availability zone due to infrastructure failure which in turn could be due to many reasons listed above. Network connectivity to the machines running in an availability zone is either severely impacted or broken. Subset of essential services (e.g. EBS volumes in case of AWS provider) are unhealthy which might also have a cascading effect on other services. Elevated API request failure rates when creating or updating infrastructure resources like machines, load balancers, etc.  One can further derive that either there is a complete breakdown of an entire availability zone (1 and 2 above) or there is a degradation or unavailability of a subset of essential services.\nIn the first version of this document we define an AZ outage only when either of (1) or (2) occurs as defined above.\nImpact of a Zone Outage As part of the current recovery mechanisms, if Machine-Controller-Manager is able to delete the machines, then per MachineDeployment it will delete one machine at a time and wait for a new machine to transition from Pending to Running state. In case of a network outage, it will be able to delete a machine and subsequently launch a new machine but the newly launched machine will be stuck in Pending state as the Kubelet running on the machine will not be able to create its lease. There will also not be any corresponding Node object for the newly launched machine. Rest of the machines in this MachineDeployment will be stuck in Unknown state.\nKube Apiserver, Gardener Resource Manager \u0026 seed system components\nThese pods are stateless, losing one pod can be tolerated since there will be two other replicas that will continue to run in other two zones which are available (considering that there are 3 zones in a region).\netcd\nA minimum and default size of HA etcd cluster setup is 3. This allows tolerance of one AZ failure. If more than one AZ fails or is unreachable then etcd cluster will lose its quorum. Pod Anti-Affinity policies that are initially set will not allow automatic rescheduling of etcd pod onto another zone (unless of course affinity rules are dynamically changed). Reference to PVC will also prevent an etcd member from getting scheduled in another zone, since persistent volumes are not shared across availability zones.\nOther Shoot Control Plane Components\nAll the other shoot control plane components have:\n Single replicas No affinity rules influencing their scheduling  See the current recovery mechanisms described above.\nIdentify a Zone outage  NOTE: This section should be read in context of the currently limited definition of a zone outage as described above.\n In case the Machine-Controller-Manager is unable to delete Failed machines, then following will be observed:\n All nodes in that zone will be stuck in NotReady or Unknown state and there will be an additional taint key: node.kubernetes.io/unreachable, effect: NoSchedule on the node resources. Across all MachineDeployment in the affected zone, one machine will be in Terminating state and other existing machines will be in Unknown state. There might one additional machine in CrashLoopBackOff.  In case the Machine-Controller-Manager is able to delete the Failed machines then following will be observed:\n For every MachineDeployment in the affected zone, there will be one machine stuck in Pending state and all other machines will be in Unknown state. For the machine in Pending state there will not be any corresponding node object. For all the machines in Unknown state, corresponding node resource will be in NotReady/Unknown state and there will be an additional taint key: node.kubernetes.io/unreachable, effect: NoSchedule on each of the node.  If the above state is observed for an extended period of time (beyond a threshold that could be defined), then it can be deduced that there is a zone outage.\nIdentify zone recovery  NOTE: This section should be read in context of the current limited definition of a zone outage as described above.\n The machines which were previously stuck in either Pending or CrashLoopBackOff state are now in Running state and if there are corresponding Node resources created for machines, then the zonal recovery has started.\nRecovery Current Recovery Mechanisms Gardener and upstream Kubernetes already provide recovery mechanism for node and pod recovery in case of a failure of a node. Those have been tested in the scope of a availability zone outage simulation.\nMachine recovery\nIn the seed control plane, Kube Controller Manager will detect that a node has not renewed its lease and after a timeout (usually 40 seconds - configurable via --node-monitor-grace-period) it will transition the Node to Unknown state. Machine-Controller-Manager will detect that an existing Node has transitioned to Unknown state and will do the following:\n It will transition the corresponding Machine to Failed state after waiting for a duration (currently 10 mins, configured via the –machine-health-timeout flag). Thereafter a deletion timestamp will be put on this machine indicating that the machine is now going to be terminated, transitioning the machine to Terminating state. It attempts to drain the node first and if it is unable to drain the node, then currently after a period of 2 hours (configurable via machine-drain-timeout), it will attempt to force-delete the Machine and create a new machine. Draining a node will be skipped if certain conditions are met e.g. node in NotReady state, node condition reported by node-problem-exporter as ReadonlyFilesystem etc.  In case Machine-Controller-Manager is unable to delete a machine, then that machine will be stuck in Terminating state. It will attempt to launch a new machine and if that also fails, then the new machine will transition to CrashLoopBackoff and will be stuck in this state.\nPod recovery\nOnce Kube Controller Manager transitions the node to Unknown/NotReady, it also puts the following taints on the node:\ntaints: - effect: NoSchedule  key: node.kubernetes.io/unreachable - effect: NoExecute  key: node.kubernetes.io/unreachable This annotation has the following effect:\n New pods will not be scheduled unless they have a toleration added which is all permissive or matches the effect and/or key.  In case where kubelet cannot reach the control plane, evicting the pod is not possible without intervention. See here for more details. It has been observed that the pods will be stuck in Terminating state.\nRecovery from Node failure If there is a single node failure in any availability zone irrespective of whether it is a single-zone or multi-zone setup, then the recovery is automatic (see current recovery mechanisms). In the mean time, if there are other available nodes (as per affinity rules) in the same availability zone, then the scheduler will deploy the affected shoot control plane components on these nodes.\nRecovery from Zone failure In the following section, options are presented to recover from an availability zone failure in a multi-zone shoot control plane setup.\nOption #1: Leverage existing recovery options - Preferred In this option existing recovery mechanisms as described above are used. There is no change to the current replicas for all shoot control plane components and there is no dynamic re-balancing of quorum based pods considered.\nPros:\n Less complex to implement since no dynamic re-balancing of pods is required and there is no need to determine if there is an AZ outage. Additional cost to host an HA shoot control plane is kept to the bare minimum. Existing recovery mechanisms are leveraged.  Cons:\n Existing recovery of pods will result in a downtime of up to a total of –machine-health-timeout + –machine-drain-timeout (which as of today is 2hr10min) for pods which can be rescheduled onto another node/zone. Stateful pods due to the affinity rules will not recover, however stateless pods will eventually recover. etcd cluster will run with one less member resulting in no tolerance to any further failure. If it takes a long time to recover a zone then etcd cluster is now susceptible to a quorum loss, if any further failure happens. Any zero downtime maintenance is disabled during this time. If the recovery of the zone takes a long time, then it is possible that difference revision between the leader and the follower (which was in the zone that is not available) becomes large. When the AZ is restored and the etcd pod is deployed again, then there will be an additional load on the etcd leader to synchronize this etcd member.  Option #2: Redundencies for all critical control plane components In this option :\n Kube Apiserver, Gardener Resource Manager and etcd will be setup with a minimum of 3 replicas as it is done today. All other critical control plane components are setup with more than one replicas. Based on the criticality of the functionality different replica count (\u003e1) could be decided. As in Option #1 no additional recovery mechanism other than what currently exists are provided.  Pros:\n Toleration to at least a single AZ is now provided for all critical control plane components. There is no need for dynamic re-balancing of pods in the event of an AZ failure and there is also no need to determine if there is an AZ outage reducing the complexity.  Cons:\n Provisioning redundancies entails additional hosting cost. With all critical components now set up with more than one replica, the overall requirement for compute resources will increase. Increase in the overall resource requirements will result in lesser number of shoot control planes that can be hosted in a seed, thereby requiring more seeds to be provisioned, which also increases the cost of hosting seeds. If the recovery of the zone takes a long time, then it is possible that difference revision between the leader and the follower (which was in the zone that is not available) becomes large. When the AZ is restored and the etcd pod is deployed again, then there will be an additional load on the etcd leader to synchronize this etcd member.   NOTE:\n Before increasing the replicas for control plane components that currently have a single replica following needs to be checked:\n Is the control plane component stateless? If it is stateless, then it is easier to increase the replicas. If the control plane component is not stateless, then check if leader election is required to ensure that at any time there is only one leader and the rest of the replicas will only be followers. This will require additional changes to be implemented if they are not already there.   Option #3: Auto-rebalance pods in the event of AZ failure  NOTE: Prerequisite for this option is to have the ability to detect an outage and recover from it.\n Kube Apiserver, Gardener Resource Manager, etcd and seed system compontents will be setup with multiple replicas spread across zones. Rest of the control plane components will continue to have a single replica. However, in this option etcd cluster members will be rebalanced to ensure that the desired replicas are available at all times.\nRecovering etcd cluster to its full strength\nAffinity rules set on etcd statefulset enforces that there will be at most one etcd member per zone. Two approaches could be taken:\nVariant-#1\nChange the affinity rules and always use preferredDuringSchedulingIgnoredDuringExecution for topologyKey: topology.kubernetes.io/zone. If all zones are available then it will prefer to distribute the etcd members across zones, each zone having just one replica. In case of zonal failure, kube scheduler will be able to re-schedule this pod in another zone while ensuring that it chooses a node within that zone that does not already have an etcd member running.\nPros\n Simpler to implement as it does not require any change in the affinity rules upon identification of a zonal failure. Etcd cluster runs with full strength as long as there is a single zone where etcd pods can be rescheduled.  Cons\n It is possible that even when there is no zonal failure, more than one etcd member can be provisioned in a single zone. The chances of that happening are slim as typically there is a dedicate worker pool for hosting etcd pods.  Variant-#2\nUse requiredDuringSchedulingIgnoredDuringExecution for topologyKey: topology.kubernetes.io/zone during the initial setup to strictly enforce one etcd member per zone.\nIf and when a zonal failure is detected then etcd-druid should do the following:\n Remove the PV and PVC for the etcd member in a zone having an outage Change the affinity rules for etcd pods to now use preferredDuringSchedulingIgnoredDuringExecution for topologyKey: topology.kubernetes.io/zone during the downtime duration of a zone. Delete the etcd pod in the zone which has an outage  This will force the kube-scheduler to schedule the new pod in another zone.\nWhen it is detected that the zone has now recovered then it should re-balance the etcd members. To achieve that the following etcd-druid should do the following:\n Change the affinity rule to again have requiredDuringSchedulingIgnoredDuringExecution for topologyKey: topology.kubernetes.io/zone Delete an etcd pod from a zone which has 2 pods running. Subsequently also delete the associated PV and PVC.  The kube-scheduler will now schedule this pod in the just recovered zone.\nConsequence of doing this is that etcd-druid, which today runs with a single replica, now needs to have a HA setup across zones.\nPros\n When all the zones are healthy and available, it ensures that there is at most one pod per zone, thereby providing the desired QoS w.r.t failure tolerance. Only in the case of a zone failure will it relax the rule for spreading etcd members to allowing more than one member in a zone to be provisioned. However this would ideally be temporary. Etcd cluster runs with full strength as long as there is a single zone where etcd pods can be rescheduled.  Cons\n It is complex to implement. Requires etcd-druid to be highly available as it now plays a key role in ensuring that the affinity rules are changed and PV/PVC’s are deleted.  Cost Implications on hosting HA control plane Compute \u0026 Storage Cost differential as compared to current setup will be due to:\n Consider a 3 zone cluster (in case of multi-zonal shoot control plane) and 3 node cluster (in case of single-zonal shoot control plane)\n  Machines: Depending on the number of zones, a minimum of one additional machine per zone will be provisioned. Persistent Volume: 4 additional PVs need to be provisioned for etcd-main and etcd-events pods. Backup-Bucket: etcd backup-restore container uses provider object store as a backup-bucket to store full and delta snapshots. In a multi-node etcd setup, there will only be a leading backup-bucket sidecar (in etcd leader pod) that will only be taking snapshots and uploading it to the object store. Therefore there is no additional cost incurred as compared to the current non-HA shoot control plane setup.  Network latency Network latency measurements were done focusing on etcd. Three different etcd topologies were considered for comparison - single node etcd (cluster-size = 1), multi-node etcd (within a single zone, cluster-size = 3) and multi-node etcd (across 3 zones, cluster-size = 3).\nTest Details\n etcd benchmark tool is used to generate load and reports generated from the benchmark tool is used. A subset of etcd requests (namely PUT, RANGE) are considered for network analysis. Following are the parameters that have been considered for each test run across all etcd topologies:  Number of connections Number of clients connecting to etcd concurrently Size and the number of key-value pairs that are and queried Consistency which can either be serializable or linearizable For each PUT or GET (range) request leader and followers are targetted   Zones have other workloads running and therefore measurements will have outliers which are ignored.  Acronyms used\n sn-sz - single node, single zone etcd cluster mn-sz - multi node, single zone etcd cluster mn-mz - multi node, multi zone etcd cluster  Test findings for PUT requests\n When number of clients and connections are kept at 1 then it is observed that sn-sz latency is lesser (range of 20%-50%) as compared to mn-sz. The variance is due to changes in payload size. For the following observations it was ensured that the only a leader is targetted for both multi-node etcd cluster topologies and that the leader is in the same zone as that of sn-sz to have a fair comparison.  When the number of clients, connections and payload size is increased then it has been observed that sn-sz latency is more (in the range of 8% to 30%) as compared to mn-sz and mn-mz.   When comparing mn-sz and mn-mz following observations were made:  When number of clients and connections are kept at 1 then irrespective of the payload size, mn-sz latency is lesser (~3%) as compared to mn-mz. However the difference is in the usually is within 1ms. mn-sz latency is lesser (range of 5%-20%) as compared to mn-mz when the request is directly serviced by the leader. However the difference is in a range of a micro-seconds to a few milli-seconds. mn-sz latency is lesser (range of 20-30%) as compared to mn-mz when the request is serviced by a follower. However the difference is usually within the same millisecond. When the number of clients and connections are kept at 1 then irrespective of the payload size it is observed that latency of a leader is lesser than any follower. This is on the expected lines. However if the number of clients and connections are increased then leader seems to have a higher latency as compared to a follower which could not be explained.    Test findings for GET (Range) requests\nUsing etcd benchmark tool range requests were generated.\nRange Request to fetch one key per request\nWe fixed the number of connections and clients to 1 and varied the payload size. Range requests were directed to leader and follower etcd members and network latencies were measured. Following are the findings:\n sn-sz latency is ~40% greater as compared to mn-sz and around 30% greater as compared to mn-mz for smaller payload sizes. However for larger payload sizes (~1MB) the trend reverses and we see that sn-sz latency is around (15-20%) lesser as compared to mn-sz and mn-mz. mn-sz latency is ~20% lesser than mn-mz With consistency set to serializable, latency was lesser (in the range of 15-40%) as compared to when consistency was set to linearizable. When requesting a single key at time (keeping number of connections and clients to 1):  sn-sz latency is ~40% greater as compared to mn-sz and around 30% greater as compared to mn-mz. mn-sz latency is ~20% lesser than mn-mz With consistency serializable latency was ~40% lesser as compared to when consistency was set to linearizable. For both mn-sz and mn-mz leader latency is in general lesser (in the range of 20% to 50%) than that of the follower. However the difference is still in milliseconds range when consistency is set to linearizable and in micro seconds range when it is set to serializable.    When connections and clients were increased keeping the payload size fixed then following were the findings:\n sn-sz latency is ~30% greater as compared to mn-sz and mn-mz with consistency set to linearizable. This is consistent with the above finding as well. However when consistency is set to serializable then across all topologies latencies are comparable (within ~1 millisecond). With increased connections and clients the latencies of mn-sz and mn-mz are almost similar. With consistency set to serializable, latency was ~20% lesser as compared to when consistency was set to linearizable. This is also consistent with the above findings. When range requests are served by the follower then mn-sz latency is ~20% lesser than mn-mz when consistency is set to linearizable. However it is quite the opposite when consistency is set to serializable.  Range requests to fetch all keys per request\nFor these tests - for payload size = 1MB, total number of key-value’s retrieved per request are 1000 and for payload-size = 256 bytes, total number of key-value pairs retrived per request are 100000.\n sn-sz latency is around 5% lesser than both mn-sz and mn-mz. This is a deviation for smaller payloads (see above), but for larger payloads this finding is consistent. There is hardly any difference in latency between mn-sz and mn-mz. There seems to be no significant different between serializable and linearizable consistency setting. However, when follower etcd instances serviced the request, there were mixed results and nothing could be concluded.  Summary\n For range requests consistency of Serializable has a lesser network latency as compared to Linearizable which is on the expected lines as linearlizable requests must go through the raft consensus process. For PUT requests:  sn-sz has a lower network latency when number of clients and connections are less. However it starts to deteriorate once that is increased along with increase in payload size makine multi-node etcd clusters out-perform single-node etcd in terms of network latency. In general mn-sz has lesser network latency as compared to mn-mz but it is still within milliseconds and therefore is not of concern. Requests that go directly to the leader have lesser overall network latency as compared to when the request goes to the follower. This is also expected as the follower will have to forward all PUT requests to the leader as an additional hop.   For GET requests:  For lower payload sizes sn-sz latency is greater as compared to mn-sz and mn-mz but with larger payload sizes this trend reverses. With lower number of connections and clients mn-sz has lower latencies as compared to mn-mz however this difference diminishes as number of connections/clients/payload size is increased. In general when consistency is set to serializable it has lower overall latency as compared to linearizable. There were some outliers w.r.t etcd followers but currently we do not give too much weightage to it.    In a nutshell we do not see any major concerns w.r.t latencies in a multi-zonal setup as compared to single-zone HA setup or single node etcd.\n NOTE: Detailed network latency analysis can be viewed here.\n Cross-Zonal traffic Providers typically do not charge ingress and egress traffic which is contained within an availability zone. However, they do charge traffic that is across zones.\nCross zonal traffic rates for some of the providers are:\n AWS: https://aws.amazon.com/vpc/pricing/ and https://aws.amazon.com/ec2/pricing/on-demand/ Azure: https://azure.microsoft.com/en-in/pricing/details/bandwidth/ GCP: https://cloud.google.com/vpc/network-pricing  Setting up shoot control plane with failure tolerance zone will therefore have a higher running cost due to ingress/egress cost as compared to a HA shoot with failure tolerance of node or to a non-HA control plane.\nIngress/Egress traffic analysis Majority of the cross zonal traffic is generated via the following communication lines:\n Between Kube Apiserver and etcd members (ingress/egress) Amongst etcd members (ingress/egress)  Since both of these components are spread across zones, their contribution to the cross-zonal network cost is the largest. In this section the focus is only on these components and the cross-zonal traffic that gets generated.\nDetails of the network traffic is described in Ingress/Egress traffic analysis section.\nObservation Summary\nTerminology\n Idle state: In an idle state of a shoot control plane, there is no user driven activity which results in a call to the API server. All the controllers have started and initial listing of watched resources has been completed (in other words informer caches are now in sync).  Findings\n etcd inherently uses raft consensus protocol to provide consistency and linearizability guarantees. All PUT or DELETE requests are always and only serviced by the leader etcd pod. Kube Apiserver can either connect to a leader or a follower etcd.  If Kube Apiserver connects to the leader then for every PUT, the leader will additionally distribute the request payload to all the followers and only if the majority of followers responded with a successful update to their local boltDB database, will the leader commit the message and subsequently respond back to the client. For Delete, a similar flow is executed but instead of passing around the entire k8s resource, only keys that need to be deleted are passed, making this operation significantly lighter from the network bandwidth consumption perspective. If the Kube Apiserver connects to a follower then for every PUT, the follower will first forward the PUT request along with the request payload to the leader, who in turn will attempt to get consensus from majority of the followers by again sending the entire request payload to all the followers. Rest of the flow is the same as above. There is an additional network traffic from follower to leader and is equal to the weight of the request payload. For Delete a similar flow is executed where the follower will forward the keys that need to be deleted to the leader as an additional step. Rest of the flow is the same as the PUT request flow. Since the keys are quite small in size, the network bandwidth consumed is very small.   GET calls made to the Kube Apiserver with labels + selector get translated to range requests to etcd. etcd’s database does not understand labels and selectors and is therefore not optimized for k8s query patterns. This call can either be serviced by the leader or follower etcd member. follower etcd will not forward the call to the leader. From within controllers, periodic informer resync which generates reconcile events does not make calls to the Kube Apiserver (under the condition that no change is made to the resources for which a watch is created). If a follower etcd is not in sync (w.r.t revisions) with the leader etcd, then it will reject the call. The client (in this case Kube Apiserver) retries. Needs to be checked, if it retries by connecting to another etcd member. This will result in additional cross zonal traffic. This is currently not a concern as members are generally kept in sync and will only go out of sync in case of a crash of a member or addition of a new member (as a learner) or during rolling updates. However, the time it takes to complete the sync is generally quick. etcd cluster members which are spread across availability zones generated a total cross zonal traffic of ~84 Kib/s in an ideal multi-zonal shoot control plane. Across several runs we have seen this number go up to ~100Kib/s. etcd follower to another etcd follower remains consistent at ~2Kib/s in all the cases that have been tested (see Appendix). Kube Apiserver making a PUT call to a etcd follower is more expensive than directly making the call to the etcd leader. A PUT call also carries the entire payload of the k8s resource that is being created. Topology aware hints should be evaluated to potentially reduce the network cost to some extent. In case of a large difference (w.r.t revision) between a follower and a leader, significant network traffic is observed between the leader and the follower. This is usually an edge case, but occurrence of these cases should be monitored.  Optimizing Cost: Topology Aware Hint In a multi-zonal shoot control plane setup there will be multiple replicas of Kube Apiserver and etcd spread across different availability zones. Network cost and latency is much lower when the communication is within a zone and increases once zonal boundary is crossed. Network traffic amongst etcd members cannot be optimized as these are strictly spread across different zones. However, what could be optimized is the network traffic between Kube Apiserver and etcd member (leader or follower) deployed within a single zone. Kubernetes provides topology aware hints to influence how clients should consume endpoints. Additional metadata is added to EndpointSlice to influence routing of traffic to the endpoints closer to the caller. Kube-Proxy utilizes the hints (added as metadata) to favor routing to topologically closer endpoints.\nDisclaimer: Topology Aware Hints won’t improve network traffic if the seed has worker nodes in more than three zones and the Kube Apiserver is scaled beyond three replicas at the same time. In this case, Kube Apiserver replicas run in zones which don’t have an etcd and thus cross zone traffic is inevitable.\nDuring evaluation of this feature some caveats were discovered:\nFor each cluster, gardener provides a capability to create one or more Worker Pool/Group. Each worker pool can span across one or more availability zones. For a combination of each worker pool and zone there will be a corresponding MachineDeployment which will also map 1:1 to a Node Group which is understood by cluster-autoscaler.\nConsider the following cluster setup: EndpointSliceController does the following:\n Computes the overall allocatable CPU across all zones - call it TotalCPU Computes the allocatable CPU for all nodes per zone - call it ZoneTotalCPU For each zone it computes the CPU ratio via ZoneTotalCPU/TotalCPU. If the ratio between any two zones is approaching 2x, then it will remove all topology hints.  Given that the cluster-autoscaler can scale the individual node groups based on unscheduled pods or lower than threshold usage, it is possible that topological hints are added and removed dynamically. This results in non-determinism w.r.t request routing across zones, resulting in difficult to estimate cross-zonal network cost and network latencies.\nK8S#110714 has been raised.\nReferences  https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/ https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/  Appendix ETCD Active-Passive Options In this topology there will be just one active/primary etcd instance and all other etcd instances will be running as hot-standby. Each etcd instance serves as an independent single node cluster. There are three options to setup an active-passive etcd.\n Option-1: Independent single node etcd clusters Primary etcd will periodically take a snapshot (full and delta) and will push these snapshots to the backup-bucket. Hot-Standy etcd instances will periodically query the backup-bucket and sync its database accordingly. If a new full snapshot is available which has a higher revision number than what is available in its local etcd database then it will restore from a full snapshot. It will additionally check if there are delta snapshots having a higher revision number. If that is the case then it will apply the delta snapshots directly to its local etcd database.\n NOTE: There is no need to run an embedded etcd to apply delta snapshots.\n For the sake of illustration only assume that there are two etcd pods etcd-0 and etcd-1 with corresponding labels which uniquely identify each pod. Assume that etcd-0 is the current primary/active etcd instance.\netcd-druid will take an additional responsibility to monitor the health of etcd-0 and etcd-1. When it detects that the etcd-0 is no longer healthy it will patch the etcd service to point to the etcd-1 pod by updating the label/selector so that it becomes the primary etcd. It will then restart etc-0 pod and henceforth that will serve as a hot-standby.\nPros\n There is no leader election, no quorum related issues to be handled. It is simpler to setup and manage. Allows you to just have a total of two etcd nodes - one is active and another is passive. This allows high availability across zones in cases where regions only have 2 zones (e.g. CCloud and Azure regions that do not have more than 2 zones). For all PUT calls the maximum cost in terms of network bandwidth is one call (cross-zonal) from Kube ApiServer to etcd instance which carries the payload with it. In comparison in a three member etcd cluster, the leader will have to send the PUT request to other members (cross zonal) in the etcd cluster which will be slightly more expensive than just having a single member etcd.  Cons\n As compared to an active-active etcd cluster there is not much difference in cost of compute resources (CPU, Memory, Storage) etcd-druid will have to periodically check the health of both the primary and hot-standby nodes and ensure that these are up and running. There will be a potential delay in determining that a primary etcd instance is no longer healthy. Thereby increasing the delay in switching to the hot-standy etcd instance causing longer downtime. It is also possible that at the same time hot-standy also went down or is otherwise unhealthy resulting in a complete downtime. The amount of time it will take to recover from such a situation would be several minutes (time to start etcd pod + time to restore either from full snapshot or apply delta snapshots). Synchronization is always via backup-bucket which will be less frequent as compared to an active-active etcd cluster where there is real-time synchronization done for any updates by the leader to majority or all of its followers. If the primary crashes, the time. During the switchover from primary to hot-standby if the hot-standy etcd is in process of applying delta snaphots or restoring from a new full snapshot then hot-standby should ensure that the backup-restore container sets the readiness probe to indicate that it is not ready yet causing additional downtime.    Option-2: Perpetual Learner In this option etcd cluster and learner facilities are leveraged. etcd-druid will bootstrap a cluster with one member. Once this member is ready to serve client requests then an additional learner will be added which joins the etcd cluster as a non-voting member. Learner will reject client reads and writes requests and the clients will have to reattempt. Typically switching to another member in a cluster post retries is provided out-of-the-box by etcd clientv3. The only member who is also the leader will serve all client requests.\nPros\n All the pros in Option-1 are also applicable for this option. Leaner will be continuously updated by the leader and will remain in-sync with the leader. Using a learner retains historical data and its ordering and is therefore better in that aspect as compared to Option-2.  Cons\n All the cons in Option-1 are also applicable for this option. etcd-druid will now have to additionally play an active role in managing members of an etcd cluster by add new members as learner and promoting learner to an active member if the leader is no longer available. This will increase the complexity in etcd-druid. To prevent clients from re-attempting to reach the learner, etcd-druid will have to ensure that the label on the learner are set differently than the leader. This needs to be done every time there is a switch in the leader/learner. Since etcd only allows addition of 1 learner at a time, this means that the HA setup can only have one failover etcd node, limiting its capability to have more than one hot-standby.   Topology Spread Constraints evaluation and findings  Finding #1 Consider the following setup:\nSingle zone \u0026 multiple nodes\nWhen the constraints defined above are applied then the following was the findings:\n With 3 replicas of etcd, all three got scheduled (one per node). This was a bit unexpected. As per the documentation if there are multiple constraints then they will be evaluated in conjunction. The first constraint should only allow 1 etcd pod per zone and the remaining 2 should not have been scheduled and should continue to be stuck in pending state. However all 3 etcd pods got scheduled and started successfully.    Finding #2 Consider the following setup:\n_Multiple zones \u0026 multiple nodes\nWhen the constraints defined above are applied then the following was the findings:\n Both constraints are evaluated in conjunction and the scheduling is done as expected. TSC behaves correctly till replicas=5. Beyond that TSC fails. This was reported as an issue kubernetes#109364    Finding #3   NOTE: Also see the Known Limitations\n Availability Zone Outage simulation A zone outage was simulated by doing the following (Provider:AWS):\n Network ACL were replaced with empty ACL (which denies all ingress and egress). This was done for all subnets in a zone. Impact of denying all traffic:  Kubelet running on the nodes in this zone will not be able to communicate to the Kube Apiserver. This will inturn result in Kube-Controller-Manager changing the status of the corresponding Node objects to Unknown. Control plane components will not be able to communicate to the kubelet, thereby unable to drain the node.   To simulate the scenario where Machine-Controller-Manager is unable to create/delete machines, cloudprovider credentials were changed so that any attempt to create/delete machines will be un-authorized.  Worker groups were configured to use region: eu-west-1 and zones: eu-west-1a, eu-west-1b, eu-west-1c. eu-west-1a zone was brought down following the above steps. State before and after the outage simulation is captured below.\n State before the outage simulation kubectl get po -n \u003cshoot-control-ns\u003e # list of pods in the shoot control plane    NAME READY STATUS NODE     cert-controller-manager-6cf9787df6-wzq86 1/1 Running ip-10-242-20-17.eu-west-1.compute.internal   cloud-controller-manager-7748bcf697-n66t7 1/1 Running ip-10-242-20-17.eu-west-1.compute.internal   csi-driver-controller-6cd9bc7997-m7hr6 6/6 Running ip-10-242-20-17.eu-west-1.compute.internal   csi-snapshot-controller-5f774d57b4-2bghj 1/1 Running ip-10-242-20-17.eu-west-1.compute.internal   csi-snapshot-validation-7c99986c85-rr7zk 1/1 Running ip-10-242-20-17.eu-west-1.compute.internal   etcd-events-0 2/2 Running ip-10-242-60-155.eu-west-1.compute.internal   etcd-events-1 2/2 Running ip-10-242-73-89.eu-west-1.compute.internal   etcd-events-2 2/2 Running ip-10-242-20-17.eu-west-1.compute.internal   etcd-main-0 2/2 Running ip-10-242-73-77.eu-west-1.compute.internal   etcd-main-1 2/2 Running ip-10-242-22-85.eu-west-1.compute.internal   etcd-main-2 2/2 Running ip-10-242-53-131.eu-west-1.compute.internal   gardener-resource-manager-7fff9f77f6-jwggx 1/1 Running ip-10-242-73-89.eu-west-1.compute.internal   gardener-resource-manager-7fff9f77f6-jwggx 1/1 Running ip-10-242-60-155.eu-west-1.compute.internal   gardener-resource-manager-7fff9f77f6-jwggx 1/1 Running ip-10-242-20-17.eu-west-1.compute.internal   grafana-operators-79b9cd58bb-z6hc2 1/1 Running ip-10-242-20-17.eu-west-1.compute.internal   grafana-operators-79b9cd58bb-z6hc2 1/1 Running ip-10-242-20-17.eu-west-1.compute.internal   kube-apiserver-5fcb7f4bff-7p4xc 1/1 Running ip-10-242-20-17.eu-west-1.compute.internal   kube-apiserver-5fcb7f4bff-845p7 1/1 Running ip-10-242-73-89.eu-west-1.compute.internal   kube-apiserver-5fcb7f4bff-mrspt 1/1 Running ip-10-242-60-155.eu-west-1.compute.internal   kube-controller-manager-6b94bcbc4-9bz8q 1/1 Running ip-10-242-20-17.eu-west-1.compute.internal   kube-scheduler-7f855ffbc4-8c9pg 1/1 Running ip-10-242-20-17.eu-west-1.compute.internal   kube-state-metrics-5446bb6d56-xqqnt 1/1 Running ip-10-242-20-17.eu-west-1.compute.internal   loki-0 4/4 Running ip-10-242-60-155.eu-west-1.compute.internal   machine-controller-manager-967bc89b5-kgdwx 2/2 Running ip-10-242-20-17.eu-west-1.compute.internal   prometheus-0 3/3 Running ip-10-242-60-155.eu-west-1.compute.internal   shoot-dns-service-75768bd764-4957h 1/1 Running ip-10-242-20-17.eu-west-1.compute.internal   vpa-admission-controller-6994f855c9-5vmh6 1/1 Running ip-10-242-20-17.eu-west-1.compute.internal   vpa-recommender-5bf4cfccb6-wft4b 1/1 Running ip-10-242-20-17.eu-west-1.compute.internal   vpa-updater-6f795d7bb8-snq67 1/1 Running ip-10-242-20-17.eu-west-1.compute.internal   vpn-seed-server-748674b7d8-qmjbm 2/2 Running ip-10-242-20-17.eu-west-1.compute.internal    kubectl get nodes -o=custom-columns='NAME:metadata.name,ZONE:metadata.labels.topology\\.kubernetes\\.io\\/zone' # list of nodes with name, zone and status (was taken separately)    NAME STATUS ZONE     ip-10-242-20-17.eu-west-1.compute.internal Ready eu-west-1a   ip-10-242-22-85.eu-west-1.compute.internal Ready eu-west-1a   ip-10-242-3-0.eu-west-1.compute.internal Ready eu-west-1a   ip-10-242-53-131.eu-west-1.compute.internal Ready eu-west-1b   ip-10-242-60-155.eu-west-1.compute.internal Ready eu-west-1b   ip-10-242-73-77.eu-west-1.compute.internal Ready eu-west-1c   ip-10-242-73-89.eu-west-1.compute.internal Ready eu-west-1c    kubectl get machines # list of machines for the multi-AZ shoot control plane    NAME STATUS     shoot–garden–aws-ha2-cpu-worker-etcd-z1-66659-sf4wt Running   shoot–garden–aws-ha2-cpu-worker-etcd-z2-c767d-s8cmf Running   shoot–garden–aws-ha2-cpu-worker-etcd-z3-9678d-6p8w5 Running   shoot–garden–aws-ha2-cpu-worker-z1-766bc-pjq6n Running   shoot–garden–aws-ha2-cpu-worker-z2-85968-5qmjh Running   shoot–garden–aws-ha2-cpu-worker-z3-5f499-hnrs6 Running   shoot–garden–aws-ha2-etcd-compaction-z1-6bd58-9ffc7 Running      State during outage kubectl get po -n \u003cshoot-control-ns\u003e # list of pods in the shoot control plane    NAME READY STATUS NODE     cert-controller-manager-6cf9787df6-dt5nw 1/1 Running ip-10-242-60-155.eu-west-1.compute.internal   cloud-controller-manager-7748bcf697-t2pn7 1/1 Running ip-10-242-73-89.eu-west-1.compute.internal   csi-driver-controller-6cd9bc7997-bn82b 6/6 Running ip-10-242-60-155.eu-west-1.compute.internal   csi-snapshot-controller-5f774d57b4-rskwj 1/1 Running ip-10-242-73-89.eu-west-1.compute.internal   csi-snapshot-validation-7c99986c85-ft2qp 1/1 Running ip-10-242-60-155.eu-west-1.compute.internal   etcd-events-0 2/2 Running ip-10-242-60-155.eu-west-1.compute.internal   etcd-events-1 2/2 Running ip-10-242-73-89.eu-west-1.compute.internal   etcd-events-2 0/2 Pending    etcd-main-0 2/2 Running ip-10-242-73-77.eu-west-1.compute.internal   etcd-main-1 0/2 Pending    etcd-main-2 2/2 Running ip-10-242-53-131.eu-west-1.compute.internal   gardener-resource-manager-7fff9f77f6-8wr5n 1/1 Running ip-10-242-73-89.eu-west-1.compute.internal   gardener-resource-manager-7fff9f77f6-jwggx 1/1 Running ip-10-242-73-89.eu-west-1.compute.internal   gardener-resource-manager-7fff9f77f6-lkgjh 1/1 Running ip-10-242-60-155.eu-west-1.compute.internal   grafana-operators-79b9cd58bb-m55sx 1/1 Running ip-10-242-60-155.eu-west-1.compute.internal   grafana-users-85c7b6856c-gx48n 1/1 Running ip-10-242-73-89.eu-west-1.compute.internal   kube-apiserver-5fcb7f4bff-845p7 1/1 Running ip-10-242-73-89.eu-west-1.compute.internal   kube-apiserver-5fcb7f4bff-mrspt 1/1 Running ip-10-242-60-155.eu-west-1.compute.internal   kube-apiserver-5fcb7f4bff-vkrdh 1/1 Running ip-10-242-73-89.eu-west-1.compute.internal   kube-controller-manager-6b94bcbc4-49v5x 1/1 Running ip-10-242-60-155.eu-west-1.compute.internal   kube-scheduler-7f855ffbc4-6xnbk 1/1 Running ip-10-242-73-89.eu-west-1.compute.internal   kube-state-metrics-5446bb6d56-g8wkp 1/1 Running ip-10-242-73-89.eu-west-1.compute.internal   loki-0 4/4 Running ip-10-242-73-89.eu-west-1.compute.internal   machine-controller-manager-967bc89b5-rr96r 2/2 Running ip-10-242-60-155.eu-west-1.compute.internal   prometheus-0 3/3 Running ip-10-242-60-155.eu-west-1.compute.internal   shoot-dns-service-75768bd764-7xhrw 1/1 Running ip-10-242-73-89.eu-west-1.compute.internal   vpa-admission-controller-6994f855c9-7xt8p 1/1 Running ip-10-242-73-89.eu-west-1.compute.internal   vpa-recommender-5bf4cfccb6-8wdpr 1/1 Running ip-10-242-73-89.eu-west-1.compute.internal   vpa-updater-6f795d7bb8-gccv2 1/1 Running ip-10-242-60-155.eu-west-1.compute.internal   vpn-seed-server-748674b7d8-cb8gh 2/2 Running ip-10-242-60-155.eu-west-1.compute.internal    Most of the pods except etcd-events-2 and etcd-main-1 are stuck in Pending state. Rest all of the pods are which were running on nodes in eu-west-1a zone were rescheduled automatically.\nkubectl get nodes -o=custom-columns='NAME:metadata.name,ZONE:metadata.labels.topology\\.kubernetes\\.io\\/zone' # list of nodes with name, zone and status (was taken separately)    NAME STATUS ZONE     ip-10-242-20-17.eu-west-1.compute.internal NotReady eu-west-1a   ip-10-242-22-85.eu-west-1.compute.internal NotReady eu-west-1a   ip-10-242-3-0.eu-west-1.compute.internal NotReady eu-west-1a   ip-10-242-53-131.eu-west-1.compute.internal Ready eu-west-1b   ip-10-242-60-155.eu-west-1.compute.internal Ready eu-west-1b   ip-10-242-73-77.eu-west-1.compute.internal Ready eu-west-1c   ip-10-242-73-89.eu-west-1.compute.internal Ready eu-west-1c    kubectl get machines # list of machines for the multi-AZ shoot control plane    NAME STATUS AGE     shoot–garden–aws-ha2-cpu-worker-etcd-z1-66659-jlv56 Terminating 21m   shoot–garden–aws-ha2-cpu-worker-etcd-z1-66659-sf4wt Unknown 3d   shoot–garden–aws-ha2-cpu-worker-etcd-z2-c767d-s8cmf Running 3d   shoot–garden–aws-ha2-cpu-worker-etcd-z3-9678d-6p8w5 Running 3d   shoot–garden–aws-ha2-cpu-worker-z1-766bc-9m45j CrashLoopBackOff 2m55s   shoot–garden–aws-ha2-cpu-worker-z1-766bc-pjq6n Unknown 2d21h   shoot–garden–aws-ha2-cpu-worker-z1-766bc-vlflq Terminating 28m   shoot–garden–aws-ha2-cpu-worker-z2-85968-5qmjh Running 3d   shoot–garden–aws-ha2-cpu-worker-z2-85968-zs9lr CrashLoopBackOff 7m26s   shoot–garden–aws-ha2-cpu-worker-z3-5f499-hnrs6 Running 2d21h   shoot–garden–aws-ha2-etcd-compaction-z1-6bd58-8qzln CrashLoopBackOff 12m   shoot–garden–aws-ha2-etcd-compaction-z1-6bd58-9ffc7 Terminating 3d    MCM attempts to delete the machines and since it is unable to the machines transition to Terminating state and are stuck there. It subsequently attempts to launch new machines which also fails and these machines transition to CrashLoopBackOff state.\n Ingress/Egress Traffic Analysis Details Consider the following etcd cluster:\n$ etcdctl endpoint status --cluster -w table    ENDPOINT ID VERSION DB SIZE IS LEADER IS LEARNER RAFT TERM RAFT INDEX RAFT APPLIED INDEX     https://etcd-main-0.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 37e93e9d1dd2cc8e 3.4.13 7.6 MB false false 47 3863 3863   https://etcd-main-2.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 65fe447d73e9dc58 3.4.13 7.6 MB true false 47 3863 3863   https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 ad4fe89f4e731298 3.4.13 7.6 MB false false 47 3863 3863     Multi-zonal shoot control plane ingress/egress traffic in a fresh shoot cluster with no user activity The steady state traffic (post all controllers have made initial list requests to refresh their informer caches) is depicted below (span = 1hr):\nObservations:\n Leader to per follower max egress: ~20Kib/s One Follower to Leader max egress: ~20Kib/s Follower to follower max egress: ~2Kibs/s  Total ingress + egress traffic amongst etcd members = ~84Kib/s.\n  Traffic generated during PUT requests to etcd leader Generating a load 100 put requests/second for 30 seconds duration by targeting etcd leader, This will generate ~100KiB/s traffic (value size is 1kib).\n benchmark put --target-leader --rate 100 --conns=400 --clients=400 --sequential-keys --key-starts 0 --val-size=1024 --total=3000 \\  --endpoints=https://etcd-main-client:2379 \\  --key=/var/etcd/ssl/client/client/tls.key \\  --cacert=/var/etcd/ssl/client/ca/bundle.crt \\  --cert=/var/etcd/ssl/client/client/tls.crt Observations:\n Leader to per follower max egress: ~155 KiB/s (pattern duration: 50 secs) One Follower to Leader max egress: ~50Kib/s (pattern duration: 50 secs) Follower to follower max egress: ~2Kibs/s  Total ingress + egress traffic amongst etcd members = ~412Kib/s.\n  Traffic generated during PUT requests to etcd follower Generating a load 100 put requests/second for 30 seconds duration by targeting etcd follower, This will generate ~100KiB/s traffic(value size is 1kib). benchmark put --rate 100 --conns=400 --clients=400 --sequential-keys --key-starts 3000 --val-size=1024 --total=3000 \\  --endpoints=https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 \\  --key=/var/etcd/ssl/client/client/tls.key \\  --cacert=/var/etcd/ssl/client/ca/bundle.crt \\  --cert=/var/etcd/ssl/client/client/tls.crt Observations:\n In this case, the follower(etcd-main-1) redirects the put request to leader(etcd-main2) max egress: ~168 KiB/s Leader to per follower max egress: ~150 KiB/s (pattern duration: 50 secs) One Follower to Leader max egress: ~45Kib/s (pattern duration: 50 secs) Follower to follower max egress: ~2Kibs/s  Total ingress + egress traffic amongst etcd members = ~517KiB/s.\n  Traffic generated during etcd follower initial sync with large revision difference Consider the following etcd cluster:\n$ etcdctl endpoint status --cluster -w table    ENDPOINT ID VERSION DB SIZE IS LEADER IS LEARNER RAFT TERM RAFT INDEX RAFT APPLIED INDEX     https://etcd-main-0.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 37e93e9d1dd2cc8e 3.4.13 5.1 GB false false 48 47527 47527   https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 ad4fe89f4e731298 3.4.13 5.1 GB true false 48 47576 47576    In this case, new follower or crashed follower etcd-main-2 joins the etcd cluster with large revision difference (5.1 GB DB size). The etcd follower had its DB size = 14MB when it crashed. There was a flurry of activity and that increased the leader DB size to 5.1GB thus creating a huge revision difference.\n Scale up etcd member to 3\n kubectl scale statefulsets etcd-main --replicas=3 -n shoot--ash-garden--mz-neem\n   ENDPOINT ID VERSION DB SIZE IS LEADER IS LEARNER RAFT TERM RAFT INDEX RAFT APPLIED INDEX     https://etcd-main-0.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 37e93e9d1dd2cc8e 3.4.13 5.1 GB false false 48 50502 50502   https://etcd-main-2.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 65fe447d73e9dc58 3.4.13 5.0 GB false false 48 50502 50502   https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 ad4fe89f4e731298 3.4.13 5.1 GB true false 48 50502 50502    Observations:\n Leader to new follower or crashed follower etcd-main-2 (which joins with large revision difference) max egress: ~120 MiB/s (noticeable pattern duration: 40 secs). New follower etcd-main-2 to Leader max egress: ~159 KiB/s (pattern duration: 40 secs). Leader to another follower etcd-main-0 max egress: \u003c20KiB/s. Follower etcd-main-0 to Leader max egress: \u003c20\u003e Kib/s. Follower to follower max egress: ~2Kibs/s  Total ingress + egress traffic amongst etcd members = ~121MiB/s.\n  Traffic generated during GET requests to etcd leader In this case, trying to get the keys which matches between 1 and 17999 by targeting leader etcd-main-1. This will dump both keys and values.\nExecuting the following command from etcd-client pod(running in same namespace).\nroot@etcd-client:/# etcdctl --endpoints=https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 get 1 17999 \u003e /tmp/range2.txt root@etcd-client:/# du -h /tmp/range2.txt 607M\t/tmp/range2.txt Observations:\n Downloaded dump file is around 607 MiB. Leader etcd-main-1 to etcd-client max egress ~34MiBs. Etcd intra cluster network traffic remains same, observed there is no change in network traffic pattern    Traffic generated during GET requests to etcd follower In this case, trying to get the keys which matches between 1 and 17999 by targeting followeretcd-main-2. This will dump both keys and values.\nroot@etcd-client:/# etcdctl --endpoints=https://etcd-main-2.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 get 1 17999 \u003e /tmp/range.txt root@etcd-client:/# du -h /tmp/range.txt 607M\t/tmp/range.txt root@etcd-client:/# Observations:\n Downloaded dump file is around 607 MiB. Follower etcd-main-2 to etcd-client max egress ~32MiBs. Etcd intra cluster network traffic remains same, observed there is no change in network traffic pattern Watch requests are not forwarded to Leader etcd-main-1 from etcd-main-2    Traffic generated during DELETE requests to etcd leader In this case, trying to delete the keys which matches between 0 and 99999 by targeting leader.\nExecuting the following command from etcd-client pod.\nroot@etcd-client:/# time etcdctl --endpoints=https://etcd-main-2.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 del 0 99999 --dial-timeout=300s --command-timeout=300s 99999  real\t0m0.664s user\t0m0.008s sys\t0m0.016s root@etcd-client:/# Observations:\n Downloaded dump file is around 607 MiB. Leader etcd-main-2 to etcd-client max egress ~226B/s. Etcd intra cluster network traffic remains same, observed there is no change in network traffic pattern.    Traffic generated during DELETE requests to etcd follower In this case, trying to delete the keys which matches between 0 and 99999 by targeting follower.\nExecuting the following command from etcd-client pod.\nroot@etcd-client:/# time etcdctl --endpoints=https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 del 0 99999 --dial-timeout=300s --command-timeout=300s 99999  real\t0m0.664s user\t0m0.008s sys\t0m0.016s root@etcd-client:/# Observations:\n Downloaded dump file is around 607 MiB. Leader etcd-main-2 to etcd-client max egress ~222B/s. Etcd intra cluster network traffic remains same, observed there is no change in network traffic pattern.    Traffic generated during WATCH requests to etcd Etcd cluster state\n   ENDPOINT ID VERSION DB SIZE IS LEADER IS LEARNER RAFT TERM RAFT INDEX RAFT APPLIED INDEX     https://etcd-main-0.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 37e93e9d1dd2cc8e 3.4.13 673 MB false false 388 970471 970471   https://etcd-main-2.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 65fe447d73e9dc58 3.4.13 673 MB true false 388 970472 970472   https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 ad4fe89f4e731298 3.4.13 673 MB false false 388 970472 970472    Watching the keys which matches between 0 and 99999 by targeting follower.\nExecuting the following command from etcd-client pod.\ntime etcdctl --endpoints=https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 watch 0 99999 --dial-timeout=300s --command-timeout=300s In parallel generating 100000 keys and each value size is 1Kib by targeting etcd leader for this case around(500rps)\nbenchmark put --target-leader --rate 500 --conns=400 --clients=800 --sequential-keys --key-starts 0 --val-size=1024 --total=100000 \\ --endpoints=https://etcd-main-client:2379 \\ --key=/var/etcd/ssl/client/client/tls.key \\ --cacert=/var/etcd/ssl/client/ca/bundle.crt \\ --cert=/var/etcd/ssl/client/client/tls.crt Observations:\n Etcd intra cluster network traffic remains same, observed there is no change in network traffic pattern. Follower etcd-main-1 to etcd-client max egress is 496 KiBs. Watch requests are not forwarded to Leader etcd-main-2 from follower etcd-main-1 .  Deleting the keys which matches between 0 and 99999 by targeting follower and in parallel watching the keys.\nroot@etcd-client:/# time etcdctl --endpoints=https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 del 0 99999 --dial-timeout=300s --command-timeout=300s 99999  real\t0m0.590s user\t0m0.018s sys\t0m0.006s Observations:\n Etcd intra cluster network traffic remains same, observed there is no change in network traffic pattern. Follower etcd-main-1 to etcd-client max egress is 222B/s. watch lists the keys which are deleted, not values.   ","categories":"","description":"","excerpt":"GEP20 - Highly Available Shoot Control Planes Table of Contents  GEP20 …","ref":"/docs/gardener/proposals/20-ha-control-planes/","tags":"","title":"Highly Available Shoot Control Planes"},{"body":"If you need information on how to deploy your cluster on your local machine, a step-by-step tutorial can be found at Running Gardener locally.\nYou can also watch the recorded Community Call to get a better understanding of the process.\n","categories":"","description":"","excerpt":"If you need information on how to deploy your cluster on your local …","ref":"/docs/faq/gardener-locally/","tags":"","title":"How can you deploy Gardener locally?"},{"body":"There are two ways to get the health information of a shoot API server.\n Try to reach the public endpoint of the shoot API server via \"https://api.\u003cshoot-name\u003e.\u003cproject-name\u003e.shoot.\u003ccanary|office|live\u003e.k8s-hana.ondemand.com/healthz\"  The endpoint is secured, therefore you need to authenticate via basic auth or client cert. Both are available in the admin kubeconfig of the shoot cluster. Note that with those credentials you have full (admin) access to the cluster, therefore it is highly recommended to create custom credentials with some RBAC rules and bindings which only allow access to the /healthz endpoint.\n Fetch the shoot resource of your cluster via the programmatic API of the Gardener and get the availability information from the status. You need a kubeconfig for the Garden cluster, which you can get via the Gardener dashboard. Then you could fetch your shoot resource and query for the availability information via:   kubectl get shoot \u003cshoot-name\u003e -o json | jq -r '.status.conditions[] | select(.type==\"APIServerAvailable\")' The availability information in the second scenario is collected by the Gardener. If you want to collect the information independently from Gardener, you should choose the first scenario.\nIf you want to archive a simple pull monitor in the AvS for a shoot cluster, you also need to use the first scenario, because with it you have a stable endpoint for the API server which you can query.\n","categories":"","description":"","excerpt":"There are two ways to get the health information of a shoot API …","ref":"/docs/faq/clusterhealthz/","tags":"","title":"How can you get the status of a shoot API server?"},{"body":"Monitoring your Kubernetes deployments is possible through the use of Dynatrace or Prometheus.\nHere you can also read about monitoring your Kubernetes applications.\n","categories":"","description":"","excerpt":"Monitoring your Kubernetes deployments is possible through the use of …","ref":"/docs/faq/dynatrace/","tags":"","title":"How can you monitor your Kubernetes deployments?"},{"body":"Configuration of Multi-AZ worker pools depends on the infrastructure.\nThe zone distribution for the worker pools can be configured generically across all infrastructures. You can find provider-specific details in the InfrastructureConfig section of each extension provider repository:\n AWS (a VPC with a subnet is required in each zone you want to support) GCP Azure AliCloud OpenStack  ","categories":"","description":"","excerpt":"Configuration of Multi-AZ worker pools depends on the infrastructure. …","ref":"/docs/faq/configure-worker-pools/","tags":"","title":"How do you configure Multi-AZ worker pools for different extensions?"},{"body":"End-users must provide credentials such that Gardener and Kubernetes controllers can communicate with the respective cloud provider APIs in order to perform infrastructure operations. These credentials should be regularly rotated.\nHow to do so is explained in Shoot Credentials Rotation.\n","categories":"","description":"","excerpt":"End-users must provide credentials such that Gardener and Kubernetes …","ref":"/docs/faq/rotate-iaas-keys/","tags":"","title":"How do you rotate IaaS keys for a running cluster?"},{"body":"Introduction Kubernetes offers powerful options to get more details about startup or runtime failures of pods as e.g. described in Application Introspection and Debugging or Debug Pods and Replication Controllers.\nIn order to identify pods with potential issus you could e.g. run kubectl get pods --all-namespaces | grep -iv Running  to filter out the pods which are not in the state Running. One of frequent error state is CrashLoopBackOff, which tells that a pod crashes right after the start. Kubernetes then tries to restart the pod, but often the pod startup fails again.\nHere is a short list of possible reasons which might lead to a pod crash:\n error during image pull caused by e.g. wrong/missing secrets or wrong/missing image the app runs in an error state caused e.g. by missing environmental variables (ConfigMaps) or secrets liveness probe failed too high resource consumption (memory and/or CPU) or too strict quota settings persistent volumes can’t be created/mounted the container image is not updated  Basically, the commands kubectl logs ... and kubectl describe ... with additional parameters are used to get more detailed information. By calling e.g. kubectl logs --help you get more detailed information about the command and its parameters.\nIn the next sections you’ll find some basic approaches to get some ideas what went wrong.\nRemarks:\n Even if the pods seem to be running as the status Running indicates, a high counter of the Restarts shows potential problems There is as well an interactive Tutorial Troubleshooting with Kubectl available which explains basic debugging activities The examples below are deployed into the namespace default. In case you want to change it use the optional parameter --namespace \u003cyour-namespace\u003e to select the target namespace. They require Kubernetes release ≥ 1.8.  Prerequisites Your deployment was successful (no logical/syntactical errors in the manifest files), but the pod(s) aren’t running.\nError caused by wrong image name You run kubectl describe pod \u003cyour-pod\u003e \u003cyour-namespace\u003e to get detailed information about the pod startup.\nIn the Events section, you get an error message like Failed to pull image ... and Reason: Failed. The pod is in state ImagePullBackOff.\nThe example below is based on demo in Kubernetes documentation. In all examples the default namespace is used.\nFirst, cleanup with\nkubectl delete pod termination-demo Next, create a resource based on the yaml content below\napiVersion: v1 kind: Pod metadata:  name: termination-demo spec:  containers:  - name: termination-demo-container  image: debiann  command: [\"/bin/sh\"]  args: [\"-c\", \"sleep 10 \u0026\u0026 echo Sleep expired \u003e /dev/termination-log\"] kubectl describe pod termination-demo lists the following content in the Event section\nEvents:  FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage  ---------\t--------\t-----\t----\t-------------\t--------\t------\t-------  2m\t2m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned termination-demo to ip-10-250-17-112.eu-west-1.compute.internal  2m\t2m\t1\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \"default-token-sgccm\"  2m\t1m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulling\tpulling image \"debiann\"  2m\t1m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tWarning\tFailed\tFailed to pull image \"debiann\": rpc error: code = Unknown desc = Error: image library/debiann:latest not found  2m\t54s\t10\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tWarning\tFailedSync\tError syncing pod  2m\t54s\t6\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tBackOff\tBack-off pulling image \"debiann\" The error message with Reason: Failed tells that there is an error during pulling the image. A closer look at the image name indicates a misspelling.\nApp runs in an error state caused by missing ConfigMaps or Secrets This example illustrates the behavior in case of the app expecting environment variables but the corresponding Kubernetes artifacts are missing.\nFirst, cleanup with\nkubectl delete deployment termination-demo kubectl delete configmaps app-env Next, deploy this manifest\napiVersion: apps/v1 kind: Deployment metadata:  name: termination-demo  labels:  app: termination-demo spec:  replicas: 1  selector:  matchLabels:  app: termination-demo  template:  metadata:  labels:  app: termination-demo  spec:  containers:  - name: termination-demo-container  image: debian  command: [\"/bin/sh\"]  args: [\"-c\", \"sed \\\"s/foo/bar/\\\" \u003c $MYFILE\"] Now, the command kubectl get pods lists the pod termination-demo-xxx in the state Error or CrashLoopBackOff. The command kubectl describe pod termination-demo-xxx tells that there is no error during startup but gives no clue about what caused the crash.\nEvents:  FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage  ---------\t--------\t-----\t----\t-------------\t--------\t------\t-------  19m\t19m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned termination-demo-5fb484867d-xz2x9 to ip-10-250-17-112.eu-west-1.compute.internal  19m\t19m\t1\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \"default-token-sgccm\"  19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulling\tpulling image \"debian\"  19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulled\tSuccessfully pulled image \"debian\"  19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tCreated\tCreated container  19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tStarted\tStarted container  19m\t14m\t24\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tWarning\tBackOff\tBack-off restarting failed container  19m\t4m\t69\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tWarning\tFailedSync\tError syncing pod The command kubectl get logs termination-demo-xxx gives access to the output, the application writes on stderr and stdout. In this case, you get an output like\n/bin/sh: 1: cannot open : No such file So you need to have a closer look at the application. In this case the environmental variable MYFILEis missing. To fix this issue you could e.g. add a ConfigMap to your deployment as it is shown in the manifest listed below.\napiVersion: v1 kind: ConfigMap metadata:  name: app-env data:  MYFILE: \"/etc/profile\" --- apiVersion: apps/v1 kind: Deployment metadata:  name: termination-demo  labels:  app: termination-demo spec:  replicas: 1  selector:  matchLabels:  app: termination-demo  template:  metadata:  labels:  app: termination-demo  spec:  containers:  - name: termination-demo-container  image: debian  command: [\"/bin/sh\"]  args: [\"-c\", \"sed \\\"s/foo/bar/\\\" \u003c $MYFILE\"]  envFrom:  - configMapRef:  name: app-env Note that once you fix the error and re-run the scenario, you might still see the pod in CrashLoopBackOff status. It is because the container finishes the command sed ... and runs to completion. In order to keep the container in Running status, a long running task is required, e.g.\napiVersion: v1 kind: ConfigMap metadata:  name: app-env data:  MYFILE: \"/etc/profile\"  SLEEP: \"5\" --- apiVersion: apps/v1 kind: Deployment metadata:  name: termination-demo  labels:  app: termination-demo spec:  replicas: 1  selector:  matchLabels:  app: termination-demo  template:  metadata:  labels:  app: termination-demo  spec:  containers:  - name: termination-demo-container  image: debian  command: [\"/bin/sh\"]  # args: [\"-c\", \"sed \\\"s/foo/bar/\\\" \u003c $MYFILE\"]  args: [\"-c\", \"while true; do sleep $SLEEP; echo sleeping; done;\"]  envFrom:  - configMapRef:  name: app-env Too high resource consumption or too strict quota settings You can optionally specify the amount of memory and/or CPU your container gets during runtime. In case these settings are missing, the default requests settings are taken: CPU: 0m (in Milli CPU) and RAM: 0Gi which indicate no other limits than the ones of the node(s) itself. Find more details in Configure Default Memory Requests and Limits for a Namespace,\nIn case your application needs more resources, Kubernetes distinguishes between requests and limit settings: requests specify the guaranteed amount of resource, whereas limit tells Kubernetes the maximum amount of resource the container might need. Mathematically both settings could be described by the relation 0 \u003c= requests \u003c= limit. For both settings you need to consider the total amount of resources the available nodes provide. For a detailed description of the concept see Resource Quality of Service in Kubernetes.\nUse kubectl describe nodes to get a first overview of the resource consumption of your cluster. Of special interest are the figures indicating the amount of CPU and Memory Requests at the bottom of the output.\nThe next example demonstrates what happens in case the CPU request is too high in order to be managed by your cluster.\nFirst, cleanup with\nkubectl delete deployment termination-demo kubectl delete configmaps app-env Next, adapt the cpu in the yaml below to be slightly higher than the remaining cpu resources in your cluster and deploy this manifest. In this example 600m (milli CPUs) are requested in a Kubernetes system with a single 2 Core worker node which results in an error message.\napiVersion: apps/v1 kind: Deployment metadata:  name: termination-demo  labels:  app: termination-demo spec:  replicas: 1  selector:  matchLabels:  app: termination-demo  template:  metadata:  labels:  app: termination-demo  spec:  containers:  - name: termination-demo-container  image: debian  command: [\"/bin/sh\"]  args: [\"-c\", \"sleep 10 \u0026\u0026 echo Sleep expired \u003e /dev/termination-log\"]  resources:  requests:  cpu: \"600m\" The command kubectl get pods lists the pod termination-demo-xxx in the state Pending. More details on why this happens could be found by using the command kubectl describe pod termination-demo-xxx:\n$ kubectl describe po termination-demo-fdb7bb7d9-mzvfw Name: termination-demo-fdb7bb7d9-mzvfw Namespace: default ... Containers:  termination-demo-container:  Image: debian  Port: \u003cnone\u003e  Host Port: \u003cnone\u003e  Command:  /bin/sh  Args:  -c  sleep 10 \u0026\u0026 echo Sleep expired \u003e /dev/termination-log  Requests:  cpu: 6  Environment: \u003cnone\u003e  Mounts:  /var/run/secrets/kubernetes.io/serviceaccount from default-token-t549m (ro) Conditions:  Type Status  PodScheduled False Events:  Type Reason Age From Message  ---- ------ ---- ---- -------  Warning FailedScheduling 9s (x7 over 40s) default-scheduler 0/2 nodes are available: 2 Insufficient cpu. More details in\n Managing Compute Resources for Containters Resource Quality of Service in Kubernetes  Remark:\n This example works similarly when specifying a too high request for memory In case you configured an autoscaler range when creating your Kubernetes cluster another worker node will be started automatically if you didn’t reach the maximum number of worker nodes If your app is running out of memory (the memory settings are too small), you typically find OOMKilled (Out Of Memory) message in the Events section fo the kubectl describe pod ... output  Why was the container image not updated? You applied a fix in your app, created a new container image and pushed it into your container repository. After redeploying your Kubernetes manifests you expected to get the updated app, but still the same bug is in the new deployment present.\nThis behavior is related to how Kubernetes decides whether to pull a new docker image or to use the cached one.\nIn case you didn’t change the image tag, the default image policy IfNotPresent tells Kubernetes to use the cached image (see Images).\nAs a best practice you should not use the tag latest and change the image tag whenever you changed anything in your image (see Configuration Best Practices).\nFind more details in FAQ Container Image not updating.\nLinks  Application Introspection and Debugging Debug Pods and Replication Controllers Logging Architecture Configure Default Memory Requests and Limits for a Namespace Managing Compute Resources for Containters Resource Quality of Service in Kubernetes Interactive Tutorial Troubleshooting with Kubectl Images Kubernetes Best Practises  ","categories":"","description":"Your pod doesn't run as expected. Are there any log files? Where? How could I debug a pod?","excerpt":"Your pod doesn't run as expected. Are there any log files? Where? How …","ref":"/docs/guides/monitoring_and_troubleshooting/debug-a-pod/","tags":"","title":"How to Debug a Pod"},{"body":"Image Vector The Gardenlet is deploying several different container images into the seed and the shoot clusters. The image repositories and tags are defined in a central image vector file. Obviously, the image versions defined there must fit together with the deployment manifests (e.g., some command-line flags do only exist in certain versions).\nExample images: - name: pause-container  sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile  repository: gcr.io/google_containers/pause-amd64  tag: \"3.0\"  version: 1.17.x  architectures:  - amd64 - name: pause-container  sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile  repository: gcr.io/google_containers/pause-amd64  tag: \"3.1\"  version: \"\u003e= 1.18\"  architectures:  - amd64 ... That means that the Gardenlet will use the pause-container in with tag 3.0 for all seed/shoot clusters with Kubernetes version 1.17.x, and tag 3.1 for all clusters with Kubernetes \u003e= 1.18.\nImage Vector Architecture images: - name: pause-container  sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile  repository: registry.k8s.io/pause  tag: \"3.0\"  architectures:  - amd64 - name: pause-container  sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile  repository: registry.k8s.io/pause  tag: \"3.0\"  architectures:  - arm64 - name: pause-container  sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile  repository: registry.k8s.io/pause  tag: \"3.0\"  architectures:  - amd64  - arm64 ... Architectures is an optional field of image. It is a list of strings specifying CPU architecture of machines on which this image can be used. The valid options for architectures field are as follow:\n amd64 : This sepecifies image can run only on machines having CPU architecture amd64. arm64 : This sepecifies image can run only on machines having CPU architecture arm64.  If image doesn’t specifies any architectures then by default it is considered to support both amd64 and arm64 architectures.\nOverwrite image vector In some environment it is not possible to use these “pre-defined” images that come with a Gardener release. A prominent example for that is Alicloud in China which does not allow access to Google’s GCR. In these cases you might want to overwrite certain images, e.g., point the pause-container to a different registry.\n⚠️ If you specify an image that does not fit to the resource manifest then the seed/shoot reconciliation might fail.\nIn order to overwrite the images you must provide a similar file to Gardenlet:\nimages: - name: pause-container  sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile  repository: my-custom-image-registry/pause-amd64  tag: \"3.0\"  version: 1.17.x - name: pause-container  sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile  repository: my-custom-image-registry/pause-amd64  tag: \"3.1\"  version: \"\u003e= 1.18\" ... During deployment of the gardenlet create a ConfigMap containing the above content and mount it as a volume into the gardenlet pod. Next, specify the environment variable IMAGEVECTOR_OVERWRITE whose value must be the path to the file you just mounted:\napiVersion: v1 kind: ConfigMap metadata:  name: gardenlet-images-overwrite  namespace: garden data:  images_overwrite.yaml: |images: - ... --- apiVersion: apps/v1 kind: Deployment metadata:  name: gardenlet  namespace: garden spec:  template:  ...  spec:  containers:  - name: gardenlet  env:  - name: IMAGEVECTOR_OVERWRITE  value: /charts-overwrite/images_overwrite.yaml  volumeMounts:  - name: gardenlet-images-overwrite  mountPath: /charts-overwrite  ...  volumes:  - name: gardenlet-images-overwrite  configMap:  name: gardenlet-images-overwrite  ... Image vectors for dependent components The gardenlet is deploying a lot of different components that might deploy other images themselves. These components might use an image vector as well. Operators might want to customize the image locations for these transitive images as well, hence, they might need to specify an image vector overwrite for the components directly deployed by Gardener.\nIt is possible to specify the IMAGEVECTOR_OVERWRITE_COMPONENTS environment variable to the gardenlet that points to a file with the following content:\ncomponents: - name: etcd-druid  imageVectorOverwrite: |images: - name: etcd tag: v1.2.3 repository: etcd/etcd ... The gardenlet will, if supported by the directly deployed component (etcd-druid in this example), inject the given imageVectorOverwrite into the Deployment manifest. The respective component is responsible for using the overwritten images instead of its defaults.\n","categories":"","description":"","excerpt":"Image Vector The Gardenlet is deploying several different container …","ref":"/docs/gardener/deployment/image_vector/","tags":"","title":"Image Vector"},{"body":"Contract: Infrastructure resource Every Kubernetes cluster requires some low-level infrastructure to be setup in order to work properly. Examples for that are networks, routing entries, security groups, IAM roles, etc. Before introducing the Infrastructure extension resource Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see here). Now, Gardener commissions an external, provider-specific controller to take over this task.\nWhich infrastructure resources are required? Unfortunately, there is no general answer to this question as it is highly provider specific. Consider the above mentioned resources, i.e. VPC, subnets, route tables, security groups, IAM roles, SSH key pairs. Most of the resources are required in order to create VMs (the shoot cluster worker nodes), load balancers, and volumes.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: Infrastructure metadata:  name: infrastructure  namespace: shoot--foo--bar spec:  type: azure  region: eu-west-1  secretRef:  name: cloudprovider  namespace: shoot--foo--bar  providerConfig:  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  resourceGroup:  name: mygroup  networks:  vnet: # specify either 'name' or 'cidr'  # name: my-vnet  cidr: 10.250.0.0/16  workers: 10.250.0.0/19 The .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. However, the most important section is the .spec.providerConfig. It contains an embedded declaration of the provider specific configuration for the infrastructure (that cannot be known by Gardener itself). You are responsible for designing how this configuration looks like. Gardener does not evaluate it but just copies this part from what has been provided by the end-user in the Shoot resource.\nAfter your controller has created the required resources in your provider’s infrastructure it needs to generate an output that can be used by other controllers in subsequent steps. An example for that is the Worker extension resource controller. It is responsible for creating virtual machines (shoot worker nodes) in this prepared infrastructure. Everything that it needs to know in order to do that (e.g., the network IDs, security group names, etc. (again: provider-specific)) needs to be provided as output in the Infrastructure resource:\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: Infrastructure metadata:  name: infrastructure  namespace: shoot--foo--bar spec:  ... status:  lastOperation: ...  providerStatus:  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureStatus  resourceGroup:  name: mygroup  networks:  vnet:  name: my-vnet  subnets:  - purpose: nodes  name: my-subnet  availabilitySets:  - purpose: nodes  id: av-set-id  name: av-set-name  routeTables:  - purpose: nodes  name: route-table-name  securityGroups:  - purpose: nodes  name: sec-group-name In order to support a new infrastructure provider you need to write a controller that watches all Infrastructures with .spec.type=\u003cmy-provider-name\u003e. You can take a look at the below referenced example implementation for the Azure provider.\nDynamic nodes network for shoot clusters Some environments do not allow end-users to statically define a CIDR for the network that shall be used for the shoot worker nodes. In these cases it is possible for the extension controllers to dynamically provision a network for the nodes (as part of their reconciliation loops), and to provide the CIDR in the status of the Infrastructure resource:\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: Infrastructure metadata:  name: infrastructure  namespace: shoot--foo--bar spec:  ... status:  lastOperation: ...  providerStatus: ...  nodesCIDR: 10.250.0.0/16 Gardener will pick this nodesCIDR and use it to configure the VPN components to establish network connectivity between the control plane and the worker nodes. If the Shoot resource already specifies a nodes CIDR in .spec.networking.nodes and the extension controller provides also a value in .status.nodesCIDR in the Infrastructure resource then the latter one will always be considered with higher priority by Gardener.\nNon-provider specific information required for infrastructure creation Some providers might require further information that is not provider specific but already part of the shoot resource. One example for this is the GCP infrastructure controller which needs the pod and the service network of the cluster in order to prepare and configure the infrastructure correctly. As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information that is not part of the Infrastructure resource itself.\nImplementation details Actuator interface Most existing infrastructure controller implementations follow a common pattern where a generic Reconciler delegates to an Actuator interface that contains the methods Reconcile, Delete, Migrate, and Restore. These methods are called by the generic Reconciler for the respective operations, and should be implemented by the extension according to the contract described here and the migration guidelines.\nConfigValidator interface For infrastructure controllers, the generic Reconciler also delegates to a ConfigValidator interface that contains a single Validate method. This method is called by the generic Reconciler at the beginning of every reconciliation, and can be implemented by the extension to validate the .spec.providerConfig part of the Infrastructure resource with the respective cloud provider, typically the existence and validity of cloud provider resources such as AWS VPCs or GCP Cloud NAT IPs.\nThe Validate method returns a list of errors. If this list is non-empty, the generic Reconciler will fail with an error. This error will have the error code ERR_CONFIGURATION_PROBLEM, unless there is at least one error in the list that has its ErrorType field set to field.ErrorTypeInternal.\nReferences and additional resources  Infrastructure API (Golang specification) Sample implementation for the Azure provider Sample ConfigValidator implementation  ","categories":"","description":"","excerpt":"Contract: Infrastructure resource Every Kubernetes cluster requires …","ref":"/docs/gardener/extensions/infrastructure/","tags":"","title":"Infrastructure"},{"body":"This guide walks you through the installation of the latest version of Knative using pre-built images on a Gardener created cluster environment. To set up your own Gardener, see the documentation or have a look at the landscape-setup-template project. To learn more about this open source project, read the blog on kubernetes.io.\nBefore you begin Knative requires a Kubernetes cluster v1.15 or newer.\nInstall and configure kubectl   If you already have kubectl CLI, run kubectl version --short to check the version. You need v1.10 or newer. If your kubectl is older, follow the next step to install a newer version.\n  Install the kubectl CLI.\n  Access Gardener   Create a project in the Gardener dashboard. This will essentially create a Kubernetes namespace with the name garden-\u003cmy-project\u003e.\n  Configure access to your Gardener project using a kubeconfig. If you are not the Gardener Administrator already, you can create a technical user in the Gardener dashboard: go to the “Members” section and add a service account. You can then download the kubeconfig for your project. You can skip this step if you create your cluster using the user interface; it is only needed for programmatic access, make sure you set export KUBECONFIG=garden-my-project.yaml in your shell.   Creating a Kubernetes cluster You can create your cluster using kubectl cli by providing a cluster specification yaml file. You can find an example for GCP here. Make sure the namespace matches that of your project. Then just apply the prepared so-called “shoot” cluster crd with kubectl:\nkubectl apply --filename my-cluster.yaml The easier alternative is to create the cluster following the cluster creation wizard in the Gardener dashboard: Configure kubectl for your cluster You can now download the kubeconfig for your freshly created cluster in the Gardener dashboard or via cli as follows:\nkubectl --namespace shoot--my-project--my-cluster get secret kubecfg --output jsonpath={.data.kubeconfig} | base64 --decode \u003e my-cluster.yaml This kubeconfig file has full administrators access to you cluster. For the rest of this guide be sure you have export KUBECONFIG=my-cluster.yaml set.\nInstalling Istio Knative depends on Istio. If your cloud platform offers a managed Istio installation, we recommend installing Istio that way, unless you need the ability to customize your installation.\nOtherwise, see the Installing Istio for Knative guide to install Istio.\nYou must install Istio on your Kubernetes cluster before continuing with these instructions to install Knative.\nInstalling cluster-local-gateway for serving cluster-internal traffic If you installed Istio, you can install a cluster-local-gateway within your Knative cluster so that you can serve cluster-internal traffic. If you want to configure your revisions to use routes that are visible only within your cluster, install and use the cluster-local-gateway.\nInstalling Knative The following commands install all available Knative components as well as the standard set of observability plugins. Knative’s installation guide - Installing Knative.\n  If you are upgrading from Knative 0.3.x: Update your domain and static IP address to be associated with the LoadBalancer istio-ingressgateway instead of knative-ingressgateway. Then run the following to clean up leftover resources:\nkubectl delete svc knative-ingressgateway -n istio-system kubectl delete deploy knative-ingressgateway -n istio-system If you have the Knative Eventing Sources component installed, you will also need to delete the following resource before upgrading:\nkubectl delete statefulset/controller-manager -n knative-sources While the deletion of this resource during the upgrade process will not prevent modifications to Eventing Source resources, those changes will not be completed until the upgrade process finishes.\n  To install Knative, first install the CRDs by running the kubectl apply command once with the -l knative.dev/crd-install=true flag. This prevents race conditions during the install, which cause intermittent errors:\nkubectl apply --selector knative.dev/crd-install=true \\ --filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml \\ --filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml \\ --filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml   To complete the install of Knative and its dependencies, run the kubectl apply command again, this time without the --selector flag, to complete the install of Knative and its dependencies:\nkubectl apply --filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml \\ --filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml \\ --filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml   Monitor the Knative components until all of the components show a STATUS of Running:\nkubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing kubectl get pods --namespace knative-monitoring   Set your custom domain  Fetch the external IP or CNAME of the knative-ingressgateway  kubectl --namespace istio-system get service knative-ingressgateway NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE knative-ingressgateway LoadBalancer 100.70.219.81 35.233.41.212 80:32380/TCP,443:32390/TCP,32400:32400/TCP 4d Create a wildcard DNS entry in your custom domain to point to above IP or CNAME  *.knative.\u003cmy domain\u003e == A 35.233.41.212 # or CNAME if you are on AWS *.knative.\u003cmy domain\u003e == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Adapt your knative config-domain (set your domain in the data field)  kubectl --namespace knative-serving get configmaps config-domain --output yaml apiVersion: v1 data: knative.\u003cmy domain\u003e: \"\" kind: ConfigMap name: config-domain namespace: knative-serving What’s next Now that your cluster has Knative installed, you can see what Knative has to offer.\nTo deploy your first app with the Getting Started with Knative App Deployment guide.\nGet started with Knative Eventing by walking through one of the Eventing Samples.\nInstall Cert-Manager if you want to use the automatic TLS cert provisioning feature.\nCleaning up Use the Gardener dashboard to delete your cluster, or execute the following with kubectl pointing to your garden-my-project.yaml kubeconfig:\nkubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project annotate shoot my-cluster confirmation.gardener.cloud/deletion=true kubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project delete shoot my-cluster ","categories":"","description":"A walkthrough the steps for installing Knative in Gardener shoot clusters.","excerpt":"A walkthrough the steps for installing Knative in Gardener shoot …","ref":"/docs/tutorials/knative-install/","tags":"","title":"Install Knative in Gardener Clusters"},{"body":"Gardener Networking Policy Filter for Shoots Introduction Gardener allows shoot clusters to filter egress traffic on node level. To support this the Gardener must be installed with the shoot-networking-filter extension.\nConfiguration To generally enable the networking filter for shoot objects the shoot-networking-filter extension must be registered by providing an appropriate extension registration in the garden cluster.\nHere it is possible to decide whether the extension should be always available for all shoots or whether the extension must be separately enabled per shoot.\nIf the extension should be used for all shoots the globallyEnabled flag should be set to true.\napiVersion: core.gardener.cloud/v1beta1 kind: ControllerRegistration ... spec:  resources:  - kind: Extension  type: shoot-networking-filter  globallyEnabled: true ControllerRegistration An example of a ControllerRegistration for the shoot-networking-filter can be found here: https://github.com/gardener/gardener-extension-shoot-networking-filter/blob/master/example/controller-registration.yaml\nThe ControllerRegistration contains a Helm chart which eventually deploys the shoot-networking-filter to seed clusters. It offers some configuration options, mainly to set up a static filter list or provide the configuration for downloading the filter list from a service endpoint.\napiVersion: core.gardener.cloud/v1beta1 kind: ControllerDeployment ...  values:  egressFilter:  blackholingEnabled: true   filterListProviderType: static  staticFilterList:  - network: 1.2.3.4/31  policy: BLOCK_ACCESS  - network: 5.6.7.8/32  policy: BLOCK_ACCESS  - network: ::2/128  policy: BLOCK_ACCESS   #filterListProviderType: download  #downloaderConfig:  # endpoint: https://my.filter.list.server/lists/policy  # oauth2Endpoint: https://my.auth.server/oauth2/token  # refreshPeriod: 1h   ## if the downloader needs an OAuth2 access token, client credentials can be provided with oauth2Secret  #oauth2Secret:  # clientID: 1-2-3-4  # clientSecret: secret!!  ## either clientSecret of client certificate is required  # client.crt.pem: |  # -----BEGIN CERTIFICATE-----  # ...  # -----END CERTIFICATE-----  # client.key.pem: |  # -----BEGIN PRIVATE KEY-----  # ...  # -----END PRIVATE KEY----- Enablement for a Shoot If the shoot networking filter is not globally enabled by default (depends on the extension registration on the garden cluster), it can be enabled per shoot. To enable the service for a shoot, the shoot manifest must explicitly add the shoot-networking-filter extension.\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot ... spec:  extensions:  - type: shoot-networking-filter ... If the shoot networking filter is globally enabled by default, it can be disabled per shoot. To disable the service for a shoot, the shoot manifest must explicitly state it.\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot ... spec:  extensions:  - type: shoot-networking-filter  disabled: true ... ","categories":"","description":"","excerpt":"Gardener Networking Policy Filter for Shoots Introduction Gardener …","ref":"/docs/extensions/others/gardener-extension-shoot-networking-filter/docs/installation/installation/","tags":"","title":"Installation"},{"body":"Gardener OIDC Service for Shoots Introduction Gardener allows Shoot clusters to dynamically register OpenID Connect providers. To support this the Gardener must be installed with the shoot-oidc-service extension.\nConfiguration To generally enable the OIDC service for shoot objects the shoot-oidc-service extension must be registered by providing an appropriate extension registration in the garden cluster.\nHere it is possible to decide whether the extension should be always available for all shoots or whether the extension must be separately enabled per shoot.\nIf the extension should be used for all shoots the globallyEnabled flag should be set to true.\nspec:  resources:  - kind: Extension  type: shoot-oidc-service  globallyEnabled: true Shoot Feature Gate If the shoot OIDC service is not globally enabled by default (depends on the extension registration on the garden cluster), it can be enabled per shoot. To enable the service for a shoot, the shoot manifest must explicitly add the shoot-oidc-service extension.\n... spec:  extensions:  - type: shoot-oidc-service ... If the shoot OIDC service is globally enabled by default, it can be disabled per shoot. To disable the service for a shoot, the shoot manifest must explicitly state it.\n... spec:  extensions:  - type: shoot-oidc-service  disabled: true ... ","categories":"","description":"","excerpt":"Gardener OIDC Service for Shoots Introduction Gardener allows Shoot …","ref":"/docs/extensions/others/gardener-extension-shoot-oidc-service/docs/installation/installation/","tags":"","title":"Installation"},{"body":"Integration tests Usage General setup \u0026 configurations Integration tests for machine-controller-manager-provider-{provider-name} can be executed manually by following below steps.\n Clone the repository machine-controller-manager-provider-{provider-name} on the local system. Navigate to machine-controller-manager-provider-{provider-name} directory and create a dev sub-directory in it. Copy the kubeconfig of Control Cluster from into dev/control-kubeconfig.yaml. (optional) Copy the kubeconfig of Target Cluster into dev/target-kubeconfig.yaml and update the Makefile variable TARGET_KUBECONFIG to point to dev/target-kubeconfig.yaml. If the tags on instances \u0026 associated resources on the provider are of String type (for example, GCP tags on its instances are of type String and not key-value pair) then add TAGS_ARE_STRINGS := true in the Makefile and export it. Atleast, one of the two controllers’ container images must be set in the Makefile variables MCM_IMAGE_TAG and MC_IMAGE_TAG for the controllers to run in the Control Cluster . These images will be used along with kubernetes/deployment.yaml to deploy/update controllers in the Control Cluster . If the intention is to run the controllers locally then unset the variables MCM_IMAGE_TAG and MC_IMAGE_TAG and set variable MACHINE_CONTROLLER_MANAGER_DEPLOYMENT_NAME := machine-controller-manager in the Makefile. In order to apply the CRDs when the Control Cluster is a Gardener Shoot or if none of the controller images are specified, machine-controller-manager repository will be cloned automatically. Incase, this repository already exists in local system, then create a softlink as below which helps to test changes in machine-controller-manager quickly. ln -sf \u003cpath-for-machine-controller-manager-repo\u003e dev/mcm   Scenario based additional configurations Gardener Shoot as the Control Cluster If the Control Cluster is a Gardener Shoot cluster then,\n Deploy a Secret named test-mc-secret (that contains the provider secret and cloud-config) in the default namespace of the Control Cluster. Refer these MachineClass templates for the same. Create a dev/machineclassv1.yaml file in the cloned repository. The name of the MachineClass itself should be test-mc-v1. The value of providerSpec.secretRef.name should be test-mc-secret. (Optional) Create an additional dev/machineclassv2.yaml file similar to above but with a bigger machine type and update the Makefile variable MACHINECLASS_V2 to point to dev/machineclassv2.yaml.  Gardener Seed as the Control Cluster If the Control Cluster is a Gardener SEED cluster then, the suite ideally employs the already existing MachineClass and Secrets. However,\n (Optional) User can employ a custom MachineClass for the tests using below steps:  Deploy a Secret named test-mc-secret (that contains the provider secret and cloud-config) in the shoot namespace of the Control Cluster. That is, the value of metadata.namespace should be technicalID of the Shoot and it will be of the pattern shoot--\u003cproject\u003e--\u003cshoot-name\u003e. Refer these MachineClass templates for the same. Create a dev/machineclassv1.yaml file.  providerSpec.secretRef.name should refer the secret created in the previous step. metadata.namespace and providerSpec.secretRef.namespace should be technicalID (shoot--\u003cproject\u003e--\u003cshoot-name\u003e) of the shoot. The name of the MachineClass itself should be test-mc-v1.      Running the tests  There is a rule test-integration in the Makefile, which can be used to start the integration test: $ make test-integration Starting integration tests... Running Suite: Controller Suite ===============================  The controllers log files (mcm_process.log and mc_process.log) are stored in .ci/controllers-test/logs repo and can be used later.  Adding Integration Tests for new providers For a new provider, Running Integration tests works with no changes. But for the orphan resource test cases to work correctly, the provider-specific API calls and the Resource Tracker Interface (RTI) should be implemented. Please check machine-controller-manager-provider-aws for reference.\nExtending integration tests  Update ControllerTests to be extend the testcases for all providers. Common testcases for machine|machineDeployment creation|deletion|scaling are packaged into ControllerTests. To extend the provider specfic test cases, the changes should be done in the machine-controller-manager-provider-{provider-name} repository. For example, to extended the testcases for machine-controller-manager-provider-aws, make changes to test/integration/controller/controller_test.go inside the machine-controller-manager-provider-aws repository. commons contains the Cluster and Clientset objects that makes it easy to extend the tests.  ","categories":"","description":"","excerpt":"Integration tests Usage General setup \u0026 configurations Integration …","ref":"/docs/other-components/machine-controller-manager/docs/development/integration_tests/","tags":"","title":"Integration Tests"},{"body":"Introduction When transferring data among networked systems, trust is a central concern. In particular, when communicating over an untrusted medium such as the internet, it is critical to ensure the integrity and immutability of all the data a system operates on. Especially if you use Docker Engine to push and pull images (data) to a public registry.\nThis immutability offers me a guarantee that any and all containers that I instantiate will be absolutely identical at inception. Surprise surprise, deterministic operations.\nA Lesson in Deterministic Ops Docker Tags are about as reliable and disposable as this guy down here.\nSeems simple enough. You have probably already deployed hundreds of YAML’s or started endless count of Docker container.\ndocker run --name mynginx1 -P -d nginx:1.13.9 or\napiVersion: apps/v1 kind: Deployment metadata:  name: rss-site spec:  replicas: 1  selector:  matchLabels:  app: web  template:  metadata:  labels:  app: web  spec:  containers:  - name: front-end  image: nginx:1.13.9  ports:  - containerPort: 80 But Tags are mutable and humans are prone to error. Not a good combination. Here we’ll dig into why the use of tags can be dangerous and how to deploy your containers across a pipeline and across environments, you guessed it, with determinism in mind.\nI want to ensure that whether it’s today or 5 years from now, that specific deployment uses the very same image that I defined. Any updates or newer versions of an image should be executed as a new deployment. The solution: digest\nA digest takes the place of the tag when pulling an image, for example, to pull the above image by digest, run the following command:\ndocker run --name mynginx1 -P -d nginx@sha256:4771d09578c7c6a65299e110b3ee1c0a2592f5ea2618d23e4ffe7a4cab1ce5de You can now make sure that the same image is always loaded at every deployment. It doesn’t matter if the TAG of the image has been changed or not. This solves the problem of repeatability.\nContent Trust However, there’s an additionally hidden danger. It is possible for an attacker to replace a server image with another one infected with malware.\nDocker Content trust gives you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel.\nPrior to version 1.8, Docker didn’t have a way to verify the authenticity of a server image. But in v1.8, a new feature called Docker Content Trust was introduced to automatically sign and verify the signature of a publisher.\nSo, as soon as a server image is downloaded, it is cross-checked with the signature of the publisher to see if someone tampered with it in any way. This solves the problem of trust.\nIn addition you should scan all images for known vulnerabilities, this can fill another book\n","categories":"","description":"Ensure that you get always the right image","excerpt":"Ensure that you get always the right image","ref":"/docs/guides/applications/content_trust/","tags":"","title":"Integrity and Immutability"},{"body":"Istio Istio offers a service mesh implementation with focus on several important features - traffic, observability, security and policy.\nGardener ManagedIstio feature gate When enabled in gardenlet the ManagedIstio feature gate can be used to deploy a Gardener-tailored Istio installation in Seed clusters. It’s main usage is to enable features such as Shoot API server SNI. This feature should not be enabled on a Seed cluster where Istio is already deployed.\nHowever, this feature gate is deprecated, turned on by default and will be removed in a future version of Gardener. This means that Gardener will unconditionally deploy Istio with its desired configuration to seed clusters. Consequently, existing/bring-your-own Istio deployments will no longer be supported.\nPrerequisites  Third-party JWT is used, therefore each Seed cluster where this feature is enabled must have Service Account Token Volume Projection enabled. Kubernetes 1.16+  Differences with Istio’s default profile The default profile which is recommended for production deployment, is not suitable for the Gardener use case as it offers more functionality than desired. The current installation goes through heavy refactorings due to the IstioOperator and the mixture of Helm values + Kubernetes API specification makes configuring and fine-tuning it very hard. A more simplistic deployment is used by Gardener. The differences are the following:\n Telemetry is not deployed. istiod is deployed. istio-ingress-gateway is deployed in a separate istio-ingress namespace. istio-egress-gateway is not deployed. None of the Istio addons are deployed. Mixer (deprecated) is not deployed Mixer CDRs are not deployed. Kubernetes Service, Istio’s VirtualService and ServiceEntry are NOT advertised in the service mesh. This means that if a Service needs to be accessed directly from the Istio Ingress Gateway, it should have networking.istio.io/exportTo: \"*\" annotation. VirtualService and ServiceEntry must have .spec.exportTo: [\"*\"] set on them respectively. Istio injector is not enabled. mTLS is enabled by default.  ","categories":"","description":"","excerpt":"Istio Istio offers a service mesh implementation with focus on several …","ref":"/docs/gardener/usage/istio/","tags":"","title":"Istio"},{"body":"Use the Kubernetes command-line tool, kubectl, to deploy and manage applications on Kubernetes. Using kubectl, you can inspect cluster resources; create, delete, and update components\nBy default, the kubectl configuration is located at ~/.kube/config.\nSuppose you have two clusters, one for development work and one for scratch work.\nHow to handle this easily without copying the used configuration always to the right place?\nExport the KUBECONFIG enviroment variable bash$ export KUBECONFIG=\u003cPATH-TO-M\u003e-CONFIG\u003e/kubeconfig-dev.yaml How to determine which cluster is used by the kubectl command?\nDetermine active cluster bash$ kubectl cluster-info Kubernetes master is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com KubeDNS is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com/api/v1/proxy/namespaces/kube-system/services/kube-dns  To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. bash$ Display cluster in the bash - Linux and alike I found this tip on Stackoverflow and find it worth to be added here. Edit your ~/.bash_profile and add the following code snippet to show the current k8s context in the shell’s prompt.\nprompt_k8s(){  k8s_current_context=$(kubectl config current-context 2\u003e /dev/null)  if [[ $? -eq 0 ]] ; then echo -e \"(${k8s_current_context}) \"; fi }   PS1+='$(prompt_k8s)' After this your bash command prompt contains the active KUBECONFIG context and you always know which cluster is active - develop or production.\ne.g.\nbash$ export KUBECONFIG=/Users/d023280/Documents/workspace/gardener-ui/kubeconfig_gardendev.yaml bash (garden_dev)$ Note the (garden_dev) prefix in the bash command prompt.\nThis helps immensely to avoid thoughtless mistakes.\nDisplay cluster in the PowerShell - Windows Display current k8s cluster in the title of PowerShell window.\nCreate a profile file for your shell under %UserProfile%\\Documents\\Windows­PowerShell\\Microsoft.PowerShell_profile.ps1\nCopy following code to Microsoft.PowerShell_profile.ps1\n function prompt_k8s {  $k8s_current_context = (kubectl config current-context) | Out-String  if($?) {  return $k8s_current_context  }else {  return \"No K8S contenxt found\"  }  }   $host.ui.rawui.WindowTitle = prompt_k8s If you want to switch to different cluster, you can set KUBECONFIG to new value, and re-run the file Microsoft.PowerShell_profile.ps1\n","categories":"","description":"Expose the active kubeconfig into bash","excerpt":"Expose the active kubeconfig into bash","ref":"/docs/guides/client_tools/bash_kubeconfig/","tags":"","title":"Kubeconfig Context as bash Prompt"},{"body":"Deploying the Machine Controller Manager into a Kubernetes cluster  Deploying the Machine Controller Manager into a Kubernetes cluster  Prepare the cluster Build the Docker image Configuring optional parameters while deploying Usage    As already mentioned, the Machine Controller Manager is designed to run as controller in a Kubernetes cluster. The existing source code can be compiled and tested on a local machine as described in Setting up a local development environment. You can deploy the Machine Controller Manager using the steps described below.\nPrepare the cluster  Connect to the remote kubernetes cluster where you plan to deploy the Machine Controller Manager using the kubectl. Set the environment variable KUBECONFIG to the path of the yaml file containing the cluster info. Now, create the required CRDs on the remote cluster using the following command,  $ kubectl apply -f kubernetes/crds.yaml Build the Docker image  ⚠️ Modify the Makefile to refer to your own registry.\n  Run the build which generates the binary to bin/machine-controller-manager  $ make build  Build docker image from latest compiled binary  $ make docker-image  Push the last created docker image onto the online docker registry.  $ make push  Now you can deploy this docker image to your cluster. A sample development file is given at. By default, the deployment manages the cluster it is running in. Optionally, the kubeconfig could also be passed as a flag as described in /kubernetes/deployment/in-tree/deployment.yaml. This is done when you want your controller running outside the cluster to be managed from.  $ kubectl apply -f kubernetes/deployment/in-tree/deployment.yaml  Also deploy the required clusterRole and clusterRoleBindings  $ kubectl apply -f kubernetes/deployment/in-tree/clusterrole.yaml $ kubectl apply -f kubernetes/deployment/in-tree/clusterrolebinding.yaml Configuring optional parameters while deploying Machine-controller-manager supports several configurable parameters while deploying. Refer to the following lines, to know how each parameter can be configured, and what it’s purpose is for.\nUsage To start using Machine Controller Manager, follow the links given at usage here.\n","categories":"","description":"","excerpt":"Deploying the Machine Controller Manager into a Kubernetes cluster …","ref":"/docs/other-components/machine-controller-manager/docs/deployment/kubernetes/","tags":"","title":"Kubernetes"},{"body":"This HowTo covers common kubernetes antipatterns that we have seen over the past months.\nRunning as root user. Whenever possible, do not run containers as root user. One could be tempted to say that Kubernetes Pods and Node are well separated. Host and containers running on it share the same kernel. If a container is compromised, the root user in the container has full control over the underlying node.\nWatch the very good presentation by Liz Rice at the KubeCon 2018\n Use RUN groupadd -r anygroup \u0026\u0026 useradd -r -g anygroup myuser to create a group and add a user to it. Use the USER command to switch to this user. Note that you may also consider to provide an explicit UID/GID if required.\nFor Example:\nARG GF_UID=\"500\" ARG GF_GID=\"500\" # add group \u0026 user RUN groupadd -r -g $GF_GID appgroup \u0026\u0026 \\ useradd appuser -r -u $GF_UID -g appgroup USER appuser Store data or logs in containers Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the container, as they are lost when the container is closed. Use persistence volumes instead to persist data outside of containers. Using an ELK stack is another good option for storing and processing logs.\nUsing pod IP addresses Each pod is assigned an IP address. It is necessary for pods to communicate with each other to build an application, e.g. an application must communicate with a database. Existing pods are terminated and new pods are constantly started. If you would rely on the IP address of a pod or container, you would need to update the application configuration constantly. This makes the application fragile. Create services instead. They provide a logical name that can be assigned independently of the varying number and IP addresses of containers. Services are the basic concept for load balancing within Kubernetes.\nMore than one process in a container A docker file provides a CMD and ENTRYPOINT to start the image. CMD is often used around a script that makes a configuration and then starts the container. Do not try to start multiple processes with this script. It is important to consider the separation of concerns when creating docker images. Running multiple processes in a single pod makes managing your containers, collecting logs and updating each process more difficult. You can split the image into multiple containers and manage them independently - even in one pod. Bear in mind that Kubernetes only monitors the process with PID=1. If more than one process is started within a container, then these no longer fall under the control of Kubernetes.\nCreating images in a running container A new image can be created with the docker commit command. This is useful if changes have been made to the container and you want to persist them for later error analysis. However, images created like this are not reproducible and completely worthless for a CI/CD environment. Furthermore, another developer cannot recognize which components the image contains. Instead, always make changes to the docker file, close existing containers and start a new container with the updated image.\nSaving passwords in docker image 💀 Do not save passwords in a Docker file. They are in plain text and are checked into a repository. That makes them completely vulnerable even if you are using a private repository like the Artifactory. Always use Secrets or ConfigMaps to provision passwords or inject them by mounting a persistent volume.\nUsing the ’latest’ tag Starting an image with tomcat is tempting. If no tags are specified, a container is started with the tomcat:latest image. This image may no longer be up to date and refers to an older version instead. Running a production application requires complete control of the environment with exact versions of the image. Make sure you always use a tag or even better the sha256 hash of the image e.g. tomcat@sha256:c34ce3c1fcc0c7431e1392cc3abd0dfe2192ffea1898d5250f199d3ac8d8720f. Why use the sha256 hash? Tags are not immutable and can be overwritten by a developer at any time. In this case you don’t have complete control over your image - which is bad.\nDifferent images per environment Don’t create different images for development, testing, staging and production environments. The image should be the source of truth and should only be created once and pushed to the repository. This image:tag should be used for different environments in the future.\nDepend on start order of pods Applications often depend on containers being started in a certain order. For example, a database container must be up and running before an application can connect to it. The application should be resilient to such changes, as the db pod can be unreachable or restarted at any time. The application container should be able to handle such situations without terminating or crashing.\nAdditional anti-patterns and patterns… In the community vast experience have been collected to improve stability and usability of Docker and Kubernetes. Refer to the following link for more information\n Kubernetes Production Patterns  ","categories":"","description":"Common Antipatterns for Kubernetes and Docker","excerpt":"Common Antipatterns for Kubernetes and Docker","ref":"/docs/guides/applications/antipattern/","tags":"","title":"Kubernetes Antipatterns"},{"body":"Kubernetes Clients in Gardener This document aims at providing a general developer guideline on different aspects of using Kubernetes clients in a large-scale distributed system and project like Gardener. The points included here are not meant to be consulted as absolute rules, but rather as general rules of thumb, that allow developers to get a better feeling about certain gotchas and caveats. It should be updated with lessons learned from maintaining the project and running Gardener in production.\nPrerequisites:\nPlease familiarize yourself with the following basic Kubernetes API concepts first, if you’re new to Kubernetes. A good understanding of these basics will help you better comprehend the following document.\n Kubernetes API Concepts (including terminology, watch basics, etc.) Extending the Kubernetes API (including Custom Resources and aggregation layer / extension API servers) Extend the Kubernetes API with CustomResourceDefinitions Working with Kubernetes Objects Sample Controller (the diagram helps to build an understanding of an controller’s basic structure)  Client Types: Client-Go, Generated, Controller-Runtime For historical reasons, you will find different kinds of Kubernetes clients in Gardener:\nClient-Go Clients client-go is the default/official client for talking to the Kubernetes API in Golang. It features so called “client sets” for all built-in Kubernetes API groups and versions (e.g. v1 (aka core/v1), apps/v1, etc.). client-go clients are generated from the built-in API types using client-gen and are composed of interfaces for every known API GroupVersionKind. A typical client-go usage looks like this:\nvar (  ctx context.Context  c kubernetes.Interface // \"k8s.io/client-go/kubernetes\"  deployment *appsv1.Deployment // \"k8s.io/api/apps/v1\" )  updatedDeployment, err := c.AppsV1().Deployments(\"default\").Update(ctx, deployment, metav1.UpdateOptions{}) Important characteristics of client-go clients:\n clients are specific to a given API GroupVersionKind, i.e., clients are hard-coded to corresponding API-paths (don’t need to use the discovery API to map GVK to a REST endpoint path). client’s don’t modify the passed in-memory object (e.g. deployment in the above example). Instead, they return a new in-memory object.\nThis means, controllers have to continue working with the new in-memory object or overwrite the shared object to not lose any state updates.  Generated Client Sets for Gardener APIs Gardener’s APIs extend the Kubernetes API by registering an extension API server (in the garden cluster) and CustomResourceDefinitions (on Seed clusters), meaning that the Kubernetes API will expose additional REST endpoints to manage Gardener resources in addition to the built-in API resources. In order to talk to these extended APIs in our controllers and components, client-gen is used to generate client-go-style clients to pkg/client/{core,extensions,seedmanagement,...}.\nUsage of these clients is equivalent to client-go clients, and the same characteristics apply. For example:\nvar (  ctx context.Context  c gardencoreclientset.Interface // \"github.com/gardener/gardener/pkg/client/core/clientset/versioned\"  shoot *gardencorev1beta1.Shoot // \"github.com/gardener/gardener/pkg/apis/core/v1beta1\" )  updatedShoot, err := c.CoreV1beta1().Shoots(\"garden-my-project\").Update(ctx, shoot, metav1.UpdateOptions{}) Controller-Runtime Clients controller-runtime is a Kubernetes community project (kubebuilder subproject) for building controllers and operators for custom resources. Therefore, it features a generic client, that follows a different approach and does not rely on generated client sets. Instead, the client can be used for managing any Kubernetes resources (built-in or custom) homogeneously. For example:\nvar (  ctx context.Context  c client.Client // \"sigs.k8s.io/controller-runtime/pkg/client\"  deployment *appsv1.Deployment // \"k8s.io/api/apps/v1\"  shoot *gardencorev1beta1.Shoot // \"github.com/gardener/gardener/pkg/apis/core/v1beta1\" )  err := c.Update(ctx, deployment) // or err = c.Update(ctx, shoot) A brief introduction to controller-runtime and its basic constructs can be found here.\nImportant characteristics of controller-runtime clients:\n The client functions take a generic client.Object or client.ObjectList value. These interfaces are implemented by all Golang types, that represent Kubernetes API objects or lists respectively which can be interacted with via usual API requests. [1] The client first consults a runtime.Scheme (configured during client creation) for recognizing the object’s GroupVersionKind (this happens on the client-side only).\nA runtime.Scheme is basically a registry for Golang API types, defaulting and conversion functions. Schemes are usually provided per GroupVersion (see this example for apps/v1) and can be combined to one single scheme for further usage (example). In controller-runtime clients, schemes are used only for mapping a typed API object to its GroupVersionKind. It then consults a meta.RESTMapper (also configured during client creation) for mapping the GroupVersionKind to a RESTMapping, which contains the GroupVersionResource and Scope (namespaced or cluster-scoped). From these values, the client can unambiguously determine the REST endpoint path of the corresponding API resource. For instance: appsv1.DeploymentList is available at /apis/apps/v1/deployments or /apis/apps/v1/namespaces/\u003cnamespace\u003e/deployments respectively.  There are different RESTMapper implementations, but generally they are talking to the API server’s discovery API for retrieving RESTMappings for all API resources known to the API server (either built-in, registered via API extension or CustomResourceDefinitions). The default implementation of controller-runtime (which Gardener uses as well), is the dynamic RESTMapper. It caches discovery results (i.e. RESTMappings) in-memory and only re-discovers resources from the API server, when a client tries to use an unknown GroupVersionKind, i.e., when it encounters a No{Kind,Resource}MatchError.   The client writes back results from the API server into the passed in-memory object.  This means, that controllers don’t have to worry about copying back the results and should just continue to work on the given in-memory object. This is a nice and flexible pattern and helper functions should try to follow it wherever applicable. Meaning, if possible accept an object param, pass it down to clients and keep working on the same in-memory object instead of creating a new one in your helper function. The benefit is, that you don’t lose updates to the API object and always have the last-known state in memory. Therefore, you don’t have to read it again, e.g., for getting the current resourceVersion when working with optimistic locking, and thus minimize the chances for running into conflicts. However, controllers must not use the same in-memory object concurrently in multiple goroutines. For example, decoding results from the API server in multiple goroutines into the same maps (e.g., labels, annotations) will cause panics because of “concurrent map writes”. Also, reading from an in-memory API object in one goroutine while decoding into it in another goroutine will yield non-atomic reads, meaning data might be corrupt and represent a non-valid/non-existing API object. Therefore, if you need to use the same in-memory object in multiple goroutines concurrently (e.g., shared state), remember to leverage proper synchronization techniques like channels, mutexes, atomic.Value and/or copy the object prior to use. The average controller however, will not need to share in-memory API objects between goroutines, and it’s typically an indicator that the controller’s design should be improved.   The client decoder erases the object’s TypeMeta (apiVersion and kind fields) after retrieval from the API server, see kubernetes/kubernetes#80609, kubernetes-sigs/controller-runtime#1517. Unstructured and metadata-only requests objects are an exception to this because the contained TypeMeta is the only way to identify the object’s type. Because of this behavior, obj.GetObjectKind().GroupVersionKind() is likely to return an empty GroupVersionKind. I.e., you must not rely on TypeMeta being set or GetObjectKind() to return something usable.\nIf you need to identify an object’s GroupVersionKind, use a scheme and its ObjectKinds function instead (or the helper function apiutil.GVKForObject). This is not specific to controller-runtime clients and applies to client-go clients as well.  [1] Other lower level, config or internal API types (e.g., such as AdmissionReview) don’t implement client.Object. However, you also can’t interact with such objects via the Kubernetes API and thus also not via a client, so this can be disregarded at this point.\nMetadata-Only Clients Additionally, controller-runtime clients can be used to easily retrieve metadata-only objects or lists. This is useful for efficiently checking if at least one object of a given kind exists, or retrieving metadata of an object, if one is not interested in the rest (e.g., spec/status).\nThe Accept header sent to the API server then contains application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1, which makes the API server only return metadata of the retrieved object(s). This saves network traffic and cpu/memory load on the API server and client side. If the client fully lists all objects of a given kind including their spec/status, the resulting list can be quite large and easily exceed the controllers available memory. That’s why it’s important to carefully check, if a full list is actually needed or if metadata-only list can be used instead.\nFor example:\nvar (  ctx context.Context  c client.Client // \"sigs.k8s.io/controller-runtime/pkg/client\"  shootList = \u0026metav1.PartialObjectMetadataList{} // \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) shootList.SetGroupVersionKind(gardencorev1beta1.SchemeGroupVersion.WithKind(\"ShootList\"))  if err := c.List(ctx, shootList, client.InNamespace(\"garden-my-project\"), client.Limit(1)); err != nil {  return err }  if len(shootList.Items) \u003e 0 {  // project has at least one shoot } else {  // project doesn't have any shoots } Gardener’s Client Collection, ClientMaps The Gardener codebase has a collection of clients (kubernetes.Interface), which can return all the above mentioned client types. Additionally, it contains helpers for rendering and applying helm charts (ChartRender, ChartApplier) and retrieving the API server’s version (Version).\nClient sets are managed by so called ClientMaps, which are a form of registry for all client set for a given type of cluster, i.e., Garden, Seed and Shoot. ClientMaps manage the whole lifecycle of clients: they take care of creating them if they don’t exist already, running their caches, refreshing their cached server version and invalidating them when they are no longer needed.\nvar (  ctx context.Context  cm clientmap.ClientMap // \"github.com/gardener/gardener/pkg/client/kubernetes/clientmap\"  shoot *gardencorev1beta1.Shoot )  cs, err := cm.GetClient(ctx, keys.ForShoot(shoot)) // kubernetes.Interface if err != nil {  return err }  c := cs.Client() // client.Client The client collection mainly exist for historical reasons (there used to be a lot of code using the client-go style clients). However, Gardener is in the process of moving more towards controller-runtime and only using their clients, as they provide many benefits and are much easier to use. Also, gardener/gardener#4251 aims at refactoring our controller and admission components to native controller-runtime components.\n ⚠️ Please always prefer controller-runtime clients over other clients when writing new code or refactoring existing code.\n Cache Types: Informers, Listers, Controller-Runtime Caches Similar to the different types of client(set)s, there are also different kinds of Kubernetes client caches. However, all of them are based on the same concept: Informers. An Informer is a watch-based cache implementation, meaning it opens watch connections to the API server and continuously updates cached objects based on the received watch events (ADDED, MODIFIED, DELETED). Informers offer to add indices to the cache for efficient object lookup (e.g., by name or labels) and to add EventHandlers for the watch events. The latter is used by controllers to fill queues with objects that should be reconciled on watch events.\nInformers are used in and created via several higher-level constructs:\nSharedInformerFactories, Listers The generated clients (built-in as well as extended) feature a SharedInformerFactory for every API group, which can be used to create and retrieve Informers for all GroupVersionKinds. Similarly, it can be used to retrieve Listers, that allow getting and listing objects from the Informer’s cache. However, both of these constructs are only used for historical reasons, and we are in the process of migrating away from them in favor of cached controller-runtime clients (see gardener/gardener#2414, gardener/gardener#2822). Thus, they are described only briefly here.\nImportant characteristics of Listers:\n Objects read from Informers and Listers can always be slightly out-out-date (i.e., stale) because the client has to first observe changes to API objects via watch events (which can intermittently lag behind by a second or even more). Thus, don’t make any decisions based on data read from Listers if the consequences of deciding wrongfully based on stale state might be catastrophic (e.g. leaking infrastructure resources). In such cases, read directly from the API server via a client instead. Objects retrieved from Informers or Listers are pointers to the cached objects, so they must not be modified without copying them first, otherwise the objects in the cache are also modified.  Controller-Runtime Caches controller-runtime features a cache implementation that can be used equivalently as their clients. In fact, it implements a subset of the client.Client interface containing the Get and List functions. Under the hood, a cache.Cache dynamically creates Informers (i.e., opens watches) for every object GroupVersionKind that is being retrieved from it.\nNote, that the underlying Informers of a controller-runtime cache (cache.Cache) and the ones of a SharedInformerFactory (client-go) are not related in any way. Both create Informers and watch objects on the API server individually. This means, that if you read the same object from different cache implementations, you may receive different versions of the object because the watch connections of the individual Informers are not synced.\n ⚠️ Because of this, controllers/reconcilers should get the object from the same cache in the reconcile loop, where the EventHandler was also added to set up the controller. For example, if a SharedInformerFactory is used for setting up the controller then read the object in the reconciler from the Lister instead of from a cached controller-runtime client.\n By default, the client.Client created by a controller-runtime Manager is a DelegatingClient. It delegates Get and List calls to a Cache and all other calls to a client, that talks directly to the API server. Exceptions are requests with *unstructured.Unstructured objects and object kinds that were configured to be excluded from the cache in the DelegatingClient.\n ℹ️ kubernetes.Interface.Client() returns a DelegatingClient that uses the cache returned from kubernetes.Interface.Cache() under the hood. This means, all Client() usages need to be ready for cached clients and should be able to cater with stale cache reads.\n Important characteristics of cached controller-runtime clients:\n Like for Listers, objects read from a controller-runtime cache can always be slightly out of date. Hence, don’t base any important decisions on data read from the cache (see above). In contrast to Listers, controller-runtime caches fill the passed in-memory object with the state of the object in the cache (i.e., they perform something like a “deep copy into”). This means that objects read from a controller-runtime cache can safely be modified without unintended side effects. Reading from a controller-runtime cache or a cached controller-runtime client implicitly starts a watch for the given object kind under the hood. This has important consequences:  Reading a given object kind from the cache for the first time can take up to a few seconds depending on size and amount of objects as well as API server latency. This is because the cache has to do a full list operation and wait for an initial watch sync before returning results. ⚠️ Controllers need appropriate RBAC permissions for the object kinds they retrieve via cached clients (i.e., list and watch). ⚠️ By default, watches started by a controller-runtime cache are cluster-scoped, meaning it watches and caches objects across all namespaces. Thus, be careful which objects to read from the cache as it might significantly increase the controller’s memory footprint.   There is no interaction with the cache on writing calls (Create, Update, Patch and Delete), see below.  Uncached objects, filtered caches, APIReaders:\nIn order to allow more granular control over which object kinds should be cached and which calls should bypass the cache, controller-runtime offers a few mechanisms to further tweak the client/cache behavior:\n When creating a DelegatingClient, certain object kinds can be configured to always be read directly from the API instead of from the cache. Note that this does not prevent starting a new Informer when retrieving them directly from the cache. Watches can be restricted to a given (set of) namespace(s) by using cache.MultiNamespacedCacheBuilder or setting cache.Options.Namespace. Watches can be filtered (e.g., by label) per object kind by configuring cache.Options.SelectorsByObject on creation of the cache. Retrieving metadata-only objects or lists from a cache results in a metadata-only watch/cache for that object kind. The APIReader can be used to always talk directly to the API server for a given Get or List call (use with care and only as a last resort!).  To Cache or Not to Cache Although watch-based caches are an important factor for the immense scalability of Kubernetes, it definitely comes at a price (mainly in terms of memory consumption). Thus, developers need to be careful when introducing new API calls and caching new object kinds. Here are some general guidelines on choosing whether to read from a cache or not:\n Always try to use the cache wherever possible and make your controller able to tolerate stale reads.  Leverage optimistic locking: use deterministic naming for objects you create (this is what the Deployment controller does [2]). Leverage optimistic locking / concurrency control of the API server: send updates/patches with the last-known resourceVersion from the cache (see below). This will make the request fail, if there were concurrent updates to the object (conflict error), which indicates that we have operated on stale data and might have made wrong decisions. In this case, let the controller handle the error with exponential backoff. This will make the controller eventually consistent. Track the actions you took, e.g., when creating objects with generateName (this is what the ReplicaSet controller does [3]). The actions can be tracked in memory and repeated if the expected watch events don’t occur after a given amount of time. Always try to write controllers with the assumption that data will only be eventually correct and can be slightly out of date (even if read directly from the API server!). If there is already some other code that needs a cache (e.g., a controller watch), reuse it instead of doing extra direct reads. Don’t read an object again if you just sent a write request. Write requests (Create, Update, Patch and Delete) don’t interact with the cache. Hence, use the current state that the API server returned (filled into the passed in-memory object), which is basically a “free direct read”, instead of reading the object again from a cache, because this will probably set back the object to an older resourceVersion.   If you are concerned about the impact of the resulting cache, try to minimize that by using filtered or metadata-only watches. If watching and caching an object type is not feasible, for example because there will be a lot of updates, and you are only interested in the object every ~5m, or because it will blow up the controllers memory footprint, fallback to a direct read. This can either be done by disabling caching the object type generally or doing a single request via an APIReader. In any case, please bear in mind that every direct API call results in a quorum read from etcd, which can be costly in a heavily-utilized cluster and impose significant scalability limits. Thus, always try to minimize the impact of direct calls by filtering results by namespace or labels, limiting the number of results and/or using metadata-only calls.  [2] The Deployment controller uses the pattern \u003cdeployment-name\u003e-\u003cpodtemplate-hash\u003e for naming ReplicaSets. This means, the name of a ReplicaSet it tries to create/update/delete at any given time is deterministically calculated based on the Deployment object. By this, it is insusceptible to stale reads from its ReplicaSets cache.\n[3] In simple terms, the ReplicaSet controller tracks its CREATE pod actions as follows: when creating new Pods, it increases a counter of expected ADDED watch events for the corresponding ReplicaSet. As soon as such events arrive, it decreases the counter accordingly. It only creates new Pods for a given ReplicaSet, once all expected events occurred (counter is back to zero) or a timeout occurred. This way, it prevents creating more Pods than desired because of stale cache reads and makes the controller eventually consistent.\nConflicts, Concurrency Control and Optimistic Locking Every Kubernetes API object contains the metadata.resourceVersion field, which identifies an object’s version in the backing data store, i.e., etcd. Every write to an object in etcd results in a newer resourceVersion. This field is mainly used for concurrency control on the API server in an optimistic locking fashion, but also for efficient resumption of interrupted watch connections.\nOptimistic locking in the Kubernetes API sense means that when a client wants to update an API object then it includes the object’s resourceVersion in the request to indicate the object’s version the modifications are based on. If the resourceVersion in etcd has not changed in the meantime, the update request is accepted by the API server and the updated object is written to etcd. If the resourceVersion sent by the client does not match the one of the object stored in etcd, there were concurrent modifications to the object. Consequently, the request is rejected with a conflict error (status code 409, API reason Conflict), for example:\n{  \"kind\": \"Status\",  \"apiVersion\": \"v1\",  \"metadata\": {},  \"status\": \"Failure\",  \"message\": \"Operation cannot be fulfilled on configmaps \\\"foo\\\": the object has been modified; please apply your changes to the latest version and try again\",  \"reason\": \"Conflict\",  \"details\": {  \"name\": \"foo\",  \"kind\": \"configmaps\"  },  \"code\": 409 } This concurrency control is an important mechanism in Kubernetes as there are typically multiple clients acting on API objects at the same time (humans, different controllers, etc.). If a client receives a conflict error, it should read the object’s latest version from the API server, make the modifications based on the newest changes and retry the update. The reasoning behind this is that a client might choose to make different decisions based on the concurrent changes made by other actors compared to the outdated version that it operated on.\nImportant points about concurrency control and conflicts:\n The resourceVersion field carries a string value and clients must not assume numeric values (the type and structure of versions depend on the backing data store). This means clients may compare resourceVersion values to detect whether objects were changed. But they must not compare resourceVersions to figure out which one is newer/older, i.e., no greater/less-than comparisons are allowed. By default, update calls (e.g. via client-go and controller-runtime clients) use optimistic locking as the passed in-memory usually object contains the latest resourceVersion known to the controller which is then also sent to the API server. API servers can also choose to accept update calls without optimistic locking (i.e., without a resourceVersion in the object’s metadata) for any given resource. However, sending update requests without optimistic locking is strongly discouraged as doing so overwrites the entire object discarding any concurrent changes made to it. On the other side, patch requests can always be executed either with or without optimistic locking, by (not) including the resourceVersion in the patched object’s metadata. Sending patch requests without optimistic locking might be safe and even desirable as a patch typically updates only a specific section of the object. However, there are also situations where patching without optimistic locking is not safe (see below).  Don’t Retry on Conflict Similar to how a human would typically handle a conflict error, there are helper functions implementing RetryOnConflict-semantics, i.e., try an update call, then re-read the object if a conflict occurs, apply the modification again and retry the update. However, controllers should generally not use RetryOnConflict-semantics. Instead, controllers should abort their current reconciliation run and let the queue handle the conflict error with exponential backoff. The reasoning behind this is, that a conflict error indicates that the controller has operated on stale data and might have made wrong decisions earlier on in the reconciliation. When using a helper function that implements RetryOnConflict-semantics, the controller doesn’t check which fields were changed and doesn’t revise its previous decisions accordingly. Instead, retrying on conflict basically just ignores any conflict error and blindly applies the modification.\nTo properly solve the conflict situation, controllers should immediately return with the error from the update call. This will cause retries with exponential backoff so that the cache has a chance to observe the latest changes to the object. In a later run, the controller will then make correct decisions based on the newest version of the object, not run into conflict errors and will then be able to successfully reconcile the object. This way, the controller becomes eventually consistent.\nThe other way to solve the situation is to modify objects without optimistic locking in order to avoid running into a conflict in the first place (only if this is safe). This can be a preferable solution for controllers with long-running reconciliations (which is actually an anti-pattern but quite unavoidable in some of Gardener’s controllers). Aborting the entire reconciliation run is rather undesirable in such cases as it will add a lot of unnecessary waiting time for end users and overhead in terms of compute and network usage.\nHowever, in any case retrying on conflict is probably not the right option to solve the situation (there are some correct use cases for it, though, they are very rare). Hence, don’t retry on conflict.\nTo Lock or Not to Lock As explained before, conflicts are actually important and prevent clients from doing wrongful concurrent updates. This means, conflicts are not something we generally want to avoid or ignore. However, in many cases controllers are exclusive owners of the fields they want to update and thus it might be safe to run without optimistic locking.\nFor example, the gardenlet is the exclusive owner of the spec section of the Extension resources it creates on behalf of a Shoot (e.g., the Infrastructure resource for creating VPC, etc.). Meaning, it knows the exact desired state and no other actor is supposed to update the Infrastructure’s spec fields. When the gardenlet now updates the Infrastructures spec section as part of the Shoot reconciliation, it can simply issue a PATCH request that only updates the spec and runs without optimistic locking. If another controller concurrently updated the object in the meantime (e.g., the status section), the resourceVersion got changed which would cause a conflict error if running with optimistic locking. However, concurrent status updates would not change the gardenlet’s mind on the desired spec of the Infrastructure resource as it is determined only by looking at the Shoot’s specification. If the spec section was changed concurrently, it’s still fine to overwrite it because the gardenlet should reconcile the spec back to its desired state.\nGenerally speaking, if a controller is the exclusive owner of a given set of fields and they are independent of concurrent changes to other fields in that object, it can patch these fields without optimistic locking. This might ignore concurrent changes to other fields or blindly overwrite changes to the same fields, but this is fine if the mentioned conditions apply. Obviously, this applies only to patch requests that modify only a specific set of fields but not to update requests that replace the entire object.\nIn such cases, it’s even desirable to run without optimistic locking as it will be more performant and save retries. If certain requests are made with high frequency and have a good chance of causing conflicts, retries because of optimistic locking can cause a lot of additional network traffic in a large-scale Gardener installation.\nUpdates, Patches, Server-side Apply There are different ways of modifying Kubernetes API objects. The following snippet demonstrates how to do a given modification with the most frequently used options using a controller-runtime client:\nvar (  ctx context.Context  c client.Client  shoot *gardencorev1beta1.Shoot )  // update shoot.Spec.Kubernetes.Version = \"1.22\" err := c.Update(ctx, shoot)  // json merge patch patch := client.MergeFrom(shoot.DeepCopy()) shoot.Spec.Kubernetes.Version = \"1.22\" err = c.Patch(ctx, shoot, patch)  // strategic merge patch patch = client.StrategicMergeFrom(shoot.DeepCopy()) shoot.Spec.Kubernetes.Version = \"1.22\" err = c.Patch(ctx, shoot, patch) Important characteristics of the shown request types:\n Update requests always send the entire object to the API server and update all fields accordingly. By default, optimistic locking is used (resourceVersion is included). Both patch types run without optimistic locking by default. However, it can be enabled explicitly if needed: // json merge patch + optimistic locking patch := client.MergeFromWithOptions(shoot.DeepCopy(), client.MergeFromWithOptimisticLock{}) // ...  // strategic merge patch + optimistic locking patch = client.StrategicMergeFrom(shoot.DeepCopy(), client.MergeFromWithOptimisticLock{}) // ...  Patch requests only contain the changes made to the in-memory object between the copy passed to client.*MergeFrom and the object passed to Client.Patch(). The diff is calculated on the client-side based on the in-memory objects only. This means, if in the meantime some fields were changed on the API server to a different value than the one on the client-side, the fields will not be changed back as long as they are not changed on the client-side as well (there will be no diff in memory). Thus, if you want to ensure a given state using patch requests, always read the object first before patching it, as there will be no diff otherwise, meaning the patch will be empty. Also see gardener/gardener#4057 and comments in gardener/gardener#4027. Also, always send updates and patch requests even if your controller hasn’t made any changes to the current state on the API server. I.e., don’t make any optimization for preventing empty patches or no-op updates. There might be mutating webhooks in the system that will modify the object and that rely on update/patch requests being sent (even if they are no-op). Gardener’s extension concept makes heavy use of mutating webhooks, so it’s important to keep this in mind. JSON merge patches always replace lists as a whole and don’t merge them. Keep this in mind when operating on lists with merge patch requests. If the controller is the exclusive owner of the entire list, it’s safe to run without optimistic locking. Though, if you want to prevent overwriting concurrent changes to the list or its items made by other actors (e.g., additions/removals to the metadata.finalizers list), enable optimistic locking. Strategic merge patches are able to make more granular modifications to lists and their elements without replacing the entire list. It uses Golang struct tags of the API types to determine which and how lists should be merged. See this document or the strategic merge patch documentation for more in-depth explanations and comparison with JSON merge patches. With this, controllers might be able to issue patch requests for individual list items without optimistic locking, even if they are not exclusive owners of the entire list. Remember to check the patchStrategy and patchMergeKey struct tags of the fields you want to modify before blindly adding patch requests without optimistic locking. Strategic merge patches are only supported by built-in Kubernetes resources and custom resources served by Extension API servers. Strategic merge patches are not supported by custom resources defined by CustomResourceDefinitions (see this comparison). In that case, fallback to JSON merge patches. Server-side Apply is yet another mechanism to modify API objects, which is supported by all API resources (in newer Kubernetes versions). However, it has a few problems and more caveats preventing us from using it in Gardener at the time of writing. See gardener/gardener#4122 for more details.   Generally speaking, patches are often the better option compared to update requests because they can save network traffic, encoding/decoding effort and avoid conflicts under the presented conditions. If choosing a patch type, consider which type is supported by the resource you’re modifying and what will happen in case of a conflict. Consider whether your modification is safe to run without optimistic locking. However, there is no simple rule of thumb on which patch type to choose.\n On Helper Functions Here is a note on some helper functions, that should be avoided and why:\ncontrollerutil.CreateOrUpdate does a basic get, mutate and create or update call chain, which is often used in controllers. We should avoid using this helper function in Gardener, because it is likely to cause conflicts for cached clients and doesn’t send no-op requests if nothing was changed, which can cause problems because of the heavy use of webhooks in Gardener extensions (see above). That’s why usage of this function was completely replaced in gardener/gardener#4227 and similar PRs.\ncontrollerutil.CreateOrPatch is similar to CreateOrUpdate but does a patch request instead of an update request. It has the same drawback as CreateOrUpdate regarding no-op updates. Also, controllers can’t use optimistic locking or strategic merge patches when using CreateOrPatch. Another reason for avoiding use of this function is, that it also implicitly patches the status section if it was changed, which is confusing for others reading the code. To accomplish this, the func does some back and forth conversion, comparison and checks, which are unnecessary in most of our cases and simply wasted CPU cycles and complexity we want to avoid.\nThere were some Try{Update,UpdateStatus,Patch,PatchStatus} helper functions in Gardener that were already removed by gardener/gardener#4378 but are still used in some extension code at the time of writing. The reason for eliminating these functions is that they implement RetryOnConflict-semantics. Meaning, they first get the object, mutate it, then try to update and retry if a conflict error occurs. As explained above, retrying on conflict is a controller anti-pattern and should be avoided in almost every situation. The other problem with these functions is that they read the object first from the API server (always do a direct call), although in most cases we already have a recent version of the object at hand. So, using this function generally does unnecessary API calls and therefore causes unwanted compute and network load.\nFor the reasons explained above, there are similar helper functions that accomplish similar things but address the mentioned drawbacks: controllerutils.{GetAndCreateOrMergePatch,GetAndCreateOrStrategicMergePatch}. These can be safely used as replacements for the aforementioned helper funcs. If they are not fitting for your use case, for example because you need to use optimistic locking, just do the appropriate calls in the controller directly.\nFurther Resources  Kubernetes Client usage in Gardener (Community Meeting talk, 2020-06-26)  These resources are only partially related to the topics covered in this doc, but might still be interesting for developer seeking a deeper understanding of Kubernetes API machinery, architecture and foundational concepts.\n API Conventions The Kubernetes Resource Model  ","categories":"","description":"","excerpt":"Kubernetes Clients in Gardener This document aims at providing a …","ref":"/docs/gardener/development/kubernetes-clients/","tags":"","title":"Kubernetes Clients"},{"body":"Local development Purpose Develop new feature and fix bug on the Gardener Dashboard.\nRequirements  Yarn. For the required version, refer to .engines.yarn in package.json. Node.js. For the required version, refer to .engines.node in package.json.  Steps 1. Clone repository Clone the gardener/dashboard repository\ngit clone git@github.com:gardener/dashboard.git 2. Install dependencies Run yarn at the repository root to install all dependencies.\ncd dashboard yarn 3. Configuration Place the Gardener Dashboard configuration under ${HOME}/.gardener/config.yaml or alternatively set the path to the configuration file using the GARDENER_CONFIG environment variable.\nA local configuration example for minikube and dex could look like follows:\nport: 3030 logLevel: debug logFormat: text apiServerUrl: https://minkube # garden cluster kube-apiserver url sessionSecret: c2VjcmV0 # symetric key used for encryption oidc:  issuer: https://minikube:32001  client_id: dashboard  client_secret: c2VjcmV0 # oauth client secret  redirect_uri: http://localhost:8080/auth/callback  scope: 'openid email profile groups audience:server:client_id:dashboard audience:server:client_id:kube-kubectl'  clockTolerance: 15 frontend:  dashboardUrl:  pathname: /api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy/  defaultHibernationSchedule:  evaluation:  - start: 00 17 * * 1,2,3,4,5  development:  - start: 00 17 * * 1,2,3,4,5  end: 00 08 * * 1,2,3,4,5  production: ~ 5. Run it locally The Gardener Dashboard backend server requires a kubeconfig for the Garden Cluster. You can set it e.g. by using the KUBECONFIG environment variable.\nConcurrently run the backend server (port 3030) and the frontend server (port 8080) with hot reload enabled.\ncd backend export KUBECONFIG=/path/to/garden/cluster/kubeconfig.yaml yarn serve cd frontend yarn serve You can now access the UI on http://localhost:8080/\nBuild Build docker image locally.\nmake build Push Push docker image to Google Container Registry.\nmake push This command expects a valid gcloud configuration named gardener.\ngcloud config configurations describe gardener is_active: true name: gardener properties:  core:  account: john.doe@example.org  project: johndoe-1008 ","categories":"","description":"","excerpt":"Local development Purpose Develop new feature and fix bug on the …","ref":"/docs/dashboard/development/local-setup/","tags":"","title":"Local Setup"},{"body":"admission-alicloud admission-alicloud is an admission webhook server which is responsible for the validation of the cloud provider (Alicloud in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn’t be able to perform similar validation.\nFollow the steps below to run the admission webhook server locally.\n  Start the Gardener API server.\nFor details, check the Gardener local setup.\n  Start the webhook server\nMake sure that the KUBECONFIG environment variable is pointing to the local garden cluster.\nmake start-admission   Setup the ValidatingWebhookConfiguration.\nhack/dev-setup-admission-alicloud.sh will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the ValidatingWebhookConfiguration manifest.\n./hack/dev-setup-admission-alicloud.sh   You are now ready to experiment with the admission-alicloud webhook server locally.\n","categories":"","description":"","excerpt":"admission-alicloud admission-alicloud is an admission webhook server …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-alicloud/docs/local-setup/","tags":"","title":"Local Setup"},{"body":"admission-aws admission-aws is an admission webhook server which is responsible for the validation of the cloud provider (AWS in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn’t be able to perform similar validation.\nFollow the steps below to run the admission webhook server locally.\n  Start the Gardener API server.\nFor details, check the Gardener local setup.\n  Start the webhook server\nMake sure that the KUBECONFIG environment variable is pointing to the local garden cluster.\nmake start-admission   Setup the ValidatingWebhookConfiguration.\nhack/dev-setup-admission-aws.sh will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the ValidatingWebhookConfiguration manifest.\n./hack/dev-setup-admission-aws.sh   You are now ready to experiment with the admission-aws webhook server locally.\n","categories":"","description":"","excerpt":"admission-aws admission-aws is an admission webhook server which is …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/local-setup/","tags":"","title":"Local Setup"},{"body":"admission-azure admission-azure is an admission webhook server which is responsible for the validation of the cloud provider (Azure in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn’t be able to perform similar validation.\nFollow the steps below to run the admission webhook server locally.\n  Start the Gardener API server.\nFor details, check the Gardener local setup.\n  Start the webhook server\nMake sure that the KUBECONFIG environment variable is pointing to the local garden cluster.\nmake start-admission   Setup the ValidatingWebhookConfiguration.\nhack/dev-setup-admission-azure.sh will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the ValidatingWebhookConfiguration manifest.\n./hack/dev-setup-admission-azure.sh   You are now ready to experiment with the admission-azure webhook server locally.\n","categories":"","description":"","excerpt":"admission-azure admission-azure is an admission webhook server which …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-azure/docs/local-setup/","tags":"","title":"Local Setup"},{"body":"admission-gcp admission-gcp is an admission webhook server which is responsible for the validation of the cloud provider (GCP in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn’t be able to perform similar validation.\nFollow the steps below to run the admission webhook server locally.\n  Start the Gardener API server.\nFor details, check the Gardener local setup.\n  Start the webhook server\nMake sure that the KUBECONFIG environment variable is pointing to the local garden cluster.\nmake start-admission   Setup the ValidatingWebhookConfiguration.\nhack/dev-setup-admission-gcp.sh will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the ValidatingWebhookConfiguration manifest.\n./hack/dev-setup-admission-gcp.sh   You are now ready to experiment with the admission-gcp webhook server locally.\n","categories":"","description":"","excerpt":"admission-gcp admission-gcp is an admission webhook server which is …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-gcp/docs/local-setup/","tags":"","title":"Local Setup"},{"body":"admission-openstack admission-openstack is an admission webhook server which is responsible for the validation of the cloud provider (OpenStack in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn’t be able to perform similar validation.\nFollow the steps below to run the admission webhook server locally.\n  Start the Gardener API server.\nFor details, check the Gardener local setup.\n  Start the webhook server\nMake sure that the KUBECONFIG environment variable is pointing to the local garden cluster.\nmake start-admission   Setup the ValidatingWebhookConfiguration.\nhack/dev-setup-admission-openstack.sh will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the ValidatingWebhookConfiguration manifest.\n./hack/dev-setup-admission-openstack.sh   You are now ready to experiment with the admission-openstack webhook server locally.\n","categories":"","description":"","excerpt":"admission-openstack admission-openstack is an admission webhook server …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-openstack/docs/local-setup/","tags":"","title":"Local Setup"},{"body":"Deployment admission-vsphere admission-vsphere is an admission webhook server which is responsible for the validation of the cloud provider (vSphere in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn’t be able to perform similar validation.\nFollow the steps below to run the admission webhook server locally.\n  Start the Gardener API server.\nFor details, check the Gardener local setup.\n  Start the webhook server\nMake sure that the KUBECONFIG environment variable is pointing to the local garden cluster.\nmake start-admission   Setup the ValidatingWebhookConfiguration.\nhack/dev-setup-admission-vsphere.sh will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the ValidatingWebhookConfiguration manifest.\n./hack/dev-setup-admission-vsphere.sh   You are now ready to experiment with the admission-vsphere webhook server locally.\n","categories":"","description":"","excerpt":"Deployment admission-vsphere admission-vsphere is an admission webhook …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/docs/local-setup/","tags":"","title":"Local Setup"},{"body":"Overview Conceptually, all Gardener components are designed to run as a Pod inside a Kubernetes cluster. The Gardener API server extends the Kubernetes API via the user-aggregated API server concepts. However, if you want to develop it, you may want to work locally with the Gardener without building a Docker image and deploying it to a cluster each and every time. That means that the Gardener runs outside a Kubernetes cluster which requires providing a Kubeconfig in your local filesystem and point the Gardener to it when starting it (see below).\nFurther details can be found in\n Principles of Kubernetes, and its components Kubernetes Development Guide Architecture of Gardener  This guide is split into three main parts:\n Preparing your setup by installing all dependencies and tools Building and starting Gardener components locally Using your local Gardener setup to create a Shoot  Limitations of the local development setup You can run Gardener (API server, controller manager, scheduler, gardenlet) against any local Kubernetes cluster, however, your seed and shoot clusters must be deployed to a cloud provider. Currently, it is not possible to run Gardener entirely isolated from any cloud provider. This means that to be able create Shoot clusters you need to register an external Seed cluster (e.g., one created in AWS).\nPreparing the Setup [macOS only] Installing homebrew The copy-paste instructions in this guide are designed for macOS and use the package manager Homebrew.\nOn macOS run\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" Installing git We use git as VCS which you need to install. On macOS run\nbrew install git For other OS, please check the Git installation documentation.\nInstalling Go Install the latest version of Go. On macOS run\nbrew install go For other OS, please check Go installation documentation.\nInstalling kubectl Install kubectl. Please make sure that the version of kubectl is at least v1.11.x. On macOS run\nbrew install kubernetes-cli For other OS, please check the kubectl installation documentation.\nInstalling helm You also need the Helm CLI. On macOS run\nbrew install helm For other OS please check the Helm installation documentation.\nInstalling Docker You need to have docker installed and running. On macOS run\nbrew install --cask docker For other OS please check the docker installation documentation.\nInstalling iproute2 iproute2 provides a collection of utilities for network administration and configuration. On macOS run\nbrew install iproute2mac Installing jq brew install jq Installing GNU Parallel GNU Parallel is a shell tool for executing jobs in parallel, used by the code generation scripts (make generate). On macOS run\nbrew install parallel [macOS only] Install GNU core utilities When running on macOS, install the GNU core utilities and friends:\nbrew install coreutils gnu-sed gnu-tar grep This will create symbolic links for the GNU utilities with g prefix in /usr/local/bin, e.g., gsed or gbase64. To allow using them without the g prefix please put /usr/local/opt/coreutils/libexec/gnubin etc. at the beginning of your PATH environment variable, e.g., export PATH=/usr/local/opt/coreutils/libexec/gnubin:$PATH (brew will print out instructions for each installed formula).\nexport PATH=/usr/local/opt/coreutils/libexec/gnubin:$PATH export PATH=/usr/local/opt/gnu-sed/libexec/gnubin:$PATH export PATH=/usr/local/opt/gnu-tar/libexec/gnubin:$PATH export PATH=/usr/local/opt/grep/libexec/gnubin:$PATH [Windows only] WSL2 Apart from Linux distributions and macOS, the local gardener setup can also run on the Windows Subsystem for Linux 2.\nWhile WSL1, plain docker for windows and various Linux distributions and local Kubernetes environments may be supported, this setup was verified with:\n WSL2 Docker Desktop WSL2 Engine Ubuntu 18.04 LTS on WSL2 Nodeless local garden (see below)  The Gardener repository and all the above-mentioned tools (git, golang, kubectl, …) should be installed in your WSL2 distro, according to the distribution-specific Linux installation instructions.\nStart Gardener locally Get the sources Clone the repository from GitHub into your $GOPATH.\nmkdir -p $GOPATH/src/github.com/gardener cd $GOPATH/src/github.com/gardener git clone git@github.com:gardener/gardener.git cd gardener  Note: Gardener is using Go modules and cloning the repository into $GOPATH is not a hard requirement. However it is still recommended to clone into $GOPATH because k8s.io/code-generator does not work yet outside of $GOPATH - kubernetes/kubernetes#86753.\n Start the Gardener ℹ️ In the following guide, you have to define the configuration (CloudProfiles, SecretBindings, Seeds, etc.) manually for the infrastructure environment you want to develop against. Additionally, you have to register the respective Gardener extensions manually. If you are rather looking for a quick start guide to develop entirely locally on your machine (no real cloud provider or infrastructure involved) then you should rather follow this guide.\nStart a local kubernetes cluster For the development of Gardener you need a Kubernetes API server on which you can register Gardener’s own Extension API Server as APIService. This cluster doesn’t need any worker nodes to run pods, though, therefore, you can use the “nodeless Garden cluster setup” residing in hack/local-garden. This will start all minimally required components of a Kubernetes cluster (etcd, kube-apiserver, kube-controller-manager) and an etcd Instance for the gardener-apiserver as Docker containers. This is the easiest way to get your Gardener development setup up and running.\nUsing the nodeless cluster setup\nUse the provided Makefile rules to start your local Garden:\nmake local-garden-up [...] Starting gardener-dev kube-etcd cluster..! Starting gardener-dev kube-apiserver..! Starting gardener-dev kube-controller-manager..! Starting gardener-dev gardener-etcd cluster..! namespace/garden created clusterrole.rbac.authorization.k8s.io/gardener.cloud:admin created clusterrolebinding.rbac.authorization.k8s.io/front-proxy-client created [...] ℹ️ [Optional] If you want to develop the SeedAuthorization feature then you have to run make ACTIVATE_SEEDAUTHORIZER=true local-garden-up. However, please note that this forces you to start the gardener-admission-controller via make start-admission-controller.\nTo tear down the local Garden cluster and remove the Docker containers, simply run:\nmake local-garden-down  Alternative: Using a local kubernetes cluster Instead of starting a kubernetes API server and etcd as docker containers, you can also opt for running a local kubernetes cluster, provided by e.g. minikube, kind or docker desktop.\n Note: Gardener requires self-contained kubeconfig files because of a security issue. You can configure your minikube to create self-contained kubeconfig files via:\nminikube config set embed-certs true or when starting the local cluster\nminikube start --embed-certs    Alternative: Using a remote kubernetes cluster For some testing scenarios, you may want to use a remote cluster instead of a local one as your Garden cluster. To do this, you can use the “remote Garden cluster setup” residing in hack/remote-garden. This will start an etcd instance for the gardener-apiserver as a Docker container, and open tunnels for accessing local gardener components from the remote cluster.\nTo avoid mistakes, the remote cluster must have a garden namespace labeled with gardener.cloud/purpose=remote-garden. You must create the garden namespace and label it manually before running make remote-garden-up as described below.\nUse the provided Makefile rules to bootstrap your remote Garden:\nexport KUBECONFIG=\u003cpath to kubeconfig\u003e make remote-garden-up [...] # Start gardener etcd used to store gardener resources (e.g., seeds, shoots) Starting gardener-dev-remote gardener-etcd cluster! [...] # Open tunnels for accessing local gardener components from the remote cluster [...] To close the tunnels and remove the locally-running Docker containers, run:\nmake remote-garden-down  Note: The minimum K8S version of the remote cluster that can be used as Garden cluster is 1.19.x.\n ℹ️ [Optional] If you want to use the remote Garden cluster setup with the SeedAuthorization feature you have to adapt the kube-apiserver process of your remote Garden cluster. To do this, perform the following steps after running make remote-garden-up:\n  Create an authorization webhook configuration file using the IP of the garden/quic-server pod running in your remote Garden cluster and port 10444 that tunnels to your locally running gardener-admission-controller process.\napiVersion: v1 kind: Config current-context: seedauthorizer clusters: - name: gardener-admission-controller  cluster:  insecure-skip-tls-verify: true  server: https://\u003cquic-server-pod-ip\u003e:10444/webhooks/auth/seed users: - name: kube-apiserver  user: {} contexts: - name: seedauthorizer  context:  cluster: gardener-admission-controller  user: kube-apiserver   Change or add the following command line parameters to your kube-apiserver process:\n --authorization-mode=\u003c...\u003e,Webhook --authorization-webhook-config-file=\u003cpath to config file\u003e --authorization-webhook-cache-authorized-ttl=0 --authorization-webhook-cache-unauthorized-ttl=0    Delete the cluster role and rolebinding gardener.cloud:system:seeds from your remote Garden cluster.\n  If your remote Garden cluster is a Gardener shoot, and you can access the seed on which this shoot is scheduled, you can automate the above steps by running the enable-seed-authorizer script and passing the kubeconfig of the seed cluster and the shoot namespace as parameters:\nhack/local-development/remote-garden/enable-seed-authorizer \u003cseed kubeconfig\u003e \u003cnamespace\u003e  Note: The configuration changes introduced by this script result in a working SeedAuthorization feature only on shoots for which the ReversedVPN feature is not enabled. If the corresponding feature gate is enabled in gardenlet, add the annotation alpha.featuregates.shoot.gardener.cloud/reversed-vpn: 'false' to the remote Garden shoot to disable it for that particular shoot.\n To prevent Gardener from reconciling the shoot and overwriting your changes, add the annotation shoot.gardener.cloud/ignore: 'true' to the remote Garden shoot. Note that this annotation takes effect only if it is enabled via the constollers.shoot.respectSyncPeriodOverwrite: true option in the gardenlet configuration.\nTo disable the seed authorizer again, run the same script with -d as a third parameter:\nhack/local-development/remote-garden/enable-seed-authorizer \u003cseed kubeconfig\u003e \u003cnamespace\u003e -d If the seed authorizer is enabled, you also have to start the gardener-admission-controller via make start-admission-controller.\n ⚠️ In the remote garden setup all Gardener components run with administrative permissions, i.e., there is no fine-grained access control via RBAC (as opposed to productive installations of Gardener).\n  Prepare the Gardener Now, that you have started your local cluster, we can go ahead and register the Gardener API Server. Just point your KUBECONFIG environment variable to the cluster you created in the previous step and run:\nmake dev-setup [...] namespace/garden created namespace/garden-dev created deployment.apps/etcd created service/etcd created service/gardener-apiserver created service/gardener-admission-controller created endpoints/gardener-apiserver created endpoints/gardener-admission-controller created apiservice.apiregistration.k8s.io/v1alpha1.core.gardener.cloud created apiservice.apiregistration.k8s.io/v1beta1.core.gardener.cloud created apiservice.apiregistration.k8s.io/v1alpha1.seedmanagement.gardener.cloud created apiservice.apiregistration.k8s.io/v1alpha1.settings.gardener.cloud created ℹ️ [Optional] If you want to enable logging, in the Gardenlet configuration add:\nlogging:  enabled: true The Gardener exposes the API servers of Shoot clusters via Kubernetes services of type LoadBalancer. In order to establish stable endpoints (robust against changes of the load balancer address), it creates DNS records pointing to these load balancer addresses. They are used internally and by all cluster components to communicate. You need to have control over a domain (or subdomain) for which these records will be created. Please provide an internal domain secret (see this for an example) which contains credentials with the proper privileges. Further information can be found here.\nkubectl apply -f example/10-secret-internal-domain-unmanaged.yaml secret/internal-domain-unmanaged created Run the Gardener Next, run the Gardener API Server, the Gardener Controller Manager (optionally), the Gardener Scheduler (optionally), and the Gardenlet in different terminal windows/panes using rules in the Makefile.\nmake start-apiserver [...] I0306 15:23:51.044421 74536 plugins.go:84] Registered admission plugin \"ResourceReferenceManager\" I0306 15:23:51.044523 74536 plugins.go:84] Registered admission plugin \"DeletionConfirmation\" [...] I0306 15:23:51.626836 74536 secure_serving.go:116] Serving securely on [::]:8443 [...] (Optional) Now you are ready to launch the Gardener Controller Manager.\nmake start-controller-manager time=\"2019-03-06T15:24:17+02:00\" level=info msg=\"Starting Gardener controller manager...\" time=\"2019-03-06T15:24:17+02:00\" level=info msg=\"Feature Gates: \" time=\"2019-03-06T15:24:17+02:00\" level=info msg=\"Starting HTTP server on 0.0.0.0:2718\" time=\"2019-03-06T15:24:17+02:00\" level=info msg=\"Acquired leadership, starting controllers.\" time=\"2019-03-06T15:24:18+02:00\" level=info msg=\"Starting HTTPS server on 0.0.0.0:2719\" time=\"2019-03-06T15:24:18+02:00\" level=info msg=\"Found internal domain secret internal-domain-unmanaged for domain nip.io.\" time=\"2019-03-06T15:24:18+02:00\" level=info msg=\"Successfully bootstrapped the Garden cluster.\" time=\"2019-03-06T15:24:18+02:00\" level=info msg=\"Gardener controller manager (version 1.0.0-dev) initialized.\" time=\"2019-03-06T15:24:18+02:00\" level=info msg=\"ControllerRegistration controller initialized.\" time=\"2019-03-06T15:24:18+02:00\" level=info msg=\"SecretBinding controller initialized.\" time=\"2019-03-06T15:24:18+02:00\" level=info msg=\"Project controller initialized.\" time=\"2019-03-06T15:24:18+02:00\" level=info msg=\"Quota controller initialized.\" time=\"2019-03-06T15:24:18+02:00\" level=info msg=\"CloudProfile controller initialized.\" [...] (Optional) Now you are ready to launch the Gardener Scheduler.\nmake start-scheduler time=\"2019-05-02T16:31:50+02:00\" level=info msg=\"Starting Gardener scheduler ...\" time=\"2019-05-02T16:31:50+02:00\" level=info msg=\"Starting HTTP server on 0.0.0.0:10251\" time=\"2019-05-02T16:31:50+02:00\" level=info msg=\"Acquired leadership, starting scheduler.\" time=\"2019-05-02T16:31:50+02:00\" level=info msg=\"Gardener scheduler initialized (with Strategy: SameRegion)\" time=\"2019-05-02T16:31:50+02:00\" level=info msg=\"Scheduler controller initialized.\" [...] The Gardener should now be ready to operate on Shoot resources. You can use\nkubectl get shoots No resources found. to operate against your local running Gardener API Server.\n Note: It may take several seconds until the Gardener API server has been started and is available. No resources found is the expected result of our initial development setup.\n Create a Shoot The steps below describe the general process of creating a Shoot. Have in mind that the steps do not provide full example manifests. The reader needs to check the provider documentation and adapt the manifests accordingly.\n1. Copy the example manifests The next steps require modifications of the example manifests. These modifications are part of local setup and should not be git push-ed. To do not interfere with git, let’s copy the example manifests to dev/ which is ignored by git.\ncp example/*.yaml dev/ 2. Create a Project Every Shoot is associated with a Project. Check the corresponding example manifests dev/00-namespace-garden-dev.yaml and dev/05-project-dev.yaml. Adapt them and create them.\nkubectl apply -f dev/00-namespace-garden-dev.yaml kubectl apply -f dev/05-project-dev.yaml Make sure that the Project is successfully reconciled:\n$ kubectl get project dev NAME NAMESPACE STATUS OWNER CREATOR AGE dev garden-dev Ready john.doe@example.com kubernetes-admin 6s 3. Create a CloudProfile The CloudProfile resource is provider specific and describes the underlying cloud provider (available machine types, regions, machine images, etc.). Check the corresponding example manifest dev/30-cloudprofile.yaml. Check also the documentation and example manifests of the provider extension. Adapt dev/30-cloudprofile.yaml and apply it.\nkubectl apply -f dev/30-cloudprofile.yaml 4. Install necessary Gardener Extensions The Known Extension Implementations section contains a list of available extension implementations. You need to create a ControllerRegistration and ControllerDeployment for\n at least one infrastructure provider a dns provider (if the DNS for the Seed is not disabled) at least one operating system extension at least one network plugin extension  As a convention, the example ControllerRegistration manifest (containing also the necessary ControllerDeployment) for an extension is located under example/controller-registration.yaml in the corresponding repository (for example for AWS the ControllerRegistration can be found here). An example creation for provider-aws (make sure to replace \u003cversion\u003e with the newest released version tag):\nkubectl apply -f https://raw.githubusercontent.com/gardener/gardener-extension-provider-aws/\u003cversion\u003e/example/controller-registration.yaml Instead of updating extensions manually you can use Gardener Extensions Manager to install and update extension controllers. This is especially useful if you want to keep and maintain your development setup for a longer time. Also, please refer to this document for further information about how extensions are registered in case you want to use other versions than the latest releases.\n5. Register a Seed Shoot controlplanes run in seed clusters, so we need to create our first Seed now.\nCheck the corresponding example manifest dev/40-secret-seed.yaml and dev/50-seed.yaml. Update dev/40-secret-seed.yaml with base64 encoded kubeconfig of the cluster that will be used as Seed (the scope of the permissions should be identical to the kubeconfig that the Gardenlet creates during bootstrapping - for now, cluster-admin privileges are recommended).\nkubectl apply -f dev/40-secret-seed.yaml Adapt dev/50-seed.yaml - adjust .spec.secretRef to refer the newly created Secret, adjust .spec.provider with the Seed cluster provider and revise the other fields.\nkubectl apply -f dev/50-seed.yaml 6. Start Gardenlet Once the Seed is created, start the Gardenlet to reconcile it. The make start-gardenlet command will automatically configure the local Gardenlet process to use the Seed and its kubeconfig. If you have multiple Seeds, you have to specify which to use by setting the SEED_NAME environment variable like in make start-gardenlet SEED_NAME=my-first-seed.\nmake start-gardenlet time=\"2019-11-06T15:24:17+02:00\" level=info msg=\"Starting Gardenlet...\" time=\"2019-11-06T15:24:17+02:00\" level=info msg=\"Feature Gates: HVPA=true, Logging=true\" time=\"2019-11-06T15:24:17+02:00\" level=info msg=\"Acquired leadership, starting controllers.\" time=\"2019-11-06T15:24:18+02:00\" level=info msg=\"Found internal domain secret internal-domain-unmanaged for domain nip.io.\" time=\"2019-11-06T15:24:18+02:00\" level=info msg=\"Gardenlet (version 1.0.0-dev) initialized.\" time=\"2019-11-06T15:24:18+02:00\" level=info msg=\"ControllerInstallation controller initialized.\" time=\"2019-11-06T15:24:18+02:00\" level=info msg=\"Shoot controller initialized.\" time=\"2019-11-06T15:24:18+02:00\" level=info msg=\"Seed controller initialized.\" [...] The Gardenlet will now reconcile the Seed. Check the progess from time to time until it’s Ready:\nkubectl get seed NAME STATUS PROVIDER REGION AGE VERSION K8S VERSION seed-aws Ready aws eu-west-1 4m v1.11.0-dev v1.18.12 7. Create a Shoot A Shoot requires a SecretBinding. The SecretBinding refers to a Secret that contains the cloud provider credentials. The Secret data keys are provider specific and you need to check the documentation of the provider to find out which data keys are expected (for example for AWS the related documentation can be found here). Adapt dev/70-secret-provider.yaml and dev/80-secretbinding.yaml and apply them.\nkubectl apply -f dev/70-secret-provider.yaml kubectl apply -f dev/80-secretbinding.yaml After the SecretBinding creation, you are ready to proceed with the Shoot creation. You need to check the documentation of the provider to find out the expected configuration (for example for AWS the related documentation and example Shoot manifest can be found here). Adapt dev/90-shoot.yaml and apply it.\nTo make sure that a specific Seed cluster will be chosen or to skip the scheduling (the sheduling requires Gardener Scheduler to be running), specify the .spec.seedName field (see here).\nkubectl apply -f dev/90-shoot.yaml Watch the progress of the operation and make sure that the Shoot will be successfully created.\nwatch kubectl get shoot --all-namespaces ","categories":"","description":"","excerpt":"Overview Conceptually, all Gardener components are designed to run as …","ref":"/docs/gardener/development/local_setup/","tags":"","title":"Local Setup"},{"body":"Preparing the Local Development Setup (Mac OS X)  Preparing the Local Development Setup (Mac OS X)  Installing Golang environment Installing Docker (Optional) Setup Docker Hub account (Optional) Local development  Installing the Machine Controller Manager locally   Prepare the cluster Getting started Testing Machine Classes Usage    Conceptionally, the Machine Controller Manager is designed to run in a container within a Pod inside a Kubernetes cluster. For development purposes, you can run the Machine Controller Manager as a Go process on your local machine. This process connects to your remote cluster to manage VMs for that cluster. That means that the Machine Controller Manager runs outside a Kubernetes cluster which requires providing a Kubeconfig in your local filesystem and point the Machine Controller Manager to it when running it (see below).\nAlthough the following installation instructions are for Mac OS X, similar alternate commands could be found for any Linux distribution.\nInstalling Golang environment Install the latest version of Golang (at least v1.8.3 is required) by using Homebrew:\n$ brew install golang In order to perform linting on the Go source code, install Golint:\n$ go get -u golang.org/x/lint/golint Installing Docker (Optional) In case you want to build Docker images for the Machine Controller Manager you have to install Docker itself. We recommend using Docker for Mac OS X which can be downloaded from here.\nSetup Docker Hub account (Optional) Create a Docker hub account at Docker Hub if you don’t already have one.\nLocal development ⚠️ Before you start developing, please ensure to comply with the following requirements:\n You have understood the principles of Kubernetes, and its components, what their purpose is and how they interact with each other. You have understood the architecture of the Machine Controller Manager  The development of the Machine Controller Manager could happen by targetting any cluster. You basically need a Kubernetes cluster running on a set of machines. You just need the Kubeconfig file with the required access permissions attached to it.\nInstalling the Machine Controller Manager locally Clone the repository from GitHub.\n$ git clone git@github.com:gardener/machine-controller-manager.git $ cd machine-controller-manager Prepare the cluster  Connect to the remote kubernetes cluster where you plan to deploy the Machine Controller Manager using kubectl. Set the environment variable KUBECONFIG to the path of the yaml file containing your cluster info Now, create the required CRDs on the remote cluster using the following command,  $ kubectl apply -f kubernetes/crds.yaml Getting started  Create a dev directory. Copy the kubeconfig of kubernetes cluster where you wish to deploy the machines into dev/target-kubeconfig.yaml. (optional) Copy the kubeconfig of kubernetes cluster from where you wish to manage the machines into dev/control-kubeconfig.yaml. If you do this, also update the Makefile variable CONTROL_KUBECONFIG to point to dev/control-kubeconfig.yaml and CONTROL_NAMESPACE to the namespace in which your controller watches over. There is a rule dev in the Makefile which will automatically start the Machine Controller Manager with development settings:  $ make start I1227 11:08:19.963638 55523 controllermanager.go:204] Starting shared informers I1227 11:08:20.766085 55523 controller.go:247] Starting machine-controller-manager ⚠️ The file dev/target-kubeconfig.yaml points to the cluster whose nodes you want to manage. dev/control-kubeconfig.yaml points to the cluster from where you want to manage the nodes from. However, dev/control-kubeconfig.yaml is optional.\nThe Machine Controller Manager should now be ready to manage the VMs in your kubernetes cluster.\n⚠️ This is assuming that your MCM is built to manage machines for any in-tree supported providers. There is a new way to deploy and manage out of tree (external) support for providers whose development can be found here\nTesting Machine Classes To test the creation/deletion of a single instance for one particular machine class you can use the managevm cli. The corresponding INFRASTRUCTURE-machine-class.yaml and the INFRASTRUCTURE-secret.yaml need to be defined upfront. To build and run it\nGO111MODULE=on go build -mod=vendor -o managevm cmd/machine-controller-manager-cli/main.go # create machine ./managevm --secret PATH_TO/INFRASTRUCTURE-secret.yaml --machineclass PATH_TO/INFRASTRUCTURE-machine-class.yaml --classkind INFRASTRUCTURE --machinename test # delete machine ./managevm --secret PATH_TO/INFRASTRUCTURE-secret.yaml --machineclass PATH_TO/INFRASTRUCTURE-machine-class.yaml --classkind INFRASTRUCTURE --machinename test --machineid INFRASTRUCTURE:///REGION/INSTANCE_ID Usage To start using Machine Controller Manager, follow the links given at usage here.\n","categories":"","description":"","excerpt":"Preparing the Local Development Setup (Mac OS X)  Preparing the Local …","ref":"/docs/other-components/machine-controller-manager/docs/development/local_setup/","tags":"","title":"Local Setup"},{"body":"How to create log parser for container into fluent-bit If our log message is parsed correctly, it has to be showed in Grafana like this:\n{\"log\":\"OpenAPI AggregationController: Processing item v1beta1.metrics.k8s.io\",\"pid\":\"1\",\"severity\":\"INFO\",\"source\":\"controller.go:107\"} Otherwise it will looks like this:\n{ \"log\":\"{ \\\"level\\\":\\\"info\\\",\\\"ts\\\":\\\"2020-06-01T11:23:26.679Z\\\",\\\"logger\\\":\\\"gardener-resource-manager.health-reconciler\\\",\\\"msg\\\":\\\"Finished ManagedResource health checks\\\",\\\"object\\\":\\\"garden/provider-aws-dsm9r\\\" }\\n\" } } Lets make a custom parser now   First of all we need to know how does the log for the specific container look like (for example lets take a log from the alertmanager : level=info ts=2019-01-28T12:33:49.362015626Z caller=main.go:175 build_context=\"(go=go1.11.2, user=root@4ecc17c53d26, date=20181109-15:40:48))\n  We can see that this log contains 4 subfields(severity=info, timestamp=2019-01-28T12:33:49.362015626Z, source=main.go:175 and the actual message). So we have to write a regex which matches this log in 4 groups(We can use https://regex101.com/ like helping tool). So for this purpose our regex looks like this:\n  ^level=(?\u003cseverity\u003e\\w+)\\s+ts=(?\u003ctime\u003e\\d{4}-\\d{2}-\\d{2}[Tt].*[zZ])\\s+caller=(?\u003csource\u003e[^\\s]*+)\\s+(?\u003clog\u003e.*)  Now we have to create correct time format for the timestamp(We can use this site for this purpose: http://ruby-doc.org/stdlib-2.4.1/libdoc/time/rdoc/Time.html#method-c-strptime). So our timestamp matches correctly the following format:  %Y-%m-%dT%H:%M:%S.%L  It’s a time to apply our new regex into fluent-bit configuration. Go to fluent-bit-configmap.yaml and create new filter using the following template:  [FILTER]  Name parser  Match kubernetes.\u003c\u003c pod-name \u003e\u003e*\u003c\u003c container-name \u003e\u003e*  Key_Name log  Parser \u003c\u003c parser-name \u003e\u003e  Reserve_Data True EXAMPLE [FILTER]  Name parser  Match kubernetes.alertmanager*alertmanager*  Key_Name log  Parser alermanagerParser  Reserve_Data True  Now lets check if there is already exists parser with such a regex and time format that we need. if not, let`s create one:  [PARSER]  Name \u003c\u003c parser-name \u003e\u003e  Format regex  Regex \u003c\u003c regex \u003e\u003e  Time_Key time  Time_Format \u003c\u003c time-format \u003e\u003e EXAMPLE [PARSER]  Name alermanagerParser  Format regex  Regex ^level=(?\u003cseverity\u003e\\w+)\\s+ts=(?\u003ctime\u003e\\d{4}-\\d{2}-\\d{2}[Tt].*[zZ])\\s+caller=(?\u003csource\u003e[^\\s]*+)\\s+(?\u003clog\u003e.*)  Time_Key time  Time_Format %Y-%m-%dT%H:%M:%S.%L Follow your development setup to validate that parsers are working correctly. ","categories":"","description":"","excerpt":"How to create log parser for container into fluent-bit If our log …","ref":"/docs/gardener/development/log_parsers/","tags":"","title":"Log Parsers"},{"body":"Logging in Gardener Components This document aims at providing a general developer guideline on different aspects of logging practices and conventions used in the Gardener codebase. It contains mostly Gardener-specific points and references other existing and commonly accepted logging guidelines for general advice. Developers and reviewers should consult this guide when writing, refactoring and reviewing Gardener code. If parts are unclear or new learnings arise, this guide should be adapted accordingly.\nLogging Libraries / Implementations Historically, Gardener components have been using logrus. There is a global logrus logger (logger.Logger) that is initialized by components on startup and used across the codebase. In most places, it is used as a printf-style logger and only in some instances we make use of logrus’ structured logging functionality.\nIn the process of migrating our components to native controller-runtime components (see gardener/gardener#4251), we also want to make use of controller-runtime’s built-in mechanisms for streamlined logging. controller-runtime uses logr, a simple structured logging interface, for library-internal logging and logging in controllers.\nlogr itself is only an interface and doesn’t provide an implementation out of the box. Instead, it needs to be backed by a logging implementation like zapr. Code that uses the logr interface is thereby not tied to a specific logging implementation and makes the implementation easily exchangeable. controller-runtime already provides a set of helpers for constructing zapr loggers, i.e., logr loggers backed by zap, which is a popular logging library in the go community. Hence, we are migrating our component logging from logrus to logr (backed by zap) as part of gardener/gardener#4251.\n ⚠️ logger.Logger (logrus logger) is deprecated in Gardener and shall not be used in new code – use logr loggers when writing new code! (also see Migration from logrus to logr)\nℹ️ Don’t use zap loggers directly, always use the logr interface in order to avoid tight coupling to a specific logging implementation.\n gardener-apiserver differs from the other components as it is based on the apiserver library and therefore uses klog – just like kube-apiserver. As gardener-apiserver writes (almost) no logs in our coding (outside the apiserver library), there is currently no plan for switching the logging implementation. Hence, the following sections focus on logging in the controller and admission components only.\nlogcheck Tool To ensure a smooth migration to logr and make logging in Gardener components more consistent, the logcheck tool was added. It enforces (parts of) this guideline and detects programmer-level errors early on in order to prevent bugs. Please check out the tool’s documentation for a detailed description.\nStructured Logging Similar to efforts in the Kubernetes project, we want to migrate our component logs to structured logging. As motivated above, we will use the logr interface instead of klog though.\nYou can read more about the motivation behind structured logging in logr’s background and FAQ (also see this blog post by Dave Cheney). Also, make sure to check out controller-runtime’s logging guideline with specifics for projects using the library. The following sections will focus on the most important takeaways from those guidelines and give general instructions on how to apply them to Gardener and its controller-runtime components. Note: some parts in this guideline differ slightly from controller-runtime’s document.\nTL;DR of Structured Logging ❌ stop using printf-style logging:\nvar logger *logrus.Logger logger.Infof(\"Scaling deployment %s/%s to %d replicas\", deployment.Namespace, deployment.Name, replicaCount) ✅ instead, write static log messages and enrich them with additional structured information in form of key-value pairs:\nvar logger logr.Logger logger.Info(\"Scaling deployment\", \"deployment\", client.ObjectKeyFromObject(deployment), \"replicas\", replicaCount) Log Configuration Gardener components can be configured to either log in json (default) or text format: json format is supposed to be used in production, while text format might be nicer for development.\n# json {\"level\":\"info\",\"ts\":\"2021-12-16T08:32:21.059+0100\",\"msg\":\"Hello botanist\",\"garden\":\"eden\"}  # text 2021-12-16T08:32:21.059+0100 INFO Hello botanist {\"garden\": \"eden\"} Components can be set to one of the following log levels (with increasing verbosity): error, info (default), debug.\n ℹ️ Note: some Gardener components don’t feature a configurable log level and format yet. In this case, they log at info in json format. We might add configuration options via command line flags that can be used in all components in the future though (see gardener/gardener#5191).\n Log Levels logr uses V-levels (numbered log levels), higher V-level means higher verbosity. V-levels are relative (in contrast to klog’s absolute V-levels), i.e., V(1) creates a logger, that is one level more verbose than its parent logger.\nIn Gardener components, the mentioned log levels in the component config (error, info, debug) map to the zap levels with the same names (see here). Hence, our loggers follow the same mapping from numerical logr levels to named zap levels like described in zapr, i.e.:\n component config specifies debug ➡️ both V(0) and V(1) are enabled component config specifies info ➡️ V(0) is enabled, V(1) will not be shown component config specifies error ➡️ neither V(0) nor V(1) will be shown Error() logs will always be shown  This mapping applies to the components’ root loggers (the ones that are not “derived” from any other logger; constructed on component startup). If you derive a new logger with e.g. V(1), the mapping will shift by one. For example, V(0) will then log at zap’s debug level.\nThere is no warning level (see Dave Cheney’s post). If there is an error condition (e.g., unexpected error received from a called function), the error should either be handled or logged at error if it is neither handled nor returned. If you have an error value at hand that doesn’t represent an actual error condition, but you still want to log it as an informational message, log it at info level with key err.\nWe might consider to make use of a broader range of log levels in the future when introducing more logs and common command line flags for our components (comparable to --v of Kubernetes components). For now, we stick to the mentioned two log levels like controller-runtime: info (V(0)) and debug (V(1)).\nLogging in Controllers Named Loggers Controllers should use named loggers that include their name, e.g.:\ncontrollerLogger := rootLogger.WithName(\"controller\").WithName(\"shoot\") controllerLogger.Info(\"Deploying kube-apiserver\") results in\n2021-12-16T09:27:56.550+0100 INFO controller.shoot Deploying kube-apiserver Logger names are hierarchical. You can make use of it, where controllers are composed of multiple “subcontrollers”, e.g., controller.shoot.hibernation or controller.shoot.maintenance.\nUsing the global logger logf.Log directly is discouraged and should be rather exceptional because it makes correlating logs with code harder. Preferably, all parts of the code should use some named logger.\nReconciler Loggers In your Reconcile function, retrieve a logger from the given context.Context. It inherits from the controller’s logger (i.e., is already named) and is preconfigured with name and namespace values for the reconciliation request:\nfunc (r *reconciler) Reconcile(ctx context.Context, request reconcile.Request) (reconcile.Result, error) {  log := logf.FromContext(ctx)  log.Info(\"Reconciling Shoot\")  // ...  return reconcile.Result{}, nil } results in\n2021-12-16T09:35:59.099+0100 INFO controller.shoot Reconciling Shoot {\"name\": \"sunflower\", \"namespace\": \"garden-greenhouse\"} The logger is injected by controller-runtime’s Controller implementation and our controllerutils.CreateWorker alike (if a logger is passed using controllerutils.WithLogger). The logger returned by logf.FromContext is never nil. If the context doesn’t carry a logger, it falls back to the global logger (logf.Log), which might discard logs if not configured, but is also never nil.\n ⚠️ Make sure that you don’t overwrite the name or namespace value keys for such loggers, otherwise you will lose information about the reconciled object.\n The controller implementation (controller-runtime / CreateWorker) itself takes care of logging the error returned by reconcilers. Hence, don’t log an error that you are returning. Generally, functions should not return an error, if they already logged it, because that means the error is already handled and not an error anymore. See Dave Cheney’s post for more on this.\nMessages  Log messages should be static. Don’t put variable content in there, i.e., no fmt.Sprintf or string concatenation (+). Use key-value pairs instead. Log messages should be capitalized. Note: this contrasts with error messages, that should not be capitalized. However, both should not end with a punctuation mark.  Keys and Values   Use WithValues instead of repeatedly adding key-value pairs for multiple log statements. WithValues creates a new logger from the parent, that carries the given key-value pairs. E.g., use it when acting on one object in multiple steps and logging something for each step:\nlog := parentLog.WithValues(\"infrastructure\", client.ObjectKeyFromObject(infrastrucutre)) // ... log.Info(\"Creating Infrastructure\") // ... log.Info(\"Waiting for Infrastructure to be reconciled\") // ... Note: WithValues bypasses controller-runtime’s special zap encoder that nicely encodes ObjectKey/NamespacedName and runtime.Object values, see kubernetes-sigs/controller-runtime#1290. Thus, the end result might look different depending on the value and its Stringer implementation.\n  Use lowerCamelCase for keys. Don’t put spaces in keys, as it will make log processing with simple tools like jq harder.\n  Keys should be constant, human-readable, consistent across the codebase and naturally match parts of the log message, see logr guideline.\n  When logging object keys (name and namespace), use the object’s type as the log key and a client.ObjectKey/types.NamespacedName value as value, e.g.:\nvar deployment *appsv1.Deployment log.Info(\"Creating Deployment\", \"deployment\", client.ObjectKeyFromObject(deployment)) which results in\n{\"level\":\"info\",\"ts\":\"2021-12-16T08:32:21.059+0100\",\"msg\":\"Creating Deployment\",\"deployment\":{\"name\": \"bar\", \"namespace\": \"foo\"}} Earlier, we often used kutil.ObjectName() for logging object keys, which encodes them into a flat string like foo/bar. However, this flat string cannot be processed so easily by logging stacks (or jq) like a structured log. Hence, the use of kutil.ObjectName() for logging object keys is discouraged. Existing usages should be refactored to use client.ObjectKeyFromObject() instead.\n  There are cases where you don’t have the full object key or the object itself at hand, e.g., if an object references another object (in the same namespace) by name (think secretRef or similar). In such a cases, either construct the full object key including the implied namespace or log the object name under a key ending in Name, e.g.:\nvar (  // object to reconcile  shoot *gardencorev1beta1.Shoot  // retrieved via logf.FromContext, preconfigured by controller with namespace and name of reconciliation request  log logr.Logger )  // option a: full object key, manually constructed log.Info(\"Shoot uses SecretBinding\", \"secretBinding\", client.ObjectKey{Namespace: shoot.Namespace, Name: shoot.Spec.SecretBindingName}) // option b: only name under respective *Name log key log.Info(\"Shoot uses SecretBinding\", \"secretBindingName\", shoot.Spec.SecretBindingName) Both options result in well-structured logs, that are easy to interpret and process:\n{\"level\":\"info\",\"ts\":\"2022-01-18T18:00:56.672+0100\",\"msg\":\"Shoot uses SecretBinding\",\"name\":\"my-shoot\",\"namespace\":\"garden-project\",\"secretBinding\":{\"namespace\":\"garden-project\",\"name\":\"aws\"}} {\"level\":\"info\",\"ts\":\"2022-01-18T18:00:56.673+0100\",\"msg\":\"Shoot uses SecretBinding\",\"name\":\"my-shoot\",\"namespace\":\"garden-project\",\"secretBindingName\":\"aws\"}   When handling generic client.Object values (e.g. in helper funcs), use object as key.\n  When adding timestamps to key-value pairs, use time.Time values. By this, they will be encoded in the same format as the log entry’s timestamp.\nDon’t use metav1.Time values, as they will be encoded in a different format by their Stringer implementation. Pass \u003csomeTimestamp\u003e.Time to loggers in case you have a metav1.Time value at hand.\n  Same applies to durations. Use time.Duration values instead of *metav1.Duration. Durations can be handled specially by zap just like timestamps.\n  Event recorders not only create Event objects but also log them. However, both Gardener’s manually instantiated event recorders and the ones that controller-runtime provides log to debug level and use generic formats, that are not very easy to interpret or process (no structured logs). Hence, don’t use event recorders as replacements for well-structured logs. If a controller records an event for a completed action or important information, it should probably log it as well, e.g.:\nlog.Info(\"Creating ManagedSeed\", \"replica\", r.GetObjectKey()) a.recorder.Eventf(managedSeedSet, corev1.EventTypeNormal, EventCreatingManagedSeed, \"Creating ManagedSeed %s\", r.GetFullName())   Logging in Test Code   If the tested production code requires a logger, you can pass logr.Discard() or logf.NullLogger{} in your test, which simply discards all logs.\n  logf.Log is safe to use in tests and will not cause a nil pointer deref, even if it’s not initialized via logf.SetLogger. It is initially set to a NullLogger by default, which means all logs are discarded, unless logf.SetLogger is called in the first 30 seconds of execution.\n  Pass zap.WriteTo(GinkgoWriter) in tests where you want to see the logs on test failure but not on success, for example:\nlogf.SetLogger(logger.MustNewZapLogger(logger.DebugLevel, logger.FormatJSON, zap.WriteTo(GinkgoWriter))) log := logf.Log.WithName(\"test\")   Migration from logrus to logr These points might be helpful when refactoring existing code during the migration period:\n For migrating an existing controller to logr:  Create a named logger (example). Pass controllerutils.WithLogger to CreateWorker (example). This allows logf.FromContext to be used in reconcilers. Use logf.FromContext in Reconcile to retrieve the logr logger and use it from there on (example). Make sure to follow the other guidelines mentioned above as well (see Logging in Controllers).   Libraries might expect a different logging implementation than the component which uses it. E.g., a controller that already uses logr might want to use the flow package which still uses logrus. In such cases:  You can consider refactoring the library along with the component itself, if feasible. It is acceptable for the migration period to use a logger derived from the respective global logger (logger.Logger or logf.Log) and pass it to the library. However, please add a TODO for cleaning it up later on, once the migration is completed. E.g.: // TODO: switch to logr once flow package is migrated err := shootFlow.Run(flow.Opts{  Logger: logger.Logger.WithFields(logrus.Fields{\"logger\": \"controller.\" + ControllerName, \"name\": shoot.Name, \"namespace\": shoot.Namespace}) })     ","categories":"","description":"","excerpt":"Logging in Gardener Components This document aims at providing a …","ref":"/docs/gardener/development/logging/","tags":"","title":"Logging"},{"body":"Logging stack Motivation Kubernetes uses the underlying container runtime logging, which does not persist logs for stopped and destroyed containers. This makes it difficult to investigate issues in the very common case of not running containers. Gardener provides a solution to this problem for the managed cluster components, by introducing its own logging stack.\nComponents:  A Fluent-bit daemonset which works like a log collector and custom Golang plugin which spreads log messages to their Loki instances One Loki Statefulset in the garden namespace which contains logs for the seed cluster and one per shoot namespace which contains logs for shoot’s controlplane. One Grafana Deployment in garden namespace and two Deployments per shoot namespace (one exposed to the end users and one for the operators). Grafana is the UI component used in the logging stack.  Extension of the logging stack The logging stack is extended to scrape logs from the systemd services of each shoots’ nodes and from all Gardener components in the shoot kube-system namespace. These logs are exposed only to the Gardener operators.\nAlso, in the shoot control plane an event-logger pod is deployed which scrapes events from the shoot kube-system namespace and shoot control-plane namespace in the seed. The event-logger logs the events to the standard output. Then the fluent-bit gets these events as container logs and sends them to the Loki in the shoot control plane (similar to how it works for any other control plane component).\nHow to access the logs The first step is to authenticate in front of the Grafana ingress. There are two Grafana instances where the logs are accessible from.\n  The user (stakeholder/cluster-owner) Grafana consist of a predefined Monitoring and Logging dashboards which help the end-user to get the most important metrics and logs out of the box. This Grafana UI is dedicated only for the end-user and does not show logs from components which could log a sensitive information. Also, the Explore tab is not available. Those logs are in the predefined dashboard named Controlplane Logs Dashboard. In this dashboard the user can search logs by pod name, container name, severity and a phrase or regex. The user Grafana URL can be found in the Logging and Monitoring section of a cluster in the Gardener Dashboard alongside with the credentials, when opened as cluster owner/user. The secret with the credentials can be found in garden-\u003cproject\u003e namespace under \u003cshoot-name\u003e.monitoring in the garden cluster or in the control-plane (shoot–project–shoot-name) namespace under observability-ingress-users-\u003chash\u003e secrets in the seed cluster. Also, the Grafana URL can be found in the control-plane namespace under the grafana-users ingress in the seed. The end-user has access only to the logs of some of the control-plane components.\n  In addition to the dashboards in the User Grafana, the Operator Grafana contains several other dashboards that aim to facilitate the work of operators. The operator Grafana URL can be found in the Logging and Monitoring section of a cluster in the Gardener Dashboard alongside with the credentials, when opened as Gardener operator. Also, it can be found in the control-plane namespace under the grafana-operators ingress in the seed. Operators have access to the Explore tab. The secret with the credentials can be found in the control-plane (shoot–project–shoot-name) namespace under observability-ingress-\u003chash\u003e-\u003chash\u003e secrets in the seed. From Explore tab, operators have unlimited abilities to extract and manipulate logs. The Grafana itself helps them with suggestions and auto-completion.\n   NOTE: Operators are people part of the Gardener team with operator permissions, not operators of the end-user cluster!\n How to use Explore tab. If you click on the Log browser \u003e button you will see all of the available labels. Clicking on the label you can see all of its available values for the given period of time you have specified. If you are searching for logs for the past one hour do not expect to see labels or values for which there were no logs for that period of time. By clicking on a value, Grafana automatically eliminates all other label and/or values with which no valid log stream can be made. After choosing the right labels and their values, click on Show logs button. This will build Log query and execute it. This approach is convenient when you don’t know the labels names or they values. Once you felt comfortable, you can start to use the LogQL language to search for logs. Next to the Log browser \u003e button is the place where you can type log queries.\nExamples:\n  If you want to get logs for calico-node-\u003chash\u003e pod in the cluster kube-system. The name of the node on which calico-node was running is known but not the hash suffix of the calico-node pod. Also we want to search for errors in the logs.\n{pod_name=~\"calico-node-.+\", nodename=\"ip-10-222-31-182.eu-central-1.compute.internal\"} |~ \"error\"\nHere, you will get as much help as possible from the Grafana by giving you suggestions and auto-completion.\n  If you want to get the logs from kubelet systemd service of a given node and search for a pod name in the logs.\n{unit=\"kubelet.service\", nodename=\"ip-10-222-31-182.eu-central-1.compute.internal\"} |~ \"pod name\"\n   NOTE: Under unit label there is only the docker, containerd, kubelet and kernel logs.\n  If you want to get the logs from cloud-config-downloader systemd service of a given node and search for a string in the logs.\n{job=\"systemd-combine-journal\",nodename=\"ip-10-222-31-182.eu-central-1.compute.internal\"} | unpack | unit=\"cloud-config-downloader.service\" |~ \"last execution was\"\n   NOTE: {job=\"systemd-combine-journal\",nodename=\"\u003cnode name\u003e\"} stream pack all logs from systemd services except docker, containerd, kubelet and kernel. To filter those log by unit you have to unpack them first.\n Retrieving events:    If you want to get the events from the shoot kube-system namespace generated by kubelet and related to the node-problem-detector:\n{job=\"event-logging\"} | unpack | origin_extracted=\"shoot\",source=\"kubelet\",object=~\".*node-problem-detector.*\"\n  If you want to get the events generated by MCM in the shoot control plane in the seed:\n{job=\"event-logging\"} | unpack | origin_extracted=\"seed\",source=~\".*machine-controller-manager.*\"\n   NOTE: In order to group events by origin one has to specify origin_extracted because origin label is reserved for all of the logs from the seed and the event-logger resides in the seed, so all of its logs are coming as they are only from the seed. The actual origin is embedded in the unpacked event. When unpacked the embedded origin becomes origin_extracted.\n Expose logs for component to User Grafana Exposing logs for a new component to the User’s Grafana is described here\nConfiguration Fluent-bit The Fluent-bit configurations can be found on charts/seed-bootstrap/charts/fluent-bit/templates/fluent-bit-configmap.yaml There are five different specifications:\n SERVICE: Defines the location of the server specifications INPUT: Defines the location of the input stream of the logs OUTPUT: Defines the location of the output source (Loki for example) FILTER: Defines filters which match specific keys PARSER: Defines parsers which are used by the filters  Loki The Loki configurations can be found on charts/seed-bootstrap/charts/loki/templates/loki-configmap.yaml\nThe main specifications there are:\n Index configuration: Currently is used the following one:   schema_config: configs: - from: 2018-04-15 store: boltdb object_store: filesystem schema: v11 index: prefix: index_ period: 24h  from: is the date from which logs collection is started. Using a date in the past is okay. store: The DB used for storing the index. object_store: Where the data is stored schema: Schema version which should be used (v11 is currently recommended) index.prefix: The prefix for the index. index.period: The period for updating the indices  Adding of new index happens with new config block definition. from field should start from the current day + previous index.period and should not overlap with the current index. The prefix also should be different\n schema_config: configs: - from: 2018-04-15 store: boltdb object_store: filesystem schema: v11 index: prefix: index_ period: 24h - from: 2020-06-18 store: boltdb object_store: filesystem schema: v11 index: prefix: index_new_ period: 24h  chunk_store_config Configuration   chunk_store_config: max_look_back_period: 336h chunk_store_config.max_look_back_period should be the same as the retention_period\n table_manager Configuration   table_manager: retention_deletes_enabled: true retention_period: 336h table_manager.retention_period is the living time for each log message. Loki will keep messages for sure for (table_manager.retention_period - index.period) time due to specification in the Loki implementation.\nGrafana The Grafana configurations can be found on charts/seed-bootstrap/charts/templates/grafana/grafana-datasources-configmap.yaml and charts/seed-monitoring/charts/grafana/tempates/grafana-datasources-configmap.yaml\nThis is the Loki configuration that Grafana uses:\n - name: loki type: loki access: proxy url: http://loki.{{ .Release.Namespace }}.svc:3100 jsonData: maxLines: 5000  name: is the name of the datasource type: is the type of the datasource access: should be set to proxy url: Loki’s url svc: Loki’s port jsonData.maxLines: The limit of the log messages which Grafana will show to the users.  Decrease this value if the browser works slowly!\n","categories":"","description":"","excerpt":"Logging stack Motivation Kubernetes uses the underlying container …","ref":"/docs/gardener/usage/logging/","tags":"","title":"Logging"},{"body":"Logging and Monitoring for Extensions Gardener provides an integrated logging and monitoring stack for alerting, monitoring and troubleshooting of its managed components by operators or end users. For further information how to make use of it in these roles, refer to the corresponding guides for exploring logs and for monitoring with Grafana.\nThe components that constitute the logging and monitoring stack are managed by Gardener. By default, it deploys Prometheus, Alertmanager and Grafana into the garden namespace of all seed clusters. If the Logging feature gate in the gardenlet configuration is enabled, it will deploy fluent-bit and Loki in the garden namespace too.\nEach shoot namespace hosts managed logging and monitoring components. As part of the shoot reconciliation flow, Gardener deploys a shoot-specific Prometheus, Grafana and, if configured, an Alertmanager into the shoot namespace, next to the other control plane components. If the Logging feature gate is enabled and the shoot purpose is not testing, it deploys a shoot-specific Loki in the shoot namespace too.\nThe logging and monitoring stack is extensible by configuration. Gardener extensions can take advantage of that and contribute configurations encoded in ConfigMaps for their own, specific dashboards, alerts, log parsers and other supported assets and integrate with it. As with other Gardener resources, they will be continuously reconciled.\nThis guide is about the roles and extensibility options of the logging and monitoring stack components, and how to integrate extensions with:\n Monitoring Logging  Monitoring The central Prometheus instance in the garden namespace fetches metrics and data from all seed cluster nodes and all seed cluster pods. It uses the federation concept to allow the shoot-specific instances to scrape only the metrics for the pods of the control plane they are responsible for. This mechanism allows to scrape the metrics for the nodes/pods once for the whole cluster, and to have them distributed afterwards.\nThe shoot-specific metrics are then made available to operators and users in the shoot Grafana, using the shoot Prometheus as data source.\nExtension controllers might deploy components as part of their reconciliation next to the shoot’s control plane. Examples for this would be a cloud-controller-manager or CSI controller deployments. Extensions that want to have their managed control plane components integrated with monitoring can contribute their per-shoot configuration for scraping Prometheus metrics, Alertmanager alerts or Grafana dashboards.\nExtensions monitoring integration Before deploying the shoot-specific Prometheus instance, Gardener will read all ConfigMaps in the shoot namespace, which are labeled with extensions.gardener.cloud/configuration=monitoring. Such ConfigMaps may contain four fields in their data:\n scrape_config: This field contains Prometheus scrape configuration for the component(s) and metrics that shall be scraped. alerting_rules: This field contains Alertmanager rules for alerts that shall be raised. (deprecated)dashboard_operators: This field contains a Grafana dashboard in JSON that is only relevant for Gardener operators. (deprecated)dashboard_users: This field contains a Grafana dashboard in JSON that is only relevant for Gardener users (shoot owners).  Example: A ControlPlane controller deploying a cloud-controller-manager into the shoot namespace wants to integrate monitoring configuration for scraping metrics, alerting rules, dashboards and logging configuration for exposing logs to the end users.\napiVersion: v1 kind: ConfigMap metadata:  name: extension-controlplane-monitoring-ccm  namespace: shoot--project--name  labels:  extensions.gardener.cloud/configuration: monitoring data:  scrape_config: |- job_name: cloud-controller-manager scheme: https tls_config: insecure_skip_verify: true authorization: type: Bearer credentials_file: /var/run/secrets/gardener.cloud/shoot/token/token honor_labels: false kubernetes_sd_configs: - role: endpoints namespaces: names: [shoot--project--name] relabel_configs: - source_labels: - __meta_kubernetes_service_name - __meta_kubernetes_endpoint_port_name action: keep regex: cloud-controller-manager;metrics # common metrics - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [ __meta_kubernetes_pod_name ] target_label: pod metric_relabel_configs: - process_max_fds - process_open_fds   alerting_rules: |cloud-controller-manager.rules.yaml: | groups: - name: cloud-controller-manager.rules rules: - alert: CloudControllerManagerDown expr: absent(up{job=\"cloud-controller-manager\"} == 1) for: 15m labels: service: cloud-controller-manager severity: critical type: seed visibility: all annotations: description: All infrastructure specific operations cannot be completed (e.g. creating load balancers or persistent volumes). summary: Cloud controller manager is down. Logging In Kubernetes clusters, container logs are non-persistent and do not survive stopped and destroyed containers. Gardener addresses this problem for the components hosted in a seed cluster, by introducing its own managed logging solution. It is integrated with the Gardener monitoring stack to have all troubleshooting context in one place.\nGardener logging consists of components in three roles - log collectors and forwarders, log persistency and exploration/consumption interfaces. All of them live in the seed clusters in multiple instances:\n Logs are persisted by Loki instances deployed as StatefulSets - one per shoot namespace, if the Logging feature gate is enabled and the shoot purpose is not testing, and one in the garden namespace. The shoot instances store logs from the control plane components hosted there. The garden Loki instance is responsible for logs from the rest of the seed namespaces - kube-system, garden extension-* and others. Fluent-bit DaemonSets deployed on each seed node collect logs from it. A custom plugin takes care to distribute the collected log messages to the Loki instances that they are intended for. This allows to fetch the logs once for the whole cluster, and to distribute them afterwards. Grafana is the UI component used to explore monitoring and log data together for easier troubleshooting and in context. Grafana instances are configured to use the coresponding Loki instances, sharing the same namespace, as data providers. There is one Grafana Deployment in the garden namespace and two Deployments per shoot namespace (one exposed to the end users and one for the operators).  Logs can be produced from various sources, such as containers or systemd, and in different formats. The fluent-bit design supports configurable data pipeline to address that problem. Gardener provides such configuration for logs produced by all its core managed components as a ConfigMap. Extensions can contribute their own, specific configurations as ConfigMaps too. See for example the logging configuration for the Gardener AWS provider extension. The Gardener reconciliation loop watches such resources and updates the fluent-bit agents dynamically.\nFluent-bit log parsers and filters To integrate with Gardener logging, extensions can and should specify how fluent-bit will handle the logs produced by the managed components that they contribute to Gardener. Normally, that would require to configure a parser for the specific logging format, if none of the available is applicable, and a filter defining how to apply it. For a complete reference for the configuration options, refer to fluent-bit’s documentation.\nNote: At the moment only parser and filter configurations are supported.\nTo contribute its own configuration to the fluent-bit agents data pipelines, an extension must provide it as a ConfigMap labeled extensions.gardener.cloud/configuration=logging and deployed in the seed’s garden namespace. Unlike the monitoring stack, where configurations are deployed per shoot, here a single configuration ConfigMap is sufficient and it applies to all fluent-bit agents in the seed. Its data field can have the following properties:\n filter-kubernetes.conf - configuration for data pipeline filters parser.conf - configuration for data pipeline parsers  Note: Take care to provide the correct data pipeline elements in the coresponding data field and not to mix them.\nExample: Logging configuration for provider-specific (OpenStack) worker controller deploying a machine-controller-manager component into a shoot namespace that reuses the kubeapiserverParser defined in fluent-bit-configmap.yaml to parse the component logs\napiVersion: v1 kind: ConfigMap metadata:  name: gardener-extension-provider-openstack-logging-config  namespace: garden  labels:  extensions.gardener.cloud/configuration: logging data:  filter-kubernetes.conf: |[FILTER] Name parser Match kubernetes.machine-controller-manager*openstack-machine-controller-manager* Key_Name log Parser kubeapiserverParser Reserve_Data True How to expose logs to the users To expose logs from extension components to the users, the extension owners have to specify a modify filter which will add __gardener_multitenant_id__=operator;user entry to the log record. This entry contains all of the tenants, which have to receive this log. The tenants are semicolon separated. This specific dedicated entry will be extracted and removed from the log in the gardener fluent-bit-to-loki output plugin and added to the label set of that log. Then it will be parsed and removed from the label set. Any whitespace will be truncated during the parsing. The extension components logs can be found in Controlplane Logs Dashboard Grafana dashboard.\nExample: In this example we configure fluent-bit when it finds a log with field tag, which match the Condition, to add __gardener_multitenant_id__=operator;user into the log record.\napiVersion: v1 kind: ConfigMap metadata:  name: gardener-extension-provider-aws-logging-config  namespace: garden  labels:  extensions.gardener.cloud/configuration: logging data:  filter-kubernetes.conf: |[FILTER] Name modify Match kubernetes.* Condition Key_value_matches tag ^kubernetes\\.var\\.log\\.containers\\.(cloud-controller-manager-.+?_.+?_aws-cloud-controller-manager|csi-driver-controller-.+?_.+?_aws-csi)_.+? Add __gardener_multitenant_id__ operator;user In this case we have predefined filter which copies the log’s tag into the log record under the tag field. The tag consists of the container logs directories path, plus \u003cpod_name\u003e_\u003cshoot_controlplane_namespace\u003e_\u003ccontainer_name\u003e_\u003ccontainer_id\u003e, so here we say:\n When you see a record from pod cloud-controller-manager and some of the aws-cloud-controller-manager, csi-driver-controller or aws-csi containers add __gardener_multitenant_id__ key with operator;user value into the log record.\n Further details how to define parsers and use them with examples can be found in the following guide.\nGrafana The three types of Grafana instances found in a seed cluster are configured to expose logs of different origin in their dashboards:\n Garden Grafana dashboards expose logs from non-shoot namespaces of the seed clusters  Pod Logs Extensions Systemd Logs   Shoot User Grafana dashboards expose a subset of the logs shown to operators  Kube Apiserver Kube Controller Manager Kube Scheduler Cluster Autoscaler VPA components   Shoot Operator Grafana dashboards expose logs from the shoot cluster namespace where they belong  All user’s dashboards Kubernetes Pods    If the type of logs exposed in the Grafana instances needs to be changed, it is necessary to update the coresponding instance dashboard configurations.\nTips  Be careful to match exactly the log names that you need for a particular parser in your filters configuration. The regular expression you will supply will match names in the form kubernetes.pod_name.\u003cmetadata\u003e.container_name. If there are extensions with the same container and pod names, they will all match the same parser in a filter. That may be a desired effect, if they all share the same log format. But it will be a problem if they don’t. To solve it, either the pod or container names must be unique, and the regular expression in the filter has to match that unique pattern. A recommended approach is to prefix containers with the extension name and tune the regular expression to match it. For example, using myextension-container as container name, and a regular expression kubernetes.mypod.*myextension-container will guarantee match of the right log name. Make sure that the regular expression does not match more than you expect. For example, kubernetes.systemd.*systemd.* will match both systemd-service and systemd-monitor-service. You will want to be as specific as possible. It’s a good idea to put the logging configuration into the Helm chart that also deploys the extension controller, while the monitoring configuration can be part of the Helm chart/deployment routine that deploys the component managed by the controller.  References and additional resources  GitHub issue describing the concept Exemplary implementation (monitoring) for the GCP provider Exemplary implementation (logging) for the OpenStack provider  ","categories":"","description":"","excerpt":"Logging and Monitoring for Extensions Gardener provides an integrated …","ref":"/docs/gardener/extensions/logging-and-monitoring/","tags":"","title":"Logging And Monitoring"},{"body":"Creating/Deleting machines (VM)  Creating/Deleting machines (VM)  Setting up your usage environment Important : Creating machine Inspect status of machine Delete machine    Setting up your usage environment  Follow the steps described here  Important :  Make sure that the kubernetes/machine_objects/machine.yaml points to the same class name as the kubernetes/machine_classes/aws-machine-class.yaml.\n  Similarly kubernetes/machine_objects/aws-machine-class.yaml secret name and namespace should be same as that mentioned in kubernetes/secrets/aws-secret.yaml\n Creating machine  Modify kubernetes/machine_objects/machine.yaml as per your requirement and create the VM as shown below:  $ kubectl apply -f kubernetes/machine_objects/machine.yaml You should notice that the Machine Controller Manager has immediately picked up your manifest and started to create a new machine by talking to the cloud provider.\n Check Machine Controller Manager machines in the cluster  $ kubectl get machine NAME STATUS AGE test-machine Running 5m A new machine is created with the name provided in the kubernetes/machine_objects/machine.yaml file.\n After a few minutes (~3 minutes for AWS), you should notice a new node joining the cluster. You can verify this by running:  $ kubectl get nodes NAME STATUS AGE VERSION ip-10-250-14-52.eu-east-1.compute.internal. Ready 1m v1.8.0 This shows that a new node has successfully joined the cluster.\nInspect status of machine To inspect the status of any created machine, run the command given below.\n$ kubectl get machine test-machine -o yaml apiVersion: machine.sapcloud.io/v1alpha1 kind: Machine metadata:  annotations:  kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"machine.sapcloud.io/v1alpha1\",\"kind\":\"Machine\",\"metadata\":{\"annotations\":{},\"labels\":{\"test-label\":\"test-label\"},\"name\":\"test-machine\",\"namespace\":\"\"},\"spec\":{\"class\":{\"kind\":\"AWSMachineClass\",\"name\":\"test-aws\"}}}  clusterName: \"\"  creationTimestamp: 2017-12-27T06:58:21Z  finalizers:  - machine.sapcloud.io/operator  generation: 0  initializers: null  labels:  node: ip-10-250-14-52.eu-east-1.compute.internal  test-label: test-label  name: test-machine  namespace: \"\"  resourceVersion: \"12616948\"  selfLink: /apis/machine.sapcloud.io/v1alpha1/test-machine  uid: 535e596c-ead3-11e7-a6c0-828f843e4186 spec:  class:  kind: AWSMachineClass  name: test-aws  providerID: aws:///eu-east-1/i-00bef3f2618ffef23 status:  conditions:  - lastHeartbeatTime: 2017-12-27T07:00:46Z  lastTransitionTime: 2017-12-27T06:59:16Z  message: kubelet has sufficient disk space available  reason: KubeletHasSufficientDisk  status: \"False\"  type: OutOfDisk  - lastHeartbeatTime: 2017-12-27T07:00:46Z  lastTransitionTime: 2017-12-27T06:59:16Z  message: kubelet has sufficient memory available  reason: KubeletHasSufficientMemory  status: \"False\"  type: MemoryPressure  - lastHeartbeatTime: 2017-12-27T07:00:46Z  lastTransitionTime: 2017-12-27T06:59:16Z  message: kubelet has no disk pressure  reason: KubeletHasNoDiskPressure  status: \"False\"  type: DiskPressure  - lastHeartbeatTime: 2017-12-27T07:00:46Z  lastTransitionTime: 2017-12-27T07:00:06Z  message: kubelet is posting ready status  reason: KubeletReady  status: \"True\"  type: Ready  currentStatus:  lastUpdateTime: 2017-12-27T07:00:06Z  phase: Running  lastOperation:  description: Machine is now ready  lastUpdateTime: 2017-12-27T07:00:06Z  state: Successful  type: Create  node: ip-10-250-14-52.eu-west-1.compute.internal Delete machine To delete the VM using the kubernetes/machine_objects/machine.yaml as shown below\n$ kubectl delete -f kubernetes/machine_objects/machine.yaml Now the Machine Controller Manager picks up the manifest immediately and starts to delete the existing VM by talking to the cloud provider. The node should be detached from the cluster in a few minutes (~1min for AWS).\n","categories":"","description":"","excerpt":"Creating/Deleting machines (VM)  Creating/Deleting machines (VM) …","ref":"/docs/other-components/machine-controller-manager/docs/usage/machine/","tags":"","title":"Machine"},{"body":"machine-controller-manager  \n⚠️ We are in the progress of migrating and deprecating all the in-tree providers to OOT. Please avoid making any new feature enhancements to the intree providers. Kindly make it on the OOT providers available here. More details on adding new OOT providers can be found here.\nOverview Machine Controller Manager aka MCM is a group of cooperative controllers that manage the lifecycle of the worker machines. It is inspired by the design of Kube Controller Manager in which various sub controllers manage their respective Kubernetes Clients. MCM gives you the following benefits:\n seamlessly manage machines/nodes with a declarative API (of course, across different cloud providers) integrate generically with the cluster autoscaler plugin with tools such as the node-problem-detector transport the immutability design principle to machine/nodes implement e.g. rolling upgrades of machines/nodes  MCM supports following providers. These provider code is maintained externally (out-of-tree), and the links for the same are linked below:\n Alicloud AWS Azure Equinix Metal GCP KubeVirt Metal Stack Openstack V Sphere Yandex  It can easily be extended to support other cloud providers as well.\nExample of managing machine:\nkubectl create/get/delete machine vm1 Key terminologies Nodes/Machines/VMs are different terminologies used to represent similar things. We use these terms in the following way\n VM: A virtual machine running on any cloud provider. It could also refer to a physical machine (PM) in case of a bare metal setup. Node: Native kubernetes node objects. The objects you get to see when you do a “kubectl get nodes”. Although nodes can be either physical/virtual machines, for the purposes of our discussions it refers to a VM. Machine: A VM that is provisioned/managed by the Machine Controller Manager.  Design of Machine Controller Manager The design of the Machine Controller Manager is influenced by the Kube Controller Manager, where-in multiple sub-controllers are used to manage the Kubernetes clients.\nDesign Principles It’s designed to run in the master plane of a Kubernetes cluster. It follows the best principles and practices of writing controllers, including, but not limited to:\n Reusing code from kube-controller-manager leader election to allow HA deployments of the controller workqueues and multiple thread-workers SharedInformers that limit to minimum network calls, de-serialization and provide helpful create/update/delete events for resources rate-limiting to allow back-off in case of network outages and general instability of other cluster components sending events to respected resources for easy debugging and overview Prometheus metrics, health and (optional) profiling endpoints  Objects of Machine Controller Manager Machine Controller Manager reconciles a set of Custom Resources namely MachineDeployment, MachineSet and Machines which are managed \u0026 monitored by their controllers MachineDeployment Controller, MachineSet Controller, Machine Controller respectively along with another cooperative controller called the Safety Controller.\nMachine Controller Manager makes use of 4 CRD objects and 1 Kubernetes secret object to manage machines. They are as follows:\n   Custom ResourceObject Description     MachineClass A MachineClass represents a template that contains cloud provider specific details used to create machines.   Machine A Machine represents a VM which is backed by the cloud provider.   MachineSet A MachineSet ensures that the specified number of Machine replicas are running at a given point of time.   MachineDeployment A MachineDeployment provides a declarative update for MachineSet and Machines.   Secret A Secret here is a Kubernetes secret that stores cloudconfig (initialization scripts used to create VMs) and cloud specific credentials.    See here for CRD API Documentation\nComponents of Machine Controller Manager    Controller Description     MachineDeployment controller Machine Deployment controller reconciles the MachineDeployment objects and manages the lifecycle of MachineSet objects. MachineDeployment consumes provider specific MachineClass` in its spec.template.spec which is the template of the VM spec that would be spawned on the cloud by MCM.   MachineSet controller MachineSet controller reconciles the MachineSet objects and manages the lifecycle of Machine objects.   Safety controller There is a Safety Controller responsible for handling the unidentified or unknown behaviours from the cloud providers. Safety Controller:   freezes the MachineDeployment controller and MachineSet controller if the number of Machine objects goes beyond a certain threshold on top of Spec.replicas. It can be configured by the flag --safety-up or --safety-down and also --machine-safety-overshooting-period`.   freezes the functionality of the MCM if either of the target-apiserver or the control-apiserver is not reachable.   unfreezes the MCM automatically once situation is resolved to normal. A freeze label is applied on MachineDeployment/MachineSet to enforce the freeze condition.       Along with the above Custom Controllers and Resources, MCM requires the MachineClass to use K8s Secret that stores cloudconfig (initialization scripts used to create VMs) and cloud specific credentials. All these controllers work in an co-operative manner. They form a parent-child relationship with MachineDeployment Controller being the grandparent, MachineSet Controller being the parent, and Machine Controller being the child.\nDevelopment To start using or developing the Machine Controller Manager, see the documentation in the /docs repository, please find the index here.\nFAQ An FAQ is available here\nCluster-api Implementation  cluster-api branch of machine-controller-manager implements the machine-api aspect of the cluster-api project. Link: https://github.com/gardener/machine-controller-manager/tree/cluster-api Once cluster-api project gets stable, we may make master branch of MCM as well cluster-api compliant, with well-defined migration notes.  ","categories":"","description":"Declarative way of managing machines for Kubernetes cluster","excerpt":"Declarative way of managing machines for Kubernetes cluster","ref":"/docs/other-components/machine-controller-manager/","tags":"","title":"Machine Controller Manager"},{"body":"Frequently Asked Questions The answers in this FAQ apply to the newest (HEAD) version of Machine Controller Manager. If you’re using an older version of MCM please refer to corresponding version of this document. Few of the answers assume that the MCM being used is in conjuction with cluster-autoscaler:\nTable of Contents:   Basics\n What is Machine Controller Manager? Why is my machine deleted? What are the different sub-controllers in MCM? What is Safety Controller in MCM?    How to?\n How to install MCM in a Kubernetes cluster? How to better control the rollout process of the worker nodes? How to scale down MachineDeployment by selective deletion of machines? How to force delete a machine? How to pause the ongoing rolling-update of the machinedeployment? How to avoid garbage collection of your node?    Internals\n What is the high level design of MCM? What are the different configuration options in MCM? What are the different timeouts/configurations in a machine’s lifecycle? How is the drain of a machine implemented? How are the stateful applications drained during machine deletion? How does maxEvictRetries configuration work with drainTimeout configuration? What are the different phases of a machine?    Troubleshooting\n My machine is stuck in deletion for 1 hr, why? My machine is not joining the cluster, why?    Developer\n How should I test my code before submitting a PR? I need to change the APIs, what are the recommended steps? How can I update the dependencies of MCM?    In the context of Gardener\n How can I configure MCM using Shoot resource? How is my worker-pool spread across zones?    Basics What is Machine Controller Manager? Machine Controller Manager aka MCM is a bunch of controllers used for the lifecycle management of the worker machines. It reconciles a set of CRDs such as Machine, MachineSet, MachineDeployment which depicts the functionality of Pod, Replicaset, Deployment of the core Kubernetes respectively. Read more about it at README.\n Gardener uses MCM to manage its Kubernetes nodes of the shoot cluster. However, by design, MCM can be used independent of Gardener.  Why is my machine deleted? A machine is deleted by MCM generally for 2 reasons-\n  Machine is unhealthy for at least MachineHealthTimeout period. The default MachineHealthTimeout is 10 minutes.\n By default, a machine is considered unhealthy if any of the following node conditions - DiskPressure, KernelDeadlock, FileSystem, Readonly is set to true, or KubeletReady is set to false. However, this is something that is configurable using the following flag.    Machine is scaled down by the MachineDeployment resource.\n This is very usual when an external controller cluster-autoscaler (aka CA) is used with MCM. CA deletes the under-utilized machines by scaling down the MachineDeployment. Read more about cluster-autoscaler’s scale down behavior here.    What are the different sub-controllers in MCM? MCM mainly contains the following sub-controllers:\n MachineDeployment Controller: Responsible for reconciling the MachineDeployment objects. It manages the lifecycle of the MachineSet objects. MachineSet Controller: Responsible for reconciling the MachineSet objects. It manages the lifecycle of the Machine objects. Machine Controller: responsible for reconciling the Machine objects. It manages the lifecycle of the actual VMs/machines created in cloud/on-prem. This controller has been moved out of tree. Please refer an AWS machine controller for more info - link. Safety-controller: Responsible for handling the unidentified/unknown behaviors from the cloud providers. Please read more about its functionality below.  What is Safety Controller in MCM? Safety Controller contains following functions:\n Orphan VM handler:  It lists all the VMs in the cloud matching the tag of given cluster name and maps the VMs with the machine objects using the ProviderID field. VMs without any backing machine objects are logged and deleted after confirmation. This handler runs every 30 minutes and is configurable via machine-safety-orphan-vms-period flag.   Freeze mechanism:  Safety Controller freezes the MachineDeployment and MachineSet controller if the number of machine objects goes beyond a certain threshold on top of Spec.Replicas. It can be configured by the flag –safety-up or –safety-down and also machine-safety-overshooting-period. Safety Controller freezes the functionality of the MCM if either of the target-apiserver or the control-apiserver is not reachable. Safety Controller unfreezes the MCM automatically once situation is resolved to normal. A freeze label is applied on MachineDeployment/MachineSet to enforce the freeze condition.    How to? How to install MCM in a Kubernetes cluster? MCM can be installed in a cluster with following steps:\n  Apply all the CRDs from here\n  Apply all the deployment, role-related objects from here.\n Control cluster is the one where the machine-* objects are stored. Target cluster is where all the node objects are registered.    How to better control the rollout process of the worker nodes? MCM allows configuring the rollout of the worker machines using maxSurge and maxUnavailable fields. These fields are applicable only during the rollout process and means nothing in general scale up/down scenarios. The overall process is very similar to how the Deployment Controller manages pods during RollingUpdate.\n maxSurge refers to the number of additional machines that can be added on top of the Spec.Replicas of MachineDeployment during rollout process. maxUnavailable refers to the number of machines that can be deleted from Spec.Replicas field of the MachineDeployment during rollout process.  How to scale down MachineDeployment by selective deletion of machines? During scale down, triggered via MachineDeployment/MachineSet, MCM prefers to delete the machine/s which have the least priority set. Each machine object has an annotation machinepriority.machine.sapcloud.io set to 3 by default. Admin can reduce the priority of the given machines by changing the annotation value to 1. The next scale down by MachineDeployment shall delete the machines with the least priority first.\nHow to force delete a machine? A machine can be force deleted by adding the label force-deletion: \"True\" on the machine object before executing the actual delete command. During force deletion, MCM skips the drain function and simply triggers the deletion of the machine. This label should be used with caution as it can violate the PDBs for pods running on the machine.\nHow to pause the ongoing rolling-update of the machinedeployment? An ongoing rolling-update of the machine-deployment can be paused by using spec.paused field. See the example below:\napiVersion: machine.sapcloud.io/v1alpha1 kind: MachineDeployment metadata: name: test-machine-deployment spec: paused: true It can be unpaused again by removing the Paused field from the machine-deployment.\nHow to avoid garbage collection of your node? MCM provides an in-built safety mechanism to garbage collect VMs which have no corresponding machine object. This is done to save costs and is one of the key features of MCM. However, sometimes users might like to add nodes directly to the cluster without the help of MCM and would prefer MCM to not garbage collect such VMs. To do so they should remove/not-use tags on their VMs containing the following strings:\n kubernetes.io/cluster/ kubernetes.io/role/ kubernetes-io-cluster- kubernetes-io-role-  Internals What is the high level design of MCM? Please refer the following document.\nWhat are the different configuration options in MCM? MCM allows configuring many knobs to fine-tune its behavior according to the user’s need. Please refer to the link to check the exact configuration options.\nWhat are the different timeouts/configurations in a machine’s lifecycle? A machine’s lifecycle is governed by mainly following timeouts, which can be configured here.\n MachineDrainTimeout: Amount of time after which drain times out and the machine is force deleted. Default ~2 hours. MachineHealthTimeout: Amount of time after which an unhealthy machine is declared Failed and the machine is replaced by MachineSet controller. MachineCreationTimeout: Amount of time after which a machine creation is declared Failed and the machine is replaced by the MachineSet controller. NodeConditions: List of node conditions which if set to true for MachineHealthTimeout period, the machine is declared Failed and replaced by MachineSet controller. MaxEvictRetries: An integer number depicting the number of times a failed eviction should be retried on a pod during drain process. A pod is deleted after max-retries.  How is the drain of a machine implemented? MCM imports the functionality from the upstream Kubernetes-drain library. Although, few parts have been modified to make it work best in the context of MCM. Drain is executed before machine deletion for graceful migration of the applications. Drain internally uses the EvictionAPI to evict the pods and triggers the Deletion of pods after MachineDrainTimeout. Please note:\n Stateless pods are evicted in parallel. Stateful applications (with PVCs) are serially evicted. Please find more info in this answer below.  How are the stateful applications drained during machine deletion? Drain function serially evicts the stateful-pods. It is observed that serial eviction of stateful pods yields better overall availability of pods as the underlying cloud in most cases detaches and reattaches disks serially anyways. It is implemented in the following manner:\n Drain lists all the pods with attached volumes. It evicts very first stateful-pod and waits for its related entry in Node object’s .status.volumesAttached to be removed by KCM. It does the same for all the stateful-pods. It waits for PvDetachTimeout (default 2 minutes) for a given pod’s PVC to be removed, else moves forward.  How does maxEvictRetries configuration work with drainTimeout configuration? It is recommended to only set MachineDrainTimeout. It satisfies the related requirements. MaxEvictRetries is auto-calculated based on MachineDrainTimeout, if maxEvictRetries is not provided. Following will be the overall behavior of both configurations together:\n If maxEvictRetries isn’t set and only maxDrainTimeout is set:  MCM auto calculates the maxEvictRetries based on the drainTimeout.   If drainTimeout isn’t set and only maxEvictRetries is set:  Default drainTimeout and user provided maxEvictRetries for each pod is considered.   If both maxEvictRetries and drainTimoeut are set:  Then both will be respected.   If none are set:  Defaults are respected.    What are the different phases of a machine? A phase of a machine can be identified with Machine.Status.CurrentStatus.Phase. Following are the possible phases of a machine object:\n Pending: Machine creation call has succeeded. MCM is waiting for machine to join the cluster. CrashLoopBackOff: Machine creation call has failed. MCM will retry the operation after a minor delay. Running: Machine creation call has succeeded. Machine has joined the cluster successfully. Unknown: Machine health checks are failing, eg kubelet has stopped posting the status. Failed: Machine health checks have failed for a prolonged time. Hence it is declared failed. MachineSet controller will replace such machines immediately. Terminating: Machine is being terminated. Terminating state is set immediately when the deletion is triggered for the machine object. It also includes time when it’s being drained.  Troubleshooting My machine is stuck in deletion for 1 hr, why? In most cases, the Machine.Status.LastOperation provides information around why a machine can’t be deleted. Though following could be the reasons but not limited to:\n Pod/s with misconfigured PDBs block the drain operation. PDBs with maxUnavailable set to 0, doesn’t allow the eviction of the pods. Hence, drain/eviction is retried till MachineDrainTimeout. Default MachineDrainTimeout could be as large as ~2hours. Hence, blocking the machine deletion.  Short term: User can manually delete the pod in the question, with caution. Long term: Please set more appropriate PDBs which allow disruption of at least one pod.   Expired cloud credentials can block the deletion of the machine from infrastructure. Cloud provider can’t delete the machine due to internal errors. Such situations are best debugged by using cloud provider specific CLI or cloud console.  My machine is not joining the cluster, why? In most cases, the Machine.Status.LastOperation provides information around why a machine can’t be created. It could possibly be debugged with following steps:\n Verify if the machine is actually created in the cloud. User can use the Machine.Spec.ProviderId to query the machine in cloud. A Kubernetes node is generally bootstrapped with the cloud-config. Please verify, if MachineDeployment is pointing the correct MachineClass, and MachineClass is pointing to the correct Secret. The secret object contains the actual cloud-config in base64 format which will be used to boot the machine. User must also check the logs of the MCM pod to understand any broken logical flow of reconciliation.  Developer How should I test my code before submitting a PR?  Developer can locally setup the MCM using following guide Developer must also enhance the unit tests related to the incoming changes. Developer can locally run the unit test by executing:  make test-unit I need to change the APIs, what are the recommended steps? Developer should add/update the API fields at both of the following places:\n https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/types.go https://github.com/gardener/machine-controller-manager/tree/master/pkg/apis/machine/v1alpha1  Once API changes are done, auto-generate the code using following command:\n./hack/generate-code Please ignore the API-violation errors for now.\nHow can I update the dependencies of MCM? MCM uses gomod for depedency management. Developer should add/udpate depedency in the go.mod file. Please run following command to automatically revendor the dependencies.\nmake revendor In the context of Gardener How can I configure MCM using Shoot resource? All of the knobs of MCM can be configured by the workers section of the shoot resource.\n Gardener creates a MachineDeployment per zone for each worker-pool under workers section. workers.dataVolumes allows to attach multiple disks to a machine during creation. Refer the link. workers.machineControllerManager allows configuration of multiple knobs of the MachineDeployment from the shoot resource.  How is my worker-pool spread across zones? Shoot resource allows the worker-pool to spread across multiple zones using the field workers.zones. Refer link.\n Gardener creates one MachineDeployment per zone. Each MachineDeployment is initiated with the following replica:  MachineDeployment.Spec.Replicas = (Workers.Minimum)/(Number of availibility zones) ","categories":"","description":"Commonly asked questions about MCM","excerpt":"Commonly asked questions about MCM","ref":"/docs/other-components/machine-controller-manager/docs/faq/","tags":"","title":"Machine Controller Manager FAQ"},{"body":"Maintaining machine replicas using machines-deployments  Maintaining machine replicas using machines-deployments  Setting up your usage environment  Important ⚠️   Creating machine-deployment Inspect status of machine-deployment Health monitoring Update your machines  Inspect existing cluster configuration Perform a rolling update Re-check cluster configuration More variants of updates   Undo an update Pause an update Delete machine-deployment    Setting up your usage environment Follow the steps described here\nImportant ⚠️  Make sure that the kubernetes/machine_objects/machine-deployment.yaml points to the same class name as the kubernetes/machine_classes/aws-machine-class.yaml.\n  Similarly kubernetes/machine_classes/aws-machine-class.yaml secret name and namespace should be same as that mentioned in kubernetes/secrets/aws-secret.yaml\n Creating machine-deployment  Modify kubernetes/machine_objects/machine-deployment.yaml as per your requirement. Modify the number of replicas to the desired number of machines. Then, create an machine-deployment.  $ kubectl apply -f kubernetes/machine_objects/machine-deployment.yaml Now the Machine Controller Manager picks up the manifest immediately and starts to create a new machines based on the number of replicas you have provided in the manifest.\n Check Machine Controller Manager machine-deployments in the cluster  $ kubectl get machinedeployment NAME READY DESIRED UP-TO-DATE AVAILABLE AGE test-machine-deployment 3 3 3 0 10m You will notice a new machine-deployment with your given name\n Check Machine Controller Manager machine-sets in the cluster  $ kubectl get machineset NAME DESIRED CURRENT READY AGE test-machine-deployment-5bc6dd7c8f 3 3 0 10m You will notice a new machine-set backing your machine-deployment\n Check Machine Controller Manager machines in the cluster  $ kubectl get machine NAME STATUS AGE test-machine-deployment-5bc6dd7c8f-5d24b Pending 5m test-machine-deployment-5bc6dd7c8f-6mpn4 Pending 5m test-machine-deployment-5bc6dd7c8f-dpt2q Pending 5m Now you will notice N (number of replicas specified in the manifest) new machines whose name are prefixed with the machine-deployment object name that you created.\n After a few minutes (~3 minutes for AWS), you would see that new nodes have joined the cluster. You can see this using  $ kubectl get nodes NAME STATUS AGE VERSION ip-10-250-20-19.eu-west-1.compute.internal Ready 1m v1.8.0 ip-10-250-27-123.eu-west-1.compute.internal Ready 1m v1.8.0 ip-10-250-31-80.eu-west-1.compute.internal Ready 1m v1.8.0 This shows how new nodes have joined your cluster\nInspect status of machine-deployment To inspect the status of any created machine-deployment run the command below,\n$ kubectl get machinedeployment test-machine-deployment -o yaml You should get the following output.\napiVersion: machine.sapcloud.io/v1alpha1 kind: MachineDeployment metadata:  annotations:  deployment.kubernetes.io/revision: \"1\"  kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"machine.sapcloud.io/v1alpha1\",\"kind\":\"MachineDeployment\",\"metadata\":{\"annotations\":{},\"name\":\"test-machine-deployment\",\"namespace\":\"\"},\"spec\":{\"minReadySeconds\":200,\"replicas\":3,\"selector\":{\"matchLabels\":{\"test-label\":\"test-label\"}},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":1,\"maxUnavailable\":1},\"type\":\"RollingUpdate\"},\"template\":{\"metadata\":{\"labels\":{\"test-label\":\"test-label\"}},\"spec\":{\"class\":{\"kind\":\"AWSMachineClass\",\"name\":\"test-aws\"}}}}}  clusterName: \"\"  creationTimestamp: 2017-12-27T08:55:56Z  generation: 0  initializers: null  name: test-machine-deployment  namespace: \"\"  resourceVersion: \"12634168\"  selfLink: /apis/machine.sapcloud.io/v1alpha1/test-machine-deployment  uid: c0b488f7-eae3-11e7-a6c0-828f843e4186 spec:  minReadySeconds: 200  replicas: 3  selector:  matchLabels:  test-label: test-label  strategy:  rollingUpdate:  maxSurge: 1  maxUnavailable: 1  type: RollingUpdate  template:  metadata:  creationTimestamp: null  labels:  test-label: test-label  spec:  class:  kind: AWSMachineClass  name: test-aws status:  availableReplicas: 3  conditions:  - lastTransitionTime: 2017-12-27T08:57:22Z  lastUpdateTime: 2017-12-27T08:57:22Z  message: Deployment has minimum availability.  reason: MinimumReplicasAvailable  status: \"True\"  type: Available  readyReplicas: 3  replicas: 3  updatedReplicas: 3 Health monitoring Health monitor is also applied similar to how it’s described for machine-sets\nUpdate your machines Let us consider the scenario where you wish to update all nodes of your cluster from t2.xlarge machines to m5.xlarge machines. Assume that your current test-aws has its spec.machineType: t2.xlarge and your deployment test-machine-deployment points to this AWSMachineClass.\nInspect existing cluster configuration  Check Nodes present in the cluster  $ kubectl get nodes NAME STATUS AGE VERSION ip-10-250-20-19.eu-west-1.compute.internal Ready 1m v1.8.0 ip-10-250-27-123.eu-west-1.compute.internal Ready 1m v1.8.0 ip-10-250-31-80.eu-west-1.compute.internal Ready 1m v1.8.0  Check Machine Controller Manager machine-sets in the cluster. You will notice one machine-set backing your machine-deployment  $ kubectl get machineset NAME DESIRED CURRENT READY AGE test-machine-deployment-5bc6dd7c8f 3 3 3 10m  Login to your cloud provider (AWS). In the VM management console, you will find N VMs created of type t2.xlarge.  Perform a rolling update To update this machine-deployment VMs to m5.xlarge, we would do the following:\n Copy your existing aws-machine-class.yaml  cp kubernetes/machine_classes/aws-machine-class.yaml kubernetes/machine_classes/aws-machine-class-new.yaml  Modify aws-machine-class-new.yaml, and update its metadata.name: test-aws2 and spec.machineType: m5.xlarge Now create this modified MachineClass  kubectl apply -f kubernetes/machine_classes/aws-machine-class-new.yaml  Edit your existing machine-deployment  kubectl edit machinedeployment test-machine-deployment  Update from spec.template.spec.class.name: test-aws to spec.template.spec.class.name: test-aws2  Re-check cluster configuration After a few minutes (~3mins)\n Check nodes present in cluster now. They are different nodes.  $ kubectl get nodes NAME STATUS AGE VERSION ip-10-250-11-171.eu-west-1.compute.internal Ready 4m v1.8.0 ip-10-250-17-213.eu-west-1.compute.internal Ready 5m v1.8.0 ip-10-250-31-81.eu-west-1.compute.internal Ready 5m v1.8.0  Check Machine Controller Manager machine-sets in the cluster. You will notice two machine-sets backing your machine-deployment  $ kubectl get machineset NAME DESIRED CURRENT READY AGE test-machine-deployment-5bc6dd7c8f 0 0 0 1h test-machine-deployment-86ff45cc5 3 3 3 20m  Login to your cloud provider (AWS). In the VM management console, you will find N VMs created of type t2.xlarge in terminated state, and N new VMs of type m5.xlarge in running state.  This shows how a rolling update of a cluster from nodes with t2.xlarge to m5.xlarge went through.\nMore variants of updates  The above demonstration was a simple use case. This could be more complex like - updating the system disk image versions/ kubelet versions/ security patches etc. You can also play around with the maxSurge and maxUnavailable fields in machine-deployment.yaml You can also change the update strategy from rollingupdate to recreate  Undo an update  Edit the existing machine-deployment  $ kubectl edit machinedeployment test-machine-deployment  Edit the deployment to have this new field of spec.rollbackTo.revision: 0 as shown as comments in kubernetes/machine_objects/machine-deployment.yaml This will undo your update to the previous version.  Pause an update  You can also pause the update while update is going on by editing the existing machine-deployment  $ kubectl edit machinedeployment test-machine-deployment   Edit the deployment to have this new field of spec.paused: true as shown as comments in kubernetes/machine_objects/machine-deployment.yaml\n  This will pause the rollingUpdate if it’s in process\n  To resume the update, edit the deployment as mentioned above and remove the field spec.paused: true updated earlier\n  Delete machine-deployment  To delete the VM using the kubernetes/machine_objects/machine-deployment.yaml  $ kubectl delete -f kubernetes/machine_objects/machine-deployment.yaml The Machine Controller Manager picks up the manifest and starts to delete the existing VMs by talking to the cloud provider. The nodes should be detached from the cluster in a few minutes (~1min for AWS).\n","categories":"","description":"","excerpt":"Maintaining machine replicas using machines-deployments  Maintaining …","ref":"/docs/other-components/machine-controller-manager/docs/usage/machine_deployment/","tags":"","title":"Machine Deployment"},{"body":"Machine Error code handling Notational Conventions The keywords “MUST”, “MUST NOT”, “REQUIRED”, “SHALL”, “SHALL NOT”, “SHOULD”, “SHOULD NOT”, “RECOMMENDED”, “NOT RECOMMENDED”, “MAY”, and “OPTIONAL” are to be interpreted as described in RFC 2119 (Bradner, S., “Key words for use in RFCs to Indicate Requirement Levels”, BCP 14, RFC 2119, March 1997).\nThe key words “unspecified”, “undefined”, and “implementation-defined” are to be interpreted as described in the rationale for the C99 standard.\nAn implementation is not compliant if it fails to satisfy one or more of the MUST, REQUIRED, or SHALL requirements for the protocols it implements. An implementation is compliant if it satisfies all the MUST, REQUIRED, and SHALL requirements for the protocols it implements.\nTerminology    Term Definition     CR Custom Resource (CR) is defined by a cluster admin using the Kubernetes Custom Resource Definition primitive.   VM A Virtual Machine (VM) provisioned and managed by a provider. It could also refer to a physical machine in case of a bare metal provider.   Machine Machine refers to a VM that is provisioned/managed by MCM. It typically describes the metadata used to store/represent a Virtual Machine   Node Native kubernetes Node object. The objects you get to see when you do a “kubectl get nodes”. Although nodes can be either physical/virtual machines, for the purposes of our discussions it refers to a VM.   MCM Machine Controller Manager (MCM) is the controller used to manage higher level Machine Custom Resource (CR) such as machine-set and machine-deployment CRs.   Provider/Driver/MC Provider (or) Driver (or) Machine Controller (MC) is the driver responsible for managing machine objects present in the cluster from whom it manages these machines. A simple example could be creation/deletion of VM on the provider.    Pre-requisite MachineClass Resources MCM introduces the CRD MachineClass. This is a blueprint for creating machines that join a certain cluster as nodes in a certain role. The provider only works with MachineClass resources that have the structure described here.\nProviderSpec The MachineClass resource contains a providerSpec field that is passed in the ProviderSpec request field to CMI methods such as CreateMachine. The ProviderSpec can be thought of as a machine template from which the VM specification must be adopted. It can contain key-value pairs of these specs. An example for these key-value pairs are given below.\n   Parameter Mandatory Type Description     vmPool Yes string VM pool name, e.g. TEST-WOKER-POOL   size Yes string VM size, e.g. xsmall, small, etc. Each size maps to a number of CPUs and memory size.   rootFsSize No int Root (/) filesystem size in GB   tags Yes map Tags to be put on the created VM    Most of the ProviderSpec fields are not mandatory. If not specified, the provider passes an empty value in the respective Create VM parameter.\nThe tags can be used to map a VM to its corresponding machine object’s Name\nThe ProviderSpec is validated by methods that receive it as a request field for presence of all mandatory parameters and tags, and for validity of all parameters.\nSecrets The MachineClass resource also contains a secretRef field that contains a reference to a secret. The keys of this secret are passed in the Secrets request field to CMI methods.\nThe secret can contain sensitive data such as\n cloud-credentials secret data used to authenticate at the provider cloud-init scripts used to initialize a new VM. The cloud-init script is expected to contain scripts to initialize the Kubelet and make it join the cluster.  Identifying Cluster Machines To implement certain methods, the provider should be able to identify all machines associated with a particular Kubernetes cluster. This can be achieved using one/more of the below mentioned ways:\n Names of VMs created by the provider are prefixed by the cluster ID specified in the ProviderSpec. VMs created by the provider are tagged with the special tags like kubernetes.io/cluster (for the cluster ID) and kubernetes.io/role (for the role), specified in the ProviderSpec. Mapping Resource Groups to individual cluster.  Error Scheme All provider API calls defined in this spec MUST return a machine error status, which is very similar to standard machine status.\nMachine Provider Interface  The provider MUST have a unique way to map a machine object to a VM which triggers the deletion for the corresponding VM backing the machine object. The provider SHOULD have a unique way to map the ProviderSpec of a machine-class to a unique Cluster. This avoids deletion of other machines, not backed by the MCM.  CreateMachine A Provider is REQUIRED to implement this interface method. This interface method will be called by the MCM to provision a new VM on behalf of the requesting machine object.\n  This call requests the provider to create a VM backing the machine-object.\n  If VM backing the Machine.Name already exists, and is compatible with the specified Machine object in the CreateMachineRequest, the Provider MUST reply 0 OK with the corresponding CreateMachineResponse.\n  The provider can OPTIONALLY make use of the MachineClass supplied in the MachineClass in the CreateMachineRequest to communicate with the provider.\n  The provider can OPTIONALLY make use of the secrets supplied in the Secret in the CreateMachineRequest to communicate with the provider.\n  The provider can OPTIONALLY make use of the Status.LastKnownState in the Machine object to decode the state of the VM operation based on the last known state of the VM. This can be useful to restart/continue an operations which are mean’t to be atomic.\n  The provider MUST have a unique way to map a machine object to a VM. This could be implicitly provided by the provider by letting you set VM-names (or) could be explicitly specified by the provider using appropriate tags to map the same.\n  This operation SHOULD be idempotent.\n  The CreateMachineResponse returned by this method is expected to return\n ProviderID that uniquely identifys the VM at the provider. This is expected to match with the node.Spec.ProviderID on the node object. NodeName that is the expected name of the machine when it joins the cluster. It must match with the node name. LastKnownState is an OPTIONAL field that can store details of the last known state of the VM. It can be used by future operation calls to determine current infrastucture state. This state is saved on the machine object.    // CreateMachine call is responsible for VM creation on the provider CreateMachine(context.Context, *CreateMachineRequest) (*CreateMachineResponse, error)// CreateMachineRequest is the create request for VM creation type CreateMachineRequest struct {\t// Machine object from whom VM is to be created \tMachine *v1alpha1.Machine\t// MachineClass backing the machine object \tMachineClass *v1alpha1.MachineClass\t// Secret backing the machineClass object \tSecret *corev1.Secret}// CreateMachineResponse is the create response for VM creation type CreateMachineResponse struct {\t// ProviderID is the unique identification of the VM at the cloud provider. \t// ProviderID typically matches with the node.Spec.ProviderID on the node object. \t// Eg: gce://project-name/region/vm-ID \tProviderID string\t// NodeName is the name of the node-object registered to kubernetes. \tNodeName string\t// LastKnownState represents the last state of the VM during an creation/deletion error \tLastKnownState string}CreateMachine Errors If the provider is unable to complete the CreateMachine call successfully, it MUST return a non-ok ginterface method code in the machine status. If the conditions defined below are encountered, the provider MUST return the specified machine error code. The MCM MUST implement the specified error recovery behavior when it encounters the machine error code.\n   machine Code Condition Description Recovery Behavior Auto Retry Required     0 OK Successful The call was successful in creating/adopting a VM that matches supplied creation request. The CreateMachineResponse is returned with desired values  N   1 CANCELED Cancelled Call was cancelled. Perform any pending clean-up tasks and return the call  N   2 UNKNOWN Something went wrong Not enough information on what went wrong Retry operation after sometime Y   3 INVALID_ARGUMENT Re-check supplied parameters Re-check the supplied Machine.Name and ProviderSpec. Make sure all parameters are in permitted range of values. Exact issue to be given in .message Update providerSpec to fix issues. N   4 DEADLINE_EXCEEDED Timeout The call processing exceeded supplied deadline Retry operation after sometime Y   6 ALREADY_EXISTS Already exists but desired parameters doesn’t match Parameters of the existing VM don’t match the ProviderSpec Create machine with a different name N   7 PERMISSION_DENIED Insufficent permissions The requestor doesn’t have enough permissions to create an VM and it’s required dependencies Update requestor permissions to grant the same N   8 RESOURCE_EXHAUSTED Resource limits have been reached The requestor doesn’t have enough resource limits to process this creation request Enhance resource limits associated with the user/account to process this N   9 PRECONDITION_FAILED VM is in inconsistent state The VM is in a state that is invalid for this operation Manual intervention might be needed to fix the state of the VM N   10 ABORTED Operation is pending Indicates that there is already an operation pending for the specified machine Wait until previous pending operation is processed Y   11 OUT_OF_RANGE Resources were out of range The requested number of CPUs, memory size, of FS size in ProviderSpec falls outside of the corresponding valid range Update request paramaters to request valid resource requests N   12 UNIMPLEMENTED Not implemented Unimplemented indicates operation is not implemented or not supported/enabled in this service. Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state N   13 INTERNAL Major error Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken. Needs manual intervension to fix this N   14 UNAVAILABLE Not Available Unavailable indicates the service is currently unavailable. Retry operation after sometime Y   16 UNAUTHENTICATED Missing provider credentials Request does not have valid authentication credentials for the operation Fix the provider credentials N    The status message MUST contain a human readable description of error, if the status code is not OK. This string MAY be surfaced by MCM to end users.\nDeleteMachine A Provider is REQUIRED to implement this driver call. This driver call will be called by the MCM to deprovision/delete/terminate a VM backed by the requesting machine object.\n  If a VM corresponding to the specified machine-object’s name does not exist or the artifacts associated with the VM do not exist anymore (after deletion), the Provider MUST reply 0 OK.\n  The provider SHALL only act on machines belonging to the cluster-id/cluster-name obtained from the ProviderSpec.\n  The provider can OPTIONALY make use of the secrets supplied in the Secrets map in the DeleteMachineRequest to communicate with the provider.\n  The provider can OPTIONALY make use of the Spec.ProviderID map in the Machine object.\n  The provider can OPTIONALLY make use of the Status.LastKnownState in the Machine object to decode the state of the VM operation based on the last known state of the VM. This can be useful to restart/continue an operations which are mean’t to be atomic.\n  This operation SHOULD be idempotent.\n  The provider must have a unique way to map a machine object to a VM which triggers the deletion for the corresponding VM backing the machine object.\n  The DeleteMachineResponse returned by this method is expected to return\n LastKnownState is an OPTIONAL field that can store details of the last known state of the VM. It can be used by future operation calls to determine current infrastucture state. This state is saved on the machine object.    // DeleteMachine call is responsible for VM deletion/termination on the provider DeleteMachine(context.Context, *DeleteMachineRequest) (*DeleteMachineResponse, error)// DeleteMachineRequest is the delete request for VM deletion type DeleteMachineRequest struct {\t// Machine object from whom VM is to be deleted \tMachine *v1alpha1.Machine\t// MachineClass backing the machine object \tMachineClass *v1alpha1.MachineClass\t// Secret backing the machineClass object \tSecret *corev1.Secret}// DeleteMachineResponse is the delete response for VM deletion type DeleteMachineResponse struct {\t// LastKnownState represents the last state of the VM during an creation/deletion error \tLastKnownState string}DeleteMachine Errors If the provider is unable to complete the DeleteMachine call successfully, it MUST return a non-ok machine code in the machine status. If the conditions defined below are encountered, the provider MUST return the specified machine error code.\n   machine Code Condition Description Recovery Behavior Auto Retry Required     0 OK Successful The call was successful in deleting a VM that matches supplied deletion request.  N   1 CANCELED Cancelled Call was cancelled. Perform any pending clean-up tasks and return the call  N   2 UNKNOWN Something went wrong Not enough information on what went wrong Retry operation after sometime Y   3 INVALID_ARGUMENT Re-check supplied parameters Re-check the supplied Machine.Name and make sure that it is in the desired format and not a blank value. Exact issue to be given in .message Update Machine.Name to fix issues. N   4 DEADLINE_EXCEEDED Timeout The call processing exceeded supplied deadline Retry operation after sometime Y   7 PERMISSION_DENIED Insufficent permissions The requestor doesn’t have enough permissions to delete an VM and it’s required dependencies Update requestor permissions to grant the same N   9 PRECONDITION_FAILED VM is in inconsistent state The VM is in a state that is invalid for this operation Manual intervention might be needed to fix the state of the VM N   10 ABORTED Operation is pending Indicates that there is already an operation pending for the specified machine Wait until previous pending operation is processed Y   12 UNIMPLEMENTED Not implemented Unimplemented indicates operation is not implemented or not supported/enabled in this service. Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state N   13 INTERNAL Major error Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken. Needs manual intervension to fix this N   14 UNAVAILABLE Not Available Unavailable indicates the service is currently unavailable. Retry operation after sometime Y   16 UNAUTHENTICATED Missing provider credentials Request does not have valid authentication credentials for the operation Fix the provider credentials N    The status message MUST contain a human readable description of error, if the status code is not OK. This string MAY be surfaced by MCM to end users.\nGetMachineStatus A Provider can OPTIONALLY implement this driver call. Else should return a UNIMPLEMENTED status in error. This call will be invoked by the MC to get the status of a machine. This optional driver call helps in optimizing the working of the provider by avoiding unwanted calls to CreateMachine() and DeleteMachine().\n If a VM corresponding to the specified machine object’s Machine.Name exists on provider the GetMachineStatusResponse fields are to be filled similar to the CreateMachineResponse. The provider SHALL only act on machines belonging to the cluster-id/cluster-name obtained from the ProviderSpec. The provider can OPTIONALY make use of the secrets supplied in the Secrets map in the GetMachineStatusRequest to communicate with the provider. The provider can OPTIONALY make use of the VM unique ID (returned by the provider on machine creation) passed in the ProviderID map in the GetMachineStatusRequest. This operation MUST be idempotent.  // GetMachineStatus call get's the status of the VM backing the machine object on the provider GetMachineStatus(context.Context, *GetMachineStatusRequest) (*GetMachineStatusResponse, error)// GetMachineStatusRequest is the get request for VM info type GetMachineStatusRequest struct {\t// Machine object from whom VM status is to be fetched \tMachine *v1alpha1.Machine\t// MachineClass backing the machine object \tMachineClass *v1alpha1.MachineClass\t// Secret backing the machineClass object \tSecret *corev1.Secret}// GetMachineStatusResponse is the get response for VM info type GetMachineStatusResponse struct {\t// ProviderID is the unique identification of the VM at the cloud provider. \t// ProviderID typically matches with the node.Spec.ProviderID on the node object. \t// Eg: gce://project-name/region/vm-ID \tProviderID string\t// NodeName is the name of the node-object registered to kubernetes. \tNodeName string}GetMachineStatus Errors If the provider is unable to complete the GetMachineStatus call successfully, it MUST return a non-ok machine code in the machine status. If the conditions defined below are encountered, the provider MUST return the specified machine error code.\n   machine Code Condition Description Recovery Behavior Auto Retry Required     0 OK Successful The call was successful in getting machine details for given machine Machine.Name  N   1 CANCELED Cancelled Call was cancelled. Perform any pending clean-up tasks and return the call  N   2 UNKNOWN Something went wrong Not enough information on what went wrong Retry operation after sometime Y   3 INVALID_ARGUMENT Re-check supplied parameters Re-check the supplied Machine.Name and make sure that it is in the desired format and not a blank value. Exact issue to be given in .message Update Machine.Name to fix issues. N   4 DEADLINE_EXCEEDED Timeout The call processing exceeded supplied deadline Retry operation after sometime Y   5 NOT_FOUND Machine isn’t found at provider The machine could not be found at provider Not required N   7 PERMISSION_DENIED Insufficent permissions The requestor doesn’t have enough permissions to get details for the VM and it’s required dependencies Update requestor permissions to grant the same N   9 PRECONDITION_FAILED VM is in inconsistent state The VM is in a state that is invalid for this operation Manual intervention might be needed to fix the state of the VM N   11 OUT_OF_RANGE Multiple VMs found Multiple VMs found with matching machine object names Orphan VM handler to cleanup orphan VMs / Manual intervention maybe required if orphan VM handler isn’t enabled. Y   12 UNIMPLEMENTED Not implemented Unimplemented indicates operation is not implemented or not supported/enabled in this service. Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state N   13 INTERNAL Major error Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken. Needs manual intervension to fix this N   14 UNAVAILABLE Not Available Unavailable indicates the service is currently unavailable. Retry operation after sometime Y   16 UNAUTHENTICATED Missing provider credentials Request does not have valid authentication credentials for the operation Fix the provider credentials N    The status message MUST contain a human readable description of error, if the status code is not OK. This string MAY be surfaced by MCM to end users.\nListMachines A Provider can OPTIONALLY implement this driver call. Else should return a UNIMPLEMENTED status in error. The Provider SHALL return the information about all the machines associated with the MachineClass. Make sure to use appropriate filters to achieve the same to avoid data transfer overheads. This optional driver call helps in cleaning up orphan VMs present in the cluster. If not implemented, any orphan VM that might have been created incorrectly by the MCM/Provider (due to bugs in code/infra) might require manual clean up.\n If the Provider succeeded in returning a list of Machine.Name with their corresponding ProviderID, then return 0 OK. The ListMachineResponse contains a map of MachineList whose  Key is expected to contain the ProviderID \u0026 Value is expected to contain the Machine.Name corresponding to it’s kubernetes machine CR object   The provider can OPTIONALY make use of the secrets supplied in the Secrets map in the ListMachinesRequest to communicate with the provider.  // ListMachines lists all the machines that might have been created by the supplied machineClass ListMachines(context.Context, *ListMachinesRequest) (*ListMachinesResponse, error)// ListMachinesRequest is the request object to get a list of VMs belonging to a machineClass type ListMachinesRequest struct {\t// MachineClass object \tMachineClass *v1alpha1.MachineClass\t// Secret backing the machineClass object \tSecret *corev1.Secret}// ListMachinesResponse is the response object of the list of VMs belonging to a machineClass type ListMachinesResponse struct {\t// MachineList is the map of list of machines. Format for the map should be \u003cProviderID, MachineName\u003e. \tMachineList map[string]string}ListMachines Errors If the provider is unable to complete the ListMachines call successfully, it MUST return a non-ok machine code in the machine status. If the conditions defined below are encountered, the provider MUST return the specified machine error code. The MCM MUST implement the specified error recovery behavior when it encounters the machine error code.\n   machine Code Condition Description Recovery Behavior Auto Retry Required     0 OK Successful The call for listing all VMs associated with ProviderSpec was successful.  N   1 CANCELED Cancelled Call was cancelled. Perform any pending clean-up tasks and return the call  N   2 UNKNOWN Something went wrong Not enough information on what went wrong Retry operation after sometime Y   3 INVALID_ARGUMENT Re-check supplied parameters Re-check the supplied ProviderSpec and make sure that all required fields are present in their desired value format. Exact issue to be given in .message Update ProviderSpec to fix issues. N   4 DEADLINE_EXCEEDED Timeout The call processing exceeded supplied deadline Retry operation after sometime Y   7 PERMISSION_DENIED Insufficent permissions The requestor doesn’t have enough permissions to list VMs and it’s required dependencies Update requestor permissions to grant the same N   12 UNIMPLEMENTED Not implemented Unimplemented indicates operation is not implemented or not supported/enabled in this service. Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state N   13 INTERNAL Major error Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken. Needs manual intervension to fix this N   14 UNAVAILABLE Not Available Unavailable indicates the service is currently unavailable. Retry operation after sometime Y   16 UNAUTHENTICATED Missing provider credentials Request does not have valid authentication credentials for the operation Fix the provider credentials N    The status message MUST contain a human readable description of error, if the status code is not OK. This string MAY be surfaced by MCM to end users.\nGetVolumeIDs A Provider can OPTIONALLY implement this driver call. Else should return a UNIMPLEMENTED status in error. This driver call will be called by the MCM to get the VolumeIDs for the list of PersistentVolumes (PVs) supplied. This OPTIONAL (but recommended) driver call helps in serailzied eviction of pods with PVs while draining of machines. This implies applications backed by PVs would be evicted one by one, leading to shorter application downtimes.\n On succesful returnal of a list of Volume-IDs for all supplied PVSpecs, the Provider MUST reply 0 OK. The GetVolumeIDsResponse is expected to return a repeated list of strings consisting of the VolumeIDs for PVSpec that could be extracted. If for any PV the Provider wasn’t able to identify the Volume-ID, the provider MAY chose to ignore it and return the Volume-IDs for the rest of the PVs for whom the Volume-ID was found. Getting the VolumeID from the PVSpec depends on the Cloud-provider. You can extract this information by parsing the PVSpec based on the ProviderType  https://github.com/kubernetes/api/blob/release-1.15/core/v1/types.go#L297-L339 https://github.com/kubernetes/api/blob/release-1.15//core/v1/types.go#L175-L257   This operation MUST be idempotent.  // GetVolumeIDsRequest is the request object to get a list of VolumeIDs for a PVSpec type GetVolumeIDsRequest struct {\t// PVSpecsList is a list of PV specs for whom volume-IDs are required \t// Plugin should parse this raw data into pre-defined list of PVSpecs \tPVSpecs []*corev1.PersistentVolumeSpec}// GetVolumeIDsResponse is the response object of the list of VolumeIDs for a PVSpec type GetVolumeIDsResponse struct {\t// VolumeIDs is a list of VolumeIDs. \tVolumeIDs []string}GetVolumeIDs Errors    machine Code Condition Description Recovery Behavior Auto Retry Required     0 OK Successful The call getting list of VolumeIDs for the list of PersistentVolumes was successful.  N   1 CANCELED Cancelled Call was cancelled. Perform any pending clean-up tasks and return the call  N   2 UNKNOWN Something went wrong Not enough information on what went wrong Retry operation after sometime Y   3 INVALID_ARGUMENT Re-check supplied parameters Re-check the supplied PVSpecList and make sure that it is in the desired format. Exact issue to be given in .message Update PVSpecList to fix issues. N   4 DEADLINE_EXCEEDED Timeout The call processing exceeded supplied deadline Retry operation after sometime Y   12 UNIMPLEMENTED Not implemented Unimplemented indicates operation is not implemented or not supported/enabled in this service. Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state N   13 INTERNAL Major error Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken. Needs manual intervension to fix this N   14 UNAVAILABLE Not Available Unavailable indicates the service is currently unavailable. Retry operation after sometime Y    The status message MUST contain a human readable description of error, if the status code is not OK. This string MAY be surfaced by MCM to end users.\nGenerateMachineClassForMigration A Provider SHOULD implement this driver call, else it MUST return a UNIMPLEMENTED status in error. This driver call will be called by the Machine Controller to try to perform a machineClass migration for an unknown machineClass Kind. This helps in migration of one kind of machineClass to another kind. For instance an machineClass custom resource of AWSMachineClass to MachineClass.\n On successful generation of machine class the Provider MUST reply 0 OK (or) nil error. GenerateMachineClassForMigrationRequest expects the provider-specific machine class (eg. AWSMachineClass) to be supplied as the ProviderSpecificMachineClass. The provider is responsible for unmarshalling the golang struct. It also passes a reference to an existing MachineClass object. The provider is expected to fill in thisMachineClass object based on the conversions. An optional ClassSpec containing the type ClassSpec struct is also provided to decode the provider info. GenerateMachineClassForMigration is only responsible for filling up the passed MachineClass object. The task of creating the new CR of the new kind (MachineClass) with the same name as the previous one and also annotating the old machineClass CR with a migrated annotation and migrating existing references is done by the calling library implicitly. This operation MUST be idempotent.  // GenerateMachineClassForMigrationRequest is the request for generating the generic machineClass // for the provider specific machine class type GenerateMachineClassForMigrationRequest struct {\t// ProviderSpecificMachineClass is provider specfic machine class object. \t// E.g. AWSMachineClass \tProviderSpecificMachineClass interface{}\t// MachineClass is the machine class object generated that is to be filled up \tMachineClass *v1alpha1.MachineClass\t// ClassSpec contains the class spec object to determine the machineClass kind \tClassSpec *v1alpha1.ClassSpec}// GenerateMachineClassForMigrationResponse is the response for generating the generic machineClass // for the provider specific machine class type GenerateMachineClassForMigrationResponse struct{}MigrateMachineClass Errors    machine Code Condition Description Recovery Behavior Auto Retry Required     0 OK Successful Migration of provider specific machine class was successful Machine reconcilation is retried once the new class has been created Y   12 UNIMPLEMENTED Not implemented Unimplemented indicates operation is not implemented or not supported/enabled in this provider. None N   13 INTERNAL Major error Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken. Might need manual intervension to fix this Y    The status message MUST contain a human readable description of error, if the status code is not OK. This string MAY be surfaced by MCM to end users.\nConfiguration and Operation Supervised Lifecycle Management  For Providers packaged in software form:  Provider Packages SHOULD use a well-documented container image format (e.g., Docker, OCI). The chosen package image format MAY expose configurable Provider properties as environment variables, unless otherwise indicated in the section below. Variables so exposed SHOULD be assigned default values in the image manifest. A Provider Supervisor MAY programmatically evaluate or otherwise scan a Provider Package’s image manifest in order to discover configurable environment variables. A Provider SHALL NOT assume that an operator or Provider Supervisor will scan an image manifest for environment variables.    Environment Variables  Variables defined by this specification SHALL be identifiable by their MC_ name prefix. Configuration properties not defined by the MC specification SHALL NOT use the same MC_ name prefix; this prefix is reserved for common configuration properties defined by the MC specification. The Provider Supervisor SHOULD supply all RECOMMENDED MC environment variables to a Provider. The Provider Supervisor SHALL supply all REQUIRED MC environment variables to a Provider.  Logging  Providers SHOULD generate log messages to ONLY standard output and/or standard error.  In this case the Provider Supervisor SHALL assume responsibility for all log lifecycle management.   Provider implementations that deviate from the above recommendation SHALL clearly and unambiguously document the following:  Logging configuration flags and/or variables, including working sample configurations. Default log destination(s) (where do the logs go if no configuration is specified?) Log lifecycle management ownership and related guidance (size limits, rate limits, rolling, archiving, expunging, etc.) applicable to the logging mechanism embedded within the Provider.   Providers SHOULD NOT write potentially sensitive data to logs (e.g. secrets).  Available Services  Provider Packages MAY support all or a subset of CMI services; service combinations MAY be configurable at runtime by the Provider Supervisor.  This specification does not dictate the mechanism by which mode of operation MUST be discovered, and instead places that burden upon the VM Provider.   Misconfigured provider software SHOULD fail-fast with an OS-appropriate error code.  Linux Capabilities  Providers SHOULD clearly document any additionally required capabilities and/or security context.  Cgroup Isolation  A Provider MAY be constrained by cgroups.  Resource Requirements  VM Providers SHOULD unambiguously document all of a Provider’s resource requirements.  Deploying  Recommended: The MCM and Provider are typically expected to run as two containers inside a common Pod. However, for the security reasons they could execute on seperate Pods provided they have a secure way to exchange data between them.  ","categories":"","description":"","excerpt":"Machine Error code handling Notational Conventions The keywords …","ref":"/docs/other-components/machine-controller-manager/docs/development/machine_error_codes/","tags":"","title":"Machine Error Codes"},{"body":"Maintaining machine replicas using machines-sets  Maintaining machine replicas using machines-sets  Setting up your usage environment Important ⚠️ Creating machine-set Inspect status of machine-set Health monitoring Delete machine-set    Setting up your usage environment  Follow the steps described here  Important ⚠️  Make sure that the kubernetes/machines_objects/machine-set.yaml points to the same class name as the kubernetes/machine_classes/aws-machine-class.yaml.\n  Similarly kubernetes/machine_classes/aws-machine-class.yaml secret name and namespace should be same as that mentioned in kubernetes/secrets/aws-secret.yaml\n Creating machine-set  Modify kubernetes/machine_objects/machine-set.yaml as per your requirement. You can modify the number of replicas to the desired number of machines. Then, create an machine-set:  $ kubectl apply -f kubernetes/machine_objects/machine-set.yaml You should notice that the Machine Controller Manager has immediately picked up your manifest and started to create a new machines based on the number of replicas you have provided in the manifest.\n Check Machine Controller Manager machine-sets in the cluster  $ kubectl get machineset NAME DESIRED CURRENT READY AGE test-machine-set 3 3 0 1m You will see a new machine-set with your given name\n Check Machine Controller Manager machines in the cluster:  $ kubectl get machine NAME STATUS AGE test-machine-set-b57zs Pending 5m test-machine-set-c4bg8 Pending 5m test-machine-set-kvskg Pending 5m Now you will see N (number of replicas specified in the manifest) new machines whose names are prefixed with the machine-set object name that you created.\n After a few minutes (~3 minutes for AWS), you should notice new nodes joining the cluster. You can verify this by running:  $ kubectl get nodes NAME STATUS AGE VERSION ip-10-250-0-234.eu-west-1.compute.internal Ready 3m v1.8.0 ip-10-250-15-98.eu-west-1.compute.internal Ready 3m v1.8.0 ip-10-250-6-21.eu-west-1.compute.internal Ready 2m v1.8.0 This shows how new nodes have joined your cluster\nInspect status of machine-set  To inspect the status of any created machine-set run the following command:  $ kubectl get machineset test-machine-set -o yaml apiVersion: machine.sapcloud.io/v1alpha1 kind: MachineSet metadata:  annotations:  kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"machine.sapcloud.io/v1alpha1\",\"kind\":\"MachineSet\",\"metadata\":{\"annotations\":{},\"name\":\"test-machine-set\",\"namespace\":\"\",\"test-label\":\"test-label\"},\"spec\":{\"minReadySeconds\":200,\"replicas\":3,\"selector\":{\"matchLabels\":{\"test-label\":\"test-label\"}},\"template\":{\"metadata\":{\"labels\":{\"test-label\":\"test-label\"}},\"spec\":{\"class\":{\"kind\":\"AWSMachineClass\",\"name\":\"test-aws\"}}}}}  clusterName: \"\"  creationTimestamp: 2017-12-27T08:37:42Z  finalizers:  - machine.sapcloud.io/operator  generation: 0  initializers: null  name: test-machine-set  namespace: \"\"  resourceVersion: \"12630893\"  selfLink: /apis/machine.sapcloud.io/v1alpha1/test-machine-set  uid: 3469faaa-eae1-11e7-a6c0-828f843e4186 spec:  machineClass: {}  minReadySeconds: 200  replicas: 3  selector:  matchLabels:  test-label: test-label  template:  metadata:  creationTimestamp: null  labels:  test-label: test-label  spec:  class:  kind: AWSMachineClass  name: test-aws status:  availableReplicas: 3  fullyLabeledReplicas: 3  machineSetCondition: null  lastOperation:  lastUpdateTime: null  observedGeneration: 0  readyReplicas: 3  replicas: 3 Health monitoring  If you try to delete/terminate any of the machines backing the machine-set by either talking to the Machine Controller Manager or from the cloud provider, the Machine Controller Manager recreates a matching healthy machine to replace the deleted machine. Similarly, if any of your machines are unreachable or in an unhealthy state (kubelet not ready / disk pressure) for longer than the configured timeout (~ 5mins), the Machine Controller Manager recreates the nodes to replace the unhealthy nodes.  Delete machine-set  To delete the VM using the kubernetes/machine_objects/machine-set.yaml:  $ kubectl delete -f kubernetes/machine-set.yaml Now the Machine Controller Manager has immediately picked up your manifest and started to delete the existing VMs by talking to the cloud provider. Your nodes should be detached from the cluster in a few minutes (~1min for AWS).\n","categories":"","description":"","excerpt":"Maintaining machine replicas using machines-sets  Maintaining machine …","ref":"/docs/other-components/machine-controller-manager/docs/usage/machine_set/","tags":"","title":"Machine Set"},{"body":"Register Shoot as Seed An existing shoot can be registered as a seed by creating a ManagedSeed resource. This resource contains:\n The name of the shoot that should be registered as seed. An optional seedTemplate section that contains the Seed spec and parts of its metadata, such as labels and annotations. An optional gardenlet section that contains:  gardenlet deployment parameters, such as the number of replicas, the image, etc. The GardenletConfiguration resource that contains controllers configuration, feature gates, and a seedConfig section that contains the Seed spec and parts of its metadata. Additional configuration parameters, such as the garden connection bootstrap mechanism (see TLS Bootstrapping), and whether to merge the provided configuration with the configuration of the parent gardenlet.    Either the seedTemplate or the gardenlet section must be specified, but not both:\n If the seedTemplate section is specified, gardenlet is not deployed to the shoot, and a new Seed resource is created based on the template. If the gardenlet section is specified, gardenlet is deployed to the shoot, and it registers a new seed upon startup based on the seedConfig section of the GardenletConfiguration resource.  Note the following important aspects:\n Unlike the Seed resource, the ManagedSeed resource is namespaced. Currently, managed seeds are restricted to the garden namespace. The newly created Seed resource always has the same name as the ManagedSeed resource. Attempting to specify a different name in seedTemplate or seedConfig will fail. The ManagedSeed resource must always refer to an existing shoot. Attempting to create a ManagedSeed referring to a non-existing shoot will fail. A shoot that is being referred to by a ManagedSeed cannot be deleted. Attempting to delete such a shoot will fail. You can omit practically everything from the seedTemplate or gardenlet section, including all or most of the Seed spec fields. Proper defaults will be supplied in all cases, based either on the most common use cases or the information already available in the Shoot resource. Also, if your seed is configured to host HA shoot control planes, then gardenlet will be deployed with multiple replicas across nodes or availability zones by default. Some Seed spec fields, for example the provider type and region, networking CIDRs for pods, services, and nodes, etc., must be the same as the corresponding Shoot spec fields of the shoot that is being registered as seed. Attempting to use different values (except empty ones, so that they are supplied by the defaulting mechanims) will fail.  Deploying Gardenlet to the Shoot To register a shoot as a seed and deploy gardenlet to the shoot using a default configuration, create a ManagedSeed resource similar to the following:\napiVersion: seedmanagement.gardener.cloud/v1alpha1 kind: ManagedSeed metadata:  name: my-managed-seed  namespace: garden spec:  shoot:  name: crazy-botany  gardenlet: {} For an example that uses non-default configuration, see 55-managed-seed-gardenlet.yaml\nRenewing the Gardenlet Kubeconfig Secret In order to making the ManagedSeed controller renew the gardenlet’s kubeconfig secret, annotate the ManagedSeed with gardener.cloud/operation=renew-kubeconfig. This will trigger a reconciliation during which the kubeconfig secret is deleted and the bootstrapping is performed again (during which gardenlet obtains a new client certificate).\nIt is also possible to trigger the renewal on the secret directly, see here.\nCreating a Seed from a Template To register a shoot as a seed from a template without deploying gardenlet to the shoot using a default configuration, create a ManagedSeed resource similar to the following:\napiVersion: seedmanagement.gardener.cloud/v1alpha1 kind: ManagedSeed metadata:  name: my-managed-seed  namespace: garden spec:  shoot:  name: crazy-botany  seedTemplate:  spec:  dns:  ingressDomain: \"\"  networks:  pods: \"\"  services: \"\"  provider:  type: \"\"  region: \"\" For an example that uses non-default configuration, see 55-managed-seed-seedtemplate.yaml\nSpecifying apiServer replicas and autoscaler options There are few configuration options that are not supported in a Shoot resource but due to backward compatibility reasons it is possible to specify them for a Shoot that is referred by a ManagedSeed. These options are:\n   Option Description     apiServer.autoscaler.minReplicas Controls the minimum number of kube-apiserver replicas for the shoot registered as seed cluster.   apiServer.autoscaler.maxReplicas Controls the maximum number of kube-apiserver replicas for the shoot registered as seed cluster.   apiServer.replicas Controls how many kube-apiserver replicas the shoot registered as seed cluster gets by default.    It is possible to specify these options via the shoot.gardener.cloud/managed-seed-api-server annotation on the Shoot resource. Example configuration:\n annotations:  shoot.gardener.cloud/managed-seed-api-server: \"apiServer.replicas=3,apiServer.autoscaler.minReplicas=3,apiServer.autoscaler.maxReplicas=6\" Enforced configuration options The following configuration options are enforced by Gardener API server for the ManagedSeed resources:\n  The vertical pod autoscaler should be enabled from the Shoot specification.\nThe vertical pod autoscaler is a prerequisite for a Seed cluster. It is possible to enable the VPA feature for a Seed (using the Seed spec) and for a Shoot (using the Shoot spec). In context of ManagedSeeds, enabling the VPA in the Seed spec (instead of the Shoot spec) offers less flexibility and increases the network transfer and cost. Due to these reasons, the Gardener API server enforces the vertical pod autoscaler to be enabled from the Shoot specification.\n  The nginx-ingress addon should not be enabled for a Shoot referred by a ManagedSeed.\nAn Ingress controller is also a prerequisite for a Seed cluster. For a Seed cluster it is possible to enable Gardener managed Ingress controller or to deploy self-managed Ingress controller. There is also the nginx-ingress addon that can be enabled for a Shoot (using the Shoot spec). However, the Shoot nginx-ingress addon is in deprecated mode and it is not recommended for production clusters. Due to these reasons, the Gardener API server does not allow the Shoot nginx-ingress addon to be enabled for ManagedSeeds.\n  ","categories":"","description":"","excerpt":"Register Shoot as Seed An existing shoot can be registered as a seed …","ref":"/docs/gardener/usage/managed_seed/","tags":"","title":"Managed Seed"},{"body":"Deploy resources to the Shoot cluster We have introduced a component called gardener-resource-manager that is deployed as part of every shoot control plane in the seed. One of its tasks is to manage CRDs, so called ManagedResources. Managed resources contain Kubernetes resources that shall be created, reconciled, updated, and deleted by the gardener-resource-manager.\nExtension controllers may create these ManagedResources in the shoot namespace if they need to create any resource in the shoot cluster itself, for example RBAC roles (or anything else).\nWhere can I find more examples and more information how to use ManagedResources? Please take a look at the respective documentation.\n","categories":"","description":"","excerpt":"Deploy resources to the Shoot cluster We have introduced a component …","ref":"/docs/gardener/extensions/managedresources/","tags":"","title":"Managedresources"},{"body":"Manually adding a node to an existing cluster Gardener has an excellent ability to automatically scale machines for the cluster. From the point of view of scalability, there is no need for manual intervention.\nThis tutorial is useful for those end-users who need specifically configured nodes, which are not yet supported by Gardener. For example: an end-user who wants some workload that requires runnc instead of runc as container runtime.\nDisclaimer  Here we will look at the steps on how to add a node to an existing cluster without the support of Gardener. Such a node will not be managed by Gardener, and if it goes down for any reason, Gardener will not be responsible to replace it.\n How   Create a new instance in the same VPC/network as other machines in the cluster. You should be able to ssh into the machine. So save its private key, and assign a public IP to it. If adding a public IP is not preferred, then ssh into any other machine in the cluster, and then ssh from there into the new machine using its private key.\nTo ssh into a machine which is already in the cluster, use the steps defined here.\nAttach the same IAM role to the new machine which is attached to the existing machines in the cluster. This is required by kubelet in the new machine so that it can contact the cloud provider to query the node’s name.\n  On the new machine, create file /var/lib/kubelet/kubeconfig-bootstrap with the following content:\n  apiVersion: v1 kind: Config current-context: kubelet-bootstrap@default clusters: - cluster:  certificate-authority-data: \u003cCA Certificate\u003e  server: \u003cServer\u003e  name: default contexts: - context:  cluster: default  user: kubelet-bootstrap  name: kubelet-bootstrap@default users: - name: kubelet-bootstrap  user:  as-user-extra: {}  token: \u003cToken\u003e ssh into an existing node, and run these commands to get the values of and  to be replaced in above file:   \u003cServr\u003e   /opt/bin/hyperkube kubectl \\  --kubeconfig /var/lib/kubelet/kubeconfig-real \\  config view \\  -o go-template='{{index .clusters 0 \"cluster\" \"server\"}}' \\  --raw  \u003cCA Certificate\u003e  /opt/bin/hyperkube kubectl \\  --kubeconfig /var/lib/kubelet/kubeconfig-real \\  config view \\  -o go-template='{{index .clusters 0 \"cluster\" \"certificate-authority-data\"}}' \\  --raw  \u003cToken\u003e\nThe kubelet on the new machine needs a bootstrap token to authenticate with the kube-apiserver when adding itself to the cluster. Kube-apiserver uses a secret in the kube-system namespace to authenticate this token. This token is valid for 90 minutes from the time of creation, and the corresponding secret captures this detail in its .data.expiration field. The name of this secret is of the format bootstrap-token-*. Gardener takes care of creating new bootstrap tokens, and the corresponding secrets. To get an unexpired token, find the secrets with the name format bootstrap-token-* in the kube-system namespace in the cluster, and pick the one with minimum age. Eg. bootstrap-token-abcdef.\nRun these commands to get the token:\n tokenid=$(kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template='{{index .data \"token-id\"}}' | base64 --decode)   tokensecret=$(kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template='{{index .data \"token-secret\"}}' | base64 --decode)   echo $tokenid.$tokensecret The value of $TOKEN will be tokenid.tokensecret. Replace $TOKEN in above file with this value\n  Copy contents of the files - /var/lib/kubelet/config/kubelet, /var/lib/kubelet/ca.crt and /etc/systemd/system/kubelet.service - from an existing node to the new node\n  Run the following command in the new node to start the kubelet:\n  systemctl enable kubelet \u0026\u0026 systemctl start kubelet The new node should be added to the existing cluster within a couple of minutes.\n","categories":"","description":"How to add a node to an existing cluster without the support of Gardener","excerpt":"How to add a node to an existing cluster without the support of …","ref":"/docs/guides/install_gardener/add-node-to-cluster/","tags":"","title":"Manually Adding a Node to an Existing Cluster"},{"body":"Hugo uses Markdown for its simple content format. However, there are a lot of things that Markdown doesn’t support well. You could use pure HTML to expand possibilities. A typical example is reducing the original dimensions of an image.\nHowever, use HTML judicially and to the minimum extent possible. Using HTML in markdowns makes it harder to maintain and publish coherent documentation bundles. This is a job typically performed by a publishing platform mechanisms, such as Hugo’s layouts. Considering that the source documentation might be published by multiple platforms you should be considerate in using markup that may bind it to a particular one.\nWe support the use of certain shortcodes. You can find more about the currently supported shortcodes and their documentation here.\n","categories":"","description":"","excerpt":"Hugo uses Markdown for its simple content format. However, there are a …","ref":"/docs/contribute/20_documentation/25_markup/","tags":"","title":"Markdown"},{"body":"Migrate Azure Shoot Load Balancer from basic to standard SKU This guide descibes how to migrate the Load Balancer of an Azure Shoot cluster from the basic SKU to the standard SKU. Be aware: You need to delete and recreate all services of type Load Balancer, which means that the public ip addresses of your service endpoints will change. Please do this only if the Stakeholder really needs to migrate this Shoot to use standard Load Balancers. All new Shoot clusters will automatically use Azure Standard Load Balancers.\n Disable temporarily Gardeners reconciliation.\nThe Gardener Controller Manager need to be configured to allow ignoring Shoot clusters. This can be configured in its the ControllerManagerConfiguration via the field .controllers.shoot.respectSyncPeriodOverwrite=\"true\".  # In the Garden cluster. kubectl annotate shoot \u003cshoot-name\u003e shoot.garden.sapcloud.io/ignore=\"true\"  # In the Seed cluster. kubectl -n \u003cshoot-namespace\u003e scale deployment gardener-resource-manager --replicas=0 Backup all Kubernetes services of type Load Balancer.  # In the Shoot cluster. # Determine all Load Balancer services. kubectl get service --all-namespaces | grep LoadBalancer  # Backup each Load Balancer service. echo \"---\" \u003e\u003e service-backup.yaml \u0026\u0026 kubectl -n \u003cnamespace\u003e get service \u003cservice-name\u003e -o yaml \u003e\u003e service-backup.yaml Delete all Load Balancer services.  # In the Shoot cluster. kubectl -n \u003cnamespace\u003e delete service \u003cservice-name\u003e Wait until until Load Balancer is deleted. Wait until all services of type Load Balancer are deleted and the Azure Load Balancer resource is also deleted. Check via the Azure Portal if the Load Balancer within the Shoot Resource Group has been deleted. This should happen automatically after all Kubernetes Load Balancer service are gone within a few minutes.  Alternatively the Azure cli can be used to check the Load Balancer in the Shoot Resource Group. The credentials to configure the cli are available on the Seed cluster in the Shoot namespace.\n# In the Seed cluster. # Fetch the credentials from cloudprovider secret. kubectl -n \u003cshoot-namespace\u003e get secret cloudprovider -o yaml  # Configure the Azure cli, with the base64 decoded values of the cloudprovider secret. az login --service-principal --username \u003cclientID\u003e --password \u003cclientSecret\u003e --tenant \u003ctenantID\u003e az account set -s \u003csubscriptionID\u003e  # Fetch the constantly the Shoot Load Balancer in the Shoot Resource Group. Wait until the resource is gone. watch 'az network lb show -g shoot--\u003cproject-name\u003e--\u003cshoot-name\u003e -n shoot--\u003cproject-name\u003e--\u003cshoot-name\u003e'  # Logout. az logout Modify the cloud-povider-config configmap in the Seed namespace of the Shoot. The key cloudprovider.conf contains the Kubernetes cloud-provider configuration. The value is a multiline string. Please change the value of the field loadBalancerSku from basic to standard. Iff the field does not exists then append loadBalancerSku: \\\"standard\\\"\\n to the value/string.  # In the Seed cluster. kubectl -n \u003cshoot-namespace\u003e edit cm cloud-provider-config Enable Gardeners reconcilation and trigger a reconciliation.  # In the Garden cluster # Enable reconcilation kubectl annotate shoot \u003cshoot-name\u003e shoot.garden.sapcloud.io/ignore- # Trigger reconcilation kubectl annotate shoot \u003cshoot-name\u003e shoot.garden.sapcloud.io/operation=\"reconcile\" Wait until the cluster has been reconciled.\nRecreate the services from the backup file. Probably you need to remove some fields from the service defintions e.g. .spec.clusterIP, .metadata.uid or .status etc.  kubectl apply -f service-backup.yaml If successful remove backup file.  # Delete the backup file. rm -f service-backup.yaml ","categories":"","description":"","excerpt":"Migrate Azure Shoot Load Balancer from basic to standard SKU This …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-azure/docs/migrate-loadbalancer/","tags":"","title":"Migrate Loadbalancer"},{"body":"Control Plane Migration Control Plane Migration is a new Gardener feature that has been recently implemented as proposed in GEP-7 Shoot Control Plane Migration. It should be properly supported by all extensions controllers. This document outlines some important points that extension maintainers should keep in mind to properly support migration in their extensions.\nOverall Principles The following principles should always be upheld:\n All state maintained by the extension that is external from the seed cluster, for example infrastructure resources in a cloud provider, DNS entries, etc., should be kept during the migration. No such state should be deleted and then recreated, as this might cause disruption in the availability of the shoot cluster. All Kubernetes resources maintained by the extension in the shoot cluster itself should also be kept during the migration. No such resources should be deleted and then recreated.  Migrate and Restore Operations Two new operations have been introduced in Gardener. They can be specified as values of the gardener.cloud/operation annotation on an extension resource to indicate that an operation different from a normal reconcile should be performed by the corresponding extension controller:\n The migrate operation is used to ask the extension controller in the source seed to stop reconciling extension resources (in case they are requeued due to errors) and perform cleanup activities, if such are required. These cleanup activities might involve removing finalizers on resources in the shoot namespace that have been previously created by the extension controller and deleting them without actually deleting any resources external to the seed cluster. The restore operation is used to ask extension controller in the destination seed to restore any state saved in the extension resource status, before performing the actual reconciliation.  Unlike the reconcile operation, extension controllers must remove the gardener.cloud/operation annotation at the end of a successful reconciliation when the current operation is migrate or restore, not at the beginning of a reconciliation.\nCleaning-up Source Seed Resources All resources in the source seed that have been created by an extension controller, for example secrets, config maps, managed resources, etc. should be properly cleaned up by the extension controller when the current operation is migrate. As mentioned above, such resources should be deleted without actually deleting any resources external to the seed cluster.\nFor many custom resources, for example MCM resources, the above requirement means in practice that any finalizers should be removed before deleting the resource, in addition to ensuring that the resource deletion is not reconciled by its respective controller if there is no finalizer. For managed resources, the above requirement means in practice that the spec.keepObjects field should be set to true before deleting the extension resource.\nHere it is assumed that any resources that contain state needed by the extension controller can be safely deleted, since any such state has been saved as described in Saving and Restoring Extension States at the end of the last successful reconciliation.\nSaving and Restoring Extension States Some extension controllers create and maintain their own state when reconciling extension resources. For example, most infrastructure controllers use Terraform and maintain the terraform state in a special config map in the shoot namespace. This state must be properly migrated to the new seed cluster during control plane migration, so that subsequent reconciliations in the new seed could find and use it appropriately.\nAll extension controllers that require such state migration must save their state in the status.state field of their extension resource at the end of a successful reconciliation. They must also restore their state from that same field upon reconciling an extension resource when the current operation is restore, as specified by the gardener.cloud/operation annotation, before performing the actual reconciliation.\nAs an example, an infrastructure controller that uses Terraform must save the terraform state in the status.state field of the Infrastructure resource. An Infrastructure resource with a properly saved state might look as follows:\napiVersion: extensions.gardener.cloud/v1alpha1 kind: Infrastructure metadata:  name: infrastructure  namespace: shoot--foo--bar spec:  type: azure  region: eu-west-1  secretRef:  name: cloudprovider  namespace: shoot--foo--bar  providerConfig:  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  resourceGroup:  name: mygroup  ... status:  state: |{ \"version\": 3, \"terraform_version\": \"0.11.14\", \"serial\": 2, \"lineage\": \"3a1e2faa-e7b6-f5f0-5043-368dd8ea6c10\", ... } Extension controllers that do not use a saved state and therefore do not require state migration could leave the status.state field as nil at the end of a successful reconciliation, and just perform a normal reconciliation when the current operation is restore.\nIn addition, extension controllers that use referenced resources (usually secrets) must also make sure that these resources are added to the status.resources field of their extension resource at the end of a successful reconciliation, so they could be properly migrated by Gardener to the destination seed.\nImplementation Details Migrate and Restore Actuator Methods Most extension controller implementations follow a common pattern where a generic Reconciler implementation delegates to an Actuator interface that contains the methods Reconcile and Delete, provided by the extension. The two new methods Migrate and Restore have been added to all such Actuator interfaces, see the infrastructure Actuator interface as an example. These methods are called by the generic reconcilers for the migrate and restore operations respectively, and should be implemented by the extension according to the above guidelines.\nOwner Checks The so called “bad case” scenario for control plane migration proposed in GEP-17 introduced the requirement for extension controllers to check whether they are currently operating in the source or destination seed during reconciliations to avoid the case in which controllers from different seeds can operate on the same IaaS resources (split brain scenario). To that end a special “owner checking” mechanism has been added to the Reconciler implementations of all extension controllers. For an example usage of this mechanism see the infrastructure Reconciler implementation. The purpose of the owner check is to interrupt reconciliations of extension controllers that do not operate in the seed that is currently configured to host the shoot’s control plane. Note that Migrate operations must not be interrupted, as they are required to clean up kubernetes resources left in the shoot’s control plane namespace and do not act on IaaS resources.\nExtension Controllers Based on Generic Actuators In practice, the implementation of many extension controllers (for example, the controlplane and worker controllers in most provider extensions) are based on a generic Actuator implementation that only delegates to extension methods for behavior that is truly provider specific. In all such cases, the Migrate and Restore methods have already been implemented properly in the generic actuators and there is nothing more to do in the extension itself.\nIn some rare cases, extension controllers based on a generic actuator might still introduce a custom Actuator implementation to override some of the generic actuator methods in order to enhance or change their behavior in a certain way. In such cases, the Migrate and Restore methods might need to be overridden as well, see the Azure controlplane controller as an example.\nExtension Controllers Not Based on Generic Actuators The implementation of some extension controllers (for example, the infrastructure controllers in all provider extensions) are not based on a generic Actuator implementation. Such extension controllers must always provide a proper implementation of the Migrate and Restore methods according to the above guidelines, see the AWS infrastructure controller as an example. In practice this might result in code duplication between the different extensions, since the Migrate and Restore code is usually not provider or OS-specific.\n","categories":"","description":"","excerpt":"Control Plane Migration Control Plane Migration is a new Gardener …","ref":"/docs/gardener/extensions/migration/","tags":"","title":"Migration"},{"body":"Migration from Gardener v0 to v1 Please refer to the document for older Gardener versions.\n","categories":"","description":"","excerpt":"Migration from Gardener v0 to v1 Please refer to the document for …","ref":"/docs/gardener/deployment/migration_v0_to_v1/","tags":"","title":"Migration V0 To V1"},{"body":"Extending the Monitoring Stack This document provides instructions to extend the Shoot cluster monitoring stack by integrating new scrape targets, alerts and dashboards.\nPlease ensure that you have understood the basic principles of Prometheus and its ecosystem before you continue.\n‼️ The purpose of the monitoring stack is to observe the behaviour of the control plane and the system components deployed by Gardener onto the worker nodes. Monitoring of custom workloads running in the cluster is out of scope.\nOverview Each Shoot cluster comes with its own monitoring stack. The following components are deployed into the seed and shoot:\n Seed  Prometheus Grafana blackbox-exporter kube-state-metrics (Seed metrics) kube-state-metrics (Shoot metrics) Alertmanager (Optional)   Shoot  node-exporter(s) kube-state-metrics blackbox-exporter    In each Seed cluster there is a Prometheus in the garden namespace responsible for collecting metrics from the Seed kubelets and cAdvisors. These metrics are provided to each Shoot Prometheus via federation.\nThe alerts for all Shoot clusters hosted on a Seed are routed to a central Alertmanger running in the garden namespace of the Seed. The purpose of this central alertmanager is to forward all important alerts to the operators of the Gardener setup.\nThe Alertmanager in the Shoot namespace on the Seed is only responsible for forwarding alerts from its Shoot cluster to a cluster owner/cluster alert receiver via email. The Alertmanager is optional and the conditions for a deployment are already described here.\nAdding New Monitoring Targets After exploring the metrics which your component provides or adding new metrics, you should be aware which metrics are required to write the needed alerts and dashboards.\nPrometheus prefers a pull based metrics collection approach and therefore the targets to observe need to be defined upfront. The targets are defined in charts/seed-monitoring/charts/prometheus/templates/config.yaml. New scrape jobs can be added in the section scrape_configs. Detailed information how to configure scrape jobs and how to use the kubernetes service discovery are available in the Prometheus documentation.\nThe job_name of a scrape job should be the name of the component e.g. kube-apiserver or vpn. The collection interval should be the default of 30s. You do not need to specify this in the configuration.\nPlease do not ingest all metrics which are provided by a component. Rather collect only those metrics which are needed to define the alerts and dashboards (i.e. whitelist). This can be achieved by adding the following metric_relabel_configs statement to your scrape jobs (replace exampleComponent with component name).\n - job_name: example-component  ...  metric_relabel_configs: {{ include \"prometheus.keep-metrics.metric-relabel-config\" .Values.allowedMetrics.exampleComponent | indent 6 }} The whitelist for the metrics of your job can be maintained in charts/seed-monitoring/charts/prometheus/values.yaml in section allowedMetrics.exampleComponent (replace exampleComponent with component name). Check the following example:\nallowedMetrics:  ...  exampleComponent:  * metrics_name_1  * metrics_name_2  ... Adding Alerts The alert definitons are located in charts/seed-monitoring/charts/prometheus/rules. There are two approaches for adding new alerts.\n Adding additional alerts for a component which already has a set of alerts. In this case you have to extend the existing rule file for the component. Adding alerts for a new component. In this case a new rule file with name scheme example-component.rules.yaml needs to be added. Add the new alert to alertInhibitionGraph.dot, add any required inhibition flows and render the new graph. To render the graph run:  dot -Tpng ./content/alertInhibitionGraph.dot -o ./content/alertInhibitionGraph.png  Create a test for the new alert. See Alert Tests.  Example alert:\ngroups: * name: example.rules  rules:  * alert: ExampleAlert  expr: absent(up{job=\"exampleJob\"} == 1)  for: 20m  labels:  service: example  severity: critical # How severe is the alert? (blocker|critical|info|warning)  type: shoot # For which topology is the alert relevant? (seed|shoot)  visibility: all # Who should receive the alerts? (all|operator|owner)  annotations:  description: A longer description of the example alert that should also explain the impact of the alert.  summary: Short summary of an example alert. If the deployment of component is optional then the alert definitions needs to be added to charts/seed-monitoring/charts/prometheus/optional-rules instead. Furthermore the alerts for component need to be activatable in charts/seed-monitoring/charts/prometheus/values.yaml via rules.optional.example-component.enabled. The default should be true.\nBasic instruction how to define alert rules can be found in the Prometheus documentation.\nRouting tree The Alertmanager is grouping incoming alerts based on labels into buckets. Each bucket has its own configuration like alert receivers, initial delaying duration or resending frequency etc. You can find more information about Alertmanager routing in the Prometheus/Alertmanager documentation. The routing trees for the Alertmanagers deployed by Gardener are depicted below.\nCentral Seed Alertmanager\n∟ main route (all alerts for all shoots on the seed will enter)  ∟ group by project and shoot name  ∟ group by visibility \"all\" and \"operator\"  ∟ group by severity \"blocker\", \"critical\", and \"info\" → route to Garden operators  ∟ group by severity \"warning\" (dropped)  ∟ group by visibility \"owner\" (dropped) Shoot Alertmanager\n∟ main route (only alerts for one Shoot will enter)  ∟ group by visibility \"all\" and \"owner\"  ∟ group by severity \"blocker\", \"critical\", and \"info\" → route to cluster alert receiver  ∟ group by severity \"warning\" (dropped, will change soon → route to cluster alert receiver)  ∟ group by visibility \"operator\" (dropped) Alert Inhibition All alerts related to components running on the Shoot workers are inhibited in case of an issue with the vpn connection, because those components can’t be scraped anymore and Prometheus will fire alerts in consequence. The components running on the workers are probably healthy and the alerts are presumably false positives. The inhibition flow is shown in the figure below. If you add a new alert make sure to add it to the diagram.\nAlert Attributes Each alert rule definition has to contain the following annotations:\n summary: A short description of the issue. description: A detailed explanation of the issue with hints to the possible root causes and the impact assessment of the issue.  In addtion each alert must contain the following labels:\n type  shoot: Components running on the Shoot worker nodes in the kube-system namespace. seed: Components running on the Seed in the Shoot namespace as part of/next to the control plane.   service  Name of the component (in lowercase) e.g. kube-apiserver, alertmanager or vpn.   severity  blocker: All issues which make the cluster entirely unusable e.g. KubeAPIServerDown or KubeSchedulerDown critical: All issues which affect single functionalities/components but not affect the cluster in its core functionality e.g. VPNDown or KubeletDown. info: All issues that do not affect the cluster or its core functionality, but if this component is down we cannot determine if a blocker alert is firing. (i.e. A component with an info level severity is a dependency for a component with a blocker severity) warning: No current existing issue, rather a hint for situations which could lead to real issue in the close future e.g. HighLatencyApiServerToWorkers or ApiServerResponseSlow.    Alert Tests To test the Prometheus alerts:\nmake test-prometheus If you want to add alert tests:\n  Create a new file in rules-tests in the form \u003calert-group-name\u003e.rules.test.yaml or if the alerts are for an existing component with existing tests, simply add the tests to the appropriate files.\n  Make sure that newly added tests succeed. See above.\n  Adding Grafana Dashboards The dashboard definition files are located in charts/seed-monitoring/charts/grafana/dashboards. Every dashboard needs its own file.\nIf you are adding a new component dashboard please also update the overview dashboard by adding a chart for its current up/down status and with a drill down option to the component dashboard.\nDashboard Structure The dashboards should be structured in the following way. The assignment of the component dashboards to the categories should be handled via dashboard tags.\n Kubernetes control plane components (Tag: control-plane)  All components which are part of the Kubernetes control plane e. g. Kube API Server, Kube Controller Manager, Kube Scheduler and Cloud Controller Manager ETCD + Backup/Restore Kubernetes Addon Manager   Node/Machine components (Tag: node/machine)  All metrics which are related to the behaviour/control of the Kubernetes nodes and kubelets Machine-Controller-Manager + Cluster Autoscaler   Networking components (Tag: network)  CoreDNS, KubeProxy, Calico, VPN, Nginx Ingress   Addon components (Tag: addon)  Cert Broker   Monitoring components (Tag: monitoring) Logging components (Tag: logging)  Mandatory Charts for Component Dashboards For each new component, its corresponding dashboard should contain the following charts in the first row, before adding custom charts for the component in the subsequent rows.\n Pod up/down status up{job=\"example-component\"} Pod/containers cpu utilization Pod/containers memorty consumption Pod/containers network i/o  These information is provided by the cAdvisor metrics. These metrics are already integrated. Please check the other dashboards for detailed information on how to query.\nChart Requirements Each chart needs to contain:\n a meaningful name a detailed description (for non trivial charts) appropriate x/y axis descriptions appropriate scaling levels for the x/y axis proper units for the x/y axis  Dashboard Parameters The following parameters should be added to all dashboards to ensure a homogeneous experience across all dashboards.\nDashboards have to …\n contain a title which refers to the component name(s) contain a timezone statement which should be the browser time contain tags which express where the component is running (seed or shoot) and to which category the component belong (see dashboard structure) contain a version statement with a value of 1 be immutable  Example dashboard configuration\n{  \"title\": \"example-component\",  \"timezone\": \"utc\",  \"tags\": [  \"seed\",  \"control-plane\"  ],  \"version\": 1,  \"editable\": \"false\" } Furthermore all dashboards should contain the following time options:\n{  \"time\": {  \"from\": \"now-1h\",  \"to\": \"now\"  },  \"timepicker\": {  \"refresh_intervals\": [  \"30s\",  \"1m\",  \"5m\"  ],  \"time_options\": [  \"5m\",  \"15m\",  \"1h\",  \"6h\",  \"12h\",  \"24h\",  \"2d\",  \"10d\"  ]  } } ","categories":"","description":"","excerpt":"Extending the Monitoring Stack This document provides instructions to …","ref":"/docs/gardener/development/monitoring-stack/","tags":"","title":"Monitoring Stack"},{"body":"GEP-19: Monitoring Stack - Migrating to the prometheus-operator Table of Contents  GEP-19: Monitoring Stack - Migrating to the prometheus-operator  Table of Contents Summary Motivation  Goals Non-Goals   Proposal  API Prometheus Operator CRDs Shoot Monitoring Seed Monitoring BYOMC (Bring your own monitoring configuration) Grafana Sidecar Migration   Alternatives    Summary As Gardener has grown, the monitoring configuration has also evolved with it. Many components must be monitored and the configuration for these components must also be managed. This poses a challenge because the configuration is distributed across the Gardener project among different folders and even different repositories (for example extensions). While it is not possible to centralize the configuration, it is possible to improve the developer experience and improve the general stability of the monitoring. This can be done by introducing the prometheus-operator. This operator will make it easier for monitoring configuration to be discovered and picked up with the use of the Custom Resources provided by the prometheus-operator. These resources can also be directly referenced in Go and be deployed by their respective components, instead of creating .yaml files in Go, or templating charts. With the addition of the prometheus-operator it should also be easier to add new features, such as Thanos.\nMotivation Simplify monitoring changes and extensions with the use of the prometheus-operator. The current extension contract is described here. This document aims to define a new contract.\nMake it easier to add new monitoring features and make new changes. For example, when using the prometheus-operator components can bring their own monitoring configuration and specify exactly how they should be monitored without needing to add this configuration directly into Prometheus.\nThe prometheus-operator handles validation of monitoring configuration. It will be more difficult to give Prometheus invalid config.\nGoals   Migrate from the current monitoring stack to the prometheus-operator.\n  Improve the monitoring extensibility and improve developer experience when adding or editing configuration. This includes the monitoring extensions in addition to core Gardener components.\n  Provide a clear direction on how monitoring resources should be managed. Currently, there are many ways that monitoring configuration is being deployed and this should be unified.\n  Improve how dashboards are discovered and provisioned for Grafana. Currently, all dashboards are appended into a single configmap. This can be an issue if the maximum configmap size of 1MiB is ever exceeded.\n  Non-Goals   Changes to the logging stack.\n  Feature parity between the current solution and the one proposed in this GEP. The new stack should provide similar monitoring coverage, but it will be very difficult to evaluate if there is feature parity. Perhaps some features will be missing, but others may be added.\n  Proposal Today, Gardener provides monitoring for shoot clusters (i.e. system components and the control plane) and for the seed cluster. The proposal is to migrate these monitoring stacks to use the prometheus-operator. The proposal is lined out below:\nAPI Use the API provided by the prometheus-operator and create Go structs.\nPrometheus Operator CRDs Deploy the prometheus-operator and its CRDs. These components can be deployed via ManagedResources. The operator itself and some other components outlined in the GEP will be deployed in a new namespace called monitoring. The CRDs for the prometheus-operator and the operator itself can be found here. This step is a prerequisite for all other steps.\nShoot Monitoring Gardener will create a monitoring stack similar to the current one with the prometheus-operator custom resources.\n  Most of the shoot monitoring is deployed via this chart. The goal is to create a similar stack, but not necessarily with feature parity, using the prometheus-operator.\n An example Prometheus object that would be deployed in a shoot’s control plane.  apiVersion: monitoring.coreos.com/v1 kind: Prometheus metadata:  labels:  app: prometheus  name: prometheus  namespace: shoot--project--name spec:  enableAdminAPI: false  logFormat: logfmt  logLevel: info  image: image:tag  paused: false  portName: web  replicas: 1  retention: 30d  routePrefix: /  serviceAccountName: prometheus  serviceMonitorNamespaceSelector:  matchExpressions:  - key: kubernetes.io/metadata.name  operator: In  values:  - shoot--project--name  podMonitorNamespaceSelector:  matchExpressions:  - key: kubernetes.io/metadata.name  operator: In  values:  - shoot--project--name  ruleNamespaceSelector:  matchExpressions:  - key: kubernetes.io/metadata.name  operator: In  values:  - shoot--project--name  serviceMonitorSelector:  matchLabels:  monitoring.gardener.cloud/monitoring-target: shoot-control-plane  podMonitorSelector:  matchLabels:  monitoring.gardener.cloud/monitoring-target: shoot-control-plane  storage:  volumeClaimTemplate:  spec:  accessModes:  - ReadWriteOnce  resources:  requests:  storage: 20Gi  version: v2.35.0   Contract between the shoot Prometheus and its configuration.\n  Prometheus can discover *Monitors in different namespaces and also by using labels.\n  In some cases, specific configuration is required (e.g. specific configuration due to K8s versions). In this case, the configuration will also be deployed in the shoot’s namespace and Prometheus will also be able to discover this configuration.\n  Prometheus must also distinguish between *Monitors relevant for shoot control plane and shoot targets. This can be done with a serviceMonitorSelector and podMonitorSelector where monitoring.gardener.cloud/monitoring-target=shoot-control-plane. For a ServiceMonitor it would look like this:\nserviceMonitorSelector:  matchLabels:  monitoring.gardener.cloud/monitoring-target: shoot-control-plane   In addition to a Prometheus, the configuration must also be created. To do this, each job in the Prometheus configuration will need to be replaced with either a ServiceMonitor, PodMonitor, or Probe. This ServiceMonitor will be picked up by the Prometheus defined in the previous step. This ServiceMonitor will scrape any service that has the label app=prometheus on the port called metrics.\napiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata:  labels:  monitoring.gardener.cloud/monitoring-target: shoot-control-plane  name: prometheus-job  namespace: shoot--project--name spec:  endpoints:  - port: metrics  selector:  matchLabels:  app: prometheus     Prometheus needs to discover targets running in the shoot cluster. Normally, this is done by changing the api_server field in the config (example). This is currently not possible with the prometheus-operator, but there is an open issue.\n  Preferred approach: A second Prometheus can be created that is running in agent mode. This Prometheus can also be deployed/managed by the prometheus-operator. The agent Prometheus can be configured to use the API Server for the shoot cluster and use service discovery in the shoot. The metrics can then be written via remote write to the “normal” Prometheus or federated. This Prometheus will also discover configuration in the same way as the other Prometheus with 1 difference. Instead of discovering configuration with the label monitoring.gardener.cloud/monitoring-target=shoot-control-plane it will find configuration with the label monitoring.gardener.cloud/monitoring-target=shoot.\n  Alternative: Use additional scrape config. In this case, the Prometheus config snippet is put into a secret and the prometheus-operator will append it to the config. The downside here is that it is only possible to have 1 additional-scrape-config per Prometheus. This could be an issue if multiple components will need to use this.\n    Deploy an optional alertmanager that is deployed whenever the end-user specifies alerting.\n  Create an Alertmanager resource\n  Create the AlertmanagerConfig\n    Health checks - The gardenlet periodically checks the status of the Prometheus StatefulSet among other components in the shoot care controller. The gardenlet knows which component to check based on labels. Since the gardenlet is no longer deploying the StatefulSet directly and rather a Prometheus resource, it does not know which labels are attached to the Prometheus StatefulSet. However, the prometheus-operator will create StatefulSets with the same labelset that the Prometheus resource has. The gardenlet will be able to discover the StatefulSet because it knows the labelset of the Prometheus resource.\n  Seed Monitoring There is a monitoring stack deployed for each seed cluster. A similar setup must also be provided using the prometheus-operator. The steps for this are very similar to the shoot monitoring.\n  Replace the Prometheis and their configuration.\n  Replace the alertmanager and its configuration.\n  BYOMC (Bring your own monitoring configuration)   In general, components should bring their own monitoring configuration. Gardener currently does this for some components such as the gardener-resource-manager. This configuration is then appended to the existing Prometheus configuration. The goal is to replace the inline yaml with PodMonitors and/or ServiceMonitors instead.\n  If alerting rules or recording rules need to be created for a component, it can bring its own PrometheusRules.\n  The same thing can potentially be done for components such as kube-state-metrics which are still currently deployed via the [seed-bootstrap].\n  If an extension requires monitoring it must bring its own configuration in the form of PodMonitors, ServiceMonitors or PrometheusRules.\n  Adding monitoring config to the control plane: In some scenarios extensions will add components to the controlplane that need to be monitored. An example of this is the provider-aws extension that deploys a cloud-controller-manager. In the current setup, if an extension needs something to be monitored in the control plane, it brings its own configmap with Prometheus config. The configmap has the label extensions.gardener.cloud/configuration=monitoring to specify that the config should be appended to the current Prometheus config. Below is an example of what this looks like for the cloud controller manager.\napiVersion: v1 kind: ConfigMap metadata:  labels:  extensions.gardener.cloud/configuration: monitoring  name: cloud-controller-manager-observability-config  namespace: shoot--project--name data:  alerting_rules: |cloud-controller-manager.rules.yaml: | groups: - name: cloud-controller-manager.rules rules: - alert: CloudControllerManagerDown expr: absent(up{job=\"cloud-controller-manager\"} == 1) for: 15m labels: service: cloud-controller-manager severity: critical type: seed visibility: all annotations: description: All infrastruture specific operations cannot be completed (e.g. creating loadbalancers or persistent volumes). summary: Cloud controller manager is down.  observedComponents: |observedPods: - podPrefix: cloud-controller-manager isExposedToUser: true  scrape_config: |- job_name: cloud-controller-manager scheme: https tls_config: insecure_skip_verify: true authorization: type: Bearer credentials_file: /var/run/secrets/gardener.cloud/shoot/token/token honor_labels: false kubernetes_sd_configs: - role: endpoints namespaces: names: [shoot--project--name] relabel_configs: - source_labels: - __meta_kubernetes_service_name - __meta_kubernetes_endpoint_port_name action: keep regex: cloud-controller-manager;metrics # common metrics - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [ __meta_kubernetes_pod_name ] target_label: pod metric_relabel_configs: - source_labels: [ __name__ ] regex: ^(rest_client_requests_total|process_max_fds|process_open_fds)$ action: keep     This configmap will be split up into 2 separate resources. One for the alerting_rules and another for the scrape_config. The alerting_rules should be converted into a PrometheusRules object. Since the scrape_config only has one job_name we will only need one ServiceMonitor or PodMonitor for this. The following is an example of how this could be done. There are multiple ways to get the same results and this is just one example.\napiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata:  labels:  cluster: shoot--project--name  name: cloud-controller-manager  namespace: shoot--project--name spec:  endpoints:  - port: metrics # scrape the service port with name `metrics`  bearerTokenFile: /var/run/secrets/gardener.cloud/shoot/token/token # could also be replaced with a secret  metricRelabelings:  - sourceLabels: [ __name__ ]  regex: ^(rest_client_requests_total|process_max_fds|process_open_fds)$  action: keep  namespaceSelector:  matchNames:  - shoot--project--name  selector:  matchLabels:  role: cloud-controller-manager # discover any service with this label apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata:  labels:  cluster: shoot--project--name  name: cloud-controller-manager-rules  namespace: shoot--project--name spec:  groups:  - name: cloud-controller-manager.rules  rules:  - alert: CloudControllerManagerDown  expr: absent(up{job=\"cloud-controller-manager\"} == 1)  for: 15m  labels:  service: cloud-controller-manager  severity: critical  type: seed  visibility: all  annotations:  description: All infrastruture specific operations cannot be completed (e.g. creating loadbalancers or persistent volumes).  summary: Cloud controller manager is down.   Adding meta monitoring for the extensions: If the extensions need to be scraped for monitoring, the extensions must bring their own Custom Resources.\n Currently the contract between extensions and gardener is that anything that needs to be scraped must have the labels: prometheus.io/scrape=true and prometheus.io/port=\u003cport\u003e. This is defined here. This is something that we can still support with a PodMonitor that will scrape any pod in a specified namespace with these labels.    Grafana Sidecar Add a sidecar to Grafana that will pickup dashboards and provision them. Each dashboard gets its own configmap.\n  Grafana in the control plane\n  Most dashboards provisioned by Grafana are the same for each shoot cluster. To avoid unnecessary duplication of configmaps, the dashboards could be added once in a single namespace. These “common” dashboards can then be discovered by each Grafana and provisioned.\n  In some cases, dashboards are more “specific” because they are related to a certain Kubernetes version.\n  Contract between dashboards in configmaps and the Grafana sidecar.\n  Label schema: monitoring.gardener.cloud/dashboard-{seed,shoot,shoot-user}=true\n  Each common dashboard will be deployed in the monitoring namespace as a configmap. If the dashboard should be provisioned by the user Grafana in a shoot cluster it should have the label monitoring.gardener.cloud/dashboard-shoot-user=true. For dashboards that should be provisioned by the operator Grafana the label monitoring.gardener.cloud/dashboard-shoot=true is required.\n  Each specific dashboard will be deployed in the shoot namespace. The configmap will use the same label scheme.\n  The Grafana sidecar must be configured with:\n   env:  - name: METHOD  value: WATCH  - name: LABEL  value: monitoring.gardener.cloud/dashboard-shoot # monitoring.gardener.cloud/dashboard-shoot-user for user Grafana  - name: FOLDER  value: /tmp/dashboards  - name: NAMESPACE  value: monitoring,\u003cshoot namespace\u003e     Grafana in the seed\n  There is also a Grafana deployed in the seed. This Grafana will be configured in a very similar way, except it will discover dashboards with a different label.\n  The seed Grafana can discover configmaps labeled with monitoring.gardener.cloud/dashboard-seed.\n  The sidecar will be configured in a similar way:\n   env:  - name: METHOD  value: WATCH  - name: LABEL  value: monitoring.gardener.cloud/dashboard-seed  - name: FOLDER  value: /tmp/dashboards  - name: NAMESPACE  value: monitoring,garden   Dashboards can have multiple labels and be provisioned in a seed and/or shoot Grafana.\n  Migration  Deploy the prometheus-operator and its custom resources. Delete the old monitoring-stack. Configure Prometheus to “reuse” the pv from the old Prometheus’s pvc. An init container will be temporarily needed for this migration. This ensures that no data is lost and provides a clean migration. Any extension or monitoring configuration that is not migrated to the prometheus-operator right away will be collected and added to an additionalScrapeConfig. Once all extensions and components have migrated, this can be dropped.  Alternatives  Continue using the current setup.  ","categories":"","description":"","excerpt":"GEP-19: Monitoring Stack - Migrating to the prometheus-operator Table …","ref":"/docs/gardener/proposals/19-migrating-to-prometheus-operator/","tags":"","title":"Monitoring Stack - Migrating to the prometheus-operator"},{"body":"…or DENY all traffic from other namespaces\nYou can configure a NetworkPolicy to deny all the traffic from other namespaces while allowing all the traffic coming from the same namespace the pod was deployed into.\nThere are many reasons why you may chose to employ Kubernetes network policies:\n Isolate multi-tenant deployments Regulatory compliance Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each other  Kubernetes network policies are application centric compared to infrastructure/network centric standard firewalls. There are no explicit CIDRs or IP addresses used for matching source or destination IP’s. Network policies build up on labels and selectors which are key concepts of Kubernetes that are used to organize (for e.g all DB tier pods of an app) and select subsets of objects.\nExample We create two nginx HTTP-Servers in two namespaces and block all traffic between the two namespaces. E.g. you are unable to get content from namespace1 if you are sitting in namespace2.\nSetup the namespaces # create two namespaces for test purpose kubectl create ns customer1 kubectl create ns customer2  # create a standard HTTP web server kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer1 kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer2  # expose the port 80 for external access kubectl expose deployment nginx --port=80 --type=NodePort -n=customer1 kubectl expose deployment nginx --port=80 --type=NodePort -n=customer2  Test without NP Create a pod with curl preinstalled inside the namespace customer1\n# create a \"bash\" pod in one namespace kubectl run -i --tty client --image=tutum/curl -n=customer1 try to curl the exposed nginx server to get the default index.html page. Execute this in the bash prompt of the pod created above.\n# get the index.html from the nginx of the namespace \"customer1\" =\u003e success curl http://nginx.customer1 # get the index.html from the nginx of the namespace \"customer2\" =\u003e success curl http://nginx.customer2 Both calls are done in a pod within namespace customer1 and both nginx servers are always reachable, no matter in what namespace.\n Test with NP Install the NetworkPolicy from your shell\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata:  name: deny-from-other-namespaces spec:  podSelector:  matchLabels:  ingress:  - from:  - podSelector: {}  it applies the policy to ALL pods in the named namespace as the spec.podSelector.matchLabels is empty and therefore selects all pods. it allows traffic from ALL pods in the named namespace, as spec.ingress.from.podSelector is empty and therefore selects all pods.  kubectl apply -f ./network-policy.yaml -n=customer1 kubectl apply -f ./network-policy.yaml -n=customer2 after this curl http://nginx.customer2 shouldn’t work anymore if you are a service inside the namespace customer1 and vice versa\nNote: This policy, once applied, will also disable all external traffic to these pods. For example you can create a service of type LoadBalancer in namespace customer1 that match the nginx pod. When you request the service by its \u003cEXTERNAL_IP\u003e:\u003cPORT\u003e, then the network policy will deny the ingress traffic from the service and the request will time out.\nMore You can get more information how to configure the NetworkPolicies on:\n Calico WebSite Kubernetes NP Recipes  ","categories":"","description":"","excerpt":"…or DENY all traffic from other namespaces\nYou can configure a …","ref":"/docs/guides/applications/network-isolation/","tags":"","title":"Namespace Isolation"},{"body":"Gardener Network Extension Gardener is an open-source project that provides a nested user model. Basically, there are two types of services provided by Gardener to its users:\n Managed: end-users only request a Kubernetes cluster (Clusters-as-a-Service) Hosted: operators utilize Gardener to provide their own managed version of Kubernetes (Cluster-Provisioner-as-a-service)  Whether an operator or an end-user, it makes sense to provide choice. For example, for an end-user it might make sense to choose a network-plugin that would support enforcing network policies (some plugins does not come with network-policy support by default). For operators however, choice only matters for delegation purposes i.e., when providing an own managed-service, it becomes important to also provide choice over which network-plugins to use.\nFurthermore, Gardener provisions clusters on different cloud-providers with different networking requirements. For example, Azure does not support Calico Networking [1], this leads to the introduction of manual exceptions in static add-on charts which is error prone and can lead to failures during upgrades.\nFinally, every provider is different, and thus the network always needs to adapt to the infrastructure needs to provide better performance. Consistency does not necessarily lie in the implementation but in the interface.\nMotivation Prior to the Network Extensibility concept, Gardener followed a mono network-plugin support model (i.e., Calico). Although this seemed to be the easier approach, it did not completely reflect the real use-case. The goal of the Gardener Network Extensions is to support different network plugins, therefore, the specification for the network resource won’t be fixed and will be customized based on the underlying network plugin.\nTo do so, a ProviderConfig field in the spec will be provided where each plugin will define. Below is an example for how to deploy Calico as the cluster network plugin.\nThe Network Extensions Resource Here is what a typical Network resource would look-like:\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: Network metadata:  name: my-network spec:  podCIDR: 100.244.0.0/16  serviceCIDR: 100.32.0.0/13  type: calico  providerConfig:  apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1  kind: NetworkConfig  backend: bird  ipam:  cidr: usePodCIDR  type: host-local The above resources is divided into two parts (more information can be found here):\n global configuration (e.g., podCIDR, serviceCIDR, and type) provider specific config (e.g., for calico we can choose to configure a bird backend)   Note: certain cloud-provider extensions might have webhooks that would modify the network-resource to fit into their network specific context. As previously mentioned, Azure does not support IPIP, as a result, the Azure provider extension implements a webhook to mutate the backend and set it to None instead of bird.\n Supporting a new Network Extension Provider To add support for another networking provider (e.g., weave, Cilium, Flannel, etc.) a network extension controller needs to be implemented which would optionally have its own custom configuration specified in the spec.providerConfig in the Network resource. For example, if support for a network plugin named gardenet is required, the following Network resource would be created:\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: Network metadata:  name: my-network spec:  podCIDR: 100.244.0.0/16  serviceCIDR: 100.32.0.0/13  type: gardenet  providerConfig:  apiVersion: gardenet.networking.extensions.gardener.cloud/v1alpha1  kind: NetworkConfig  gardenetCustomConfigField: \u003cvalue\u003e  ipam:  cidr: usePodCIDR  type: host-local Once applied, the presumably implemented Gardenet extension controller, would pick the configuration up, parse the providerConfig and create the necessary resources in the shoot.\nFor additional reference, please have a look at the networking-calico provider extension, which provides more information on how to configure the necessary charts as well as the actuators required to reconcile networking inside the Shoot cluster to the desired state.\nSupporting kube-proxy less Service Routing Some networking extensions support service routing without the kube-proxy component. This is why Gardener supports disabling of kube-proxy for service routing by setting .spec.kubernetes.kubeproxy.enabled to false in the Shoot specification. The implicit contract of the flag is: If kube-proxy is disabled then the networking extension is responsible for the service routing. The networking extensions need to handle this twofold:\n During the reconciliation of the networking resources, the extension needs to check whether kube-proxy takes care of the service routing or the networking extension itself should handle it. In case the networking extension should be responsible according to .spec.kubernetes.kubeproxy.enabled (but is unable to perform the service routing) it should raise an error during the reconciliation. If the networking extension should handle the service routing it may reconfigure itself accordingly. (optional) In case the networking extension does not support taking over the service routing (in some scenarios), it is recommended to also provide a validating admission webhook to reject corresponding changes early on. The validation may take the current operating mode of the networking extension into consideration.  References [1] Azure support for Calico Networking\n","categories":"","description":"","excerpt":"Gardener Network Extension Gardener is an open-source project that …","ref":"/docs/gardener/extensions/network/","tags":"","title":"Network"},{"body":"Network Policies in Gardener As Seed clusters can host the Kubernetes control planes of many Shoot clusters, it is necessary to isolate the control planes from each other for security reasons. Besides deploying each control plane in its own namespace, Gardener creates network policies to also isolate the networks. Essentially, network policies make sure that pods can only talk to other pods over the network they are supposed to. As such, network policies are an important part of Gardener’s tenant isolation.\nGardener deploys network policies into\n each namespace hosting the Kubernetes control plane of the Shoot cluster. the namespace dedicated to Gardener seed-wide global controllers. This namespace is often called garden and contains e.g. the Gardenlet. the kube-system namespace in the Shoot.  The aforementioned namespaces in the Seed contain a deny-all network policy that denies all ingress and egress traffic. This secure by default setting requires pods to allow network traffic. This is done by pods having labels matching to the selectors of the network policies deployed by Gardener.\nMore details on the deployed network policies can be found in the development and usage sections.\n","categories":"","description":"","excerpt":"Network Policies in Gardener As Seed clusters can host the Kubernetes …","ref":"/docs/gardener/concepts/network_policies/","tags":"","title":"Network Policies"},{"body":"Adding Cloud Providers This document provides an overview of how to integrate a new cloud provider into Gardener. Each component that requires integration has a detailed description of how to integrate it and the steps required.\nCloud Components Gardener is composed of 2 or more Kubernetes clusters:\n Shoot: These are the end-user clusters, the regular Kubernetes clusters you have seen. They provide places for your workloads to run. Seed: This is the “management” cluster. It manages the control planes of shoots by running them as native Kubernetes workloads.  These two clusters can run in the same cloud provider, but they do not need to. For example, you could run your Seed in AWS, while having one shoot in Azure, two in Google, two in Alicloud, and three in Equinix Metal.\nThe Seed cluster deploys and manages the Shoot clusters. Importantly, for this discussion, the etcd data store backing each Shoot runs as workloads inside the Seed. Thus, to use the above example, the clusters in Azure, Google, Alicloud and Equinix Metal will have their worker nodes and master nodes running in those clouds, but the etcd clusters backing them will run as separate deployments in the Seed Kubernetes cluster on AWS.\nThis distinction becomes important when preparing the integration to a new cloud provider.\nGardener Cloud Integration Gardener and its related components integrate with cloud providers at the following key lifecycle elements:\n Create/destroy/get/list machines for the Shoot Create/destroy/get/list infrastructure components for the Shoot, e.g. VPCs, subnets, routes, etc. Backup/restore etcd for the Seed via writing files to and reading them from object storage  Thus, the integrations you need for your cloud provider depend on whether you want to deploy Shoot clusters to the provider, Seed or both.\n Shoot Only: machine lifecycle management, infrastructure. Seed: etcd backup/restore  Gardener API In addition to the requirements to integrate with the cloud provider, you also need to enable the core Gardener app to receive, validate and process requests to use that cloud provider.\n Expose the cloud provider to the consumers of the Gardener API, so it can be told to use that cloud provider as an option Validate that API as requests come in Write cloud provider specific implementation (called “provider extension”)  Cloud Provider API Requirements In order for a cloud provider to integrate with Gardener, the provider must have an API to perform machine lifecycle events, specifically:\n Create a machine Destroy a machine Get information about a machine and its state List machines  In addition, if the Seed is to run on the given provider, it also must have an API to save files to block storage and retrieve them, for etcd backup/restore.\nThe current integration with cloud providers is to add their API calls to Gardener and the Machine Controller Manager. As both Gardener and the Machine Controller Manager are written in go, the cloud provider should have a go SDK. However, if it has an API that is wrappable in go, e.g. a REST API, then you can use that to integrate.\nThe Gardener team is working on bringing cloud provider integrations out-of-tree, making them pluggable, which should simplify the process and make it possible to use other SDKs.\nSummary To add a new cloud provider, you need some or all of the following. Each repository contains instructions on how to extend it to a new cloud provider.\n   Type Purpose Location Documentation     Seed or Shoot Machine Lifecycle machine-controller-manager MCM new cloud provider   Seed only etcd backup/restore etcd-backup-restore In process   All Extension implementation gardener Extension controller    ","categories":"","description":"","excerpt":"Adding Cloud Providers This document provides an overview of how to …","ref":"/docs/gardener/development/new-cloud-provider/","tags":"","title":"New Cloud Provider"},{"body":"New core.gardener.cloud/v1beta1 APIs required to extract cloud-specific/OS-specific knowledge out of Gardener core Table of Contents  Table of Contents Summary Motivation  Goals Non-Goals   Proposal  CloudProfile resource Seed resource Project resource SecretBinding resource Quota resource BackupBucket resource BackupEntry resource Shoot resource Plant resource    Summary In GEP-1 we have proposed how to (re-)design Gardener to allow providers maintaining their provider-specific knowledge out of the core tree. Meanwhile, we have progressed a lot and are about to remove the CloudBotanist interface entirely. The only missing aspect that will allow providers to really maintain their code out of the core is to design new APIs.\nThis proposal describes how the new Shoot, Seed etc. APIs will be re-designed to cope with the changes made with extensibility. We already have the new core.gardener.cloud/v1beta1 API group that will be the new default soon.\nMotivation We want to allow providers to individually maintain their specific knowledge without the necessity to touch the Gardener core code. In order to achieve the same, we have to provide proper APIs.\nGoals  Provide proper APIs to allow providers maintaining their code outside of the core codebase. Do not complicate the APIs for end-users such that they can easily create, delete, and maintain shoot clusters.  Non-Goals  Let’s try to not split everything up into too many different resources. Instead, let’s try to keep all relevant information in the same resources when possible/appropriate.  Proposal In GEP-1 we already have proposed a first version for new CloudProfile and Shoot resources. In order to deprecate the existing/old garden.sapcloud.io/v1beta1 API group (and remove it, eventually) we should move all existing resources to the new core.gardener.cloud/v1beta1 API group.\nCloudProfile resource apiVersion: core.gardener.cloud/v1beta1 kind: CloudProfile metadata:  name: cloudprofile1 spec:  type: \u003csome-provider-name\u003e # {aws,azure,gcp,...} # Optional list of labels on `Seed` resources that marks those seeds whose shoots may use this provider profile. # An empty list means that all seeds of the same provider type are supported. # This is useful for environments that are of the same type (like openstack) but may have different \"instances\"/landscapes. # seedSelector: # matchLabels: # foo: bar  kubernetes:  versions:  - version: 1.12.1  - version: 1.11.0  - version: 1.10.6  - version: 1.10.5  expirationDate: 2020-04-05T01:02:03Z # optional  machineImages:  - name: coreos  versions:  - version: 2023.5.0  - version: 1967.5.0  expirationDate: 2020-04-05T08:00:00Z  - name: ubuntu  versions:  - version: 18.04.201906170  machineTypes:  - name: m5.large  cpu: \"2\"  gpu: \"0\"  memory: 8Gi  # storage: 20Gi # optional (not needed in every environment, may only be specified if no volumeTypes have been specified)  usable: true  volumeTypes: # optional (not needed in every environment, may only be specified if no machineType has a `storage` field)  - name: gp2  class: standard  - name: io1  class: premium  regions:  - name: europe-central-1  zones: # optional (not needed in every environment)  - name: europe-central-1a  - name: europe-central-1b  - name: europe-central-1c  # unavailableMachineTypes: # optional, list of machine types defined above that are not available in this zone  # - m5.large  # unavailableVolumeTypes: # optional, list of volume types defined above that are not available in this zone  # - io1 # CA bundle that will be installed onto every shoot machine that is using this provider profile. # caBundle: | # -----BEGIN CERTIFICATE----- # ... # -----END CERTIFICATE-----  providerConfig:  \u003csome-provider-specific-cloudprofile-config\u003e  # We don't have concrete examples for every existing provider yet, but these are the proposals:  #  # Example for Alicloud:  #  # apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1  # kind: CloudProfileConfig  # machineImages:  # - name: coreos  # version: 2023.5.0  # id: coreos_2023_4_0_64_30G_alibase_20190319.vhd  #  #  # Example for AWS:  #  # apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1  # kind: CloudProfileConfig  # machineImages:  # - name: coreos  # version: 1967.5.0  # regions:  # - name: europe-central-1  # ami: ami-0f46c2ed46d8157aa  #  #  # Example for Azure:  #  # apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1  # kind: CloudProfileConfig  # machineImages:  # - name: coreos  # version: 1967.5.0  # publisher: CoreOS  # offer: CoreOS  # sku: Stable  # countFaultDomains:  # - region: westeurope  # count: 2  # countUpdateDomains:  # - region: westeurope  # count: 5  #  #  # Example for GCP:  #  # apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1  # kind: CloudProfileConfig  # machineImages:  # - name: coreos  # version: 2023.5.0  # image: projects/coreos-cloud/global/images/coreos-stable-2023-5-0-v20190312  #  #  # Example for OpenStack:  #  # apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1  # kind: CloudProfileConfig  # machineImages:  # - name: coreos  # version: 2023.5.0  # image: coreos-2023.5.0  # keyStoneURL: https://url-to-keystone/v3/  # dnsServers:  # - 10.10.10.10  # - 10.10.10.11  # dhcpDomain: foo.bar  # requestTimeout: 30s  # constraints:  # loadBalancerProviders:  # - name: haproxy  # floatingPools:  # - name: fip1  # loadBalancerClasses:  # - name: class1  # floatingSubnetID: 04eed401-f85f-4610-8041-c4835c4beea6  # floatingNetworkID: 23949a30-1cdd-4732-ba47-d03ced950acc  # subnetID: ac46c204-9d0d-4a4c-a90d-afefe40cfc35  #  #  # Example for Packet:  #  # apiVersion: packet.provider.extensions.gardener.cloud/v1alpha1  # kind: CloudProfileConfig  # machineImages:  # - name: coreos  # version: 2079.3.0  # id: d61c3912-8422-4daf-835e-854efa0062e4 Seed resource Special note: The proposal contains fields that are not yet existing in the current garden.sapcloud.io/v1beta1.Seed resource, but they should be implemented (open issues that require them are linked).\napiVersion: v1 kind: Secret metadata:  name: seed-secret  namespace: garden type: Opaque data:  kubeconfig: base64(kubeconfig-for-seed-cluster)  --- apiVersion: v1 kind: Secret metadata:  name: backup-secret  namespace: garden type: Opaque data:  # \u003csome-provider-specific data keys\u003e  # https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-backupbucket.yaml#L9-L11  # https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-infrastructure.yaml#L9-L10  # https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-backupbucket.yaml#L9-L10  # https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-backupbucket.yaml#L9  # https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-backupbucket.yaml#L9-L13  --- apiVersion: core.gardener.cloud/v1beta1 kind: Seed metadata:  name: seed1 spec:  provider:  type: \u003csome-provider-name\u003e # {aws,azure,gcp,...}  region: europe-central-1  secretRef:  name: seed-secret  namespace: garden  # Motivation for DNS section: https://github.com/gardener/gardener/issues/201.  dns:  provider: \u003csome-provider-name\u003e # {aws-route53, google-clouddns, ...}  secretName: my-dns-secret # must be in `garden` namespace  ingressDomain: seed1.dev.example.com  volume: # optional (introduced to get rid of `persistentvolume.garden.sapcloud.io/minimumSize` and `persistentvolume.garden.sapcloud.io/provider` annotations)  minimumSize: 20Gi  providers:  - name: foo  purpose: etcd-main  networks: # Seed and Shoot networks must be disjunct  nodes: 10.240.0.0/16  pods: 10.241.128.0/17  services: 10.241.0.0/17  # Shoot default networks, see also https://github.com/gardener/gardener/issues/895.  # shootDefaults:  # pods: 100.96.0.0/11  # services: 100.64.0.0/13  taints:  - key: seed.gardener.cloud/protected  - key: seed.gardener.cloud/invisible  blockCIDRs:  - 169.254.169.254/32  backup: # See https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md.  type: \u003csome-provider-name\u003e # {aws,azure,gcp,...}  # region: eu-west-1  secretRef:  name: backup-secret  namespace: garden status:  conditions:  - lastTransitionTime: \"2020-07-14T19:16:42Z\"  lastUpdateTime: \"2020-07-14T19:18:17Z\"  message: all checks passed  reason: Passed  status: \"True\"  type: Available  gardener:  id: 4c9832b3823ee6784064877d3eb10c189fc26e98a1286c0d8a5bc82169ed702c  name: gardener-controller-manager-7fhn9ikan73n-7jhka  version: 1.0.0  observedGeneration: 1 Project resource Special note: The members and viewers field of the garden.sapcloud.io/v1beta1.Project resource will be merged together into one members field. Every member will have a role that is either admin or viewer. This will allow us to add new roles without changing the API.\napiVersion: core.gardener.cloud/v1beta1 kind: Project metadata:  name: example spec:  description: Example project  members:  - apiGroup: rbac.authorization.k8s.io  kind: User  name: john.doe@example.com  role: admin  - apiGroup: rbac.authorization.k8s.io  kind: User  name: joe.doe@example.com  role: viewer  namespace: garden-example  owner:  apiGroup: rbac.authorization.k8s.io  kind: User  name: john.doe@example.com  purpose: Example project status:  observedGeneration: 1  phase: Ready SecretBinding resource Special note: No modifications needed compared to the current garden.sapcloud.io/v1beta1.SecretBinding resource.\napiVersion: v1 kind: Secret metadata:  name: secret1  namespace: garden-core type: Opaque data:  # \u003csome-provider-specific data keys\u003e  # https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-infrastructure.yaml#L14-L15  # https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-infrastructure.yaml#L9-L10  # https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-infrastructure.yaml#L14-L17  # https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-infrastructure.yaml#L14  # https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-infrastructure.yaml#L15-L18  # https://github.com/gardener/gardener-extension-provider-packet/blob/master/example/30-infrastructure.yaml#L14-L15  #  # If you use your own domain (not the default domain of your landscape) then you have to add additional keys to this secret.  # The reason is that the DNS management is not part of the Gardener core code base but externalized, hence, it might use other  # key names than Gardener itself.  # The actual values here depend on the DNS extension that is installed to your landscape.  # For example, check out https://github.com/gardener/external-dns-management and find a lot of example secret manifests here:  # https://github.com/gardener/external-dns-management/tree/master/examples  --- apiVersion: core.gardener.cloud/v1beta1 kind: SecretBinding metadata:  name: secretbinding1  namespace: garden-core secretRef:  name: secret1 # namespace: namespace-other-than-'garden-core' // optional quotas: [] # - name: quota-1 # # namespace: namespace-other-than-'garden-core' // optional Quota resource Special note: No modifications needed compared to the current garden.sapcloud.io/v1beta1.Quota resource.\napiVersion: core.gardener.cloud/v1beta1 kind: Quota metadata:  name: trial-quota  namespace: garden-trial spec:  scope:  apiGroup: core.gardener.cloud  kind: Project # clusterLifetimeDays: 14  metrics:  cpu: \"200\"  gpu: \"20\"  memory: 4000Gi  storage.standard: 8000Gi  storage.premium: 2000Gi  loadbalancer: \"100\" BackupBucket resource Special note: This new resource is cluster-scoped.\n# See also: https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md.  apiVersion: v1 kind: Secret metadata:  name: backup-operator-provider  namespace: backup-garden type: Opaque data:  # \u003csome-provider-specific data keys\u003e  # https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-backupbucket.yaml#L9-L11  # https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-backupbucket.yaml#L9-L10  # https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-backupbucket.yaml#L9-L10  # https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-backupbucket.yaml#L9  # https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-backupbucket.yaml#L9-L13  --- apiVersion: core.gardener.cloud/v1beta1 kind: BackupBucket metadata:  name: \u003cseed-provider-type\u003e-\u003cregion\u003e-\u003cseed-uid\u003e  ownerReferences:  - kind: Seed  name: seed1 spec:  provider:  type: \u003csome-provider-name\u003e # {aws,azure,gcp,...}  region: europe-central-1  seed: seed1  secretRef:  name: backup-operator-provider  namespace: backup-garden status:  lastOperation:  description: Backup bucket has been successfully reconciled.  lastUpdateTime: '2020-04-13T14:34:27Z'  progress: 100  state: Succeeded  type: Reconcile  observedGeneration: 1 BackupEntry resource Special note: This new resource is cluster-scoped.\n# See also: https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md.  apiVersion: v1 kind: Secret metadata:  name: backup-operator-provider  namespace: backup-garden type: Opaque data:  # \u003csome-provider-specific data keys\u003e  # https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-backupbucket.yaml#L9-L11  # https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-backupbucket.yaml#L9-L10  # https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-backupbucket.yaml#L9-L10  # https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-backupbucket.yaml#L9  # https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-backupbucket.yaml#L9-L13  --- apiVersion: core.gardener.cloud/v1beta1 kind: BackupEntry metadata:  name: shoot--core--crazy-botany--3ef42  namespace: garden-core  ownerReferences:  - apiVersion: core.gardener.cloud/v1beta1  blockOwnerDeletion: false  controller: true  kind: Shoot  name: crazy-botany  uid: 19a9538b-5058-11e9-b5a6-5e696cab3bc8 spec:  bucketName: cloudprofile1-random[:5]  seed: seed1 status:  lastOperation:  description: Backup entry has been successfully reconciled.  lastUpdateTime: '2020-04-13T14:34:27Z'  progress: 100  state: Succeeded  type: Reconcile  observedGeneration: 1 Shoot resource Special notes:\n kubelet configuration in the worker pools may override the default .spec.kubernetes.kubelet configuration (that applies for all worker pools if not overridden). Moved remaining control plane configuration to new .spec.provider.controlplane section.  apiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata:  name: crazy-botany  namespace: garden-core spec:  secretBindingName: secretbinding1  cloudProfileName: cloudprofile1  region: europe-central-1 # seedName: seed1  provider:  type: \u003csome-provider-name\u003e # {aws,azure,gcp,...}  infrastructureConfig:  \u003csome-provider-specific-infrastructure-config\u003e  # https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-infrastructure.yaml#L56-L64  # https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-infrastructure.yaml#L43-L53  # https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-infrastructure.yaml#L63-L71  # https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-infrastructure.yaml#L53-L57  # https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-infrastructure.yaml#L56-L64  # https://github.com/gardener/gardener-extension-provider-packet/blob/master/example/30-infrastructure.yaml#L48-L49  controlPlaneConfig:  \u003csome-provider-specific-controlplane-config\u003e  # https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-controlplane.yaml#L60-L65  # https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-controlplane.yaml#L60-L64  # https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-controlplane.yaml#L61-L66  # https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-controlplane.yaml#L59-L64  # https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-controlplane.yaml#L64-L70  # https://github.com/gardener/gardener-extension-provider-packet/blob/master/example/30-controlplane.yaml#L60-L61  workers:  - name: cpu-worker  minimum: 3  maximum: 5  # maxSurge: 1  # maxUnavailable: 0  machine:  type: m5.large  image:  name: \u003csome-os-name\u003e  version: \u003csome-os-version\u003e  # providerConfig:  # \u003csome-os-specific-configuration\u003e  volume:  type: gp2  size: 20Gi  # providerConfig:  # \u003csome-provider-specific-worker-config\u003e  # labels:  # key: value  # annotations:  # key: value  # taints: # See also https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/  # - key: foo  # value: bar  # effect: NoSchedule  # caBundle: \u003csome-ca-bundle-to-be-installed-to-all-nodes-in-this-pool\u003e  # kubernetes:  # kubelet:  # cpuCFSQuota: true  # cpuManagerPolicy: none  # podPidsLimit: 10  # featureGates:  # SomeKubernetesFeature: true  # zones: # optional, only relevant if the provider supports availability zones  # - europe-central-1a  # - europe-central-1b  kubernetes:  version: 1.15.1  # allowPrivilegedContainers: true # 'true' means that all authenticated users can use the \"gardener.privileged\" PodSecurityPolicy, allowing full unrestricted access to Pod features.  # kubeAPIServer:  # featureGates:  # SomeKubernetesFeature: true  # runtimeConfig:  # scheduling.k8s.io/v1alpha1: true  # oidcConfig:  # caBundle: |  # -----BEGIN CERTIFICATE-----  # Li4u  # -----END CERTIFICATE-----  # clientID: client-id  # groupsClaim: groups-claim  # groupsPrefix: groups-prefix  # issuerURL: https://identity.example.com  # usernameClaim: username-claim  # usernamePrefix: username-prefix  # signingAlgs: RS256,some-other-algorithm  #-#-# only usable with Kubernetes \u003e= 1.11  # requiredClaims:  # key: value  # admissionPlugins:  # - name: PodNodeSelector  # config: |  # podNodeSelectorPluginConfig:  # clusterDefaultNodeSelector: \u003cnode-selectors-labels\u003e  # namespace1: \u003cnode-selectors-labels\u003e  # namespace2: \u003cnode-selectors-labels\u003e  # auditConfig:  # auditPolicy:  # configMapRef:  # name: auditpolicy  # kubeControllerManager:  # featureGates:  # SomeKubernetesFeature: true  # horizontalPodAutoscaler:  # syncPeriod: 30s  # tolerance: 0.1  #-#-# only usable with Kubernetes \u003c 1.12  # downscaleDelay: 15m0s  # upscaleDelay: 1m0s  #-#-# only usable with Kubernetes \u003e= 1.12  # downscaleStabilization: 5m0s  # initialReadinessDelay: 30s  # cpuInitializationPeriod: 5m0s  # kubeScheduler:  # featureGates:  # SomeKubernetesFeature: true  # kubeProxy:  # featureGates:  # SomeKubernetesFeature: true  # mode: IPVS  # kubelet:  # cpuCFSQuota: true  # cpuManagerPolicy: none  # podPidsLimit: 10  # featureGates:  # SomeKubernetesFeature: true  # clusterAutoscaler:  # scaleDownUtilizationThreshold: 0.5  # scaleDownUnneededTime: 30m  # scaleDownDelayAfterAdd: 60m  # scaleDownDelayAfterFailure: 10m  # scaleDownDelayAfterDelete: 10s  # scanInterval: 10s  dns:  # When the shoot shall use a cluster domain no domain and no providers need to be provided - Gardener will  # automatically compute a correct domain.  domain: crazy-botany.core.my-custom-domain.com  providers:  - type: aws-route53  secretName: my-custom-domain-secret  domains:  include:  - my-custom-domain.com  - my-other-custom-domain.com  exclude:  - yet-another-custom-domain.com  zones:  include:  - zone-id-1  exclude:  - zone-id-2  extensions:  - type: foobar  # providerConfig:  # apiVersion: foobar.extensions.gardener.cloud/v1alpha1  # kind: FooBarConfiguration  # foo: bar  networking:  type: calico  pods: 100.96.0.0/11  services: 100.64.0.0/13  nodes: 10.250.0.0/16  # providerConfig:  # apiVersion: calico.extensions.gardener.cloud/v1alpha1  # kind: NetworkConfig  # ipam:  # type: host-local  # cidr: usePodCIDR  # backend: bird  # typha:  # enabled: true  # See also: https://github.com/gardener/gardener/blob/master/docs/proposals/03-networking.md  maintenance:  timeWindow:  begin: 220000+0100  end: 230000+0100  autoUpdate:  kubernetesVersion: true  machineImageVersion: true # hibernation: # enabled: false # schedules: # - start: \"0 20 * * *\" # Start hibernation every day at 8PM # end: \"0 6 * * *\" # Stop hibernation every day at 6AM # location: \"America/Los_Angeles\" # Specify a location for the cron to run in  addons:  nginx-ingress:  enabled: false  # loadBalancerSourceRanges: []  kubernetes-dashboard:  enabled: true  # authenticationMode: basic # allowed values: basic,token status:  conditions:  - type: APIServerAvailable  status: 'True'  lastTransitionTime: '2020-01-30T10:38:15Z'  lastUpdateTime: '2020-04-13T14:35:21Z'  reason: HealthzRequestFailed  message: API server /healthz endpoint responded with success status code. [response_time:3ms]  - type: ControlPlaneHealthy  status: 'True'  lastTransitionTime: '2020-04-02T05:18:58Z'  lastUpdateTime: '2020-04-13T14:35:21Z'  reason: ControlPlaneRunning  message: All control plane components are healthy.  - type: EveryNodeReady  status: 'True'  lastTransitionTime: '2020-04-01T16:27:21Z'  lastUpdateTime: '2020-04-13T14:35:21Z'  reason: EveryNodeReady  message: Every node registered to the cluster is ready.  - type: SystemComponentsHealthy  status: 'True'  lastTransitionTime: '2020-04-03T18:26:28Z'  lastUpdateTime: '2020-04-13T14:35:21Z'  reason: SystemComponentsRunning  message: All system components are healthy.  gardener:  id: 4c9832b3823ee6784064877d3eb10c189fc26e98a1286c0d8a5bc82169ed702c  name: gardener-controller-manager-7fhn9ikan73n-7jhka  version: 1.0.0  lastOperation:  description: Shoot cluster state has been successfully reconciled.  lastUpdateTime: '2020-04-13T14:34:27Z'  progress: 100  state: Succeeded  type: Reconcile  observedGeneration: 1  seed: seed1  hibernated: false  technicalID: shoot--core--crazy-botany  uid: d8608cfa-2856-11e8-8fdc-0a580af181af Plant resource apiVersion: v1 kind: Secret metadata:  name: crazy-plant-secret  namespace: garden-core type: Opaque data:  kubeconfig: base64(kubeconfig-for-plant-cluster)  --- apiVersion: core.gardener.cloud/v1beta1 kind: Plant metadata:  name: crazy-plant  namespace: garden-core spec:  secretRef:  name: crazy-plant-secret  endpoints:  - name: Cluster GitHub repository  purpose: management  url: https://github.com/my-org/my-cluster-repo  - name: GKE cluster page  purpose: management  url: https://console.cloud.google.com/kubernetes/clusters/details/europe-west1-b/plant?project=my-project\u0026authuser=1\u0026tab=details status:  clusterInfo:  provider:  type: gce  region: europe-west4-c  kubernetes:  version: v1.11.10-gke.5  conditions:  - lastTransitionTime: \"2020-03-01T11:31:37Z\"  lastUpdateTime: \"2020-04-14T18:00:29Z\"  message: API server /healthz endpoint responded with success status code. [response_time:8ms]  reason: HealthzRequestFailed  status: \"True\"  type: APIServerAvailable  - lastTransitionTime: \"2020-04-01T06:26:56Z\"  lastUpdateTime: \"2020-04-14T18:00:29Z\"  message: Every node registered to the cluster is ready.  reason: EveryNodeReady  status: \"True\"  type: EveryNodeReady ","categories":"","description":"","excerpt":"New core.gardener.cloud/v1beta1 APIs required to extract …","ref":"/docs/gardener/proposals/04-new-core-gardener-cloud-apis/","tags":"","title":"New Core Gardener Cloud APIs"},{"body":"Adding Support For A New Kubernetes Version This document describes the steps needed to perform in order to confidently add support for a new Kubernetes minor version.\n ⚠️ Typically, once a minor Kubernetes version vX.Y is supported by Gardener then all patch versions vX.Y.Z are also automatically supported without any required action. This is because patch versions do not introduce any new feature or API changes, so there is nothing that needs to be adapted in gardener/gardener code.\n The Kubernetes community release a new minor version roughly every 4 months. Please refer to the official documentation about their release cycles for any additional information.\nShortly before a new release, an “umbrella” issue should be opened which is used to collect the required adaptations and to track the work items. For example, #5102 can be used as a template for the issue description.\nAs you can see, the task of supporting a new Kubernetes version also includes the provider extensions maintained in the gardener GitHub organization and is not restricted to gardener/gardener only.\nGenerally, the work items can be split into two groups: The first group contains Kubernetes release-independent tasks, the second group contains tasks specific to the changes in the given Kubernetes release.\n ℹ️ Upgrading the k8s.io/* and sigs.k8s.io/controller-runtime Golang dependencies is typically tracked and worked on separately (see e.g. #4772 or #5282).\n Deriving Release-Specific Tasks Most new minor Kubernetes releases incorporate API changes, deprecations or new features. The community announces them via their change logs. In order to derive the release-specific tasks, the respective change log for the new version vX.Y has to be read and understood (for example, this document for v1.24).\nAs already mentioned, typical changes to watch out for are:\n API version promotions or deprecations Feature gate promotions or deprecations CLI flag changes for Kubernetes components New default values in resources New available fields in resources New features potentially relevant for the Gardener system Changes of labels or annotations Gardener relies on …  Obviously, this requires a certain experience and understanding of the Gardener project so that all “relevant changes” can be identified. While reading the change log, add the tasks (along with the respective PR in kubernetes/kubernetes to the umbrella issue).\n ℹ️ Some of the changes might be specific to certain cloud providers. Pay attention to those as well and add related tasks to the issue.\n List Of Release-Independent Tasks The following paragraphs describe recurring tasks that need to be performed for each new release.\nReleasing A New hyperkube Image The gardener/hyperkube repository is used to release container images consisting of the kubectl and kubelet binaries.\nRun the .ci/check-and-release script to automatically build the image (make sure Docker is running!), push the images to the GCR (make sure gcloud is configured properly!) and publish the release on GitHub (make sure git is configured properly!).\nAdapting Gardener  Allow instantiation of a Kubernetes client for the new minor version and update the README.md:  See this example commit.   Maintain the Kubernetes feature gates used for validation of Shoot resources:  The feature gates are maintained in this file. To maintain this list for new Kubernetes versions, run hack/compare-k8s-feature-gates.sh \u003cold-version\u003e \u003cnew-version\u003e (e.g. hack/compare-k8s-feature-gates.sh v1.22 v1.23). It will present 3 lists of feature gates: those added and those removed in \u003cnew-version\u003e compared to \u003cold-version\u003e and feature gates that got locked to default in \u003cnew-version\u003e. Add all added feature gates to the map with \u003cnew-version\u003e as AddedInVersion and no RemovedInVersion. For any removed feature gates, add \u003cnew-version\u003e as RemovedInVersion to the already existing feature gate in the map. For feature gates locked to default, add \u003cnew-version\u003e as LockedToDefaultInVersion to the already existing feature gate in the map. See this example commit.   Maintain the Kubernetes kube-apiserver admission plugins used for validation of Shoot resources:  The admission plugins are maintained in this file. To maintain this list for new Kubernetes versions, run hack/compare-k8s-admission-plugins.sh \u003cold-version\u003e \u003cnew-version\u003e (e.g. hack/compare-k8s-admission-plugins.sh 1.24 1.25). It will present 2 lists of admission plugins: those added and those removed in \u003cnew-version\u003e compared to \u003cold-version\u003e. Add all added admission plugins to the admissionPluginsVersionRanges map with \u003cnew-version\u003e as AddedInVersion and no RemovedInVersion. For any removed admission plugins, add \u003cnew-version\u003e as RemovedInVersion to the already existing admission plugin in the map. Flag any admission plugins that are required (plugins that must not be disabled in the Shoot spec) by setting the Required boolean variable to true for the admission plugin in the map. Flag any admission plugins that are forbidden by setting the Forbidden boolean variable to true for the admission plugin in the map.   Maintain the ServiceAccount names for the controllers part of kube-controller-manager:  The names are maintained in this file. To maintain this list for new Kubernetes versions, run hack/compare-k8s-controllers.sh \u003cold-version\u003e \u003cnew-version\u003e (e.g. hack/compare-k8s-controllers.sh 1.22 1.23). It will present 2 lists of controllers: those added and those removed in \u003cnew-version\u003e compared to \u003cold-version\u003e. Double check whether such ServiceAccount indeed appears in the kube-system namespace when creating a cluster with \u003cnew-version\u003e. Note that it sometimes might be hidden behind a default-off feature gate. You can create a local cluster with the new version using the local provider. If it appears, add all added controllers to the list based on the Kubernetes version (example). For any removed controllers, add them only to the Kubernetes version if it is low enough.   Bump the used Kubernetes version for local Shoot and local e2e test.  See this example commit.    Filing The Pull Request Work on all the tasks you have collected and validate them using the local provider. Execute the e2e tests and if everything looks good, then go ahead and file the PR (example PR). Generally, it is great if you add the PRs also to the umbrella issue so that they can be tracked more easily.\nAdapting Provider Extensions After the PR in gardener/gardener for the support of the new version has been merged, you can go ahead and work on the provider extensions.\n Actually, you can already start even if the PR is not yet merged and use the branch of your fork.\n  Revendor the github.com/gardener/gardener dependency in the extension and update the README.md. Work on release-specific tasks related to this provider.  Maintaining The cloud-controller-manager Images Some of the cloud providers are not yet using upstream cloud-controller-manager images. Instead, we build and maintain them ourselves:\n https://github.com/gardener/cloud-provider-aws https://github.com/gardener/cloud-provider-azure (since v1.23, we use the upstream image) https://github.com/gardener/cloud-provider-gcp  Until we switch to upstream images, you need to revendor the Kubernetes dependencies and release a new image. The required steps are as follows:\n Checkout the legacy-cloud-provider branch of the respective repository Bump the versions in the Dockerfile (example commit). Update the VERSION to vX.Y.Z-dev where Z is the latest available Kubernetes patch version for the vX.Y minor version. Update the k8s.io/* dependencies in the go.mod file to vX.Y.Z and run go mod vendor and go mod tidy (example commit). Checkout a new release-vX.Y branch and release it (example)   As you are already on it, it is great if you also bump the k8s.io/* dependencies for the last three minor releases as well. In this case, you need to checkout the release-vX.{Y-{1,2,3}} branches and only perform the last three steps (example branch, example commit).\n Now you need to update the new releases in the charts/images.yaml of the respective provider extension so that they are used (see this example commit for reference).\nFiling The Pull Request Again, work on all the tasks you have collected. This time, you cannot use the local provider for validation but should create real clusters on the various infrastructures. Typically, the following validations should be performed:\n Create new clusters with versions \u003c vX.Y Create new clusters with version = vX.Y Upgrade old clusters from version vX.{Y-1} to version vX.Y Delete clusters with versions \u003c vX.Y Delete clusters with version = vX.Y  If everything looks good, then go ahead and file the PR (example PR). Generally, it is again great if you add the PRs also to the umbrella issue so that they can be tracked more easily.\n","categories":"","description":"","excerpt":"Adding Support For A New Kubernetes Version This document describes …","ref":"/docs/gardener/development/new-kubernetes-version/","tags":"","title":"New Kubernetes Version"},{"body":"NodeLocalDNS Configuration This is a short guide describing how to enable DNS caching on the shoot cluster nodes.\nBackground Currently in Gardener we are using CoreDNS as a deployment that is auto-scaled horizontally to cover for QPS-intensive applications. However, doing so does not seem to be enough to completely circumvent DNS bottlenecks such as:\n Cloud provider limits for DNS lookups. Unreliable UDP connections that forces a period of timeout in case packets are dropped. Unnecessary node hopping since CoreDNS is not deployed on all nodes, and as a result DNS queries end-up traversing multiple nodes before reaching the destination server. Inefficient load-balancing of services (e.g., round-robin might not be enough when using IPTables mode) and more …  To workaround the issues described above, node-local-dns was introduced. The architecture is described below. The idea is simple:\n For new queries, the connection is upgraded from UDP to TCP and forwarded towards the cluster IP for the original CoreDNS server. For previously resolved queries, an immediate response from the same node where the requester workload / pod resides is provided.  Configuring NodeLocalDNS All that needs to be done to enable the usage of the node-local-dns feature is to set the corresponding option (spec.systemComponents.nodeLocalDNS.enabled) in the Shoot resource to true:\n... spec:  ...  systemComponents:  nodeLocalDNS:  enabled: true ... It is worth noting that:\n When migrating from IPVS to IPTables, existing pods will continue to leverage the node-local-dns cache. When migrating from IPtables to IPVS, only newer pods will be switched to the node-local-dns cache. The annotation will take effect during the next shoot reconciliation. This happens automatically once per day in the maintenance period (unless you have disabled it). During the reconfiguration of the node-local-dns there might be a short disruption in terms of domain name resolution depending on the setup. Usually, dns requests are repeated for some time as udp is an unreliable protocol, but that strictly depends on the application/way the domain name resolution happens. It is recommended to let the shoot be reconciled during the next maintenance period. If a short DNS outage is not a big issue, you can trigger reconciliation directly after setting the annotation. Switching node-local-dns off by removing the annotation can be a rather destructive operation that will result in pods without a working dns configuration.  For more information about node-local-dns please refer to the KEP or to the usage documentation.\n","categories":"","description":"","excerpt":"NodeLocalDNS Configuration This is a short guide describing how to …","ref":"/docs/gardener/usage/node-local-dns/","tags":"","title":"NodeLocalDNS Configuration"},{"body":"Gardener Extension for openid connect services \nProject Gardener implements the automated management and operation of Kubernetes clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.\nRecently, most of the vendor specific logic has been developed in-tree. However, the project has grown to a size where it is very hard to extend, maintain, and test. With GEP-1 we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.\nThis controller implements Gardener’s extension contract for the shoot-oidc-service extension.\nAn example for a ControllerRegistration resource that can be used to register this controller to Gardener can be found here.\nPlease find more information regarding the extensibility concepts and a detailed proposal here.\nExtension Resources Example extension resource:\napiVersion: extensions.gardener.cloud/v1alpha1 kind: Extension metadata:  name: extension-shoot-oidc-service  namespace: shoot--project--abc spec:  type: shoot-oidc-service When an extension resource is reconciled, the extension controller will create an instance of OIDC Webhook Authenticator. These resources are placed inside the shoot namespace on the seed. Also, the controller takes care about generating necessary RBAC resources for the seed as well as for the shoot.\nPlease note, this extension controller relies on the Gardener-Resource-Manager to deploy k8s resources to seed and shoot clusters.\nHow to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start.\nWe are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io GEP-1 (Gardener Enhancement Proposal) on extensibility Extensibility API documentation Gardener Extensions Golang library Gardener API Reference  ","categories":"","description":"Gardener extension controller for OpenID Connect services for shoot clusters","excerpt":"Gardener extension controller for OpenID Connect services for shoot …","ref":"/docs/extensions/others/gardener-extension-shoot-oidc-service/","tags":"","title":"OpenID Connect services"},{"body":"ClusterOpenIDConnectPreset and OpenIDConnectPreset This page provides an overview of ClusterOpenIDConnectPresets and OpenIDConnectPresets, which are objects for injecting OpenIDConnect Configuration into Shoot at creation time. The injected information contains configuration for the Kube API Server and optionally configuration for kubeconfig generation using said configuration.\nOpenIDConnectPreset An OpenIDConnectPreset is an API resource for injecting additional runtime OIDC requirements into a Shoot at creation time. You use label selectors to specify the Shoot to which a given OpenIDConnectPreset applies.\nUsing a OpenIDConnectPresets allows project owners to not have to explicitly provide the same OIDC configuration for every Shoot in their Project.\nFor more information about the background, see the issue for OpenIDConnectPreset.\nHow OpenIDConnectPreset works Gardener provides an admission controller (OpenIDConnectPreset) which, when enabled, applies OpenIDConnectPresets to incoming Shoot creation requests. When a Shoot creation request occurs, the system does the following:\n  Retrieve all OpenIDConnectPreset available for use in the Shoot namespace.\n  Check if the shoot label selectors of any OpenIDConnectPreset matches the labels on the Shoot being created.\n  If multiple presets are matched then only one is chosen and results are sorted based on:\n .spec.weight value. lexicographically ordering their names ( e.g. 002preset \u003e 001preset )    If the Shoot already has a .spec.kubernetes.kubeAPIServer.oidcConfig then no mutation occurs.\n  Simple OpenIDConnectPreset example This is a simple example to show how a Shoot is modified by the OpenIDConnectPreset\napiVersion: settings.gardener.cloud/v1alpha1 kind: OpenIDConnectPreset metadata:  name: test-1  namespace: default spec:  shootSelector:  matchLabels:  oidc: enabled  server:  clientID: test-1  issuerURL: https://foo.bar  # caBundle: |  # -----BEGIN CERTIFICATE-----  # Li4u  # -----END CERTIFICATE-----  groupsClaim: groups-claim  groupsPrefix: groups-prefix  usernameClaim: username-claim  usernamePrefix: username-prefix  signingAlgs:  - RS256  requiredClaims:  key: value  client:  secret: oidc-client-secret  extraConfig:  extra-scopes: \"email,offline_access,profile\"  foo: bar  weight: 90 Create the OpenIDConnectPreset:\nkubectl apply -f preset.yaml Examine the created OpenIDConnectPreset:\nkubectl get openidconnectpresets NAME ISSUER SHOOT-SELECTOR AGE test-1 https://foo.bar oidc=enabled 1s Simple Shoot example:\nThis is a sample of a Shoot with some fields omitted:\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata:  name: preset  namespace: default  labels:  oidc: enabled spec:  kubernetes:  allowPrivilegedContainers: true  version: 1.20.2 Create the Shoot:\nkubectl apply -f shoot.yaml Examine the created Shoot:\nkubectl get shoot preset -o yaml apiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata:  name: preset  namespace: default  labels:  oidc: enabled spec:  kubernetes:  kubeAPIServer:  oidcConfig:  clientAuthentication:  extraConfig:  extra-scopes: email,offline_access,profile  foo: bar  secret: oidc-client-secret  clientID: test-1  groupsClaim: groups-claim  groupsPrefix: groups-prefix  issuerURL: https://foo.bar  requiredClaims:  key: value  signingAlgs:  - RS256  usernameClaim: username-claim  usernamePrefix: username-prefix  version: 1.20.2 Disable OpenIDConnectPreset The OpenIDConnectPreset admission control is enabled by default. To disable it use the --disable-admission-plugins flag on the gardener-apiserver.\nFor example:\n--disable-admission-plugins=OpenIDConnectPreset ClusterOpenIDConnectPreset A ClusterOpenIDConnectPreset is an API resource for injecting additional runtime OIDC requirements into a Shoot at creation time. In contrast to OpenIDConnect it’s a cluster-scoped resource. You use label selectors to specify the Project and Shoot to which a given OpenIDCConnectPreset applies.\nUsing a OpenIDConnectPresets allows cluster owners to not have to explicitly provide the same OIDC configuration for every Shoot in specific Project.\nFor more information about the background, see the issue for ClusterOpenIDConnectPreset.\nHow ClusterOpenIDConnectPreset works Gardener provides an admission controller (ClusterOpenIDConnectPreset) which, when enabled, applies ClusterOpenIDConnectPresets to incoming Shoot creation requests. When a Shoot creation request occurs, the system does the following:\n  Retrieve all ClusterOpenIDConnectPresets available.\n  Check if the project label selector of any ClusterOpenIDConnectPreset matches the labels of the Project in which the Shoot is being created.\n  Check if the shoot label selectors of any ClusterOpenIDConnectPreset matches the labels on the Shoot being created.\n  If multiple presets are matched then only one is chosen and results are sorted based on:\n .spec.weight value. lexicographically ordering their names ( e.g. 002preset \u003e 001preset )    If the Shoot already has a .spec.kubernetes.kubeAPIServer.oidcConfig then no mutation occurs.\n   Note: Due to the previous requirement if a Shoot is matched by both OpenIDConnectPreset and ClusterOpenIDConnectPreset then OpenIDConnectPreset takes precedence over ClusterOpenIDConnectPreset.\n Simple ClusterOpenIDConnectPreset example This is a simple example to show how a Shoot is modified by the ClusterOpenIDConnectPreset\napiVersion: settings.gardener.cloud/v1alpha1 kind: ClusterOpenIDConnectPreset metadata:  name: test spec:  shootSelector:  matchLabels:  oidc: enabled  projectSelector: {} # selects all projects.  server:  clientID: cluster-preset  issuerURL: https://foo.bar  # caBundle: |  # -----BEGIN CERTIFICATE-----  # Li4u  # -----END CERTIFICATE-----  groupsClaim: groups-claim  groupsPrefix: groups-prefix  usernameClaim: username-claim  usernamePrefix: username-prefix  signingAlgs:  - RS256  requiredClaims:  key: value  client:  secret: oidc-client-secret  extraConfig:  extra-scopes: \"email,offline_access,profile\"  foo: bar  weight: 90 Create the ClusterOpenIDConnectPreset:\nkubectl apply -f preset.yaml Examine the created ClusterOpenIDConnectPreset:\nkubectl get clusteropenidconnectpresets NAME ISSUER PROJECT-SELECTOR SHOOT-SELECTOR AGE test https://foo.bar \u003cnone\u003e oidc=enabled 1s This is a sample of a Shoot with some fields omitted:\nkind: Shoot apiVersion: core.gardener.cloud/v1beta1 metadata:  name: preset  namespace: default  labels:  oidc: enabled spec:  kubernetes:  allowPrivilegedContainers: true  version: 1.20.2 Create the Shoot:\nkubectl apply -f shoot.yaml Examine the created Shoot:\nkubectl get shoot preset -o yaml apiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata:  name: preset  namespace: default  labels:  oidc: enabled spec:  kubernetes:  kubeAPIServer:  oidcConfig:  clientAuthentication:  extraConfig:  extra-scopes: email,offline_access,profile  foo: bar  secret: oidc-client-secret  clientID: cluster-preset  groupsClaim: groups-claim  groupsPrefix: groups-prefix  issuerURL: https://foo.bar  requiredClaims:  key: value  signingAlgs:  - RS256  usernameClaim: username-claim  usernamePrefix: username-prefix  version: 1.20.2 Disable ClusterOpenIDConnectPreset The ClusterOpenIDConnectPreset admission control is enabled by default. To disable it use the --disable-admission-plugins flag on the gardener-apiserver.\nFor example:\n--disable-admission-plugins=ClusterOpenIDConnectPreset ","categories":"","description":"","excerpt":"ClusterOpenIDConnectPreset and OpenIDConnectPreset This page provides …","ref":"/docs/gardener/usage/openidconnect-presets/","tags":"","title":"OpenIDConnect Presets"},{"body":"Register OpenID Connect provider in Shoot Clusters Introduction Within a shoot cluster, it is possible to dynamically register OpenID Connect providers. It is necessary that the Gardener installation your shoot cluster runs in is equipped with a shoot-oidc-service extension. Please ask your Gardener operator if the extension is available in your environment.\nShoot Feature Gate In most of the Gardener setups the shoot-oidc-service extension is not enabled globally and thus must be configured per shoot cluster. Please adapt the shoot specification by the configuration shown below to activate the extension individually.\nkind: Shoot ... spec:  extensions:  - type: shoot-oidc-service ... OpenID Connect provider In order to register an OpenID Connect provider an openidconnect resource should be deployed in the shoot cluster.\nIt is strongly recommended to NOT disable prefixing since it may result in unwanted impersonations. The rule of thumb is to always use meaningful and unique prefixes for both username and groups. A good way to ensure this is to use the name of the openidconnect resource as shown in the example below.\napiVersion: authentication.gardener.cloud/v1alpha1 kind: OpenIDConnect metadata:  name: abc spec:  # issuerURL is the URL the provider signs ID Tokens as.  # This will be the \"iss\" field of all tokens produced by the provider and is used for configuration discovery.  issuerURL: https://abc-oidc-provider.example   # clientID is the audience for which the JWT must be issued for, the \"aud\" field.  clientID: my-shoot-cluster   # usernameClaim is the JWT field to use as the user's username.  usernameClaim: sub   # usernamePrefix, if specified, causes claims mapping to username to be prefix with the provided value.  # A value \"oidc:\" would result in usernames like \"oidc:john\".  # If not provided, the prefix defaults to \"( .metadata.name )/\". The value \"-\" can be used to disable all prefixing.  usernamePrefix: \"abc:\"   # groupsClaim, if specified, causes the OIDCAuthenticator to try to populate the user's groups with an ID Token field.  # If the groupsClaim field is present in an ID Token the value must be a string or list of strings.  # groupsClaim: groups   # groupsPrefix, if specified, causes claims mapping to group names to be prefixed with the value.  # A value \"oidc:\" would result in groups like \"oidc:engineering\" and \"oidc:marketing\".  # If not provided, the prefix defaults to \"( .metadata.name )/\".  # The value \"-\" can be used to disable all prefixing.  # groupsPrefix: \"abc:\"   # caBundle is a PEM encoded CA bundle which will be used to validate the OpenID server's certificate. If unspecified, system's trusted certificates are used.  # caBundle: \u003cbase64 encoded bundle\u003e   # supportedSigningAlgs sets the accepted set of JOSE signing algorithms that can be used by the provider to sign tokens.  # The default value is RS256.  # supportedSigningAlgs:  # - RS256   # maxTokenExpirationSeconds if specified, sets a limit in seconds to the maximum validity duration of a token.  # Tokens issued with validity greater that this value will not be verified.  # Setting this will require that the tokens have the \"iat\" and \"exp\" claims.  # maxTokenExpirationSeconds: 3600   # jwks if specified, provides an option to specify JWKS keys offline.  # jwks:  # keys is a base64 encoded JSON webkey Set. If specified, the OIDCAuthenticator skips the request to the issuer's jwks_uri endpoint to retrieve the keys.  # keys: \u003cbase64 encoded jwks\u003e ","categories":"","description":"","excerpt":"Register OpenID Connect provider in Shoot Clusters Introduction Within …","ref":"/docs/extensions/others/gardener-extension-shoot-oidc-service/docs/usage/openidconnects/","tags":"","title":"Openidconnects"},{"body":"Contract: OperatingSystemConfig resource Gardener uses the machine API and leverages the functionalities of the machine-controller-manager (MCM) in order to manage the worker nodes of a shoot cluster. The machine-controller-manager itself simply takes a reference to an OS-image and (optionally) some user-data (a script or configuration that is executed when a VM is bootstrapped), and forwards both to the provider’s API when creating VMs. MCM does not have any restrictions regarding supported operating systems as it does not modify or influence the machine’s configuration in any way - it just creates/deletes machines with the provided metadata.\nConsequently, Gardener needs to provide this information when interacting with the machine-controller-manager. This means that basically every operating system is possible to be used as long as there is some implementation that generates the OS-specific configuration in order to provision/bootstrap the machines.\n⚠️ Currently, there are a few requirements:\n The operating system must have built-in Docker support. The operating system must have systemd support. The operating system must have wget pre-installed. The operating system must have jq pre-installed.  The reasons for that will become evident later.\nWhat does the user-data bootstrapping the machines contain? Gardener installs a few components onto every worker machine in order to allow it to join the shoot cluster. There is the kubelet process, some scripts for continuously checking the health of kubelet and docker, but also configuration for log rotation, CA certificates, etc. The complete configuration you can find here. We are calling this the “original” user-data.\nHow does Gardener bootstrap the machines? Usually, you would submit all the components you want to install onto the machine as part of the user-data during creation time. However, some providers do have a size limitation (like ~16KB) for that user-data. That’s why we do not send the “original” user-data to the machine-controller-manager (who forwards it then to the provider’s API). Instead, we only send a small script that downloads the “original” data and applies it on the machine directly. This way we can extend the “original” user-data without any size restrictions - plus we can modify it without the necessity of re-creating the machine (because we run a script that downloads and updates it continuously).\nThe high-level flow is as follows:\n  For every worker pool X in the Shoot specification, Gardener creates a Secret named cloud-config-\u003cX\u003e in the kube-system namespace of the shoot cluster. The secret contains the “original” user-data.\n  Gardener generates a kubeconfig with minimal permissions just allowing reading these secrets. It is used by the downloader script later.\n  Gardener provides the downloader script, the kubeconfig, and the machine image stated in the Shoot specification to the machine-controller-manager.\n  Based on this information the machine-controller-manager creates the VM.\n  After the VM has been provisioned the downloader script starts and fetches the appropriate Secret for its worker pool (containing the “original” user-data) and applies it.\n  Detailed bootstrap flow with a worker generated bootstrap-token With gardener v1.23 a file with the content \u003c\u003cBOOTSTRAP_TOKEN\u003e\u003e is added to the cloud-config-\u003cworker-group\u003e-downloader OperatingSystemConfig (part of step 2 in the graphic below). Via the OS extension the new file (with its content in clear-text) gets passed to the corresponding Worker resource.\nThe Worker controller has to guarantee that:\n a bootstrap token is created. the \u003c\u003cBOOTSTRAP_TOKEN\u003e\u003e in the user data is replaced by the generated token. One implementation of that is depicted in the picture where the machine-controller-manager creates a temporary token and replaces the placeholder.  As part of the user-data the bootstrap-token is placed on the newly created VM under a defined path. The cloud-config-script will then refer to the file path of the added bootstrap token in the kubelet-bootstrap script.\nCompatibility matrix for node bootstrap-token With Gardener v1.23, we replaced the long-valid bootstrap-token shared between nodes with a short-lived token unique for each node, ref: #3898.\n❗ When updating to Gardener version \u003e=1.35 the old bootstrap-token will be removed. You are required to update your extensions to the following versions when updating Gardener:\n   Extension Version Release Date Pull Request     os-gardenlinux v0.9.0 2 Jul https://github.com/gardener/gardener-extension-os-gardenlinux/pull/29   os-suse-chost v1.11.0 2 Jul https://github.com/gardener/gardener-extension-os-suse-chost/pull/41   os-ubuntu v1.11.0 2 Jul https://github.com/gardener/gardener-extension-os-ubuntu/pull/42   os-flatcar v1.7.0 2 Jul https://github.com/gardener/gardener-extension-os-coreos/pull/24   infrastructure-provider using Machine Controller Manager varies ~ end of 2019 https://github.com/gardener/machine-controller-manager/pull/351    ⚠️ If you run a provider extension that does not use Machine Controller Manager (MCM) you need to implement the functionality of creating a temporary bootstrap-token before updating your Gardener version to v1.35 or higher. All provider extensions maintained in https://github.com/gardener/ use MCM.\nHow does Gardener update the user-data on already existing machines? With ongoing development and new releases of Gardener some new components could be required to get installed onto every shoot worker VM, or existing components need to be changed. Gardener achieves that by simply updating the user-data inside the Secrets mentioned above (step 1). The downloader script is continuously (every 30s) reading the secret’s content (which might include an updated user-data) and storing it onto the disk. In order to re-apply the (new) downloaded data the secrets do not only contain the “original” user-data but also another short script (called “execution” script). This script checks whether the downloaded user-data differs from the one previously applied - and if required - re-applies it. After that it uses systemctl to restart the installed systemd units.\nWith the help of the execution script Gardener can centrally control how machines are updated without the need of OS providers to (re-)implement that logic. However, as stated in the mentioned requirements above, the execution script assumes existence of Docker and systemd.\nWhat needs to be implemented to support a new operating system? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: OperatingSystemConfig metadata:  name: pool-01-original  namespace: default spec:  type: \u003cmy-operating-system\u003e  purpose: reconcile  reloadConfigFilePath: /var/lib/cloud-config-downloader/cloud-config  units:  - name: docker.service  dropIns:  - name: 10-docker-opts.conf  content: |[Service] Environment=\"DOCKER_OPTS=--log-opt max-size=60m --log-opt max-file=3\"  - name: docker-monitor.service  command: start  enable: true  content: |[Unit] Description=Docker-monitor daemon After=kubelet.service [Install] WantedBy=multi-user.target [Service] Restart=always EnvironmentFile=/etc/environment ExecStart=/opt/bin/health-monitor docker  files:  - path: /var/lib/kubelet/ca.crt  permissions: 0644  encoding: b64  content:  secretRef:  name: default-token-5dtjz  dataKey: token  - path: /etc/sysctl.d/99-k8s-general.conf  permissions: 0644  content:  inline:  data: |# A higher vm.max_map_count is great for elasticsearch, mongo, or other mmap users # See https://github.com/kubernetes/kops/issues/1340 vm.max_map_count = 135217728 In order to support a new operating system you need to write a controller that watches all OperatingSystemConfigs with .spec.type=\u003cmy-operating-system\u003e. For those it shall generate a configuration blob that fits to your operating system. For example, a CoreOS controller might generate a CoreOS cloud-config or Ignition, SLES might generate cloud-init, and others might simply generate a bash script translating the .spec.units into systemd units, and .spec.files into real files on the disk.\nOperatingSystemConfigs can have two purposes which can be used (or ignored) by the extension controllers: either provision or reconcile.\n The provision purpose is used by Gardener for the user-data that it later passes to the machine-controller-manager (and then to the provider’s API) when creating new VMs. It contains the downloader unit. The reconcile purpose contains the “original” user-data (that is then stored in Secrets in the shoot’s kube-system namespace (see step 1). This is downloaded and applies late (see step 5).  As described above, the “original” user-data must be re-applicable to allow in-place updates. The way how this is done is specific to the generated operating system config (e.g., for CoreOS cloud-init the command is /usr/bin/coreos-cloudinit --from-file=\u003cpath\u003e, whereas SLES would run cloud-init --file \u003cpath\u003e single -n write_files --frequency=once). Consequently, besides the generated OS config, the extension controller must also provide a command for re-application an updated version of the user-data. As visible in the mentioned examples the command requires a path to the user-data file. Gardener will provide the path to the file in the OperatingSystemConfigs .spec.reloadConfigFilePath field (only if .spec.purpose=reconcile). As soon as Gardener detects that the user data has changed it will reload the systemd daemon and restart all the units provided in the .status.units[] list (see below example). The same logic applies during the very first application of the whole configuration.\nAfter generation extension controllers are asked to store their OS config inside a Secret (as it might contain confidential data) in the same namespace. The secret’s .data could look like this:\napiVersion: v1 kind: Secret metadata:  name: osc-result-pool-01-original  namespace: default  ownerReferences:  - apiVersion: extensions.gardener.cloud/v1alpha1  blockOwnerDeletion: true  controller: true  kind: OperatingSystemConfig  name: pool-01-original  uid: 99c0c5ca-19b9-11e9-9ebd-d67077b40f82 data:  cloud_config: base64(generated-user-data) Finally, the secret’s metadata, the OS-specific command to re-apply the configuration, and the list of systemd units that shall be considered to be restarted if an updated version of the user-data is re-applied must be provided in the OperatingSystemConfig’s .status field:\n... status:  cloudConfig:  secretRef:  name: osc-result-pool-01-original  namespace: default  command: /usr/bin/coreos-cloudinit --from-file=/var/lib/cloud-config-downloader/cloud-config  lastOperation:  description: Successfully generated cloud config  lastUpdateTime: \"2019-01-23T07:45:23Z\"  progress: 100  state: Succeeded  type: Reconcile  observedGeneration: 5  units:  - docker-monitor.service (The .status.command field is optional and must only be provided if .spec.reloadConfigFilePath exists).\nOnce the .status indicates that the extension controller finished reconciling Gardener will continue with the next step of the shoot reconciliation flow.\nCRI Support Gardener supports specifying Container Runtime Interface (CRI) configuration in the OperatingSystemConfig resource. If the .spec.cri section exists then the name property is mandatory. The only supported values for cri.name at the moment are: containerd and docker, which uses the in-tree dockershim. For example:\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: OperatingSystemConfig metadata:  name: pool-01-original  namespace: default spec:  type: \u003cmy-operating-system\u003e  purpose: reconcile  reloadConfigFilePath: /var/lib/cloud-config-downloader/cloud-config  cri:  name: containerd ... To support ContainerD, an OS extension must :\n The operating system must have built-in ContainerD and the Client CLI ContainerD must listen on its default socket path: unix:///run/containerd/containerd.sock ContainerD must be configured to work with the default configuration file in: /etc/containerd/config.toml (Created by Gardener).  If CRI configurations are not supported it is recommended create a validating webhook running in the garden cluster that prevents specifying the .spec.providers.workers[].cri section in the Shoot objects.\nReferences and additional resources  OperatingSystemConfig API (Golang specification) downloader script (fetching the “original” user-data and the execution script) Original user-data templates Execution script (applying the “original” user-data)  ","categories":"","description":"","excerpt":"Contract: OperatingSystemConfig resource Gardener uses the machine API …","ref":"/docs/gardener/extensions/operatingsystemconfig/","tags":"","title":"Operatingsystemconfig"},{"body":"Packages:\n  operations.gardener.cloud/v1alpha1   operations.gardener.cloud/v1alpha1  Package v1alpha1 is a version of the API.\nResource Types:  Bastion  Bastion   Bastion holds details about an SSH bastion for a shoot cluster.\n   Field Description      apiVersion string   operations.gardener.cloud/v1alpha1      kind string  Bastion    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  BastionSpec     Specification of the Bastion.\n     shootRef  Kubernetes core/v1.LocalObjectReference     ShootRef defines the target shoot for a Bastion. The name field of the ShootRef is immutable.\n    seedName  string    (Optional) SeedName is the name of the seed to which this Bastion is currently scheduled. This field is populated at the beginning of a create/reconcile operation.\n    providerType  string    (Optional) ProviderType is cloud provider used by the referenced Shoot.\n    sshPublicKey  string    SSHPublicKey is the user’s public key. This field is immutable.\n    ingress  []BastionIngressPolicy     Ingress controls from where the created bastion host should be reachable.\n       status  BastionStatus     (Optional) Most recently observed status of the Bastion.\n    BastionIngressPolicy   (Appears on: BastionSpec)  BastionIngressPolicy represents an ingress policy for SSH bastion hosts.\n   Field Description      ipBlock  Kubernetes networking/v1.IPBlock     IPBlock defines an IP block that is allowed to access the bastion.\n    BastionSpec   (Appears on: Bastion)  BastionSpec is the specification of a Bastion.\n   Field Description      shootRef  Kubernetes core/v1.LocalObjectReference     ShootRef defines the target shoot for a Bastion. The name field of the ShootRef is immutable.\n    seedName  string    (Optional) SeedName is the name of the seed to which this Bastion is currently scheduled. This field is populated at the beginning of a create/reconcile operation.\n    providerType  string    (Optional) ProviderType is cloud provider used by the referenced Shoot.\n    sshPublicKey  string    SSHPublicKey is the user’s public key. This field is immutable.\n    ingress  []BastionIngressPolicy     Ingress controls from where the created bastion host should be reachable.\n    BastionStatus   (Appears on: Bastion)  BastionStatus holds the most recently observed status of the Bastion.\n   Field Description      ingress  Kubernetes core/v1.LoadBalancerIngress     (Optional) Ingress holds the public IP and/or hostname of the bastion instance.\n    conditions  []github.com/gardener/gardener/pkg/apis/core/v1alpha1.Condition     (Optional) Conditions represents the latest available observations of a Bastion’s current state.\n    lastHeartbeatTimestamp  Kubernetes meta/v1.Time     (Optional) LastHeartbeatTimestamp is the time when the bastion was last marked as not to be deleted. When this is set, the ExpirationTimestamp is advanced as well.\n    expirationTimestamp  Kubernetes meta/v1.Time     (Optional) ExpirationTimestamp is the time after which a Bastion is supposed to be garbage collected.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Bastion. It corresponds to the Bastion’s generation, which is updated on mutation by the API Server.\n      Generated with gen-crd-api-reference-docs \n","categories":"","description":"","excerpt":"Packages:\n  operations.gardener.cloud/v1alpha1 …","ref":"/docs/gardener/api-reference/operations/","tags":"","title":"Operations"},{"body":"Operator Alerts    Alertname Severity Type Description     ApiServerUnreachableViaKubernetesService critical shoot The Api server has been unreachable for 15 minutes via the kubernetes service in the shoot.   KubeletTooManyOpenFileDescriptorsSeed critical seed Seed-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubePersistentVolumeUsageCritical critical seed The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} is only {{ printf \"%0.2f\" $value }}% free.   KubePersistentVolumeFullInFourDays warning seed Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ printf \"%0.2f\" $value }}% is available.   KubePodPendingControlPlane warning seed Pod {{ $labels.pod }} is stuck in \"Pending\" state for more than 30 minutes.   KubePodNotReadyControlPlane warning  Pod {{ $labels.pod }} is not ready for more than 30 minutes.   PrometheusCantScrape warning seed Prometheus failed to scrape metrics. Instance {{ $labels.instance }}, job {{ $labels.job }}.   PrometheusConfigurationFailure warning seed Latest Prometheus configuration is broken and Prometheus is using the previous one.   VPNProbeAPIServerProxyFailed critical shoot The API Server proxy functionality is not working. Probably the vpn connection from an API Server pod to the vpn-shoot endpoint on the Shoot workers does not work.    ","categories":"","description":"","excerpt":"Operator Alerts    Alertname Severity Type Description …","ref":"/docs/gardener/monitoring/operator_alerts/","tags":"","title":"Operator Alerts"},{"body":"Disclaimer If an application depends on other services deployed separately do not rely on a certain start sequence of containers but ensure that the application can cope with unavailability of the services it depends on.\nIntroduction Kubernetes offers a feature called InitContainers to perform some tasks during a pod’s initialization. In this tutorial we demonstrate how to use it orchestrate starting sequence of multiple containers. The tutorial uses the example app url-shortener which consists of two components:\n postgresql database webapp which depends on postgresql database and provides two endpoints: create a short url from a given location, and redirect from a given short URL to the corresponding target location.  This app represents the minimal example where an application relies on another service or database. In this example, if the application starts before database is ready, the application will fail as shown below:\n$ kubectl logs webapp-958cf5567-h247n time=\"2018-06-12T11:02:42Z\" level=info msg=\"Connecting to Postgres database using: host=`postgres:5432` dbname=`url_shortener_db` username=`user`\\n\" time=\"2018-06-12T11:02:42Z\" level=fatal msg=\"failed to start: failed to open connection to database: dial tcp: lookup postgres on 100.64.0.10:53: no such host\\n\"   $ kubectl get po -w NAME READY STATUS RESTARTS AGE webapp-958cf5567-h247n 0/1 Pending 0 0s webapp-958cf5567-h247n 0/1 Pending 0 0s webapp-958cf5567-h247n 0/1 ContainerCreating 0 0s webapp-958cf5567-h247n 0/1 ContainerCreating 0 1s webapp-958cf5567-h247n 0/1 Error 0 2s webapp-958cf5567-h247n 0/1 Error 1 3s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 1 4s webapp-958cf5567-h247n 0/1 Error 2 18s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 2 29s webapp-958cf5567-h247n 0/1 Error 3 43s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 3 56s If the restartPolicy is set to Always (default) in yaml, the application will continue to restart the pod with an exponential back-off delay in case of failure.\nUsing InitContaniner To avoid such situation, InitContainers  can be defined which are executed prior to the application container. If one InitContainers fails, the application container won’t be triggered.\napiVersion: apps/v1 kind: Deployment metadata:  name: webapp spec:  selector:  matchLabels:  app: webapp  template:  metadata:  labels:  app: webapp  spec:  initContainers: # check if DB is ready, and only continue when true  - name: check-db-ready  image: postgres:9.6.5  command: ['sh', '-c', 'until pg_isready -h postgres -p 5432; do echo waiting for database; sleep 2; done;']  containers:  - image: xcoulon/go-url-shortener:0.1.0  name: go-url-shortener  env:  - name: POSTGRES_HOST  value: postgres  - name: POSTGRES_PORT  value: \"5432\"  - name: POSTGRES_DATABASE  value: url_shortener_db  - name: POSTGRES_USER  value: user  - name: POSTGRES_PASSWORD  value: mysecretpassword  ports:  - containerPort: 8080 In above example, the InitContainers uses docker image postgres:9.6.5 which is different from the application container. This also brings the advantage of not having to include unnecessary tools (e.g. pg_isready) in the application container.\nWith introduction of InitContainers, the pod startup will look like following in case database is not available yet:\n$ kubectl get po -w NAME READY STATUS RESTARTS AGE nginx-deployment-5cc79d6bfd-t9n8h 1/1 Running 0 5d privileged-pod 1/1 Running 0 4d webapp-fdcb49cbc-4gs4n 0/1 Pending 0 0s webapp-fdcb49cbc-4gs4n 0/1 Pending 0 0s webapp-fdcb49cbc-4gs4n 0/1 Init:0/1 0 0s webapp-fdcb49cbc-4gs4n 0/1 Init:0/1 0 1s   $ kubectl logs webapp-fdcb49cbc-4gs4n Error from server (BadRequest): container \"go-url-shortener\" in pod \"webapp-fdcb49cbc-4gs4n\" is waiting to start: PodInitializing ","categories":"","description":"How to orchestrate startup sequence of multiple containers","excerpt":"How to orchestrate startup sequence of multiple containers","ref":"/docs/guides/applications/container-startup/","tags":"","title":"Orchestration of Container Startup"},{"body":"The Gardener project implements the documentation-as-code paradigm. Essentially this means that:\n Documentation resides close to the code it describes - in the corresponding GitHub repositories. Only documentation with regards to cross-cutting concerns that cannot be affiliated to a specific component repository is hosted in the general gardener/documentation repository. We use tools to develop, validate and integrate documentation sources The change management process is largely automated with automatic validation, integration and deployment using docforge and docs-toolbelt. The documentation sources are intended for reuse and not bound to a specific publishing platform. The physical organization in a repository is irrelevant for the tool support. What needs to be maintained is the intended result in a docforge documentation bundle manifest configuration, very much like virtual machines configurations, that docforge can reliably recreate in any case. We use GitHub as distributed, versioning storage system and docforge to pull sources in their desired state to forge documentation bundles according to a desired specification provided as a manifest.  Content Organization Documentation that can be affiliated to component is hosted and maintained in the component repository.\nA recommended template for organizing documentation sources is to place them all in a docs folder and organize it there per role activity. For example:\nrepositoryX |_ docs |_ usage | |_ images | |_ 01.png | |_ hibernation.md |_ operations |_ deployment Do not use folders just because they are in the template. Stick to the predefined roles and corresponding activities for naming convention. A system makes it easier to maintain and get oriented.\n User: usage Operator: operations Gardener (service) provider: deployment Gardener Developer: development Gardener Extension Developer: extensions  Publishing on gardener.cloud The Gardener website is one of the multiple optional publishing channels where the source material might end up as documentation. We use docforge and automated integration and publish process to enable transparent change management.\nTo have documentation published on the website it is necessary to use the docforge manifests available at [gardener/documentation/.docforge] adn register a reference to your documentation.\nNote This is work in progress and we are transitioning to a more transparent way of integrating component documentation. This guide will be updated as we progress.  These manifests describe a particular publishing goal, i.e. using Hugo to publish on the website, and you will find out that they contain Hugo-specific front-matter properties. Consult with the documentation maintainers for details. Use the gardener channel in slack or open a PR.\n","categories":"","description":"","excerpt":"The Gardener project implements the documentation-as-code paradigm. …","ref":"/docs/contribute/20_documentation/10_organisation/","tags":"","title":"Organization"},{"body":"Organizing Access Using kubeconfig Files The kubectl command-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.\nProblem If you’ve become aware of a security breach that affects you, you may want to revoke or cycle credentials in case anything was leaked. However, this is not possible with the initial or master kubeconfig from your cluster.\nPitfall Never distribute the kubeconfig, which you can download directly within the Gardener dashboard, for a productive cluster.\nCreate custom kubeconfig file for each user Create a separate kubeconfig for each user. One of the big advantages is, that you can revoke them and control the permissions better. A limitation to single namespaces is also possible here.\nThe script creates a new ServiceAccount with read privileges in the whole cluster (Secretes are excluded). To run the script jq, a lightweight and flexible command-line JSON processor, must be installed.\n#!/bin/bash  if [[ -z \"$1\" ]] ;then  echo \"usage: $0\u003cusername\u003e\"  exit 1 fi  user=$1 kubectl create sa ${user} secret=$(kubectl get sa ${user} -o json | jq -r .secrets[].name) kubectl get secret ${secret} -o json | jq -r '.data[\"ca.crt\"]' | base64 -D \u003e ca.crt  user_token=$(kubectl get secret ${secret} -o json | jq -r '.data[\"token\"]' | base64 -D) c=`kubectl config current-context` cluster_name=`kubectl config get-contexts $c | awk '{print $3}' | tail -n 1` endpoint=`kubectl config view -o jsonpath=\"{.clusters[?(@.name == \\\"${cluster_name}\\\")].cluster.server}\"`  # Set up the config KUBECONFIG=k8s-${user}-conf kubectl config set-cluster ${cluster_name} \\  --embed-certs=true \\  --server=${endpoint} \\  --certificate-authority=./ca.crt  KUBECONFIG=k8s-${user}-conf kubectl config set-credentials ${user}-${cluster_name#cluster-} --token=${user_token} KUBECONFIG=k8s-${user}-conf kubectl config set-context ${user}-${cluster_name#cluster-} \\  --cluster=${cluster_name} \\  --user=${user}-${cluster_name#cluster-} KUBECONFIG=k8s-${user}-conf kubectl config use-context ${user}-${cluster_name#cluster-}  cat \u003c\u003cEOF | kubectl create -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: view-${user}-global subjects: - kind: ServiceAccount name: ${user} namespace: default roleRef: kind: ClusterRole name: view apiGroup: rbac.authorization.k8s.io EOF   echo \"done! Test with: \" echo \"export KUBECONFIG=k8s-${user}-conf\" echo \"kubectl get pods\" If edit or admin rights are to be assigned, the ClusterRoleBinding must be adapted in the roleRef section with the roles listed below.\nFurthermore, you can restrict this to a single namespace by not creating a ClusterRoleBinding but only a RoleBinding within the desired namespace.\n   Default ClusterRole Default ClusterRoleBinding Description     cluster-admin system:masters group Allows super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding’s namespace, including the namespace itself.   admin None Allows admin access, intended to be granted within a namespace using a RoleBinding. If used in a RoleBinding, allows read/write access to most resources in a namespace, including the ability to create roles and rolebindings within the namespace. It does not allow write access to resource quota or to the namespace itself.   edit None Allows read/write access to most objects in a namespace. It does not allow viewing or modifying roles or rolebindings.   view None Allows read-only access to see most objects in a namespace. It does not allow viewing roles or rolebindings. It does not allow viewing secrets, since those are escalating.    ","categories":"","description":"","excerpt":"Organizing Access Using kubeconfig Files The kubectl command-line tool …","ref":"/docs/guides/client_tools/working-with-kubeconfig/","tags":"","title":"Organizing Access Using kubeconfig Files"},{"body":"Problem After updating your HTML and JavaScript sources in your web application, the kubernetes cluster delivers outdated versions - why?\nPreamble By default, Kubernetes service pods are not accessible from the external network, but only from other pods within the same Kubernetes cluster.\nThe Gardener cluster has a built-in configuration for HTTP load balancing called Ingress, defining rules for external connectivity to Kubernetes services. Users who want external access to their Kubernetes services create an ingress resource that defines rules, including the URI path, backing service name, and other information. The Ingress controller can then automatically program a frontend load balancer to enable Ingress configuration.\nExample Ingress Configuration apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata:  name: vuejs-ingress spec:  rules:  - host: test.ingress.\u003cGARDENER-CLUSTER\u003e.\u003cGARDENER-PROJECT\u003e.shoot.canary.k8s-hana.ondemand.com  http:  paths:  - backend:  serviceName: vuejs-svc  servicePort: 8080 where:\n \u003cGARDENER-CLUSTER\u003e: The cluster name in the Gardener \u003cGARDENER-PROJECT\u003e: You project name in the Gardener  What is the underlying problem? The ingress controller we are using is NGINX.\n NGINX is a software load balancer, web server, and content cache built on top of open source NGINX.\n NGINX caches the content as specified in the HTTP header. If the HTTP header is missing, it is assumed that the cache is forever and NGINX never updates the content in the stupidest case.\nSolution In general you can avoid this pitfall with one of the solutions below:\n use a cache buster + HTTP-Cache-Control(prefered) use HTTP-Cache-Control with a lower retention period disable the caching in the ingress (just for dev purpose)  Learning how to set the HTTP header or setup a cache buster is left to the read as an exercise for your web framework (e.g. Express/NodeJS, SpringBoot,…)\nHere an example how to disable the cache control for your ingress done with an annotation in your ingress YAML (during development).\n--- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata:  annotations:  ingress.kubernetes.io/cache-enable: \"false\"  name: vuejs-ingress spec:  rules:  - host: test.ingress.\u003cGARDENER-CLUSTER\u003e.\u003cGARDENER-PROJECT\u003e.shoot.canary.k8s-hana.ondemand.com  http:  paths:  - backend:  serviceName: vuejs-svc  servicePort: 8080 ","categories":"","description":"Why is my application always outdated?","excerpt":"Why is my application always outdated?","ref":"/docs/guides/applications/service-cache-control/","tags":"","title":"Out-Dated HTML and JS Files Delivered"},{"body":"Machine Controller Manager CORE – ./machine-controller-manager(provider independent) Out of tree : Machine controller (provider specific) MCM is a set controllers:\n  Machine Deployment Controller\n  Machine Set Controller\n  Machine Controller\n  Machine Safety Controller\n  Questions and refactoring Suggestions Refactoring    Statement FilePath Status     ConcurrentNodeSyncs” bad name - nothing to do with node syncs actually. If its value is ’10’ then it will start 10 goroutines (workers) per resource type (machine, machinist, machinedeployment, provider-specific-class, node - study the different resource types. cmd/machine-controller-manager/app/options/options.go pending   LeaderElectionConfiguration is very similar to the one present in “client-go/tools/leaderelection/leaderelection.go” - can we simply used the one in client-go instead of defining again? pkg/options/types.go - MachineControllerManagerConfiguration pending   Have all userAgents as constant. Right now there is just one. cmd/app/controllermanager.go pending   Shouldn’t run function be defined on MCMServer struct itself? cmd/app/controllermanager.go pending   clientcmd.BuildConfigFromFlags fallsback to inClusterConfig which will surely not work as that is not the target. Should it not check and exit early? cmd/app/controllermanager.go - run Function pending   A more direct way to create an in cluster config is using k8s.io/client-go/rest -\u003e rest.InClusterConfig instead of using clientcmd.BuildConfigFromFlags passing empty arguments and depending upon the implementation to fallback to creating a inClusterConfig. If they change the implementation that you get affected. cmd/app/controllermanager.go - run Function pending   Introduce a method on MCMServer which gets a target KubeConfig and controlKubeConfig or alternatively which creates respective clients. cmd/app/controllermanager.go - run Function pending   Why can’t we use Kubernetes.NewConfigOrDie also for kubeClientControl? cmd/app/controllermanager.go - run Function pending   I do not see any benefit of client builders actually. All you need to do is pass in a config and then directly use client-go functions to create a client. cmd/app/controllermanager.go - run Function pending   Function: getAvailableResources - rename this to getApiServerResources cmd/app/controllermanager.go pending   Move the method which waits for API server to up and ready to a separate method which returns a discoveryClient when the API server is ready. cmd/app/controllermanager.go - getAvailableResources function pending   Many methods in client-go used are now deprecated. Switch to the ones that are now recommended to be used instead. cmd/app/controllermanager.go - startControllers pending   This method needs a general overhaul cmd/app/controllermanager.go - startControllers pending   If the design is influenced/copied from KCM then its very different. There are different controller structs defined for deployment, replicaset etc which makes the code much more clearer. You can see “kubernetes/cmd/kube-controller-manager/apps.go” and then follow the trail from there. - agreed needs to be changed in future (if time permits) pkg/controller/controller.go pending   I am not sure why “MachineSetControlInterface”, “RevisionControlInterface”, “MachineControlInterface”, “FakeMachineControl” are defined in this file? pkg/controller/controller_util.go pending   IsMachineActive - combine the first 2 conditions into one with OR. pkg/controller/controller_util.go pending   Minor change - correct the comment, first word should always be the method name. Currently none of the comments have correct names. pkg/controller/controller_util.go pending   There are too many deep copies made. What is the need to make another deep copy in this method? You are not really changing anything here. pkg/controller/deployment.go - updateMachineDeploymentFinalizers pending   Why can’t these validations be done as part of a validating webhook? pkg/controller/machineset.go - reconcileClusterMachineSet pending   Small change to the following if condition. else if is not required a simple else is sufficient. Code1     pkg/controller/machineset.go - reconcileClusterMachineSet pending    Why call these inactiveMachines, these are live and running and therefore active. pkg/controller/machineset.go - terminateMachines pending    Clarification    Statement FilePath Status     Why are there 2 versions - internal and external versions? General pending   Safety controller freezes MCM controllers in the following cases: * Num replicas go beyond a threshold (above the defined replicas) * Target API service is not reachable There seems to be an overlap between DWD and MCM Safety controller. In the meltdown scenario why is MCM being added to DWD, you could have used Safety controller for that. General pending   All machine resources are v1alpha1 - should we not promote it to beta. V1alpha1 has a different semantic and does not give any confidence to the consumers. cmd/app/controllermanager.go pending   Shouldn’t controller manager use context.Context instead of creating a stop channel? - Check if signals (os.Interrupt and SIGTERM are handled properly. Do not see code where this is handled currently.) cmd/app/controllermanager.go pending   What is the rationale behind a timeout of 10s? If the API server is not up, should this not just block as it can anyways not do anything. Also, if there is an error returned then you exit the MCM which does not make much sense actually as it will be started again and you will again do the poll for the API server to come back up. Forcing an exit of MCM will not have any impact on the reachability of the API server in anyway so why exit? cmd/app/controllermanager.go - getAvailableResources pending   There is a very weird check - availableResources[machineGVR] || availableResources[machineSetGVR] || availableResources[machineDeploymentGVR] * Shouldn’t this be conjunction instead of disjunction? * What happens if you do not find one or all of these resources? Currently an error log is printed and nothing else is done. MCM can be used outside gardener context where consumers can directly create MachineClass and Machine and not create MachineSet / Maching Deployment. There is no distinction made between context (gardener or outside-gardener). cmd/app/controllermanager.go - StartControllers pending   Instead of having an empty select {} to block forever, isn’t it better to wait on the stop channel? cmd/app/controllermanager.go - StartControllers pending   Do we need provider specific queues and syncs and listers pkg/controller/controller.go pending   Why are resource types prefixed with “Cluster”? - not sure , check PR pkg/controller/controller.go pending   When will forgetAfterSuccess be false and why? - as per the current code this is never the case. - Himanshu will check cmd/app/controllermanager.go - createWorker pending   What is the use of “ExpectationsInterface” and “UIDTrackingContExpectations”? * All expectations related code should be in its own file “expectations.go” and not in this file. pkg/controller/controller_util.go pending   Why do we not use lister but directly use the controlMachingClient to get the deployment? Is it because you want to avoid any potential delays caused by update of the local cache held by the informer and accessed by the lister? What is the load on API server due to this? pkg/controller/deployment.go - reconcileClusterMachineDeployment pending   Why is this conversion needed? code2 pkg/controller/deployment.go - reconcileClusterMachineDeployment pending   A deep copy of machineDeployment is already passed and within the function another deepCopy is made. Any reason for it? pkg/controller/deployment.go - addMachineDeploymentFinalizers pending   What is an Status.ObservedGeneration? *Read more about generations and observedGeneration at: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata  https://alenkacz.medium.com/kubernetes-operator-best-practices-implementing-observedgeneration-250728868792 Ideally the update to the ObservedGeneration should only be made after successful reconciliation and not before. I see that this is just copied from deployment_controller.go as is pkg/controller/deployment.go - reconcileClusterMachineDeployment pending   Why and when will a MachineDeployment be marked as frozen and when will it be un-frozen? pkg/controller/deployment.go - reconcileClusterMachineDeployment pending   Shoudn’t the validation of the machine deployment be done during the creation via a validating webhook instead of allowing it to be stored in etcd and then failing the validation during sync? I saw the checks and these can be done via validation webhook. pkg/controller/deployment.go - reconcileClusterMachineDeployment pending   RollbackTo has been marked as deprecated. What is the replacement? code3 pkg/controller/deployment.go - reconcileClusterMachineDeployment pending   What is the max machineSet deletions that you could process in a single run? The reason for asking this question is that for every machineSetDeletion a new goroutine spawned. * Is the Delete call a synchrounous call? Which means it blocks till the machineset deletion is triggered which then also deletes the machines (due to cascade-delete and blockOwnerDeletion= true)? pkg/controller/deployment.go - terminateMachineSets pending   If there are validation errors or error when creating label selector then a nil is returned. In the worker reconcile loop if the return value is nil then it will remove it from the queue (forget + done). What is the way to see any errors? Typically when we describe a resource the errors are displayed. Will these be displayed when we discribe a MachineDeployment? pkg/controller/deployment.go - reconcileClusterMachineSet pending   If an error is returned by updateMachineSetStatus and it is IsNotFound error then returning an error will again queue the MachineSet. Is this desired as IsNotFound indicates the MachineSet has been deleted and is no longer there? pkg/controller/deployment.go - reconcileClusterMachineSet pending   is machineControl.DeleteMachine a synchronous operation which will wait till the machine has been deleted? Also where is the DeletionTimestamp set on the Machine? Will it be automatically done by the API server? pkg/controller/deployment.go - prepareMachineForDeletion pending    Bugs/Enhancements    Statement + TODO FilePath Status     This defines QPS and Burst for its requests to the KAPI. Check if it would make sense to explicitly define a FlowSchema and PriorityLevelConfiguration to ensure that the requests from this controller are given a well-defined preference. What is the rational behind deciding these values? pkg/options/types.go - MachineControllerManagerConfiguration pending   In function “validateMachineSpec” fldPath func parameter is never used. pkg/apis/machine/validation/machine.go pending   If there is an update failure then this method recursively calls itself without any sort of delays which could lead to a LOT of load on the API server. (opened: https://github.com/gardener/machine-controller-manager/issues/686) pkg/controller/deployment.go - updateMachineDeploymentFinalizers pending   We are updating filteredMachines by invoking syncMachinesNodeTemplates, syncMachinesConfig and syncMachinesClassKind but we do not create any deepCopy here. Everywhere else the general principle is when you mutate always make a deepCopy and then mutate the copy instead of the original as a lister is used and that changes the cached copy. Fix: SatisfiedExpectations check has been commented and there is a TODO there to fix it. Is there a PR for this? pkg/controller/machineset.go - reconcileClusterMachineSet pending    Code references\n1.1 code1  if machineSet.DeletionTimestamp == nil {   // manageReplicas is the core machineSet method where scale up/down occurs   // It is not called when deletion timestamp is set   manageReplicasErr = c.manageReplicas(ctx, filteredMachines, machineSet)   ​   } else if machineSet.DeletionTimestamp != nil {   //FIX: change this to simple else without the if 1.2 code2  defer dc.enqueueMachineDeploymentAfter(deployment, 10*time.Minute)   * `Clarification`: Why is this conversion needed?   err = v1alpha1.Convert_v1alpha1_MachineDeployment_To_machine_MachineDeployment(deployment, internalMachineDeployment, nil) 1.3 code3  // rollback is not re-entrant in case the underlying machine sets are updated with a new  \t// revision so we should ensure that we won't proceed to update machine sets until we  \t// make sure that the deployment has cleaned up its rollback spec in subsequent enqueues.  \tif d.Spec.RollbackTo != nil {  \treturn dc.rollback(ctx, d, machineSets, machineMap)  \t} ","categories":"","description":"","excerpt":"Machine Controller Manager CORE – …","ref":"/docs/other-components/machine-controller-manager/docs/todo/outline/","tags":"","title":"Outline"},{"body":"Extensibility overview Initially, everything was developed in-tree in the Gardener project. All cloud providers and the configuration for all the supported operating systems were released together with the Gardener core itself. But as the project grew, it got more and more difficult to add new providers and maintain the existing code base. As a consequence and in order to become agile and flexible again, we proposed GEP-1 (Gardener Enhancement Proposal). The document describes an out-of-tree extension architecture that keeps the Gardener core logic independent of provider-specific knowledge (similar to what Kubernetes has achieved with out-of-tree cloud providers or with CSI volume plugins).\nBasic concepts Gardener keeps running in the “garden cluster” and implements the core logic of shoot cluster reconciliation/deletion. Extensions are Kubernetes controllers themselves (like Gardener) and run in the seed clusters. As usual, we try to use Kubernetes wherever applicable. We rely on Kubernetes extension concepts in order to enable extensibility for Gardener. The main ideas of GEP-1 are the following:\n  During the shoot reconciliation process Gardener will write CRDs into the seed cluster that are watched and managed by the extension controllers. They will reconcile (based on the .spec) and report whether everything went well or errors occurred in the CRD’s .status field.\n  Gardener keeps deploying the provider-independent control plane components (etcd, kube-apiserver, etc.). However, some of these components might still need little customization by providers, e.g., additional configuration, flags, etc. In this case, the extension controllers register webhooks in order to manipulate the manifests.\n  Example 1:\nGardener creates a new AWS shoot cluster and requires the preparation of infrastructure in order to proceed (networks, security groups, etc.). It writes the following CRD into the seed cluster:\napiVersion: extensions.gardener.cloud/v1alpha1 kind: Infrastructure metadata:  name: infrastructure  namespace: shoot--core--aws-01 spec:  type: aws  providerConfig:  apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  vpc:  cidr: 10.250.0.0/16  internal:  - 10.250.112.0/22  public:  - 10.250.96.0/22  workers:  - 10.250.0.0/19  zones:  - eu-west-1a  dns:  apiserver: api.aws-01.core.example.com  region: eu-west-1  secretRef:  name: my-aws-credentials  sshPublicKey: | base64(key) Please note that the .spec.providerConfig is a raw blob and not evaluated or known in any way by Gardener. Instead, it was specified by the user (in the Shoot resource) and just “forwarded” to the extension controller. Only the AWS controller understands this configuration and will now start provisioning/reconciling the infrastructure. It reports in the .status field the result:\nstatus:  observedGeneration: ...  state: ...  lastError: ..  lastOperation: ...  providerStatus:  apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureStatus  vpc:  id: vpc-1234  subnets:  - id: subnet-acbd1234  name: workers  zone: eu-west-1  securityGroups:  - id: sg-xyz12345  name: workers  iam:  nodesRoleARN: \u003csome-arn\u003e  instanceProfileName: foo  ec2:  keyName: bar Gardener waits until the .status.lastOperation/.status.lastError indicates that the operation reached a final state and either continuous with the next step or stops and reports the potential error. The extension-specific output in .status.providerStatus is - similar to .spec.providerConfig - not evaluated and simply forwarded to CRDs in subsequent steps.\nExample 2:\nGardener deploys the control plane components into the seed cluster, e.g. the kube-controller-manager deployment with the following flags:\napiVersion: apps/v1 kind: Deployment ... spec:  template:  spec:  containers:  - command:  - /usr/local/bin/kube-controller-manager  - --allocate-node-cidrs=true  - --attach-detach-reconcile-sync-period=1m0s  - --controllers=*,bootstrapsigner,tokencleaner  - --cluster-cidr=100.96.0.0/11  - --cluster-name=shoot--core--aws-01  - --cluster-signing-cert-file=/srv/kubernetes/ca/ca.crt  - --cluster-signing-key-file=/srv/kubernetes/ca/ca.key  - --concurrent-deployment-syncs=10  - --concurrent-replicaset-syncs=10 ... The AWS controller requires some additional flags in order to make the cluster functional. It needs to provide a Kubernetes cloud-config and also some cloud-specific flags. Consequently, it registers a MutatingWebhookConfiguration on Deployments and adds these flags to the container:\n - --cloud-provider=external  - --external-cloud-volume-plugin=aws  - --cloud-config=/etc/kubernetes/cloudprovider/cloudprovider.conf Of course, it would have needed to create a ConfigMap containing the cloud config and to add the proper volume and volumeMounts to the manifest as well.\n(Please note for this special example: The Kubernetes community is also working on making the kube-controller-manager provider-independent. However, there will most probably be still components other than the kube-controller-manager which need to be adapted by extensions.)\nIf you are interested in writing an extension, or generally in digging deeper to find out the nitty-gritty details of the extension concepts please read GEP-1. We are truly looking forward to your feedback!\nCurrent status Meanwhile, the out-of-tree extension architecture of Gardener is in place and has been productively validated. We are tracking all internal and external extensions of Gardener in the repo: Gardener Extensions Library.\n","categories":"","description":"","excerpt":"Extensibility overview Initially, everything was developed in-tree in …","ref":"/docs/gardener/extensions/overview/","tags":"","title":"Overview"},{"body":"Migrating From PodSecurityPolicys To PodSecurity Admission Controller Kubernetes has deprecated the PodSecurityPolicy API in v1.21 and it will be removed in v1.25. With v1.23, a new feature called PodSecurity was promoted to beta. From v1.25 onwards, there will be no API serving PodSecurityPolicys, so you have to cleanup all the existing PSPs before upgrading your cluster. Detailed migration steps are described here.\nAfter migration, you should disable the PodSecurityPolicy admission plugin. To do so, you have to add:\nadmissionPlugins: - name: PodSecurityPolicy  disabled: true in spec.kubernetes.kubeAPIServer.admissionPlugins field in the Shoot resource. Please refer the example Shoot manifest here.\nOnly if the PodSecurityPolicy admission plugin is disabled the cluster can be upgraded to v1.25.\n ⚠️ You should disable the admission plugin and wait until Gardener finish at least one Shoot reconciliation before upgrading to v1.25. This is to make sure all the PodSecurityPolicy related resources deployed by Gardener are cleaned up.\n Admission Configuration For The PodSecurity Admission Plugin If you wish to add your custom configuration for the PodSecurity plugin and your cluster version is v1.23+, you can do so in the Shoot spec under .spec.kubernetes.kubeAPIServer.admissionPlugins by adding:\nadmissionPlugins: - name: PodSecurity  config:  apiVersion: pod-security.admission.config.k8s.io/v1beta1  kind: PodSecurityConfiguration  # Defaults applied when a mode label is not set.  #  # Level label values must be one of:  # - \"privileged\" (default)  # - \"baseline\"  # - \"restricted\"  #  # Version label values must be one of:  # - \"latest\" (default)   # - specific version like \"v1.24\"  defaults:  enforce: \"privileged\"  enforce-version: \"latest\"  audit: \"privileged\"  audit-version: \"latest\"  warn: \"privileged\"  warn-version: \"latest\"  exemptions:  # Array of authenticated usernames to exempt.  usernames: []  # Array of runtime class names to exempt.  runtimeClasses: []  # Array of namespaces to exempt.  namespaces: [] If your cluster version is v1.22, use apiVersion: pod-security.admission.config.k8s.io/v1alpha1. Please note that in v1.22 the feature gate PodSecurity is not enabled by default. You have to add:\nfeatureGates:  PodSecurity: true under .spec.kubernetes.kubeAPIServer. For proper functioning of Gardener, kube-system namespace will also be automatically added to the exemptions.namespaces list.\n.spec.kubernetes.allowPrivilegedContainers in the Shoot spec If this field is set to true then all authenticated users can use the “gardener.privileged” PodSecurityPolicy, allowing full unrestricted access to Pod features. However, the PodSecurityPolicy admission plugin is removed in Kubernetes v1.25 and PodSecurity has taken its place as its successor. Therefore, this field doesn’t have any relevance in versions \u003e= v1.25 anymore. If you need to set a default pod admission level for your cluster, follow this documentation.\n","categories":"","description":"","excerpt":"Migrating From PodSecurityPolicys To PodSecurity Admission Controller …","ref":"/docs/gardener/usage/pod-security/","tags":"","title":"Pod Security"},{"body":"vSphere / NSX-T Preparation for Gardener Extension “vSphere Provider”  vSphere / NSX-T Preparation for Gardener Extension “vSphere Provider”  vSphere Preparation  User and Role Creation  vCenter/vSphere NSX-T   Create Folders Upload VM Templates for Worker Nodes Prepare for Kubernetes Zones and Regions  Create Resource Pool(s) Tag Regions and Zones Storage policies  Tag Zone Storages Create or clone VM Storage Policy       NSX-T Preparation  Create IP pools  Sizing the IP pools   Check edge cluster sizing   Get VDS UUIDs    Several preparational steps are necessary for VMware vSphere and NSX-T, before this extension can be used to create Gardener shoot clusters.\nThe main version target of this extension is vSphere 7.x together with NSX-T 3.x. The recommended environment is a system setup with VMware Cloud Foundation (VCF) 4.1. Older versions like vSphere 6.7U3 with NSX-T 2.5 or 3.0 should still work, but are not tested extensively.\nvSphere Preparation User and Role Creation This extension needs credentials for both the vSphere/vCenter and the NSX-T endpoints. This section guides through the creation of appropriate roles and users.\nvCenter/vSphere The vCenter/vSphere user used for this provider should have been assigned to a role including these permissions (use vCenter/vSphere Client / Menu Administration / Access Control / Role to define a role and assign it to the user with Global Permissions)\n Datastore  Allocate space Browse datastore Low level file operations Remove file Update virtual machine files Update virtual machine metadata   Global  Cancel task Manage custom attributes Set custom attribute   Network  Assign network   Resource  Assign virtual machine to resource pool   Tasks  Create task Update task   vApp  Add virtual machine Assign resource pool Assign vApp Clone Power off Power on View OVF environment vApp application configuration vApp instance configuration vApp managedBy configuration vApp resource configuration   Virtual machine  Change Configuration  Acquire disk lease Add existing disk Add new disk Add or remove device Advanced configuration Change CPU count Change Memory Change Settings Change Swapfile placement Change resource Configure Host USB device Configure Raw device Configure managedBy Display connection settings Extend virtual disk Modify device settings Query Fault Tolerance compatibility Query unowned files Reload from path Remove disk Rename Reset guest information Set annotation Toggle disk change tracking Toggle fork parent Upgrade virtual machine compatibility   Edit Inventory  Create from existing Create new Move Register Remove Unregister   Guest operations  Guest operation alias modification Guest operation alias query Guest operation modifications Guest operation program execution Guest operation queries   Interaction  Power off Power on Reset   Provisioning  Allow disk access Allow file access Allow read-only disk access Allow virtual machine files upload Clone template Clone virtual machine Customize guest Deploy template Mark as virtual machine Modify customization specification Promote disks Read customization specifications      NSX-T The NSX-T API is accessed from the infrastructure controller of the vsphere-provider for setting up the network infrastructure resources and the cloud-controller-manager for managing load balancers. Currently, the NSX-T user must have the Enterprise Admin role.\nCreate Folders Two folders need to be created: - a folder which will contain the VMs of the shoots (cloud profile spec.providerConfig.folder) - a folder containing templates (used by cloud profile spec.providerConfig.machineImages[*].versions[*].path)\nIn vSphere client:\n From the Menu in the vSphere Client toolbar choose VMs and Templates Select the vSphere Datacenter of the work load vCenter in the browser From the context menu select New Folder \u003e New VM and Template Folder, set folder name to e.g. “gardener” From the context menu of the new folder gardener select New Folder, set folder name to “templates”  Upload VM Templates for Worker Nodes Upload gardenlinux OVA or flatcar OVA templates.\n From the context menu of the folder gardener/templates choose Deploy OVF Template… Adjust name if needed Select any compute cluster as compute resource Select a storage (e.g. VSAN) Select any network (not important) No need to customize the template After deployment is finished select from the context menu of the new deployed VM Template \u003e Convert To Template  Prepare for Kubernetes Zones and Regions This step has to be done regardless of whether you actually have more than a single region and zone or not! Two labels need to be defined in the cloud profile (section spec.providerConfig.failureDomainLabels):\n failureDomainLabels:  region: k8s-region  zone: k8s-zone A Kubernetes zone can either be a vCenter or one of its datacenters\nZones must be sub-resources of it. If the region is a complete vCenter, the zone must specify datacenter and either compute cluster or resource pool. Otherwise, i.e. tf the region is a datacenter, the zone must specify either compute cluster or resource pool.\nIn the following steps it is assumed: - the region is specified by a datacenter - the zone is specified by a compute cluster or one of its resource pools\nCreate Resource Pool(s) Create a resource pool for every zone:\n From the Menu in the vSphere Client toolbar choose Hosts and Clusters From the context menu of the compute cluster select New Resource Pool… and provide the name of the zone. CPU and Memory settings are optional.  Tag Regions and Zones Each zone must be tagged with the category defined by the label defined in the cloud profile (spec.providerConfig.failureDomainLabels.region). Assuming that the region is a datacenter and the region label is k8s-region:\n From the Menu in the vSphere Client toolbar choose Hosts and Clusters Select the region’s datacenter in the browser In the Summary tab there is a sub-window titled Tags. Click the Assign… link. In the Assign Tag dialog select the ADD TAG link above of the table In the Create Tag dialog choose the k8s-region category. If it is not defined, click the Create New Category link to create the category. Enter the Name of the region. Back in the Assign Tag mark the checkbox of the region tag you just have created. Click the ASSIGN button  Assuming that the zones are specified by resource pools and the zone label is k8s-zone:\n From the Menu in the vSphere Client toolbar choose Hosts and Clusters Select the zone’s Compute Cluster in the browser In the Summary tab there is a sub-window titled Tags. Click the Assign… link. In the Assign Tag dialog select the ADD TAG link above of the table In the Create Tag dialog choose the k8s-zone category. If it is not defined, click the Create New Category link to create the category. Enter the Name of the zone. Back in the Assign Tag mark the checkbox of the zone tag you just have created. Click the ASSIGN button  Storage policies Each zone can have a separate storage. In this case a storage policy is needed to be compatible with all the zone storages.\nTag Zone Storages For each zone tag the storage with the corresponding k8s-zone tag for the zone.\n From the Menu in the vSphere Client toolbar choose Storage Select the zone’s storage in the browser In the Summary tab there is a sub-window titled Tags. Click the Assign… link. In the Assign Tag dialog select the ADD TAG link above of the table In the Create Tag dialog choose the k8s-zone category. If it is not defined, click the Create New Category link to create the category. Enter the Name of the zone. Back in the Assign Tag mark the checkbox of the zone tag you just have created. Click the ASSIGN button  Create or clone VM Storage Policy   From the Menu in the vSphere Client toolbar choose Policies and Profiles\n  In the Policies and Profiles list select VM Storage Policies\n  Create or clone an existing storage policy\na) set name, e.g. “ Storage Policy” (will be needed for the cloud profile later in spec.providerConfig.defaultClassStoragePolicyName)\nb) On the page Policy structure check only the checkbox Enable tag based placement rules\nc) On the page Tage based placement press the ADD TAG RULE button.\nd) For Rule 1 select\n*Tag category* = *k8s-zone* *Usage option* = *Use storage tagged with* *Tags* = *all zone tags*. e) Validate the compatible storages on the page Storage compatibility\nf) Press FINISH on the Review and finish page\n  IMPORTANT: Repeat steps 1-3 and create a second StoragePolicy by the name of garden-etcd-fast-main. This will be used by Gardener to provision shoot’s etcd PVCs.\n  NSX-T Preparation A shared NSX-T is needed for all zones of a region. External IP address ranges are needed for SNAT and load balancers. Besides the edge cluster must be sized large enough to deal with the load balancers of all shoots.\nCreate IP pools Two IP pools are needed for external IP addresses.\n IP pool for SNAT The IP pool name needs to be specified in the cloud profile at spec.providerConfig.regions[*].snatIPPool. Each shoot cluster needs one SNAT IP address for outgoing traffic. IP pool(s) for the load balancers The IP pool name(s) need to be specified in the cloud profile at spec.providerConfig.contraints.loadBalancerConfig.classes[*].ipPoolName. An IP address is needed for every port of every Kubernetes service of type LoadBalancer.  To create them, follow these steps in the NSX-T Manager UI in the web browser:\n From the toolbar at the top of the page choose Networking From the left side list choose IP Address Pools below the IP Management Press the ADD IP ADRESS POOL button Enter Name Enter at least one subnet by clicking on Sets Press the Save button  Sizing the IP pools Each shoot cluster needs one IP address for SNAT and at least two IP addresses for load balancers VIPs (kube-apiservcer and Gardener shoot-seed VPN). A third IP address may be needed for ingress. Depending on the payload of a shoot cluster, there may be additional services of type LoadBalancer. An IP address is needed for every port of every Kubernetes service of type LoadBalancer.\nCheck edge cluster sizing For load balancer related configurations limitations of NSX-T, please see the web pages VMWare Configuration Maximums. The link shows the limitations for NSX-T 3.1, if you have another version, please select the version from the left panel under Select Version and press the VIEW LIMITS button to update the view.\nBy default, settings, each shoot cluster has an own T1 gateway and an own LB service (instance) of “T-shirt” size SMALL.\nExamples for limitations on NSX-T 3.1 using Large Edge Node and SMALL load balancers instances:\n  There is a limit of 40 small LB instances per egde cluster (for HA 40 per pair of edge nodes)\n=\u003e maximum number of shoot clusters = 40 * (number of edge nodes) / 2\n  For SMALL load balancers, there is a maximum of 20 virtual servers. A virtual server is needed for every port of a service of type LoadBalancer\n=\u003e maximum number of services/ports pairs = 20 * (number of edge nodes) / 2\nThe load balancer “T-shirt” size can be set on cloud profile level (spec.providerConfig.contraints.loadBalancerConfig.size) or in the shoot manifest (spec.provider.controlPlaneConfig.loadBalancerSize)\n  The number of pool members is limited to 7,500. For every K8s service port, every worker node is a pool member.\n=\u003e If every shoot cluster has an average number of 15 worker nodes, there can be 500 service/port pairs over all shoot clusters per pair of edge nodes\n  Get VDS UUIDs This step is only needed, if there are several VDS (virtual distributed switches) for each zone.\nIn this case, their UUIDs need to be fetched and set in the cloud profile at spec.providerConfig.regions[*].zones[*].switchUuid.\nUnfortunately, they are not displayed in the vSphere Client.\nHere the command line tool govc is used to look them up.\n Run govc find / -type DistributedVirtualSwitch to get the full path of all vds/dvs For each switch run govc dvs.portgroup.info \u003cswitch-path\u003e | grep DvsUuid  ","categories":"","description":"","excerpt":"vSphere / NSX-T Preparation for Gardener Extension “vSphere Provider” …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/docs/prepare-vsphere/","tags":"","title":"Prepare Vsphere"},{"body":"Setting up the usage environment  Setting up the usage environment  Important ⚠️ Set KUBECONFIG Replace provider credentials and desired VM configurations Deploy required CRDs and Objects Check current cluster state    Important ⚠️  All paths are relative to the root location of this project repository.\n  Run the Machine Controller Manager either as described in Setting up a local development environment or Deploying the Machine Controller Manager into a Kubernetes cluster.\n  Make sure that the following steps are run before managing machines/ machine-sets/ machine-deploys.\n Set KUBECONFIG Using the existing Kubeconfig, open another Terminal panel/window with the KUBECONFIG environment variable pointing to this Kubeconfig file as shown below,\n$ export KUBECONFIG=\u003cPATH_TO_REPO\u003e/dev/kubeconfig.yaml Replace provider credentials and desired VM configurations Open kubernetes/machine_classes/aws-machine-class.yaml and replace required values there with the desired VM configurations.\nSimilarily open kubernetes/secrets/aws-secret.yaml and replace - userData, providerAccessKeyId, providerSecretAccessKey with base64 encoded values of cloudconfig file, AWS access key id, and AWS secret access key respectively. Use the following command to get the base64 encoded value of your details\n$ echo \"sample-cloud-config\" | base64 base64-encoded-cloud-config Do the same for your access key id and secret access key.\nDeploy required CRDs and Objects Create all the required CRDs in the cluster using kubernetes/crds.yaml\n$ kubectl apply -f kubernetes/crds.yaml Create the class template that will be used as an machine template to create VMs using kubernetes/machine_classes/aws-machine-class.yaml\n$ kubectl apply -f kubernetes/machine_classes/aws-machine-class.yaml Create the secret used for the cloud credentials and cloudconfig using kubernetes/secrets/aws-secret.yaml\n$ kubectl apply -f kubernetes/secrets/aws-secret.yaml Check current cluster state Get to know the current cluster state using the following commands,\n Checking aws-machine-class in the cluster  $ kubectl get awsmachineclass NAME MACHINE TYPE AMI AGE test-aws t2.large ami-123456 5m  Checking kubernetes secrets in the cluster  $ kubectl get secret NAME TYPE DATA AGE test-secret Opaque 3 21h  Checking kubernetes nodes in the cluster  $ kubectl get nodes Lists the default set of nodes attached to your cluster\n Checking Machine Controller Manager machines in the cluster  $ kubectl get machine No resources found.  Checking Machine Controller Manager machine-sets in the cluster  $ kubectl get machineset No resources found.  Checking Machine Controller Manager machine-deploys in the cluster  $ kubectl get machinedeployment No resources found. ","categories":"","description":"","excerpt":"Setting up the usage environment  Setting up the usage environment …","ref":"/docs/other-components/machine-controller-manager/docs/usage/prerequisite/","tags":"","title":"Prerequisite"},{"body":"PriorityClasses in Gardener Clusters Gardener makes use of PriorityClasses to improve overall robustness of the system. In order to benefit from the full potential of PriorityClasses, gardenlet manages a set of well-known PriorityClasses with fine-granular priority values.\nAll components of the system should use these well-known PriorityClasses instead of creating and using separate ones with arbitrary values, which would compromise the overall goal of using PriorityClasses in the first place. Gardenlet manages the well-known PriorityClasses listed in this document, so that third parties (e.g., Gardener extensions) can rely on them to be present when deploying components to Seed and Shoot clusters.\nThe listed well-known PriorityClasses follow this rough concept:\n Values are close to the maximum that can be declared by the user. This is important to ensure that Shoot system components have higher priority than the workload deployed by end-users. Values have a bit of headroom in between to ensure flexibility when the need for intermediate priority values arises. Values of PriorityClasses created on Seed clusters are lower than the ones on Shoots to ensure that Shoot system components have higher priority than Seed components, if the Seed is backed by a Shoot (ManagedSeed), e.g. coredns should have higher priority than gardenlet. Names simply include the last digits of the value to minimize confusion caused by many (similar) names like critical, importance-high, etc.  PriorityClasses for Shoot System Components    Name Priority Associated Components (Examples)     system-node-critical (created by Kubernetes) 2000001000 calico-node, kube-proxy, apiserver-proxy, csi-driver, egress-filter-applier   system-cluster-critical (created by Kubernetes) 2000000000 calico-typha, calico-kube-controllers, coredns, vpn-shoot   gardener-shoot-system-900 999999900 node-problem-detector   gardener-shoot-system-800 999999800 calico-typha-horizontal-autoscaler, calico-typha-vertical-autoscaler   gardener-shoot-system-700 999999700 blackbox-exporter, node-exporter   gardener-shoot-system-600 999999600 addons-nginx-ingress-controller, addons-nginx-ingress-k8s-backend, kubernetes-dashboard, kubernetes-metrics-scraper    PriorityClasses for Seed System Components    Name Priority Associated Components (Examples)     gardener-system-critical 999998950 gardenlet, gardener-resource-manager, istio-ingressgateway, istiod   gardener-system-900 999998900 Extensions, gardener-seed-admission-controller, reversed-vpn-auth-server   gardener-system-800 999998800 dependency-watchdog-endpoint, dependency-watchdog-probe, etcd-druid, (auditlog-)mutator, vpa-admission-controller   gardener-system-700 999998700 auditlog-seed-controller, hvpa-controller, vpa-recommender, vpa-updater   gardener-system-600 999998600 aggregate-alertmanager, alertmanager, fluent-bit, grafana, kube-state-metrics, nginx-ingress-controller, nginx-k8s-backend, prometheus, loki, seed-prometheus   gardener-reserve-excess-capacity -5 reserve-excess-capacity (ref)    PriorityClasses for Shoot Control Plane Components    Name Priority Associated Components (Examples)     gardener-system-500 999998500 etcd-events, etcd-main, kube-apiserver   gardener-system-400 999998400 gardener-resource-manager   gardener-system-300 999998300 cloud-controller-manager, cluster-autoscaler, csi-driver-controller, kube-controller-manager, kube-scheduler, machine-controller-manager, terraformer, vpn-seed-server   gardener-system-200 999998200 csi-snapshot-controller, csi-snapshot-validation, cert-controller-manager, shoot-dns-service, vpa-admission-controller, vpa-recommender, vpa-updater   gardener-system-100 999998100 alertmanager, grafana-operators, grafana-users, kube-state-metrics, prometheus, loki, event-logger    There is also a legacy PriorityClass called gardener-shoot-controlplane with value 100. This PriorityClass is deprecated and will be removed in a future release. Make sure to migrate all your components to the above listed fine-granular PriorityClasses.\n","categories":"","description":"","excerpt":"PriorityClasses in Gardener Clusters Gardener makes use of …","ref":"/docs/gardener/development/priority-classes/","tags":"","title":"Priority Classes"},{"body":"Releases, Features, Hotfixes This document describes how to contribute features or hotfixes, and how new Gardener releases are usually scheduled, validated, etc.\n Releases Contributing new Features or Fixes Cherry Picks  Releases The @gardener-maintainers are trying to provide a new release roughly every other week (depending on their capacity and the stability/robustness of the master branch).\nHotfixes are usually maintained for the latest three minor releases, though, there are no fixed release dates.\nRelease Responsible Plan    Version Week No Begin Validation Phase Due Date Release Responsible     v1.56 Week 37-38 September 12, 2022 September 25, 2022 @shafeeqes   v1.57 Week 39-40 September 26, 2022 October 9, 2022 @ary1992   v1.58 Week 41-42 October 10, 2022 October 23, 2022 @plkokanov   v1.59 Week 43-44 October 24, 2022 November 6, 2022 @rfranzke   v1.60 Week 45-46 November 7, 2022 November 20, 2022 @acumino   v1.61 Week 47-48 November 21, 2022 December 4, 2022 @ialidzhikov   v1.62 Week 49-50 December 5, 2022 December 18, 2022 @oliver-goetz   v1.63 Week 01-04 January 2, 2023 January 29, 2023 @shafeeqes    Apart from the release of the next version, the release responsible is also taking care of potential hotfix releases of the last three minor versions. The release responsible is the main contact person for coordinating new feature PRs for the next minor versions or cherry-pick PRs for the last three minor versions.\n Click to expand the archived release responsible associations!    Version Week No Begin Validation Phase Due Date Release Responsible     v1.17 Week 07-08 February 15, 2021 February 28, 2021 @rfranzke   v1.18 Week 09-10 March 1, 2021 March 14, 2021 @danielfoehrKn   v1.19 Week 11-12 March 15, 2021 March 28, 2021 @timebertt   v1.20 Week 13-14 March 29, 2021 April 11, 2021 @vpnachev   v1.21 Week 15-16 April 12, 2021 April 25, 2021 @timuthy   v1.22 Week 17-18 April 26, 2021 May 9, 2021 @BeckerMax   v1.23 Week 19-20 May 10, 2021 May 23, 2021 @ialidzhikov   v1.24 Week 21-22 May 24, 2021 June 5, 2021 @stoyanr   v1.25 Week 23-24 June 7, 2021 June 20, 2021 @rfranzke   v1.26 Week 25-26 June 21, 2021 July 4, 2021 @danielfoehrKn   v1.27 Week 27-28 July 5, 2021 July 18, 2021 @timebertt   v1.28 Week 29-30 July 19, 2021 August 1, 2021 @ialidzhikov   v1.29 Week 31-32 August 2, 2021 August 15, 2021 @timuthy   v1.30 Week 33-34 August 16, 2021 August 29, 2021 @BeckerMax   v1.31 Week 35-36 August 30, 2021 September 12, 2021 @stoyanr   v1.32 Week 37-38 September 13, 2021 September 26, 2021 @vpnachev   v1.33 Week 39-40 September 27, 2021 October 10, 2021 @voelzmo   v1.34 Week 41-42 October 11, 2021 October 24, 2021 @plkokanov   v1.35 Week 43-44 October 25, 2021 November 7, 2021 @kris94   v1.36 Week 45-46 November 8, 2021 November 21, 2021 @timebertt   v1.37 Week 47-48 November 22, 2021 December 5, 2021 @danielfoehrKn   v1.38 Week 49-50 December 6, 2021 December 19, 2021 @rfranzke   v1.39 Week 01-04 January 3, 2022 January 30, 2022 @ialidzhikov, @timuthy   v1.40 Week 05-06 January 31, 2022 February 13, 2022 @BeckerMax   v1.41 Week 07-08 February 14, 2022 February 27, 2022 @plkokanov   v1.42 Week 09-10 February 28, 2022 March 13, 2022 @kris94   v1.43 Week 11-12 March 14, 2022 March 27, 2022 @rfranzke   v1.44 Week 13-14 March 28, 2022 April 10, 2022 @timebertt   v1.45 Week 15-16 April 11, 2022 April 24, 2022 @acumino   v1.46 Week 17-18 April 25, 2022 May 8, 2022 @ialidzhikov   v1.47 Week 19-20 May 9, 2022 May 22, 2022 @shafeeqes   v1.48 Week 21-22 May 23, 2022 June 5, 2022 @ary1992   v1.49 Week 23-24 June 6, 2022 June 19, 2022 @plkokanov   v1.50 Week 25-26 June 20, 2022 July 3, 2022 @rfranzke   v1.51 Week 27-28 July 4, 2022 July 17, 2022 @timebertt   v1.52 Week 29-30 July 18, 2022 July 31, 2022 @acumino   v1.53 Week 31-32 August 1, 2022 August 14, 2022 @kris94   v1.54 Week 33-34 August 15, 2022 August 28, 2022 @ialidzhikov   v1.55 Week 35-36 August 29, 2022 September 11, 2022 @oliver-goetz     Release Validation The release phase for a new minor version lasts two weeks. Typically, the first week is used for the validation of the release. This phase includes the following steps:\n master (or latest release-* branch) is deployed to a development landscape that already hosts some existing seed and shoot clusters. An extended test suite is triggered by the “release responsible” which  executes the Gardener integration tests for different Kubernetes versions, infrastructures, and Shoot settings. executes the Kubernetes conformance tests. executes further tests like Kubernetes/OS patch/minor version upgrades.   Additionally, every four hours (or on demand) more tests (e.g., including the Kubernetes e2e test suite) are executed for different infrastructures. The “release responsible” is verifying new features or other notable changes (derived of the draft release notes) in this development system.  Usually, the new release is triggered in the beginning of the second week if all tests are green, all checks were successful, and if all of the planned verifications were performed by the release responsible.\nContributing new Features or Fixes Please refer to the Gardener contributor guide. Besides a lot of a general information, it also provides a checklist for newly created pull requests that may help you to prepare your changes for an efficient review process. If you are contributing a fix or major improvement, please take care to open cherry-pick PRs to all affected and still supported versions once the change is approved and merged in the master branch.\n⚠️ Please ensure that your modifications pass the verification checks (linting, formatting, static code checks, tests, etc.) by executing\nmake verify before filing your pull request.\nThe guide applies for both changes to the master and to any release-* branch. All changes must be submitted via a pull request and be reviewed and approved by at least one code owner.\nCherry Picks This section explains how to initiate cherry picks on release branches within the gardener/gardener repository.\n Prerequisites Initiate a Cherry Pick  Prerequisites Before you initiate a cherry pick, make sure that the following prerequisites are accomplished.\n A pull request merged against the master branch. The release branch exists (check in the branches section) Have the gardener/gardener repository cloned as follows:  the origin remote should point to your fork (alternatively this can be overwritten by passing FORK_REMOTE=\u003cfork-remote\u003e) the upstream remote should point to the Gardener github org (alternatively this can be overwritten by passing UPSTREAM_REMOTE=\u003cupstream-remote\u003e)   Have hub installed, which is most easily installed via go get github.com/github/hub assuming you have a standard golang development environment. A github token which has permissions to create a PR in an upstream branch.  Initiate a Cherry Pick   Run the cherry pick script\nThis example applies a master branch PR #3632 to the remote branch upstream/release-v3.14:\nGITHUB_USER=\u003cyour-user\u003e hack/cherry-pick-pull.sh upstream/release-v3.14 3632   Be aware the cherry pick script assumes you have a git remote called upstream that points at the Gardener github org.\n  You will need to run the cherry pick script separately for each patch release you want to cherry pick to. Cherry picks should be applied to all active release branches where the fix is applicable.\n  When asked for your github password, provide the created github token rather than your actual github password. Refer https://github.com/github/hub/issues/2655#issuecomment-735836048\n    ","categories":"","description":"","excerpt":"Releases, Features, Hotfixes This document describes how to contribute …","ref":"/docs/gardener/development/process/","tags":"","title":"Process"},{"body":"Profiling Gardener Components Similar to Kubernetes, Gardener components support profiling using standard Go tools for analyzing CPU and memory usage by different code sections and more. This document shows how to enable and use profiling handlers with Gardener components.\nEnabling profiling handlers and the ports on which they are exposed differs between components. However, once the handlers are enabled, they provide profiles via the same HTTP endpoint paths, from which you can retrieve them via curl/wget or directly using go tool pprof. (You might need to use kubectl port-forward in order to access HTTP endpoints of Gardener components running in clusters.)\nFor example (gardener-controller-manager):\n$ curl http://localhost:2718/debug/pprof/heap \u003e /tmp/heap-controller-manager $ go tool pprof /tmp/heap-controller-manager Type: inuse_space Time: Sep 3, 2021 at 10:05am (CEST) Entering interactive mode (type \"help\" for commands, \"o\" for options) (pprof) or\n$ go tool pprof http://localhost:2718/debug/pprof/heap Fetching profile over HTTP from http://localhost:2718/debug/pprof/heap Saved profile in /Users/timebertt/pprof/pprof.alloc_objects.alloc_space.inuse_objects.inuse_space.008.pb.gz Type: inuse_space Time: Sep 3, 2021 at 10:05am (CEST) Entering interactive mode (type \"help\" for commands, \"o\" for options) (pprof) gardener-apiserver gardener-apiserver provides the same flags as kube-apiserver for enabling profiling handlers (enabled by default):\n--contention-profiling Enable lock contention profiling, if profiling is enabled --profiling Enable profiling via web interface host:port/debug/pprof/ (default true) The handlers are served on the same port as the API endpoints (configured via --secure-port). This means, you will also have to authenticate against the API server according to the configured authentication and authorization policy.\nFor example, in the local-setup you can use:\n$ curl -k --cert ./hack/local-development/local-garden/certificates/certs/default-admin.crt --key ./hack/local-development/local-garden/certificates/keys/default-admin.key https://localhost:8443/debug/pprof/heap \u003e /tmp/heap $ go tool pprof /tmp/heap gardener-admission-controller, gardener-controller-manager, gardener-scheduler, gardenlet gardener-controller-manager, gardener-admission-controller, gardener-scheduler and gardenlet also allow enabling profiling handlers via their respective component configs (currently disabled by default). Here is an example for the gardener-admission-controller’s configuration and how to enable it (it looks similar for the other components):\napiVersion: admissioncontroller.config.gardener.cloud/v1alpha1 kind: AdmissionControllerConfiguration # ... server:  metrics:  port: 2723 debugging:  enableProfiling: true  enableContentionProfiling: true However, the handlers are served on the same port as configured in server.metrics.port via HTTP.\nFor example (gardener-admission-controller):\n$ curl http://localhost:2723/debug/pprof/heap \u003e /tmp/heap $ go tool pprof /tmp/heap gardener-seed-admission-controller, gardener-resource-manager gardener-seed-admission-controller and gardener-resource-manager provide the following flags for enabling profiling handlers (disabled by default):\n--contention-profiling Enable lock contention profiling, if profiling is enabled --profiling Enable profiling via web interface host:port/debug/pprof/ The handlers are served on the same port as configured in the --metrics-bind-address flag (defaults to \":8080\") via HTTP.\nFor example (gardener-seed-admission-controller):\n$ curl http://localhost:8080/debug/pprof/heap \u003e /tmp/heap $ go tool pprof /tmp/heap ","categories":"","description":"","excerpt":"Profiling Gardener Components Similar to Kubernetes, Gardener …","ref":"/docs/gardener/monitoring/profiling/","tags":"","title":"Profiling"},{"body":"Project Namespace Access Service Account Manager With Gardener v1.47 a new role called serviceaccountmanager was introduced. This role allows to fully manage ServiceAccount’s in the project namespace and request tokens for them. This is the preferred way of managing the access to a project namespace as it aims to replace the usage of the default ServiceAccount secrets that will no longer be generated automatically with Kubernetes v1.24+.\nCreate a Service Account Once given the serviceaccountmanager role a user can create/update/delete ServiceAccounts in the project namespace. In order to create a ServiceAccount named “robot-user” run the following kubectl command:\nkubectl -n project-abc create sa robot-user Request a token for a Service Account A token for the “robot-user” ServiceAccount can be requested via the TokenRequest API in several ways:\n using kubectl \u003e= v1.24  kubectl -n project-abc create token robot-user --duration=3600s  using kubectl \u003c v1.24  cat \u003c\u003cEOF | kubectl create -f - --raw /api/v1/namespaces/project-abc/serviceaccounts/robot-user/token { \"apiVersion\": \"authentication.k8s.io/v1\", \"kind\": \"TokenRequest\", \"spec\": { \"expirationSeconds\": 3600 } } EOF  directly calling the Kubernetes HTTP API  curl -X POST https://api.gardener/api/v1/namespaces/project-abc/serviceaccounts/robot-user/token \\  -H \"Authorization: Bearer \u003cauth-token\u003e\" \\  -H \"Content-Type: application/json\" \\  -d '{ \"apiVersion\": \"authentication.k8s.io/v1\", \"kind\": \"TokenRequest\", \"spec\": { \"expirationSeconds\": 3600 } }' Mind that the returned token is not stored within the Kubernetes cluster, will be valid for 3600 seconds and will be invalidated if the “robot-user” ServiceAccount is deleted. Although expirationSeconds can be modified depending on the needs, the returned token’s validity will not exceed the configured service-account-max-token-expiration duration for the garden cluster. It is advised that the actual expirationTimestamp is verified so that expectations are met. This can be done by asserting the expirationTimestamp in the TokenRequestStatus or the exp claim in the token itself.\nDelete a Service Account In order to delete the ServiceAccount named “robot-user” run the following kubectl command:\nkubectl -n project-abc delete sa robot-user This will invalidate all existing tokens for the “robot-user” ServiceAccount.\n","categories":"","description":"","excerpt":"Project Namespace Access Service Account Manager With Gardener v1.47 a …","ref":"/docs/gardener/usage/project_namespace_access/","tags":"","title":"Project Namespace Access"},{"body":"Project Operations This section demonstrates how to use the standard Kubernetes tool for cluster operation kubectl for common cluster operations with emphasis on Gardener resources. For more information on kubectl, see kubectl on kubernetes.io.\n Prerequisites Downloading kubeconfig for remote project operations List Gardener API resources Check your permissions Working with projects Working with clusters  Prerequisites  You’re logged on to the Gardener Dashboard. You’ve created a cluster and its status is operational.  It’s recommended that you get acquainted with the resources in the Gardener API.\nDownloading kubeconfig for remote project operations The kubeconfig for project operations is different from the one for cluster operations. It has a larger scope and allows a different set of operations that are applicable for a project administrator role, such as lifecycle control on clusters and managing project members.\nDepending on your goal, you create a service account suitable for automation and download its kubeconfig, or you can get a user-specific kubeconfig. The difference is the identity on behalf of which the operations are performed.\nDownload kubeconfig for a user Kubernetes doesn’t offer an own resource type for human users that access the API server. Instead, you either have to manage unique user strings, or use an OpenID-Connect (OIDC) compatible Identity Provider (IDP) to do the job.\nOnce the latter is set up, each Gardener user can use the kubelogin plugin for kubectl to authenticate against the API server:\n  Set up kubelogin if you don’t have it yet. More information: kubelogin setup.\n  Open the menu at the top right of the screen, then choose MY ACCOUNT.\n  On the Access card, choose the arrow to see all options for the personalized command-line interface access.\n The personal bearer token that is also offered here only provides access for a limited amount of time for one time operations, for example, in curl commands. The kubeconfig provided for the personalized access is used by kubelogin to grant access to the Gardener API for the user permanently by using a refresh token.\n   Check that the right Project is chosen and keep the settings otherwise. Download the kubeconfig file and add its path to the KUBECONFIG environment variable.\n  You can now execute kubectl commands on the garden cluster using the identity of your user.\nDownload kubeconfig for a Service Account   Go to a service account and choose Download.\n  Add the downloaded kubeconfig to your configuration.\n  You can now execute kubectl commands on the garden cluster using the technical service account.\nList Gardener API resources   Using a kubeconfig for project operations, you can list the Gardner API resources using the following command:\nkubectl api-resources | grep garden The response looks like this:\nbackupbuckets bbc core.gardener.cloud false BackupBucket backupentries bec core.gardener.cloud true BackupEntry cloudprofiles cprofile,cpfl core.gardener.cloud false CloudProfile controllerinstallations ctrlinst core.gardener.cloud false ControllerInstallation controllerregistrations ctrlreg core.gardener.cloud false ControllerRegistration plants pl core.gardener.cloud true Plant projects core.gardener.cloud false Project quotas squota core.gardener.cloud true Quota secretbindings sb core.gardener.cloud true SecretBinding seeds core.gardener.cloud false Seed shoots core.gardener.cloud true Shoot shootstates core.gardener.cloud true ShootState terminals dashboard.gardener.cloud true Terminal clusteropenidconnectpresets coidcps settings.gardener.cloud false ClusterOpenIDConnectPreset openidconnectpresets oidcps settings.gardener.cloud true OpenIDConnectPreset   Enter the following command to view the Gardener API versions:\nkubectl api-versions | grep garden The response looks like this:\ncore.gardener.cloud/v1alpha1 core.gardener.cloud/v1beta1 dashboard.gardener.cloud/v1alpha1 settings.gardener.cloud/v1alpha1   Check your permissions   The operations on project resources are limited by the role of the identity that tries to perform them. To get an overview over your permissions, use the following command:\nkubectl auth can-i --list | grep garden The response looks like this:\nplants.core.gardener.cloud [] [] [create delete deletecollection get list patch update watch] quotas.core.gardener.cloud [] [] [create delete deletecollection get list patch update watch] secretbindings.core.gardener.cloud [] [] [create delete deletecollection get list patch update watch] shoots.core.gardener.cloud [] [] [create delete deletecollection get list patch update watch] terminals.dashboard.gardener.cloud [] [] [create delete deletecollection get list patch update watch] openidconnectpresets.settings.gardener.cloud [] [] [create delete deletecollection get list patch update watch] cloudprofiles.core.gardener.cloud [] [] [get list watch] projects.core.gardener.cloud [] [flowering] [get patch update delete] namespaces [] [garden-flowering] [get]   Try to execute an operation that you aren’t allowed, for example:\nkubectl get projects You receive an error message like this:\nError from server (Forbidden): projects.core.gardener.cloud is forbidden: User \"system:serviceaccount:garden-flowering:robot\" cannot list resource \"projects\" in API group \"core.gardener.cloud\" at the cluster scope   Working with projects   You can get the details for a project, where you (or the service account) is a member.\nkubectl get project flowering The response looks like this:\nNAME NAMESPACE STATUS OWNER CREATOR AGE flowering garden-flowering Ready [PROJECT-ADMIN]@domain [PROJECT-ADMIN]@domain system 45m  For more information, see Project in the API reference.\n   To query the names of the members of a project, use the following command:\nkubectl get project docu -o jsonpath='{.spec.members[*].name }' The response looks like this:\n[PROJECT-ADMIN]@domain system:serviceaccount:garden-flowering:robot  For more information, see members in the API reference.\n   Working with clusters The Gardener domain object for a managed cluster is called Shoot.\nList project clusters To query the clusters in a project:\nkubectl get shoots The output looks like this:\nNAME CLOUDPROFILE VERSION SEED DOMAIN HIBERNATION OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE geranium aws 1.18.3 aws-eu1 geranium.flowering.shoot.\u003ctruncated\u003e Awake Succeeded 100 True True True True 74m Create a new cluster To create a new cluster using the command line, you need a YAML definition of the Shoot resource.\n  To get started, copy the following YAML definition to a new file, for example, daffodil.yaml (or copy file shoot.yaml to daffodil.yaml) and adapt it to your needs.\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata: name: daffodil namespace: garden-flowering spec: secretBindingName: trial-secretbinding-gcp cloudProfileName: gcp region: europe-west1 purpose: evaluation provider: type: gcp infrastructureConfig: kind: InfrastructureConfig apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1 networks: workers: 10.250.0.0/16 controlPlaneConfig: apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1 zone: europe-west1-c kind: ControlPlaneConfig workers: - name: cpu-worker maximum: 2 minimum: 1 maxSurge: 1 maxUnavailable: 0 machine: type: n1-standard-2 image: name: coreos version: 2303.3.0 volume: type: pd-standard size: 50Gi zones: - europe-west1-c networking: type: calico pods: 100.96.0.0/11 nodes: 10.250.0.0/16 services: 100.64.0.0/13 maintenance: timeWindow: begin: 220000+0100 end: 230000+0100 autoUpdate: kubernetesVersion: true machineImageVersion: true hibernation: enabled: true schedules: - start: '00 17 * * 1,2,3,4,5' location: Europe/Kiev kubernetes: allowPrivilegedContainers: true kubeAPIServer: enableBasicAuthentication: false kubeControllerManager: nodeCIDRMaskSize: 24 kubeProxy: mode: IPTables version: 1.18.3 addons: nginxIngress: enabled: false kubernetesDashboard: enabled: false   In your new YAML definition file, replace the value of field metadata.namespace with your namespace following the convention garden-[YOUR-PROJECTNAME].\n  Create a cluster using this manifest (with flag --wait=false the command returns immediately, otherwise it doesn’t return until the process is finished):\nkubectl apply -f daffodil.yaml --wait=false The response looks like this:\nshoot.core.gardener.cloud/daffodil created   It takes 5–10 minutes until the cluster is created. To watch the progress, get all shoots and use the -w flag.\nkubectl get shoots -w   For a more extended example, see Gardener example shoot manifest.\nDelete cluster To delete a shoot cluster, you must first annotate the shoot resource to confirm the operation with confirmation.gardener.cloud/deletion: \"true\":\n  Add the annotation to your manifest (daffodil.yaml in the previous example):\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata: name: daffodil namespace: garden-flowering annotations: confirmation.gardener.cloud/deletion: \"true\" spec: addons: ...   Apply your changes of daffodil.yaml.\nkubectl apply -f daffodil.yaml The response looks like this:\nshoot.core.gardener.cloud/daffodil configured   Trigger the deletion.\nkubectl delete shoot daffodil --wait=false The response looks like this:\nshoot.core.gardener.cloud \"daffodil\" deleted   It takes 5–10 minutes to delete the cluster. To watch the progress, get all shoots and use the -w flag.\nkubectl get shoots -w   Get kubeconfig for a cluster To get the kubeconfig for a cluster:\nkubectl get secrets daffodil.kubeconfig -o jsonpath='{.data.kubeconfig}' | base64 -d The response looks like this:\n--- apiVersion: v1 kind: Config current-context: shoot--flowering--daffodil clusters: - name: shoot--flowering--daffodil cluster: certificate-authority-data: LS0tLS1CRUdJTiBDR \u003ctruncated\u003e server: https://api.daffodil.flowering.shoot.\u003ctruncated\u003e contexts: - name: shoot--flowering--daffodil context: cluster: shoot--flowering--daffodil user: shoot--flowering--daffodil-token users: - name: shoot--flowering--daffodil-token user: token: HbjYIMuR9hmyb9 \u003ctruncated\u003e The name of the Secret containing the kubeconfig is in the form \u003ccluster-name\u003e.kubeconfig, that is, in this example: daffodil.kubeconfig\nRelated Links Working with Service Accounts\nAuthenticating with an Identity Provider.\n","categories":"","description":"","excerpt":"Project Operations This section demonstrates how to use the standard …","ref":"/docs/dashboard/usage/project-operations/","tags":"","title":"Project Operations"},{"body":"Extending project roles The Project resource allows to specify a list of roles for every member (.spec.members[*].roles). There are a few standard roles defined by Gardener itself. Please consult this document for further information.\nHowever, extension controllers running in the garden cluster may also create CustomResourceDefinitions that project members might be able to CRUD. For this purpose Gardener also allows to specify extension roles.\nAn extension role is prefixed with extension:, e.g.\napiVersion: core.gardener.cloud/v1beta1 kind: Project metadata:  name: dev spec:  members:  - apiGroup: rbac.authorization.k8s.io  kind: User  name: alice.doe@example.com  role: admin  roles:  - owner  - extension:foo The project controller will, for every extension role, create a ClusterRole with name gardener.cloud:extension:project:\u003cprojectName\u003e:\u003croleName\u003e, i.e., for above example: gardener.cloud:extension:project:dev:foo. This ClusterRole aggregates other ClusterRoles that are labeled with rbac.gardener.cloud/aggregate-to-extension-role=foo which might be created by extension controllers.\nExtension that might want to contribute to the core admin or viewer roles can use the labels rbac.gardener.cloud/aggregate-to-project-member=true or rbac.gardener.cloud/aggregate-to-project-viewer=true, respectively.\nPlease note that the names of the extension roles are restricted to 20 characters!\nMoreover, the project controller will also create a corresponding RoleBinding with the same name in the project namespace. It will automatically assign all members that are assigned to this extension role.\n","categories":"","description":"","excerpt":"Extending project roles The Project resource allows to specify a list …","ref":"/docs/gardener/extensions/project-roles/","tags":"","title":"Project Roles"},{"body":"Projects The Gardener API server supports a cluster-scoped Project resource which is used for data isolation between individual Gardener consumers. For example, each development team has its own project to manage its own shoot clusters.\nEach Project is backed by a Kubernetes Namespace that contains the actual related Kubernetes resources like Secrets or Shoots.\nExample resource:\napiVersion: core.gardener.cloud/v1beta1 kind: Project metadata:  name: dev spec:  namespace: garden-dev  description: \"This is my first project\"  purpose: \"Experimenting with Gardener\"  owner:  apiGroup: rbac.authorization.k8s.io  kind: User  name: john.doe@example.com  members:  - apiGroup: rbac.authorization.k8s.io  kind: User  name: alice.doe@example.com  role: admin  # roles:  # - viewer   # - uam  # - serviceaccountmanager  # - extension:foo  - apiGroup: rbac.authorization.k8s.io  kind: User  name: bob.doe@example.com  role: viewer # tolerations: # defaults: # - key: \u003csome-key\u003e # whitelist: # - key: \u003csome-key\u003e The .spec.namespace field is optional and is initialized if unset. The name of the resulting namespace will be determined based on the Project name and UID, e.g. garden-dev-5aef3. It’s also possible to adopt existing namespaces by labeling them gardener.cloud/role=project and project.gardener.cloud/name=dev beforehand (otherwise, they cannot be adopted).\nWhen deleting a Project resource, the corresponding namespace is also deleted. To keep a namespace after project deletion, an administrator/operator (not Project members!) can annotate the project-namespace with namespace.gardener.cloud/keep-after-project-deletion.\nThe spec.description and .spec.purpose fields can be used to describe to fellow team members and Gardener operators what this project is used for.\nEach project has one dedicated owner, configured in .spec.owner using the rbac.authorization.k8s.io/v1.Subject type. The owner is the main contact person for Gardener operators. Please note that the .spec.owner field is deprecated and will be removed in future API versions in favor of the owner role, see below.\nThe list of members (again a list in .spec.members[] using the rbac.authorization.k8s.io/v1.Subject type) contains all the people that are associated with the project in any way. Each project member must have at least one role (currently described in .spec.members[].role, additional roles can be added to .spec.members[].roles[]). The following roles exist:\n admin: This allows to fully manage resources inside the project (e.g., secrets, shoots, configmaps, and similar). Mind that the admin role has readonly access to service accounts. serviceaccountmanager: This allows to fully manage service accounts inside the project namespace and request tokens for them. The permissions of the created service accounts are instead managed by the admin role. Please refer to this document. uam: This allows to add/modify/remove human users or groups to/from the project member list. viewer: This allows to read all resources inside the project except secrets. owner: This combines the admin, uam and serviceaccountmanager roles. Extension roles (prefixed with extension:): Please refer to this document.  The project controller inside the Gardener Controller Manager is managing RBAC resources that grant the described privileges to the respective members.\nThere are three central ClusterRoles gardener.cloud:system:project-member, gardener.cloud:system:project-viewer and gardener.cloud:system:project-serviceaccountmanager that grant the permissions for namespaced resources (e.g., Secrets, Shoots, ServiceAccounts, etc.). Via referring RoleBindings created in the respective namespace the project members get bound to these ClusterRoles and, thus, the needed permissions. There are also project-specific ClusterRoles granting the permissions for cluster-scoped resources, e.g. the Namespace or Project itself.\nFor each role, the following ClusterRoles, ClusterRoleBindings, and RoleBindings are created:\n   Role ClusterRole ClusterRoleBinding RoleBinding     admin gardener.cloud:system:project-member:\u003cprojectName\u003e gardener.cloud:system:project-member:\u003cprojectName\u003e gardener.cloud:system:project-member   serviceaccountmanager   gardener.cloud:system:project-serviceaccountmanager   uam gardener.cloud:system:project-uam:\u003cprojectName\u003e gardener.cloud:system:project-uam:\u003cprojectName\u003e    viewer gardener.cloud:system:project-viewer:\u003cprojectName\u003e gardener.cloud:system:project-viewer:\u003cprojectName\u003e gardener.cloud:system:project-viewer   owner gardener.cloud:system:project:\u003cprojectName\u003e gardener.cloud:system:project:\u003cprojectName\u003e    extension:* gardener.cloud:extension:project:\u003cprojectName\u003e:\u003cextensionRoleName\u003e  gardener.cloud:extension:project:\u003cprojectName\u003e:\u003cextensionRoleName\u003e    User Access Management For Projects created before Gardener v1.8 all admins were allowed to manage other members. Beginning with v1.8 the new uam role is being introduced. It is backed by the manage-members custom RBAC verb which allows to add/modify/remove human users or groups to/from the project member list. Human users are subjects with kind=User and name!=system:serviceaccount:*, and groups are subjects with kind=Group. The management of service account subjects (kind=ServiceAccount or name=system:serviceaccount:*) is not controlled via the uam custom verb but with the standard update/patch verbs for projects.\nAll newly created projects will only bind the owner to the uam role. The owner can still grant the uam role to other members if desired. For projects created before Gardener v1.8 the Gardener Controller Manager will migrate all projects to also assign the uam role to all admin members (to not break existing use-cases). The corresponding migration logic is present in Gardener Controller Manager from v1.8 to v1.13. The project owner can gradually remove these roles if desired.\nStale Projects When a project is not actively used for some period of time the project is marked as “stale”. This is done by controller called “Stale Projects Reconciler”. Once the project is marked as stale there is a time frame in which if not used it will be deleted by that controller. More detailed information can be found here.\n","categories":"","description":"","excerpt":"Projects The Gardener API server supports a cluster-scoped Project …","ref":"/docs/gardener/usage/projects/","tags":"","title":"Projects"},{"body":"Gardener Extension for Alicloud provider  \nProject Gardener implements the automated management and operation of Kubernetes clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.\nRecently, most of the vendor specific logic has been developed in-tree. However, the project has grown to a size where it is very hard to extend, maintain, and test. With GEP-1 we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.\nThis controller implements Gardener’s extension contract for the Alicloud provider.\nAn example for a ControllerRegistration resource that can be used to register this controller to Gardener can be found here.\nPlease find more information regarding the extensibility concepts and a detailed proposal here.\nSupported Kubernetes versions This extension controller supports the following Kubernetes versions:\n   Version Support Conformance test results     Kubernetes 1.24 1.24.0+    Kubernetes 1.23 1.23.0+    Kubernetes 1.22 1.22.0+    Kubernetes 1.21 1.21.0+    Kubernetes 1.20 1.20.0+    Kubernetes 1.19 1.19.0+    Kubernetes 1.18 1.18.0+    Kubernetes 1.17 1.17.0+     Please take a look here to see which versions are supported by Gardener in general.\n How to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start.\nStatic code checks and tests can be executed by running make verify. We are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io GEP-1 (Gardener Enhancement Proposal) on extensibility GEP-4 (New core.gardener.cloud/v1alpha1 API) Extensibility API documentation Gardener Extensions Golang library Gardener API Reference  ","categories":"","description":"Gardener extension controller for the Alibaba cloud provider","excerpt":"Gardener extension controller for the Alibaba cloud provider","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-alicloud/","tags":"","title":"Provider Alicloud"},{"body":"Gardener Extension for AWS provider  \nProject Gardener implements the automated management and operation of Kubernetes clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.\nRecently, most of the vendor specific logic has been developed in-tree. However, the project has grown to a size where it is very hard to extend, maintain, and test. With GEP-1 we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.\nThis controller implements Gardener’s extension contract for the AWS provider.\nAn example for a ControllerRegistration resource that can be used to register this controller to Gardener can be found here.\nPlease find more information regarding the extensibility concepts and a detailed proposal here.\nSupported Kubernetes versions This extension controller supports the following Kubernetes versions:\n   Version Support Conformance test results     Kubernetes 1.25 1.25.0+ N/A   Kubernetes 1.24 1.24.0+    Kubernetes 1.23 1.23.0+    Kubernetes 1.22 1.22.0+    Kubernetes 1.21 1.21.0+    Kubernetes 1.20 1.20.0+    Kubernetes 1.19 1.19.0+    Kubernetes 1.18 1.18.0+    Kubernetes 1.17 1.17.0+     Please take a look here to see which versions are supported by Gardener in general.\nCompatibility The following lists known compatibility issues of this extension controller with other Gardener components.\n   AWS Extension Gardener Action Notes     \u003c= v1.15.0 \u003ev1.10.0 Please update the provider version to \u003e v1.15.0 or disable the feature gate MountHostCADirectories in the Gardenlet. Applies if feature flag MountHostCADirectories in the Gardenlet is enabled. Shoots with CSI enabled (Kubernetes version \u003e= 1.18) miss a mount to the directory /etc/ssl in the Shoot API Server. This can lead to not trusting external Root CAs when the API Server makes requests via webhooks or OIDC.     How to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start.\nStatic code checks and tests can be executed by running make verify. We are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io GEP-1 (Gardener Enhancement Proposal) on extensibility GEP-4 (New core.gardener.cloud/v1alpha1 API) Extensibility API documentation Gardener Extensions Golang library Gardener API Reference  ","categories":"","description":"Gardener extension controller for the AWS cloud provider","excerpt":"Gardener extension controller for the AWS cloud provider","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/","tags":"","title":"Provider AWS"},{"body":"Gardener Extension for Azure provider  \nProject Gardener implements the automated management and operation of Kubernetes clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.\nRecently, most of the vendor specific logic has been developed in-tree. However, the project has grown to a size where it is very hard to extend, maintain, and test. With GEP-1 we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.\nThis controller implements Gardener’s extension contract for the Azure provider.\nAn example for a ControllerRegistration resource that can be used to register this controller to Gardener can be found here.\nPlease find more information regarding the extensibility concepts and a detailed proposal here.\nSupported Kubernetes versions This extension controller supports the following Kubernetes versions:\n   Version Support Conformance test results     Kubernetes 1.25 1.25.0+ N/A   Kubernetes 1.24 1.24.0+    Kubernetes 1.23 1.23.0+    Kubernetes 1.22 1.22.0+    Kubernetes 1.21 1.21.0+    Kubernetes 1.20 1.20.0+    Kubernetes 1.19 1.19.0+    Kubernetes 1.18 1.18.0+    Kubernetes 1.17 1.17.0+     Please take a look here to see which versions are supported by Gardener in general.\n How to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start.\nStatic code checks and tests can be executed by running make verify. We are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io GEP-1 (Gardener Enhancement Proposal) on extensibility GEP-4 (New core.gardener.cloud/v1alpha1 API) Extensibility API documentation Gardener Extensions Golang library Gardener API Reference  ","categories":"","description":"Gardener extension controller for the Azure cloud provider","excerpt":"Gardener extension controller for the Azure cloud provider","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-azure/","tags":"","title":"Provider Azure"},{"body":"Gardener Extension for Equinix Metal provider  \nProject Gardener implements the automated management and operation of Kubernetes clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.\nRecently, most of the vendor specific logic has been developed in-tree. However, the project has grown to a size where it is very hard to extend, maintain, and test. With GEP-1 we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.\nThis controller implements Gardener’s extension contract for the Equinix Metal provider.\nAn example for a ControllerRegistration resource that can be used to register this controller to Gardener can be found here.\nPlease find more information regarding the extensibility concepts and a detailed proposal here.\nSupported Kubernetes versions This extension controller supports the following Kubernetes versions:\n   Version Support Conformance test results     Kubernetes 1.24 untested N/A   Kubernetes 1.23 untested N/A   Kubernetes 1.22 untested N/A   Kubernetes 1.21 untested N/A   Kubernetes 1.20 untested N/A   Kubernetes 1.19 untested N/A   Kubernetes 1.18 untested N/A   Kubernetes 1.17 untested N/A    Please take a look here to see which versions are supported by Gardener in general.\n How to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start.\nStatic code checks and tests can be executed by running make verify. We are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io GEP-1 (Gardener Enhancement Proposal) on extensibility GEP-4 (New core.gardener.cloud/v1alpha1 API) Extensibility API documentation Gardener Extensions Golang library Gardener API Reference  ","categories":"","description":"Gardener extension controller for the Equinix Metal cloud provider","excerpt":"Gardener extension controller for the Equinix Metal cloud provider","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-equinix-metal/","tags":"","title":"Provider Equinix Metal"},{"body":"Gardener Extension for GCP provider  \nProject Gardener implements the automated management and operation of Kubernetes clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.\nRecently, most of the vendor specific logic has been developed in-tree. However, the project has grown to a size where it is very hard to extend, maintain, and test. With GEP-1 we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.\nThis controller implements Gardener’s extension contract for the GCP provider.\nAn example for a ControllerRegistration resource that can be used to register this controller to Gardener can be found here.\nPlease find more information regarding the extensibility concepts and a detailed proposal here.\nSupported Kubernetes versions This extension controller supports the following Kubernetes versions:\n   Version Support Conformance test results     Kubernetes 1.25 1.25.0+ N/A   Kubernetes 1.24 1.24.0+    Kubernetes 1.23 1.23.0+    Kubernetes 1.22 1.22.0+    Kubernetes 1.21 1.21.0+    Kubernetes 1.20 1.20.0+    Kubernetes 1.19 1.19.0+    Kubernetes 1.18 1.18.0+    Kubernetes 1.17 1.17.0+     Please take a look here to see which versions are supported by Gardener in general.\n How to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start.\nStatic code checks and tests can be executed by running make verify. We are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io GEP-1 (Gardener Enhancement Proposal) on extensibility GEP-4 (New core.gardener.cloud/v1alpha1 API) Extensibility API documentation Gardener Extensions Golang library Gardener API Reference  ","categories":"","description":"Gardener extension controller for the GCP cloud provider","excerpt":"Gardener extension controller for the GCP cloud provider","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-gcp/","tags":"","title":"Provider GCP"},{"body":"Packages:\n  local.provider.extensions.gardener.cloud/v1alpha1   local.provider.extensions.gardener.cloud/v1alpha1  Package v1alpha1 contains the local provider API resources.\nResource Types:  CloudProfileConfig  WorkerStatus  CloudProfileConfig   CloudProfileConfig contains provider-specific configuration that is embedded into Gardener’s CloudProfile resource.\n   Field Description      apiVersion string   local.provider.extensions.gardener.cloud/v1alpha1      kind string  CloudProfileConfig    machineImages  []MachineImages     MachineImages is the list of machine images that are understood by the controller. It maps logical names and versions to provider-specific identifiers.\n    WorkerStatus   WorkerStatus contains information about created worker resources.\n   Field Description      apiVersion string   local.provider.extensions.gardener.cloud/v1alpha1      kind string  WorkerStatus    machineImages  []MachineImage     (Optional) MachineImages is a list of machine images that have been used in this worker. Usually, the extension controller gets the mapping from name/version to the provider-specific machine image data from the CloudProfile. However, if a version that is still in use gets removed from this componentconfig it cannot reconcile anymore existing Worker resources that are still using this version. Hence, it stores the used versions in the provider status to ensure reconciliation is possible.\n    MachineImage   (Appears on: WorkerStatus)  MachineImage is a mapping from logical names and versions to provider-specific machine image data.\n   Field Description      name  string    Name is the logical name of the machine image.\n    version  string    Version is the logical version of the machine image.\n    image  string    Image is the image for the machine image.\n    MachineImageVersion   (Appears on: MachineImages)  MachineImageVersion contains a version and a provider-specific identifier.\n   Field Description      version  string    Version is the version of the image.\n    image  string    Image is the image for the machine image.\n    MachineImages   (Appears on: CloudProfileConfig)  MachineImages is a mapping from logical names and versions to provider-specific identifiers.\n   Field Description      name  string    Name is the logical name of the machine image.\n    versions  []MachineImageVersion     Versions contains versions and a provider-specific identifier.\n      Generated with gen-crd-api-reference-docs \n","categories":"","description":"","excerpt":"Packages:\n  local.provider.extensions.gardener.cloud/v1alpha1 …","ref":"/docs/gardener/api-reference/provider-local/","tags":"","title":"Provider Local"},{"body":"Local Provider Extension The “local provider” extension is used to allow the usage of seed and shoot clusters which run entirely locally without any real infrastructure or cloud provider involved. It implements Gardener’s extension contract (GEP-1) and thus comprises several controllers and webhooks acting on resources in seed and shoot clusters.\nThe code is maintained in pkg/provider-local.\nMotivation The motivation for maintaining such extension is the following:\n 🛡 Output Qualification: Run fast and cost-efficient end-to-end tests, locally and in CI systems (increased confidence ⛑ before merging pull requests) ⚙️ Development Experience: Develop Gardener entirely on a local machine without any external resources involved (improved costs 💰 and productivity 🚀) 🤝 Open Source: Quick and easy setup for a first evaluation of Gardener and a good basis for first contributions  Current Limitations The following enlists the current limitations of the implementation. Please note that all of them are no technical limitations/blockers but simply advanced scenarios that we haven’t had invested yet into.\n  Shoot clusters can only have multiple nodes, but inter-pod communication for pods on different nodes does not work.\nWe are using the networking-calico extension for the CNI plugin in shoot clusters, however, it doesn’t seem to be configured correctly yet to support this scenario.\n  No owner TXT DNSRecords (hence, no “bad-case” control plane migration).\nIn order to realize DNS (see Implementation Details section below), the /etc/hosts file is manipulated. This does not work for TXT records. In the future, we could look into using CoreDNS instead.\n  No load balancers for Shoot clusters.\nWe have not yet developed a cloud-controller-manager which could reconcile load balancer Services in the shoot cluster. Hence, when the gardenlet’s ReversedVPN feature gate is disabled then the kube-system/vpn-shoot Service must be manually patched (with {\"status\": {\"loadBalancer\": {\"ingress\": [{\"hostname\": \"vpn-shoot\"}]}}}) to make the reconciliation work.\n  Only one shoot cluster possible when gardenlet’s APIServerSNI feature gate is disabled.\nWhen APIServerSNI is disabled then gardenlet uses load balancer Services in order to expose the shoot clusters’ kube-apiservers. Typically, local Kubernetes clusters don’t support this. In this case, the local extension uses the host IP to expose the kube-apiserver, however, this can only be done once.\nHowever, given that the APIServerSNI feature gate is deprecated and will be removed in the future (see gardener/gardener#6007), we will probably not invest into this.\n  ManagedSeeds It is possible to deploy ManagedSeeds with provider-local by first creating a Shoot in the garden namespace and then creating a referencing ManagedSeed object.\n Please note that this is only supported by the Skaffold-based setup.\n The corresponding e2e test can be run via\n./hack/test-e2e-local.sh --label-filter \"ManagedSeed\" Implementation Details The images locally built by Skaffold for the Gardener components which are deployed to this shoot cluster are managed by a container registry in the registry namespace in the kind cluster. provider-local configures this registry as mirror for the shoot by mutating the OperatingSystemConfig and using the default contract for extending the containerd configuration.\nIn order to bootstrap a seed cluster, the gardenlet deploys PersistentVolumeClaims and Services of type LoadBalancer. While storage is supported in shoot clusters by using the local-path-provisioner, load balancers are not supported yet. However, provider-local runs a Service controller which specifically reconciles the seed-related Services of type LoadBalancer. This way, they get an IP and gardenlet can finish its bootstrapping process. Note that these IPs are not reachable, however for the sake of developing ManagedSeeds this is sufficient for now.\nAlso, please note that the provider-local extension only gets deployed because of the Always deployment policy in its corresponding ControllerRegistration and because the DNS provider type of the seed is set to local.\nImplementation Details This section contains information about how the respective controllers and webhooks in provider-local are implemented and what their purpose is.\nBootstrapping The Helm chart of the provider-local extension defined in its ControllerDeployment contains a special deployment for a CoreDNS instance in a gardener-extension-provider-local-coredns namespace in the seed cluster.\nThis CoreDNS instance is responsible for enabling the components running in the shoot clusters to be able to resolve the DNS names when they communicate with their kube-apiservers.\nIt contains static configuration to resolve the DNS names based on local.gardener.cloud to either the istio-ingressgateway.istio-ingress.svc or the kube-apiserver.\u003cshoot-namespace\u003e.svc addresses (depending on whether the --apiserver-sni-enabled flag is set to true or false).\nControllers There are controllers for all resources in the extensions.gardener.cloud/v1alpha1 API group except for BackupBucket and BackupEntrys.\nControlPlane This controller is deploying the local-path-provisioner as well as a related StorageClass in order to support PersistentVolumeClaims in the local shoot cluster. Additionally, it creates a few (currently unused) dummy secrets (CA, server and client certificate, basic auth credentials) for the sake of testing the secrets manager integration in the extensions library.\nDNSRecord This controller manipulates the /etc/hosts file and adds a new line for each DNSRecord it observes. This enables accessing the shoot clusters from the respective machine, however, it also requires to run the extension with elevated privileges (sudo).\nThe /etc/hosts would be extended as follows:\n# Begin of gardener-extension-provider-local section 10.84.23.24 api.local.local.external.local.gardener.cloud 10.84.23.24 api.local.local.internal.local.gardener.cloud ... # End of gardener-extension-provider-local section Infrastructure This controller generates a NetworkPolicy which allows the control plane pods (like kube-apiserver) to communicate with the worker machine pods (see Worker section)).\nIn addition, it creates a Service named vpn-shoot which is only used in case the gardenlet’s ReversedVPN feature gate is disabled. This Service enables the vpn-seed containers in the kube-apiserver pods in the seed cluster to communicate with the vpn-shoot pod running in the shoot cluster.\nNetwork This controller is not implemented anymore. In the initial version of provider-local, there was a Network controller deploying kindnetd (see https://github.com/gardener/gardener/tree/v1.44.1/pkg/provider-local/controller/network). However, we decided to drop it because this setup prevented us from using NetworkPolicys (kindnetd does not ship a NetworkPolicy controller). In addition, we had issues with shoot clusters having more than one node (hence, we couldn’t support rolling updates, see https://github.com/gardener/gardener/pull/5666/commits/491b3cd16e40e5c20ef02367fda93a34ff9465eb).\nOperatingSystemConfig This controller leverages the standard oscommon library in order to render a simple cloud-init template which can later be executed by the shoot worker nodes.\nThe shoot worker nodes are Pods with a container based on the kindest/node image. This is maintained in https://github.com/gardener/machine-controller-manager-provider-local/tree/master/node and has a special run-userdata systemd service which executes the cloud-init generated earlier by the OperatingSystemConfig controller.\nWorker This controller leverages the standard generic Worker actuator in order to deploy the machine-controller-manager as well as the machine-controller-manager-provider-local.\nAdditionally, it generates the MachineClasses and the MachineDeployments based on the specification of the Worker resources.\nIngress Gardenlet creates a wildcard DNS record for the Seed’s ingress domain pointing to the nginx-ingress-controller’s LoadBalancer. This domain is commonly used by all Ingress objects created in the Seed for Seed and Shoot components. However, provider-local implements the DNSRecord extension API by writing the DNS record to /etc/hosts, which doesn’t support wildcard entries. To make Ingress domains resolvable on the host, this controller reconciles all Ingresses and creates DNSRecords of type local for each host included in spec.rules.\nService This controller reconciles Services of type LoadBalancer in the local Seed cluster. Since the local Kubernetes clusters used as Seed clusters typically don’t support such services, this controller sets the .status.ingress.loadBalancer.ip[0] to the IP of the host. It makes important LoadBalancer Services (e.g. istio-ingress/istio-ingressgateway and garden/nginx-ingress-controller) available to the host by setting spec.ports[].nodePort to well-known ports that are mapped to hostPorts in the kind cluster configuration.\nIf the --apiserver-sni-enabled flag is set to true (default), istio-ingress/istio-ingressgateway is set to be exposed on nodePort 30433 by this controller. Otherwise, the kube-apiserver Service in the shoot namespaces in the seed cluster needs to be patched to be exposed on 30443 by the Control Plane Exposure Webhook.\nETCD Backups This controller reconciles the BackuBucket and BackupEntry of the shoot allowing the etcd-backup-restore to create and copy backups using the local provider functionality. The backups are stored on the host file system. This is achieved by mounting that directory to the etcd-backup-restore container.\nHealth Checks The health check controller leverages the health check library in order to\n check the health of the ManagedResource/extension-controlplane-shoot-webhooks and populate the SystemComponentsHealthy condition in the ControlPlane resource. check the health of the ManagedResource/extension-networking-local and populate the SystemComponentsHealthy condition in the Network resource. check the health of the ManagedResource/extension-worker-mcm-shoot and populate the SystemComponentsHealthy condition in the Worker resource. check the health of the Deployment/machine-controller-manager and populate the ControlPlaneHealthy condition in the Worker resource. check the health of the Nodes and populate the EveryNodeReady condition in the Worker resource.  Webhooks Control Plane This webhook reacts on the OperatingSystemConfig containing the configuration of the kubelet and sets the failSwapOn to false (independent of what is configured in the Shoot spec) (ref).\nControl Plane Exposure This webhook reacts on the kube-apiserver Service in shoot namespaces in the seed in case the gardenlet’s APIServerSNI feature gate is disabled. It sets the nodePort to 30443 to enable communication from the host (this requires a port mapping to work when creating the local cluster).\nDNS Config This webhook reacts on events for the dependency-watchdog-probe Deployment, the prometheus StatefulSet as well as on events for Pods created when the machine-controller-manager reconciles Machines. All these pods need to be able to resolve the DNS names for shoot clusters. It sets the .spec.dnsPolicy=None and .spec.dnsConfig.nameServers to the cluster IP of the coredns Service created in the gardener-extension-provider-local-coredns namespaces so that these pods can resolve the DNS records for shoot clusters (see the Bootstrapping section for more details).\nNode This webhook reacts on updates to nodes/status in both seed and shoot clusters and sets the .status.{allocatable,capacity}.cpu=\"100\" and .status.{allocatable,capacity}.memory=\"100Gi\" fields.\nBackground: Typically, the .status.{capacity,allocatable} values are determined by the resources configured for the Docker daemon (see for example this for Mac). Since many of the Pods deployed by Gardener have quite high .spec.resources.requests, the Nodes easily get filled up and only a few Pods can be scheduled (even if they barely consume any of their reserved resources). In order to improve the user experience, on startup/leader election the provider-local extension submits an empty patch which triggers the “node webhook” (see below section) for the seed cluster. The webhook will increase the capacity of the Nodes to allow all Pods to be scheduled. For the shoot clusters, this empty patch trigger is not needed since the MutatingWebhookConfiguration is reconciled by the ControlPlane controller and exists before the Node object gets registered.\nShoot This webhook reacts on the ConfigMap used by the kube-proxy and sets the maxPerCore field to 0 since other values don’t work well in conjunction with the kindest/node image which is used as base for the shoot worker machine pods (ref).\nFuture Work Future work could mostly focus on resolving above listed limitations, i.e.,\n Implement a cloud-controller-manager and deploy it via the ControlPlane controller. Properly implement .spec.machineTypes in the CloudProfiles (i.e., configure .spec.resources properly for the created shoot worker machine pods).  ","categories":"","description":"","excerpt":"Local Provider Extension The “local provider” extension is used to …","ref":"/docs/gardener/extensions/provider-local/","tags":"","title":"Provider Local"},{"body":"Gardener Extension for OpenStack provider  \nProject Gardener implements the automated management and operation of Kubernetes clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.\nRecently, most of the vendor specific logic has been developed in-tree. However, the project has grown to a size where it is very hard to extend, maintain, and test. With GEP-1 we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.\nThis controller implements Gardener’s extension contract for the OpenStack provider.\nAn example for a ControllerRegistration resource that can be used to register this controller to Gardener can be found here.\nPlease find more information regarding the extensibility concepts and a detailed proposal here.\nSupported Kubernetes versions This extension controller supports the following Kubernetes versions:\n   Version Support Conformance test results     Kubernetes 1.25 1.25.0+ N/A   Kubernetes 1.24 1.24.0+    Kubernetes 1.23 1.23.0+    Kubernetes 1.22 1.22.0+    Kubernetes 1.21 1.21.0+    Kubernetes 1.20 1.20.0+    Kubernetes 1.19 1.19.0+    Kubernetes 1.18 1.18.0+    Kubernetes 1.17 1.17.0+     Please take a look here to see which versions are supported by Gardener in general.\n Compatibility The following lists known compatibility issues of this extension controller with other Gardener components.\n   OpenStack Extension Gardener Action Notes     \u003c v1.12.0 \u003e v1.10.0 Please update the provider version to \u003e= v1.12.0 or disable the feature gate MountHostCADirectories in the Gardenlet. Applies if feature flag MountHostCADirectories in the Gardenlet is enabled. This is to prevent duplicate volume mounts to /usr/share/ca-certificates in the Shoot API Server.    How to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start.\nStatic code checks and tests can be executed by running make verify. We are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io GEP-1 (Gardener Enhancement Proposal) on extensibility GEP-4 (New core.gardener.cloud/v1alpha1 API) Extensibility API documentation Gardener Extensions Golang library Gardener API Reference  ","categories":"","description":"Gardener extension controller for the OpenStack cloud provider","excerpt":"Gardener extension controller for the OpenStack cloud provider","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-openstack/","tags":"","title":"Provider Openstack"},{"body":"Gardener Extension for vSphere provider  \nOverview The Gardener Extension for vSphere is a GEP-1 provider implementation that allows Gardener to leverage vSphere clusters for machine provisioning.\nvSphere is an undeniable class leader for commercially supported virtual machine orchestration. The Gardener extension for vSphere provider compliments this leadership by allowing Gardener to create Kubernetes nodes within vSphere.\nLike other Gardener provider extensions, the vSphere provider pairs with a provider-specific Machine Controller Manager providing node services to Kubernetes clusters. This extension provides complimentary APIs to Gardener. A Gardener-hosted Kubernetes cluster does not know anything about it’s environment (such as bare metal vs. public cloud or within a hyperscaler vs. standalone), only that the MCM abstraction can manage requests such as cluster autoscaling.\nAn example for a ControllerRegistration resource that can be used to register this controller to Gardener can be found here.\nPlease find more information regarding the extensibility concepts and the architecture details in the GEP-1 proposal.\nUse Cases The primary use case for this extension is organizations who wish to deploy a substantial Gardener landscape and use vSphere for data center fleet management. We intentionally sidestep prescribing any particular extension as this is an intimately local determination and the benefits of different solutions are more than adequately debated in industry literature.\nWhile we may inadvertently duplicate some documentation in the mainline Gardener documentation, it is only to reduce tedium as new evaluators and developers come up-to-speed with the concepts relevant to successful deployment. We refer directly to the mainline Gardener documentation for the most up-to-date information.\nSupported Kubernetes versions This extension controller supports the following Kubernetes versions:\n   Version Support Conformance test results     Kubernetes 1.25 untested not yet available   Kubernetes 1.24 untested not yet available   Kubernetes 1.23 untested not yet available   Kubernetes 1.22 untested not yet available   Kubernetes 1.21 untested not yet available   Kubernetes 1.20 untested not yet available    Older versions of the extension (v0.16.0 and earlier) are supported prior to current releases.\nPlease take a look here to see which versions are supported by Gardener in general.\n Deployment patterns As with any production software, deployment of Gardener and this extension should be considered in the context of both lifecycle and automation. Orgs should aspire to have apply\nHow to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start.\nStatic code checks and tests can be executed by running make verify. We are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io GEP-1 (Gardener Enhancement Proposal) on extensibility GEP-4 (New core.gardener.cloud/v1alpha1 API) Extensibility API documentation Gardener Extensions Golang library Gardener API Reference  ","categories":"","description":"Gardener extension controller for the vSphere cloud provider","excerpt":"Gardener extension controller for the vSphere cloud provider","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/","tags":"","title":"Provider vSphere"},{"body":"Reconcile trigger Gardener dictates the time of reconciliation for resources of the API group extensions.gardener.cloud. It does that by annotating the respected resource with gardener.cloud/operation=reconcile. Extension controllers shall react to this annotation and start reconciling the resource. They have to remove this annotation as soon as they begin with their reconcile operation and maintain the status of the extension resource accordingly.\nThe reason for this behaviour is that it is possible to configure Gardener to reconcile only in the shoots’ maintenance time windows. In order to avoid that extension controllers reconcile outside of the shoot’s maintenance time window we have introduced this contract. This way extension controllers don’t need to care about when the shoot maintenance time window happens. Gardener keeps control and decides when the shoot shall be reconciled/updated.\nOur extension controller library provides all the required utilities to conveniently implement this behaviour.\n","categories":"","description":"","excerpt":"Reconcile trigger Gardener dictates the time of reconciliation for …","ref":"/docs/gardener/extensions/reconcile-trigger/","tags":"","title":"Reconcile Trigger"},{"body":"What is impacted during a reconciliation? Infrastructure and DNSRecord reconciliation are only done during usual reconciliation if there were relevant changes. Otherwise, they are only done during maintenance.\nHow do you steer a reconciliation? Reconciliation is bound to the maintenance time window of a cluster. This means that your shoot will be reconciled regularly, without need for input.\nOutside of the maintenance time window your shoot will only reconcile if you change the specification or if you explicitly trigger it. To learn how, see Trigger shoot operations.\n","categories":"","description":"","excerpt":"What is impacted during a reconciliation? Infrastructure and DNSRecord …","ref":"/docs/faq/reconciliation-impact/","tags":"","title":"Reconciliation"},{"body":"Topic Title Content This section gives the user all the information needed in order to understand the topic.\n   Content Type Definition Example     Name 1 Definition of Name 1 Relevant link   Name 2 Definition of Name 2 Relevant link    Related Links  Link 1 Link 2  ","categories":"","description":"Describes the contents of a reference topic","excerpt":"Describes the contents of a reference topic","ref":"/docs/contribute/20_documentation/40_style_guide/reference_template/","tags":"","title":"Reference Topic Structure"},{"body":"Referenced Resources The Shoot resource can include a list of resources (usually secrets) that can be referenced by name in extension providerConfig and other Shoot sections, for example:\nkind: Shoot apiVersion: core.gardener.cloud/v1beta1 metadata:  name: crazy-botany  namespace: garden-dev  ... spec:  ...  extensions:  - type: foobar  providerConfig:  apiVersion: foobar.extensions.gardener.cloud/v1alpha1  kind: FooBarConfig  foo: bar  secretRef: foobar-secret  resources:  - name: foobar-secret  resourceRef:  apiVersion: v1  kind: Secret  name: my-foobar-secret Gardener expects to find these referenced resources in the project namespace (e.g. garden-dev) and will copy them to the Shoot namespace in the Seed cluster when reconciling a Shoot, adding a prefix to their names to avoid naming collisions with Gardener’s own resources.\nExtension controllers can resolve the references to these resources by accessing the Shoot via the Cluster resource. To properly read a referenced resources, extension controllers should use the utility function GetObjectByReference from the extensions/pkg/controller package, for example:\n ...  ref = \u0026autoscalingv1.CrossVersionObjectReference{  APIVersion: \"v1\",  Kind: \"Secret\",  Name: \"foo\",  }  secret := \u0026corev1.Secret{}  if err := controller.GetObjectByReference(ctx, client, ref, \"shoot--test--foo\", secret); err != nil {  return err  }  // Use secret  ... ","categories":"","description":"","excerpt":"Referenced Resources The Shoot resource can include a list of …","ref":"/docs/gardener/extensions/referenced-resources/","tags":"","title":"Referenced Resources"},{"body":"Gardener Resource Manager Initially, the gardener-resource-manager was a project similar to the kube-addon-manager. It manages Kubernetes resources in a target cluster which means that it creates, updates, and deletes them. Also, it makes sure that manual modifications to these resources are reconciled back to the desired state.\nIn the Gardener project we were using the kube-addon-manager since more than two years. While we have progressed with our extensibility story (moving cloud providers out-of-tree) we had decided that the kube-addon-manager is no longer suitable for this use-case. The problem with it is that it needs to have its managed resources on its file system. This requires storing the resources in ConfigMaps or Secrets and mounting them to the kube-addon-manager pod during deployment time. The gardener-resource-manager uses CustomResourceDefinitions which allows to dynamically add, change, and remove resources with immediate action and without the need to reconfigure the volume mounts/restarting the pod.\nMeanwhile, the gardener-resource-manager has evolved to a more generic component comprising several controllers and webhook handlers. It is deployed by gardenlet once per seed (in the garden namespace) and once per shoot (in the respective shoot namespaces in the seed).\nControllers ManagedResource controller This controller watches custom objects called ManagedResources in the resources.gardener.cloud/v1alpha1 API group. These objects contain references to secrets which itself contain the resources to be managed. The reason why a Secret is used to store the resources is that they could contain confidential information like credentials.\n--- apiVersion: v1 kind: Secret metadata:  name: managedresource-example1  namespace: default type: Opaque data:  objects.yaml: YXBpVmVyc2lvbjogdjEKa2luZDogQ29uZmlnTWFwCm1ldGFkYXRhOgogIG5hbWU6IHRlc3QtMTIzNAogIG5hbWVzcGFjZTogZGVmYXVsdAotLS0KYXBpVmVyc2lvbjogdjEKa2luZDogQ29uZmlnTWFwCm1ldGFkYXRhOgogIG5hbWU6IHRlc3QtNTY3OAogIG5hbWVzcGFjZTogZGVmYXVsdAo=  # apiVersion: v1  # kind: ConfigMap  # metadata:  # name: test-1234  # namespace: default  # ---  # apiVersion: v1  # kind: ConfigMap  # metadata:  # name: test-5678  # namespace: default --- apiVersion: resources.gardener.cloud/v1alpha1 kind: ManagedResource metadata:  name: example  namespace: default spec:  secretRefs:  - name: managedresource-example1 In the above example, the controller creates two ConfigMaps in the default namespace. When a user is manually modifying them they will be reconciled back to the desired state stored in the managedresource-example secret.\nIt is also possible to inject labels into all the resources:\n--- apiVersion: v1 kind: Secret metadata:  name: managedresource-example2  namespace: default type: Opaque data:  other-objects.yaml: YXBpVmVyc2lvbjogYXBwcy92MSAjIGZvciB2ZXJzaW9ucyBiZWZvcmUgMS45LjAgdXNlIGFwcHMvdjFiZXRhMgpraW5kOiBEZXBsb3ltZW50Cm1ldGFkYXRhOgogIG5hbWU6IG5naW54LWRlcGxveW1lbnQKc3BlYzoKICBzZWxlY3RvcjoKICAgIG1hdGNoTGFiZWxzOgogICAgICBhcHA6IG5naW54CiAgcmVwbGljYXM6IDIgIyB0ZWxscyBkZXBsb3ltZW50IHRvIHJ1biAyIHBvZHMgbWF0Y2hpbmcgdGhlIHRlbXBsYXRlCiAgdGVtcGxhdGU6CiAgICBtZXRhZGF0YToKICAgICAgbGFiZWxzOgogICAgICAgIGFwcDogbmdpbngKICAgIHNwZWM6CiAgICAgIGNvbnRhaW5lcnM6CiAgICAgIC0gbmFtZTogbmdpbngKICAgICAgICBpbWFnZTogbmdpbng6MS43LjkKICAgICAgICBwb3J0czoKICAgICAgICAtIGNvbnRhaW5lclBvcnQ6IDgwCg==  # apiVersion: apps/v1  # kind: Deployment  # metadata:  # name: nginx-deployment  # spec:  # selector:  # matchLabels:  # app: nginx  # replicas: 2 # tells deployment to run 2 pods matching the template  # template:  # metadata:  # labels:  # app: nginx  # spec:  # containers:  # - name: nginx  # image: nginx:1.7.9  # ports:  # - containerPort: 80  --- apiVersion: resources.gardener.cloud/v1alpha1 kind: ManagedResource metadata:  name: example  namespace: default spec:  secretRefs:  - name: managedresource-example2  injectLabels:  foo: bar In this example the label foo=bar will be injected into the Deployment as well as into all created ReplicaSets and Pods.\nPreventing Reconciliations If a ManagedResource is annotated with resources.gardener.cloud/ignore=true then it will be skipped entirely by the controller (no reconciliations or deletions of managed resources at all). However, when the ManagedResource itself is deleted (for example when a shoot is deleted) then the annotation is not respected and all resources will be deleted as usual. This feature can be helpful to temporarily patch/change resources managed as part of such ManagedResource. Condition checks will be skipped for such ManagedResources.\nModes The gardener-resource-manager can manage a resource in different modes. The supported modes are:\n Ignore  The corresponding resource is removed from the ManagedResource status (.status.resources). No action is performed on the cluster - the resource is no longer “managed” (updated or deleted). The primary use case is a migration of a resource from one ManagedResource to another one.    The mode for a resource can be specified with the resources.gardener.cloud/mode annotation. The annotation should be specified in the encoded resource manifest in the Secret that is referenced by the ManagedResource.\nSkipping health check If a resource in the ManagedResource is annotated with resources.gardener.cloud/skip-health-check=true then the resource will be skipped during health checks by the health controller. The ManagedResource conditions will not reflect the health condition of this resource anymore. The ResourcesProgressing condition will also be set to False.\nResource Class By default, gardener-resource-manager controller watches for ManagedResources in all namespaces. --namespace flag can be specified to gardener-resource-manager binary to restrict the watch to ManagedResources in a single namespace. A ManagedResource has an optional .spec.class field that allows to indicate that it belongs to given class of resources. --resource-class flag can be specified to gardener-resource-manager binary to restrict the watch to ManagedResources with the given .spec.class. A default class is assumed if no class is specified.\nConditions A ManagedResource has a ManagedResourceStatus, which has an array of Conditions. Conditions currently include:\n   Condition Description     ResourcesApplied True if all resources are applied to the target cluster   ResourcesHealthy True if all resources are present and healthy   ResourcesProgressing False if all resources have been fully rolled out    ResourcesApplied may be False when:\n the resource apiVersion is not known to the target cluster the resource spec is invalid (for example the label value does not match the required regex for it) …  ResourcesHealthy may be False when:\n the resource is not found the resource is a Deployment and the Deployment does not have the minimum availability. …  ResourcesProgressing may be True when:\n a Deployment, StatefulSet or DaemonSet has not been fully rolled out yet, i.e. not all replicas have been updated with the latest changes to spec.template.  Each Kubernetes resources has different notion for being healthy. For example, a Deployment is considered healthy if the controller observed its current revision and if the number of updated replicas is equal to the number of replicas.\nThe following status.conditions section describes a healthy ManagedResource:\nconditions: - lastTransitionTime: \"2022-05-03T10:55:39Z\"  lastUpdateTime: \"2022-05-03T10:55:39Z\"  message: All resources are healthy.  reason: ResourcesHealthy  status: \"True\"  type: ResourcesHealthy - lastTransitionTime: \"2022-05-03T10:55:36Z\"  lastUpdateTime: \"2022-05-03T10:55:36Z\"  message: All resources have been fully rolled out.  reason: ResourcesRolledOut  status: \"False\"  type: ResourcesProgressing - lastTransitionTime: \"2022-05-03T10:55:18Z\"  lastUpdateTime: \"2022-05-03T10:55:18Z\"  message: All resources are applied.  reason: ApplySucceeded  status: \"True\"  type: ResourcesApplied Ignoring Updates In some cases it is not desirable to update or re-apply some of the cluster components (for example, if customization is required or needs to be applied by the end-user). For these resources, the annotation “resources.gardener.cloud/ignore” needs to be set to “true” or a truthy value (Truthy values are “1”, “t”, “T”, “true”, “TRUE”, “True”) in the corresponding managed resource secrets, this can be done from the components that create the managed resource secrets, for example Gardener extensions or Gardener. Once this is done, the resource will be initially created and later ignored during reconciliation.\nPreserving replicas or resources in Workload Resources The objects which are part of the ManagedResource can be annotated with\n resources.gardener.cloud/preserve-replicas=true in case the .spec.replicas field of workload resources like Deployments, StatefulSets, etc. shall be preserved during updates. resources.gardener.cloud/preserve-resources=true in case the .spec.containers[*].resources fields of all containers of workload resources like Deployments, StatefulSets, etc. shall be preserved during updates.   This can be useful if there are non-standard horizontal/vertical auto-scaling mechanisms in place. Standard mechanisms like HorizontalPodAutoscaler or VerticalPodAutoscaler will be auto-recognized by gardener-resource-manager, i.e., in such cases the annotations are not needed.\n Origin All the objects managed by the resource manager get a dedicated annotation resources.gardener.cloud/origin describing the ManagedResource object that describes this object. The default format is \u003cnamespace\u003e/\u003cobjectname\u003e.\nIn multi-cluster scenarios (the ManagedResource objects are maintained in a cluster different from the one the described objects are managed), it might be useful to include the cluster identity, as well.\nThis can be enforced by setting the --cluster-id option. Here, several possibilities are supported:\n given a direct value: use this as id for the source cluster \u003ccluster\u003e: read the cluster identity from a cluster-identity config map in the kube-system namespace (attribute cluster-identity). This is automatically maintained in all clusters managed or involved in a gardener landscape. \u003cdefault\u003e: try to read the cluster identity from the config map. If not found, no identity is used empty string: no cluster identity is used (completely cluster local scenarios)  By default cluster id is not used. If cluster id is specified the format is \u003ccluster id\u003e:\u003cnamespace\u003e/\u003cobjectname\u003e.\nIn addition to the origin annotation, all objects managed by the resource manager get a dedicated label resources.gardener.cloud/managed-by. This label can be used to describe these objects with a selector. By default it is set to “gardener”, but this can be overwritten by setting the --managed-by-label option.\nGarbage Collector For Immutable ConfigMaps/Secrets In Kubernetes, workload resources (e.g., Pods) can mount ConfigMaps or Secrets or reference them via environment variables in containers. Typically, when the content of such ConfigMap/Secret gets changed then the respective workload is usually not dynamically reloading the configuration, i.e., a restart is required. The most commonly used approach is probably having so-called checksum annotations in the pod template which makes Kubernetes to recreate the pod if the checksum changes. However, it has the downside that old, still running versions of the workload might not be able to properly work with the already updated content in the ConfigMap/Secret, potentially causing application outages.\nIn order to protect users from such outages (and to also improve the performance of the cluster), the Kubernetes community provides the “immutable ConfigMaps/Secrets feature”. Enabling immutability requires ConfigMaps/Secrets to have unique names. Having unique names requires the client to delete ConfigMaps/Secret`s no longer in use.\nIn order to provide a similarly lightweight experience for clients (compared to the well-established checksum annotation approach), the Gardener Resource Manager features an optional garbage collector controller (disabled by default). The purpose of this controller is cleaning up such immutable ConfigMaps/Secrets if they are no longer in use.\nHow does the garbage collector work? The following algorithm is implemented in the GC controller:\n List all ConfigMaps and Secrets labeled with resources.gardener.cloud/garbage-collectable-reference=true. List all Deployments, StatefulSets, DaemonSets, Jobs, CronJobs, Pods and for each of them  iterate over the .metadata.annotations and for each of them  If the annotation key follows the reference.resources.gardener.cloud/{configmap,secret}-\u003chash\u003e scheme and the value equals \u003cname\u003e then consider it as “in-use”.     Delete all ConfigMaps and Secrets not considered as “in-use”.  Consequently, clients need to\n  Create immutable ConfigMaps/Secrets with unique names (e.g., a checksum suffix based on the .data).\n  Label such ConfigMaps/Secrets with resources.gardener.cloud/garbage-collectable-reference=true.\n  Annotate their workload resources with reference.resources.gardener.cloud/{configmap,secret}-\u003chash\u003e=\u003cname\u003e for all ConfigMaps/Secrets used by the containers of the respective Pods.\n⚠️ Add such annotations to .metadata.annotations as well as to all templates of other resources (e.g., .spec.template.metadata.annotations in Deployments or .spec.jobTemplate.metadata.annotations and .spec.jobTemplate.spec.template.metadata.annotations for CronJobs. This ensures that the GC controller does not unintentionally consider ConfigMaps/Secrets as “not in use” just because there isn’t a Pod referencing them anymore (e.g., they could still be used by a Deployment scaled down to 0).\n  ℹ️ For the last step, there is a helper function InjectAnnotations in the pkg/controller/garbagecollector/references which you can use for your convenience.\nExample:\n--- apiVersion: v1 kind: ConfigMap metadata:  name: test-1234  namespace: default  labels:  resources.gardener.cloud/garbage-collectable-reference: \"true\" --- apiVersion: v1 kind: ConfigMap metadata:  name: test-5678  namespace: default  labels:  resources.gardener.cloud/garbage-collectable-reference: \"true\" --- apiVersion: v1 kind: Pod metadata:  name: example  namespace: default  annotations:  reference.resources.gardener.cloud/configmap-82a3537f: test-5678 spec:  containers:  - name: nginx  image: nginx:1.14.2  terminationGracePeriodSeconds: 2 The GC controller would delete the ConfigMap/test-1234 because it is considered as not “in-use”.\nℹ️ If the GC controller is activated then the ManagedResource controller will no longer delete ConfigMaps/Secrets having the above label.\nHow to activate the garbage collector? The GC controller can be activated by providing the --garbage-collector-sync-period flag with a value larger than 0 (e.g., 1h) to the Gardener Resource Manager.\nTokenInvalidator The Kubernetes community is slowly transitioning from static ServiceAccount token Secrets to ServiceAccount Token Volume Projection. Typically, when you create a ServiceAccount\napiVersion: v1 kind: ServiceAccount metadata:  name: default then the serviceaccount-token controller (part of kube-controller-manager) auto-generates a Secret with a static token:\napiVersion: v1 kind: Secret metadata:  annotations:  kubernetes.io/service-account.name: default  kubernetes.io/service-account.uid: 86e98645-2e05-11e9-863a-b2d4d086dd5a)  name: default-token-ntxs9 type: kubernetes.io/service-account-token data:  ca.crt: base64(cluster-ca-cert)  namespace: base64(namespace)  token: base64(static-jwt-token) Unfortunately, when using ServiceAccount Token Volume Projection in a Pod, this static token is actually not used at all:\napiVersion: v1 kind: Pod metadata:  name: nginx spec:  serviceAccountName: default  containers:  - image: nginx  name: nginx  volumeMounts:  - mountPath: /var/run/secrets/tokens  name: token  volumes:  - name: token  projected:  sources:  - serviceAccountToken:  path: token  expirationSeconds: 7200 While the Pod is now using an expiring and auto-rotated token, the static token is still generated and valid.\nAs of Kubernetes v1.22, there is neither a way of preventing kube-controller-manager to generate such static tokens, nor a way to proactively remove or invalidate them:\n https://github.com/kubernetes/kubernetes/issues/77599 https://github.com/kubernetes/kubernetes/issues/77600  Disabling the serviceaccount-token controller is an option, however, especially in the Gardener context it may either break end-users or it may not even be possible to control such settings. Also, even if a future Kubernetes version supports native configuration of above behaviour, Gardener still supports older versions which won’t get such features but need a solution as well.\nThis is where the TokenInvalidator comes into play: Since it is not possible to prevent kube-controller-manager from generating static ServiceAccount Secrets, the TokenInvalidator is - as its name suggests - just invalidating these tokens. It considers all such Secrets belonging to ServiceAccounts with .automountServiceAccountToken=false. By default, all namespaces in the target cluster are watched, however, this can be configured by specifying the --target-namespace flag.\napiVersion: v1 kind: ServiceAccount metadata:  name: my-serviceaccount automountServiceAccountToken: false This will result in a static ServiceAccount token secret whose token value is invalid:\napiVersion: v1 kind: Secret metadata:  annotations:  kubernetes.io/service-account.name: my-serviceaccount  kubernetes.io/service-account.uid: 86e98645-2e05-11e9-863a-b2d4d086dd5a  name: my-serviceaccount-token-ntxs9 type: kubernetes.io/service-account-token data:  ca.crt: base64(cluster-ca-cert)  namespace: base64(namespace)  token: AAAA Any attempt to regenerate the token or creating a new such secret will again make the component invalidating it.\n You can opt-out of this behaviour for ServiceAccounts setting .automountServiceAccountToken=false by labeling them with token-invalidator.resources.gardener.cloud/skip=true.\n In order to enable the TokenInvalidator you have to set --token-invalidator-max-concurrent-workers to a value larger than 0.\nBelow graphic shows an overview of the Token Invalidator for Service account secrets in the Shoot cluster. TokenRequestor This controller provides the service to create and auto-renew tokens via the TokenRequest API.\nIt provides a functionality similar to the kubelet’s Service Account Token Volume Projection. It was created to handle the special case of issuing tokens to pods that run in a different cluster than the API server they communicate with (hence, using the native token volume projection feature is not possible).\nThe controller differentiates between source cluster and target cluster. The source cluster hosts the gardener-resource-manager pod. Secrets in this cluster are watched and modified by the controller. The target cluster can be configured to point to another cluster. The existence of ServiceAccounts are ensured and token requests are issued against the target. When the gardener-resource-manager is deployed next to the Shoot’s controlplane in the Seed the source cluster is the Seed while the target cluster points to the Shoot.\nReconciliation Loop This controller reconciles secrets in all namespaces in the source cluster with the label: resources.gardener.cloud/purpose: token-requestor. See here for an example of the secret.\nThe controller ensures a ServiceAccount exists in the target cluster as specified in the annotations of the Secret in the source cluster:\nserviceaccount.resources.gardener.cloud/name: \u003csa-name\u003e serviceaccount.resources.gardener.cloud/namespace: \u003csa-namespace\u003e The requested tokens will act with the privileges which are assigned to this ServiceAccount.\nThe controller will then request a token via the TokenRequest API and populate it into the .data.token field to the Secret in the source cluster.\nAlternatively, the client can provide a raw kubeconfig (in YAML or JSON format) via the Secret’s .data.kubeconfig field. The controller will then populate the requested token in the kubeconfig for the user used in the .current-context. For example, if .data.kubeconfig is\napiVersion: v1 clusters: - cluster:  certificate-authority-data: AAAA  server: some-server-url  name: shoot--foo--bar contexts: - context:  cluster: shoot--foo--bar  user: shoot--foo--bar-token  name: shoot--foo--bar current-context: shoot--foo--bar kind: Config preferences: {} users: - name: shoot--foo--bar-token  user:  token: \"\" then the .users[0].user.token field of the kubeconfig will be updated accordingly.\nThe controller also adds an annotation to the Secret to keep track when to renew the token before it expires. By default, the tokens are issued to expire after 12 hours. The expiration time can be set with the following annotation:\nserviceaccount.resources.gardener.cloud/token-expiration-duration: 6h It automatically renews once 80% of the lifetime is reached or after 24h.\nOptionally, the controller can also populate the token into a Secret in the target cluster. This can be requested by annotating the Secret in the source cluster with\ntoken-requestor.resources.gardener.cloud/target-secret-name: \"foo\" token-requestor.resources.gardener.cloud/target-secret-namespace: \"bar\" Overall, the TokenRequestor controller provides credentials with limited lifetime (JWT tokens) used by Shoot control plane components running in the Seed to talk to the Shoot API Server. Please see the graphic below:\nKubelet Server CertificateSigningRequest Approver Gardener configures the kubelets such that they request two certificates via the CertificateSigningRequest API:\n client certificate for communicating with the kube-apiserver server certificate for serving its HTTPS server  For client certificates, the kubernetes.io/kube-apiserver-client-kubelet signer is used (see this document for more details). The kube-controller-manager’s csrapprover controller is responsible for auto-approving such CertificateSigningRequests so that the respective certificates can be issued.\nFor server certificates, the kubernetes.io/kubelet-serving signer is used. Unfortunately, the kube-controller-manager is not able to auto-approve such CertificateSigningRequests (see kubernetes/kubernetes#73356 for details).\nThat’s the motivation for having this controller as part of gardener-resource-manager. It watches CertificateSigningRequests with the kubernetes.io/kubelet-serving signer and auto-approves them when all the following conditions are met:\n The .spec.username is prefixed with system:node:. There must be at least one DNS name or IP address as part of the certificate SANs. The common name in the CSR must match the .spec.username. The organization in the CSR must only contain system:nodes. There must be a Node object with the same name in the shoot cluster. There must be exactly one Machine for the node in the seed cluster. The DNS names part of the SANs must be equal to all .status.addresses[] of type Hostname in the Node. The IP addresses part of the SANs must be equal to all .status.addresses[] of type InternalIP in the Node.  If one of these requirements is violated the CertificateSigningRequest will be denied. Otherwise, once approved the kube-controller-manager’s csrsigner controller will issue the requested certificate.\nWebhooks Auto-Mounting Projected ServiceAccount Tokens When this webhook is activated then it automatically injects projected ServiceAccount token volumes into Pods and all its containers if all of the following preconditions are fulfilled:\n The Pod is NOT labeled with projected-token-mount.resources.gardener.cloud/skip=true. The Pod’s .spec.serviceAccountName field is NOT empty and NOT set to default. The ServiceAccount specified in the Pod’s .spec.serviceAccountName sets .automountServiceAccountToken=false. The Pod’s .spec.volumes[] DO NOT already contain a volume with a name prefixed with kube-api-access-.  The projected volume will look as follows:\nspec:  volumes:  - name: kube-api-access-gardener  projected:  defaultMode: 420  sources:  - serviceAccountToken:  expirationSeconds: 43200  path: token  - configMap:  items:  - key: ca.crt  path: ca.crt  name: kube-root-ca.crt  - downwardAPI:  items:  - fieldRef:  apiVersion: v1  fieldPath: metadata.namespace  path: namespace  The expirationSeconds are defaulted to 12h and can be overwritten with the --projected-token-mount-expiration-seconds flag, or with the projected-token-mount.resources.gardener.cloud/expiration-seconds annotation on a Pod resource.\n The volume will be mounted into all containers specified in the Pod to the path /var/run/secrets/kubernetes.io/serviceaccount. This is the default location where client libraries expect to find the tokens and mimics the upstream ServiceAccount admission plugin, see this document for more information.\nOverall, this webhook is used to inject projected service account tokens into pods running in the Shoot and the Seed cluster. Hence, it is served from the Seed GRM and each Shoot GRM. Please find an overview below for pods deployed in the Shoot cluster:\nPod Zone Affinity When this webhook is activated and namespaces are annotated with control-plane.shoot.gardener.cloud/enforce-zone then it automatically adds a pod affinity to all Pods created in these namespaces:\nspec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: {} topologyKey: topology.kubernetes.io/zone In addition, if the annotation key control-plane.shoot.gardener.cloud/enforce-zone has a value \u003czone-value\u003e, i.e. zone assigned, this information is added as part of a node affinity.\nspec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone operator: In values: - \u003czone-value\u003e Those terms let pods within a namespace being scheduled to nodes residing in the very same zone which is either randomly picked, or to a very specific zone. In addition, the webhook removes any (anti-)affinities with topology.kubernetes.io/zone because they potentially contradict the above shown configuration. Gardener uses this webhook to schedule control-plane pods within a single zone on a multi-zonal seed (seed with worker nodes across zones). The goal is to reduce cross zonal network traffic within the seed with this approach.\nPod Topology Spread Constraints When this webhook is enabled then it mimics the topologyKey feature for Topology Spread Constraints (TSC) on the label pod-template-hash. Concretely, when a pod is labelled with pod-template-hash the handler of this webhook extends any topology spread constraint in the pod:\nmetadata:  labels:  pod-template-hash: 123abc spec:  topologySpreadConstraints:  - maxSkew: 1  topologyKey: topology.kubernetes.io/zone  whenUnsatisfiable: DoNotSchedule  labelSelector:  matchLabels:  pod-template-hash: 123abc # added by webhook The procedure circumvents a known limitation with TSCs which leads to imbalanced deployments after rolling updates. Gardener enables this webhook to schedule pods of deployments across nodes and zones.\nPlease note, the gardener-resource-manager itself as well as pods labelled with topology-spread-constraints.resources.gardener.cloud/skip are excluded from any mutations.\n","categories":"","description":"","excerpt":"Gardener Resource Manager Initially, the gardener-resource-manager was …","ref":"/docs/gardener/concepts/resource-manager/","tags":"","title":"Resource Manager"},{"body":"Packages:\n  resources.gardener.cloud/v1alpha1   resources.gardener.cloud/v1alpha1  Package v1alpha1 contains the configuration of the Gardener Resource Manager.\nResource Types:  ManagedResource   ManagedResource describes a list of managed resources.\n   Field Description      metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ManagedResourceSpec     Spec contains the specification of this managed resource.\n     class  string    (Optional) Class holds the resource class used to control the responsibility for multiple resource manager instances\n    secretRefs  []Kubernetes core/v1.LocalObjectReference     SecretRefs is a list of secret references.\n    injectLabels  map[string]string    (Optional) InjectLabels injects the provided labels into every resource that is part of the referenced secrets.\n    forceOverwriteLabels  bool    (Optional) ForceOverwriteLabels specifies that all existing labels should be overwritten. Defaults to false.\n    forceOverwriteAnnotations  bool    (Optional) ForceOverwriteAnnotations specifies that all existing annotations should be overwritten. Defaults to false.\n    keepObjects  bool    (Optional) KeepObjects specifies whether the objects should be kept although the managed resource has already been deleted. Defaults to false.\n    equivalences  [][]k8s.io/apimachinery/pkg/apis/meta/v1.GroupKind    (Optional) Equivalences specifies possible group/kind equivalences for objects.\n    deletePersistentVolumeClaims  bool    (Optional) DeletePersistentVolumeClaims specifies if PersistentVolumeClaims created by StatefulSets, which are managed by this resource, should also be deleted when the corresponding StatefulSet is deleted (defaults to false).\n       status  ManagedResourceStatus     Status contains the status of this managed resource.\n    ManagedResourceSpec   (Appears on: ManagedResource)  ManagedResourceSpec contains the specification of this managed resource.\n   Field Description      class  string    (Optional) Class holds the resource class used to control the responsibility for multiple resource manager instances\n    secretRefs  []Kubernetes core/v1.LocalObjectReference     SecretRefs is a list of secret references.\n    injectLabels  map[string]string    (Optional) InjectLabels injects the provided labels into every resource that is part of the referenced secrets.\n    forceOverwriteLabels  bool    (Optional) ForceOverwriteLabels specifies that all existing labels should be overwritten. Defaults to false.\n    forceOverwriteAnnotations  bool    (Optional) ForceOverwriteAnnotations specifies that all existing annotations should be overwritten. Defaults to false.\n    keepObjects  bool    (Optional) KeepObjects specifies whether the objects should be kept although the managed resource has already been deleted. Defaults to false.\n    equivalences  [][]k8s.io/apimachinery/pkg/apis/meta/v1.GroupKind    (Optional) Equivalences specifies possible group/kind equivalences for objects.\n    deletePersistentVolumeClaims  bool    (Optional) DeletePersistentVolumeClaims specifies if PersistentVolumeClaims created by StatefulSets, which are managed by this resource, should also be deleted when the corresponding StatefulSet is deleted (defaults to false).\n    ManagedResourceStatus   (Appears on: ManagedResource)  ManagedResourceStatus is the status of a managed resource.\n   Field Description      conditions  []github.com/gardener/gardener/pkg/apis/core/v1beta1.Condition         observedGeneration  int64    ObservedGeneration is the most recent generation observed for this resource.\n    resources  []ObjectReference     (Optional) Resources is a list of objects that have been created.\n    secretsDataChecksum  string    (Optional) SecretsDataChecksum is the checksum of referenced secrets data.\n    ObjectReference   (Appears on: ManagedResourceStatus)  ObjectReference is a reference to another object.\n   Field Description      ObjectReference  Kubernetes core/v1.ObjectReference      (Members of ObjectReference are embedded into this type.)     labels  map[string]string    Labels is a map of labels that were used during last update of the resource.\n    annotations  map[string]string    Annotations is a map of annotations that were used during last update of the resource.\n      Generated with gen-crd-api-reference-docs \n","categories":"","description":"","excerpt":"Packages:\n  resources.gardener.cloud/v1alpha1 …","ref":"/docs/gardener/api-reference/resources/","tags":"","title":"Resources"},{"body":"GEP-14: Reversed Cluster VPN Table of Contents  Motivation Proposal Alternatives  Motivation It is necessary to describe the current VPN solution and outline its shortcomings in order to motivate this proposal.\nProblem Statement Today’s Gardener cluster VPN solution has several issues including:\n Connection establishment is always from the seed cluster to the shoot cluster. This means that there needs to be connectivity both ways which is not desirable in many cases (OpenStack, VMware) and causes high effort in firewall configuration or extra infrastructure. These firewall configurations are prohibited in some cases due to security policies. Shoot clusters must provide a VPN endpoint. This means extra cost for the endpoint (roughly €20/month on hyperscalers) or will consume scarce resources (limited number of VMware NSX-T load balancers).  A first implementation has been provided to resolve the issues with the konnectivity server. As we did find several shortcomings with the underlying technology component, the apiserver-network-proxy we believe that this is not a suitable way ahead. We have opened an issue and provided two solution proposals to the community. We do see some remedies, e.g. using the Quick Protocol instead of GRPC but we (a) consider the implementation effort significantly higher compared to this proposal and (b) would use an experimental protocol to solve a problem that can also be solved with existing and proven core network technologies.\nWe will therefore not continue to invest into this approach. We will however research a similar approach (see below in “Further Research”).\nCurrent Solution Outline The current solution consists of multiple VPN connections from each API server pod and the Prometheus pod of a control plane to an OpenVPN server running in the shoot cluster. This OpenVPN server is exposed via a load balancer that must have an IP address which is reachable from the seed cluster. The routing in the seed cluster pods is configured to route all traffic for the node, pod, and service ranges to the shoot cluster. This means that there is no address overlap allowed between seed- and shoot cluster node, pod, and service ranges.\nIn the seed cluster the vpn-seed container is a sidecar to the kube-apiserver and prometheus pods. OpenVPN acts as a TCP client connecting to an OpenVPN TCP server. This is not optimal (e.g. tunneling TCP over TCP is discouraged) but at the time of development there was no UDP load balancer available on at least one of the major hyperscalers. Connectivity could have been switched to UDP later but the development effort was not spent.\nThe solution is depicted in this diagram:\nThese are the essential parts of the OpenVPN client configuration in the vpn-seed sidecar container:\n# use TCP instead of UDP (commonly not supported by load balancers) proto tcp-client [...] # get all routing information from server pull tls-client key \"/srv/secrets/vpn-seed/tls.key\" cert \"/srv/secrets/vpn-seed/tls.crt\" ca \"/srv/secrets/vpn-seed/ca.crt\" tls-auth \"/srv/secrets/tlsauth/vpn.tlsauth\" 1 cipher AES-256-CBC # https://openvpn.net/index.php/open-source/documentation/howto.html#mitm remote-cert-tls server # pull filter pull-filter accept \"route 100.64.0.0 255.248.0.0\" pull-filter accept \"route 100.96.0.0 255.224.0.0\" pull-filter accept \"route 10.1.60.0 255.255.252.0\" pull-filter accept \"route 192.168.123.\" pull-filter ignore \"route\" pull-filter ignore redirect-gateway pull-filter ignore route-ipv6 pull-filter ignore redirect-gateway-ipv6 Encryption is based on SSL certificates with an additional HMAC signature to all SSL/TLS handshake packets. As multiple clients connect to the OpenVPN server in the shoot cluster, all clients must be assigned a unique IP address. This is done by the OpenVPN server pushing that configuration to the client (keyword pull). As this is potentially problematic because the OpenVPN server runs in an untrusted environment there are pull filters denying all but necessary routes for the pod, service, and node networks.\nThe OpenVPN server running in the shoot cluster is configured as follows:\nmode server tls-server proto tcp4-server dev tun0 [...] server 192.168.123.0 255.255.255.0 push \"route 10.243.0.0 255.255.128.0\" push \"route 10.243.128.0 255.255.128.0\" duplicate-cn key \"/srv/secrets/vpn-shoot/tls.key\" cert \"/srv/secrets/vpn-shoot/tls.crt\" ca \"/srv/secrets/vpn-shoot/ca.crt\" dh \"/srv/secrets/dh/dh2048.pem\" tls-auth \"/srv/secrets/tlsauth/vpn.tlsauth\" 0 push \"route 10.242.0.0 255.255.0.0\" It is a TCP TLS server and configured to automatically assign IP addresses for OpenVPN clients (server directive). In addition, it pushes the shoot cluster node-, pod-, and service ranges to the clients running in the seed cluster (push directive).\nNote: The network mesh spanned by OpenVPN uses the network range 192.168.123.0 - 192.168.123.255. This network range cannot be used in either shoot-, or seed clusters. If it is used this might cause subtle problem due to network range overlaps. Unfortunately, this appears not to be well documented but this restriction exists since the very beginning. We should clean up this technical debt as part of the exercise.\nGoals  We intend to supersede the current VPN solution with the solution outlined in this proposal. We intend to remove the code for the konnectivity tunnel once this solution proposal has been validated.  Non Goals  The solution is not a low latency, or high throughput solution. As the kube-apiserver to shoot cluster traffic does not demand these properties we do not intend to invest in improvements. We do not intend to provide continuous availability to the shoot-seed VPN connection. We expect the availability to be comparable to the existing solution.  Proposal The proposal is depicted in the following diagram:\nWe have added an OpenVPN server pod (vpn-seed-server) to each control plane. The OpenVPN client in the shoot cluster (vpn-shoot-client) connects to the OpenVPN server.\nThe two containers vpn-seed-server and vpn-shoot-client are new containers and are not related to containers in the github.com/gardener/vpn project. We will create a new project github.com/gardener/vpn2 for these containers. With this solution we intend to supersede the containers from the github.com/gardener/vpn project.\nA service vpn-seed-server of type ClusterIP is created for each control plane in its namespace.\nThe vpn-shoot-client pod connects to the correct vpn-seed-server service via the SNI passthrough proxy introduced with SNI Passthrough proxy for kube-apiservers on port 8132.\nShoot OpenVPN clients (vpn-shoot-client) connect to the correct OpenVPN Server using the http proxy feature provided by OpenVPN. A configuration is added to the envoy proxy to detect http proxy requests and open a connection attempt to the correct OpenVPN server.\nThe kube-apiserver to shoot cluster connections are established using the API server proxy feature via an envoy proxy sidecar container of the vpn-seed-server container.\nThe restriction regarding the 192.168.123.0/24 network range in the current VPN solution still applies to this proposal. No other restrictions are introduced. In the context of this GEP a pull requst has been filed to block usage of that range by shoot clusters.\nPerformance and Scalability We do expect performance and throughput to be slightly lower compared to the existing solution. This is because the OpenVPN server acts as an additional hop and must decrypt and re-encrypt traffic that passes through. As there are no low latency, or high thoughput requirements for this connection we do not assume this to be an issue.\nAvailability and Failure Scenarios This solution re-uses multiple instances of the envoy component used for the kube-apiserver endpoints. We assume that the availability for kube-apiservers is good enough for the cluster VPN as well.\nThe OpenVPN client- and server pods are singleton pods in this approach and therefore are affected by potential failures and during cluster-, and control plane updates. Potential outages are only restricted to single shoot clusters and are comparable to the situation with the existing solution today.\nFeature Gates and Migration Strategy We have introduced a gardenlet feature gate ReversedVPN. If APIServerSNI and ReversedVPN are enabled the proposed solution is automatically enabled for all shoot clusters hosted by the seed. If ReversedVPN is enabled but APIServerSNI is not the gardenlet will panic during startup as this is an invalid configuration. All existing shoot clusters will automatically be migrated during the next reconciliation. We assume that the ReversedVPN feature will work with Gardener as well as operator managed Istio.\nWe have also added a shoot annotation alpha.featuregates.shoot.gardener.cloud/reversed-vpn which can override the feature gate to enable or disable the solution for individual clusters. This is only respected if APIServerSNI is enabled, otherwise it is ignored.\nSecurity Review The change in the VPN solution will potentially open up new attack vectors. We will perform a thorough analysis outside of this document.\nAlternatives WireGuard and Kubelink based Cluster VPN We have done a detailed investigation and implementation of a reversed VPN based on WireGuard. While we believe that it is technically feasible and superior to the approach presented above there are some concerns with regards to scalability, and high availability. As the WireGuard scenario based on kubelink is relevant for other use cases we continue to improve this implementation and address the concerns but we concede that this might not be on time for the cluster VPN. We nevertheless keep the implementation and provide an outline as part of this proposal.\nThe general idea of the proposal is to keep the existing cluster VPN solution more or less as is, but change the underlying network used for the vpn seed =\u003e vpn shoot connection. The underlying network should be established in the reversed direction, i.e. the shoot cluster should initiate the network connection, but it nevertheless should work in both directions.\nWe achieve this by tunneling the open vpn connection through a WireGuard tunnel, which is established from the shoot to the seed (note that WireGuard uses UDP as protocol). Independent of that we can also use UDP for the OpenVPN connection, but we can also stay with TCP as it was before. While this might look like a big change, it only introduces minor changes to the existing solution, but let’s look at the details. In essence, the OpenVPN connection does not require a public endpoint in the shoot cluster but it usees the internal endpoint provided by the WireGuard tunnel.\nThis is roughly depcited in this diagram. Note, that the vpn-seed and vpn-shoot containers only require very little changes and are fully backwards compatible.\nThe WireGuard network needs a separate network range/CIDR. It has to be unique for the seed and all its shoot clusters. An example for an assumed workload of around 1000 shoot clusters would be 192.168.128.0/22 (1024 IP addresses), i.e. 192.168.128.0-192.168.131.255. The IP addresses from this range need to be managed, but the IP address management (IPAM) using the Gardener Kubernetes objects like seed and shootstate as backing store is fairly straightforward. This is especially true as we do not expect large network ranges and only infrequent IP allocations. Hence, the IP address allocation can be quite simple, i.e. scan the range for a free IP address of all shoot clusters in a seed and allocate the first free address from the range.\nThere is another restriction: in case shoot clusters are configured to be seed clusters this network range must not overlap with the “parent” seed cluster. If the parent seed cluster uses 192.168.128.0/22 the child seed cluster can for example use 192.168.132.0/22. Grandchildren can however use grandparent IP address ranges. Also 2 children seed clusters can use identical ranges.\nThis slightly adds to the restrictions described in the current solution outline. In that the arbitrary chosen 192.168.123.0/24 range is restricted. For the purpose of this implementation we propose to extend that restriction to 192.168.128.0/17 range. Most of it would be reserved for “future use” however. We are well aware that this adds to the burden of correctly configuring Gardener landscapes.\nWe do consider this to be a challenge that needs to be addressed by careful configuration of the Gardener seed cluster infrastructure. Together with the 192.168.123.0/24 address range these ranges should be automatically blocked for usage by shoots.\nWireGuard can utilize the Linux kernel so that after initialization/configuration no user space processes are required. We propose to recommend the WireGuard kernel module as the default solution for all seeds. For shoot clusters, the WireGuard kernel based approach is also recommended, but the user space solution should also work as we expect less traffic on the shoot side. We expect the userspace implementation to work on all operating systems supported by Gardener in case no kernel module is available.\nAlmost all seed clusters are already managed by Gardener and we assume that those are configured with the WireGuard kernel module. There are however some cases where we use other Kubernetes distributions as seed cluster which may not have an operating system with WireGuard module available. We will therefore generally support the user space WireGuard process on seed cluster but place a size restriction on the number of control planes on those seeds.\nThere is a user space implementation of WireGuard, which can be used on Linux distributions without the WireGuard kernel module. (WireGuard moved into the standard Linux kernel 5.6.) Our proposal can handle the kernel/user space switch transparently, i.e. we include the user space binaries and use them only when required. However, especially for the seed the kernel based solution might be more attractive. Garden Linux 318.4.0 supports WireGuard.\nWe have looked at Ubuntu and SuSE chost:\n SuSE chost does not provide the WireGuard kernel module and it is not installable via zypper. It should however be straightforward for SuSE to include that in their next release. Ubuntu does not provide the kernel module either but it can be installed using apt-get install wireguard. With that it appears straightforward to provide an image with WireGuard pre-installed.  On the seed, we add a WireGuard device to one node on the host network. For all other nodes on the seed, we adapt the routes accordingly to route traffic destined for the WireGuard network to our WireGuard node. The Kubernetes pods managing the WireGuard device and routes are only used for initial configuration and later reconfiguration. During runtime, they can restart without any impact on the operation of the WireGuard network as the WireGuard device is managed by the Linux kernel.\nWith Calico as the networking solution it is not easily possible to put the WireGuard endpoint into a pod. Putting the WireGuard endpoint into a pod would require to define it as a gateway in the api server or prometheus pods but this is not possible since Calico does not span a proper subnet. While the defined CIDR in the pod network might be 100.96.0.0/11 the network visible from within a pod is only 100.96.0.5/32. This restriction might not exist with other networking solutions.\nThe WireGuard endpoint on the seed is exposed via a load balancer. We propose to use kubelink to manage the WireGuard configuration/device on the seed. We consider the management of the WireGuard endpoint to be complex especially in error situations which is the reason for utilizing kubelink as there is already significant experience managing an endpoint. We propose moving kubelink to the Gardener org in case it is used by this proposal.\nKubelink addresses three challenges managing WireGuard interfaces on cluster nodes. First, with WireGuard interfaces directly on the node (hostNetwork=true) the lifecycle of the interface is decoupled from the lifecycle of the pod that created it. This means that there will have to be means of cleaning up the interfaces and its configuration in case the interface moves to a different node. Second, additional routing information must be distributed across the cluster. The WireGuard CIDR is unknown to the network implementation so additional routes must be distributed on all nodes of the cluster. Third, kubelink dynamincally configures the Wireguard interface with endpoints and their public keys.\nOn the shoot, we create the keys and acquire the WireGuard IP in the standard secret generation. The data is added as a secret to the control plane and to the shootstate. The vpn shoot deployment is extended to include the WireGuard device setup inside the vpn shoot pod network. For certain infrastructures (AWS), we need a re-advertiser to resolve the seed WireGuard endpoint and evaluate whether the IP address changed.\nWhile it is possible to configure a WireGuard device using DNS names only IP addresses can be stored in Linux Kernel data structures. A change of a load balancer IP address can therefore not be mitigated on that level. As WireGuard dynamically adapts endpoint IP addresses a change in load banlancer IPs is mitigated in most but not all cases. This is why a re-advertiser is required for public cloud providers such as AWS.\nThe load balancer exposing the OpenVPN endpoint in the shoot cluster is no longer required and therefore removed if this functionality is used.\nAs we want to slowly expand the usage of the WireGuard solution, we propose to introduce a feature gate for it. Furthermore, since the WireGuard network requires a separate network range, we propose to introduce a new section to the seed settings with two additional flags (enabled \u0026 cidr):\napiVersion: core.gardener.cloud/v1beta1 kind: Seed metadata: name: my-seed ... spec: ... settings: ... wireguard: enabled: true cidr: 192.168.128.0/22 Last but not least, we propose to introduce an annotation to the shoots to enable/disable the WireGuard tunnel explicitly.\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata: name: my-shoot annotations: alpha.featuregates.shoot.gardener.cloud/wireguard-tunnel: \"true\" ... Using this approach, it is easy to switch the solution on and off, i.e. migrate the shoot clusters automatically during ordinary reconciliation.\nHigh Availability There is an issue if the node that hosts the WireGuard endpoint fails. The endpoint is migrated to another node however the time required to do this might exceed the budget for downtimes although one could argue that a disruption of less than 30 seconds to 1 minute does not qualify as a downtime and will in almost all cases not noticeable by end users.\nIn this case we also assume that TCP connections won’t be interrupted - they would just appear to hang. We will confirm this behavior and the potential downtime as part of the development and testing effort as this is hard to predict.\nAs a possible mitigation we propose to instantiate 2 Kubelink instances in the seed cluster that are served by two different load balancers. The instances must run on different nodes (if possible but we assume a proper seed cluster has more than one node). Each shoot cluster connects to both endpoints. This means that the OpenVPN server is reachable with two different IP addresses. The VPN seed sidecars must attempt to connect to both of them and will continue to do so. The “Persistent Keepalive” feature is set to 21 seconds by default but could be reduced. Due to the redundancy this however appears not to be necessary.\nIt is desirable that both connections are used in an equal manner. One strategy could be to use the kubelink 1 connection if the first target WireGuard address is even (the last byte of the IPv4 address), otherwise the kubelink 2 connection. The vpn-seed sidecars can then use the following configuration in their OpenVPN configuration file:\n\u003cconnection\u003e remote 192.168.45.3 1194 udp \u003c/connection\u003e \u003cconnection\u003e remote 192.168.47.34 1194 udp \u003c/connection\u003e OpenVPN will go through the list sequentially and try to connect to these endpoints.\nAs an additional mitigation it appears possible to instantiate WireGuard devices on all hosts and replicate its relevant conntrack state across all cluster nodes. The relevant conntrack state keeps the state of all connections passing through the WireGuard interface (e.g. the WireGuard CIDR). conntrack and the tools to replicate conntrack state are part of the essential Linux netfilter tools package.\nLoad Considerations What happens in case of a failure? In this case one router will end up owning all connections as the clients will attempt to use the next connection. This could be mitigated by adding a third redundant WireGuard connection. Using this strategy, the failure of one WireGuard endpoint would result in the equal distribution of connections to the two remaining interfaces. We believe however that this will not be necessary.\nThe cluster node running the Wireguard endpoint is essentially a router that routes all traffic to the various shoot clusters. This is established and proven technology that already exists since decades and has been highly optimized since then. This is also the technology that hyperscalers rely on to provide VPN connectivity to their customers. This said, hyperscalers essentially provide solutions based on IPsec which is known not to scale as well as Wireguard. Wireguard is a relatively new technology but we have no doubt that it is less stable than existing IPsec solution.\nRegarding performance there is a lot of information on the Internet basically suggesting that Wireguard performs better than other VPN solutions such as IPsec or OpenVPN. One example is https://www.wireguard.com/performance/ and https://www.net.in.tum.de/fileadmin/bibtex/publications/papers/2020-ifip-moonwire.pdf.\nBased on this, we have no reason to believe that one router will not be able to handle all traffic going to and coming from shoot clusters. Nevertheless, we will closely monitor the situation in our tests and will take action if necessary.\nFurther Research Based on feedback on this proposal and while working on this implementation we identified two additinal approaches that we have not thought of so far. The first idea can be used to replace the “inner” OpenVPN implementation and the second can be used to replace WireGuard with OpenVPN and get rid of the single point of failure.\n  Instead of using OpenVPN for the inner seed/shoot communication we can use the proxy protocol and use a TCP proxy (e.g. envoy) in the shoot cluster to broker the seed-shoot connections. The advantage is that with this solution seed- and shoot cluster network ranges are allowed to overlap. Disadvantages are increased implementation effort and less efficient network in terms of throughput and scalability. We believe however that the reduced network efficiency does not invalidate this option.\n  There is an option in OpenVPN to specify a tcp proxy as part of the endpoint configuration.\n  ","categories":"","description":"","excerpt":"GEP-14: Reversed Cluster VPN Table of Contents  Motivation Proposal …","ref":"/docs/gardener/proposals/14-reversed-cluster-vpn/","tags":"","title":"Reversed Cluster VPN"},{"body":"Reversed VPN Tunnel Setup and Configuration This is a short guide describing how to enable tunneling traffic from shoot cluster to seed cluster instead of the default “seed to shoot” direction.\nThe OpenVPN Default By default, Gardener makes use of OpenVPN to connect the shoot controlplane running on the seed cluster to the dataplane running on the shoot worker nodes, usually in isolated networks. This is achieved by having a sidecar to certain control plane components such as the kube-apiserver and prometheus.\nWith a sidecar, all traffic directed to the cluster is intercepted by iptables rules and redirected to the tunnel endpoint in the shoot cluster deployed behind a cloud loadbalancer. This has the following disadvantages:\n Every shoot would require an additional loadbalancer, this accounts for additional overhead in terms of both costs and troubleshooting efforts. Private access use-cases would not be possible without having a seed residing in the same private domain as a hard requirement. For example, have a look at this issue Providing a public endpoint to access components in the shoot poses a security risk.  This is how it looks like today with the OpenVPN solution:\nAPIServer | VPN-seed ---\u003e internet ---\u003e LB --\u003e VPN-Shoot (4314) --\u003e Pods | Nodes | Services\nReversing the Tunnel To address the above issues, the tunnel can establishment direction can be reverted, i.e. instead of having the client reside in the seed, we deploy the client in the shoot and initiate the connection from there. This way, there is no need to deploy a special purpose loadbalancer for the sake of addressing the dataplane, in addition to saving costs, this is considered the more secure alternative. For more information on how this is achieved, please have a look at the following GEP.\nHow it should look like at the end:\nAPIServer --\u003e Envoy-Proxy | VPN-Seed-Server \u003c-- Istio/Envoy-Proxy \u003c-- SNI API Server Endpoint \u003c-- LB (one for all clusters of a seed) \u003c--- internet \u003c--- VPN-Shoot-Client --\u003e Pods | Nodes | Services\nHow to Configure To enable the usage of the reversed vpn tunnel feature, either the Gardenlet ReversedVPN feature-gate must be set to true as shown below or the shoot must be annotated with \"alpha.featuregates.shoot.gardener.cloud/reversed-vpn: true\".\nfeatureGates:  ReversedVPN: true Please refer to the examples here for more information.\nTo disable the feature-gate the shoot must be annotated with \"alpha.featuregates.shoot.gardener.cloud/reversed-vpn: false\"\nOnce the feature-gate is enabled, a vpn-seed-server deployment will be added to the controlplane. The kube-apiserver will be configured to connect to resources in the dataplane such as pods, services and nodes though the vpn-seed-service via http proxy/connect protocol. In the dataplane of the cluster, the vpn-shoot will establish the connection to the vpn-seed-server indirectly using the SNI API Server endpoint as a http proxy. After the connection has been established requests from the kube-apiserver will be handled by the tunnel.\n Please note this feature is available ONLY for \u003e= 1.18 kubernetes clusters. For clusters with Kubernetes version \u003c 1.18, the default OpenVPN setup will be used by default even if the featuregate is enabled. Furthermore, this feature is still in Alpha, so you might see instabilities every now and then.\n ","categories":"","description":"","excerpt":"Reversed VPN Tunnel Setup and Configuration This is a short guide …","ref":"/docs/gardener/usage/reversed-vpn-tunnel/","tags":"","title":"Reversed VPN Tunnel"},{"body":"Gardener Scheduler The Gardener Scheduler is in essence a controller that watches newly created shoots and assigns a seed cluster to them. Conceptually, the task of the Gardener Scheduler is very similar to the task of the Kubernetes Scheduler: finding a seed for a shoot instead of a node for a pod.\nEither the scheduling strategy or the shoot cluster purpose hereby determines how the scheduler is operating. The following sections explain the configuration and flow in greater detail.\nWhy is the Gardener Scheduler needed? 1. Decoupling Previously, an admission plugin in the Gardener API server conducted the scheduling decisions. This implies changes to the API server whenever adjustments of the scheduling are needed. Decoupling the API server and the scheduler comes with greater flexibility to develop these components independently from each other.\n2. Extensibility It should be possible to easily extend and tweak the scheduler in the future. Possibly, similar to the Kubernetes scheduler, hooks could be provided which influence the scheduling decisions. It should be also possible to completely replace the standard Gardener Scheduler with a custom implementation.\nAlgorithm overview The following sequence describes the steps involved to determine a seed candidate:\n Determine usable seeds with “usable” defined as follows:  no .metadata.deletionTimestamp .spec.settings.scheduling.visible is true conditions Bootstrapped, GardenletReady, BackupBucketsReady (if available) are true   Filter seeds:  matching .spec.seedSelector in CloudProfile used by the Shoot matching .spec.seedSelector in Shoot having no network intersection with the Shoot’s networks (due to the VPN connectivity between seeds and shoots their networks must be disjoint) having .spec.settings.shootDNS.enabled=false (only if the shoot specifies a DNS domain or does not use the unmanaged DNS provider) whose taints (.spec.taints) are tolerated by the Shoot (.spec.tolerations) whose capacity for shoots would not be exceeded if the shoot is scheduled onto the seed, see Ensuring seeds capacity for shoots is not exceeded which are labelled with seed.gardener.cloud/multi-zonal if feature gate HAControlPlanes is turned on and shoot requests a high available control plane.   Apply active strategy e.g., Minimal Distance strategy Choose least utilized seed, i.e., the one with the least number of shoot control planes, will be the winner and written to the .spec.seedName field of the Shoot.  Configuration The Gardener Scheduler configuration has to be supplied on startup. It is a mandatory and also the only available flag. Here is an example scheduler configuration.\nMost of the configuration options are the same as in the Gardener Controller Manager (leader election, client connection, …). However, the Gardener Scheduler on the other hand does not need a TLS configuration, because there are currently no webhooks configurable.\nStrategies The scheduling strategy is defined in the candidateDeterminationStrategy of the scheduler’s configuration and can have the possible values SameRegion and MinimalDistance. The SameRegion strategy is the default strategy.\n  Same Region strategy\nThe Gardener Scheduler reads the spec.provider.type and .spec.region fields from the Shoot resource. It tries to find a seed that has the identical .spec.provider.type and .spec.provider.region fields set. If it cannot find a suitable seed, it adds an event to the shoot stating, that it is unschedulable.\n  Minimal Distance strategy\nThe Gardener Scheduler tries to find a valid seed with minimal distance to the shoot’s intended region. The distance is calculated based on the Levenshtein distance of the region. Therefore the region name is split into a base name and an orientation. Possible orientations are north, south, east, west and central. The distance then is twice the Levenshtein distance of the region’s base name plus a correction value based on the orientation and the provider.\nIf the orientations of shoot and seed candidate match, the correction value is 0, if they differ it is 2 and if either the seed’s or the shoot’s region does not have an orientation it is 1. If the provider differs the correction value is additionally incremented by 2.\nBecause of this a matching region with a matching provider is always prefered.\n  In order to put the scheduling decision into effect, the scheduler sends an update request for the Shoot resource to the API server. After validation, the Gardener Aggregated API server updates the shoot to have the spec.seedName field set. Subsequently, the Gardenlet picks up and starts to create the cluster on the specified seed.\nSpecial handling based on shoot cluster purpose  Every shoot cluster can have a purpose that describes what the cluster is used for, and also influences how the cluster is setup (see this document for more information).\nIn case the shoot has the testing purpose then the scheduler only reads the .spec.provider.type from the Shoot resource and tries to find a Seed that has the identical .spec.provider.type. The region does not matter, i.e., testing shoots may also be scheduled on a seed in a complete different region if it is better for balancing the whole Gardener system.\nshoots/binding subresource The shoots/binding subresource is used to bind a Shoot to a Seed. On creation of shoot clusters, the scheduler updates the binding automatically if an appropriate seed cluster is available. Only an operator with necessary RBAC can update this binding manually. This can be done by changing the .spec.seedName of the shoot. However, if a different seed is already assigned to the shoot, this will trigger a control-plane migration. For required steps, Please see Triggering the Migration.\nspec.seedName field in the Shoot specification Similar to the .spec.nodeName field in Pods, the Shoot specification has an optional .spec.seedName field. If this field is set on creation, the shoot will be scheduled to this seed. However, this field can only be set by users having RBAC for the shoots/binding subresource. If this field is not set, the scheduler will assign a suitable seed automatically and populate this field with the seed name.\nseedSelector field in the Shoot specification Similar to the .spec.nodeSelector field in Pods, the Shoot specification has an optional .spec.seedSelector field. It allows the user to provide a label selector that must match the labels of Seeds in order to be scheduled to one of them. The labels on Seeds are usually controlled by Gardener administrators/operators - end users cannot add arbitrary labels themselves. If provided, the Gardener Scheduler will only consider those seeds as “suitable” whose labels match those provided in the .spec.seedSelector of the Shoot.\nBy default only seeds with the same provider than the shoot are selected. By adding a providerTypes field to the seedSelector a dedicated set of possible providers (* means all provider types) can be selected.\nEnsuring seeds capacity for shoots is not exceeded Seeds have a practical limit of how many shoots they can accommodate. Exceeding this limit is undesirable as the system performance will be noticeably impacted. Therefore, the scheduler ensures that a seed’s capacity for shoots is not exceeded by taking into account a maximum number of shoots that can be scheduled onto a seed.\nThis mechanism works as follows:\n The gardenlet is configured with certain resources and their total capacity (and, for certain resources, the amount reserved for Gardener), see /example/20-componentconfig-gardenlet.yaml. Currently, the only such resource is the maximum number of shoots that can be scheduled onto a seed. The gardenlet seed controller updates the capacity and allocatable fields in Seed status with the capacity of each resource and how much of it is actually available to be consumed by shoots. The allocatable value of a resource is equal to capacity minus reserved. When scheduling shoots, the scheduler filters out all candidate seeds whose allocatable capacity for shoots would be exceeded if the shoot is scheduled onto the seed.  Failure to determine a suitable seed In case the scheduler fails to find a suitable seed, the operation is being retried with exponential backoff.\nCurrent Limitation / Future Plans  Azure has unfortunately a geographically non-hierarchical naming pattern and does not start with the continent. This is the reason why we will exchange the implementation of the MinimalDistance strategy with a more suitable one in the future.  ","categories":"","description":"","excerpt":"Gardener Scheduler The Gardener Scheduler is in essence a controller …","ref":"/docs/gardener/concepts/scheduler/","tags":"","title":"Scheduler"},{"body":"SecretBinding Provider Controller This page describes the process on how to enable the SecretBinding provider controller.\nOverview With Gardener v1.38.0 the SecretBinding resource does now contain a new optional field .provider.type (details about the motivation can be found in https://github.com/gardener/gardener/issues/4888). To make the process of setting the new field automated and afterwards to enforce validation on the new field in backwards compatible manner, Gardener features the SecretBinding provider controller and a feature gate - SecretBindingProviderValidation.\nProcess A Gardener landscape operator can follow the following steps:\n  Enable the SecretBinding provider controller of Gardener Controller Manager.\nThe SecretBinding provider controller is responsible to populate the .provider.type field of a SecretBinding based on its current usage by Shoot resources. For example if a Shoot crazy-botany with .provider.type=aws is using a SecretBinding my-secret-binding, then the SecretBinding provider controller will take care to set the .provider.type field of the SecretBinding to the same provider type (aws). To enable the SecretBinding provider controller, in the ControllerManagerConfiguration set the controller.secretBindingProvider.concurentSyncs field (e.g set it to 5). Although that it is not recommended, the API allows Shoots from different provider types to reference the same SecretBinding (assuming that backing Secret contains data for both of the provider types). To preserve the backwards compatibility for such SecretBindings, the provider controller will maintain the multiple provider types in the field (it will join them with separator , - for example aws,gcp).\n  Disable the SecretBinding provider controller and enable SecretBindingProviderValidation feature gate of Gardener API server.\nThe SecretBindingProviderValidation feature gate of Gardener API server enables set of validations for the SecretBinding provider field. It forbids creating a Shoot that has a different provider type from the referenced SecretBinding’s one. It also enforces immutability on the field. After making sure that SecretBinding provider controller is enabled and it populated the .provider.type field of a majority of the SecretBindings on a Gardener landscape (the SecretBindings that are unused will have their provider type unset), a Gardener landscape operator has to disable the SecretBinding provider controller and to enable the SecretBindingProviderValidation feature gate of Gardener API server. To disable the SecretBinding provider controller, in the ControllerManagerConfiguration set the controller.secretBindingProvider.concurentSyncs field to 0.\n  Implementation History  Gardener v1.38: The SecretBinding resource has a new optional field .provider.type. The SecretBinding provider controller is disabled by default. The SecretBindingProviderValidation feature gate of Gardener API server is disabled by default. Gardener v1.42: The SecretBinding provider controller is enabled by default. Gardener v1.51: The SecretBindingProviderValidation feature gate of Gardener API server is enabled by default and the SecretBinding provider controller is disabled by default. Gardener v1.53: The SecretBindingProviderValidation feature gate of Gardener API server is unconditionally enabled (can no longer be disabled). Gardener v1.55: The SecretBindingProviderValidation feature gate of Gardener API server and the SecretBinding provider controller are removed.  ","categories":"","description":"","excerpt":"SecretBinding Provider Controller This page describes the process on …","ref":"/docs/gardener/deployment/secret_binding_provider_controller/","tags":"","title":"Secret Binding Provider Controller"},{"body":"Secrets Management for Seed and Shoot Cluster The gardenlet needs to create quite some amount of credentials (certificates, private keys, passwords, etc.) for seed and shoot clusters in order to ensure secure deployments. Such credentials typically should be renewed automatically when their validity expires, rotated regularly, and they potentially need to be persisted such that they don’t get lost in case of a control plane migration or a lost seed cluster.\nSecretsManager Introduction These requirements can be covered by using the SecretsManager package maintained in pkg/utils/secrets/manager. It is built on top of the ConfigInterface and DataInterface interfaces part of pkg/utils/secrets and provides the following functions:\n  Generate(context.Context, secrets.ConfigInterface, ...GenerateOption) (*corev1.Secret, error)\nThis method either retrieves the current secret for the given configuration or it (re)generates it in case the configuration changed, the signing CA changed (for certificate secrets), or when proactive rotation was triggered. If the configuration describes a certificate authority secret then this method automatically generates a bundle secret containing the current and potentially the old certificate.\nAvailable GenerateOptions:\n SignedByCA(string, ...SignedByCAOption): This is only valid for certificate secrets and automatically retrieves the correct certificate authority in order to sign the provided server or client certificate.  There are two SignedByCAOptions:  UseCurrentCA. This option will sign server certificates with the new/current CA in case of a CA rotation. For more information, please refer to the “Certificate Signing” section below. UseOldCA. This option will sign client certificates with the old CA in case of a CA rotation. For more information, please refer to the “Certificate Signing” section below.     Persist(): This marks the secret such that it gets persisted in the ShootState resource in the garden cluster. Consequently, it should only be used for secrets related to a shoot cluster. Rotate(rotationStrategy): This specifies the strategy in case this secret is to be rotated or regenerated (either InPlace which immediately forgets about the old secret, or KeepOld which keeps the old secret in the system). IgnoreOldSecrets(): This specifies that old secrets should not be considered and loaded (contrary to the default behavior). It should be used when old secrets are no longer important and can be “forgotten” (e.g. in “phase 2” (t2) of the CA certificate rotation). Such old secrets will be deleted on Cleanup(). IgnoreOldSecretsAfter(time.Duration): This specifies that old secrets should not be considered and loaded once a given duration after rotation has passed. It can be used to clean up old secrets after automatic rotation (e.g. the Seed cluster CA is automatically rotated when its validity will soon end and the old CA will be cleaned up 24 hours after triggering the rotation). Validity(time.Duration): This specifies how long the secret should be valid. For certificate secret configurations, the manager will automatically deduce this information from the generated certificate.    Get(string, ...GetOption) (*corev1.Secret, bool)\nThis method retrieves the current secret for the given name. In case the secret in question is a certificate authority secret then it retrieves the bundle secret by default. It is important that this method only knows about secrets for which there were prior Generate calls.\nAvailable GetOptions:\n Bundle (default): This retrieves the bundle secret. Current: This retrieves the current secret. Old: This retrieves the old secret.    Cleanup(context.Context) error\nThis method deletes secrets which are no longer required. No longer required secrets are those still existing in the system which weren’t detected by prior Generate calls. Consequently, only call Cleanup after you have executed Generate calls for all desired secrets.\n  Some exemplary usages would look as follows:\nsecret, err := k.secretsManager.Generate(  ctx,  \u0026secrets.CertificateSecretConfig{  Name: \"my-server-secret\",  CommonName: \"server-abc\",  DNSNames: []string{\"first-name\", \"second-name\"},  CertType: secrets.ServerCert,  SkipPublishingCACertificate: true,  },  secretsmanager.SignedByCA(\"my-ca\"),  secretsmanager.Persist(),  secretsmanager.Rotate(secretsmanager.InPlace), ) if err != nil {  return err } As explained above, the caller does not need to care about the renewal, rotation or the persistence of this secret - all of these concerns are handled by the secrets manager. Automatic renewal of secrets happens when their validity approaches 80% or less than 10d are left until expiration.\nIn case a CA certificate is needed by some component then it can be retrieved as follows:\ncaSecret, found := k.secretsManager.Get(\"my-ca\") if !found {  return fmt.Errorf(\"secret my-ca not found\") } As explained above, this returns the bundle secret for the CA my-ca which might potentially contain both the current and the old CA (in case of rotation/regeneration).\nCertificate Signing By default, client certificates are always signed by the current CA while server certificate are signed by the old CA (if it exists). This is to ensure a smooth exchange of certificate during a CA rotation (typically has two phases, ref GEP-18):\n Client certificates:  In phase 1, clients get new certificates as soon as possible to ensure that all clients have been adapted before phase 2. In phase 2, the respective server drops accepting certificates signed by the old CA.   Server certificates:  In phase 1, servers still use their old/existing certificates to allow clients to update their CA bundle used for verification of the servers’ certificates. In phase 2, the old CA is dropped, hence servers need to get a certificate signed by the new/current CA. At this point in time, clients have already adapted their CA bundles.    Always Sign Server Certificates With Current CA In case you control all clients and update them at the same time as the server, it is possible to make the secrets manager generate even server certificates with the new/current CA. This can help to prevent certificate mismatches when the CA bundle is already exchanged while the server still serves with a certificate signed by a CA no longer part of the bundle.\nLet’s consider the two following examples:\n gardenlet deploys a webhook server (gardener-resource-manager) and a corresponding MutatingWebhookConfiguration at the same time. In this case, the server certificate should be generated with the new/current CA to avoid above mentioned certificate mismatches during a CA rotation. gardenlet deploys a server (etcd) in one step, and a client (kube-apiserver) in a subsequent step. In this case, the default behaviour should apply (server certificate should be signed by old/existing CA).  Always Sign Client Certificate With Old CA In the unusual case where the client is deployed before the server, it might be useful to always use the old CA for signing the client’s certificate. This can help to prevent certificate mismatches when the client already gets a new certificate while the server still only accepts certificates signed by the old CA.\nLet’s consider the following example:\n gardenlet deploys the kube-apiserver before the kubelet. However, the kube-apiserver has a client certificate signed by the ca-kubelet in order to communicate with it (e.g., when retrieving logs or forwarding ports). In this case, the client certificate should be generated with the old CA to avoid above mentioned certificate mismatches during a CA rotation.  Reusing the SecretsManager in Other Components While the SecretsManager is primarily used by gardenlet, it can be reused by other components (e.g. extensions) as well for managing secrets that are specific to the component or extension. For example, provider extensions might use their own SecretsManager instance for managing the serving certificate of cloud-controller-manager.\nExternal components that want to reuse the SecretsManager should consider the following aspects:\n On initialization of a SecretsManager, pass an identity specific to the component, controller and purpose. For example, gardenlet’s shoot controller uses gardenlet as the SecretsManager’s identity, the Worker controller in provider-foo should use provider-foo-worker and the ControlPlane controller should use provider-foo-controlplane-exposure for ControlPlane objects of purpose exposure. The given identity is added as a value for the manager-identity label on managed Secrets. This label is used by the Cleanup function to select only those Secrets that are actually managed by the particular SecretManager instance. This is done to prevent removing still needed Secrets that are managed by other instances. Generate dedicated CAs for signing certificates instead of depending on CAs managed by gardenlet. Names of Secrets managed by external SecretsManager instances must not conflict with Secret names from other instances (e.g. gardenlet). For CAs that should be rotated in lock-step with the Shoot CAs managed by gardenlet, components need to pass information about the last rotation initiation time and the current rotation phase to the SecretsManager upon initialization. The relevant information can be retrieved from the Cluster resource under .spec.shoot.status.credentials.rotation.certificateAuthorities. Independent of the specific identity, secrets marked with the Persist option are automatically saved in the ShootState resource by gardenlet and are also restored by gardenlet on Control Plane Migration to the new Seed.  Implementation Details The source of truth for the secrets manager is the list of Secrets in the Kubernetes cluster it acts upon (typically, the seed cluster). The persisted secrets in the ShootState are only used if and only if the shoot is in the Restore phase - in this case all secrets are just synced to the seed cluster so that they can be picked up by the secrets manager.\nIn order to prevent kubelets from unneeded watches (thus, causing some significant traffic against the kube-apiserver), the Secrets are marked as immutable. Consequently, they have a unique, deterministic name which is computed as follows:\n For CA secrets, the name is just exactly the name specified in the configuration (e.g., ca). This is for backwards-compatibility and will be dropped in a future release once all components depending on the static name have been adapted. For all other secrets, the name specified in the configuration is used as prefix followed by an 8-digit hash. This hash is computed out of the checksum of the secret configuration and the checksum of the certificate of the signing CA (only for certificate configurations).  In all cases, the name of the secrets is suffixed with a 5-digit hash computed out of the time when the rotation for this secret was last started.\n","categories":"","description":"","excerpt":"Secrets Management for Seed and Shoot Cluster The gardenlet needs to …","ref":"/docs/gardener/development/secrets_management/","tags":"","title":"Secrets Management"},{"body":"Gardener Security Release Process Gardener is a growing community of volunteers and users. The Gardener community has adopted this security disclosure and response policy to ensure we responsibly handle critical issues.\nGardener Security Team Security vulnerabilities should be handled quickly and sometimes privately. The primary goal of this process is to reduce the total time users are vulnerable to publicly known exploits. The Gardener Security Team is responsible for organizing the entire response including internal communication and external disclosure but will need help from relevant developers and release managers to successfully run this process. The initial Gardener Security Team will consist of the following volunteers:\n Olaf Beier, (@olafbeier) Vasu Chandrasekhara, (@vasu1124) Alban Crequy, (@alban) Norbert Hamann, (@norberthamann) Claudia Hölters, (@hoeltcl) Oliver Kling, (@oliverkling) Vedran Lerenc, (@vlerenc) Dirk Marwinski, (@marwinski) Michael Schubert, (@schu) Matthias Sohn, (@msohn) Frederik Thormaehlen, (@ThormaehlenFred) Christian Cwienk (@ccwienk)  Disclosures Private Disclosure Processes The Gardener community asks that all suspected vulnerabilities be privately and responsibly disclosed. If you’ve found a vulnerability or a potential vulnerability in Gardener please let us know by writing an e-mail to secure@sap.com. We’ll send a confirmation e-mail to acknowledge your report, and we’ll send an additional e-mail when we’ve identified the issue positively or negatively.\nPublic Disclosure Processes If you know of a publicly disclosed vulnerability please IMMEDIATELY e-mail to secure@sap.com to inform the Gardener Security Team about the vulnerability so they may start the patch, release, and communication process.\nIf possible the Gardener Security Team will ask the person making the public report if the issue can be handled via a private disclosure process (for example if the full exploit details have not yet been published). If the reporter denies the request for private disclosure, the Gardener Security Team will move swiftly with the fix and release process. In extreme cases GitHub can be asked to delete the issue but this generally isn’t necessary and is unlikely to make a public disclosure less damaging.\nPatch, Release, and Public Communication For each vulnerability a member of the Gardener Security Team will volunteer to lead coordination with the “Fix Team” and is responsible for sending disclosure e-mails to the rest of the community. This lead will be referred to as the “Fix Lead.” The role of the Fix Lead should rotate round-robin across the Gardener Security Team. Note that given the current size of the Gardener community it is likely that the Gardener Security Team is the same as the “Fix team.” (i.e., all maintainers). The Gardener Security Team may decide to bring in additional contributors for added expertise depending on the area of the code that contains the vulnerability. All of the time lines below are suggestions and assume a private disclosure. The Fix Lead drives the schedule using his best judgment based on severity and development time. If the Fix Lead is dealing with a public disclosure all time lines become ASAP (assuming the vulnerability has a CVSS score \u003e= 7; see below). If the fix relies on another upstream project’s disclosure time line, that will adjust the process as well. We will work with the upstream project to fit their time line and best protect our users.\nFix Team Organization The Fix Lead will work quickly to identify relevant engineers from the affected projects and packages and CC those engineers into the disclosure thread. These selected developers are the Fix Team. The Fix Lead will give the Fix Team access to a private security repository to develop the fix.\nFix Development Process The Fix Lead and the Fix Team will create a CVSS using the CVSS Calculator. The Fix Lead makes the final call on the calculated CVSS; it is better to move quickly than make the CVSS perfect. The Fix Team will notify the Fix Lead that work on the fix branch is complete once there are LGTMs on all commits in the private repository from one or more maintainers. If the CVSS score is under 7.0 (a medium severity score) the Fix Team can decide to slow the release process down in the face of holidays, developer bandwidth, etc. These decisions must be discussed on the private Gardener Security mailing list.\nFix Disclosure Process With the fix development underway, the Fix Lead needs to come up with an overall communication plan for the wider community. This Disclosure process should begin after the Fix Team has developed a Fix or mitigation so that a realistic time line can be communicated to users. The Fix Lead will inform the Gardener mailing list that a security vulnerability has been disclosed and that a fix will be made available in the future on a certain release date. The Fix Lead will include any mitigating steps users can take until a fix is available. The communication to Gardener users should be actionable. They should know when to block time to apply patches, understand exact mitigation steps, etc.\nFix Release Day The Release Managers will ensure all the binaries are built, publicly available, and functional before the Release Date. The Release Managers will create a new patch release branch from the latest patch release tag + the fix from the security branch. As a practical example if v0.12.0 is the latest patch release in gardener.git a new branch will be created called v0.12.1 which includes only patches required to fix the issue. The Fix Lead will cherry-pick the patches onto the master branch and all relevant release branches. The Fix Team will LGTM and merge. The Release Managers will merge these PRs as quickly as possible. Changes shouldn’t be made to the commits even for a typo in the CHANGELOG as this will change the git sha of the already built and commits leading to confusion and potentially conflicts as the fix is cherry-picked around branches. The Fix Lead will request a CVE from the SAP Product Security Response Team via email to cna@sap.com with all the relevant information (description, potential impact, affected version, fixed version, CVSS v3 base score and supporting documentation for the CVSS score) for every vulnerability. The Fix Lead will inform the Gardener mailing list and announce the new releases, the CVE number (if available), the location of the binaries, and the relevant merged PRs to get wide distribution and user action.\nAs much as possible this e-mail should be actionable and include links how to apply the fix to users environments; this can include links to external distributor documentation. The recommended target time is 4pm UTC on a non-Friday weekday. This means the announcement will be seen morning Pacific, early evening Europe, and late evening Asia. The Fix Lead will remove the Fix Team from the private security repository.\nRetrospective These steps should be completed after the Release Date. The retrospective process should be blameless.\nThe Fix Lead will send a retrospective of the process to the Gardener mailing list including details on everyone involved, the time line of the process, links to relevant PRs that introduced the issue, if relevant, and any critiques of the response and release process. The Release Managers and Fix Team are also encouraged to send their own feedback on the process to the Gardener mailing list. Honest critique is the only way we are going to get good at this as a community.\nCommunication Channel The private or public disclosure process should be triggered exclusively by writing an e-mail to secure@sap.com.\nGardener security announcements will be communicated by the Fix Lead sending an e-mail to the Gardener mailing list (reachable via gardener@googlegroups.com) as well as posting a link in the Gardener Slack channel. Public discussions about Gardener security announcements and retrospectives, will primarily happen in the Gardener mailing list. Thus Gardener community members who are interested in participating in discussions related to the Gardener Security Release Process are encouraged to join the Gardener mailing list (how to find and join a group)\nThe members of the Gardener Security Team are subscribed to the private Gardener Security mailing list (reachable via gardener-security@googlegroups.com).\n","categories":"","description":"","excerpt":"Gardener Security Release Process Gardener is a growing community of …","ref":"/docs/contribute/10_code/12_security_guide/","tags":"","title":"Security Release Process"},{"body":"Gardener Seed Admission Controller The Gardener Seed admission controller is deployed by the Gardenlet as part of its seed bootstrapping phase and, consequently, running in every seed cluster. It’s main purpose is to serve webhooks (validating or mutating) in order to admit or deny certain requests to the seed’s API server.\nWhat is it doing concretely? Validating Webhooks Unconfirmed Deletion Prevention As part of Gardener’s extensibility concepts a lot of CustomResourceDefinitions are deployed to the seed clusters that serve as extension points for provider-specific controllers. For example, the Infrastructure CRD triggers the provider extension to prepare the IaaS infrastructure of the underlying cloud provider for a to-be-created shoot cluster. Consequently, these extension CRDs have a lot of power and control large portions of the end-user’s shoot cluster. Accidental or undesired deletions of those resource can cause tremendous and hard-to-recover-from outages and should be prevented.\nTogether with the deployment of the Gardener seed admission controller a ValidatingWebhookConfiguration for CustomResourceDefinitions and most (custom) resources in the extensions.gardener.cloud/v1alpha1 API group is registered. It prevents DELETE requests for those CustomResourceDefinitions labeled with gardener.cloud/deletion-protected=true, and for all mentioned custom resources if they were not previously annotated with the confirmation.gardener.cloud/deletion=true. This prevents that undesired kubectl delete \u003c...\u003e requests are accepted.\nMutating Webhooks The admission controller endpoint /webhooks/default-pod-scheduler-name/gardener-kube-scheduler mutates pods and adds gardener-kube-scheduler to .spec.scheduleName.\nWhen SeedKubeScheduler feature gate is enabled, all control plane components are mutated. The scheduler scores Nodes with most resource usage higher than the rest, resulting in greater resource utilization.\n","categories":"","description":"","excerpt":"Gardener Seed Admission Controller The Gardener Seed admission …","ref":"/docs/gardener/concepts/seed-admission-controller/","tags":"","title":"Seed Admission Controller"},{"body":"Seed Bootstrapping Whenever the Gardenlet is responsible for a new Seed resource its “seed controller” is being activated. One part of this controller’s reconciliation logic is deploying certain components into the garden namespace of the seed cluster itself. These components are required to spawn and manage control planes for shoot clusters later on. This document is providing an overview which actions are performed during this bootstrapping phase, and it explains the rationale behind them.\nDependency Watchdog The dependency watchdog (abbreviation: DWD) is a component developed separately in the gardener/dependency-watchdog GitHub repository. Gardener is using it for two purposes:\n Prevention of melt-down situations when the load balancer used to expose the kube-apiserver of shoot clusters goes down while the kube-apiserver itself is still up and running Fast recovery times for crash-looping pods when depending pods are again available  For the sake of separating these concerns, two instances of the DWD are deployed by the seed controller.\nProbe The dependency-watchdog-probe deployment is responsible for above mentioned first point.\nThe kube-apiserver of shoot clusters is exposed via a load balancer, usually with an attached public IP, which serves as the main entry point when it comes to interaction with the shoot cluster (e.g., via kubectl). While end-users are talking to their clusters via this load balancer, other control plane components like the kube-controller-manager or kube-scheduler run in the same namespace/same cluster, so they can communicate via the in-cluster Service directly instead of using the detour with the load balancer. However, the worker nodes of shoot clusters run in isolated, distinct networks. This means that the kubelets and kube-proxys also have to talk to the control plane via the load balancer.\nThe kube-controller-manager has a special control loop called nodelifecycle which will set the status of Nodes to NotReady in case the kubelet stops to regularly renew its lease/to send its heartbeat. This will trigger other self-healing capabilities of Kubernetes, for example the eviction of pods from such “unready” nodes to healthy nodes. Similarly, the cloud-controller-manager has a control loop that will disconnect load balancers from “unready” nodes, i.e., such workload would no longer be accessible until moved to a healthy node.\nWhile these are awesome Kubernetes features on their own, they have a dangerous drawback when applied in the context of Gardener’s architecture: When the kube-apiserver load balancer fails for whatever reason then the kubelets can’t talk to the kube-apiserver to renew their lease anymore. After a minute or so the kube-controller-manager will get the impression that all nodes have died and will mark them as NotReady. This will trigger above mentioned eviction as well as detachment of load balancers. As a result, the customer’s workload will go down and become unreachable.\nThis is exactly the situation that the DWD prevents: It regularly tries to talk to the kube-apiservers of the shoot clusters, once by using their load balancer, and once by talking via the in-cluster Service. If it detects that the kube-apiserver is reachable internally but not externally it scales down the kube-controller-manager to 0. This will prevent it from marking the shoot worker nodes as “unready”. As soon as the kube-apiserver is reachable externally again the kube-controller-manager will be scaled up to 1 again.\nEndpoint The dependency-watchdog-endpoint deployment is responsible for above mentioned second point.\nKubernetes is restarting failing pods with an exponentially increasing backoff time. While this is a great strategy to prevent system overloads it has the disadvantage that the delay between restarts is increasing up to multiple minutes very fast.\nIn the Gardener context, we are deploying many components that are depending on other components. For example, the kube-apiserver is depending on a running etcd, or the kube-controller-manager and kube-scheduler are depending on a running kube-apiserver. In case such a “higher-level” component fails for whatever reason, the dependent pods will fail and end-up in crash-loops. As Kubernetes does not know anything about these hierarchies it won’t recognize that such pods can be restarted faster as soon as their dependents are up and running again.\nThis is exactly the situation in which the DWD will become active: If it detects that a certain Service is available again (e.g., after the etcd was temporarily down while being moved to another seed node) then DWD will restart all crash-looping dependant pods. These dependant pods are detected via a pre-configured label selector.\nAs of today, the DWD is configured to restart a crash-looping kube-apiserver after etcd became available again, or any pod depending on the kube-apiserver that has a gardener.cloud/role=controlplane label (e.g., kube-controller-manager, kube-scheduler, etc.).\n","categories":"","description":"","excerpt":"Seed Bootstrapping Whenever the Gardenlet is responsible for a new …","ref":"/docs/gardener/usage/seed_bootstrapping/","tags":"","title":"Seed Bootstrapping"},{"body":"Network Policies in the Seed Cluster This document describes the Kubernetes network policies deployed by Gardener into the Seed cluster. For network policies deployed into the Shoot kube-system namespace, please see the usage section.\nNetwork policies deployed by Gardener have names and annotations describing their purpose, so this document does only highlight a subset of the policies in detail.\nNetwork policies in the Shoot namespace in the Seed The network policies in the Shoot namespace in the Seed can roughly be grouped into policies required for the control plane components and for logging \u0026 monitoring.\nThe network policy deny-all plays a special role. This policy denies all ingress and egress traffic from each pod in the Shoot namespace. So per default, a pod running in the control plane cannot talk to any other pod in the whole Seed cluster. This means the pod needs to have labels matching to appropriate network policies allowing it to talk to exactly the components required to execute its desired functionality. This has also implications for Gardener extensions that need to deploy additional components into the Shoot's control plane.\nNetwork Policies for Control Plane Components This section highlights a selection of network policies that exist in the Shoot namespace in the Seed cluster. In general, the control plane components serve different purposes and thus need access to different pods and network ranges.\nIn contrast to other network policies, the policy allow-to-shoot-networks is tailored to the individual Shoot cluster, because it is based on the network configuration in the Shoot manifest. It allows pods with the label networking.gardener.cloud/to-shoot-networks=allowed to access pods in the Shoot pod, service and node CIDR range. This is used by the Shoot API Server and the prometheus pods to communicate over VPN/proxy with pods in the Shoot cluster. This network policy is only useful if reversed vpn is disabled as otherwise the vpn-seed-server pod in the control plane is the only pod with layer 3 routing to the shoot network.\nThe policy allow-to-blocked-cidrs allows pods with the label networking.gardener.cloud/to-blocked-cidrs=allowed to access IPs that are explicitly blocked for all control planes in a Seed cluster (configurable via spec.networks.blockCIDRS). This is used for instance to block the cloud provider’s metadata service.\nAnother network policy to be highlighted is allow-to-seed-apiserver. Some components need access to the Seed API Server. This can be allowed by labeling the pod with networking.gardener.cloud/to-seed-apiserver=allowed. This policy allows exactly the IPs of the kube-apiserver of the Seed. While all other policies have a static set of permissions (do not change during the lifecycle of the Shoot), the policy allow-to-seed-apiserver is reconciled to reflect the endpoints in the default namespace. This is required because endpoint IPs are not necessarily stable (think of scaling the Seed API Server pods or hibernating the Seed cluster (acting as a managed seed) in a local development environment).\nFurthermore, the following network policies exist in the Shoot namespace. These policies are the same for every Shoot control plane.\nNAME POD-SELECTOR # Pods that need to access the Shoot API server. Used by all Kubernetes control plane components. allow-to-shoot-apiserver networking.gardener.cloud/to-shoot-apiserver=allowed # allows access to kube-dns/core-dns pods for DNS queries allow-to-dns networking.gardener.cloud/to-dns=allowed # allows access to private IP address ranges allow-to-private-networks networking.gardener.cloud/to-private-networks=allowed # allows access to all but private IP address ranges allow-to-public-networks networking.gardener.cloud/to-public-networks=allowed # allows Ingress to etcd pods from the Shoot's Kubernetes API Server allow-etcd app=etcd-statefulset,garden.sapcloud.io/role=controlplane # used by the Shoot API server to allows ingress from pods labeled # with'networking.gardener.cloud/to-shoot-apiserver=allowed', from Prometheus, and allows Egress to etcd pods allow-kube-apiserver app=kubernetes,gardener.cloud/role=controlplane,role=apiserver Network policies for Logging \u0026 Monitoring Gardener currently introduces a logging stack based on Loki. So this section is subject to change. Please checkout the Community Meeting for more information.\nThese are the logging and monitoring related network policies:\nNAME POD-SELECTOR allow-from-prometheus networking.gardener.cloud/from-prometheus=allowed allow-grafana component=grafana,gardener.cloud/role=monitoring allow-prometheus app=prometheus,gardener.cloud/role=monitoring,role=monitoring allow-to-aggregate-prometheus networking.gardener.cloud/to-aggregate-prometheus=allowed allow-to-loki networking.gardener.cloud/to-loki=allowed Let’s take for instance a look at the network policy from-prometheus. As part of the shoot reconciliation flow, Gardener deploys a shoot-specific Prometheus into the shoot namespace. Each pod that should be scraped for metrics must be labeled with networking.gardener.cloud/from-prometheus=allowed to allow incoming network requests by the prometheus pod. Most components of the Shoot cluster’s control plane expose metrics and are therefore labeled appropriately.\nImplications for Gardener Extensions Gardener extensions sometimes need to deploy additional components into the Shoot namespace in the Seed hosting the control plane. For example the Gardener extension provider-aws deploys the MachineControllerManager into the Shoot namespace, that is ultimately responsible to create the VMs with the cloud provider AWS.\nEvery Shoot namespace in the Seed contains the network policy deny-all. This requires a pod deployed by a Gardener extension to have labels from network policies, that exist in the Shoot namespace, that allow the required network ranges.\nAdditionally, extensions could also deploy their own network policies. This is used e.g by the Gardener extension provider-aws to serve Admission Webhooks for the Shoot API server that need to be reachable from within the Shoot namespace.\nThe pod can use an arbitrary combination of network policies.\nNetwork policies in the garden namespace The network policies in the garden namespace are, with a few exceptions (e.g Kubernetes control plane specific policies), the same as in the Shoot namespaces. For your reference, these are all the deployed network policies.\nNAME POD-SELECTOR allow-fluentbit app=fluent-bit,gardener.cloud/role=logging,role=logging allow-from-aggregate-prometheus networking.gardener.cloud/from-aggregate-prometheus=allowed allow-to-aggregate-prometheus networking.gardener.cloud/to-aggregate-prometheus=allowed allow-to-all-shoot-apiservers networking.gardener.cloud/to-all-shoot-apiservers=allowed allow-to-blocked-cidrs networking.gardener.cloud/to-blocked-cidrs=allowed allow-to-dns networking.gardener.cloud/to-dns=allowed allow-to-loki networking.gardener.cloud/to-loki=allowed allow-to-private-networks networking.gardener.cloud/to-private-networks=allowed allow-to-public-networks networking.gardener.cloud/to-public-networks=allowed allow-to-seed-apiserver networking.gardener.cloud/to-seed-apiserver=allowed deny-all networking.gardener.cloud/to-all=disallowed This section describes the network policies that are unique to the garden namespace.\nThe network policy allow-to-all-shoot-apiservers allows pods to access every Shoot API server in the Seed. This is for instance used by the dependency watchdog to regularly check the health of all the Shoot API servers.\nGardener deploys a central Prometheus instance in the garden namespace that fetches metrics and data from all seed cluster nodes and all seed cluster pods. The network policies allow-to-aggregate-prometheus and allow-from-aggregate-prometheus allow traffic from and to this prometheus instance.\nWorth mentioning is, that the network policy allow-to-shoot-networks does not exist in the garden namespace. This is to forbid Gardener system components to talk to workload deployed in the Shoot VPC.\n","categories":"","description":"","excerpt":"Network Policies in the Seed Cluster This document describes the …","ref":"/docs/gardener/development/seed_network_policies/","tags":"","title":"Seed Network Policies"},{"body":"Settings for Seeds The Seed resource offers a few settings that are used to control the behaviour of certain Gardener components. This document provides an overview over the available settings:\nDependency Watchdog Gardenlet can deploy two instances of the dependency-watchdog into the garden namespace of the seed cluster. One instance only activates the endpoint controller while the second instance only activates the probe controller.\nEndpoint Controller The endpoint controller helps to alleviate the delay where control plane components remain unavailable by finding the respective pods in CrashLoopBackoff status and restarting them once their dependants become ready and available again. For example, if etcd goes down then also kube-apiserver goes down (and into a CrashLoopBackoff state). If etcd comes up again then (without the endpoint controller) it might take some time until kube-apiserver gets restarted as well.\nIt can be enabled/disabled via the .spec.settings.dependencyWatchdog.endpoint.enabled field. It defaults to true.\nProbe Controller The probe controller scales down the kube-controller-manager of shoot clusters in case their respective kube-apiserver is not reachable via its external ingress. This is in order to avoid melt-down situations since the kube-controller-manager uses in-cluster communication when talking to the kube-apiserver, i.e., it wouldn’t be affected if the external access to the kube-apiserver is interrupted for whatever reason. The kubelets on the shoot worker nodes, however, would indeed be affected since they typically run in different networks and use the external ingress when talking to the kube-apiserver. Hence, without scaling down kube-controller-manager, the nodes might be marked as NotReady and eventually replaced (since the kubelets cannot report their status anymore). To prevent such unnecessary turbulences, kube-controller-manager is being scaled down until the external ingress becomes available again.\nIt can be enabled/disabled via the .spec.settings.dependencyWatchdog.probe.enabled field. It defaults to true.\nReserve Excess Capacity If the excess capacity reservation is enabled then the Gardenlet will deploy a special Deployment into the garden namespace of the seed cluster. This Deployment’s pod template has only one container, the pause container, which simply runs in an infinite loop. The priority of the deployment is very low, so any other pod will preempt these pause pods. This is especially useful if new shoot control planes are created in the seed. In case the seed cluster runs at its capacity then there is no waiting time required during the scale-up. Instead, the low-priority pause pods will be preempted and allow newly created shoot control plane pods to be scheduled fast. In the meantime, the cluster-autoscaler will trigger the scale-up because the preempted pause pods want to run again. However, this delay doesn’t affect the important shoot control plane pods which will improve the user experience.\nIt can be enabled/disabled via the .spec.settings.excessCapacityReservation.enabled field. It defaults to true.\nScheduling By default, the Gardener Scheduler will consider all seed clusters when a new shoot cluster shall be created. However, administrators/operators might want to exclude some of them from being considered by the scheduler. Therefore, seed clusters can be marked as “invisible”. In this case, the scheduler simply ignores them as if they wouldn’t exist. Shoots can still use the invisible seed but only by explicitly specifying the name in their .spec.seedName field.\nSeed clusters can be marked visible/invisible via the .spec.settings.scheduling.visible field. It defaults to true.\nShoot DNS Generally, the Gardenlet creates a few DNS records during the creation/reconciliation of a shoot cluster (see here). However, some infrastructures don’t need/want this behaviour. Instead, they want to directly use the IP addresses/hostnames of load balancers. Another use-case is a local development setup where DNS is not needed for simplicity reasons.\nBy setting the .spec.settings.shootDNS.enabled field this behavior can be controlled.\nℹ️ In previous Gardener versions (\u003c 1.5) these settings were controlled via taint keys (seed.gardener.cloud/{disable-capacity-reservation,disable-dns,invisible}). The taint keys are no longer supported and removed in version 1.12. The rationale behind it is the implementation of tolerations similar to Kubernetes tolerations. More information about it can be found in #2193.\nLoad Balancer Services Gardener creates certain Kubernetes Service objects of type LoadBalancer in the seed cluster. Most prominently, they are used for exposing the shoot control planes, namely the kube-apiserver of the shoot clusters. In most cases, the cloud-controller-manager (responsible for managing these load balancers on the respective underlying infrastructure) supports certain customization and settings via annotations. This document provides a good overview and many examples.\nBy setting the .spec.settings.loadBalancerServices.annotations field the Gardener administrator can specify a list of annotations which will be injected into the Services of type LoadBalancer.\nVertical Pod Autoscaler Gardener heavily relies on the Kubernetes vertical-pod-autoscaler component. By default, the seed controller deploys the VPA components into the garden namespace of the respective seed clusters. In case you want to manage the VPA deployment on your own or have a custom one then you might want to disable the automatic deployment of Gardener. Otherwise, you might end up with two VPAs which will cause erratic behaviour. By setting the .spec.settings.verticalPodAutoscaler.enabled=false you can disable the automatic deployment.\n⚠️ In any case, there must be a VPA available for your seed cluster. Using a seed without VPA is not supported.\nOwner Checks When a shoot is scheduled to a seed and actually reconciled, Gardener appoints the seed as the current “owner” of the shoot by creating a special “owner DNS record” and checking against it if the seed still owns the shoot in order to guard against “split brain scenario” during control plane migration, as described in GEP-17 Shoot Control Plane Migration “Bad Case” Scenario. This mechanism relies on the DNS resolution of TXT DNS records being possible and highly reliable, since if the owner check fails the shoot will be effectively disabled for the duration of the failure. In environments where resolving TXT DNS records is either not possible or not considered reliable enough, it may be necessary to disable the owner check mechanism, in order to avoid shoots failing to reconcile or temporary outages due to transient DNS failures. By setting the .spec.settings.ownerChecks.enabled=false (default is true) the creation and checking of owner DNS records can be disabled for all shoots scheduled on this seed. Note that if owner checks are disabled, migrating shoots scheduled on this seed to other seeds should be considered unsafe, and in the future will be disabled as well.\n","categories":"","description":"","excerpt":"Settings for Seeds The Seed resource offers a few settings that are …","ref":"/docs/gardener/usage/seed_settings/","tags":"","title":"Seed Settings"},{"body":"Packages:\n  seedmanagement.gardener.cloud/v1alpha1   seedmanagement.gardener.cloud/v1alpha1  Package v1alpha1 is a version of the API.\nResource Types:  ManagedSeed  ManagedSeedSet  ManagedSeed   ManagedSeed represents a Shoot that is registered as Seed.\n   Field Description      apiVersion string   seedmanagement.gardener.cloud/v1alpha1      kind string  ManagedSeed    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ManagedSeedSpec     (Optional) Specification of the ManagedSeed.\n     shoot  Shoot     (Optional) Shoot references a Shoot that should be registered as Seed. This field is immutable.\n    seedTemplate  github.com/gardener/gardener/pkg/apis/core/v1beta1.SeedTemplate     (Optional) SeedTemplate is a template for a Seed object, that should be used to register a given cluster as a Seed. Either SeedTemplate or Gardenlet must be specified. When Seed is specified, the ManagedSeed controller will not deploy a gardenlet into the cluster and an existing gardenlet reconciling the new Seed is required.\n    gardenlet  Gardenlet     (Optional) Gardenlet specifies that the ManagedSeed controller should deploy a gardenlet into the cluster with the given deployment parameters and GardenletConfiguration.\n       status  ManagedSeedStatus     (Optional) Most recently observed status of the ManagedSeed.\n    ManagedSeedSet   ManagedSeedSet represents a set of identical ManagedSeeds.\n   Field Description      apiVersion string   seedmanagement.gardener.cloud/v1alpha1      kind string  ManagedSeedSet    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ManagedSeedSetSpec     (Optional) Spec defines the desired identities of ManagedSeeds and Shoots in this set.\n     replicas  int32    (Optional) Replicas is the desired number of replicas of the given Template. Defaults to 1.\n    selector  Kubernetes meta/v1.LabelSelector     Selector is a label query over ManagedSeeds and Shoots that should match the replica count. It must match the ManagedSeeds and Shoots template’s labels. This field is immutable.\n    template  ManagedSeedTemplate     Template describes the ManagedSeed that will be created if insufficient replicas are detected. Each ManagedSeed created / updated by the ManagedSeedSet will fulfill this template.\n    shootTemplate  github.com/gardener/gardener/pkg/apis/core/v1beta1.ShootTemplate     ShootTemplate describes the Shoot that will be created if insufficient replicas are detected for hosting the corresponding ManagedSeed. Each Shoot created / updated by the ManagedSeedSet will fulfill this template.\n    updateStrategy  UpdateStrategy     (Optional) UpdateStrategy specifies the UpdateStrategy that will be employed to update ManagedSeeds / Shoots in the ManagedSeedSet when a revision is made to Template / ShootTemplate.\n    revisionHistoryLimit  int32    (Optional) RevisionHistoryLimit is the maximum number of revisions that will be maintained in the ManagedSeedSet’s revision history. Defaults to 10. This field is immutable.\n       status  ManagedSeedSetStatus     (Optional) Status is the current status of ManagedSeeds and Shoots in this ManagedSeedSet.\n    Bootstrap (string alias)\n  (Appears on: Gardenlet)  Bootstrap describes a mechanism for bootstrapping gardenlet connection to the Garden cluster.\nGardenlet   (Appears on: ManagedSeedSpec)  Gardenlet specifies gardenlet deployment parameters and the GardenletConfiguration used to configure gardenlet.\n   Field Description      deployment  GardenletDeployment     (Optional) Deployment specifies certain gardenlet deployment parameters, such as the number of replicas, the image, etc.\n    config  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) Config is the GardenletConfiguration used to configure gardenlet.\n    bootstrap  Bootstrap     (Optional) Bootstrap is the mechanism that should be used for bootstrapping gardenlet connection to the Garden cluster. One of ServiceAccount, BootstrapToken, None. If set to ServiceAccount or BootstrapToken, a service account or a bootstrap token will be created in the garden cluster and used to compute the bootstrap kubeconfig. If set to None, the gardenClientConnection.kubeconfig field will be used to connect to the Garden cluster. Defaults to BootstrapToken. This field is immutable.\n    mergeWithParent  bool    (Optional) MergeWithParent specifies whether the GardenletConfiguration of the parent gardenlet should be merged with the specified GardenletConfiguration. Defaults to true. This field is immutable.\n    GardenletDeployment   (Appears on: Gardenlet)  GardenletDeployment specifies certain gardenlet deployment parameters, such as the number of replicas, the image, etc.\n   Field Description      replicaCount  int32    (Optional) ReplicaCount is the number of gardenlet replicas. Defaults to 1.\n    revisionHistoryLimit  int32    (Optional) RevisionHistoryLimit is the number of old gardenlet ReplicaSets to retain to allow rollback. Defaults to 10.\n    serviceAccountName  string    (Optional) ServiceAccountName is the name of the ServiceAccount to use to run gardenlet pods.\n    image  Image     (Optional) Image is the gardenlet container image.\n    resources  Kubernetes core/v1.ResourceRequirements     (Optional) Resources are the compute resources required by the gardenlet container.\n    podLabels  map[string]string    (Optional) PodLabels are the labels on gardenlet pods.\n    podAnnotations  map[string]string    (Optional) PodAnnotations are the annotations on gardenlet pods.\n    additionalVolumes  []Kubernetes core/v1.Volume     (Optional) AdditionalVolumes is the list of additional volumes that should be mounted by gardenlet containers.\n    additionalVolumeMounts  []Kubernetes core/v1.VolumeMount     (Optional) AdditionalVolumeMounts is the list of additional pod volumes to mount into the gardenlet container’s filesystem.\n    env  []Kubernetes core/v1.EnvVar     (Optional) Env is the list of environment variables to set in the gardenlet container.\n    vpa  bool    (Optional) VPA specifies whether to enable VPA for gardenlet. Defaults to true.\n    failureToleranceType  github.com/gardener/gardener/pkg/apis/core/v1beta1.FailureToleranceType     (Optional) FailureToleranceType determines how gardenlet replicas are spread across the failure domains, possible values are either node or zone. Please make sure to adjust the replicaCount accordingly if you intend to run an HA setup for gardenlet.\n    Image   (Appears on: GardenletDeployment)  Image specifies container image parameters.\n   Field Description      repository  string    (Optional) Repository is the image repository.\n    tag  string    (Optional) Tag is the image tag.\n    pullPolicy  Kubernetes core/v1.PullPolicy     (Optional) PullPolicy is the image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if latest tag is specified, or IfNotPresent otherwise.\n    ManagedSeedSetSpec   (Appears on: ManagedSeedSet)  ManagedSeedSetSpec is the specification of a ManagedSeedSet.\n   Field Description      replicas  int32    (Optional) Replicas is the desired number of replicas of the given Template. Defaults to 1.\n    selector  Kubernetes meta/v1.LabelSelector     Selector is a label query over ManagedSeeds and Shoots that should match the replica count. It must match the ManagedSeeds and Shoots template’s labels. This field is immutable.\n    template  ManagedSeedTemplate     Template describes the ManagedSeed that will be created if insufficient replicas are detected. Each ManagedSeed created / updated by the ManagedSeedSet will fulfill this template.\n    shootTemplate  github.com/gardener/gardener/pkg/apis/core/v1beta1.ShootTemplate     ShootTemplate describes the Shoot that will be created if insufficient replicas are detected for hosting the corresponding ManagedSeed. Each Shoot created / updated by the ManagedSeedSet will fulfill this template.\n    updateStrategy  UpdateStrategy     (Optional) UpdateStrategy specifies the UpdateStrategy that will be employed to update ManagedSeeds / Shoots in the ManagedSeedSet when a revision is made to Template / ShootTemplate.\n    revisionHistoryLimit  int32    (Optional) RevisionHistoryLimit is the maximum number of revisions that will be maintained in the ManagedSeedSet’s revision history. Defaults to 10. This field is immutable.\n    ManagedSeedSetStatus   (Appears on: ManagedSeedSet)  ManagedSeedSetStatus represents the current state of a ManagedSeedSet.\n   Field Description      observedGeneration  int64    ObservedGeneration is the most recent generation observed for this ManagedSeedSet. It corresponds to the ManagedSeedSet’s generation, which is updated on mutation by the API Server.\n    replicas  int32    Replicas is the number of replicas (ManagedSeeds and their corresponding Shoots) created by the ManagedSeedSet controller.\n    readyReplicas  int32    ReadyReplicas is the number of ManagedSeeds created by the ManagedSeedSet controller that have a Ready Condition.\n    nextReplicaNumber  int32    NextReplicaNumber is the ordinal number that will be assigned to the next replica of the ManagedSeedSet.\n    currentReplicas  int32    CurrentReplicas is the number of ManagedSeeds created by the ManagedSeedSet controller from the ManagedSeedSet version indicated by CurrentRevision.\n    updatedReplicas  int32    UpdatedReplicas is the number of ManagedSeeds created by the ManagedSeedSet controller from the ManagedSeedSet version indicated by UpdateRevision.\n    currentRevision  string    CurrentRevision, if not empty, indicates the version of the ManagedSeedSet used to generate ManagedSeeds with smaller ordinal numbers during updates.\n    updateRevision  string    UpdateRevision, if not empty, indicates the version of the ManagedSeedSet used to generate ManagedSeeds with larger ordinal numbers during updates\n    collisionCount  int32    (Optional) CollisionCount is the count of hash collisions for the ManagedSeedSet. The ManagedSeedSet controller uses this field as a collision avoidance mechanism when it needs to create the name for the newest ControllerRevision.\n    conditions  []github.com/gardener/gardener/pkg/apis/core/v1beta1.Condition     (Optional) Conditions represents the latest available observations of a ManagedSeedSet’s current state.\n    pendingReplica  PendingReplica     (Optional) PendingReplica, if not empty, indicates the replica that is currently pending creation, update, or deletion. This replica is in a state that requires the controller to wait for it to change before advancing to the next replica.\n    ManagedSeedSpec   (Appears on: ManagedSeed, ManagedSeedTemplate)  ManagedSeedSpec is the specification of a ManagedSeed.\n   Field Description      shoot  Shoot     (Optional) Shoot references a Shoot that should be registered as Seed. This field is immutable.\n    seedTemplate  github.com/gardener/gardener/pkg/apis/core/v1beta1.SeedTemplate     (Optional) SeedTemplate is a template for a Seed object, that should be used to register a given cluster as a Seed. Either SeedTemplate or Gardenlet must be specified. When Seed is specified, the ManagedSeed controller will not deploy a gardenlet into the cluster and an existing gardenlet reconciling the new Seed is required.\n    gardenlet  Gardenlet     (Optional) Gardenlet specifies that the ManagedSeed controller should deploy a gardenlet into the cluster with the given deployment parameters and GardenletConfiguration.\n    ManagedSeedStatus   (Appears on: ManagedSeed)  ManagedSeedStatus is the status of a ManagedSeed.\n   Field Description      conditions  []github.com/gardener/gardener/pkg/apis/core/v1beta1.Condition     (Optional) Conditions represents the latest available observations of a ManagedSeed’s current state.\n    observedGeneration  int64    ObservedGeneration is the most recent generation observed for this ManagedSeed. It corresponds to the ManagedSeed’s generation, which is updated on mutation by the API Server.\n    ManagedSeedTemplate   (Appears on: ManagedSeedSetSpec)  ManagedSeedTemplate is a template for creating a ManagedSeed object.\n   Field Description      metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ManagedSeedSpec     (Optional) Specification of the desired behavior of the ManagedSeed.\n     shoot  Shoot     (Optional) Shoot references a Shoot that should be registered as Seed. This field is immutable.\n    seedTemplate  github.com/gardener/gardener/pkg/apis/core/v1beta1.SeedTemplate     (Optional) SeedTemplate is a template for a Seed object, that should be used to register a given cluster as a Seed. Either SeedTemplate or Gardenlet must be specified. When Seed is specified, the ManagedSeed controller will not deploy a gardenlet into the cluster and an existing gardenlet reconciling the new Seed is required.\n    gardenlet  Gardenlet     (Optional) Gardenlet specifies that the ManagedSeed controller should deploy a gardenlet into the cluster with the given deployment parameters and GardenletConfiguration.\n       PendingReplica   (Appears on: ManagedSeedSetStatus)  PendingReplica contains information about a replica that is currently pending creation, update, or deletion.\n   Field Description      name  string    Name is the replica name.\n    reason  PendingReplicaReason     Reason is the reason for the replica to be pending.\n    since  Kubernetes meta/v1.Time     Since is the moment in time since the replica is pending with the specified reason.\n    retries  int32    (Optional) Retries is the number of times the shoot operation (reconcile or delete) has been retried after having failed. Only applicable if Reason is ShootReconciling or ShootDeleting.\n    PendingReplicaReason (string alias)\n  (Appears on: PendingReplica)  PendingReplicaReason is a string enumeration type that enumerates all possible reasons for a replica to be pending.\nRollingUpdateStrategy   (Appears on: UpdateStrategy)  RollingUpdateStrategy is used to communicate parameters for RollingUpdateStrategyType.\n   Field Description      partition  int32    (Optional) Partition indicates the ordinal at which the ManagedSeedSet should be partitioned. Defaults to 0.\n    Shoot   (Appears on: ManagedSeedSpec)  Shoot identifies the Shoot that should be registered as Seed.\n   Field Description      name  string    Name is the name of the Shoot that will be registered as Seed.\n    UpdateStrategy   (Appears on: ManagedSeedSetSpec)  UpdateStrategy specifies the strategy that the ManagedSeedSet controller will use to perform updates. It includes any additional parameters necessary to perform the update for the indicated strategy.\n   Field Description      type  UpdateStrategyType     (Optional) Type indicates the type of the UpdateStrategy. Defaults to RollingUpdate.\n    rollingUpdate  RollingUpdateStrategy     (Optional) RollingUpdate is used to communicate parameters when Type is RollingUpdateStrategyType.\n    UpdateStrategyType (string alias)\n  (Appears on: UpdateStrategy)  UpdateStrategyType is a string enumeration type that enumerates all possible update strategies for the ManagedSeedSet controller.\n  Generated with gen-crd-api-reference-docs \n","categories":"","description":"","excerpt":"Packages:\n  seedmanagement.gardener.cloud/v1alpha1 …","ref":"/docs/gardener/api-reference/seedmanagement/","tags":"","title":"Seedmanagement"},{"body":"The Seed Cluster The landscape-setup-template is meant to provide an as-simple-as-possible Gardener installation. Therefore it just registers the cluster where the Gardener is deployed on as a seed cluster. While this is easy, it might be insecure. Clusters created with Kubify don’t have network policies, for example. See Hardening the Gardener Community Setup for more information.\nTo have network policies on the seed cluster and avoid having the seed on the same cluster as the Gardener, the easiest option is probably to simply create a shoot and then register that shoot as seed. This way you can also leverage other advantages of shooted clusters for your seed, e.g. autoscaling.\nSetting up the Shoot The first step is to create a shoot cluster. Unfortunately, the Gardener dashboard currently does not allow to change the CIDRs for the created shoot clusters, and your shoots won’t work if they have overlapping CIDR ranges with their corresponding seed cluster. So either your seed cluster is deployed with different CIDRs - not using the dashboard, but kubectl apply and a yaml file - or all of your shoots on that seed need to be created this way. In order to be able to use the dashboard for the shoots, it makes sense to create the seed with different CIDRs.\nSo, create yourself a shoot with modified CIDRs. You can find templates for the shoot manifest here. You could, for example, change the CIDRs to this:\n ...  networks:  internal:  - 10.254.112.0/22  nodes: 10.254.0.0/19  pods: 10.255.0.0/17  public:  - 10.254.96.0/22  services: 10.255.128.0/17  vpc:  cidr: 10.254.0.0/16  workers:  - 10.254.0.0/19  ... Also make sure that your new seed cluster has enough resources for the expected number of shoots.\nRegistering the Shoot as Seed The seed itself is a Kubernetes resource that can be deployed via a yaml file, but it has some dependencies. You can find templated versions of these files in the seed-config component of the landscape-setup-template project. If you have set up your Gardener using this project, there should also be rendered versions of these files in the state/seed-config/ directory of your landscape folder (they are probably easier to work with). Examples for all these files can also be found in the aforementioned example folder in the Gardener repo.\n1. Seed Namespace First, you should create a namespace for your new seed and everything that belongs to it. This is not necessary, but it will keep your cluster organized. For this example, the namespace will be called seed-test.\n2. Cloud Provider Secret The Gardener needs to create resources on the seed and thus needs a kubeconfig for it. It is provided with the cloud provider secret (below is an example for AWS).\napiVersion: v1 kind: Secret metadata:  name: test-seed-secret  namespace: seed-test  labels:  cloudprofile.garden.sapcloud.io/name: aws type: Opaque data:  accessKeyID: \u003cbase64-encoded AWS access key\u003e  secretAccessKey: \u003cbase64-encoded AWS secret key\u003e  kubeconfig: \u003cbase64-encoded kubeconfig\u003e Deploy the secret into your seed namespace. Apart from the kubeconfig, also infrastructure credentials are required. They will only be used for the etcd backup, so in case for AWS, S3 privileges should be sufficient.\n3. Secretbinding for Cloud Provider Secret Create a secretbinding for your cloud provider secret:\napiVersion: core.gardener.cloud/v1beta1 kind: SecretBinding metadata:  name: test-seed-secret  namespace: seed-test  labels:  cloudprofile.garden.sapcloud.io/name: aws secretRef:  name: test-seed-secret # namespace: only required if in different namespace than referenced secret quotas: [] You can give it the same name as the referenced secret.\n4. Cloudprofile The cloudprofile contains the information which shoots can be created with this seed. You could create a new cloudprofile, but you can also just reference the existing cloudprofile if you don’t want to change anything.\n5. Seed Now the seed resource can be created. Choose a name, reference cloudprofile and secretbinding, fill in your ingress domain, and set the CIDRs to the same values as in the underlying shoot cluster.\napiVersion: core.gardener.cloud/v1beta1 kind: Seed metadata:  name: aws-secure spec:  provider:  type: aws  region: eu-west-1  secretRef:  name: test-seed-secret  namespace: seed-test  dns:  ingressDomain: ingress.\u003cyour cluster domain\u003e  networks:  nodes: 10.254.0.0/19  pods: 10.255.0.0/17  services: 10.255.128.0/17 6. Hide Original Seed In the dashboard, it is not possible to select the seed for a shoot (it is possible when deploying the shoot using a yaml file, however). Since both seeds probably reference the same cloudprofile, the Gardener will try to distribute the shoots equally among both seeds.\nTo solve this problem, edit the original seed and set its spec.visible field to false. This will prevent the Gardener from choosing this seed, so now all shoots created via the dashboard should have their control plane on the new, more secure seed.\n","categories":"","description":"How to configure a Kubernetes cluster as a Gardener seed","excerpt":"How to configure a Kubernetes cluster as a Gardener seed","ref":"/docs/guides/install_gardener/setup-seed/","tags":"","title":"Setting Up a Seed Cluster"},{"body":"Packages:\n  settings.gardener.cloud/v1alpha1   settings.gardener.cloud/v1alpha1  Package v1alpha1 is a version of the API.\nResource Types:  ClusterOpenIDConnectPreset  OpenIDConnectPreset  ClusterOpenIDConnectPreset   ClusterOpenIDConnectPreset is a OpenID Connect configuration that is applied to a Shoot objects cluster-wide.\n   Field Description      apiVersion string   settings.gardener.cloud/v1alpha1      kind string  ClusterOpenIDConnectPreset    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ClusterOpenIDConnectPresetSpec     Spec is the specification of this OpenIDConnect preset.\n     OpenIDConnectPresetSpec  OpenIDConnectPresetSpec      (Members of OpenIDConnectPresetSpec are embedded into this type.)     projectSelector  Kubernetes meta/v1.LabelSelector     (Optional) Project decides whether to apply the configuration if the Shoot is in a specific Project matching the label selector. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Defaults to the empty LabelSelector, which matches everything.\n       OpenIDConnectPreset   OpenIDConnectPreset is a OpenID Connect configuration that is applied to a Shoot in a namespace.\n   Field Description      apiVersion string   settings.gardener.cloud/v1alpha1      kind string  OpenIDConnectPreset    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  OpenIDConnectPresetSpec     Spec is the specification of this OpenIDConnect preset.\n     server  KubeAPIServerOpenIDConnect     Server contains the kube-apiserver’s OpenID Connect configuration. This configuration is not overwritting any existing OpenID Connect configuration already set on the Shoot object.\n    client  OpenIDConnectClientAuthentication     (Optional) Client contains the configuration used for client OIDC authentication of Shoot clusters. This configuration is not overwritting any existing OpenID Connect client authentication already set on the Shoot object.\n    shootSelector  Kubernetes meta/v1.LabelSelector     (Optional) ShootSelector decides whether to apply the configuration if the Shoot has matching labels. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Default to the empty LabelSelector, which matches everything.\n    weight  int32    Weight associated with matching the corresponding preset, in the range 1-100. Required.\n       ClusterOpenIDConnectPresetSpec   (Appears on: ClusterOpenIDConnectPreset)  ClusterOpenIDConnectPresetSpec contains the OpenIDConnect specification and project selector matching Shoots in Projects.\n   Field Description      OpenIDConnectPresetSpec  OpenIDConnectPresetSpec      (Members of OpenIDConnectPresetSpec are embedded into this type.)     projectSelector  Kubernetes meta/v1.LabelSelector     (Optional) Project decides whether to apply the configuration if the Shoot is in a specific Project matching the label selector. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Defaults to the empty LabelSelector, which matches everything.\n    KubeAPIServerOpenIDConnect   (Appears on: OpenIDConnectPresetSpec)  KubeAPIServerOpenIDConnect contains configuration settings for the OIDC provider. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      caBundle  string    (Optional) If set, the OpenID server’s certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host’s root CA set will be used.\n    clientID  string    The client ID for the OpenID Connect client. Required.\n    groupsClaim  string    (Optional) If provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This field is experimental, please see the authentication documentation for further details.\n    groupsPrefix  string    (Optional) If provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies.\n    issuerURL  string    The URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT). Required.\n    requiredClaims  map[string]string    (Optional) key=value pairs that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value.\n    signingAlgs  []string    (Optional) List of allowed JOSE asymmetric signing algorithms. JWTs with a ‘alg’ header value not in this list will be rejected. Values are defined by RFC 7518 https://tools.ietf.org/html/rfc7518#section-3.1 Defaults to [RS256]\n    usernameClaim  string    (Optional) The OpenID claim to use as the user name. Note that claims other than the default (‘sub’) is not guaranteed to be unique and immutable. This field is experimental, please see the authentication documentation for further details. Defaults to “sub”.\n    usernamePrefix  string    (Optional) If provided, all usernames will be prefixed with this value. If not provided, username claims other than ‘email’ are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value ‘-’.\n    OpenIDConnectClientAuthentication   (Appears on: OpenIDConnectPresetSpec)  OpenIDConnectClientAuthentication contains configuration for OIDC clients.\n   Field Description      secret  string    (Optional) The client Secret for the OpenID Connect client.\n    extraConfig  map[string]string    (Optional) Extra configuration added to kubeconfig’s auth-provider. Must not be any of idp-issuer-url, client-id, client-secret, idp-certificate-authority, idp-certificate-authority-data, id-token or refresh-token\n    OpenIDConnectPresetSpec   (Appears on: OpenIDConnectPreset, ClusterOpenIDConnectPresetSpec)  OpenIDConnectPresetSpec contains the Shoot selector for which a specific OpenID Connect configuration is applied.\n   Field Description      server  KubeAPIServerOpenIDConnect     Server contains the kube-apiserver’s OpenID Connect configuration. This configuration is not overwritting any existing OpenID Connect configuration already set on the Shoot object.\n    client  OpenIDConnectClientAuthentication     (Optional) Client contains the configuration used for client OIDC authentication of Shoot clusters. This configuration is not overwritting any existing OpenID Connect client authentication already set on the Shoot object.\n    shootSelector  Kubernetes meta/v1.LabelSelector     (Optional) ShootSelector decides whether to apply the configuration if the Shoot has matching labels. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Default to the empty LabelSelector, which matches everything.\n    weight  int32    Weight associated with matching the corresponding preset, in the range 1-100. Required.\n      Generated with gen-crd-api-reference-docs \n","categories":"","description":"","excerpt":"Packages:\n  settings.gardener.cloud/v1alpha1 …","ref":"/docs/gardener/api-reference/settings/","tags":"","title":"Settings"},{"body":"Gardener Certificate Management Introduction Gardener comes with an extension that enables shoot owners to request X.509 compliant certificates for shoot domains.\nExtension Installation The Shoot-Cert-Service extension can be deployed and configured via Gardener’s native resource ControllerRegistration.\nPrerequisites To let the Shoot-Cert-Service operate properly, you need to have:\n a DNS service in your seed contact details and optionally a private key for a pre-existing Let’s Encrypt account  ControllerRegistration An example of a ControllerRegistration for the Shoot-Cert-Service can be found here: https://github.com/gardener/gardener-extension-shoot-cert-service/blob/master/example/controller-registration.yaml\nThe ControllerRegistration contains a Helm chart which eventually deploy the Shoot-Cert-Service to seed clusters. It offers some configuration options, mainly to set up a default issuer for shoot clusters. With a default issuer, pre-existing Let’s Encrypt accounts can be used and shared with shoot clusters (See “One Account or Many?” of the Integration Guide).\n Please keep the Let’s Encrypt Rate Limits in mind when using this shared account model. Depending on the amount of shoots and domains it is recommended to use an account with increased rate limits.\n apiVersion: core.gardener.cloud/v1beta1 kind: ControllerRegistration ...  values:  certificateConfig:  defaultIssuer:  acme:  email: foo@example.com  privateKey: |------BEGIN RSA PRIVATE KEY----- ... -----END RSA PRIVATE KEY----- server: https://acme-v02.api.letsencrypt.org/directory  name: default-issuer # restricted: true # restrict default issuer to any sub-domain of shoot.spec.dns.domain  # defaultRequestsPerDayQuota: 50  # precheckNameservers: 8.8.8.8,8.8.4.4  # caCertificates: | # optional custom CA certificates when using private ACME provider # -----BEGIN CERTIFICATE----- # ... # -----END CERTIFICATE----- # # -----BEGIN CERTIFICATE----- # ... # -----END CERTIFICATE-----   shootIssuers:  enabled: false # if true, allows to specify issuers in the shoot clusters Enablement If the Shoot-Cert-Service should be enabled for every shoot cluster in your Gardener managed environment, you need to globally enable it in the ControllerRegistration:\napiVersion: core.gardener.cloud/v1beta1 kind: ControllerRegistration ...  resources:  - globallyEnabled: true  kind: Extension  type: shoot-cert-service Alternatively, you’re given the option to only enable the service for certain shoots:\nkind: Shoot apiVersion: core.gardener.cloud/v1beta1 ... spec:  extensions:  - type: shoot-cert-service ... ","categories":"","description":"","excerpt":"Gardener Certificate Management Introduction Gardener comes with an …","ref":"/docs/extensions/others/gardener-extension-shoot-cert-service/docs/installation/setup/","tags":"","title":"Setup"},{"body":"Gardener DNS Management for Shoots Introduction Gardener allows Shoot clusters to request DNS names for Ingresses and Services out of the box. To support this the gardener must be installed with the shoot-dns-service extension. This extension uses the seed’s dns management infrastructure to maintain DNS names for shoot clusters. So, far only the external DNS domain of a shoot (already used for the kubernetes api server and ingress DNS names) can be used for managed DNS names.\nConfiguration To generally enable the DNS management for shoot objects the shoot-dns-service extension must be registered by providing an appropriate extension registration in the garden cluster.\nHere it is possible to decide whether the extension should be always available for all shoots or whether the extension must be separately enabled per shoot.\nIf the extension should be used for all shoots, the registration must set the globallyEnabled flag to true.\nspec:  resources:  - kind: Extension  type: shoot-dns-service  globallyEnabled: true Deployment of DNS controller manager If you are using Gardener version \u003e= 1.54, please make sure to deploy the DNS controller manager by adding the dnsControllerManager section to the providerConfig.values section.\nFor example:\napiVersion: core.gardener.cloud/v1beta1 kind: ControllerDeployment metadata:  name: extension-shoot-dns-service type: helm providerConfig:  chart: ...  values:  image:  ...  dnsControllerManager:  configuration:  cacheTtl: 300  controllers: dnscontrollers,dnssources  dnsPoolResyncPeriod: 30m  #poolSize: 20  #providersPoolResyncPeriod: 24h  serverPortHttp: 8080  createCRDs: false  deploy: true  replicaCount: 1  #resources:  # limits:  # memory: 1Gi  # requests:  # cpu: 50m  # memory: 500Mi Providing Base Domains usable for a Shoot So, far only the external DNS domain of a shoot already used for the kubernetes api server and ingress DNS names can be used for managed DNS names. This is either the shoot domain as subdomain of the default domain configured for the gardener installation, or a dedicated domain with dedicated access credentials configured for a dedicated shoot via the shoot manifest.\nAlternatively, you can specify DNSProviders and its credentials Secret directly in the shoot, if this feature is enabled. By default, DNSProvider replication is disabled, but it can be enabled globally in the ControllerDeployment or for a shoot cluster in the shoot manifest (details see further below).\napiVersion: core.gardener.cloud/v1beta1 kind: ControllerDeployment metadata:  name: extension-shoot-dns-service type: helm providerConfig:  chart: ...  values:  image:  ...  dnsProviderReplication:  enabled: true See example files (20-* and 30-*) for details for the various provider types.\nShoot Feature Gate If the shoot DNS feature is not globally enabled by default (depends on the extension registration on the garden cluster), it must be enabled per shoot.\nTo enable the feature for a shoot, the shoot manifest must explicitly add the shoot-dns-service extension.\n... spec:  extensions:  - type: shoot-dns-service ... Enable/disable DNS provider replication for a shoot The DNSProvider` replication feature enablement can be overwritten in the shoot manifest, e.g.\nKind: Shoot ... spec:  extensions:  - type: shoot-dns-service  providerConfig:  apiVersion: service.dns.extensions.gardener.cloud/v1alpha1  kind: DNSConfig  dnsProviderReplication:  enabled: true ... DNSActivation for DNSOwner To support migration of the control plane of shoots, the DNSOwner created and used for all DNSEntries created by the Shoot-DNS-Service can optionally be activated and deactivated by a DNS record. If the DNSActivation feature is enabled, the DNSOwner will be only be active if the value of the owner DNS record managed by Gardener matches the cluster identity of the seed hosting the control plane. This feature must only be enabled if the Gardener feature gate UseDNSRecords is enabled for all seeds.\nBy default this feature is enabled and can be disabled in the controller deployment:\napiVersion: core.gardener.cloud/v1beta1 kind: ControllerDeployment metadata:  name: extension-shoot-dns-service type: helm providerConfig:  chart: ...  values:  image:  ...  ownerDnsActivation:  enabled: false ","categories":"","description":"","excerpt":"Gardener DNS Management for Shoots Introduction Gardener allows Shoot …","ref":"/docs/extensions/others/gardener-extension-shoot-dns-service/docs/installation/setup/","tags":"","title":"Setup"},{"body":"Deploying the Gardener into a Kubernetes cluster Similar to Kubernetes, Gardener consists out of control plane components (Gardener API server, Gardener controller manager, Gardener scheduler), and an agent component (Gardenlet). The control plane is deployed in the so-called garden cluster while the agent is installed into every seed cluster. Please note that it is possible to use the garden cluster as seed cluster by simply deploying the Gardenlet into it.\nWe are providing Helm charts in order to manage the various resources of the components. Please always make sure that you use the Helm chart version that matches the Gardener version you want to deploy.\nDeploying the Gardener control plane (API server, admission controller, controller manager, scheduler) The configuration values depict the various options to configure the different components. Please consult this document for component specific configurations and this document for authentication related specifics.\nAlso note that all resources and deployments need to be created in the garden namespace (not overrideable). If you enable the Gardener admission controller as part of you setup, please make sure the garden namespace is labelled with app: gardener. Otherwise, the backing service account for the admission controller Pod might not be created successfully. No action is necessary, if you deploy the garden namespace with the Gardener control plane Helm chart.\nAfter preparing your values in a separate controlplane-values.yaml file (values.yaml can be used as starting point), you can run the following command against your garden cluster:\nhelm install charts/gardener/controlplane \\  --namespace garden \\  --name gardener-controlplane \\  -f controlplane-values.yaml \\  --wait Deploying Gardener extensions Gardener is an extensible system that does not contain the logic for provider-specific things like DNS management, cloud infrastructures, network plugins, operating system configs, and many more.\nYou have to install extension controllers for these parts. Please consult the documentation regarding extensions to get more information.\nDeploying the Gardener agent (Gardenlet) Please refer to this document on how to deploy a Gardenlet.\n","categories":"","description":"","excerpt":"Deploying the Gardener into a Kubernetes cluster Similar to …","ref":"/docs/gardener/deployment/setup_gardener/","tags":"","title":"Setup Gardener"},{"body":"Accessing Shoot Clusters After creation of a shoot cluster, end-users require a kubeconfig to access it. There are several options available to get to such kubeconfig.\nStatic Token Kubeconfig This kubeconfig contains a static token and provides cluster-admin privileges. It is created by default and persisted in the \u003cshoot-name\u003e.kubeconfig secret in the project namespace in the garden cluster.\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot ... spec:  kubernetes:  enableStaticTokenKubeconfig: true ... It is not the recommended method to access the shoot cluster as the static token kubeconfig has some security flaws associated with it:\n The static token in the kubeconfig doesn’t have any expiration date. Read this document to learn how to rotate the static token. The static token doesn’t have any user identity associated with it. The user in that token will always be system:cluster-admin irrespective of the person accessing the cluster. Hence, it is impossible to audit the events in cluster.  shoots/adminkubeconfig subresource The shoots/adminkubeconfig subresource allows users to dynamically generate temporary kubeconfigs that can be used to access shoot cluster with cluster-admin privileges. The credentials associated with this kubeconfig are client certificates which have a very short validity and must be renewed before they expire (by calling the subresource endpoint again).\nThe username associated with such kubeconfig will be the same which is used for authenticating to the Gardener API. Apart from this advantage, the created kubeconfig will not be persisted anywhere.\nIn order to request such a kubeconfig, you can run the following commands:\nexport NAMESPACE=my-namespace export SHOOT_NAME=my-shoot kubectl create \\  -f \u003cpath\u003e/\u003cto\u003e/kubeconfig-request.json \\  --raw /apis/core.gardener.cloud/v1beta1/namespaces/${NAMESPACE}/shoots/${SHOOT_NAME}/adminkubeconfig | jq -r \".status.kubeconfig\" | base64 -d Here, the kubeconfig-request.json has the following content:\n{  \"apiVersion\": \"authentication.gardener.cloud/v1alpha1\",  \"kind\": \"AdminKubeconfigRequest\",  \"spec\": {  \"expirationSeconds\": 1000  } }  The gardenctl-v2 tool makes it easy to target shoot clusters and automatically renews such kubeconfig when required.\n OpenID Connect The kube-apiserver of shoot clusters can be provided with OpenID Connect configuration via the ShootSpec:\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot ... spec:  kubernetes:  oidcConfig:  ... It is the end-user’s responsibility to incorporate the OpenID Connect configurations in kubeconfig for accessing the cluster (i.e., Gardener will not automatically generate kubeconfig based on these OIDC settings). The recommended way is using the kubectl plugin called kubectl oidc-login for OIDC authentication.\nIf you want to use the same OIDC configuration for all your shoots by default then you can use the ClusterOpenIDConnectPreset and OpenIDConnectPreset API resources. They allow defaulting the .spec.kubernetes.kubeAPIServer.oidcConfig fields for newly created Shoots such that you don’t have to repeat yourself every time (similar to PodPreset resources in Kubernetes). ClusterOpenIDConnectPreset specified OIDC configuration applies to Projects and Shoots cluster-wide (hence, only available to Gardener operators) while OpenIDConnectPreset is Project-scoped. Shoots have to “opt-in” for such defaulting by using the oidc=enable label.\nFor further information on (Cluster)OpenIDConnectPreset, refer to this document.\n","categories":"","description":"","excerpt":"Accessing Shoot Clusters After creation of a shoot cluster, end-users …","ref":"/docs/gardener/usage/shoot_access/","tags":"","title":"Shoot Access"},{"body":"SNI Passthrough proxy for kube-apiservers This GEP tackles the problem that today a single LoadBalancer is needed for every single Shoot cluster’s control plane.\nBackground When the control plane of a Shoot cluster is provisioned, a dedicated LoadBalancer is created for it. It keeps the entire flow quite easy - the apiserver Pods are running and they are accessible via that LoadBalancer. It’s hostnames / IP addresses are used for DNS records like api.\u003cexternal-domain\u003e and api.\u003cshoot\u003e.\u003cproject\u003e.\u003cinternal-domain\u003e. While this solution is simple it comes with several issues.\nMotivation There are several problems with the current setup.\n IaaS provider costs. For example ClassicLoadBalancer on AWS costs at minimum 17 USD / month. Quotas can limit the amount of LoadBalancers you can get per account / project, limiting the number of clusters you can host under a single account. Lack of support for better loadbalancing algorithms than round-robin. Slow cluster provisioning time - depending on the provider a LoadBalancer provisioning could take quite a while. Lower downtime when workload is shuffled in the clusters as the LoadBalancer is Kubernetes-aware.  Goals  Only one LoadBalancer is used for all Shoot cluster API servers running in a Seed cluster. Out-of-cluster (end-user / robot) communication to the API server is still possible. In-cluster communication via the kubernetes master service (IPv4/v6 ClusterIP and the kubernetes.default.svc.cluster.local) is possible. Client TLS authentication works without intermediate TLS termination (TLS is terminated by kube-apiserver). Solution should be cloud-agnostic.  Proposal Seed cluster To solve the problem of having multiple kube-apiservers behind a single LoadBalancer, an intermediate proxy must be placed between the Cloud-Provider’s LoadBalancer and kube-apiservers. This proxy is going to choose the Shoot API Server with the help of Server Name Indication. From wikipedia:\n Server Name Indication (SNI) is an extension to the Transport Layer Security (TLS) computer networking protocol by which a client indicates which hostname it is attempting to connect to at the start of the handshaking process. This allows a server to present multiple certificates on the same IP address and TCP port number and hence allows multiple secure (HTTPS) websites (or any other service over TLS) to be served by the same IP address without requiring all those sites to use the same certificate. It is the conceptual equivalent to HTTP/1.1 name-based virtual hosting, but for HTTPS.\n A rough diagram of the flow of data:\n+-------------------------------+ | | | Network LB | (accessible from clients) | | | | +-------------+-------+---------+ +------------------+  | | | |  | | proxy + lb | Shoot API Server |  | | +-------------+-------------\u003e+ |  | | | | Cluster A |  | | | | |  | | | +------------------+  | | |  +----------------v----+--+  | | |  +-+--------v----------+ | +------------------+  | | | | |  | | | proxy + lb | Shoot API Server |  | Proxy | +-------------+----------\u003e+ |  | | | | Cluster B |  | | | | |  | +----+ +------------------+  +----------------+----+  |  |  | +------------------+  | | |  | proxy + lb | Shoot API Server |  +-------------------+--------------\u003e+ |  | Cluster C |  | |  +------------------+ Sequentially:\n client requests Shoot Cluster A and sets the Server Name in the TLS handshake to api.shoot-a.foo.bar. this packet goes through the Network LB and it’s forwarded to the Proxy server. (this loadbalancer should be a simple Layer-4 TCP proxy) the proxy server reads the packet and see that client requests api.shoot-a.foo.bar. based on its configuration, it maps api.shoot-a.foo.bar to Shoot API Server Cluster A. it acts as TCP proxy and simply send the data Shoot API Server Cluster A.  There are multiple OSS proxies for this case:\n nginx HAProxy Envoy traefik linkerd2-proxy  To ease integration it should:\n be configurable via Kubernetes resources not require restarting when configuration changes be fast and with little overhead  All things considered, Envoy proxy is the most fitting solution as it provides all the features Gardener would like (no process reload being the most important one + battle tested in production by various companies).\nWhile building a custom control plane for Envoy is quite simple, an already established solution might be the better path forward. Istio’s Pilot is one of the most feature-complete Envoy control plane solutions as it offers a way to configure edge ingress traffic for Envoy via Gateway and VirtualService.\nThe resources which needs to be created per Shoot clusters are the following:\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata:  name: kube-apiserver-gateway  namespace: \u003cshoot-namespace\u003e spec:  selector:  istio: ingressgateway  servers:  - port:  number: 443  name: tls  protocol: TLS  tls:  mode: PASSTHROUGH  hosts:  - api.\u003cexternal-domain\u003e  - api.\u003cshoot\u003e.\u003cproject\u003e.\u003cinternal-domain\u003e and correct VirtualService pointing to the correct API server:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata:  name: kube-apiserver  namespace: \u003cshoot-namespace\u003e spec:  hosts:  - api.\u003cexternal-domain\u003e  - api.\u003cshoot\u003e.\u003cproject\u003e.\u003cinternal-domain\u003e  gateways:  - kube-apiserver-gateway  tls:  - match:  - port: 443  sniHosts:  - api.\u003cexternal-domain\u003e  - api.\u003cshoot\u003e.\u003cproject\u003e.\u003cinternal-domain\u003e  route:  - destination:  host: kube-apiserver.\u003cshoot-namespace\u003e.svc.cluster.local  port:  number: 443 The resources above configures Envoy to forward the raw TLS data (without termination) to the Shoot kube-apiserver.\nUpdated diagram:\n+-------------------------------+ | | | Network LB | (accessible from clients) | | | | +-------------+-------+---------+ +------------------+  | | | |  | | proxy + lb | Shoot API Server |  | | +-------------+-------------\u003e+ |  | | | | Cluster A |  | | | | |  | | | +------------------+  | | |  +----------------v----+--+  | | |  +-+--------v----------+ | +------------------+  | | | | |  | | | proxy + lb | Shoot API Server |  | Envoy Proxy | +-------------+----------\u003e+ |  | (ingress Gateway) | | | Cluster B |  | | | | |  | +----+ +------------------+  +-----+----------+----+  | |  | |  | | +------------------+  | | | |  | | proxy + lb | Shoot API Server |  | +-------------------+--------------\u003e+ |  | get | Cluster C |  | configuration | |  | +------------------+  |  v Configure  +--+--------------+ +---------------------+ via Istio  | | | | Custom Resources  | Pilot +--------\u003e+ Seed API Server +\u003c------------------+  | | | |  | | | |  +-----------------+ +---------------------+ In this case the internal and external DNSEntries should be changed to the Network LoadBalancer’s IP.\nIn-cluster communication to the apiserver In Kubernetes the API server is discoverable via the master service (kubernetes in default namespace). Today, this service can only be of type ClusterIP - making in-cluster communication to the API server impossible due to:\n the client doesn’t set the Server Name in the TLS handshake, if it attempts to talk to an IP address. In this case, the TLS handshake reaches the Envoy IngressGateway proxy, but it’s rejected by it. Kubernetes services can be of type ExternalName, but the master service is not supported by kubelet.  even if this is fixed in future Kubernetes versions, this problem still exists for older versions where this functionality is not available.    Another issue occurs when the client tries to talk to the apiserver via the in-cluster DNS. For all Shoot API servers kubernetes.default.svc.cluster.local is the same and when a client tries to connect to that API server using that server name. This makes distinction between different in-cluster Shoot clients impossible by the Envoy IngressGateway.\nTo mitigate this problem an additional proxy must be deployed on every single Node. It does not terminate TLS and sends the traffic to the correct Shoot API Server. This is achieved by:\n the apiserver master service reconciler is started and pointing to the kube-apiserver’s Cluster IP in the Seed cluster (e.g. --advertise-address=10.1.2.3). the proxy runs in the host network of the Node. the proxy has a sidecar container which:  creates a dummy network interface and assigns the 10.1.2.3 to it. removes connection tracking (conntrack) if iptables/nftables is enabled as the IP address is local to the Node.   the proxy listens on the 10.1.2.3 and using the PROXY protocol it sends the data stream to the Envoy ingress gateway (EIGW). EIGW listens for PROXY protocol on a dedicated 8443 port. EIGW reads the destination IP + port from the PROXY protocol and forwards traffic to the correct upstream apiserver.  The sidecar is a standalone component. It’s possible to transparently change the proxy implementation without any modifications to the sidecar. The simplified flow looks like:\n+------------------+ +----------------+ | Shoot API Server | TCP | Envoy IGW | | +\u003c-------------------+ PROXY listener | | Cluster A | | :8443 | +------------------+ +-+--------------+  ^  |  |  |  | +-----------------------------------------------------------+  | Single Node in  | the Shoot cluster  |  | PROXY Protocol  |  |  |  +---------------------+ +----------+----------+  | Pod talking to | | |  | the kubernetes | | Proxy |  | service +------\u003e+ No TLS termination |  | | | |  +---------------------+ +---------------------+ Multiple OSS solutions can be used:\n haproxy nginx  To add a PROXY lister with Istio several resources must be created - a dedicated Gateway, dummy VirtualService and EnvoyFilter which adds listener filter (envoy.listener.proxy_protocol) on 8443 port:\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata:  name: blackhole  namespace: istio-system spec:  selector:  istio: ingressgateway  servers:  - port:  number: 8443  name: tcp  protocol: TCP  hosts:  - \"*\"  ---  apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata:  name: blackhole  namespace: istio-system spec:  hosts:  - blackhole.local  gateways:  - blackhole  tcp:  - match:  - port: 8443  route:  - destination:  host: localhost  port:  number: 9999 # any dummy port will work  ---  apiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata:  name: proxy-protocol  namespace: istio-system spec:  workloadSelector:  labels:  istio: ingressgateway  configPatches:  - applyTo: LISTENER  match:  context: ANY  listener:  portNumber: 8443  name: 0.0.0.0_8443  patch:  operation: MERGE  value:  listener_filters:  - name: envoy.filters.listener.proxy_protocol For each individual Shoot cluster, a dedicated FilterChainMatch is added. It ensures that only Shoot API servers can receive traffic from this listener:\napiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata:  name: \u003cshoot-namespace\u003e  namespace: istio-system spec:  workloadSelector:  labels:  istio: ingressgateway  configPatches:  - applyTo: FILTER_CHAIN  match:  context: ANY  listener:  portNumber: 8443  name: 0.0.0.0_8443  patch:  operation: ADD  value:  filters:  - name: envoy.filters.network.tcp_proxy  typed_config:  \"@type\": type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy  stat_prefix: outbound|443||kube-apiserver.\u003cshoot-namespace\u003e.svc.cluster.local  cluster: outbound|443||kube-apiserver.\u003cshoot-namespace\u003e.svc.cluster.local  filter_chain_match:  destination_port: 443  prefix_ranges:  - address_prefix: 10.1.2.3 # kube-apiserver's cluster-ip  prefix_len: 32  Note: this additional EnvoyFilter can be removed when Istio supports full L4 matching.\n A nginx proxy client in the Shoot cluster on every node could have the following configuration:\nerror_log /dev/stdout; stream { server { listen 10.1.2.3:443; proxy_pass api.\u003cexternal-domain\u003e:8443; proxy_protocol on; proxy_protocol_timeout 5s; resolver_timeout 5s; proxy_connect_timeout 5s; } } events { } In-cluster communication to the apiserver when ExernalName is supported Even if in future versions of Kubernetes, the master service of type ExternalName is supported, we still have the problem that in-cluster workload can talk to the server via DNS. For this to work we still need the above mentioned proxy (this time listening on another IP address 10.0.0.2). An additional change to CoreDNS would be needed:\ndefault.svc.cluster.local.:8053 {  file kubernetes.default.svc.cluster.local }  .:8053 {  errors  health  kubernetes cluster.local in-addr.arpa ip6.arpa {  pods insecure  upstream  fallthrough in-addr.arpa ip6.arpa  }  prometheus :9153  forward . /etc/resolv.conf  cache 30  loop  reload  loadbalance } The content of the kubernetes.default.svc.cluster.local is going to be:\n$ORIGIN default.svc.cluster.local.  @\t30 IN\tSOA local. local. (  2017042745 ; serial  1209600 ; refresh (2 hours)  1209600 ; retry (1 hour)  1209600 ; expire (2 weeks)  30 ; minimum (1 hour)  )   30 IN NS local.  kubernetes IN A 10.0.0.2 So when a client requests kubernetes.default.svc.cluster.local, it’ll be send to the proxy listening on that IP address.\nFuture work While out of scope of this GEP, several things can be improved:\n Make the sidecar work with eBPF and environments where iptables/nftables are not enabled.  References  https://github.com/gardener/gardener/issues/1135  ","categories":"","description":"","excerpt":"SNI Passthrough proxy for kube-apiservers This GEP tackles the problem …","ref":"/docs/gardener/proposals/08-shoot-apiserver-via-sni/","tags":"","title":"Shoot APIServer via SNI"},{"body":"Audit a Kubernetes Cluster The shoot cluster is a kubernetes cluster and its kube-apiserver handles the audit events. In order to define which audit events must be logged, a proper audit policy file must be passed to the kubernetes API server. You could find more information about auditing a kubernetes cluster here.\nDefault Audit Policy By default, the Gardener will deploy the shoot cluster with audit policy defined in the kube-apiserver package.\nCustom Audit Policy If you need specific audit policy for your shoot cluster, then you could deploy the required audit policy in the garden cluster as ConfigMap resource and set up your shoot to refer this ConfigMap. Note, the policy must be stored under the key policy in the data section of the ConfigMap.\nFor example, deploy the auditpolicy ConfigMap in the same namespace as your Shoot resource:\nkubectl apply -f example/95-configmap-custom-audit-policy.yaml then set your shoot to refer that ConfigMap (only related fields are shown):\nspec:  kubernetes:  kubeAPIServer:  auditConfig:  auditPolicy:  configMapRef:  name: auditpolicy The Gardener validate the Shoot resource to refer only existing ConfigMap containing valid audit policy, and rejects the Shoot on failure. If you want to switch back to the default audit policy, you have to remove the section\nauditPolicy:  configMapRef:  name: \u003cconfigmap-name\u003e from the shoot spec.\nRolling Out Changes to the Audit Policy Gardener is not automatically rolling out changes to the Audit Policy to minimize the amount of Shoot reconciliations in order to prevent cloud provider rate limits, etc. Gardener will pick up the changes on the next reconciliation of Shoots referencing the Audit Policy ConfigMap. If users want to immediately rollout Audit Policy changes, they can manually trigger a Shoot reconciliation as described in triggering an immediate reconciliation. This is similar to changes to the cloud provider secret referenced by Shoots.\n","categories":"","description":"","excerpt":"Audit a Kubernetes Cluster The shoot cluster is a kubernetes cluster …","ref":"/docs/gardener/usage/shoot_auditpolicy/","tags":"","title":"Shoot Auditpolicy"},{"body":"Auto-Scaling in Shoot Clusters There are two parts that relate to auto-scaling in Kubernetes clusters in general:\n Horizontal node auto-scaling, i.e., dynamically adding and removing worker nodes Vertical pod auto-scaling, i.e., dynamically raising or shrinking the resource requests/limits of pods  This document provides an overview of both scenarios.\nHorizontal Node Auto-Scaling Every shoot cluster that has at least one worker pool with minimum \u003c maximum nodes configuration will get a cluster-autoscaler deployment. Gardener is leveraging the upstream community Kubernetes cluster-autoscaler component. We have forked it to gardener/autoscaler so that it supports the way how Gardener manages the worker nodes (leveraging gardener/machine-controller-manager). However, we have not touched the logic how it performs auto-scaling decisions. Consequently, please refer to the offical documentation for this component.\nThe Shoot API allows to configure a few flags of the cluster-autoscaler:\n .spec.kubernetes.clusterAutoscaler.ScaleDownDelayAfterAdd defines how long after scale up that scale down evaluation resumes (default: 1h). .spec.kubernetes.clusterAutoscaler.ScaleDownDelayAfterDelete defines how long after node deletion that scale down evaluation resumes (defaults to ScanInterval). .spec.kubernetes.clusterAutoscaler.ScaleDownDelayAfterFailure defines how long after scale down failure that scale down evaluation resumes (default: 3m). .spec.kubernetes.clusterAutoscaler.ScaleDownUnneededTime defines how long a node should be unneeded before it is eligible for scale down (default: 30m). .spec.kubernetes.clusterAutoscaler.ScaleDownUtilizationThreshold defines the threshold under which a node is being removed (default: 0.5). .spec.kubernetes.clusterAutoscaler.ScanInterval defines how often cluster is reevaluated for scale up or down (default: 10s). .spec.kubernetes.clusterAutoscaler.IgnoreTaints specifies a list of taint keys to ignore in node templates when considering to scale a node group (default: nil).  Vertical Pod Auto-Scaling This form of auto-scaling is not enabled by default and must be explicitly enabled in the Shoot by setting .spec.kubernetes.verticalPodAutoscaler.enabled=true. The reason is that it was only introduced lately, and some end-users might have already deployed their own VPA into their clusters, i.e., enabling it by default would interfere with such custom deployments and lead to issues, eventually.\nGardener is also leveraging an upstream community tool, i.e., the Kubernetes vertical-pod-autoscaler component. If enabled, Gardener will deploy it as part of the control plane into the seed cluster. It will also be used for the vertical autoscaling of Gardener’s system components deployed into the kube-system namespace of shoot clusters, for example, kube-proxy or metrics-server.\nYou might want to refer to the official documentation for this component to get more information how to use it.\nThe Shoot API allows to configure a few flags of the vertical-pod-autoscaler:\n .spec.kubernetes.verticalPodAutoscaler.evictAfterOOMThreshold defines the threshold that will lead to pod eviction in case it OOMed in less than the given threshold since its start and if it has only one container (default: 10m0s). .spec.kubernetes.verticalPodAutoscaler.evictionRateBurst defines the burst of pods that can be evicted (default: 1). .spec.kubernetes.verticalPodAutoscaler.evictionRateLimit defines the number of pods that can be evicted per second. A rate limit set to 0 or -1 will disable the rate limiter (default: -1). .spec.kubernetes.verticalPodAutoscaler.evictionTolerance defines the fraction of replica count that can be evicted for update in case more than one pod can be evicted (default: 0.5). .spec.kubernetes.verticalPodAutoscaler.recommendationMarginFraction is the fraction of usage added as the safety margin to the recommended request (default: 0.15). .spec.kubernetes.verticalPodAutoscaler.updaterInterval is the interval how often the updater should run (default: 1m0s). .spec.kubernetes.verticalPodAutoscaler.recommenderInterval is the interval how often metrics should be fetched (default: 1m0s).  ⚠️ Please note that if you disable the VPA again then the related CustomResourceDefinitions will remain in your shoot cluster (although, nobody will act on them). This will also keep all existing VerticalPodAutoscaler objects in the system, including those that might be created by you. You can delete the CustomResourceDefinitions yourself using kubectl delete crd if you want to get rid of them.\n","categories":"","description":"","excerpt":"Auto-Scaling in Shoot Clusters There are two parts that relate to …","ref":"/docs/gardener/usage/shoot_autoscaling/","tags":"","title":"Shoot Autoscaling"},{"body":"GEP-18: Automated Shoot CA Rotation Table of Contents  Summary Motivation  Goals Non-Goals   Proposal Alternatives Open Questions  Summary This proposal outlines an on-demand, multi-step approach to rotate all certificate authorities (CA) used in a Shoot cluster. This process includes creating new CAs, invalidating the old ones and recreating all certificates signed by the CAs.\nWe propose to bundle the rotation of all CAs in the Shoot together as one triggerable action. This includes the recreation and invalidation of the following CAs and all certificates signed by them:\n Cluster CA (currently used for signing kube-apiserver serving certificates and client certificates) kubelet CA (used for signing client certificates for talking to kubelet API, e.g. kube-apiserver-kubelet) etcd CA (used for signing etcd serving certificates and client certificates) front-proxy CA (used for signing client certificates that kube-aggregator (part of kube-apiserver) uses to talk to extension API servers, filled into extension-apiserver-authentication ConfigMap and read by extension API servers to verify incoming kube-aggregator requests) metrics-server CA (used for signing serving certificates, filled into APIService caBundle field and read by kube-aggregator to verify the presented serving certificate) ReversedVPN CA (used for signing vpn-seed-server serving certificate and vpn-shoot client certificate)  Out of scope for now:\n kubelet serving CA is self-generated (valid for 1y) and self-signed by kubelet on startup  kube-apiserver does not seem to verify the presented serving certificate kubelet can be configured to request serving certificate via CSR that can be verified by kube-apiserver, though, we consider this as a separate improvement outside of this GEP   Legacy VPN solution uses the cluster CA for both serving and client certificates. As the solution is soon to be dropped in favor of the new ReversedVPN solution, we don’t intend to introduce a dedicated CA for this component. If ReversedVPN is disabled and the CA rotation is triggered, we make sure to propagate the cluster CA to the relevant places in the legacy VPN solution.  Naturally, not all certificates used for communication with the kube-apiserver are under control of Gardener. An example for a Gardener-controlled certificate is the kubelet client certificate used to communicate with the api server. An example for credentials not controlled by gardener are kubeconfigs or client certificates requested via CertificateSigningRequests by the shoot owner.\nWe propose to use a two step approach to rotate CAs. The start of each phase is triggered by the shoot owner. In summary the first phase is used to create new CAs (for example the new api server and client CA). Then we make sure that all servers and clients under Gardener’s control trust both old and new CA. Next we renew all client certificates that are under Gardener’s control so they are now signed by the new CAs. This includes a node rollout in order to propagate the certificates to kubelets and restart all pods. Afterwards the user needs to change their client credentials to trust both old and new cluster CA. In the second phase, we remove all trust to the old CA for servers and clients under Gardener’s control. This does not include a node rollout but all still running pods using ServiceAccounts will continue to trust the old CA until they restart. Also, the user needs to retrieve the new CA bundle to no longer trust the old CA.\nA detailed overview of all steps required for each phase is given in the proposal section of this GEP.\nIntroducing a new client CA\nCurrently, client certificates and the kube-apiserver certificate are signed by the same CA. We propose to create a separate client CA when triggering the rotation. The client CA is used to sign certificates of clients talking to the API Server.\nMotivation There are a few reasons for rotating shoot cluster CAs:\n If we have to invalidate client certificates for the kube-apiserver or any other component we are forced to rotate the CA. The only way to invalidate them is to stop trusting all client certificates that are signed by the respective CA as kubernetes does not support revoking certificates. If the CA itself got leaked. If the CA is about to expire. If a company policy requires to rotate a CA after a certain point in time.  In each of those cases we currently need to basically manually recreate and replace all CAs and certificates. The process of rotating by hand is cumbersome and could lead to errors due to the many steps needing to be performed in the right order. By automating the process we want to create a way to securely and easily rotate shoot CAs.\nGoals  Offer an automated and safe solution to rotate all CAs in a shoot cluster. Offer a process that is easily understandable for developers and users. Rotate the different CAs in the shoot with a similar process to reduce complexity. Add visibility for Shoot owners when the last CA rotation happened  Non-Goals  Offer an automated solution for rotating other static credentials (like static token).  Later on, a similar two-phase approach could be implemented for the kubeconfig rotation. However, this is out of scope for this enhancement.   Creating a process that runs fully automated without shoot owner interaction. As the shoot owner controls some secrets that would probably not even be possible. Forcing the shoot owner to rotate after a certain time period. Our goal rather is to issue long-running certificates and let the user decide depending on their requirements to rotate as needed. Configurable default CA lifetime  Proposal We will add a new feature gate CARotation for gardener-apiserver and gardenlet which allows to enable or disable the possibility to trigger the rotation.\nTriggering the CA Rotation  Triggered via gardener.cloud/operation annotation in symmetry with other operations like reconciliation, kubeconfig rotation, etc.  annotation increases the generation value for triggering first phase: start-ca-rotation value for triggering the second phase: complete-ca-rotation gardener-apiserver performs the needful validation: user can’t trigger another rotation if one is already in progress, user can’t trigger complete-ca-rotation if first phase has not been compeleted, etc.   The annotation triggers a usual shoot reconciliation (just like a kubeconfig or SSH key rotation) gardenlet begins the CA rotation sequence by setting the new status section .status.credentials.caRotation (probably in updateShootStatusOperationStart) and removes the annotation afterwards  shoot reconciliation needs to be idemptotent to CA rotation phase, i.e. if a usual reconciliation or maintenance operation is triggered in between, no new CAs are generated or similar things that would interfere with the CA rotation sequence    Changing the Shoot Status A new section in the Shoot status is added when the first rotation is triggered:\nstatus:  credentials:  rotation:  certificateAuthorities:  phase: Prepare # Prepare|Finalize|Completed  lastCompletion: 2022-02-07T14:23:44Z  # kubeconfig:  # phase:  # lastCompletion: Later on, this section could be augmented with other information like the names of the credentials secrets (e.g. gardener/gardener#1749)\nstatus:  credentials:  resources:  - type: kubeconfig  kind: Secret  name: shoot-foo.kubeconfig Rotation Sequence for Cluster and Client CA The proposal section includes a detailed description of all steps involved for rotating from a given CA0 to the target CA1.\nt0: Today’s situation\n kube-apiserver uses SERVER CERT signed by CA0 and trusts CLIENT CERTS signed by CA0 kube-controller-manager issues new CLIENT CERTS signed by CA0 kubeconfig trusts only CA0 ServiceAccount secrets trust only CA0 kubelet uses CLIENT CERT signed by CA0  t1: Shoot owner triggers first step of CA rotation process (–\u003e phase one is started):\n Generate CA1 Generate CLIENT_CA1 Update kube-apiserver, kube-scheduler, etc. to trust CLIENT CERTS signed by both CA0 and CLIENT_CA1 (--client-ca-file flag) Update kube-controller-manager to issue new CLIENT CERTS now with CLIENT_CA1 Update kubeconfig so that its CA bundle contains both CA0 andCA1 (if kubeconfig still contains a legacy CLIENT CERT then rotate the kubeconfig) Update generic-token-kubeconfig so that its CA bundle contains both CA0 andCA1 Update kube-controller-manager to populate both CA0 and CA1 in ServiceAccount secrets. Restart control plane components so that their CA bundle contains both CA0 and CA1 Renew CLIENT CERTS (sign them with CLIENT_CA1) for the following control plane components: Prometheus, DWD, legacy VPN), if not dropped already in the context of gardener/gardener#4661 Trigger node rollout  This issues new CLIENT CERTS for all kubelets signed by CLIENT_CA1 This restarts all Pods and propagates CA0 and CA1 into their mounted ServiceAccount secrets (note CAs can not be reloaded by go client, therefore we need a restart of pods.)   Ask user to exchange all their client credentials (kubeconfig, CLIENT CERTS issued by CertificateSigningRequests) to trust both CA0 and CA1  t2: Shoot owner triggers second step of CA rotation process (–\u003e phase two is started):\nPrerequisite: All Gardener-controlled actions listed in t1 were executed successfully (for example node rollout). The shoot owner has guaranteed that they exchanged their client credentials and triggered step 2 via an annotation.\n Renew SERVER CERTS (sign them with CA1) for kube-apiserver, kube-controller-manager, cloud-controller-manager etc. Update kube-apiserver, kube-scheduler, etc. to trust only CLIENT CERTS signed by CLIENT_CA1 Update kubeconfig so that its CA bundle contains only CA1 Update generic-token-kubeconfig so that its CA bundle contains only CA1 Update kube-controller-manager to only contain CA1. ServiceAccount secrets created after this point will get secrets that include only CA1 Restart control plane components so that their CA bundle contains only CA1 Restart kubelets so that the CA bundle in their kubeconfigs contain only CA1 Delete CA0 Ask user to optionally restart their Pods since they still contain CA0 in memory in order to eliminate trust to the old cluster CA. Ask user to exchange all their client credentials (download kubeconfig containing only CA1; when using CLIENT CERTS trust only CA1)  Rotation Sequence of Other CAs Apart from the kube-apiserver CA (and the client CA) we also use 5 other CAs as mentioned above in the gardener codebase. We propose to rotate those CAs together with the kube-apiserver CA following the same trigger.\nℹ️ Note for the front-proxy CA: users need to make sure, extension API servers have reloaded the extension-apiserver-authentication ConfigMap, before triggering the second phase.\nYou can find gardener managed CAs listed here.\nRegarding the rotation steps we want to follow a similar approach to the one we defined for the kube-apiserver CA. Exemplary, we are going to show the timeline for ETCD_CA but the logic should be similiar for all the above listed CAs.\n t0  etcd trusts client certificates signed by ETCD_CA0 and uses a server certificate signed by ETCD_CA0 kube-apiserver and backup-restore use a client certificate signed by ETCD_CA0 and trust ETCD_CA0   t1:  Generate ETCD_CA1 Update etcd to trust CLIENT CERTS signed by both ETCD_CA0 and ETCD_CA1 Update kube-apiserver and backup-restore:  Adapt CA bundle to trust both ETCD_CA0 and ETCD_CA1 Renew CLIENT CERTS (sign them with ETCD_CA1)     t2:  Update etcd:  Trust only CLIENT CERTS signed by ETCD_CA1 Renew SERVER CERT (sign it with ETCD_CA1)   Update kube-apiserver and backup-restore so that their CA bundle contains only ETCD_CA1    ℹ️ This means we are requiring two restarts of etcd in total.\nAlternatives This section presents a different approach to rotate the CAs which is to temporarily create a second set of api-servers utilizing the new CA . After presenting the approach advantages and disadvantages of both approaches are listed.\nt0: Today’s situation\n kube-apiserver uses SERVER CERT signed by CA0 and trusts CLIENT CERTS signed by CA0 kube-controller-manager issues new CLIENT CERTS with CA0 kubeconfig contains only CA0 ServiceAccount secrets contain only CA0 kubelet uses CLIENT CERT signed by CA0  t1: User triggers first step of CA rotation process (–\u003e phase one):\n Generate CA1 Generate CLIENT_CA1 Create new DNSRecord, Service, Istio configuration, etc. for second kube-apiserver deployment Deploy second kube-apiserver deployment trusting only CLIENT CERTS signed by CLIENT_CA1 and using SERVER CERT signed by CA1 Update kube-scheduler, etc. to trust only CLIENT CERTS signed by CLIENT_CA1 (--client-ca-file flag) Update kube-controller-manager to issue new CLIENT CERTS with CLIENT_CA1 Update kubeconfig so that it points to the new DNSRecord and its CA bundle contains only CA1 (if kubeconfig still contains a legacy CLIENT CERT then rotate the kubeconfig) Update ServiceAccount secrets so that their CA bundle contains both CA0 and CA1 Restart control plane components so that they point to the second kube-apiserver Service and so that their CA bundle contains only CA1 Renew CLIENT CERTS (sign them with CLIENT_CA1) for control plane components (Prometheus, DWD, legacy VPN) and point them to the second kube-apiserver Service Adapt apiserver-proxy-pod-mutator to point KUBERNETES_SERVICE_HOST env variable to second kube-apiserver Trigger node rollout  This issues new CLIENT CERTS for all kubelets signed by CLIENT_CA1 and points them to the second DNSRecord This restarts all Pods and propagates CA0 and CA1 into their mounted ServiceAccount secrets   Ask user to exchange all their client credentials (kubeconfig, CLIENT CERTS issued by CertificateSigningRequests)  t2: User triggers second step of CA rotation process (–\u003e phase two):\n Update ServiceAccount secrets so that their CA bundle contains only CA1 Update apiserver-proxy to talk to second kube-apiserver Drop first DNSRecord, Service, Istio configuration and first kube-apiserver deployment Drop CA0 Ask user to optionally restart their Pods since they still contain CA0 in memory.  Advantages/Disadvantages approach two api servers  (+) User needs to adapt client credentials only once (/) Unstable API server domain (-) Probably more implementation effort (-) More complex (-) CA rotation process does not work similar for all CAs in our system  Advantages/Disadvantages of currently preferred approach (see proposal)  (+) Implementation effort seems “straight-forward” (+) CA rotation process works similar for all CAs in our system (/) Stable API server domain (-) User needs to adapt client credentials twice  ","categories":"","description":"","excerpt":"GEP-18: Automated Shoot CA Rotation Table of Contents  Summary …","ref":"/docs/gardener/proposals/18-shoot-ca-rotation/","tags":"","title":"Shoot CA Rotation"},{"body":"Cleanup of Shoot clusters in deletion When a shoot cluster is deleted then Gardener tries to gracefully remove most of the Kubernetes resources inside the cluster. This is to prevent that any infrastructure or other artefacts remain after the shoot deletion.\nThe cleanup is performed in four steps. Some resources are deleted with a grace period, and all resources are forcefully deleted (by removing blocking finalizers) after some time to not block the cluster deletion entirely.\nCleanup steps:\n All ValidatingWebhookConfigurations and MutatingWebhookConfigurations are deleted with a 5m grace period. Forceful finalization happens after 5m. All APIServices and CustomResourceDefinitions are deleted with a 5m grace period. Forceful finalization happens after 1h. All CronJobs, DaemonSets, Deployments, Ingresss, Jobs, Pods, ReplicaSets, ReplicationControllers, Services, StatefulSets, PersistentVolumeClaims are deleted with a 5m grace period. Forceful finalization happens after 5m.  If the Shoot is annotated with shoot.gardener.cloud/skip-cleanup=true then only Services and PersistentVolumeClaims are considered.\n  All VolumeSnapshots and VolumeSnapshotContents are deleted with a 5m grace period. Forceful finalization happens after 1h. All Namespaces are deleted without any grace period. Forceful finalization happens after 5m.  It is possible to override the finalization grace periods via annotations on the Shoot:\n shoot.gardener.cloud/cleanup-webhooks-finalize-grace-period-seconds (for the resources handled in step 1) shoot.gardener.cloud/cleanup-extended-apis-finalize-grace-period-seconds (for the resources handled in step 2) shoot.gardener.cloud/cleanup-kubernetes-resources-finalize-grace-period-seconds (for the resources handled in step 3) shoot.gardener.cloud/cleanup-namespaces-finalize-grace-period-seconds (for the resources handled in step 4)  ⚠️ If \"0\" is provided then all resources are finalized immediately without waiting for any graceful deletion. Please be aware that this might lead to orphaned infrastructure artefacts.\nInfrastructure Cleanup Wait Period After all above cleanup steps have been performed and the Infrastructure extension resource has been deleted the gardenlet waits for a certain duration to allow controllers to properly cleanup infrastructure resources.\nBy default, this duration is set to 5m. Only after this time has passed the shoot deletion flow continues with the entire tear-down of the remaining control plane components (including kube-apiservers, etc.).\nIt is also possible to override this wait period via an annotations on the Shoot:\n shoot.gardener.cloud/infrastructure-cleanup-wait-period-seconds   ℹ️️ All provided period values larger than the above mentioned defaults are ignored.\n ","categories":"","description":"","excerpt":"Cleanup of Shoot clusters in deletion When a shoot cluster is deleted …","ref":"/docs/gardener/usage/shoot_cleanup/","tags":"","title":"Shoot Cleanup"},{"body":"Shoot Cluster Maintenance Day two operations for shoot clusters are related to:\n The Kubernetes version of the control plane and the worker nodes the operating system version of the worker nodes  When referring to an update of the \"operating system version\" in this document, the update of the machine image of the shoot cluster's worker nodes is meant. For example, Amazon Machine Images (AMI) for AWS.  The following table summarizes what options Gardener offers to maintain these versions:\n    Auto-Update Forceful Updates Manual updates     Kubernetes version Patches only Patches and consecutive minor updates only yes   Operating system version yes yes yes    Allowed Target Versions in the CloudProfile Administrators maintain the allowed target versions that you can update to in the CloudProfile for each IaaS-Provider. Users with access to a Gardener project can check supported target versions with:\nkubectl get cloudprofile [IAAS-SPECIFIC-PROFILE] -o yaml    Path Description More information     spec.kubernetes.versions The supported Kubernetes version major.minor.patch. Patch releases   spec.machineImages The supported operating system versions for worker nodes.     Both the Kubernetes version, and the operating system version follow semantic versioning that allows Gardener to handle updates automatically.\nMore information: Semantic Versioning.\nImpact of Version Classifications on Updates Gardener allows to classify versions in the CloudProfile as preview, supported, deprecated, or expired. During maintenance operations, preview versions are excluded from updates, because they’re often recently released versions that haven’t yet undergone thorough testing and may contain bugs or security issues.\nMore information: Version Classifications.\nLet Gardener manage your updates The Maintenance Window Gardener can manage updates for you automatically. It offers users to specify a maintenance window during which updates are scheduled:\n The time interval of the maintenance window can’t be less than 30 minutes or more than 6 hours. If there’s no maintenance window specified during the creation of a shoot cluster, Gardener chooses a maintenance window randomly to spread the load.  You can either specify the maintenance window in the shoot cluster specification (.spec.maintenance.timeWindow) or the start time of the maintenance window using the Gardener dashboard (CLUSTERS \u003e [YOUR-CLUSTER] \u003e OVERVIEW \u003e Lifecycle \u003e Maintenance).\nAuto-Update and Forceful Updates To trigger updates during the maintenance window automatically, Gardener offers the following methods:\n  Auto-update: Gardener starts an update during the next maintenance window whenever there’s a version available in the CloudProfile that is higher than the one of your shoot cluster specification, and that isn’t classified as preview version. For Kubernetes versions, auto-update only updates to higher patch levels.\nYou can either activate auto-update on the Gardener dashboard (CLUSTERS \u003e [YOUR-CLUSTER] \u003e OVERVIEW \u003e Lifecycle \u003e Maintenance) or in the shoot cluster specification:\n .spec.maintenance.autoUpdate.kubernetesVersion: true .spec.maintenance.autoUpdate.machineImageVersion: true    Forceful updates: In the maintenance window, Gardener compares the current version given in the shoot cluster specification with the version list in the CloudProfile. If the version has an expiration date and if the date is before the start of the maintenance window, Gardener starts an update to the highest version available in the CloudProfile that isn’t classified as preview version. The highest version in CloudProfile can’t have an expiration date. For Kubernetes versions, Gardener only updates to higher patch levels or consecutive minor versions.\n  If you don’t want to wait for the next maintenance window, you can annotate the shoot cluster specification with shoot.gardener.cloud/operation: maintain. Gardener then checks immediately if there’s an auto-update or a forceful update needed.\nForceful version updates are even executed if the auto-update for the Kubernetes version, or the auto-update for the machine image version is deactivated (set to `false`).  With expiration dates, administrators can give shoot cluster owners more time for testing before the actual version update happens, which allows smoother transitions to new versions.\nKubernetes Update Paths The bigger the delta of the Kubernetes source version and the Kubernetes target version, the better it must be planned and executed by operators. Gardener only provides automatic support for updates that can be applied safely to the cluster workload:\n   Update Type Example Update method     Patches 1.10.12 to 1.10.13 auto-update or Forceful update   Update to consecutive minor version 1.10.12 to 1.11.10 Forceful update   Other 1.10.12 to 1.12.0 manual update    Gardener doesn’t support automatic updates of nonconsecutive minor versions, because Kubernetes doesn’t guarantee updateability in this case. However, multiple minor version updates are possible if not only the minor source version is expired, but also the minor target version is expired. Gardener then updates the Kubernetes version first to the expired target version, and waits for the next maintenance window to update this version to the next minor target version.\nWarning The administrator who maintains the `CloudProfile` has to ensure that the list of Kubernetes versions consists of consecutive minor versions, for example, from `1.10.x` to `1.11.y`. If the minor version increases in bigger steps, for example, from `1.10.x` to `1.12.y`, shoot cluster updates fail during the maintenance window.  Manual Updates To update the Kubernetes version or the node operating system manually, change the .spec.kubernetes.version field or the .spec.provider.workers.machine.image.version field correspondingly.\nManual updates are required if you would like to do a minor update of the Kubernetes version. Gardener doesn’t do such updates automatically as they can have breaking changes that could impact the cluster workload.\nManual updates are either executed immediately (default) or can be confined to the maintenance time window.\nChoosing the latter option, causes changes to the cluster (for example, node pool rolling-updates) and the subsequent reconciliation, to only predictably happen during a defined time window (available since Gardener version 1.4).\nMore information: Confine Specification Changes/Update Roll Out.\nWarning Before applying such update on minor or major releases, operators should check for all the breaking changes introduced in the target Kubernetes release changelog.  Examples In the examples for the CloudProfile and the shoot cluster specification, only the fields relevant for the example are shown.\nAuto-Update of Kubernetes Version Let’s assume Kubernetes version 1.10.5 and 1.11.0 were added in the following CloudProfile:\nspec:  kubernetes:  versions:  - version: 1.11.0  - version: 1.10.5  - version: 1.10.0 Before this change, the shoot cluster specification looked like this:\nspec:  kubernetes:  version: 1.10.0  maintenance:  timeWindow:  begin: 220000+0000  end: 230000+0000  autoUpdate:  kubernetesVersion: true As a consequence, the shoot cluster is updated to Kubernetes version 1.10.5 between 22:00-23:00 UTC. Your shoot cluster isn’t updated automatically to 1.11.0 even though it’s the highest Kubernetes version in the CloudProfile, because Gardener does only do automatic updates of the Kubernetes patch level.\nForceful Update Due to Expired Kubernetes Version Let’s assume the following CloudProfile:\nspec:  kubernetes:  versions:  - version: 1.12.8  - version: 1.11.10  - version: 1.10.13  - version: 1.10.12  expirationDate: \"2019-04-13T08:00:00Z\" Let’s assume the shoot cluster has the following specification:\nspec:  kubernetes:  version: 1.10.12  maintenance:  timeWindow:  begin: 220000+0100  end: 230000+0100  autoUpdate:  kubernetesVersion: false The shoot cluster specification refers a Kubernetes version that has an expirationDate. In the maintenance window on 2019-04-12, the Kubernetes version stays the same as it’s still not expired. But in the maintenance window on 2019-04-14 the Kubernetes version of the shoot cluster is updated to 1.10.13 (independently of the value of .spec.maintenance.autoUpdate.kubernetesVersion).\nForceful Update to New Minor Kubernetes Version Let’s assume the following CloudProfile:\nspec:  kubernetes:  versions:  - version: 1.12.8  - version: 1.11.10  - version: 1.11.09  - version: 1.10.12  expirationDate: \"2019-04-13T08:00:00Z\" Let’s assume the shoot cluster has the following specification:\nspec:  kubernetes:  version: 1.10.12  maintenance:  timeWindow:  begin: 220000+0100  end: 230000+0100  autoUpdate:  kubernetesVersion: false The shoot cluster specification refers a Kubernetes version that has an expirationDate. In the maintenance window on 2019-04-14, the Kubernetes version of the shoot cluster is updated to 1.11.10, which is the highest patch version of minor target version 1.11 that follows source version 1.10.\nAutomatic Update from Expired Machine Image Version Let’s assume the following CloudProfile:\nspec:  machineImages:  - name: coreos  versions:  - version: 2191.5.0  - version: 2191.4.1  - version: 2135.6.0  expirationDate: \"2019-04-13T08:00:00Z\" Let’s assume the shoot cluster has the following specification:\nspec:  provider:  type: aws  workers:  - name: name  maximum: 1  minimum: 1  maxSurge: 1  maxUnavailable: 0  image:  name: coreos  version: 2135.6.0  type: m5.large  volume:  type: gp2  size: 20Gi  maintenance:  timeWindow:  begin: 220000+0100  end: 230000+0100  autoUpdate:  machineImageVersion: false The shoot cluster specification refers a machine image version that has an expirationDate. In the maintenance window on 2019-04-12, the machine image version stays the same as it’s still not expired. But in the maintenance window on 2019-04-14 the machine image version of the shoot cluster is updated to 2191.5.0 (independently of the value of .spec.maintenance.autoUpdate.machineImageVersion) as version 2135.6.0 is expired.\n","categories":"","description":"Understanding and configuring Gardener's Day-2 operations for Shoot clusters.","excerpt":"Understanding and configuring Gardener's Day-2 operations for Shoot …","ref":"/docs/guides/administer_shoots/maintain-shoot/","tags":"","title":"Shoot Cluster Maintenance"},{"body":"Credentials Rotation For Shoot Clusters There are a lot of different credentials for Shoots to make sure that the various components can communicate with each other, and to make sure it is usable and operable.\nThis page explains how the varieties of credentials can be rotated so that the cluster can be considered secure.\nUser-Provided Credentials Cloud Provider Keys End-users must provide credentials such that Gardener and Kubernetes controllers can communicate with the respective cloud provider APIs in order to perform infrastructure operations. For example, Gardener uses them to setup and maintain the networks, security groups, subnets, etc., while the cloud-controller-manager uses them to reconcile load balancers and routes, and the CSI controller uses them to reconcile volumes and disks.\nDepending on the cloud provider, the required data keys of the Secret differ. Please consult the documentation of the respective provider extension documentation to get to know the concrete data keys (e.g., this document for AWS).\nIt is the responsibility of the end-user to regularly rotate those credentials. The following steps are required to perform the rotation:\n Update the data in the Secret with new credentials. ⚠️ Wait until all Shoots using the Secret are reconciled before you disable the old credentials in your cloud provider account! Otherwise, the Shoots will no longer work as expected. Check out this document to learn how to trigger a reconciliation of your Shoots. After all Shoots using the Secret were reconciled, you can go ahead and deactivate the old credentials in your provider account.  Gardener-Provided Credentials Below credentials are generated by Gardener when shoot clusters are being created. Those include\n kubeconfig (if enabled) certificate authorities (and related server and client certificates) observability passwords for Grafana SSH key pair for worker nodes ETCD encryption key ServiceAccount token signing key …  🚨 There is no auto-rotation of those credentials, and it is the responsibility of the end-user to regularly rotate them.\nWhile it is possible to rotate them one by one, there is also a convenient method to combine the rotation of all of those credentials. The rotation happens in two phases since it might be required to update some API clients (e.g., when CAs are rotated). In order to start the rotation (first phase), you have to annotate the shoot with the rotate-credentials-start operation:\nkubectl -n \u003cshoot-namespace\u003e annotate shoot \u003cshoot-name\u003e gardener.cloud/operation=rotate-credentials-start  You can check the .status.credentials.rotation field in the Shoot to see when the rotation was last initiated and last completed.\n Kindly consider the detailed descriptions below to learn how the rotation is performed and what your responsibilities are. Please note that all respective individual actions apply for this combined rotation as well (e.g., worker nodes are rolled out in the first phase).\nYou can complete the rotation (second phase) by annotating the shoot with the rotate-credentials-complete operation:\nkubectl -n \u003cshoot-namespace\u003e annotate shoot \u003cshoot-name\u003e gardener.cloud/operation=rotate-credentials-complete Kubeconfig If the .spec.kubernetes.enableStaticTokenKubeconfig field is set to true (default) then Gardener generates a kubeconfig with cluster-admin privileges for the Shoots containing credentials for communication with the kube-apiserver (see this document for more information).\nThis Secret is stored with name \u003cshoot-name\u003e.kubeconfig in the project namespace in the garden cluster and has multiple data keys:\n kubeconfig: the completed kubeconfig token: token for system:cluster-admin user username/password: basic auth credentials (if enabled via Shoot.spec.kubernetes.kubeAPIServer.enableBasicAuthentication) ca.crt: the CA bundle for establishing trust to the API server (same as in the Cluster CA bundle secret)   Shoots created with Gardener \u003c= 0.28 used to have a kubeconfig based on a client certificate instead of a static token. With the first kubeconfig rotation, such clusters will get a static token as well.\n⚠️ This does not invalidate the old client certificate. In order to do this, you should perform a rotation of the CAs (see section below).\n It is the responsibility of the end-user to regularly rotate those credentials (or disable this kubeconfig entirely). In order to rotate the token in this kubeconfig, annotate the Shoot with gardener.cloud/operation=rotate-kubeconfig-credentials. This operation is not allowed for Shoots that are already marked for deletion. Please note that only the token (and basic auth password, if enabled) are exchanged. The CA certificate remains the same (see section below for information about the rotation).\nkubectl -n \u003cshoot-namespace\u003e annotate shoot \u003cshoot-name\u003e gardener.cloud/operation=rotate-kubeconfig-credentials  You can check the .status.credentials.rotation.kubeconfig field in the Shoot to see when the rotation was last initiated and last completed.\n Certificate Authorities Gardener generates several certificate authorities (CAs) to ensure secured communication between the various components and actors. Most of those CAs are used for internal communication (e.g., kube-apiserver talks to etcd, vpn-shoot talks to the vpn-seed-server, kubelet talks to kube-apiserver etc.). However, there is also the “cluster CA” which is part of all kubeconfigs and used to sign the server certificate exposed by the kube-apiserver.\nGardener populates a Secret with name \u003cshoot-name\u003e.ca-cluster in the project namespace in the garden cluster which contains the following data keys:\n ca.crt: the CA bundle of the cluster  This bundle contains one or multiple CAs which are used for signing serving certificates of the Shoot’s API server. Hence, the certificates contained in this Secret can be used to verify the API server’s identity when communicating with its public endpoint (e.g. as certificate-authority-data in a kubeconfig). This is the same certificate that is also contained in the kubeconfig’s certificate-authority-data field.\n Shoots created with Gardener \u003e= v1.45 have a dedicated client CA which verifies the legitimacy of client certificates. For older Shoots, the client CA is equal to the cluster CA. With the first CA rotation, such clusters will get a dedicated client CA as well.\n All of the certificates are valid for 10 years. Since it requires adaptation for the consumers of the Shoot, there is no automatic rotation and it is the responsibility of the end-user to regularly rotate the CA certificates.\n Note that the CA rotation can only be triggered if the ShootCARotation feature gate is enabled.\n The rotation happens in three stages (see also GEP-18 for the full details):\n In stage one, new CAs are created and added to the bundle (together with the old CAs). Client certificates are re-issued immediately. In stage two, end-users update all cluster API clients that communicate with the control plane. In stage three, the old CAs are dropped from the bundle and server certificate are re-issued.  Technically, the Preparing phase indicates stage one. Once it is completed, the Prepared phase indicates readiness for stage two. The Completing phase indicates stage three, and the Completed phase states that the rotation process has finished.\n You can check the .status.credentials.rotation.certificateAuthorities field in the Shoot to see when the rotation was last initiated, last completed, and in which phase it currently is.\n In order to start the rotation (stage one), you have to annotate the shoot with the rotate-ca-start operation:\nkubectl -n \u003cshoot-namespace\u003e annotate shoot \u003cshoot-name\u003e gardener.cloud/operation=rotate-ca-start This will trigger a Shoot reconciliation and performs stage one. After it is completed, the .status.credentials.rotation.certificateAuthorities.phase is set to Prepared.\nNow you must update all API clients outside the cluster (such as the kubeconfigs on developer machines) to use the newly issued CA bundle in the \u003cshoot-name\u003e.ca-cluster Secret. Please also note that client certificates must be re-issued now.\nAfter updating all API clients, you can complete the rotation by annotating the shoot with the rotate-ca-complete operation:\nkubectl -n \u003cshoot-namespace\u003e annotate shoot \u003cshoot-name\u003e gardener.cloud/operation=rotate-ca-complete This will trigger another Shoot reconciliation and performs stage three. After it is completed, the .status.credentials.rotation.certificateAuthorities.phase is set to Completed. You could update your API clients again and drop the old CA from their bundle.\n Note that the CA rotation also rotates all internal CAs and signed certificates. Hence, most of the components need to be restarted (including etcd and kube-apiserver).\n⚠️ In stage one, all worker nodes of the Shoot will be rolled out to ensure that the Pods as well as the kubelets get the updated credentials as well.\n Observability Password(s) For Grafana For Shoots with .spec.purpose!=testing, Gardener deploys an observability stack with Prometheus for monitoring, Alertmanager for alerting (optional), Loki for logging, and Grafana for visualization. The Grafana instance is exposed via Ingress and accessible for end-users via basic authentication credentials generated and managed by Gardener.\nThose credentials are stored in a Secret with name \u003cshoot-name\u003e.monitoring in the project namespace in the garden cluster and has multiple data keys:\n username: the user name password: the password basic_auth.csv: the user name and password in CSV format auth: the user name with SHA-1 representation of the password  It is the responsibility of the end-user to regularly rotate those credentials. In order to rotate the password, annotate the Shoot with gardener.cloud/operation=rotate-observability-credentials. This operation is not allowed for Shoots that are already marked for deletion.\nkubectl -n \u003cshoot-namespace\u003e annotate shoot \u003cshoot-name\u003e gardener.cloud/operation=rotate-observability-credentials  You can check the .status.credentials.rotation.observability field in the Shoot to see when the rotation was last initiated and last completed.\n Operators Gardener operators have separate credentials to access their own Grafana instance or Prometheus, Alertmanager, Loki directly. These credentials are only stored in the shoot namespace in the seed cluster and can be retrieved as follows:\nkubectl -n shoot--\u003cproject\u003e--\u003cname\u003e get secret -l name=observability-ingress,managed-by=secrets-manager,manager-identity=gardenlet These credentials are only valid for 30d and get automatically rotated with the next Shoot reconciliation when 80% of the validity approaches or when there are less than 10d until expiration. There is no way to trigger the rotation manually.\nSSH Key Pair For Worker Nodes Gardener generates an SSH key pair whose public key is propagated to all worker nodes of the Shoot. The private key can be used to establish an SSH connection to the workers for troubleshooting purposes. It is recommended to use gardenctl-v2 and its gardenctl ssh command since it is required to first open up the security groups and create a bastion VM (no direct SSH access to the worker nodes is possible).\nThe private key is stored in a Secret with name \u003cshoot-name\u003e.ssh-keypair in the project namespace in the garden cluster and has multiple data keys:\n id_rsa: the private key id_rsa.pub: the public key for SSH  In order to rotate the keys, annotate the Shoot with gardener.cloud/operation=rotate-ssh-keypair. This will propagate a new key to all worker nodes while keeping the old key active and valid as well (it will only be invalidated/removed with the next rotation).\nkubectl -n \u003cshoot-namespace\u003e annotate shoot \u003cshoot-name\u003e gardener.cloud/operation=rotate-ssh-keypair  You can check the .status.credentials.rotation.sshKeypair field in the Shoot to see when the rotation was last initiated or last completed.\n The old key is stored in a Secret with name \u003cshoot-name\u003e.ssh-keypair.old in the project namespace in the garden cluster and has the same data keys as the regular Secret.\nETCD Encryption Key This key is used to encrypt the data of Secret resources inside etcd (see upstream Kubernetes documentation).\nThe encryption key has no expiration date. There is no automatic rotation and it is the responsibility of the end-user to regularly rotate the encryption key.\nThe rotation happens in three stages:\n In stage one, a new encryption key is created and added to the bundle (together with the old encryption key). In stage two, all Secrets in the cluster are rewritten by the kube-apiserver so that they become encrypted with the new encryption key. In stage three, the old encryption is dropped from the bundle.  Technically, the Preparing phase indicates the stages one and two. Once it is completed, the Prepared phase indicates readiness for stage three. The Completing phase indicates stage three, and the Completed phase states that the rotation process has finished.\n You can check the .status.credentials.rotation.etcdEncryptionKey field in the Shoot to see when the rotation was last initiated, last completed, and in which phase it currently is.\n In order to start the rotation (stage one), you have to annotate the shoot with the rotate-etcd-encryption-key-start operation:\nkubectl -n \u003cshoot-namespace\u003e annotate shoot \u003cshoot-name\u003e gardener.cloud/operation=rotate-etcd-encryption-key-start This will trigger a Shoot reconciliation and performs the stages one and two. After it is completed, the .status.credentials.rotation.etcdEncryptionKey.phase is set to Prepared. Now you can complete the rotation by annotating the shoot with the rotate-etcd-encryption-key-complete operation:\nkubectl -n \u003cshoot-namespace\u003e annotate shoot \u003cshoot-name\u003e gardener.cloud/operation=rotate-etcd-encryption-key-complete This will trigger another Shoot reconciliation and performs stage three. After it is completed, the .status.credentials.rotation.etcdEncryptionKey.phase is set to Completed.\nServiceAccount Token Signing Key Gardener generates a key which is used to sign the tokens for ServiceAccounts. Those tokens are typically used by workload Pods running inside the cluster in order to authenticate themselves with the kube-apiserver. This also includes system components running in the kube-system namespace.\nThe token signing key has no expiration date. Since it might require adaptation for the consumers of the Shoot, there is no automatic rotation and it is the responsibility of the end-user to regularly rotate the signing key.\n Note that the signing key rotation can only be triggered if the ShootSARotation feature gate is enabled.\n The rotation happens in three stages, similar to how the CA certificates are rotated:\n In stage one, a new signing key is created and added to the bundle (together with the old signing key). In stage two, end-users update all out-of-cluster API clients that communicate with the control plane via ServiceAccount tokens. In stage three, the old signing key is dropped from the bundle.  Technically, the Preparing phase indicates stage one. Once it is completed, the Prepared phase indicates readiness for stage two. The Completing phase indicates stage three, and the Completed phase states that the rotation process has finished.\n You can check the .status.credentials.rotation.serviceAccountKey field in the Shoot to see when the rotation was last initiated, last completed, and in which phase it currently is.\n In order to start the rotation (stage one), you have to annotate the shoot with the rotate-serviceaccount-key-start operation:\nkubectl -n \u003cshoot-namespace\u003e annotate shoot \u003cshoot-name\u003e gardener.cloud/operation=rotate-serviceaccount-key-start This will trigger a Shoot reconciliation and performs stage one. After it is completed, the .status.credentials.rotation.serviceAccountKey.phase is set to Prepared.\nNow you must update all API clients outside the cluster using a ServiceAccount token (such as the kubeconfigs on developer machines) to use a token issued by the new signing key. Gardener already generates new static token secrets for all ServiceAccounts in the cluster. However, if you need to create it manually, you can check out this document for instructions.\nAfter updating all API clients, you can complete the rotation by annotating the shoot with the rotate-serviceaccount-key-complete operation:\nkubectl -n \u003cshoot-namespace\u003e annotate shoot \u003cshoot-name\u003e gardener.cloud/operation=rotate-serviceaccount-key-complete This will trigger another Shoot reconciliation and performs stage three. After it is completed, the .status.credentials.rotation.serviceAccountKey.phase is set to Completed.\n ⚠️ In stage one, all worker nodes of the Shoot will be rolled out to ensure that the Pods use a new token.\n OpenVPN TLS Auth Keys This key is used to ensure encrypted communication for the VPN connection between the control plane in the seed cluster and the shoot cluster. It is currently not rotated automatically and there is no way to trigger it manually.\n","categories":"","description":"","excerpt":"Credentials Rotation For Shoot Clusters There are a lot of different …","ref":"/docs/gardener/usage/shoot_credentials_rotation/","tags":"","title":"Shoot Credentials Rotation"},{"body":"Contributing to shoot health status conditions Gardener checks regularly (every minute by default) the health status of all shoot clusters. It categorizes its checks into four different types:\n APIServerAvailable: This type indicates whether the shoot’s kube-apiserver is available or not. ControlPlaneHealthy: This type indicates whether all the control plane components deployed to the shoot’s namespace in the seed do exist and are running fine. EveryNodeReady: This type indicates whether all Nodes and all Machine objects report healthiness. SystemComponentsHealthy: This type indicates whether all system components deployed to the kube-system namespace in the shoot do exist and are running fine.  Every Shoot resource has a status.conditions[] list that contains the mentioned types, together with a status (True/False) and a descriptive message/explanation of the status.\nMost extension controllers are deploying components and resources as part of their reconciliation flows into the seed or shoot cluster. A prominent example for this is the ControlPlane controller that usually deploys a cloud-controller-manager or CSI controllers as part of the shoot control plane. Now that the extensions deploy resources into the cluster, especially resources that are essential for the functionality of the cluster, they might want to contribute to Gardener’s checks mentioned above.\nWhat can extensions do to contribute to Gardener’s health checks? Every extension resource in Gardener’s extensions.gardener.cloud/v1alpha1 API group also has a status.conditions[] list (like the Shoot). Extension controllers can write conditions to the resource they are acting on and use a type that also exist in the shoot’s conditions. One exception is that APIServerAvailable can’t be used as the Gardener clearly can identify the status of this condition and it doesn’t make sense for extensions to try to contribute/modify it.\nAs an example for the ControlPlane controller let’s take a look at the following resource:\napiVersion: extensions.gardener.cloud/v1alpha1 kind: ControlPlane metadata:  name: control-plane  namespace: shoot--foo--bar spec:  ... status:  conditions:  - type: ControlPlaneHealthy  status: \"False\"  reason: DeploymentUnhealthy  message: 'Deployment cloud-controller-manager is unhealthy: condition \"Available\" has  invalid status False (expected True) due to MinimumReplicasUnavailable: Deployment  does not have minimum availability.'  lastUpdateTime: \"2014-05-25T12:44:27Z\"  - type: ConfigComputedSuccessfully  status: \"True\"  reason: ConfigCreated  message: The cloud-provider-config has been successfully computed.  lastUpdateTime: \"2014-05-25T12:43:27Z\" The extension controller has declared in its extension resource that one of the deployments it is responsible for is unhealthy. Also, it has written a second condition using a type that is unknown by Gardener.\nGardener will pick the list of conditions and recognize that the there is one with a type ControlPlaneHealthy. It will merge it with its own ControlPlaneHealthy condition and report it back to the Shoot’s status:\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata:  labels:  shoot.gardener.cloud/status: unhealthy  name: some-shoot  namespace: garden-core spec: status:  conditions:  - type: APIServerAvailable  status: \"True\"  reason: HealthzRequestSucceeded  message: API server /healthz endpoint responded with success status code. [response_time:31ms]  lastUpdateTime: \"2014-05-23T08:26:52Z\"  lastTransitionTime: \"2014-05-25T12:45:13Z\"  - type: ControlPlaneHealthy  status: \"False\"  reason: ControlPlaneUnhealthyReport  message: 'Deployment cloud-controller-manager is unhealthy: condition \"Available\" has  invalid status False (expected True) due to MinimumReplicasUnavailable: Deployment  does not have minimum availability.'  lastUpdateTime: \"2014-05-25T12:45:13Z\"  lastTransitionTime: \"2014-05-25T12:45:13Z\"  ... Hence, the only duty extensions have is to maintain the health status of their components in the extension resource they are managing. This can be accomplished using the health check library for extensions.\nError Codes The Gardener API includes some well-defined error codes, e.g., ERR_INFRA_UNAUTHORIZED, ERR_INFRA_DEPENDENCIES, etc. Extension may set these error codes in the .status.conditions[].codes[] list in case it makes sense. Gardener will pick them up and will similarly merge them into the .status.conditions[].codes[] list in the Shoot:\nstatus:  conditions:  - type: ControlPlaneHealthy  status: \"False\"  reason: DeploymentUnhealthy  message: 'Deployment cloud-controller-manager is unhealthy: condition \"Available\" has  invalid status False (expected True) due to MinimumReplicasUnavailable: Deployment  does not have minimum availability.'  lastUpdateTime: \"2014-05-25T12:44:27Z\"  codes:  - ERR_INFRA_UNAUTHORIZED ","categories":"","description":"","excerpt":"Contributing to shoot health status conditions Gardener checks …","ref":"/docs/gardener/extensions/shoot-health-status-conditions/","tags":"","title":"Shoot Health Status Conditions"},{"body":"Highly Available Shoot Control Plane Shoot resource offers a way to request for a highly available control plane.\nFailure Tolerance Types A highly available shoot control plane can be setup with either a failure tolerance of zone or node.\nNode Failure Tolerance Failure tolerance of node will have the following characteristics:\n Control plane components will be spread across different nodes within a single availability zone. There will not be more than one replica per node for each control plane component which has more than one replica. Worker pool should have a minimum of 3 nodes. A multi-node etcd (quorum size of 3) will be provisioned offering zero-downtime capabilities with each member in a different node within a single availability zone.  Zone Failure Tolerance Failure tolerance of zone will have the following characteristics:\n Control plane components will be spread across different availability zones. There will at least be one replica per zone for each control plane component which has more than one replica. Gardener scheduler will automatically select a seed which has a minimum of 3 zones to host the shoot control plane. A multi-node etcd (quorum size of 3) will be provisioned offering zero-downtime capabilities with each member in a different zone.  Shoot Spec To request for a highly available shoot control plane gardener provides the following configuration in the shoot spec.\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot spec:  controlPlane:  highAvailability:  failureTolerance:  type: \u003cnode | zone\u003e Allowed Transitions\nIf you already have a shoot cluster with non-HA control plane then following upgrades are possible:\n Upgrade of non-HA shoot control plane to HA shoot control plane with node failure tolerance. Upgrade of non-HA shoot control plane to HA shoot control plane with zone failure tolerance. However, it is essential that the seed which is currently hosting the shoot control plane should be multi-zonal. If it is not then request to upgrade will be rejected.   NOTE: There will be a small downtime during the upgrade especially for etcd which will transition from a single node etcd cluster to a multi-node etcd cluster.\n Disallowed Transitions\nIf you have already set-up a HA shoot control plane with node failure tolerance then an upgrade to zone failure tolerance is currently not supported, mainly because already existing volumes are bound to the zone they were created in.\n","categories":"","description":"","excerpt":"Highly Available Shoot Control Plane Shoot resource offers a way to …","ref":"/docs/gardener/usage/shoot_high_availability/","tags":"","title":"Shoot High Availability"},{"body":"Shoot Info ConfigMap Overview Gardenlet maintains a ConfigMap inside the Shoot cluster that contains information about the cluster itself. The ConfigMap is named shoot-info and located in the kube-system namespace.\nFields The following fields are provided:\napiVersion: v1 kind: ConfigMap metadata:  name: shoot-info  namespace: kube-system data:  domain: crazy-botany.core.my-custom-domain.com # .spec.dns.domain field from the Shoot resource  extensions: foobar,foobaz # List of extensions that are enabled  kubernetesVersion: 1.20.1 # .spec.kubernetes.version field from the Shoot resource  maintenanceBegin: 220000+0100 # .spec.maintenance.timeWindow.begin field from the Shoot resource  maintenanceEnd: 230000+0100 # .spec.maintenance.timeWindow.end field from the Shoot resource  nodeNetwork: 10.250.0.0/16 # .spec.networking.nodes field from the Shoot resource  podNetwork: 100.96.0.0/11 # .spec.networking.pods field from the Shoot resource  projectName: dev # .metadata.name of the Project  provider: \u003csome-provider-name\u003e # .spec.provider.type field from the Shoot resource  region: europe-central-1 # .spec.region field from the Shoot resource  serviceNetwork: 100.64.0.0/13 # .spec.networking.services field from the Shoot resource  shootName: crazy-botany # .metadata.name from the Shoot resource ","categories":"","description":"","excerpt":"Shoot Info ConfigMap Overview Gardenlet maintains a ConfigMap inside …","ref":"/docs/gardener/usage/shoot_info_configmap/","tags":"","title":"Shoot Info Configmap"},{"body":"Shoot maintenance There is a general document about shoot maintenance that you might want to read. Here, we describe how you can influence certain operations that happen during a shoot maintenance.\nRestart Control Plane Controllers As outlined in above linked document, Gardener offers to restart certain control plane controllers running in the seed during a shoot maintenance.\nExtension controllers can extend the amount of pods being affected by these restarts. If your Gardener extension manages pods of a shoot’s control plane (shoot namespace in seed) and it could potentially profit from a regular restart please consider labeling it with maintenance.gardener.cloud/restart=true.\n","categories":"","description":"","excerpt":"Shoot maintenance There is a general document about shoot maintenance …","ref":"/docs/gardener/extensions/shoot-maintenance/","tags":"","title":"Shoot Maintenance"},{"body":"Shoot Maintenance Shoots configure a maintenance time window in which Gardener performs certain operations that may restart the control plane, roll out the nodes, result in higher network traffic, etc. This document outlines what happens during a shoot maintenance.\nTime Window Via the .spec.maintenance.timeWindow field in the shoot specification end-users can configure the time window in which maintenance operations are executed. Gardener runs one maintenance operation per day in this time window:\nspec:  maintenance:  timeWindow:  begin: 220000+0100  end: 230000+0100 The offset (+0100) is considered with respect to UTC time. The minimum time window is 30m and the maximum is 6h.\n⚠️ Please note that there is no guarantee that a maintenance operation that e.g. starts a node roll-out will finish within the time window. Especially for large clusters it may take several hours until a graceful rolling update of the worker nodes succeeds (also depending on the workload and the configured pod disruption budgets/termination grace periods).\nInternally, Gardener is subtracting 15m from the end of the time window to (best-effort) try to finish the maintenance until the end is reached, however, it might not work in all cases.\nIf you don’t specify a time window then Gardener will randomly compute it. You can change it later, of course.\nAutomatic Version Updates The .spec.maintenance.autoUpdate field in the shoot specification allows you to control how/whether automatic updates of Kubernetes patch and machine image versions are performed. Machine image versions are updated per worker pool.\nspec:  maintenance:  autoUpdate:  kubernetesVersion: true  machineImageVersion: true During the daily maintenance, the Gardener Controller Manager updates the Shoot’s Kubernetes and machine image version if any of the following criteria applies:\n there is a higher version available and the Shoot opted-in for automatic version updates the currently used version is expired  Gardener creates events with type MaintenanceDone on the Shoot describing the action performed during maintenance including the reason why an update has been triggered.\nMaintenanceDone Updated image of worker-pool 'coreos-xy' from 'coreos' version 'xy' to version 'abc'. Reason: AutoUpdate of MachineImage configured. MaintenanceDone Updated Kubernetes version '0.0.1' to version '0.0.5'. This is an increase in the patch level. Reason: AutoUpdate of Kubernetes version configured. MaintenanceDone Updated Kubernetes version '0.0.5' to version '0.1.5'. This is an increase in the minor level. Reason: Kubernetes version expired - force update required. Please refer to this document for more information about Kubernetes and machine image versions in Gardener.\nCluster Reconciliation Gardener administrators/operators can configure the Gardenlet in a way that it only reconciles shoot clusters during their maintenance time windows. This behaviour is not controllable by end-users but might make sense for large Gardener installations. Concretely, your shoot will be reconciled regularly during its maintenance time window. Outside of the maintenance time window it will only reconcile if you change the specification or if you explicitly trigger it, see also this document.\nConfine Specification Changes/Updates Roll Out Via the .spec.maintenance.confineSpecUpdateRollout field you can control whether you want to make Gardener roll out changes/updates to your shoot specification only during the maintenance time window. It is false by default, i.e., any change to your shoot specification triggers a reconciliation (even outside of the maintenance time window). This is helpful if you want to update your shoot but don’t want the changes to be applied immediately. One example use-case would be a Kubernetes version upgrade that you want to roll out during the maintenance time window. Any update to the specification will not increase the .metadata.generation of the Shoot which is something you should be aware of. Also, even if Gardener administrators/operators have not enabled the “reconciliation in maintenance time window only” configuration (as mentioned above) then your shoot will only reconcile in the maintenance time window. The reason is that Gardener cannot differentiate between create/update/reconcile operations.\n⚠️ If confineSpecUpdateRollout=true, please note that if you change the maintenance time window itself then it will only be effective after the upcoming maintenance.\n⚠️ There is one exceptional change in the shoot specification that triggers an immediate roll out which is changes to the .spec.hibernation.enabled field. If you hibernate or wake-up your shoot then Gardener gets active right away.\nShoot Operations In case you would like to perform a shoot credential rotation or a reconcile operation during your maintenance time window, you can annotate the Shoot with\nmaintenance.gardener.cloud/operation=\u003coperation\u003e This will execute the specified \u003coperation\u003e during the next maintenance reconciliation. Note that Gardener will remove this annotation after it has been performed in the maintenance reconciliation.\n ⚠️ This is skipped when the Shoot’s .status.lastOperation.state=Failed. Make sure to retry your shoot reconciliation beforehand.\n Special Operations During Maintenance The shoot maintenance controller triggers special operations that are performed as part of the shoot reconciliation.\nInfrastructure and DNSRecord Reconciliation The reconciliation of the Infrastructure and DNSRecord extension resources is only demanded during the shoot’s maintenance time window. The rationale behind it is to prevent sending too many requests against the cloud provider APIs, especially on large landscapes or if a user has many shoot clusters in the same cloud provider account.\nRestart Control Plane Controllers Gardener operators can make Gardener restart/delete certain control plane pods during a shoot maintenance. This feature helps to automatically solve service denials of controllers due to stale caches, dead-locks or starving routines.\nPlease note that these are exceptional cases but they are observed from time to time. Gardener, for example, takes this precautionary measure for kube-controller-manager pods.\nSee this document to see how extension developers can extend this behaviour.\nRestart Some Core Addons Gardener operators can make Gardener restart some core addons, at the moment only CoreDNS, during a shoot maintenance.\nCoreDNS benefits from this feature as it automatically solve problems with clients stuck to single replica of the deployment and thus overloading it. Please note that these are exceptional cases but they are observed from time to time.\n","categories":"","description":"","excerpt":"Shoot Maintenance Shoots configure a maintenance time window in which …","ref":"/docs/gardener/usage/shoot_maintenance/","tags":"","title":"Shoot Maintenance"},{"body":"Network policies in the Shoot Cluster In addition to deploying network policies into the Seed, Gardener deploys network policies into the kube-system namespace of the Shoot. These network policies are used by Shoot system components (that are not part of the control plane). Other namespaces in the Shoot do not contain network policies deployed by Gardener.\nAs best practice, every pod deployed into the kube-system namespace should use appropriate network policies in order to only allow required network traffic. Therefore, pods should have labels matching to the selectors of the available network policies.\nGardener deploys the following network policies:\nNAME POD-SELECTOR gardener.cloud--allow-dns k8s-app in (kube-dns) gardener.cloud--allow-from-seed networking.gardener.cloud/from-seed=allowed gardener.cloud--allow-to-apiserver networking.gardener.cloud/to-apiserver=allowed gardener.cloud--allow-to-dns networking.gardener.cloud/to-dns=allowed gardener.cloud--allow-to-from-nginx app=nginx-ingress gardener.cloud--allow-to-kubelet networking.gardener.cloud/to-kubelet=allowed gardener.cloud--allow-to-public-networks networking.gardener.cloud/to-public-networks=allowed gardener.cloud--allow-vpn app=vpn-shoot Additionally, there can be network policies deployed by Gardener extensions such as extension-calico.\nNAME POD-SELECTOR gardener.cloud--allow-from-calico-node k8s-app=calico-typha ","categories":"","description":"","excerpt":"Network policies in the Shoot Cluster In addition to deploying network …","ref":"/docs/gardener/usage/shoot_network_policies/","tags":"","title":"Shoot Network Policies"},{"body":"Shoot Networking This document contains network related information for Shoot clusters.\nPod Network A Pod network is imperative for any kind of cluster communication with Pods not started within the Node’s host network. More information about the Kubernetes network model can be found here.\nGardener allows users to configure the Pod network’s CIDR during Shoot creation:\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot spec:  networking:  type: \u003csome-network-extension-name\u003e # {calico,cilium}  pods: 100.96.0.0/16  nodes: ...  services: ...  ⚠️ The networking.pods IP configuration is immutable and cannot be changed afterwards. Please consider the following paragraph to choose a configuration which will meet your demands.\n One of the network plugin’s (CNI) tasks is to assign IP addresses to Pods started in the Pod network. Different network plugins come with different IP address management (IPAM) features, so we can’t give any definite advice how IP ranges should be configured. Nevertheless, we want to outline the standard configuration.\nInformation in .spec.networking.pods matches the –cluster-cidr flag of the Kube-Controller-Manager of your Shoot cluster. This IP range is divided into smaller subnets, also called podCIDRs (default mask /24) and assigned to Node objects .spec.podCIDR. Pods get their IP address from this smaller node subnet in a default IPAM setup. Thus, it must be guaranteed that enough of these subnets can be created for the maximum amount of nodes you expect in the cluster.\nExample 1\nPod network: 100.96.0.0/16 nodeCIDRMaskSize: /24 ------------------------- Number of podCIDRs: 256 --\u003e max. Node count Number of IPs per podCIDRs: 256 With the configuration above a Shoot cluster can at most have 256 nodes which are ready to run workload in the Pod network.\nExample 2\nPod network: 100.96.0.0/20 nodeCIDRMaskSize: /24 ------------------------- Number of podCIDRs: 16 --\u003e max. Node count Number of IPs per podCIDRs: 256 With the configuration above a Shoot cluster can at most have 16 nodes which are ready to run workload in the Pod network.\nBeside the configuration in .spec.networking.pods, users can tune the nodeCIDRMaskSize used by Kube-Controller-Manager on shoot creation. A smaller IP range per node means more podCIDRs and thus the ability to provision more nodes in the cluster, but less available IPs for Pods running on each of the nodes.\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot spec:  kubeControllerManager:  nodeCIDRMaskSize: 24 (default)  ⚠️ The nodeCIDRMaskSize configuration is immutable and cannot be changed afterwards.\n Example 3\nPod network: 100.96.0.0/20 nodeCIDRMaskSize: /25 ------------------------- Number of podCIDRs: 32 --\u003e max. Node count Number of IPs per podCIDRs: 128 With the configuration above a Shoot cluster can at most have 32 nodes which are ready to run workload in the Pod network.\n","categories":"","description":"","excerpt":"Shoot Networking This document contains network related information …","ref":"/docs/gardener/usage/shoot_networking/","tags":"","title":"Shoot Networking"},{"body":"Register Shoot Networking Filter Extension in Shoot Clusters Introduction Within a shoot cluster, it is possible to enable the networking filter. It is necessary that the Gardener installation your shoot cluster runs in is equipped with a shoot-networking-filter extension. Please ask your Gardener operator if the extension is available in your environment.\nShoot Feature Gate In most of the Gardener setups the shoot-networking-filter extension is not enabled globally and thus must be configured per shoot cluster. Please adapt the shoot specification by the configuration shown below to activate the extension individually.\nkind: Shoot ... spec:  extensions:  - type: shoot-networking-filter ... Opt-out If the shoot networking filter is globally enabled by default, it can be disabled per shoot. To disable the service for a shoot, the shoot manifest must explicitly state it.\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot ... spec:  extensions:  - type: shoot-networking-filter  disabled: true ... ","categories":"","description":"","excerpt":"Register Shoot Networking Filter Extension in Shoot Clusters …","ref":"/docs/extensions/others/gardener-extension-shoot-networking-filter/docs/usage/shoot-networking-filter/","tags":"","title":"Shoot Networking Filter"},{"body":"Trigger Shoot Operations You can trigger a few explicit operations by annotating the Shoot with an operation annotation. This might allow you to induct certain behavior without the need to change the Shoot specification. Some of the operations can also not be caused by changing something in the shoot specification because they can’t properly be reflected here. Note, once the triggered operation is considered by the controllers, the annotation will be automatically removed and you have to add it each time you want to trigger the operation.\nPlease note: If .spec.maintenance.confineSpecUpdateRollout=true then the only way to trigger a shoot reconciliation is by setting the reconcile operation, see below.\nImmediate Reconciliation Annotate the shoot with gardener.cloud/operation=reconcile to make the gardenlet start a reconciliation operation without changing the shoot spec and possibly without being in its maintenance time window:\nkubectl -n garden-\u003cproject-name\u003e annotate shoot \u003cshoot-name\u003e gardener.cloud/operation=reconcile Immediate Maintenance Annotate the shoot with gardener.cloud/operation=maintain to make the gardener-controller-manager start maintaining your shoot immediately (possibly without being in its maintenance time window). If no reconciliation starts then nothing needed to be maintained:\nkubectl -n garden-\u003cproject-name\u003e annotate shoot \u003cshoot-name\u003e gardener.cloud/operation=maintain Retry Failed Reconciliation Annotate the shoot with gardener.cloud/operation=retry to make the gardenlet start a new reconciliation loop on a failed shoot. Failed shoots are only reconciled again if a new Gardener version is deployed, the shoot specification is changed or this annotation is set\nkubectl -n garden-\u003cproject-name\u003e annotate shoot \u003cshoot-name\u003e gardener.cloud/operation=retry Credentials Rotation Operations Please consult this document for more information.\nRestart systemd Services On Particular Worker Nodes It is possible to make Gardener restart particular systemd services on your shoot worker nodes if needed. The annotation is not set on the Shoot resource but directly on the Node object you want to target. For example, the following will restart both the kubelet and the docker services:\nkubectl annotate node \u003cnode-name\u003e worker.gardener.cloud/restart-systemd-services=kubelet,docker It may take up to a minute until the service is restarted. The annotation will be removed from the Node object after all specified systemd services have been restarted. It will also be removed even if the restart of one or more services failed.\n ℹ️ In the example mentioned above, you could additionally verify when/whether the kubelet restarted by using kubectl describe node \u003cnode-name\u003e and looking for such a Starting kubelet event.\n ","categories":"","description":"","excerpt":"Trigger Shoot Operations You can trigger a few explicit operations by …","ref":"/docs/gardener/usage/shoot_operations/","tags":"","title":"Shoot Operations"},{"body":"Shoot Cluster Purpose The Shoot resource contains a .spec.purpose field indicating how the shoot is used whose allowed values are as follows:\n evaluation (default): Indicates that the shoot cluster is for evaluation scenarios. development: Indicates that the shoot cluster is for development scenarios. testing: Indicates that the shoot cluster is for testing scenarios. production: Indicates that the shoot cluster is for production scenarios. infrastructure: Indicates that the shoot cluster is for infrastructure scenarios (only allowed for shoots in the garden namespace).  Behavioral Differences The following enlists the differences in the way the shoot clusters are set up based on the selected purpose:\n testing shoot clusters do not get a monitoring or a logging stack as part of their control planes. production shoot clusters get at least two replicas of the kube-apiserver for their control planes. Auto-scaling scale down of the main ETCD is disabled for such clusters.  There are also differences with respect to how testing shoots are scheduled after creation, please consult the Scheduler documentation.\nFuture Steps We might introduce more behavioral difference depending on the shoot purpose in the future. As of today, there are no plans yet.\n","categories":"","description":"","excerpt":"Shoot Cluster Purpose The Shoot resource contains a .spec.purpose …","ref":"/docs/gardener/usage/shoot_purposes/","tags":"","title":"Shoot Purposes"},{"body":"Shoot Scheduling Profiles This guide describes the available scheduling profiles and how they can be configured in the Shoot cluster. It also clarifies how a custom scheduling profile can be configured.\nScheduling profiles The scheduling process in the kube-scheduler happens in series of stages. A scheduling profile allows configuring the different stages of the scheduling.\nAs of today, Gardener supports two predefined scheduling profiles:\n  balanced (default)\nOverview\nThe balanced profile attempts to spread Pods evenly across Nodes to obtain a more balanced resource usage. This profile provides the default kube-scheduler behavior.\nHow it works?\nThe kube-scheduler is started without any profiles. In such case, by default, one profile with the scheduler name default-scheduler is created. This profile includes the default plugins. If a Pod doesn’t specify the .spec.schedulerName field, kube-apiserver sets it to default-scheduler. Then, the Pod gets scheduled by the default-scheduler accordingly.\n  bin-packing (alpha)\nOverview\nThe bin-packing profile scores Nodes based on the allocation of resources. It prioritizes Nodes with most allocated resources. By favoring the Nodes with most allocation some of the other Nodes become under-utilized over time (because new Pods keep being scheduled to the most allocated Nodes). Then, the cluster-autoscaler identifies such under-utilized Nodes and removes them from the cluster. In this way, this profile provides a greater overall resource utilization (compared to the balanced profile).\n Note: The decision of when to remove a Node is a trade-off between optimizing for utilization or the availability of resources. Removing under-utilized Nodes improves cluster utilization, but new workloads might have to wait for resources to be provisioned again before they can run.\n  Note: The bin-packing profile can be configured only for Shoot clusters with Kubernetes version \u003e= 1.20.\n  Note: The bin-packing profile is considered as alpha feature. Use it only for evaluation purposes.\n How it works?\nThe kube-scheduler is configured with the following bin packing profile:\napiVersion: kubescheduler.config.k8s.io/v1beta3 kind: KubeSchedulerConfiguration profiles: - schedulerName: bin-packing-scheduler  pluginConfig:  - name: NodeResourcesFit  args:  scoringStrategy:  type: MostAllocated  plugins:  score:  disabled:  - name: NodeResourcesBalancedAllocation To impose the new profile, a MutatingWebhookConfiguration is deployed in the Shoot cluster. The MutatingWebhookConfiguration intercepts CREATE operations for Pods and sets the .spec.schedulerName field to bin-packing-scheduler. Then, the Pod gets scheduled by the bin-packing-scheduler accordingly. Pods that specify a custom scheduler (i.e., having .spec.schedulerName different from default-scheduler and bin-packing-scheduler) are not affected.\n  Configuring the scheduling profile The scheduling profile can be configured via the .spec.kubernetes.kubeScheduler.profile field in the Shoot:\nspec:  # ...  kubernetes:  kubeScheduler:  profile: \"balanced\" # or \"bin-packing\" Custom scheduling profiles The kube-scheduler’s component configs allows configuring custom scheduling profiles to match the cluster needs. As of today, Gardener supports only two predefined scheduling profiles. The profile configuration in the component config is quite expressive and it is not possible to easily define profiles that would match the needs of every cluster. Because of these reasons, there are no plans to add support for new predefined scheduling profiles. If a cluster owner wants to use a custom scheduling profile then they have to deploy (and maintain) a dedicated kube-scheduler deployment in the cluster itself.\n","categories":"","description":"","excerpt":"Shoot Scheduling Profiles This guide describes the available …","ref":"/docs/gardener/usage/shoot_scheduling_profiles/","tags":"","title":"Shoot Scheduling Profiles"},{"body":"ServiceAccount Configurations For Shoot Clusters The Shoot specification allows to configure some of the settings for the handling of ServiceAccounts:\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot spec:  kubernetes:  kubeAPIServer:  serviceAccountConfig:  issuer: foo  acceptedIssuers:  - foo1  - foo2  # Deprecated: This field is deprecated and will be removed in a future version of Gardener. Do not use it.  signingKeySecretName:  name: my-signing-key-secret  extendTokenExpiration: true  maxTokenExpiration: 45d ... Issuer And Accepted Issuers The .spec.kubernetes.kubeAPIServer.serviceAccountConfig.{issuer,acceptedIssuers} field are translated to the --service-account-issuer flag for the kube-apiserver. The issuer will assert its identifier in the iss claim of issued tokens. According to the upstream specification, values need to meet the following requirements:\n This value is a string or URI. If this option is not a valid URI per the OpenID Discovery 1.0 spec, the ServiceAccountIssuerDiscovery feature will remain disabled, even if the feature gate is set to true. It is highly recommended that this value comply with the OpenID spec: https://openid.net/specs/openid-connect-discovery-1_0.html. In practice, this means that service-account-issuer must be an https URL. It is also highly recommended that this URL be capable of serving OpenID discovery documents at {service-account-issuer}/.well-known/openid-configuration.\n By default, Gardener uses the internal cluster domain as issuer (e.g., https://api.foo.bar.example.com). If you specify the issuer then this default issuer will always be part of the list of accepted issuers (you don’t need to specify it yourself).\n⚠️ Caution: If you change from the default issuer to a custom issuer then all previously issued tokens are still valid/accepted. However, if you change from a custom issuer A to another issuer B (custom or default) then you have to add A to the acceptedIssuers so that previously issued tokens are not invalidated. Otherwise, the control plane components as well as system components and your workload pods might fail. You can remove A from the acceptedIssuers when all active tokens were issued by B. This can be ensured by using projected token volumes with a short validity, or by rolling out all pods. Additionally, all ServiceAccount token secrets should be recreated. Apart from this you should wait for at least 12h to make sure the control plane and system components receive a new token from Gardener.\nSigning Key Secret  🚨 This field is deprecated and will be removed in a future version of Gardener. Do not use it.\n The .spec.kubernetes.kubeAPIServer.serviceAccountConfig.signingKeySecretName.name specifies the name of Secret in the same namespace as the Shoot in the garden cluster. It should look as follows:\napiVersion: v1 kind: Secret metadata:  name: \u003cname\u003e  namespace: \u003cnamespace\u003e data:  signing-key: base64(signing-key-pem) The provided key will be used for configuring both the --service-account-signing-key-file and --service-account-key-file flags of the kube-apiserver.\nAccording to the upstream specification, they have the following effects:\n  --service-account-key-file: File containing PEM-encoded x509 RSA or ECDSA private or public keys, used to verify ServiceAccount tokens. The specified file can contain multiple keys, and the flag can be specified multiple times with different files. If specified multiple times, tokens signed by any of the specified keys are considered valid by the Kubernetes API server. --service-account-signing-key-file: Path to the file that contains the current private key of the service account token issuer. The issuer signs issued ID tokens with this private key.   Note that rotation of this key is not yet supported, hence usage is not recommended. By default, Gardener will generate a service account signing key for the cluster.\nToken Expirations The .spec.kubernetes.kubeAPIServer.serviceAccountConfig.extendTokenExpiration configures the --service-account-extend-token-expiration flag of the kube-apiserver. It is enabled by default and has the following specification:\n Turns on projected service account expiration extension during token generation, which helps safe transition from legacy token to bound service account token feature. If this flag is enabled, admission injected tokens would be extended up to 1 year to prevent unexpected failure during transition, ignoring value of service-account-max-token-expiration.\n The .spec.kubernetes.kubeAPIServer.serviceAccountConfig.maxTokenExpiration configures the --service-account-max-token-expiration flag of the kube-apiserver. It has the following specification:\n The maximum validity duration of a token created by the service account token issuer. If an otherwise valid TokenRequest with a validity duration larger than this value is requested, a token will be issued with a validity duration of this value.\n ⚠️ Note that the value for this field must be in the [30d,90d] range. The background for this limitation is that all Gardener components to rely on the TokenRequest API and the Kubernetes service account token projection feature with short-lived, auto-rotating tokens. Any values lower than 30d risk impacting the SLO for shoot clusters, and any values above 90d violate security best practices with respect to maximum validity of credentials before they must be rotated. Given that the field just specifies the upper bound, end-users can still use lower values for their individual workload by specifying the .spec.volumes[].projected.sources[].serviceAccountToken.expirationSeconds in the PodSpecs.\n","categories":"","description":"","excerpt":"ServiceAccount Configurations For Shoot Clusters The Shoot …","ref":"/docs/gardener/usage/shoot_serviceaccounts/","tags":"","title":"Shoot Serviceaccounts"},{"body":"Shoot Status This document provides an overview of the ShootStatus.\nConditions The Shoot status consists of a set of conditions. A Condition has the following fields:\n   Field name Description     type Name of the condition.   status Indicates whether the condition is applicable, with possible values True, False, Unknown, or Progressing.   lastTransitionTime Timestamp for when the condition last transitioned from one status to another.   lastUpdateTime Timestamp for when the condition was updated. Usually changes when reason or message in condition is updated.   reason Machine-readable, UpperCamelCase text indicating the reason for the condition’s last transition.   message Human-readable message indicating details about the last status transition.   codes Well-defined error codes in case the condition reports a problem.    Currently the available Shoot condition types are:\n  APIServerAvailable\nThis condition type indicates whether the Shoot’s kube-apiserver is available or not. In particular, the /healthz endpoint of the kube-apiserver is called, and the expected response code is HTTP 200.\n  ControlPlaneHealthy\nThis condition type indicates whether all the control plane components deployed to the Shoot’s namespace in the Seed do exist and are running fine.\n  EveryNodeReady\nThis condition type indicates whether at least the requested minimum number of Nodes is present per each worker pool and whether all Nodes are healthy.\n  SystemComponentsHealthy\nThis condition type indicates whether all system components deployed to the kube-system namespace in the shoot do exist and are running fine. It also reflects whether the tunnel connection between the control plane and the Shoot networks can be established.\n  The Shoot conditions are maintained by the shoot care control of gardenlet.\nSync Period The condition checks are executed periodically at interval which is configurable in the GardenletConfiguration (.controllers.shootCare.syncPeriod, defaults to 1m).\nCondition Thresholds The GardenletConfiguration also allows configuring condition thresholds (controllers.shootCare.conditionThresholds). Condition threshold is the amount of time to consider condition as Processing on condition status changes.\nLet’s check the following example to get better understanding. Let’s say that the APIServerAvailable condition of our Shoot is with status True. If the next condition check fails (for example kube-apiserver becomes unreachable), then the condition first goes to Processing state. Only if this state remains for condition threshold amount of time, then the condition finally is updated to False.\nConstraints Constraints represent conditions of a Shoot’s current state that constraint some operations on it. The current constraints are:\nHibernationPossible:\nThis constraint indicates whether a Shoot is allowed to be hibernated. The rationale behind this constraint is that a Shoot can have ValidatingWebhookConfigurations or MutatingWebhookConfigurations acting on resources that are critical for waking up a cluster. For example, if a webhook has rules for CREATE/UPDATE Pods or Nodes and failurePolicy=Fail, the webhook will block joining Nodes and creating critical system component Pods and thus block the entire wakeup operation, because the server backing the webhook is not running.\nEven if the failurePolicy is set to Ignore, high timeouts (\u003e15s) can lead to blocking requests of control plane components. That’s because most control-plane API calls are made with a client-side timeout of 30s, so if a webhook has timeoutSeconds=30 the overall request might still fail as there is overhead in communication with the API server and potential other webhooks. Generally, it’s best pratice to specify low timeouts in WebhookConfigs. Also, it’s best practice to exclude the kube-system namespace from webhooks to avoid blocking critical operations on system components of the cluster. Shoot owners can do so by adding a namespaceSelector similar to this one to their webhook configurations:\nnamespaceSelector:  matchExpressions:  - key: gardener.cloud/purpose  operator: NotIn  values:  - kube-system If the Shoot still has webhooks with either failurePolicy={Fail,nil} or failurePolicy=Ignore \u0026\u0026 timeoutSeconds\u003e15 that act on critical resources in the kube-system namespace, Gardener will set the HibernationPossible to False indicating, that the Shoot can probably not be woken up again after hibernation without manual intervention of the Gardener Operator. gardener-apiserver will prevent any Shoot with the HibernationPossible constraint set to False from being hibernated, that is via manual hibernation as well as scheduled hibernation.\n By setting .controllers.shootCare.webhookRemediatorEnabled=true in the gardenlet configuration, the auto-remediation of webhooks not following the best practices can be turned on in the shoot clusters. Concretely, missing namespaceSelectors or objectSelectors will be added and too high timeoutSeconds will be lowered. In some cases, the failurePolicy will be set from Fail to Ignore. Gardenlet will also add an annotation to make it visible to end-users that their webhook configurations were mutated and should be fixed by them in the first place. Note that all of this is no perfect solution and just done on a best effort basis. Only the owner of the webhook can know whether it indeed is problematic and configured correctly.\nWebhooks labeled with remediation.webhook.shoot.gardener.cloud/exclude=true will be excluded from auto-remediation.\n MaintenancePreconditionsSatisfied:\nThis constraint indicates whether all preconditions for a safe maintenance operation are satisfied (see also this document for more information about what happens during a shoot maintenance). As of today, the same checks as in the HibernationPossible constraint are being performed (user-deployed webhooks that might interfere with potential rolling updates of shoot worker nodes). There is no further action being performed on this constraint’s status (maintenance is still being performed). It is meant to make the user aware of potential problems that might occur due to his configurations.\nCACertificateValiditiesAcceptable:\nThis constraints indicates that there is at least one CA certificate which expires in less than 1y. It will not be added to the .status.constraints if there is no such CA certificate. However, if it’s visible, then a credentials rotation operation should be considered.\nLast Operation The Shoot status holds information about the last operation that is performed on the Shoot. The last operation field reflects overall progress and the tasks that are currently being executed. Allowed operation types are Create, Reconcile, Delete, Migrate and Restore. Allowed operation states are Processing, Succeeded, Error, Failed, Pending and Aborted. An operation in Error state is an operation that will be retried for a configurable amount of time (controllers.shoot.retryDuration field in GardenletConfiguration, defaults to 12h). If the operation cannot complete successfully for the configured retry duration, it will be marked as Failed. An operation in Failed state is an operation that won’t be retried automatically (to retry such an operation, see Retry failed operation).\nLast Errors The Shoot status also contains information about the last occurred error(s) (if any) during an operation. A LastError consists of identifier of the task returned error, human-readable message of the error and error codes (if any) associated with the error.\nError Codes Known error codes are:\n ERR_INFRA_UNAUTHENTICATED - indicates that the last error occurred due to the client request not being completed because it lacks valid authentication credentials for the requested resource. It is classified as a non-retryable error code. ERR_INFRA_UNAUTHORIZED - indicates that the last error occurred due to the server understanding the request but refusing to authorize it. It is classified as a non-retryable error code. ERR_INFRA_QUOTA_EXCEEDED - indicates that the last error occurred due to infrastructure quota limits. It is classified as a non-retryable error code. ERR_INFRA_RATE_LIMITS_EXCEEDED - indicates that the last error occurred due to exceeded infrastructure request rate limits. ERR_INFRA_DEPENDENCIES - indicates that the last error occurred due to dependent objects on the infrastructure level. It is classified as a non-retryable error code. ERR_RETRYABLE_INFRA_DEPENDENCIES - indicates that the last error occurred due to dependent objects on the infrastructure level, but the operation should be retried. ERR_INFRA_RESOURCES_DEPLETED - indicates that the last error occurred due to depleted resource in the infrastructure. ERR_CLEANUP_CLUSTER_RESOURCES - indicates that the last error occurred due to resources in the cluster that are stuck in deletion. ERR_CONFIGURATION_PROBLEM - indicates that the last error occurred due to a configuration problem. It is classified as a non-retryable error code. ERR_RETRYABLE_CONFIGURATION_PROBLEM - indicates that the last error occurred due to a retryable configuration problem. “Retryable” means that the occurred error is likely to be resolved in a ungraceful manner after given period of time. ERR_PROBLEMATIC_WEBHOOK - indicates that the last error occurred due to a webhook not following the Kubernetes best practices (https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#best-practices-and-warnings).  Status Label Shoots will be automatically labeled with the shoot.gardener.cloud/status label. Its value might either be healthy, progressing, unhealthy or unknown depending on the .status.conditions, .status.lastOperation and status.lastErrors of the Shoot. This can be used as an easy filter method to find shoots based on their “health” status.\n","categories":"","description":"","excerpt":"Shoot Status This document provides an overview of the ShootStatus. …","ref":"/docs/gardener/usage/shoot_status/","tags":"","title":"Shoot Status"},{"body":"Shoot Updates and Upgrades This document describes what happens during shoot updates (changes incorporated in a newly deployed Gardener version) and during shoot upgrades (changes for version controllable by end-users).\nUpdates Updates to all aspects of the shoot cluster happen when the gardenlet reconciles the Shoot resource.\nWhen are Reconciliations Triggered Generally, when you change the specification of your Shoot the reconciliation will start immediately, potentially updating your cluster. Please note that you can also confine the reconciliation triggered due to your specification updates to the cluster’s maintenance time window. Please find more information here.\nYou can also annotate your shoot with special operation annotations (see this document) which will cause the reconciliation to start due to your actions.\nThere is also an automatic reconciliation by Gardener. The period, i.e., how often it is performed, depends on the configuration of the Gardener administrators/operators. In some Gardener installations the operators might enable “reconciliation in maintenance time window only” (more information) which will result in at least one reconciliation during the time configured in the Shoot’s .spec.maintenance.timeWindow field.\nWhich Updates are Applied As end-users can only control the Shoot resource’s specification but not the used Gardener version, they don’t have any influence on which of the updates are rolled out (other than those settings configurable in the Shoot). A Gardener operator can deploy a new Gardener version at any point in time. Any subsequent reconciliation of Shoots will update them by rolling out the changes incorporated in this new Gardener version.\nSome examples for such shoot updates are:\n Add a new/remove an old component to/from the shoot’s control plane running in the seed, or to/from the shoot’s system components running on the worker nodes. Change the configuration of an existing control plane/system component. Restart of existing control plane/system components (this might result in a short unavailability of the Kubernetes API server, e.g., when etcd or a kube-apiserver itself is being restarted)  Behavioural Changes Generally, some of such updates (e.g., configuration changes) could theoretically result in different behaviour of controllers. If such changes would be backwards-incompatible then we usually follow one of those approaches (depends on the concrete change):\n Only apply the change for new clusters. Expose a new field in the Shoot resource that lets users control this changed behaviour to enable it at a convenient point in time. Put the change behind an alpha feature gate (disabled by default) in the gardenlet (only controllable by Gardener operators) which will be promoted to beta (enabled by default) in subsequent releases (in this case, end-users have no influence on when the behaviour changes - Gardener operators should inform their end-users and provide clear timelines when they will enable the feature gate).  Upgrades We consider shoot upgrades to change either the\n Kubernetes version (.spec.kubernetes.version) Kubernetes version of the worker pool if specified (.spec.provider.workers[].kubernetes.version) Machine image version of at least one worker pool (.spec.provider.workers[].machine.image.version)  Generally, an upgrade is also performed through a reconciliation of the Shoot resource, i.e., the same concepts like for shoot updates apply. If an end-user triggers an upgrade (e.g., by changing the Kubernetes version) after a new Gardener version was deployed but before the shoot was reconciled again, then this upgrade might incorporate the changes delivered with this new Gardener version.\nIn-Place vs. Rolling Updates If the Kubernetes patch version is changed then the upgrade happens in-place. This means that the shoot worker nodes remain untouched and only the kubelet process restarts with the new Kubernetes version binary. The same applies for configuration changes of the kubelet.\nIf the Kubernetes minor version is changed then the upgrade is done in a “rolling update” fashion, similar to how pods in Kubernetes are updated (when backed by a Deployment). The worker nodes will be terminated one after another and replaced by new machines. The existing workload is gracefully drained and evicted from the old worker nodes to new worker nodes, respecting the configured PodDisruptionBudgets (see Kubernetes documentation).\nCustomize Rolling Update Behaviour of Shoot Worker Nodes The .spec.provider.workers[] list exposes two fields that you might configure based on your workload’s needs: maxSurge and maxUnavailable. The same concepts like in Kubernetes apply. Additionally, you might customize how the machine-controller-manager (abbrev.: MCM; the component instrumenting this rolling update) is behaving. You can configure the following fields in .spec.provider.worker[].machineControllerManager:\n machineDrainTimeout: Timeout (in duration) used while draining of machine before deletion, beyond which MCM forcefully deletes machine (default: 10m). machineHealthTimeout: Timeout (in duration) used while re-joining (in case of temporary health issues) of machine before it is declared as failed (default: 10m). machineCreationTimeout: Timeout (in duration) used while joining (during creation) of machine before it is declared as failed (default: 10m). maxEvictRetries: Maximum number of times evicts would be attempted on a pod before it is forcibly deleted during draining of a machine (default: 10). nodeConditions: List of case-sensitive node-conditions which will change a machine to a Failed state after the machineHealthTimeout duration. It may further be replaced with a new machine if the machine is backed by a machine-set object (defaults: KernelDeadlock, ReadonlyFilesystem , DiskPressure).  Rolling Update Triggers Apart from the above mentioned triggers, a rolling update of the shoot worker nodes is also triggered for some changes to your worker pool specification (.spec.provider.workers[], even if you don’t change the Kubernetes or machine image version). The complete list of fields that trigger a rolling update:\n .spec.kubernetes.version (except for patch version changes) .spec.provider.workers[].machine.image.name .spec.provider.workers[].machine.image.version .spec.provider.workers[].machine.type .spec.provider.workers[].volume.type .spec.provider.workers[].volume.size .spec.provider.workers[].providerConfig .spec.provider.workers[].cri.name .spec.provider.workers[].kubernetes.version (except for patch version changes) .status.credentials.rotation.certificateAuthorities.lastInitiationTime (changed by gardener when a shoot CA rotation is initiated) .status.credentials.rotation.serviceAccountKey.lastInitiationTime (changed by gardener when a shoot service account signing key rotation is initiated)  Generally, the provider extension controllers might have additional constraints for changes leading to rolling updates, so please consult the respective documentation as well.\nRelated Documentation  Shoot Operations Shoot Maintenance Confine Specification Changes/Updates Roll Out To Maintenance Time Window.  ","categories":"","description":"","excerpt":"Shoot Updates and Upgrades This document describes what happens during …","ref":"/docs/gardener/usage/shoot_updates/","tags":"","title":"Shoot Updates"},{"body":"Shoot Kubernetes and Operating System Versioning in Gardener Motivation On the one hand-side, Gardener is responsible for managing the Kubernetes and the Operating System (OS) versions of its Shoot clusters. On the other hand-side, Gardener needs to be configured and updated based on the availability and support of the Kubernetes and Operating System version it provides. For instance, the Kubernetes community releases minor versions roughly every three months and usually maintains three minor versions (the current and the last two) with bug fixes and security updates. Patch releases are done more frequently.\nWhen using the term Machine image in the following, we refer to the OS version that comes with the machine image of the node/worker pool of a Gardener Shoot cluster. As such we are not referring to the CloudProvider specific machine image like the AMI for AWS. For more information how Gardener maps machine image versions to CloudProvider specific machine images, take a look at the individual gardener extension providers such as the provider for AWS.\nGardener should be configured accordingly to reflect the “logical state” of a version. It should be possible to define the Kubernetes or Machine image versions that still receive bug fixes and security patches, and also vice-versa to define the version that are out-of-maintenance and are potentially vulnerable. Moreover, this allows Gardener to “understand” the current state of a version and act upon it (more information in the following sections).\nOverview As a Gardener operator:\n I can classify a version based on it’s logical state (preview, supported, deprecated and expired see Version Classification). I can define which Machine image and Kubernetes versions are eligible for the auto update of clusters during the maintenance time. I can disallow the creation of clusters having a certain version (think of severe security issues).  As an end-user/Shoot owner of Gardener:\n I can get information about which Kubernetes and Machine image versions exist and their classification. I can determine the time when my Shoot clusters Machine image and Kubernetes version will be forcefully updated to the next patch or minor version (in case the cluster is running a deprecated version with an expiration date). I can get this information via API from the CloudProfile.  Version Classifications Administrators can classify versions into four distinct “logical states”: preview, supported, deprecated and expired. The version classification serves as a “point-of-reference” for end-users and also has implications during shoot creation and the maintenance time.\nIf a version is unclassified, Gardener cannot make those decision based on the “logical state”. Nevertheless, Gardener can operate without version classifications and can be added at any time to the Kubernetes and machine image versions in the CloudProfile.\nAs a best practice, versions usually start with the classification preview, then are promoted to supported, eventually deprecated and finally expired. This information is programmatically available in the CloudProfiles of the Garden cluster.\n  preview: A preview version is a new version that has not yet undergone thorough testing, possibly a new release, and needs time to be validated. Due to its short early age, there is a higher probability of undiscovered issues and is therefore not yet recommended for production usage. A Shoot does not update (neither auto-update or force-update) to a preview version during the maintenance time. Also preview versions are not considered for the defaulting to the highest available version when deliberately omitting the patch version during Shoot creation. Typically, after a fresh release of a new Kubernetes (e.g. v1.23.0) or Machine image version (e.g. coreos-2023.5), the operator tags it as preview until he has gained sufficient experience and regards this version to be reliable. After the operator gained sufficient trust, the version can be manually promoted to supported.\n  supported: A supported version is the recommended version for new and existing Shoot clusters. New Shoot clusters should use and existing clusters should update to this version. Typically for Kubernetes versions, the latest Kubernetes patch versions of the actual (if not still in preview) and the last 3 minor Kubernetes versions are maintained by the community. An operator could define these versions as being supported (e.g. v1.22.1, v1.21.4, v1.20.9 and v1.19.12).\n  deprecated: A deprecated version is a version that approaches the end of its lifecycle and can contain issues which are probably resolved in a supported version. New Shoots should not use this version any more. Existing Shoots will be updated to a newer version if auto-update is enabled (.spec.maintenance.autoUpdate.kubernetesVersion for Kubernetes version auto-update, or .spec.maintenance.autoUpdate.machineImageVersion for machine image version auto-update). Using automatic upgrades, however, does not guarantee that a Shoot runs a non-deprecated version, as the latest version (overall or of the minor version) can be deprecated as well. Deprecated versions should have an expiration date set for eventual expiration.\n  expired: An expired versions has an expiration date (based on the Golang time package) in the past. New clusters with that version cannot be created and existing clusters are forcefully migrated to a higher version during the maintenance time.\n  Below is an example how the relevant section of the CloudProfile might look like:\napiVersion: core.gardener.cloud/v1beta1 kind: CloudProfile metadata:  name: alicloud spec:  kubernetes:  versions:  - classification: supported  version: 1.17.1  - classification: deprecated  expirationDate: \"2020-07-24T16:13:26Z\"  version: 1.17.0  - classification: preview  version: 1.16.6  - classification: supported  version: 1.16.5  - classification: deprecated  expirationDate: \"2020-04-25T09:30:40Z\"  version: 1.16.4  - classification: supported  version: 1.15.7  - classification: deprecated  expirationDate: \"2020-06-09T14:01:39Z\"  version: 1.15.6 Version Requirements (Kubernetes and Machine image) The Gardener API server enforces the following requirements for versions:\nDeletion of a version  A version that is in use by a Shoot cannot be deleted from the CloudProfile.  Adding a version  A version must not have an expiration date in the past. There can be only one supported version per minor version. The latest Kubernetes version cannot have an expiration date. The latest version for a machine image can have an expiration date. [*]  [*] Useful for cases in which support for given machine image needs to be deprecated and removed (for example the machine image reaches end of life).\nForceful migration of expired versions If a Shoot is running a version after its expiration date has passed, it will be forcefully migrated during its maintenance time. This happens even if the owner has opted out of automatic cluster updates!\nFor Machine images, the Shoots worker pools will be updated to the latest non-preview version of the pools respective image.\nFor Kubernetes versions, the forceful update picks the latest non-preview patch version of the current minor version.\nIf the cluster is already on the latest patch version and the latest patch version is also expired, it will continue with the latest patch version of the next consecutive minor Kubernetes version, so it will result in an update of a minor Kubernetes version!\nPlease note, that multiple consecutive minor version upgrades are possible. This can occur if the Shoot is updated to a version that in turn is also expired. In this case, the version is again upgraded in the next maintenance time.\nDepending on the circumstances described above, it can happen that the cluster receives multiple consecutive minor Kubernetes version updates!\nKubernetes “minor version jumps” are not allowed - meaning to skip the update to the consecutive minor version and directly update to any version after that. For instance, the version 1.10.x can only update to a version 1.11.x, not to 1.12.x or any other version. This is because Kubernetes does not guarantee upgradeability in this case, leading to possibly broken Shoot clusters. The administrator has to set up the CloudProfile in such a way, that consecutive Kubernetes minor versions are available. Otherwise, Shoot clusters will fail to upgrade during the maintenance time.\nConsider the CloudProfile below with a Shoot using the Kubernetes version 1.10.12. Even though the version is expired, due to missing 1.11.x versions, the Gardener Controller Manager cannot upgrade the Shoot’s Kubernetes version.\nspec:  kubernetes:  versions:  - version: 1.12.8  - version: 1.12.7  - version: 1.10.12  expirationDate: \"\u003cexpiration date in the past\u003e\" The CloudProfile must specify versions 1.11.x of the consecutive minor version. Configuring the CloudProfile in such a way, the Shoot’s Kubernetes version will be upgraded to version 1.11.10 in the next maintenance time.\nspec:  kubernetes:  versions:  - version: 1.12.8  - version: 1.11.10  - version: 1.11.09  - version: 1.10.12  expirationDate: \"\u003cexpiration date in the past\u003e\" Related Documentation You might want to read about the Shoot Updates and Upgrades procedures to get to know the effects of such operations.\n","categories":"","description":"","excerpt":"Shoot Kubernetes and Operating System Versioning in Gardener …","ref":"/docs/gardener/usage/shoot_versions/","tags":"","title":"Shoot Versions"},{"body":"Shoot resource customization webhooks Gardener deploys several components/resources into the shoot cluster. Some of these resources are essential (like the kube-proxy), others are optional addons (like the kubernetes-dashboard or the nginx-ingress-controller). In either case, some provider extensions might need to mutate these resources and inject provider-specific bits into it.\nWhat’s the approach to implement such mutations? Similar to how control plane components in the seed are modified we are using MutatingWebhookConfigurations to achieve the same for resources in the shoot. Both, the provider extension and the kube-apiserver of the shoot cluster are running in the same seed. Consequently, the kube-apiserver can talk cluster-internally to the provider extension webhook which makes such operations even faster.\nHow is the MutatingWebhookConfiguration object created in the shoot? The preferred approach is to use a ManagedResource (see also this document) in the seed cluster. This way the gardener-resource-manager ensures that end-users cannot delete/modify the webhook configuration. The provider extension doesn’t need to care about the same.\nWhat else is needed? The shoot’s kube-apiserver must be allowed to talk to the provider extension. To achieve this you need to create a NetworkPolicy in the shoot namespace. Our extension controller library provides easy-to-use utilities and hooks to implement such a webhook. Please find an exemplary implementation here and here.\n","categories":"","description":"","excerpt":"Shoot resource customization webhooks Gardener deploys several …","ref":"/docs/gardener/extensions/shoot-webhooks/","tags":"","title":"Shoot Webhooks"},{"body":"Shortcodes are the Hugo way to extend the limitations of Markdown before resorting to HTML. There are a number of built-in shortcodes available from Hugo. This list is extended with Gardener website shortcodes designed specifically for its content. Find a complete reference to the Hugo built-in shortcodes on its website.\nBelow is a reference to the shortcodes developed for the Gardener website.\nalert {{% alert color=\"info\" title=\"Notice\" %}} text {{% /alert %}} produces Notice A notice disclaimer  All the color options are info|warning|primary\nYou can also omit the title section from an alert, useful when creating notes.\nIt is important to note that the text that the “alerts” shortcode wraps will not be processed during site building. Do not use shortcodes in it.\nYou should also avoid mixing HTML and markdown formatting in shortcodes, since it won’t render correctly when the site is built.\nAlert Examples Info color  Warning color  Primary color  mermaid The GitHub mermaid fenced code block syntax is used. You can find additional documentation at mermaid’s official website.\n```mermaid graph LR; A[Hard edge] --\u003e|Link text| B(Round edge) B --\u003e C{Decision} C --\u003e|One| D[Result one] C --\u003e|Two| E[Result two] ``` produces:\ngraph LR; A[Hard edge] --\u003e|Link text| B(Round edge) B --\u003e C{Decision} C --\u003e|One| D[Result one] C --\u003e|Two| E[Result two] Default settings can be overridden using the %%init%% header at the start of the diagram definition. See the mermaid theming documentation.\n```mermaid %%{init: {'theme': 'neutral', 'themeVariables': { 'mainBkg': '#eee'}}}%% graph LR; A[Hard edge] --\u003e|Link text| B(Round edge) B --\u003e C{Decision} C --\u003e|One| D[Result one] C --\u003e|Two| E[Result two] ``` produces:\n%%{init: {'theme': 'neutral', 'themeVariables': { 'mainBkg': '#eee'}}}%% graph LR; A[Hard edge] --\u003e|Link text| B(Round edge) B --\u003e C{Decision} C --\u003e|One| D[Result one] C --\u003e|Two| E[Result two] ","categories":"","description":"","excerpt":"Shortcodes are the Hugo way to extend the limitations of Markdown …","ref":"/docs/contribute/20_documentation/30_shortcodes/","tags":"","title":"Shortcodes"},{"body":"Problem If you commit sensitive data, such as a kubeconfig.yaml or SSH key into a Git repository, you can remove it from the history. To entirely remove unwanted files from a repository’s history you can use the git filter-branch command.\nThe git filter-branch command rewrite your repository’s history, which changes the SHAs for existing commits that you alter and any dependent commits. Changed commit SHAs may affect open pull requests in your repository. I recommend merging or closing all open pull requests before removing files from your repository.\n Warning: - if someone has already checked out the repository, then of course he has the secret on his computer. So ALWAYS revoke the OAuthToken/Password or whatever it was imediately.\n Purging a file from your repository’s history  Warning: If you run git filter-branch after stashing changes, you won’t be able to retrieve your changes with other stash commands. Before running git filter-branch, we recommend unstashing any changes you’ve made. To unstash the last set of changes you’ve stashed, run git stash show -p | git apply -R. For more information, see Git Tools Stashing.\n To illustrate how git filter-branch works, we’ll show you how to remove your file with sensitive data from the history of your repository and add it to .gitignore to ensure that it is not accidentally re-committed.\nNavigate into the repository’s working directory.\ncd YOUR-REPOSITORY Run the following command, replacing PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA with the path to the file you want to remove, not just its filename.\nThese arguments will:\n Force Git to process, but not check out, the entire history of every branch and tag Remove the specified file, as well as any empty commits generated as a result Overwrite your existing tags  git filter-branch --force --index-filter \\ 'git rm --cached --ignore-unmatch PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA' \\ --prune-empty --tag-name-filter cat -- --all Add your file with sensitive data to .gitignore to ensure that you don’t accidentally commit it again.\n echo \"YOUR-FILE-WITH-SENSITIVE-DATA\" \u003e\u003e .gitignore Double-check that you’ve removed everything you wanted to from your repository’s history, and that all of your branches are checked out.\nOnce you’re happy with the state of your repository, force-push your local changes to overwrite your GitHub repository, as well as all the branches you’ve pushed up:\ngit push origin --force --all In order to remove the sensitive file from your tagged releases, you’ll also need to force-push against your Git tags:\ngit push origin --force --tags  Warning: Tell your collaborators to rebase, not merge, any branches they created off of your old (tainted) repository history. One merge commit could reintroduce some or all of the tainted history that you just went to the trouble of purging.\n References:\n https://help.github.com/articles/removing-sensitive-data-from-a-repository/   blockquote { border:1px solid red; padding:10px; margin-top:40px; margin-bottom:40px; } blockquote p { font-size: 1.5rem; color: black; }  ","categories":"","description":"Never ever commit a kubeconfig.yaml into github","excerpt":"Never ever commit a kubeconfig.yaml into github","ref":"/docs/guides/applications/commit_secret_fail/","tags":"","title":"Storing Secrets in git 💀"},{"body":"This page gives writing style guidelines for the Gardener documentation. For formatting guidelines, see the Formatting Guide.\nThese are guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.\n Structure Language and Grammar  Structure Documentation Types Overview The following table summarizes the types of documentation and their mapping to the SAP UA taxonomy. Every topic you create will fall into one of these categories.\n   Gardener Content Type Definition Example Content Comparable UA Content Type     Concept Introduce a functionality or concept; covers background information. Services Overview, Relevant headings Concept   Reference Provide a reference, for example, list all command line options of gardenctl and what they are used for. Overview of kubectl Relevant headings Reference   Task A step-by-step description that allows users to complete a specific task. Upgrading kubeadm clusters Overview, Prerequisites, Steps, Result Complex Task   Trail Collection of all other content types to cover a big topic. Custom Networking None Maps   Tutorial A combination of many tasks that allows users to complete an example task with the goal to learn the details of a given feature. Deploying Cassandra with a StatefulSet Overview, Prerequisites, Tasks, Result Tutorial    See the Contributors Guide for more details on how to produce and contribute documentation.\nTopic Structure When creating a topic, you will need to follow a certain structure. A topic generally comprises of, in order:\n  Metadata (Specific for .md files in Gardener) - Additional information about the topic.\n  Title - A short, descriptive name for the topic.\n  Content - The main part of the topic. It contains all the information relevant to the user.\n Concept content: Overview, Relevant headings Task content: Overview, Prerequisites, Steps, Result Reference content: Relevant headings    Related Links (Optional) - A part after the main content that contains links that are not a part of the topic, but are still connected to it.\n  You can use the provided content description files as a template for your own topics.\nMetadata Metadata is information housed in a specific section of the .md files used for the Gardener documentation. It can contain:\n Title - A short, descriptive name for the topic. Description - A succint summary of the topic’s content. Must not include the title or repeat content from the topic. Other elements such as weight, creation date, author and tags.  Sample codeblock:\n--- Title: Description: Weight: --- While this section will be automatically generated if your topic has a title header, adding more detailed information helps other users, developers and technical writers better sort, classify and understand the topic.\nBy using a metadata section you can also skip adding a title header or overwrite it in the navigation section.\nGeneral Tips  Try to create a succint title and an informative description for your topics. If a topic feels too long, it might be better to split it into a few different ones. Avoid having have more than ten steps in one a task topic. When writing a tutorial, link the tasks used in it instead of copying their content.  Language and Grammar Language  Gardener documentation uses US English. Keep it simple and use words that non-native English speakers are also familiar with. Use the Merriam-Webster Dictionary when checking the spelling of words.  Writing Style  Write in a conversational manner and use simple present tense. Be friendly and refer to the person reading your content as “you”, instead of standard terms such as “user”. Use an active voice - make it clear who is performing the action.  Creating Titles and Headers  Use title case when creating titles or headers. Avoid adding additional formatting to the title or header. Concept and reference topic titles should be simple and succint. Task and tutorial topic titles begin with a verb.  Related links  Formatting Guide Contributors Guide SAPterm  ","categories":"","description":"","excerpt":"This page gives writing style guidelines for the Gardener …","ref":"/docs/contribute/20_documentation/40_style_guide/","tags":"","title":"Style Guide"},{"body":"Supported Kubernetes Versions Currently, the Gardener supports the following Kubernetes versions:\nGarden cluster version The minimum version of the garden cluster that can be used to run Gardener is 1.20.x.\nSeed cluster versions The minimum version of a seed cluster that can be connected to Gardener is 1.20.x. Please note that Gardener does not support 1.25 seeds yet.\nShoot cluster versions Gardener itself is capable of spinning up clusters with Kubernetes versions 1.17 up to 1.25. However, the concrete versions that can be used for shoot clusters depend on the installed provider extension. Consequently, please consult the documentation of your provider extension to see which Kubernetes versions are supported for shoot clusters.\n 👨🏼‍💻 Developers note: This document explains what needs to be done in order to add support for a new Kubernetes version.\n ","categories":"","description":"","excerpt":"Supported Kubernetes Versions Currently, the Gardener supports the …","ref":"/docs/gardener/usage/supported_k8s_versions/","tags":"","title":"Supported K8s Versions"},{"body":"Gardener Extension for SUSE CHost  \nThis controller operates on the OperatingSystemConfig resource in the extensions.gardener.cloud/v1alpha1 API group. It manages those objects that are requesting SUSE Container Host configuration, i.e. suse-chost type:\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: OperatingSystemConfig metadata:  name: pool-01-original  namespace: default spec:  type: suse-chost  units:  ...  files:  ... Please find a concrete example in the example folder.\nIt is also capable of supporting the vSMP MemoryOne operating system with the memoryone-chost type. Please find more information here.\nAfter reconciliation the resulting data will be stored in a secret within the same namespace (as the config itself might contain confidential data). The name of the secret will be written into the resource’s .status field:\n... status:  ...  cloudConfig:  secretRef:  name: osc-result-pool-01-original  namespace: default  command: /usr/bin/env bash \u003cpath\u003e  units:  - docker-monitor.service  - kubelet-monitor.service  - kubelet.service The secret has one data key cloud_config that stores the generation.\nAn example for a ControllerRegistration resource that can be used to register this controller to Gardener can be found here.\nThis controller is implemented using the oscommon library for operating system configuration controllers.\nPlease find more information regarding the extensibility concepts and a detailed proposal here.\n How to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the ./dev/kubeconfig file. Static code checks and tests can be executed by running make verify. We are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io Gardener Extensions Golang library GEP-1 (Gardener Enhancement Proposal) on extensibility Extensibility API documentation  ","categories":"","description":"Gardener extension controller for the SUSE Container Host operating system (CHost)","excerpt":"Gardener extension controller for the SUSE Container Host operating …","ref":"/docs/extensions/os-extensions/gardener-extension-os-suse-chost/","tags":"","title":"SUSE CHost OS"},{"body":"Problem One thing that always bothered me was that I couldn’t get logs of several pods at once with kubectl. A simple tail -f \u003cpath-to-logfile\u003e isn’t possible at all. Certainly you can use kubectl logs -f \u003cpod-id\u003e, but it doesn’t help if you want to monitor more than one pod at a time.\nThis is something you really need a lot, at least if you run several instances of a pod behind a deployment. This is even more so if you don’t have a Kibana setup or similar.\nSolution Luckily, there are smart developers out there who always come up with solutions. The finding of the week is a small bash script that allows you to aggregate log files of several pods at the same time in a simple way. The script is called kubetail and is available at GitHub.\n","categories":"","description":"Aggregate log files from different pods","excerpt":"Aggregate log files from different pods","ref":"/docs/guides/monitoring_and_troubleshooting/tail-logfile/","tags":"","title":"tail -f /var/log/my-application.log"},{"body":"Create Tanzu Cluster For gardener a Tanzu Kubernetes „guest” cluster is used. Look here for the vSphere documentation Provisioning Tanzu Kubernetes Clusters\nVirtual Machine Classes For gardener the minimum Virtual Machine Classes must set to best-effort-large.\nNetwork Settings For the deployment it is possible to provision the cluster with a minimal amount of configuration parameter. It is recommended to set the parameter Default Pod CIDR, Default Services CIDR with values which fit to your enviroment.\nStorage Class settings The storageClass Parameter should be defined to avoid problems during deployment.\nExample:\n```yaml apiVersion: run.tanzu.vmware.com/v1alpha1 #TKG API endpoint kind: TanzuKubernetesCluster #required parameter metadata: name: tkg-cluster-1 #cluster name, user defined namespace: ns1 #supervisor namespace spec: distribution: version: v1.17\t#resolved kubernetes version topology: controlPlane: count: 1 #number of control plane nodes class: best-effort-small #vmclass for control plane nodes storageClass: vsan-default-storage-policy #storageclass for control plane workers: count: 3 #number of worker nodes class: best-effort-large #vmclass for worker nodes storageClass: vsan-default-storage-policy #storageclass for worker nodes settings: network: cni: name: calico services: cidrBlocks: [\"198.51.100.0/12\"] #Cannot overlap with Supervisor Cluster pods: cidrBlocks: [\"192.0.2.0/16\"] #Cannot overlap with Supervisor Cluster ``` ","categories":"","description":"","excerpt":"Create Tanzu Cluster For gardener a Tanzu Kubernetes „guest” cluster …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/docs/tanzu-vsphere/","tags":"","title":"Tanzu Vsphere"},{"body":"Task Title Overview This section provides an overview of the topic and the information provided in it.\nPrerequisites  Prerequisite 1 Prerequisite 2  Steps  Describe step 1 here Describe step 2 here  Result Screenshot of the final status once all the steps have been followed.\nRelated Links  Link 1 Link 2  ","categories":"","description":"Describes the contents of a task topic","excerpt":"Describes the contents of a task topic","ref":"/docs/contribute/20_documentation/40_style_guide/task_template/","tags":"","title":"Task Topic Structure"},{"body":"Terminal Shortcuts As user and/or gardener administrator you can configure terminal shortcuts, which are preconfigured terminals for frequently used views.\nYou can launch the terminal shortcuts directly on the shoot details screen. You can view the definition of a terminal terminal shortcut by clicking on they eye icon What also has improved is, that when creating a new terminal you can directly alter the configuration. With expanded configuration On the Create Terminal Session dialog you can choose one or multiple terminal shortcuts. Project specific terminal shortcuts created (by a member of the project) have a project icon badge and are listed as Unverified. A warning message is displayed before a project specific terminal shortcut is ran informing the user about the risks. How to create a project specific terminal shortcut\nDisclaimer: “Project specific terminal shortcuts” is experimental feature and may change in future releases (we plan to introduce a dedicated custom resource).\nYou need to create a secret with the name terminal.shortcuts within your project namespace, containing your terminal shortcut configurations. Under data.shortcuts you add a list of terminal shortcuts (base64 encoded). Example terminal.shortcuts secret:\nkind: Secret type: Opaque metadata:  name: terminal.shortcuts  namespace: garden-myproject apiVersion: v1 data:  shortcuts: LS0tCi0gdGl0bGU6IE5ldHdvcmtEZWxheVRlc3RzCiAgZGVzY3JpcHRpb246IFNob3cgbmV0d29ya21hY2hpbmVyeS5pbydzIE5ldHdvcmtEZWxheVRlc3RzCiAgdGFyZ2V0OiBzaG9vdAogIGNvbnRhaW5lcjoKICAgIGltYWdlOiBxdWF5LmlvL2RlcmFpbGVkL2s5czpsYXRlc3QKICAgIGFyZ3M6CiAgICAtIC0taGVhZGxlc3MKICAgIC0gLS1jb21tYW5kPW5ldHdvcmtkZWxheXRlc3QKICBzaG9vdFNlbGVjdG9yOgogICAgbWF0Y2hMYWJlbHM6CiAgICAgIGZvbzogYmFyCi0gdGl0bGU6IFNjYW4gQ2x1c3RlcgogIGRlc2NyaXB0aW9uOiBTY2FucyBsaXZlIEt1YmVybmV0ZXMgY2x1c3RlciBhbmQgcmVwb3J0cyBwb3RlbnRpYWwgaXNzdWVzIHdpdGggZGVwbG95ZWQgcmVzb3VyY2VzIGFuZCBjb25maWd1cmF0aW9ucwogIHRhcmdldDogc2hvb3QKICBjb250YWluZXI6CiAgICBpbWFnZTogcXVheS5pby9kZXJhaWxlZC9rOXM6bGF0ZXN0CiAgICBhcmdzOgogICAgLSAtLWhlYWRsZXNzCiAgICAtIC0tY29tbWFuZD1wb3BleWU= How to configure the dashboard with terminal shortcuts Example values.yaml:\nfrontend:  features:  terminalEnabled: true  projectTerminalShortcutsEnabled: true # members can create a `terminal.shortcuts` secret containing the project specific terminal shortcuts  terminal:  shortcuts:  - title: \"Control Plane Pods\"  description: Using K9s to view the pods of the control plane for this cluster  target: cp  container:  image: quay.io/derailed/k9s:latest  - \"--headless\"  - \"--command=pods\"  - title: \"Cluster Overview\"  description: This gives a quick overview about the status of your cluster using K9s pulse feature  target: shoot  container:  image: quay.io/derailed/k9s:latest  args:  - \"--headless\"  - \"--command=pulses\"  - title: \"Nodes\"  description: View the nodes for this cluster  target: shoot  container:  image: quay.io/derailed/k9s:latest  command:  - bin/sh  args:  - -c  - sleep 1 \u0026\u0026 while true; do k9s --headless --command=nodes; done # shootSelector: # matchLabels: # foo: bar [...] terminal: # is generally required for the terminal feature  container:  image: eu.gcr.io/gardener-project/gardener/ops-toolbelt:0.10.0  containerImageDescriptions:  - image: /.*/ops-toolbelt:.*/  description: Run `ghelp` to get information about installed tools and packages  gardenTerminalHost:  seedRef: my-soil  garden:  operatorCredentials:  serviceAccountRef:  name: dashboard-terminal-admin  namespace: garden  bootstrap:  disabled: false  shootDisabled: false  seedDisabled: false  gardenTerminalHostDisabled: true  apiServerIngress:  annotations:  cert.gardener.cloud/purpose: managed  kubernetes.io/ingress.class: nginx  nginx.ingress.kubernetes.io/backend-protocol: HTTPS ","categories":"","description":"","excerpt":"Terminal Shortcuts As user and/or gardener administrator you can …","ref":"/docs/dashboard/usage/terminal-shortcuts/","tags":"","title":"Terminal Shortcuts"},{"body":"Testing Jest We use Jest JavaScript Testing Framework\n Jest can collect code coverage information​ Jest support snapshot testing out of the box​ All in One solution. Replaces Mocha, Chai, Sinon and Istanbul​ It works with Vue.js and Node.js projects​  To execute all tests, simply run\nyarn workspaces foreach --all run test or to include test coverage generation\nyarn workspaces foreach --all run test-coverage You can also run tests for frontend, backend and charts directly inside the respective folder via\nyarn test Lint We use ESLint for static code analyzing.\nTo execute, run\nyarn workspaces foreach --all run lint ","categories":"","description":"","excerpt":"Testing Jest We use Jest JavaScript Testing Framework\n Jest can …","ref":"/docs/dashboard/development/testing/","tags":"","title":"Testing"},{"body":"Testing Strategy and Developer Guideline This document walks you through\n what kind of tests we have in Gardener how to run each of them what purpose each kind of test serves how to best write tests that are correct, stable, fast and maintainable how to debug tests that are not working as expected  The document is aimed towards developers that want to contribute code and need to write tests, as well as maintainers and reviewers that review test code. It serves as a common guide that we commit to follow in our project to ensure consistency in our tests, good coverage for high confidence and good maintainability.\nThe guidelines are not meant to be absolute rules. Always apply common sense and adapt the guideline if it doesn’t make much sense for some cases. If in doubt, don’t hesitate to ask questions during PR review (as an author but also as a reviewer). Add new learnings as soon as we make them!\nGenerally speaking, tests are a strict requirement for contributing new code. If you touch code that is currently untested, you need to add tests for the new cases that you introduce as a minimum. Ideally though, you would add the missing test cases for the current code as well (boy scout rule – “always leave the campground cleaner than you found it”).\nWriting Tests (Relevant for All Kinds)  we follow BDD (behavior-driven development) testing principles and use Ginkgo along with Gomega  make sure to check out their extensive guides for more information and how to best leverage all of their features   use By to structure test cases with multiple steps, so that steps are easy to follow in the logs: example test call defer GinkgoRecover() if making assertions in goroutines: doc, example test use DeferCleanup instead of cleaning up manually (or use custom coding from the test framework): example test, example test  DeferCleanup makes sure to run the cleanup code in the right point in time, e.g., a DeferCleanup added in BeforeEach is executed with AfterEach   test failures should point to an exact location, so that failures in CI aren’t too difficult to debug/fix  use ExpectWithOffset for making assertions in helper funcs like expectSomethingWasCreated: example test make sure to add additional descriptions to Gomega matchers if necessary (e.g. in a loop): example test   introduce helper functions for assertions to make test more readable where applicable: example test introduce custom matchers to make tests more readable where applicable: example matcher don’t rely on accurate timing of time.Sleep and friends  if doing so, CPU throttling in CI will make tests flaky, example flake use fake clocks instead, example PR   use the same client schemes that are also used by production code to avoid subtle bugs/regressions: example PR, production schemes, usage in test make sure, your test is actually asserting the right thing and it doesn’t pass if the exact bug is introduced that you want to prevent  use specific error matchers instead of asserting any error has happened, make sure that the corresponding branch in the code is tested, e.g., prefer Expect(err).To(MatchError(\"foo\")) over Expect(err).To(HaveOccurred())  if you’re unsure about your test’s behavior, attaching the debugger can sometimes be helpful to make sure your test is correct   about overwriting global variables  this is a common pattern (or hack?) in go for faking calls to external functions however, this can lead to races, when the global variable is used from a goroutine (e.g., the function is called) alternatively, set fields on structs (passed via parameter or set directly): this is not racy, as struct values are typically (and should be) only used for a single test case alternative to dealing with function variables and fields:  add an interface, which your code depends on write a fake and a real implementation (similar to clock.Clock.Sleep) the real implementation calls the actual function (clock.RealClock.Sleep calls time.Sleep) the fake implementation does whatever you want it to do for your test (clock.FakeClock.Sleep waits until the test code advanced the time)     use constants in test code with care  typically, you should not use constants from the same package as the tested code, instead use literals if the constant value is changed, tests using the constant will still pass, although the “specification” is not fulfilled anymore there are cases where it’s fine to use constants, but keep this caveat in mind when doing so   creating sample data for tests can be a high effort  if valuable, add a package for generating common sample data, e.g. Shoot/Cluster objects   make use of the testdata directory for storing arbitrary sample data needed by tests (helm charts, YAML manifests, etc.), example PR  From https://pkg.go.dev/cmd/go/internal/test:  The go tool will ignore a directory named “testdata”, making it available to hold ancillary data needed by the tests.\n     Unit Tests Running Unit Tests Run all unit tests:\nmake test Run all unit tests with test coverage:\nmake test-cov open test.coverage.html make test-cov-clean Run unit tests of specific packages:\n# run with same settings like in CI (race dector, timeout, ...) ./hack/test.sh ./pkg/resourcemanager/controller/... ./pkg/utils/secrets/...  # freestyle go test ./pkg/resourcemanager/controller/... ./pkg/utils/secrets/... ginkgo run ./pkg/resourcemanager/controller/... ./pkg/utils/secrets/... Debugging Unit Tests Use ginkgo to focus on (a set of) test specs via code or via CLI flags. Remember to unfocus specs before contributing code, otherwise your PR tests will fail.\n$ ginkgo run --focus \"should delete the unused resources\" ./pkg/resourcemanager/controller/garbagecollector ... Will run 1 of 3 specs SS•  Ran 1 of 3 Specs in 0.003 seconds SUCCESS! -- 1 Passed | 0 Failed | 0 Pending | 2 Skipped PASS Use ginkgo to run tests until they fail:\n$ ginkgo run --until-it-fails ./pkg/resourcemanager/controller/garbagecollector ... Ran 3 of 3 Specs in 0.004 seconds SUCCESS! -- 3 Passed | 0 Failed | 0 Pending | 0 Skipped PASS  All tests passed... Will keep running them until they fail. This was attempt #58 No, seriously... you can probably stop now. Use the stress tool for deflaking tests that fail sporadically in CI, e.g., due resource contention (CPU throttling):\n# get the stress tool go install golang.org/x/tools/cmd/stress@latest  # build a test binary ginkgo build ./pkg/resourcemanager/controller/garbagecollector # alternatively go test -c ./pkg/resourcemanager/controller/garbagecollector  # run the test in parallel and report any failures $ stress -p 16 ./pkg/resourcemanager/controller/garbagecollector/garbagecollector.test -ginkgo.focus \"should delete the unused resources\" 5s: 1077 runs so far, 0 failures 10s: 2160 runs so far, 0 failures stress will output a path to a file containing the full failure message, when a test run fails.\nPurpose of Unit Tests  unit tests prove correctness of a single unit according to the specification of its interface  think: is the unit that I introduced doing what it is supposed to do for all cases?   unit tests protect against regressions caused by adding new functionality to or refactoring of a single unit  think: is the unit that was introduced earlier (by someone else) and that I changed still doing what it was supposed to do for all cases?   example units: functions (conversion, defaulting, validation, helpers), structs (helpers, basic building blocks like the Secrets Manager), predicates, event handlers for these purposes, unit tests need to cover all important cases of input for a single unit and cover edge cases / negative paths as well (e.g., errors)  because of the possible high dimensionality of test input, unit tests need to be fast to execute: individual test cases should not take more than a few seconds, test suites not more than 2 minutes fuzzing can be used as a technique in addition to usual test cases for covering edge cases   test coverage can be used as a tool during test development for covering all cases of a unit however, test coverage data can be a false safety net  full line coverage doesn’t mean you have covered all cases of valid input we don’t have strict requirements for test coverage, as it doesn’t necessarily yield the desired outcome   unit tests should not test too large components, e.g. entire controller Reconcile functions  if a function/component does many steps, it’s probably better to split it up into multiple functions/components that can be unit tested individually there might be special cases for very small Reconcile functions if there are a lot of edge cases, extract dedicated functions that cover them and use unit tests to test them usual-sized controllers should rather be tested in integration tests individual parts (e.g. helper functions) should still be tested in unit test for covering all cases, though   unit tests are especially easy to run with a debugger and can help in understanding concrete behavior of components  Writing Unit Tests  for the sake of execution speed, fake expensive calls/operations, e.g. secret generation: example test generally, prefer fakes over mocks, e.g., use controller-runtime fake client over mock clients  mocks decrease maintainability because they expect the tested component to follow a certain way to reach the desired goal (e.g., call specific functions with particular arguments), example consequence generally, fakes should be used in “result-oriented” test code (e.g., that a certain object was labelled, but the test doesn’t care if it was via patch or update as both a valid ways to reach the desired goal) although rare, there are valid use cases for mocks, e.g. if the following aspects are important for correctness:  asserting that an exact function is called asserting that functions are called in a specific order asserting that exact parameters/values/… are passed asserting that a certain function was not called many of these can also be verified with fakes, although mocks might be simpler   only use mocks if the tested code directly calls the mock; never if the tested code only calls the mock indirectly (e.g., through a helper package/function) keep in mind the maintenance implications of using mocks:  can you make a valid non-behavioral change in the code without breaking the test or dependent tests?   it’s valid to mix fakes and mocks in the same test or between test cases   generally, use the go test package, i.e., declare package \u003cproduction_package\u003e_test  helps in avoiding cyclic dependencies between production, test and helper packages also forces you to distinguish between the public (exported) API surface of your code and internal state that might not be of interest to tests it might be valid to use the same package as the tested code if you want to test unexported functions  alternatively, an internal package can be used to host “internal” helpers: example package   helpers can also be exported if no one is supposed to import the containing package (e.g. controller package)    Integration Tests (envtests) Integration tests in Gardener use the sigs.k8s.io/controller-runtime/pkg/envtest package. It sets up a temporary control plane (etcd + kube-apiserver) and runs the test against it.\nPackage github.com/gardener/gardener/pkg/envtest augments controller-runtime’s envtest package by starting and registering gardener-apiserver. This is used to test controllers that act on resources in the Gardener APIs (aggregated APIs).\nHistorically, test machinery tests have also been called “integration tests”. However, test machinery does not perform integration testing but rather executes a form of end-to-end tests against a real landscape. Hence, we tried to sharpen the terminology that we use to distinguish between “real” integration tests and test machinery tests but you might still find “integration tests” referring to test machinery tests in old issues or outdated documents.\nRunning Integration Tests The test-integration make rule prepares the environment automatically by downloading the respective binaries (if not yet present) and sets the necessary environment variables.\nmake test-integration If you want to run a specific set of integration tests, you can also execute them using ./hack/test-integration.sh directly instead of using the test-integration rule. For example:\n./hack/test-integration.sh ./test/integration/resourcemanager/tokenrequestor The script takes care of preparing the environment for you. If you want to execute the test suites directly via go test or ginkgo, you have to point the KUBEBUILDER_ASSETS environment variable to the path that contains the etcd and kube-apiserver binaries. Alternatively, you can install the binaries to /usr/local/kubebuilder/bin.\nDebugging Integration Tests You can configure envtest to use an existing cluster instead of starting a temporary control plane for your test. This can be helpful for debugging integration tests, because you can easily inspect what is going on in your test cluster with kubectl.\nRun an envtest suite (not using gardener-apiserver) against an existing cluster:\nmake kind-up export KUBECONFIG=$PWD/example/gardener-local/kind/kubeconfig export USE_EXISTING_CLUSTER=true  # run test with verbose output ./hack/test-integration.sh -v ./test/integration/resourcemanager/health -ginkgo.v  # watch test objects k get managedresource -A -w Run a gardenerenvtest suite (using gardener-apiserver) against an existing gardener setup:\nmake kind-up export KUBECONFIG=$PWD/example/gardener-local/kind/kubeconfig make dev-setup # you might need to disable some admission plugins in hack/local-development/start-apiserver # via --disable-admission-plugins depending on the test suite make start-apiserver export USE_EXISTING_GARDENER=true  # run test with verbose output ./hack/test-integration.sh -v ./test/integration/controllermanager/bastion -ginkgo.v  # watch test objects k get bastion -A -w Similar to debugging unit tests, the stress tool can help hunting flakes in integration tests. Though, you might need to run less tests in parallel though (specified via -p) and have a bit more patience. Generally, reproducing flakes in integration tests is easier when stress-testing against an existing cluster instead of starting temporary individual control planes per test run.\nStress-test an envtest suite (not using gardener-apiserver):\n# build a test binary ginkgo build ./test/integration/resourcemanager/health  # prepare a cluster to run the test against make kind-up export KUBECONFIG=$PWD/example/gardener-local/kind/kubeconfig export USE_EXISTING_CLUSTER=true  # use same timeout settings like in CI source ./hack/test-integration.env  # run the test in parallel and report any failures $ stress -ignore \"unable to grab random port\" -p 16 ./test/integration/resourcemanager/health/health.test ... Stress-test a gardenerenvtest suite (using gardener-apiserver):\n# build a test binary ginkgo build ./test/integration/controllermanager/bastion  # prepare a cluster including gardener-apiserver to run the test against make kind-up export KUBECONFIG=$PWD/example/gardener-local/kind/kubeconfig make dev-setup # you might need to disable some admission plugins in hack/local-development/start-apiserver # via --disable-admission-plugins depending on the test suite # especially the ResourceQuota plugin can quickly lead to test failures when stress-testing make start-apiserver export USE_EXISTING_GARDENER=true  # use same timeout settings like in CI source ./hack/test-integration.env  # run the test in parallel and report any failures $ stress -ignore \"unable to grab random port\" -p 16 ./test/integration/controllermanager/bastion/bastion.test ... Purpose of Integration Tests  integration tests prove that multiple units are correctly integrated into a fully-functional component of the system example components with multiple units:  a controller with its reconciler, watches, predicates, event handlers, queues, etc. a webhook with its server, handler, decoder and webhook configuration   integration tests set up a full component (including used libraries) and run it against a test environment close to the actual setup  e.g., start controllers against a real Kubernetes control plane to catch bugs that can only happen when talking to a real API server integration tests are generally more expensive to run (e.g., in terms of execution time)   integration tests should not cover each and every detailed case  rather cover a good portion of the “usual” cases that components will face during normal operation (positive and negative test cases) but don’t cover all failure cases or all cases of predicates -\u003e they should be covered in unit tests already generally, not supposed to “generate test coverage” but to provide confidence that components work well   as integration tests typically test only one component (or a cohesive set of components) isolated from others, they cannot catch bugs that occur when multiple controllers interact (could be discovered by e2e tests, though) rule of thumb: a new integration tests should be added for each new controller (an integration test doesn’t replace unit tests though)  Writing Integration Tests  make sure to have a clean test environment on both test suite and test case level:  set up dedicated test environments (envtest instances) per test suite use dedicated namespaces per test suite  use GenerateName with a test-specific prefix: example test restrict the controller-runtime manager to the test namespace by setting manager.Options.Namespace: example test alternatively, use a test-specific prefix with a random suffix determined upfront: example test  this can be used to restrict webhooks to a dedicated test namespace: example test   this allows running a test in parallel against the same existing cluster for deflaking and stress testing: example PR   if the controller works on cluster-scoped resources  label the resources with a label specific to the test run, e.g. the test namespace’s name: example test restrict the manager’s cache for these objects with a corresponding label selector: example test alternatively, use a default label selector for all objects in the manager’s cache: example test this allows running a test in parallel against the same existing cluster for deflaking and stress testing, even if it works with cluster-scoped resources that are visible to all parallel test runs: example PR   use dedicated test resources for each test case  use GenerateName: example test alternatively, use a checksum of CurrentSpecReport().LeafNodeLocation.String(): example test logging the created object names is generally a good idea to support debugging failing or flaky tests: example test always delete all resources after the test case (e.g., via DeferCleanup) that were created for the test case this avoids conflicts between test cases and cascading failures which distract from the actual root failures   don’t tolerate already existing resources (~dirty test environment), code smell: ignoring already exist errors   don’t use a cached client in test code (e.g., the one from a controller-runtime manager), always construct a dedicated test client (uncached): example test use asynchronous assertions: Eventually and Consistently  never Expect anything to happen synchronously (immediately) don’t use retry or wait until functions -\u003e use Eventually, Consistently instead: example test this allows to override the interval/timeout values from outside instead of hard-coding this in the test (see hack/test-integration.sh): example PR beware of the default Eventually / Consistently timeouts / poll intervals: docs don’t set custom (high) timeouts and intervals in test code: example PR  instead, shorten sync period of controllers, overwrite intervals of the tested code, or use fake clocks: example test   pass g Gomega to Eventually/Consistently and use g.Expect in it: docs, example test, example PR don’t forget to call {Eventually,Consistently}.Should(), otherwise the assertions always silently succeeds without errors: onsi/gomega#561   when using Gardener’s envtest (envtest.GardenerTestEnvironment):  disable gardener-apiserver’s admission plugins that are not relevant to the integration test itself by passing --disable-admission-plugins: example test this makes setup / teardown code simpler and ensures to only test code relevant to the tested component itself (but not the entire set of admission plugins) e.g., you can disable the ShootValidator plugin to create Shoots that reference non-existing SecretBindings or disable the DeletionConfirmation plugin to delete Gardener resources without adding a deletion confirmation first.   use a custom rate limiter for controllers in integration tests: example test  this can be used for limiting exponential backoff to shorten wait times otherwise, if using the default rate limiter, exponential backoff might exceed the timeout of Eventually calls and cause flakes    End-to-end (e2e) Tests (using provider-local) We run a suite of e2e tests on every pull request and periodically on the master branch. It uses a KinD cluster and skaffold to boostrap a full installation of Gardener based on the current revision, including provider-local. This allows us to run e2e tests in an isolated test environment and fully locally without any infrastructure interaction. The tests perform a set of operations on Shoot clusters, e.g. creating, deleting, hibernating and waking up.\nThese tests are executed in our prow instance at prow.gardener.cloud, see job definition and job history.\nRunning e2e Tests You can also run these tests on your development machine, using the following commands:\nmake kind-up export KUBECONFIG=$PWD/example/gardener-local/kind/kubeconfig make gardener-up make test-e2e-local # alternatively: make test-e2e-local-simple If you want to run a specific set of e2e test cases, you can also execute them using ./hack/test-e2e-local.sh directly in combination with ginkgo label filters. For example:\n./hack/test-e2e-local.sh --label-filter \"Shoot \u0026\u0026 credentials-rotation\" If you want to use an existing shoot instead of creating a new one for the test case and deleting it afterwards, you can specify the existing shoot via the following flags. This can be useful to speed of the development of e2e tests.\n./hack/test-e2e-local.sh --label-filter \"Shoot \u0026\u0026 credentials-rotation\" -- --project-namespace=garden-local --existing-shoot-name=local Also see: developing Gardener locally and deploying Gardener locally.\nDebugging e2e Tests When debugging e2e test failures in CI, logs of the cluster components can be very helpful. Our e2e test jobs export logs of all containers running in the kind cluster to prow’s artifacts storage. You can find them by clicking the Artifacts link in the top bar in prow’s job view and navigating to artifacts. This directory will contain all cluster component logs grouped by node.\nPull all artifacts using gsutil for searching and filtering the logs locally (use the path displayed in the artifacts view):\ngsutil cp -r gs://gardener-prow/pr-logs/pull/gardener_gardener/6136/pull-gardener-e2e-kind/1542030416616099840/artifacts/gardener-local-control-plane /tmp Purpose of e2e Tests  e2e tests provide a high level of confidence that our code runs as expected by users when deployed to production they are supposed to catch bugs resulting from interaction between multiple components test cases should be as close as possible to real usage by endusers  should test “from the perspective of the user” (or operator) example: I create a Shoot and expect to be able to connect to it via the provided kubeconfig accordingly, don’t assert details of the system  e.g., the user also wouldn’t expect that there is a kube-apiserver deployment in the seed, they rather expect that they can talk to it no matter how it is deployed only assert details of the system if the tested feature is not fully visible to the end-user and there is no other way of ensuring that the feature works reliably e.g., the Shoot CA rotation is not fully visible to the user but is assertable by looking at the secrets in the Seed.     pro: can be executed by developers and users without any real infrastructure (provider-local) con: they currently cannot be executed with real infrastructure (e.g., provider-aws), we will work on this as part of #6016 keep in mind that the tested scenario is still artificial in a sense of using default configuration, only a few objects, only a few config/settings combinations are covered  we will never be able to cover the full “test matrix” and this should not be our goal bugs will still be released and will still happen in production; we can’t avoid it instead, we should add test cases for preventing bugs in features or settings that were frequently regressed: example PR   usually e2e tests cover the “straight-forward cases”  however, negative test cases can also be included, especially if they are important from the user’s perspective    Writing e2e Tests  always wrap API calls and similar things in Eventually blocks: example test  at this point, we are pretty much working with a distributed system and failures can happen anytime wrapping calls in Eventually makes tests more stable and more realistic (usually, you wouldn’t call the system broken if a single API call fails because of a short connectivity issue)   most of the points from writing integration tests are relevant for e2e tests as well (especially the points about asynchronous assertions) in contrast to integration tests, in e2e tests, it might make sense to specify higher timeouts for Eventually calls, e.g., when waiting for a Shoot to be reconciled  generally, try to use the default settings for Eventually specified via the environment variables only set higher timeouts if waiting for long-running reconciliations to be finished    Test Machinery Tests Please see Test Machinery Tests.\nPurpose of Test Machinery Tests  test machinery tests have to be executed against full-blown Gardener installations they can provide a very high level of confidence that an installation is functional in its current state, this includes: all Gardener components, Extensions, the used Cloud Infrastructure, all relevant settings/configuration this brings the following benefits:  they test more realistic scenarios than e2e tests (real configuration, real infrastructure, etc.) tests run “where the users are”   however, this also brings significant drawbacks:  tests are difficult to develop and maintain tests require a full Gardener installation and cannot be executed in CI (on PR-level or against master) tests require real infrastructure (think cloud provider credentials, cost) using TestDefinitions under .test-defs requires a full test machinery installation accordingly, tests are heavyweight and expensive to run testing against real infrastructure can cause flakes sometimes (e.g., in outage situations) failures are hard to debug, because clusters are deleted after the test (for obvious cost reasons) bugs can only be caught, once it’s “too late”, i.e., when code is merged and deployed   today, test machinery tests cover a bigger “test matrix” (e.g., Shoot creation across infrastructures, kubernetes versions, machine image versions, etc.) test machinery also runs Kubernetes conformance tests however, because of the listed drawbacks, we should rather focus on augmenting our e2e tests, as we can run them locally and in CI in order to catch bugs before they get merged it’s still a good idea to add test machinery tests if a feature needs to be tested that is depending on some installation-specific configuration  Writing Test Machinery Tests  generally speaking, most points from writing integration tests and writing e2e tests apply here as well however, test machinery tests contain a lot of technical debt and existing code doesn’t follow these best practices as test machinery tests are out of our general focus, we don’t intend on reworking the tests soon or providing more guidance on how to write new ones  Manual Tests  manual tests can be useful when the cost of trying to automatically test certain functionality are too high useful for PR verification, if a reviewer wants to verify that all cases are properly tested by automated tests currently, it’s the simplest option for testing upgrade scenarios  e.g. migration coding is probably best tested manually, as it’s a high effort to write an automated test for little benefit   obviously, the need for manual tests should be kept at a bare minimum  instead, we should add e2e tests wherever sensible/valuable we want to implement some form of general upgrade tests as part of #6016    ","categories":"","description":"","excerpt":"Testing Strategy and Developer Guideline This document walks you …","ref":"/docs/gardener/development/testing/","tags":"","title":"Testing"},{"body":"Dependency management We use golang modules to manage golang dependencies. In order to add a new package dependency to the project, you can perform go get \u003cPACKAGE\u003e@\u003cVERSION\u003e or edit the go.mod file and append the package along with the version you want to use.\nUpdating dependencies The Makefile contains a rule called revendor which performs go mod vendor and go mod tidy.\ngo mod vendor resets the main module’s vendor directory to include all packages needed to build and test all the main module’s packages. It does not include test code for vendored packages.\ngo mod tidy makes sure go.mod matches the source code in the module. It adds any missing modules necessary to build the current module’s packages and dependencies, and it removes unused modules that don’t provide any relevant packages.\n$ make revendor The dependencies are installed into the vendor folder which should be added to the VCS.\n⚠️ Make sure you test the code after you have updated the dependencies!\n","categories":"","description":"","excerpt":"Dependency management We use golang modules to manage golang …","ref":"/docs/other-components/machine-controller-manager/docs/development/testing_and_dependencies/","tags":"","title":"Testing And Dependencies"},{"body":"Test Machinery Tests In order to automatically qualify Gardener releases, we execute a set of end-to-end tests using Test Machinery. This requires a full Gardener installation including infrastructure extensions as well as a setup of Test Machinery itself. These tests operate on Shoot clusters across different Cloud Providers, using different supported Kubernetes versions and various configuration options (huge test matrix).\nThis manual gives an overview about test machinery tests in Gardener.\n Structure Add a new test Test Labels Framework  Structure Gardener test machinery tests are split into two test suites that can be found under test/testmachinery/suites:\n The Gardener Test Suite contains all tests that only require a running gardener instance. The Shoot Test Suite contains all tests that require a predefined running shoot cluster.  The corresponding tests of a test suite are defined in the import statement of the suite definition see shoot/run_suite_test.go and their source code can be found under test/testmachinery\nThe test directory is structured as follows:\ntest ├── e2e # end-to-end tests (using provider-local) │ └── shoot ├── framework # helper code shared across integration, e2e and testmachinery tests ├── integration # integration tests (envtests) │ ├── controllermanager │ ├── envtest │ ├── resourcemanager │ ├── scheduler │ ├── seedadmissioncontroller │ ├── shootmaintenance │ └── ... └── testmachinery # test machinery tests  ├── gardener # actual test cases imported by suites/gardener  │ └── security  ├── shoots # actual test cases imported by suites/shoot  │ ├── applications  │ ├── care  │ ├── logging  │ ├── operatingsystem  │ ├── operations  │ └── vpntunnel  ├── suites # suites that run agains a running garden or shoot cluster  │ ├── gardener  │ └── shoot  └── system # suites that are used for building a full test flow  ├── complete_reconcile  ├── managed_seed_creation  ├── managed_seed_deletion  ├── shoot_cp_migration  ├── shoot_creation  ├── shoot_deletion  ├── shoot_hibernation  ├── shoot_hibernation_wakeup  └── shoot_update A suite can be executed by running the suite definition with ginkgo’s focus and skip flags to control the execution of specific labeled test. See example below:\ngo test -timeout=0 -mod=vendor ./test/testmachinery/suites/shoot \\  --v -ginkgo.v -ginkgo.progress -ginkgo.no-color \\  --report-file=/tmp/report.json \\ # write elasticsearch formatted output to a file  --disable-dump=false \\ # disables dumping of teh current state if a test fails  -kubecfg=/path/to/gardener/kubeconfig \\  -shoot-name=\u003cshoot-name\u003e \\ # Name of the shoot to test  -project-namespace=\u003cgardener project namespace\u003e \\ # Name of the gardener project the test shoot resides  -ginkgo.focus=\"\\[RELEASE\\]\" \\ # Run all tests that are tagged as release  -ginkgo.skip=\"\\[SERIAL\\]|\\[DISRUPTIVE\\]\" # Exclude all tests that are tagged SERIAL or DISRUPTIVE Add a new test To add a new test the framework requires the following steps (step 1. and 2. can be skipped if the test is added to an existing package):\n Create a new test file e.g. test/testmachinery/shoot/security/my-sec-test.go Import the test into the appropriate test suite (gardener or shoot): import _ \"github.com/gardener/gardener/test/testmachinery/shoot/security\" Define your test with the testframework. The framework will automatically add its initialization, cleanup and dump functions.  var _ = ginkgo.Describe(\"my suite\", func(){  f := framework.NewShootFramework(nil)   f.Beta().CIt(\"my first test\", func(ctx context.Context) {  f.ShootClient.Get(xx)  // testing ...  }) }) The newly created test can be tested by focusing the test with the default ginkgo focus f.Beta().FCIt(\"my first test\", func(ctx context.Context) and run the shoot test suite with:\ngo test -timeout=0 -mod=vendor ./test/testmachinery/suites/shoot \\ --v -ginkgo.v -ginkgo.progress -ginkgo.no-color \\ --report-file=/tmp/report.json \\ # write elasticsearch formatted output to a file --disable-dump=false \\ # disables dumping of the current state if a test fails -kubecfg=/path/to/gardener/kubeconfig \\ -shoot-name=\u003cshoot-name\u003e \\ # Name of the shoot to test -project-namespace=\u003cgardener project namespace\u003e \\ -fenced=\u003ctrue|false\u003e # Tested shoot is running in a fenced environment and cannot be reached by gardener or for the gardener suite with:\ngo test -timeout=0 -mod=vendor ./test/testmachinery/suites/gardener \\ --v -ginkgo.v -ginkgo.progress -ginkgo.no-color \\ --report-file=/tmp/report.json \\ # write elasticsearch formatted output to a file --disable-dump=false \\ # disables dumping of the current state if a test fails -kubecfg=/path/to/gardener/kubeconfig \\ -project-namespace=\u003cgardener project namespace\u003e ⚠️ Make sure that you do not commit any focused specs as this feature is only intended for local development! Ginkgo will fail the test suite if there are any focused specs.\nAlternatively, a test can be triggered by specifying a ginkgo focus regex with the name of the test e.g.\ngo test -timeout=0 -mod=vendor ./test/testmachinery/suites/gardener \\ --v -ginkgo.v -ginkgo.progress -ginkgo.no-color \\ --report-file=/tmp/report.json \\ # write elasticsearch formatted output to a file -kubecfg=/path/to/gardener/kubeconfig \\ -project-namespace=\u003cgardener project namespace\u003e \\ -ginkgo.focus=\"my first test\" # regex to match test cases Test Labels Every test should be labeled by using the predefined labels available with every framework to have consistent labeling across all test machinery tests.\nThe labels are applied to every new It()/CIt() definition by:\nf := framework.NewCommonFramework() f.Default().Serial().It(\"my test\") =\u003e \"[DEFAULT] [SERIAL] my test\"  f := framework.NewShootFramework() f.Default().Serial().It(\"my test\") =\u003e \"[DEFAULT] [SERIAL] [SHOOT] my test\"  f := framework.NewGardenerFramework() f.Default().Serial().It(\"my test\") =\u003e \"[DEFAULT] [GARDENER] [SERIAL] my test\" Labels:\n Beta: Newly created tests with no experience on stableness should be first labeled as beta tests. They should be watched (and probably improved) until stable enough to be promoted to Default. Default: Tests that were Beta before and proved to be stable are promoted to Default eventually. Default tests run more often, produce alerts and are considered during the release decision although they don’t necessarily block a release. Release: Test are release relevant. A failing Release test blocks the release pipeline. Therefore these tests need to be stable. Only tests proven to be stable will eventually be promoted to Release.  Behavior Labels:\n Serial: The test should always be executed in serial with no other tests running as it may impact other tests. Destructive: The test is destructive. Which means that is runs with no other tests and may break gardener or the shoot. Only create such tests if really necessary as the execution will be expensive (neither gardener nor the shoot can be reused in this case for other tests).  Framework The framework directory contains all the necessary functions / utilities for running test machinery tests. For example, there are methods for creation/deletion of shoots, waiting for shoot deletion/creation, downloading/installing/deploying helm charts, logging, etc.\nThe framework itself consists of 3 different framework that expect different prerequisites and offer context specific functionality.\n CommonFramework: The common framework is the base framework that handles logging and setup of commonly needed resources like helm. It also contains common functions for interacting with kubernetes clusters like Waiting for resources to be ready or Exec into a running pod. GardenerFramework contains all functions of the common framework and expects a running gardener instance with the provided gardener kubeconfig and a project namespace. It also contains functions to interact with gardener like Waiting for a shoot to be reconciled or Patch a shoot or Get a seed. ShootFramework: contains all functions of the common and the gardener framework. It expects a running shoot cluster defined by the shoot’s name and namespace(project namespace). This framework contains functions to directly interact with the specific shoot.  The whole framework also includes commonly used checks, ginkgo wrapper, etc. as well as commonly used tests. Theses common application tests (like the guestbook test) can be used within multiple tests to have a default application (with ingress, deployment, stateful backend) to test external factors.\nConfig\nEvery framework commandline flag can also be defined by a configuration file (the value of the configuration file is only used if flag is not specified by commandline). The test suite searches for a configuration file (yaml is preferred) if the command line flag --config=/path/to/config/file is provided. A framework can be defined in the configuration file by just using the flag name as root key e.g.\nverbose: debug kubecfg: /kubeconfig/path project-namespace: garden-it Report\nThe framework automatically writes the default ginkgo default report to stdout and a specifically structured elastichsearch bulk report file to a specified location. The elastichsearch bulk report will write one json document per testcase and injects metadata of the whole testsuite. An example document for one test case would look like the following document:\n{ \"suite\": { \"name\": \"Shoot Test Suite\", \"phase\": \"Succeeded\", \"tests\": 3, \"failures\": 1, \"errors\": 0, \"time\": 87.427 }, \"name\": \"Shoot application testing [DEFAULT] [RELEASE] [SHOOT] should download shoot kubeconfig successfully\", \"shortName\": \"should download shoot kubeconfig successfully\", \"labels\": [ \"DEFAULT\", \"RELEASE\", \"SHOOT\" ], \"phase\": \"Succeeded\", \"time\": 0.724512057 } Resources\nThe resources directory contains all the templates, helm config files (e.g., repositories.yaml, charts, and cache index which are downloaded upon the start of the test), shoot configs, etc.\nresources ├── charts ├── repository │ └── repositories.yaml └── templates  ├── guestbook-app.yaml.tpl  └── logger-app.yaml.tpl There are two special directories that are dynamically filled with the correct test files:\n charts: the charts will be downloaded and saved in this directory repository contains the repository.yaml file that the target helm repos will be read from and the cache where the stable-index.yaml file will be created  System Tests This directory contains the system tests that have a special meaning for the testmachinery with their own Test Definition. Currently these system tests consists of:\n Shoot creation Shoot deletion Shoot Kubernetes update Gardener Full reconcile check  Shoot Creation test Create Shoot test is meant to test shoot creation.\nExample Run\ngo test -mod=vendor -timeout=0 ./test/testmachinery/system/shoot_creation \\  --v -ginkgo.v -ginkgo.progress \\  -kubecfg=$HOME/.kube/config \\  -shoot-name=$SHOOT_NAME \\  -cloud-profile=$CLOUDPROFILE \\  -seed=$SEED \\  -secret-binding=$SECRET_BINDING \\  -provider-type=$PROVIDER_TYPE \\  -region=$REGION \\  -k8s-version=$K8S_VERSION \\  -project-namespace=$PROJECT_NAMESPACE \\  -annotations=$SHOOT_ANNOTATIONS \\  -infrastructure-provider-config-filepath=$INFRASTRUCTURE_PROVIDER_CONFIG_FILEPATH \\  -controlplane-provider-config-filepath=$CONTROLPLANE_PROVIDER_CONFIG_FILEPATH \\  -workers-config-filepath=$$WORKERS_CONFIG_FILEPATH \\  -worker-zone=$ZONE \\  -networking-pods=$NETWORKING_PODS \\  -networking-services=$NETWORKING_SERVICES \\  -networking-nodes=$NETWORKING_NODES \\  -start-hibernated=$START_HIBERNATED Shoot Deletion test Delete Shoot test is meant to test the deletion of a shoot.\nExample Run\ngo test -mod=vendor -timeout=0 -ginkgo.v -ginkgo.progress \\  ./test/testmachinery/system/shoot_deletion \\  -kubecfg=$HOME/.kube/config \\  -shoot-name=$SHOOT_NAME \\  -project-namespace=$PROJECT_NAMESPACE Shoot Update test The Update Shoot test is meant to test the kubernetes version update of a existing shoot. If no specific version is provided the next patch version is automatically selected. If there is no available newer version this test is a noop.\nExample Run\ngo test -mod=vendor -timeout=0 ./test/testmachinery/system/shoot_update \\  --v -ginkgo.v -ginkgo.progress \\  -kubecfg=$HOME/.kube/config \\  -shoot-name=$SHOOT_NAME \\  -project-namespace=$PROJECT_NAMESPACE \\  -version=$K8S_VERSION Gardener Full Reconcile test The Gardener Full Reconcile test is meant to test if all shoots of a gardener instance are successfully reconciled.\nExample Run\ngo test -mod=vendor -timeout=0 ./test/testmachinery/system/complete_reconcile \\  --v -ginkgo.v -ginkgo.progress \\  -kubecfg=$HOME/.kube/config \\  -project-namespace=$PROJECT_NAMESPACE \\  -gardenerVersion=$GARDENER_VERSION # needed to validate the last acted gardener version of a shoot ","categories":"","description":"","excerpt":"Test Machinery Tests In order to automatically qualify Gardener …","ref":"/docs/gardener/development/testmachinery_tests/","tags":"","title":"Testmachinery Tests"},{"body":"Theming Motivation Gardener landscape administrators should have the possibility to change the appearance of the Gardener Dashboard via configuration without the need to touch the code.\nColors Gardener Dashboard has been built with Vuetify. We use Vuetify’s built-in theming support to centrally configure colors that are used throughout the web application. Colors can be configured for both light and dark themes. Configuration is done via the helm chart, see the respective theme section there. Colors can be specified as HTML color code (e.g. #FF0000 for red) or by referencing a color from Vuetify’s Material Design Color Pack.\nThe following colors can be configured:\n   name usage     primary icons, chips, buttons, popovers, etc.   anchor links   main-background main navigation, login page   main-navigation-title text color on main navigation   toolbar-background background color for toolbars in cards, dialogs, etc.   toolbar-title text color for toolbars in cards, dialogs, etc.   action-button buttons in tables and cards, e.g. cluster details page   info Snotify info popups   warning Snotify warning popups, warning texts   error Snotify error popups, error texts    If you use the helm chart, you can configure those with frontendConfig.themes.light for the light theme and frontendConfig.themes.dark for the dark theme.\nExample frontend:  themes:  light:  primary: '#0b8062'  anchor: '#0b8062'  main-background: 'grey.darken3'  main-navigation-title: 'shades.white'  toolbar-background: '#0b8062'  toolbar-title: 'shades.white'  action-button: 'grey.darken4' Logos and Icons It is also possible to exchange the Dashboard logo and icons. You can replace the assets folder when using the helm chart in the frontendConfig.assets map.\nAttention: You need to set values for all files as mapping the volume will overwrite all files. It is not possible to exchange single files.\nThe files have to be encoded as base64 for the chart - to generate the encoded files for the values.yaml of the helm chart, you can use the following shorthand with bash or zsh on Linux systems. If you use macOS, install coreutils with brew (brew install coreutils) or remove the -w0 parameter.\ncat \u003c\u003c EOF ### ### COPY EVERYTHING BELOW THIS LINE ### assets: favicon-16x16.png: | $(cat frontend/public/static/assets/favicon-16x16.png | base64 -w0) favicon-32x32.png: | $(cat frontend/public/static/assets/favicon-32x32.png | base64 -w0) favicon-96x96.png: | $(cat frontend/public/static/assets/favicon-96x96.png | base64 -w0) favicon.ico: | $(cat frontend/public/static/assets/favicon.ico | base64 -w0) logo.svg: | $(cat frontend/public/static/assets/logo.svg | base64 -w0) EOF Then, swap in the base64 encoded version of your files where needed.\n","categories":"","description":"","excerpt":"Theming Motivation Gardener landscape administrators should have the …","ref":"/docs/dashboard/deployment/theming/","tags":"","title":"Theming"},{"body":"Taints and Tolerations for Seeds and Shoots Similar to taints and tolerations for Nodes and Pods in Kubernetes, the Seed resource supports specifying taints (.spec.taints, see this example) while the Shoot resource supports specifying tolerations (.spec.tolerations, see this example). The feature is used to control scheduling to seeds as well as decisions whether a shoot can use a certain seed.\nCompared to Kubernetes, Gardener’s taints and tolerations are very much down-stripped right now and have some behavioral differences. Please read the following explanations carefully if you plan to use it.\nScheduling When scheduling a new shoot then the gardener-scheduler will filter all seed candidates whose taints are not tolerated by the shoot. As Gardener’s taints/tolerations don’t support effects yet you can compare this behaviour with using a NoSchedule effect taint in Kubernetes.\nBe reminded that taints/tolerations are no means to define any affinity or selection for seeds - please use .spec.seedSelector in the Shoot to state such desires.\n⚠️ Please note that - unlike how it’s implemented in Kubernetes - a certain seed cluster may only be used when the shoot tolerates all the seed’s taints. This means that specifying .spec.seedName for a seed whose taints are not tolerated will make the gardener-apiserver rejecting the request.\nConsequently, the taints/tolerations feature can be used as means to restrict usage of certain seeds.\nToleration Defaults and Whitelist The Project resource features a .spec.tolerations object that may carry defaults and a whitelist (see this example). The corresponding ShootTolerationRestriction admission plugin (cf. Kubernetes’ PodTolerationRestriction admission plugin) is responsible for evaluating these settings during creation/update of Shoots.\nWhitelist If a shoot gets created or updated with tolerations then it is validated that only those tolerations may be used which were added to either a) the Project’s .spec.tolerations.whitelist, or b) to the global whitelist in the ShootTolerationRestriction’s admission config (see this example).\n⚠️ Please note that the tolerations whitelist of Projects can only be changed if the user trying to change it is bound to the modify-spec-tolerations-whitelist custom RBAC role, e.g. via the following ClusterRole:\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata:  name: full-project-modification-access rules: - apiGroups:  - core.gardener.cloud  resources:  - projects  verbs:  - create  - patch  - update  - modify-spec-tolerations-whitelist  - delete Defaults If a shoot gets created then the default tolerations specified in both the Project’s .spec.tolerations.defaults and global default list in the ShootTolerationRestriction admission plugin’s configuration will be added to the .spec.tolerations of the Shoot (unless it already specifies a certain key).\n","categories":"","description":"","excerpt":"Taints and Tolerations for Seeds and Shoots Similar to taints and …","ref":"/docs/gardener/usage/tolerations/","tags":"","title":"Tolerations"},{"body":"Trouble Shooting Guide Are there really issue that cannot be fixed :O? Well, of course not :P. With continuous development of Gardener, over the time its architecture and API might have to be changed to reduce complexity and support more features. In this process developers are bound to keep Gardener version backward compatible with last two releases. But maintaining backward compatibility is quite complex and effortful tasks. So, to save short term complex effort, its common practice in open source community to use work around or hacky solutions sometimes. This results in rare issues which are supposed to be resolved by human interaction across upgrades of Gardener version.\nThis guide records the issues that are quite possible across upgrade of Gardener version, root cause and the human action required for graceful resolution of issue. For troubleshooting guide of bugs which are not yet fixed, please refer the associated github issue.\nNote To Maintainers: Please use only mention the resolution of issues which are by design. For bugs please report the temporary resolution on github issue create for the bug.\nEtcd-Main pod fails to come up, since backup-restore sidecar is reporting RevisionConsistencyCheckErr Issue  Etcd-main pod goes in CrashLoopBackoff. Etcd-backup-restore sidecar reports validation error with RevisionConsistencyCheckErr.  Environment  Gardener version: 0.29.0+  Root Cause  From version 0.29.0, Gardener uses shared backup bucket for storing etcd backups, replacing old logic of having single bucket per shoot as per proposal. Since there are very rare chances that etcd data directory will get corrupt, while doing this migration, to avoid etcd down time and implementation effort, we decided to switch directly from old bucket to new shared bucket without migrating old snapshot from old bucket to new bucket. In this case just for safety side we added sanity check in etcd-backup-restore sidecar of etcd-main pod, which checks if etcd data revision is greater than the last snapshot revision from old bucket. If above check fails mean there is surely some data corruption occurred with etcd, so etcd-backup-restore reports error and then etcd-main pod goes in CrashLoopBackoff creating etcd-main down alerts.  Action  Disable the Gardener reconciliation for Shoot by annotating it with shoot.gardener.cloud/ignore=true Scale down the etcd-main statefulset in seed cluster. Find out the latest full snapshot and delta snapshot from old backup bucket. The old backup bucket name is same as the backupInfra resource associated with Shoot in Garden cluster. Move them manually to new backup bucket. Enable the Gardener reconciliation for shoot by removing annotation shoot.gardener.cloud/ignore=true.  ","categories":"","description":"","excerpt":"Trouble Shooting Guide Are there really issue that cannot be fixed :O? …","ref":"/docs/gardener/usage/trouble_shooting_guide/","tags":"","title":"Trouble Shooting Guide"},{"body":"Trusted TLS certificate for shoot control planes Shoot clusters are composed of several control plane components deployed by the Gardener and corresponding extensions.\nSome components are exposed via Ingress resources which make them addressable under the HTTPS protocol.\nExamples:\n Alertmanager Grafana for operators and end-users Prometheus  Gardener generates the backing TLS certificates which are signed by the shoot cluster’s CA by default (self-signed).\nUnlike with a self-contained Kubeconfig file, common internet browsers or operating systems don’t trust a shoot’s cluster CA and adding it as a trusted root is often undesired in enterprise environments.\nTherefore, Gardener operators can predefine trusted wildcard certificates under which the mentioned endpoints will be served instead.\nRegister a trusted wildcard certificate Since control plane components are published under the ingress domain (core.gardener.cloud/v1beta1.Seed.spec.dns.ingressDomain) a wildcard certificate is required.\nFor example:\n Seed ingress domain: dev.my-seed.example.com CN or SAN for certificate: *.dev.my-seed.example.com  A wildcard certificate matches exactly one seed. It must be deployed as part of your landscape setup as a Kubernetes Secret inside the garden namespace of the corresponding seed cluster.\nPlease ensure that the secret has the gardener.cloud/role label shown below.\napiVersion: v1 data:  ca.crt: base64-encoded-ca.crt  tls.crt: base64-encoded-tls.crt  tls.key: base64-encoded-tls.key kind: Secret metadata:  labels:  gardener.cloud/role: controlplane-cert  name: seed-ingress-certificate  namespace: garden type: Opaque Gardener copies the secret during the reconciliation of shoot clusters to the shoot namespace in the seed. Afterwards, Ingress resources in that namespace for the mentioned components will refer to the wildcard certificate.\nBest practice While it is possible to create the wildcard certificates manually and deploy them to seed clusters, it is recommended to let certificate management components do this job. Often, a seed cluster is also a shoot cluster at the same time (ManagedSeed) and might already provide a certificate service extension. Otherwise, a Gardener operator may use solutions like Cert-Management or Cert-Manager.\n","categories":"","description":"","excerpt":"Trusted TLS certificate for shoot control planes Shoot clusters are …","ref":"/docs/gardener/usage/trusted-tls-for-control-planes/","tags":"","title":"Trusted Tls For Control Planes"},{"body":"Gardener Extension for Ubuntu OS  \nThis controller operates on the OperatingSystemConfig resource in the extensions.gardener.cloud/v1alpha1 API group. It manages those objects that are requesting Ubuntu OS configuration (.spec.type=ubuntu). An experimental support for Ubuntu Pro is added (.spec.type=ubuntu-pro):\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: OperatingSystemConfig metadata:  name: pool-01-original  namespace: default spec:  type: ubuntu  units:  ...  files:  ... Please find a concrete example in the example folder.\nAfter reconciliation the resulting data will be stored in a secret within the same namespace (as the config itself might contain confidential data). The name of the secret will be written into the resource’s .status field:\n... status:  ...  cloudConfig:  secretRef:  name: osc-result-pool-01-original  namespace: default  command: /usr/bin/env bash \u003cpath\u003e  units:  - docker-monitor.service  - kubelet-monitor.service  - kubelet.service The secret has one data key cloud_config that stores the generation.\nAn example for a ControllerRegistration resource that can be used to register this controller to Gardener can be found here.\nThis controller is implemented using the oscommon library for operating system configuration controllers.\nPlease find more information regarding the extensibility concepts and a detailed proposal here.\n How to start using or developing this extension controller locally You can run the controller locally on your machine by executing make start. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the ./dev/kubeconfig file. Static code checks and tests can be executed by running make verify. We are using Go modules for Golang package dependency management and Ginkgo/Gomega for testing.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn more! Please find further resources about out project here:\n Our landing page gardener.cloud “Gardener, the Kubernetes Botanist” blog on kubernetes.io “Gardener Project Update” blog on kubernetes.io Gardener Extensions Golang library GEP-1 (Gardener Enhancement Proposal) on extensibility Extensibility API documentation  ","categories":"","description":"Gardener extension controller for the Ubuntu operating system","excerpt":"Gardener extension controller for the Ubuntu operating system","ref":"/docs/extensions/os-extensions/gardener-extension-os-ubuntu/","tags":"","title":"Ubuntu OS"},{"body":"Using the Alicloud provider extension with Gardener as end-user The core.gardener.cloud/v1beta1.Shoot resource declares a few fields that are meant to contain provider-specific configuration.\nThis document describes the configurable options for Alicloud and provides an example Shoot manifest with minimal configuration that can be used to create an Alicloud cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).\nAlicloud Provider Credentials In order for Gardener to create a Kubernetes cluster using Alicloud infrastructure components, a Shoot has to provide credentials with sufficient permissions to the desired Alicloud project. Every shoot cluster references a SecretBinding which itself references a Secret, and this Secret contains the provider credentials of the Alicloud project.\nThis Secret must look as follows:\napiVersion: v1 kind: Secret metadata:  name: core-alicloud  namespace: garden-dev type: Opaque data:  accessKeyID: base64(access-key-id)  accessKeySecret: base64(access-key-secret) The SecretBinding is configurable in the Shoot cluster with the field secretBindingName.\nThe required credentials for the Alicloud project are an AccessKey Pair associated with a Resource Access Management (RAM) User. A RAM user is a special account that can be used by services and applications to interact with Alicloud Cloud Platform APIs. Applications can use AccessKey pair to authorize themselves to a set of APIs and perform actions within the permissions granted to the RAM user.\nMake sure to create a Resource Access Management User, and create an AccessKey Pair that shall be used for the Shoot cluster.\nPermissions Please make sure the provided credentials have the correct privileges. You can use the following Alicloud RAM policy document and attach it to the RAM user backed by the credentials you provided.\n Click to expand the Alicloud RAM policy document! {  \"Statement\": [  {  \"Action\": [  \"vpc:*\"  ],  \"Effect\": \"Allow\",  \"Resource\": [  \"*\"  ]  },  {  \"Action\": [  \"ecs:*\"  ],  \"Effect\": \"Allow\",  \"Resource\": [  \"*\"  ]  },  {  \"Action\": [  \"slb:*\"  ],  \"Effect\": \"Allow\",  \"Resource\": [  \"*\"  ]  },  {  \"Action\": [  \"ram:GetRole\",  \"ram:CreateRole\",  \"ram:CreateServiceLinkedRole\"  ],  \"Effect\": \"Allow\",  \"Resource\": [  \"*\"  ]  },  {  \"Action\": [  \"ros:*\"  ],  \"Effect\": \"Allow\",  \"Resource\": [  \"*\"  ]  }  ],  \"Version\": \"1\" }  InfrastructureConfig The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.\nAn example InfrastructureConfig for the Alicloud extension looks as follows:\napiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1 kind: InfrastructureConfig networks:  vpc: # specify either 'id' or 'cidr'  # id: my-vpc  cidr: 10.250.0.0/16  # gardenerManagedNATGateway: true  zones:  - name: eu-central-1a  workers: 10.250.1.0/24  # natGateway:  # eipAllocationID: eip-ufxsdg122elmszcg The networks.vpc section describes whether you want to create the shoot cluster in an already existing VPC or whether to create a new one:\n If networks.vpc.id is given then you have to specify the VPC ID of the existing VPC that was created by other means (manually, other tooling, …). If networks.vpc.cidr is given then you have to specify the VPC CIDR of a new VPC that will be created during shoot creation. You can freely choose a private CIDR range. Either networks.vpc.id or networks.vpc.cidr must be present, but not both at the same time. When networks.vpc.id is present, in addition, you can also choose to set networks.vpc.gardenerManagedNATGateway. It is by default false. When it is set to true, Gardener will create an Enhanced NATGateway in the VPC and associate it with a VSwitch created in the first zone in the networks.zones. Please note that when networks.vpc.id is present, and networks.vpc.gardenerManagedNATGateway is false or not set, you have to manually create an Enhance NATGateway and associate it with a VSwitch that you manually created. In this case, make sure the worker CIDRs in networks.zones do not overlap with the one you created. If a NATGateway is created manually and a shoot is created in the same VPC with networks.vpc.gardenerManagedNATGateway set true, you need to manually adjust the route rule accordingly. You may refer to here.  The networks.zones section describes which subnets you want to create in availability zones. For every zone, the Alicloud extension creates one subnet:\n The workers subnet is used for all shoot worker nodes, i.e., VMs which later run your applications.  For every subnet, you have to specify a CIDR range contained in the VPC CIDR specified above, or the VPC CIDR of your already existing VPC. You can freely choose these CIDR and it is your responsibility to properly design the network layout to suit your needs.\nIf you want to use multiple availability zones then add a second, third, … entry to the networks.zones[] list and properly specify the AZ name in networks.zones[].name.\nApart from the VPC and the subnets the Alicloud extension will also create a NAT gateway (only if a new VPC is created), a key pair, elastic IPs, VSwitches, a SNAT table entry, and security groups.\nBy default, the Alicloud extension will create a corresponding Elastic IP that it attaches to this NAT gateway and which is used for egress traffic. The networks.zones[].natGateway.eipAllocationID field allows you to specify the Elastic IP Allocation ID of an existing Elastic IP allocation in case you want to bring your own. If provided, no new Elastic IP will be created and, instead, the Elastic IP specified by you will be used.\n⚠️ If you change this field for an already existing infrastructure then it will disrupt egress traffic while Alicloud applies this change, because the NAT gateway must be recreated with the new Elastic IP association. Also, please note that the existing Elastic IP will be permanently deleted if it was earlier created by the Alicloud extension.\nControlPlaneConfig The control plane configuration mainly contains values for the Alicloud-specific control plane components. Today, the Alicloud extension deploys the cloud-controller-manager and the CSI controllers.\nAn example ControlPlaneConfig for the Alicloud extension looks as follows:\napiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1 kind: ControlPlaneConfig csi:  enableADController: true cloudControllerManager:  featureGates:  CustomResourceValidation: true The csi.enableADController is used as the value of environment DISK_AD_CONTROLLER, which is used for AliCloud csi-disk-plugin. This field is optional. When a new shoot is creatd, this field is automatically set true. For an existing shoot created in previous versions, it remains unchanged. If there are persistent volumes created before year 2021, please be cautious to set this field true because they may fail to mount to nodes.\nThe cloudControllerManager.featureGates contains a map of explicitly enabled or disabled feature gates. For production usage it’s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability. If you don’t want to configure anything for the cloudControllerManager simply omit the key in the YAML specification.\nWorkerConfig The Alicloud extension does not support a specific WorkerConfig. However, it supports additional data volumes (plus encryption) per machine. By default (if not stated otherwise), all the disks are unencrypted. For each data volume, you have to specify a name. It also supports encrypted system disk. However, only Customized image is currently supported to be used as a basic image for encrypted system disk. Please be noted that the change of system disk encryption flag will cause reconciliation of a shoot, and it will result in nodes rolling update within the worker group.\nThe following YAML is a snippet of a Shoot resource:\nspec:  provider:  workers:  - name: cpu-worker  ...  volume:  type: cloud_efficiency  size: 20Gi  encrypted: true  dataVolumes:  - name: kubelet-dir  type: cloud_efficiency  size: 25Gi  encrypted: true Example Shoot manifest (one availability zone) Please find below an example Shoot manifest for one availability zone:\napiVersion: core.gardener.cloud/v1alpha1 kind: Shoot metadata:  name: johndoe-alicloud  namespace: garden-dev spec:  cloudProfileName: alicloud  region: eu-central-1  secretBindingName: core-alicloud  provider:  type: alicloud  infrastructureConfig:  apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  vpc:  cidr: 10.250.0.0/16  zones:  - name: eu-central-1a  workers: 10.250.0.0/19  controlPlaneConfig:  apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1  kind: ControlPlaneConfig  workers:  - name: worker-xoluy  machine:  type: ecs.sn2ne.large  minimum: 2  maximum: 2  volume:  size: 50Gi  type: cloud_efficiency  zones:  - eu-central-1a  networking:  nodes: 10.250.0.0/16  type: calico  kubernetes:  version: 1.16.1  maintenance:  autoUpdate:  kubernetesVersion: true  machineImageVersion: true  addons:  kubernetes-dashboard:  enabled: true  nginx-ingress:  enabled: true Example Shoot manifest (two availability zones) Please find below an example Shoot manifest for two availability zones:\napiVersion: core.gardener.cloud/v1alpha1 kind: Shoot metadata:  name: johndoe-alicloud  namespace: garden-dev spec:  cloudProfileName: alicloud  region: eu-central-1  secretBindingName: core-alicloud  provider:  type: alicloud  infrastructureConfig:  apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  vpc:  cidr: 10.250.0.0/16  zones:  - name: eu-central-1a  workers: 10.250.0.0/26  - name: eu-central-1b  workers: 10.250.0.64/26  controlPlaneConfig:  apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1  kind: ControlPlaneConfig  workers:  - name: worker-xoluy  machine:  type: ecs.sn2ne.large  minimum: 2  maximum: 4  volume:  size: 50Gi  type: cloud_efficiency  # NOTE: Below comment is for the case when encrypted field of an existing shoot is updated from false to true.   # It will cause affected nodes to be rolling updated. Users must trigger a MAINTAIN operation of the shoot.   # Otherwise, the shoot will fail to reconcile.  # You could do it either via Dashboard or annotating the shoot with gardener.cloud/operation=maintain  encrypted: true  zones:  - eu-central-1a  - eu-central-1b  networking:  nodes: 10.250.0.0/16  type: calico  kubernetes:  version: 1.16.1  maintenance:  autoUpdate:  kubernetesVersion: true  machineImageVersion: true  addons:  kubernetes-dashboard:  enabled: true  nginx-ingress:  enabled: true Kubernetes Versions per Worker Pool This extension supports gardener/gardener’s WorkerPoolKubernetesVersion feature gate, i.e., having worker pools with overridden Kubernetes versions since gardener-extension-provider-alicloud@v1.33.\nShoot CA Certificate and ServiceAccount Signing Key Rotation This extension supports gardener/gardener’s ShootCARotation feature gate since gardener-extension-provider-alicloud@v1.36 and ShootSARotation feature gate since gardener-extension-provider-alicloud@v1.37.\n","categories":"","description":"","excerpt":"Using the Alicloud provider extension with Gardener as end-user The …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-alicloud/docs/usage-as-end-user/","tags":"","title":"Usage As End User"},{"body":"Using the AWS provider extension with Gardener as end-user The core.gardener.cloud/v1beta1.Shoot resource declares a few fields that are meant to contain provider-specific configuration.\nIn this document we are describing how this configuration looks like for AWS and provide an example Shoot manifest with minimal configuration that you can use to create an AWS cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).\nProvider Secret Data Every shoot cluster references a SecretBinding which itself references a Secret, and this Secret contains the provider credentials of your AWS account. This Secret must look as follows:\napiVersion: v1 kind: Secret metadata:  name: core-aws  namespace: garden-dev type: Opaque data:  accessKeyID: base64(access-key-id)  secretAccessKey: base64(secret-access-key) The AWS documentation explains the necessary steps to enable programmatic access, i.e. create access key ID and access key, for the user of your choice.\n⚠️ For security reasons, we recommend creating a dedicated user with programmatic access only. Please avoid re-using a IAM user which has access to the AWS console (human user).\n⚠️ Depending on your AWS API usage it can be problematic to reuse the same AWS Account for different Shoot clusters in the same region due to rate limits. Please consider spreading your Shoots over multiple AWS Accounts if you are hitting those limits.\nPermissions Please make sure that the provided credentials have the correct privileges. You can use the following AWS IAM policy document and attach it to the IAM user backed by the credentials you provided (please check the official AWS documentation as well):\n Click to expand the AWS IAM policy document! {  \"Version\": \"2012-10-17\",  \"Statement\": [  {  \"Effect\": \"Allow\",  \"Action\": \"autoscaling:*\",  \"Resource\": \"*\"  },  {  \"Effect\": \"Allow\",  \"Action\": \"ec2:*\",  \"Resource\": \"*\"  },  {  \"Effect\": \"Allow\",  \"Action\": \"elasticloadbalancing:*\",  \"Resource\": \"*\"  },  {  \"Action\": [  \"iam:GetInstanceProfile\",  \"iam:GetPolicy\",  \"iam:GetPolicyVersion\",  \"iam:GetRole\",  \"iam:GetRolePolicy\",  \"iam:ListPolicyVersions\",  \"iam:ListRolePolicies\",  \"iam:ListAttachedRolePolicies\",  \"iam:ListInstanceProfilesForRole\",  \"iam:CreateInstanceProfile\",  \"iam:CreatePolicy\",  \"iam:CreatePolicyVersion\",  \"iam:CreateRole\",  \"iam:CreateServiceLinkedRole\",  \"iam:AddRoleToInstanceProfile\",  \"iam:AttachRolePolicy\",  \"iam:DetachRolePolicy\",  \"iam:RemoveRoleFromInstanceProfile\",  \"iam:DeletePolicy\",  \"iam:DeletePolicyVersion\",  \"iam:DeleteRole\",  \"iam:DeleteRolePolicy\",  \"iam:DeleteInstanceProfile\",  \"iam:PutRolePolicy\",  \"iam:PassRole\",  \"iam:UpdateAssumeRolePolicy\"  ],  \"Effect\": \"Allow\",  \"Resource\": \"*\"  }  ] }  InfrastructureConfig The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.\nAn example InfrastructureConfig for the AWS extension looks as follows:\napiVersion: aws.provider.extensions.gardener.cloud/v1alpha1 kind: InfrastructureConfig enableECRAccess: true networks:  vpc: # specify either 'id' or 'cidr'  # id: vpc-123456  cidr: 10.250.0.0/16  # gatewayEndpoints:  # - s3  zones:  - name: eu-west-1a  internal: 10.250.112.0/22  public: 10.250.96.0/22  workers: 10.250.0.0/19  # elasticIPAllocationID: eipalloc-123456 ignoreTags:  keys: # individual ignored tag keys  - SomeCustomKey  - AnotherCustomKey  keyPrefixes: # ignored tag key prefixes  - user.specific/prefix/ The enableECRAccess flag specifies whether the AWS IAM role policy attached to all worker nodes of the cluster shall contain permissions to access the Elastic Container Registry of the respective AWS account. If the flag is not provided it is defaulted to true. Please note that if the iamInstanceProfile is set for a worker pool in the WorkerConfig (see below) then enableECRAccess does not have any effect. It only applies for those worker pools whose iamInstanceProfile is not set.\n Click to expand the default AWS IAM policy document used for the instance profiles! {  \"Version\": \"2012-10-17\",  \"Statement\": [  {  \"Effect\": \"Allow\",  \"Action\": [  \"ec2:DescribeInstances\"  ],  \"Resource\": [  \"*\"  ]  },  // Only if `.enableECRAccess` is `true`.  {  \"Effect\": \"Allow\",  \"Action\": [  \"ecr:GetAuthorizationToken\",  \"ecr:BatchCheckLayerAvailability\",  \"ecr:GetDownloadUrlForLayer\",  \"ecr:GetRepositoryPolicy\",  \"ecr:DescribeRepositories\",  \"ecr:ListImages\",  \"ecr:BatchGetImage\"  ],  \"Resource\": [  \"*\"  ]  }  ] }  The networks.vpc section describes whether you want to create the shoot cluster in an already existing VPC or whether to create a new one:\n If networks.vpc.id is given then you have to specify the VPC ID of the existing VPC that was created by other means (manually, other tooling, …). Please make sure that the VPC has attached an internet gateway - the AWS controller won’t create one automatically for existing VPCs. To make sure the nodes are able to join and operate in your cluster properly, please make sure that your VPC has enabled DNS Support, explicitly the attributes enableDnsHostnames and enableDnsSupport must be set to true. If networks.vpc.cidr is given then you have to specify the VPC CIDR of a new VPC that will be created during shoot creation. You can freely choose a private CIDR range. Either networks.vpc.id or networks.vpc.cidr must be present, but not both at the same time. networks.vpc.gatewayEndpoints is optional. If specified then each item is used as service name in a corresponding Gateway VPC Endpoint.  The networks.zones section contains configuration for resources you want to create or use in availability zones. For every zone, the AWS extension creates three subnets:\n The internal subnet is used for internal AWS load balancers. The public subnet is used for public AWS load balancers. The workers subnet is used for all shoot worker nodes, i.e., VMs which later run your applications.  For every subnet, you have to specify a CIDR range contained in the VPC CIDR specified above, or the VPC CIDR of your already existing VPC. You can freely choose these CIDRs and it is your responsibility to properly design the network layout to suit your needs.\nAlso, the AWS extension creates a dedicated NAT gateway for each zone. By default, it also creates a corresponding Elastic IP that it attaches to this NAT gateway and which is used for egress traffic. The elasticIPAllocationID field allows you to specify the ID of an existing Elastic IP allocation in case you want to bring your own. If provided, no new Elastic IP will be created and, instead, the Elastic IP specified by you will be used.\n⚠️ If you change this field for an already existing infrastructure then it will disrupt egress traffic while AWS applies this change. The reason is that the NAT gateway must be recreated with the new Elastic IP association. Also, please note that the existing Elastic IP will be permanently deleted if it was earlier created by the AWS extension.\nYou can configure Gateway VPC Endpoints by adding items in the optional list networks.vpc.gatewayEndpoints. Each item in the list is used as a service name and a corresponding endpoint is created for it. All created endpoints point to the service within the cluster’s region. For example, consider this (partial) shoot config:\nspec:  region: eu-central-1  provider:  type: aws  infrastructureConfig:  apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  vpc:  gatewayEndpoints:  - s3 The service name of the S3 Gateway VPC Endpoint in this example is com.amazonaws.eu-central-1.s3.\nIf you want to use multiple availability zones then add a second, third, … entry to the networks.zones[] list and properly specify the AZ name in networks.zones[].name.\nApart from the VPC and the subnets the AWS extension will also create DHCP options and an internet gateway (only if a new VPC is created), routing tables, security groups, elastic IPs, NAT gateways, EC2 key pairs, IAM roles, and IAM instance profiles.\nThe ignoreTags section allows to configure which resource tags on AWS resources managed by Gardener should be ignored during infrastructure reconciliation. By default, all tags that are added outside of Gardener’s reconciliation will be removed during the next reconciliation. This field allows users and automation to add custom tags on AWS resources created and managed by Gardener without loosing them on the next reconciliation. Tags can ignored either by specifying exact key values (ignoreTags.keys) or key prefixes (ignoreTags.keyPrefixes). In both cases it is forbidden to ignore the Name tag or any tag starting with kubernetes.io or gardener.cloud.\nPlease note though, that the tags are only ignored on resources created on behalf of the Infrastructure CR (i.e. VPC, subnets, security groups, keypair, etc.), while tags on machines, volumes, etc. are not in the scope of this controller.\nControlPlaneConfig The control plane configuration mainly contains values for the AWS-specific control plane components. Today, the only component deployed by the AWS extension is the cloud-controller-manager.\nAn example ControlPlaneConfig for the AWS extension looks as follows:\napiVersion: aws.provider.extensions.gardener.cloud/v1alpha1 kind: ControlPlaneConfig cloudControllerManager:  featureGates:  CustomResourceValidation: true  useCustomRouteController: true storage:  managedDefaultClass: false The cloudControllerManager.featureGates contains a map of explicitly enabled or disabled feature gates. For production usage it’s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability. If you don’t want to configure anything for the cloudControllerManager simply omit the key in the YAML specification.\nThe cloudControllerManager.useCustomRouteController controls if the custom routes controller should be enabled. If enabled, it will add routes to the pod CIDRs for all nodes in the route tables for all zones.\nThe storage.managedDefaultClass controls if the default storage / volume snapshot classes are marked as default by Gardener. Set it to false to mark another storage / volume snapshot class as default without Gardener overwriting this change. If unset, this field defaults to true.\nWorkerConfig The AWS extension supports encryption for volumes plus support for additional data volumes per machine. For each data volume, you have to specify a name. By default (if not stated otherwise), all the disks (root \u0026 data volumes) are encrypted. Please make sure that your instance-type supports encryption. If your instance-type doesn’t support encryption, you will have to disable encryption (which is enabled by default) by setting volume.encrpyted to false (refer below shown YAML snippet).\nThe following YAML is a snippet of a Shoot resource:\nspec:  provider:  workers:  - name: cpu-worker  ...  volume:  type: gp2  size: 20Gi  encrypted: false  dataVolumes:  - name: kubelet-dir  type: gp2  size: 25Gi  encrypted: true  Note: The AWS extension does not support EBS volume (root \u0026 data volumes) encryption with customer managed CMK. Support for customer managed CMK is out of scope for now. Only AWS managed CMK is supported.\n Additionally, it is possible to provide further AWS-specific values for configuring the worker pools. It can be provided in .spec.provider.workers[].providerConfig and is evaluated by the AWS worker controller when it reconciles the shoot machines.\nAn example WorkerConfig for the AWS extension looks as follows:\napiVersion: aws.provider.extensions.gardener.cloud/v1alpha1 kind: WorkerConfig volume:  iops: 10000  throughput: 200 dataVolumes: - name: kubelet-dir  iops: 12345  throughput: 150  snapshotID: snap-1234 iamInstanceProfile: # (specify either ARN or name)  name: my-profile # arn: my-instance-profile-arn nodeTemplate: # (to be specified only if the node capacity would be different from cloudprofile info during runtime)  capacity:  cpu: 2  gpu: 0  memory: 50Gi The .volume.iops is the number of I/O operations per second (IOPS) that the volume supports. For io1 and gp3 volume type, this represents the number of IOPS that are provisioned for the volume. For gp2 volume type, this represents the baseline performance of the volume and the rate at which the volume accumulates I/O credits for bursting. For more information about General Purpose SSD baseline performance, I/O credits, IOPS range and bursting, see Amazon EBS Volume Types (http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html) in the Amazon Elastic Compute Cloud User Guide.\nConstraint: IOPS should be a positive value. Validation of IOPS (i.e. whether it is allowed and is in the specified range for a particular volume type) is done on aws side.\nThe volume.throughput is the throughput that the volume supports, in MiB/s. As of 16th Aug 2022, this parameter is valid only for gp3 volume types and will return an error from the provider side if specified for other volume types. Its current range of throughput is from 125MiB/s to 1000 MiB/s. To know more about throughput and its range, see the official AWS documentation here.\nThe .dataVolumes can optionally contain configurations for the data volumes stated in the Shoot specification in the .spec.provider.workers[].dataVolumes list. The .name must match to the name of the data volume in the shoot. It is also possible to provide a snapshot ID. It allows to restore the data volume from an existing snapshot.\nThe iamInstanceProfile section allows to specify the IAM instance profile name xor ARN that should be used for this worker pool. If not specified, a dedicated IAM instance profile created by the infrastructure controller is used (see above).\nExample Shoot manifest (one availability zone) Please find below an example Shoot manifest for one availability zone:\napiVersion: core.gardener.cloud/v1alpha1 kind: Shoot metadata:  name: johndoe-aws  namespace: garden-dev spec:  cloudProfileName: aws  region: eu-central-1  secretBindingName: core-aws  provider:  type: aws  infrastructureConfig:  apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  vpc:  cidr: 10.250.0.0/16  zones:  - name: eu-central-1a  internal: 10.250.112.0/22  public: 10.250.96.0/22  workers: 10.250.0.0/19  controlPlaneConfig:  apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1  kind: ControlPlaneConfig  workers:  - name: worker-xoluy  machine:  type: m5.large  minimum: 2  maximum: 2  volume:  size: 50Gi  type: gp2  # The following provider config is valid if the volume type is `io1`.  # providerConfig:  # apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1  # kind: WorkerConfig  # volume:  # iops: 10000  zones:  - eu-central-1a  networking:  nodes: 10.250.0.0/16  type: calico  kubernetes:  version: 1.24.3  maintenance:  autoUpdate:  kubernetesVersion: true  machineImageVersion: true  addons:  kubernetes-dashboard:  enabled: true  nginx-ingress:  enabled: true Example Shoot manifest (three availability zones) Please find below an example Shoot manifest for three availability zones:\napiVersion: core.gardener.cloud/v1alpha1 kind: Shoot metadata:  name: johndoe-aws  namespace: garden-dev spec:  cloudProfileName: aws  region: eu-central-1  secretBindingName: core-aws  provider:  type: aws  infrastructureConfig:  apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  vpc:  cidr: 10.250.0.0/16  zones:  - name: eu-central-1a  workers: 10.250.0.0/26  public: 10.250.96.0/26  internal: 10.250.112.0/26  - name: eu-central-1b  workers: 10.250.0.64/26  public: 10.250.96.64/26  internal: 10.250.112.64/26  - name: eu-central-1c  workers: 10.250.0.128/26  public: 10.250.96.128/26  internal: 10.250.112.128/26  controlPlaneConfig:  apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1  kind: ControlPlaneConfig  workers:  - name: worker-xoluy  machine:  type: m5.large  minimum: 3  maximum: 9  volume:  size: 50Gi  type: gp2  zones:  - eu-central-1a  - eu-central-1b  - eu-central-1c  networking:  nodes: 10.250.0.0/16  type: calico  kubernetes:  version: 1.24.3  maintenance:  autoUpdate:  kubernetesVersion: true  machineImageVersion: true  addons:  kubernetes-dashboard:  enabled: true  nginx-ingress:  enabled: true CSI volume provisioners Every AWS shoot cluster that has at least Kubernetes v1.18 will be deployed with the AWS EBS CSI driver. It is compatible with the legacy in-tree volume provisioner that was deprecated by the Kubernetes community and will be removed in future versions of Kubernetes. End-users might want to update their custom StorageClasses to the new ebs.csi.aws.com provisioner. Shoot clusters with Kubernetes v1.17 or less will use the in-tree kubernetes.io/aws-ebs volume provisioner in the kube-controller-manager and the kubelet.\nNode-specific Volume Limits The Kubernetes scheduler allows configurable limit for the number of volumes that can be attached to a node. See https://k8s.io/docs/concepts/storage/storage-limits/#custom-limits.\nCSI drivers usually have a different procedure for configuring this custom limit. By default, the EBS CSI driver parses the machine type name and then decides the volume limit. However, this is only a rough approximation and not good enough in most cases. Specifying the volume attach limit via command line flag (--volume-attach-limit) is currently the alternative until a more sophisticated solution presents itself (dynamically discovering the maximum number of attachable volume per EC2 machine type, see also https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/347). The AWS extension allows the --volume-attach-limit flag of the EBS CSI driver to be configurable via aws.provider.extensions.gardener.cloud/volume-attach-limit annotation on the Shoot resource. If the annotation is added to an existing Shoot, then reconciliation needs to be triggered manually (see Immediate reconciliation), as in general adding annotation to resource is not a change that leads to .metadata.generation increase in general.\nKubernetes Versions per Worker Pool This extension supports gardener/gardener’s WorkerPoolKubernetesVersion feature gate, i.e., having worker pools with overridden Kubernetes versions since gardener-extension-provider-aws@v1.34. Note that this feature is only usable for Shoots whose .spec.kubernetes.version is greater or equal than the CSI migration version (1.18).\nShoot CA Certificate and ServiceAccount Signing Key Rotation This extension supports gardener/gardener’s ShootCARotation and ShootSARotation feature gates since gardener-extension-provider-aws@v1.36.\n","categories":"","description":"","excerpt":"Using the AWS provider extension with Gardener as end-user The …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/usage-as-end-user/","tags":"","title":"Usage As End User"},{"body":"Using the Azure provider extension with Gardener as end-user The core.gardener.cloud/v1beta1.Shoot resource declares a few fields that are meant to contain provider-specific configuration.\nThis document describes the configurable options for Azure and provides an example Shoot manifest with minimal configuration that can be used to create an Azure cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).\nAzure Provider Credentials In order for Gardener to create a Kubernetes cluster using Azure infrastructure components, a Shoot has to provide credentials with sufficient permissions to the desired Azure subscription. Every shoot cluster references a SecretBinding which itself references a Secret, and this Secret contains the provider credentials of the Azure subscription. The SecretBinding is configurable in the Shoot cluster with the field secretBindingName.\nCreate an Azure Application and Service Principle and obtain its credentials.\nPlease ensure that the Azure application (spn) has the IAM actions defined here assigned. If no fine-grained permissions/actions required then simply assign the Contributor role.\nThe example below demonstrates how the secret containing the client credentials of the Azure Application has to look like:\napiVersion: v1 kind: Secret metadata:  name: core-azure  namespace: garden-dev type: Opaque data:  clientID: base64(client-id)  clientSecret: base64(client-secret)  subscriptionID: base64(subscription-id)  tenantID: base64(tenant-id) ⚠️ Depending on your API usage it can be problematic to reuse the same Service Principal for different Shoot clusters due to rate limits. Please consider spreading your Shoots over Service Principals from different Azure subscriptions if you are hitting those limits.\nManaged Service Principals The operators of the Gardener Azure extension can provide managed service principals. This eliminates the need for users to provide an own service principal for a Shoot.\nTo make use of a managed service principal, the Azure secret of a Shoot cluster must contain only a subscriptionID and a tenantID field, but no clientID and clientSecret. Removing those fields from the secret of an existing Shoot will also let it adopt the managed service principal.\nBased on the tenantID field, the Gardener extension will try to assign the managed service principal to the Shoot. If no managed service principal can be assigned then the next operation on the Shoot will fail.\n⚠️ The managed service principal need to be assigned to the users Azure subscription with proper permissions before using it.\nInfrastructureConfig The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.\nAn example InfrastructureConfig for the Azure extension looks as follows:\napiVersion: azure.provider.extensions.gardener.cloud/v1alpha1 kind: InfrastructureConfig networks:  vnet: # specify either 'name' and 'resourceGroup' or 'cidr'  # name: my-vnet  # resourceGroup: my-vnet-resource-group  cidr: 10.250.0.0/16  # ddosProtectionPlanID: /subscriptions/test/resourceGroups/test/providers/Microsoft.Network/ddosProtectionPlans/test-ddos-protection-plan  workers: 10.250.0.0/19  # natGateway:  # enabled: false  # idleConnectionTimeoutMinutes: 4  # zone: 1  # ipAddresses:  # - name: my-public-ip-name  # resourceGroup: my-public-ip-resource-group  # zone: 1  # serviceEndpoints:  # - Microsoft.Test  # zones:  # - name: 1  # cidr: \"10.250.0.0/24  # - name: 2  # cidr: \"10.250.0.0/24\"  # natGateway:  # enabled: false zoned: false # resourceGroup: # name: mygroup #identity: # name: my-identity-name # resourceGroup: my-identity-resource-group # acrAccess: true Currently, it’s not yet possible to deploy into existing resource groups, but in the future it will. The .resourceGroup.name field will allow specifying the name of an already existing resource group that the shoot cluster and all infrastructure resources will be deployed to.\nVia the .zoned boolean you can tell whether you want to use Azure availability zones or not. If you don’t use zones then an availability set will be created and only basic load balancers will be used. Zoned clusters use standard load balancers.\nThe networks.vnet section describes whether you want to create the shoot cluster in an already existing VNet or whether to create a new one:\n If networks.vnet.name and networks.vnet.resourceGroup are given then you have to specify the VNet name and VNet resource group name of the existing VNet that was created by other means (manually, other tooling, …). If networks.vnet.cidr is given then you have to specify the VNet CIDR of a new VNet that will be created during shoot creation. You can freely choose a private CIDR range. Either networks.vnet.name and neworks.vnet.resourceGroup or networks.vnet.cidr must be present, but not both at the same time. The networks.vnet.ddosProtectionPlanID field can be used to specify the id of a ddos protection plan which should be assigned to the VNet. This will only work for a VNet managed by Gardener. For externally managed VNets the ddos protection plan must be assigned by other means.  The networks.workers section describes the CIDR for a subnet that is used for all shoot worker nodes, i.e., VMs which later run your applications. The specified CIDR range must be contained in the VNet CIDR specified above, or the VNet CIDR of your already existing VNet. You can freely choose this CIDR and it is your responsibility to properly design the network layout to suit your needs.\nIn the networks.serviceEndpoints[] list you can specify the list of Azure service endpoints which shall be associated with the worker subnet. All available service endpoints and their technical names can be found in the (Azure Service Endpoint documentation](https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-service-endpoints-overview).\nThe networks.natGateway section contains configuration for the Azure NatGateway which can be attached to the worker subnet of a Shoot cluster. Here are some key information about the usage of the NatGateway for a Shoot cluster:\n NatGateway usage is optional and can be enabled or disabled via .networks.natGateway.enabled. If the NatGateway is not used then the egress connections initiated within the Shoot cluster will be nated via the LoadBalancer of the clusters (default Azure behaviour, see here). NatGateway is only available for zonal clusters .zoned=true. The NatGateway is currently not zone redundantly deployed. That mean the NatGateway of a Shoot cluster will always be in just one zone. This zone can be optionally selected via .networks.natGateway.zone. Caution: Modifying the .networks.natGateway.zone setting requires a recreation of the NatGateway and the managed public ip (automatically used if no own public ip is specified, see below). That mean you will most likely get a different public ip for egress connections. It is possible to bring own zonal public ip(s) via networks.natGateway.ipAddresses. Those public ip(s) need to be in the same zone as the NatGateway (see networks.natGateway.zone) and be of SKU standard. For each public ip the name, the resourceGroup and the zone need to be specified. The field networks.natGateway.idleConnectionTimeoutMinutes allows the configuration of NAT Gateway’s idle connection timeout property. The idle timeout value can be adjusted from 4 minutes, up to 120 minutes. Omitting this property will set the idle timeout to its default value according to NAT Gateway’s documentation.  In the identity section you can specify an Azure user-assigned managed identity which should be attached to all cluster worker machines. With identity.name you can specify the name of the identity and with identity.resourceGroup you can specify the resource group which contains the identity resource on Azure. The identity need to be created by the user upfront (manually, other tooling, …). Gardener/Azure Extension will only use the referenced one and won’t create an identity. Furthermore the identity have to be in the same subscription as the Shoot cluster. Via the identity.acrAccess you can configure the worker machines to use the passed identity for pulling from an Azure Container Registry (ACR). Caution: Adding, exchanging or removing the identity will require a rolling update of all worker machines in the Shoot cluster.\nApart from the VNet and the worker subnet the Azure extension will also create a dedicated resource group, route tables, security groups, and an availability set (if not using zoned clusters).\nInfrastructureConfig with dedicated subnets per zone Another deployment option for zonal clusters only, is to create and configure a separate subnet per availability zone. This network layout is recommended to users that require fine-grained control over their network setup. One prevalent usecase is to create a zone-redundant NAT Gateway deployment by taking advantage of the ability to deploy separate NAT Gateways for each subnet.\nTo use this configuration the following requirements must be met:\n the zoned field must be set to true. the networks.vnet section must not be empty and must contain a valid configuration. For existing clusters that were not using the networks.vnet section, it is enough if networks.vnet.cidr field is set to the current networks.worker value.  For each of the target zones a subnet CIDR range must be specified. The specified CIDR range must be contained in the VNet CIDR specified above, or the VNet CIDR of your already existing VNet. In addition, the CIDR ranges must not overlap with the ranges of the other subnets.\nServiceEndpoints and NatGateways can be configured per subnet. Respectively, when networks.zones is specified, the fields networks.workers, networks.serviceEndpoints and networks.natGateway cannot be set. All the configuration for the subnets must be done inside the respective zone’s configuration.\nExample:\napiVersion: azure.provider.extensions.gardener.cloud/v1alpha1 kind: InfrastructureConfig networks:  zoned: true  vnet: # specify either 'name' and 'resourceGroup' or 'cidr'  cidr: 10.250.0.0/16  zones:  - name: 1  cidr: \"10.250.0.0/24\"  - name: 2  cidr: \"10.250.0.0/24\"  natGateway:  enabled: false Migrating to zonal shoots with dedicated subnets per zone For existing zonal clusters it is possible to migrate to a network layout with dedicated subnets per zone. The migration works by creating additional network resources as specified in the configuration and progressively roll part of your existing nodes to use the new resources. To achieve the controlled rollout of your nodes, parts of the existing infrastructure must be preserved which is why the following constraint is imposed:\nOne of your specified zones must have the exact same CIDR range as the current network.workers field. Here is an example of such migration:\ninfrastructureConfig:  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  vnet:  cidr: 10.250.0.0/16  workers: 10.250.0.0/19  zoned: true to\ninfrastructureConfig:  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  vnet:  cidr: 10.250.0.0/16  zones:  - name: 3  cidr: 10.250.0.0/19 # note the preservation of the 'workers' CIDR # optionally add other zones   # - name: 2   # cidr: 10.250.32.0/19  # natGateway:  # enabled: true  zoned: true Another more advanced example with user-provided public IP addresses for the NAT Gateway and how it can be migrated:\ninfrastructureConfig:  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  vnet:  cidr: 10.250.0.0/16  workers: 10.250.0.0/19  natGateway:  enabled: true  zone: 1  ipAddresses:  - name: pip1  resourceGroup: group  zone: 1  - name: pip2  resourceGroup: group  zone: 1  zoned: true to\ninfrastructureConfig:  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  zoned: true  networks:  vnet:  cidr: 10.250.0.0/16  zones:  - name: 1  cidr: 10.250.0.0/19 # note the preservation of the 'workers' CIDR  natGateway:  enabled: true  ipAddresses:  - name: pip1  resourceGroup: group  zone: 1  - name: pip2  resourceGroup: group  zone: 1 # optionally add other zones  # - name: 2  # cidr: 10.250.32.0/19 # natGateway: # enabled: true # ipAddresses: # - name: pip3 # resourceGroup: group You can apply such change to your shoot by issuing a kubectl patch command to replace your current .spec.provider.infrastructureConfig section:\n$ cat new-infra.json [ { \"op\": \"replace\", \"path\": \"/spec/provider/infrastructureConfig\", \"value\": { \"apiVersion\": \"azure.provider.extensions.gardener.cloud/v1alpha1\", \"kind\": \"InfrastructureConfig\", \"networks\": { \"vnet\": { \"cidr\": \"\u003cyour-vnet-cidr\u003e\" }, \"zones\": [ { \"name\": 1, \"cidr\": \"10.250.0.0/24\", \"natGateway\": { \"enabled\": true } }, { \"name\": 1, \"cidr\": \"10.250.1.0/24\", \"natGateway\": { \"enabled\": true } }, ] }, \"zoned\": true } } ] kubectl patch --type=\"json\" --patch-file new-infra.json shoot \u003cmy-shoot\u003e ⚠️ The migration to shoots with dedicated subnets per zone is a one-way process. Reverting the shoot to the previous configuration is not supported.\n⚠️ During the migration a subset of the nodes will be rolled to the new subnets.\nControlPlaneConfig The control plane configuration mainly contains values for the Azure-specific control plane components. Today, the only component deployed by the Azure extension is the cloud-controller-manager.\nAn example ControlPlaneConfig for the Azure extension looks as follows:\napiVersion: azure.provider.extensions.gardener.cloud/v1alpha1 kind: ControlPlaneConfig cloudControllerManager:  featureGates:  CustomResourceValidation: true The cloudControllerManager.featureGates contains a map of explicitly enabled or disabled feature gates. For production usage it’s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability. If you don’t want to configure anything for the cloudControllerManager simply omit the key in the YAML specification.\nWorkerConfig The Azure extension supports encryption for volumes plus support for additional data volumes per machine. Please note that you cannot specify the encrypted flag for Azure disks as they are encrypted by default/out-of-the-box. For each data volume, you have to specify a name. The following YAML is a snippet of a Shoot resource:\nspec:  provider:  workers:  - name: cpu-worker  ...  volume:  type: Standard_LRS  size: 20Gi  dataVolumes:  - name: kubelet-dir  type: Standard_LRS  size: 25Gi Additionally, it supports for other Azure-specific values and could be configured under .spec.provider.workers[].providerConfig\nAn example WorkerConfig for the Azure extension looks like:\napiVersion: azure.provider.extensions.gardener.cloud/v1alpha1 kind: WorkerConfig nodeTemplate: # (to be specified only if the node capacity would be different from cloudprofile info during runtime)  capacity:  cpu: 2  gpu: 1  memory: 50Gi The .nodeTemplate is used to specify resource information of the machine during runtime. This then helps in Scale-from-Zero. Some points to note for this field: - Currently only cpu, gpu and memory are configurable. - a change in the value lead to a rolling update of the machine in the workerpool - all the resources needs to be specified\nExample Shoot manifest (non-zoned) Please find below an example Shoot manifest for a non-zoned cluster:\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata:  name: johndoe-azure  namespace: garden-dev spec:  cloudProfileName: azure  region: westeurope  secretBindingName: core-azure  provider:  type: azure  infrastructureConfig:  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  vnet:  cidr: 10.250.0.0/16  workers: 10.250.0.0/19  zoned: false  controlPlaneConfig:  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1  kind: ControlPlaneConfig  workers:  - name: worker-xoluy  machine:  type: Standard_D4_v3  minimum: 2  maximum: 2  volume:  size: 50Gi  type: Standard_LRS # providerConfig: # apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1 # kind: WorkerConfig # nodeTemplate: # (to be specified only if the node capacity would be different from cloudprofile info during runtime) # capacity: # cpu: 2 # gpu: 1 # memory: 50Gi  networking:  type: calico  pods: 100.96.0.0/11  nodes: 10.250.0.0/16  services: 100.64.0.0/13  kubernetes:  version: 1.24.3  maintenance:  autoUpdate:  kubernetesVersion: true  machineImageVersion: true  addons:  kubernetesDashboard:  enabled: true  nginxIngress:  enabled: true Example Shoot manifest (zoned) Please find below an example Shoot manifest for a zoned cluster:\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata:  name: johndoe-azure  namespace: garden-dev spec:  cloudProfileName: azure  region: westeurope  secretBindingName: core-azure  provider:  type: azure  infrastructureConfig:  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  vnet:  cidr: 10.250.0.0/16  workers: 10.250.0.0/19  zoned: true  controlPlaneConfig:  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1  kind: ControlPlaneConfig  workers:  - name: worker-xoluy  machine:  type: Standard_D4_v3  minimum: 2  maximum: 2  volume:  size: 50Gi  type: Standard_LRS  zones:  - \"1\"  - \"2\"  networking:  type: calico  pods: 100.96.0.0/11  nodes: 10.250.0.0/16  services: 100.64.0.0/13  kubernetes:  version: 1.24.3  maintenance:  autoUpdate:  kubernetesVersion: true  machineImageVersion: true  addons:  kubernetesDashboard:  enabled: true  nginxIngress:  enabled: true Example Shoot manifest (zoned with NAT Gateways per zone) Please find below an example Shoot manifest for a zoned cluster using NAT Gateways per zone:\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata:  name: johndoe-azure  namespace: garden-dev spec:  cloudProfileName: azure  region: westeurope  secretBindingName: core-azure  provider:  type: azure  infrastructureConfig:  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  vnet:  cidr: 10.250.0.0/16  zones:  - name: 1  cidr: 10.250.0.0/24  serviceEndpoints:  - Microsoft.Storage  - Microsoft.Sql  natGateway:  enabled: true  idleConnectionTimeoutMinutes: 4  - name: 2  cidr: 10.250.1.0/24  serviceEndpoints:  - Microsoft.Storage  - Microsoft.Sql  natGateway:  enabled: true  zoned: true  controlPlaneConfig:  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1  kind: ControlPlaneConfig  workers:  - name: worker-xoluy  machine:  type: Standard_D4_v3  minimum: 2  maximum: 2  volume:  size: 50Gi  type: Standard_LRS  zones:  - \"1\"  - \"2\"  networking:  type: calico  pods: 100.96.0.0/11  nodes: 10.250.0.0/16  services: 100.64.0.0/13  kubernetes:  version: 1.24.3  maintenance:  autoUpdate:  kubernetesVersion: true  machineImageVersion: true  addons:  kubernetesDashboard:  enabled: true  nginxIngress:  enabled: true CSI volume provisioners Every Azure shoot cluster that has at least Kubernetes v1.21 will be deployed with the Azure Disk CSI driver and the Azure File CSI driver. Both are compatible with the legacy in-tree volume provisioners that were deprecated by the Kubernetes community and will be removed in future versions of Kubernetes. End-users might want to update their custom StorageClasses to the new disk.csi.azure.com or file.csi.azure.com provisioner, respectively. Shoot clusters with Kubernetes v1.20 or less will use the in-tree kubernetes.io/azure-disk and kubernetes.io/azure-file volume provisioners in the kube-controller-manager and the kubelet.\nKubernetes Versions per Worker Pool This extension supports gardener/gardener’s WorkerPoolKubernetesVersion feature gate, i.e., having worker pools with overridden Kubernetes versions since gardener-extension-provider-azure@v1.25. Note that this feature is only usable for Shoots whose .spec.kubernetes.version is greater or equal than the CSI migration version (1.21).\nShoot CA Certificate and ServiceAccount Signing Key Rotation This extension supports gardener/gardener’s ShootCARotation and ShootSARotation feature gates since gardener-extension-provider-azure@v1.28.\nMiscellaneous Azure Accelerated Networking All worker machines of the cluster will be automatically configured to use Azure Accelerated Networking if the prerequisites are fulfilled. The prerequisites are that the cluster must be zoned, and the used machine type and operating system image version are compatible for Accelerated Networking. Availability Set based shoot clusters will not be enabled for accelerated networking even if the machine type and operating system support it, this is necessary because all machines from the availability set must be scheduled on special hardware, more daitls can be found here. Supported machine types are listed in the CloudProfile in .spec.providerConfig.machineTypes[].acceleratedNetworking and the supported operating system image versions are defined in .spec.providerConfig.machineImages[].versions[].acceleratedNetworking.\nPreview: Shoot clusters with VMSS Flexible Orchestration (VMSS Flex/VMO) The machines of an Azure cluster can be created while being attached to an Azure Virtual Machine ScaleSet with flexible orchestraion. The Virtual Machine ScaleSet with flexible orchestration feature is currently in preview and not yet general available on Azure. Subscriptions need to join the preview to make use of the feature.\nAzure VMSS Flex is intended to replace Azure AvailabilitySet for non-zoned Azure Shoot clusters in the mid-term (once the feature goes GA) as VMSS Flex come with less disadvantages like no blocking machine operations or compability with Standard SKU loadbalancer etc.\nTo configure an Azure Shoot cluster which make use of VMSS Flex you need to do the following:\n The InfrastructureConfig of the Shoot configuration need to contain .zoned=false Shoot resource need to have the following annotation assigned: alpha.azure.provider.extensions.gardener.cloud/vmo=true  Some key facts about VMSS Flex based clusters:\n Unlike regular non-zonal Azure Shoot clusters, which have a primary AvailabilitySet which is shared between all machines in all worker pools of a Shoot cluster, a VMSS Flex based cluster has an own VMSS for each workerpool In case the configuration of the VMSS will change (e.g. amount of fault domains in a region change; configured in the CloudProfile) all machines of the worker pool need to be rolled It is not possible to migrate an existing primary AvailabilitySet based Shoot cluster to VMSS Flex based Shoot cluster and vice versa VMSS Flex based clusters are using Standard SKU LoadBalancers instead of Basic SKU LoadBalancers for AvailabilitySet based Shoot clusters  ","categories":"","description":"","excerpt":"Using the Azure provider extension with Gardener as end-user The …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-azure/docs/usage-as-end-user/","tags":"","title":"Usage As End User"},{"body":"Using the Equinix Metal provider extension with Gardener as end-user The core.gardener.cloud/v1beta1.Shoot resource declares a few fields that are meant to contain provider-specific configuration.\nIn this document we are describing how this configuration looks like for Equinix Metal and provide an example Shoot manifest with minimal configuration that you can use to create an Equinix Metal cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).\nProvider secret data Every shoot cluster references a SecretBinding which itself references a Secret, and this Secret contains the provider credentials of your Equinix Metal project. This Secret must look as follows:\napiVersion: v1 kind: Secret metadata:  name: my-secret  namespace: garden-dev type: Opaque data:  apiToken: base64(api-token)  projectID: base64(project-id) Please look up https://metal.equinix.com/developers/api/ as well.\nWith Secret created, create a SecretBinding resource referencing it. It may look like this:\napiVersion: core.gardener.cloud/v1beta1 kind: SecretBinding metadata:  name: my-secret  namespace: garden-dev secretRef:  name: my-secret quotas: [] InfrastructureConfig Currently, there is no infrastructure configuration possible for the Equinix Metal environment.\nAn example InfrastructureConfig for the Equinix Metal extension looks as follows:\napiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1 kind: InfrastructureConfig The Equinix Metal extension will only create a key pair.\nControlPlaneConfig The control plane configuration mainly contains values for the Equinix Metal-specific control plane components. Today, the Equinix Metal extension deploys the cloud-controller-manager and the CSI controllers, however, it doesn’t offer any configuration options at the moment.\nAn example ControlPlaneConfig for the Equinix Metal extension looks as follows:\napiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1 kind: ControlPlaneConfig WorkerConfig The Equinix Metal extension supports specifying IDs for reserved devices that should be used for the machines of a specific worker pool.\nAn example WorkerConfig for the Equinix Metal extension looks as follows:\napiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1 kind: WorkerConfig reservationIDs: - my-reserved-device-1 - my-reserved-device-2 reservedDevicesOnly: false The .reservationIDs[] list contains the list of IDs of the reserved devices. The .reservedDevicesOnly field indicates whether only reserved devices from the provided list of reservation IDs should be used when new machines are created. It always will attempt to create a device from one of the reservation IDs. If none is available, the behaviour depends on the setting:\n true: return an error false: request a regular on-demand device  The default value is false.\nExample Shoot manifest Please find below an example Shoot manifest:\napiVersion: core.gardener.cloud/v1alpha1 kind: Shoot metadata:  name: my-shoot  namespace: garden-dev spec:  cloudProfileName: equinix-metal  region: ny # Corresponds to a metro  secretBindingName: my-secret  provider:  type: equinixmetal  infrastructureConfig:  apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  controlPlaneConfig:  apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1  kind: ControlPlaneConfig  workers:  - name: worker-pool1  minimum: 2  maximum: 2  volume:  size: 50Gi  type: storage_1  zones: # Optional list of facilities, all of which MUST be in the metro; if not provided, then random facilities within the metro will be chosen for each machine.  - ewr1  - ny5  - name: reserved-pool  machine:  type: t1.small  minimum: 1  maximum: 2  providerConfig:  apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1  kind: WorkerConfig  reservationIDs:  - reserved-device1  - reserved-device2  reservedDevicesOnly: true  volume:  size: 50Gi  type: storage_1  networking:  type: calico  kubernetes:  version: 1.20.2  maintenance:  autoUpdate:  kubernetesVersion: true  machineImageVersion: true  addons:  kubernetes-dashboard:  enabled: true  nginx-ingress:  enabled: true ⚠️ Note that if you specify multiple facilities in the .spec.provider.workers[].zones[] list then new machines are randomly created in one of the provided facilities. Particularly, it is not ensured that all facilities are used or that all machines are equally or unequally distributed.\nKubernetes Versions per Worker Pool This extension supports gardener/gardener’s WorkerPoolKubernetesVersion feature gate, i.e., having worker pools with overridden Kubernetes versions since gardener-extension-provider-equinix-metal@v2.2.\nShoot CA Certificate and ServiceAccount Signing Key Rotation This extension supports gardener/gardener’s ShootCARotation feature gate since gardener-extension-provider-equinix-metal@v2.3 and ShootSARotation feature gate since gardener-extension-provider-equinix-metal@v2.4.\n","categories":"","description":"","excerpt":"Using the Equinix Metal provider extension with Gardener as end-user …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-equinix-metal/docs/usage-as-end-user/","tags":"","title":"Usage As End User"},{"body":"Using the GCP provider extension with Gardener as end-user The core.gardener.cloud/v1beta1.Shoot resource declares a few fields that are meant to contain provider-specific configuration.\nThis document describes the configurable options for GCP and provides an example Shoot manifest with minimal configuration that can be used to create a GCP cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).\nGCP Provider Credentials In order for Gardener to create a Kubernetes cluster using GCP infrastructure components, a Shoot has to provide credentials with sufficient permissions to the desired GCP project. Every shoot cluster references a SecretBinding which itself references a Secret, and this Secret contains the provider credentials of the GCP project. The SecretBinding is configurable in the Shoot cluster with the field secretBindingName.\nThe required credentials for the GCP project are a Service Account Key to authenticate as a GCP Service Account. A service account is a special account that can be used by services and applications to interact with Google Cloud Platform APIs. Applications can use service account credentials to authorize themselves to a set of APIs and perform actions within the permissions granted to the service account.\nMake sure to enable the Google Identity and Access Management (IAM) API. Create a Service Account that shall be used for the Shoot cluster. Grant at least the following IAM roles to the Service Account.\n Service Account Admin Service Account Token Creator Service Account User Compute Admin  Create a JSON Service Account key for the Service Account. Provide it in the Secret (base64 encoded for field serviceaccount.json), that is being referenced by the SecretBinding in the Shoot cluster configuration.\nThis Secret must look as follows:\napiVersion: v1 kind: Secret metadata:  name: core-gcp  namespace: garden-dev type: Opaque data:  serviceaccount.json: base64(serviceaccount-json) ⚠️ Depending on your API usage it can be problematic to reuse the same Service Account Key for different Shoot clusters due to rate limits. Please consider spreading your Shoots over multiple Service Accounts on different GCP projects if you are hitting those limits, see https://cloud.google.com/compute/docs/api-rate-limits.\nInfrastructureConfig The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.\nAn example InfrastructureConfig for the GCP extension looks as follows:\napiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1 kind: InfrastructureConfig networks: # vpc: # name: my-vpc # cloudRouter: # name: my-cloudrouter  workers: 10.250.0.0/16 # internal: 10.251.0.0/16 # cloudNAT: # minPortsPerVM: 2048 # natIPNames: # - name: manualnat1 # - name: manualnat2 # flowLogs: # aggregationInterval: INTERVAL_5_SEC # flowSampling: 0.2 # metadata: INCLUDE_ALL_METADATA The networks.vpc section describes whether you want to create the shoot cluster in an already existing VPC or whether to create a new one:\n  If networks.vpc.name is given then you have to specify the VPC name of the existing VPC that was created by other means (manually, other tooling, …). If you want to get a fresh VPC for the shoot then just omit the networks.vpc field.\n  If a VPC name is not given then we will create the cloud router + NAT gateway to ensure that worker nodes don’t get external IPs.\n  If a VPC name is given then a cloud router name must also be given, failure to do so would result in validation errors and possibly clusters without egress connectivity.\n  The networks.workers section describes the CIDR for a subnet that is used for all shoot worker nodes, i.e., VMs which later run your applications.\nThe networks.internal section is optional and can describe a CIDR for a subnet that is used for internal load balancers,\nThe networks.cloudNAT.minPortsPerVM is optional and is used to define the minimum number of ports allocated to a VM for the CloudNAT\nThe networks.cloudNAT.natIPNames is optional and is used to specify the names of the manual ip addresses which should be used by the nat gateway\nThe specified CIDR ranges must be contained in the VPC CIDR specified above, or the VPC CIDR of your already existing VPC. You can freely choose these CIDRs and it is your responsibility to properly design the network layout to suit your needs.\nThe networks.flowLogs section describes the configuration for the VPC flow logs. In order to enable the VPC flow logs at least one of the following parameters needs to be specified in the flow log section:\n  networks.flowLogs.aggregationInterval an optional parameter describing the aggregation interval for collecting flow logs. For more details, see aggregation_interval reference.\n  networks.flowLogs.flowSampling an optional parameter describing the sampling rate of VPC flow logs within the subnetwork where 1.0 means all collected logs are reported and 0.0 means no logs are reported. For more details, see flow_sampling reference.\n  networks.flowLogs.metadata an optional parameter describing whether metadata fields should be added to the reported VPC flow logs. For more details, see metadata reference.\n  Apart from the VPC and the subnets the GCP extension will also create a dedicated service account for this shoot, and firewall rules.\nControlPlaneConfig The control plane configuration mainly contains values for the GCP-specific control plane components. Today, the only component deployed by the GCP extension is the cloud-controller-manager.\nAn example ControlPlaneConfig for the GCP extension looks as follows:\napiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1 kind: ControlPlaneConfig zone: europe-west1-b cloudControllerManager:  featureGates:  CustomResourceValidation: true The zone field tells the cloud-controller-manager in which zone it should mainly operate. You can still create clusters in multiple availability zones, however, the cloud-controller-manager requires one “main” zone. ⚠️ You always have to specify this field!\nThe cloudControllerManager.featureGates contains a map of explicitly enabled or disabled feature gates. For production usage it’s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability. If you don’t want to configure anything for the cloudControllerManager simply omit the key in the YAML specification.\nWorkerConfig The worker configuration contains:\n  Local SSD interface for the additional volumes attached to GCP worker machines.\nIf you attach the disk with SCRATCH type, either an NVMe interface or a SCSI interface must be specified. It is only meaningful to provide this volume interface if only SCRATCH data volumes are used.\n  Service Account with their specified scopes, authorized for this worker.\nService accounts created in advance that generate access tokens that can be accessed through the metadata server and used to authenticate applications on the instance.\n  GPU with its type and count per node. This will attach that GPU to all the machines in the worker grp\nNote:\n A rolling upgrade of the worker group would be triggered in case the acceleratorType or count is updated. Some machineTypes like a2 family come with already attached gpu of a100 type and pre-defined count. If your workerPool consists of those machineTypes, please do not specify any GPU configuration. Sufficient quota of gpu is needed in the GCP project. This includes quota to support autoscaling if enabled. GPU-attached machines can’t be live migrated during host maintenance events. Find out how to handle that in your application here GPU count specified here is considered for forming node template during scale-from-zero in Cluster Autoscaler  An example WorkerConfig for the GCP looks as follows:\n  apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1 kind: WorkerConfig volume:  interface: NVME serviceAccount:  email: foo@bar.com  scopes:  - https://www.googleapis.com/auth/cloud-platform gpu:  acceleratorType: nvidia-tesla-t4  count: 1 Example Shoot manifest Please find below an example Shoot manifest:\napiVersion: core.gardener.cloud/v1alpha1 kind: Shoot metadata:  name: johndoe-gcp  namespace: garden-dev spec:  cloudProfileName: gcp  region: europe-west1  secretBindingName: core-gcp  provider:  type: gcp  infrastructureConfig:  apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  workers: 10.250.0.0/16  controlPlaneConfig:  apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1  kind: ControlPlaneConfig  zone: europe-west1-b  workers:  - name: worker-xoluy  machine:  type: n1-standard-4  minimum: 2  maximum: 2  volume:  size: 50Gi  type: pd-standard  zones:  - europe-west1-b  networking:  nodes: 10.250.0.0/16  type: calico  kubernetes:  version: 1.24.3  maintenance:  autoUpdate:  kubernetesVersion: true  machineImageVersion: true  addons:  kubernetes-dashboard:  enabled: true  nginx-ingress:  enabled: true CSI volume provisioners Every GCP shoot cluster that has at least Kubernetes v1.18 will be deployed with the GCP PD CSI driver. It is compatible with the legacy in-tree volume provisioner that was deprecated by the Kubernetes community and will be removed in future versions of Kubernetes. End-users might want to update their custom StorageClasses to the new pd.csi.storage.gke.io provisioner. Shoot clusters with Kubernetes v1.17 or less will use the in-tree kubernetes.io/gce-pd volume provisioner in the kube-controller-manager and the kubelet.\nKubernetes Versions per Worker Pool This extension supports gardener/gardener’s WorkerPoolKubernetesVersion feature gate, i.e., having worker pools with overridden Kubernetes versions since gardener-extension-provider-gcp@v1.21. Note that this feature is only usable for Shoots whose .spec.kubernetes.version is greater or equal than the CSI migration version (1.18).\nShoot CA Certificate and ServiceAccount Signing Key Rotation This extension supports gardener/gardener’s ShootCARotation and ShootSARotation feature gates since gardener-extension-provider-gcp@v1.23.\n","categories":"","description":"","excerpt":"Using the GCP provider extension with Gardener as end-user The …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-gcp/docs/usage-as-end-user/","tags":"","title":"Usage As End User"},{"body":"Using the OpenStack provider extension with Gardener as end-user The core.gardener.cloud/v1beta1.Shoot resource declares a few fields that are meant to contain provider-specific configuration.\nIn this document we are describing how this configuration looks like for OpenStack and provide an example Shoot manifest with minimal configuration that you can use to create an OpenStack cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).\nProvider Secret Data Every shoot cluster references a SecretBinding which itself references a Secret, and this Secret contains the provider credentials of your OpenStack tenant. This Secret must look as follows:\napiVersion: v1 kind: Secret metadata:  name: core-openstack  namespace: garden-dev type: Opaque data:  domainName: base64(domain-name)  tenantName: base64(tenant-name)   # either use username/password  username: base64(user-name)  password: base64(password)   # or application credentials  #applicationCredentialID: base64(app-credential-id)  #applicationCredentialName: base64(app-credential-name) # optional  #applicationCredentialSecret: base64(app-credential-secret) Please look up https://docs.openstack.org/keystone/pike/admin/identity-concepts.html as well.\nFor authentication with username/password see Keystone username/password\nAlternatively, for authentication with application credentials see Keystone Application Credentials. Application Credentials are not supported for shoots with kubernetes versions less than v1.19.\n⚠️ Depending on your API usage it can be problematic to reuse the same provider credentials for different Shoot clusters due to rate limits. Please consider spreading your Shoots over multiple credentials from different tenants if you are hitting those limits.\nInfrastructureConfig The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.\nAn example InfrastructureConfig for the OpenStack extension looks as follows:\napiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1 kind: InfrastructureConfig floatingPoolName: MY-FLOATING-POOL # floatingPoolSubnetName: my-floating-pool-subnet-name networks: # id: 12345678-abcd-efef-08af-0123456789ab # router: # id: 1234  workers: 10.250.0.0/19 The floatingPoolName is the name of the floating pool you want to use for your shoot. If you don’t know which floating pools are available look it up in the respective CloudProfile.\nWith floatingPoolSubnetName you can explicitly define to which subnet in the floating pool network (defined via floatingPoolName) the router should be attached to.\nIf networks.id is an optional field. If it is given, you can specify the uuid of an existing private Neutron network (created manually, by other tooling, …) that should be reused. A new subnet for the Shoot will be created in it.\nThe networks.router section describes whether you want to create the shoot cluster in an already existing router or whether to create a new one:\n  If networks.router.id is given then you have to specify the router id of the existing router that was created by other means (manually, other tooling, …). If you want to get a fresh router for the shoot then just omit the networks.router field.\n  In any case, the shoot cluster will be created in a new subnet.\n  The networks.workers section describes the CIDR for a subnet that is used for all shoot worker nodes, i.e., VMs which later run your applications.\nYou can freely choose these CIDRs and it is your responsibility to properly design the network layout to suit your needs.\nApart from the router and the worker subnet the OpenStack extension will also create a network, router interfaces, security groups, and a key pair.\nControlPlaneConfig The control plane configuration mainly contains values for the OpenStack-specific control plane components. Today, the only component deployed by the OpenStack extension is the cloud-controller-manager.\nAn example ControlPlaneConfig for the OpenStack extension looks as follows:\napiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1 kind: ControlPlaneConfig loadBalancerProvider: haproxy loadBalancerClasses: - name: lbclass-1  purpose: default  floatingNetworkID: fips-1-id  floatingSubnetName: internet-* - name: lbclass-2  floatingNetworkID: fips-1-id  floatingSubnetTags: internal,private - name: lbclass-3  purpose: private  subnetID: internal-id cloudControllerManager:  featureGates:  CustomResourceValidation: true The loadBalancerProvider is the provider name you want to use for load balancers in your shoot. If you don’t know which types are available look it up in the respective CloudProfile.\nThe loadBalancerClasses field contains an optional list of load balancer classes which will be available in the cluster. Each entry can have the following fields:\n name to select the load balancer class via the kubernetes service annotations loadbalancer.openstack.org/class=name purpose with values default or private  The configuration of the default load balancer class will be used as default for all other kubernetes loadbalancer services without a class annotation The configuration of the private load balancer class will be also set to the global loadbalancer configuration of the cluster, but will be overridden by the default purpose   floatingNetworkID can be specified to receive an ip from an floating/external network, additionally the subnet in this network can be selected via  floatingSubnetName can be either a full subnet name or a regex/glob to match subnet name floatingSubnetTags a comma seperated list of subnet tags floatingSubnetID the id of a specific subnet   subnetID can be specified by to receive an ip from an internal subnet (will not have an effect in combination with floating/external network configuration)  The cloudControllerManager.featureGates contains a map of explicitly enabled or disabled feature gates. For production usage it’s not recommended to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability. If you don’t want to configure anything for the cloudControllerManager simply omit the key in the YAML specification.\nWorkerConfig Each worker group in a shoot may contain provider-specific configurations and options. These are contained in the providerConfig section of a worker group and can be configured using a WorkerConfig object. An example of a WorkerConfig looks as follows:\napiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1 kind: WorkerConfig serverGroup:  policy: soft-anti-affinity When you specify the serverGroup section in your worker group configuration, a new server group will be created with the configured policy for each worker group that enabled this setting and all machines managed by this worker group will be assigned as members of the created server group.\nFor users to have access to the server group feature, it must be enabled on the CloudProfile by your operator. Existing clusters can take advantage of this feature by updating the server group configuration of their respective worker groups. Worker groups that are already configured with server groups can update their setting to change the policy used, or remove it altogether at any time.\nUsers must be aware that any change to the server group settings will result in a rolling deployment of new nodes for the affected worker group.\nPlease note the following restrictions when deploying workers with server groups:\n The serverGroup section is optional, but if it is included in the worker configuration, it must contain a valid policy value. The available policy values that can be used, are defined in the provider specific section of CloudProfile by your operator. Certain policy values may induce further constraints. Using the affinity policy is only allowed when the worker group utilizes a single zone.  Example Shoot manifest (one availability zone) Please find below an example Shoot manifest for one availability zone:\napiVersion: core.gardener.cloud/v1alpha1 kind: Shoot metadata:  name: johndoe-openstack  namespace: garden-dev spec:  cloudProfileName: openstack  region: europe-1  secretBindingName: core-openstack  provider:  type: openstack  infrastructureConfig:  apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  floatingPoolName: MY-FLOATING-POOL  networks:  workers: 10.250.0.0/19  controlPlaneConfig:  apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1  kind: ControlPlaneConfig  loadBalancerProvider: haproxy  workers:  - name: worker-xoluy  machine:  type: medium_4_8  minimum: 2  maximum: 2  zones:  - europe-1a  networking:  nodes: 10.250.0.0/16  type: calico  kubernetes:  version: 1.24.3  maintenance:  autoUpdate:  kubernetesVersion: true  machineImageVersion: true  addons:  kubernetes-dashboard:  enabled: true  nginx-ingress:  enabled: true CSI volume provisioners Every OpenStack shoot cluster that has at least Kubernetes v1.19 will be deployed with the OpenStack Cinder CSI driver. It is compatible with the legacy in-tree volume provisioner that was deprecated by the Kubernetes community and will be removed in future versions of Kubernetes. End-users might want to update their custom StorageClasses to the new cinder.csi.openstack.org provisioner. Shoot clusters with Kubernetes v1.18 or less will use the in-tree kubernetes.io/cinder volume provisioner in the kube-controller-manager and the kubelet.\nKubernetes Versions per Worker Pool This extension supports gardener/gardener’s WorkerPoolKubernetesVersion feature gate, i.e., having worker pools with overridden Kubernetes versions since gardener-extension-provider-openstack@v1.23. Note that this feature is only usable for Shoots whose .spec.kubernetes.version is greater or equal than the CSI migration version (1.19).\nShoot CA Certificate and ServiceAccount Signing Key Rotation This extension supports gardener/gardener’s ShootCARotation and ShootSARotation feature gates since gardener-extension-provider-openstack@v1.26.\n","categories":"","description":"","excerpt":"Using the OpenStack provider extension with Gardener as end-user The …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-openstack/docs/usage-as-end-user/","tags":"","title":"Usage As End User"},{"body":"Using the vSphere provider extension with Gardener as end-user The core.gardener.cloud/v1alpha1.Shoot resource declares a few fields that are meant to contain provider-specific configuration.\nIn this document we are describing how this configuration looks like for VMware vSphere and provide an example Shoot manifest with minimal configuration that you can use to create an vSphere cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).\nProvider secret data Every shoot cluster references a SecretBinding which itself references a Secret, and this Secret contains the provider credentials of your vSphere tenant. It contains two authentication sets. One for the vSphere host and another for the NSX-T host, which is needed to set up the network infrastructure. This Secret must look as follows:\napiVersion: v1 kind: Secret metadata:  name: core-vsphere  namespace: garden-dev type: Opaque data:  vspherePassword: base64(vsphere-password)  vsphereUsername: base64(vSphere-UserName)  vsphereInsecureSSL: base64(\"true\"|\"false\")  nsxtPassword: base64(NSX-T-password)  nsxtUserName: base64(NSX-T-UserName)  nsxtInsecureSSL: base64(\"true\"|\"false\") Here base64(...) are only a placeholders for the Base64 encoded values.\nInfrastructureConfig The infrastructure configuration is used for advanced scenarios only. Nodes on all zones are using IP addresses from the common nodes network as the network is managed by NSX-T. The infrastructure controller will create several network objects using NSX-T. A network segment is used as the subnet for the VMs (nodes), a tier-1 gateway, a DHCP server, and a SNAT for the nodes.\nAn example InfrastructureConfig for the vSphere extension looks as follows. You only need to specify it, if you either want to use an existing Tier-1 gateway and load balancer service pair or if you want to overwrite the automatic selection of the NSX-T version.\ninfrastructureConfig:  apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  #overwriteNSXTInfraVersion: '1'  #networks:  # tier1GatewayPath: /infra/tier-1s/tier1gw-b8213651-9659-4180-8bfd-1e16228e8dcb  # loadBalancerServicePath: /infra/lb-services/708c5cb1-e5d0-4b16-906f-ec7177a1485d Advanced configuration settings Section networks By default, the infrastructure controller creates a separate Tier-1 gateway for each shoot cluster and the cloud controller manager (vsphere-cloud-provider) creates a load balancer service.\nIf an existing Tier-1 gateway should be used, you can specify its ‘path’. In this case, there must also be a load balancer service defined for this tier-1 gateway and its ‘path’ needs to be specified, too. In the NSX-T manager UI, the path of the tier-1 gateway can be found at Networking / Tier-1 Gateways. Then select Copy path to clipboard from the context menu of the tier-1 gateway (click on the three vertical dots on the left of the row). Do the same with the corresponding load balancer at Networking / Load balancing / Tab Load Balancers For security reasons the referenced Tier-1 gateway in NSX-T must have a tag with scope authorized-shoots and its tag value consists of a comma-separated list of the allowed shoot names in the format shoot--\u003cproject\u003e--\u003cname\u003e (optionally with wildcard *). Additionally, it must have a tag with scope garden set to the garden ID.\nExample:\ninfrastructureConfig:  apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  tier1GatewayPath: /infra/tier-1s/tier1gw-b8213651-9659-4180-8bfd-1e16228e8dcb  loadBalancerServicePath: /infra/lb-services/708c5cb1-e5d0-4b16-906f-ec7177a1485d Please ensure, that the worker nodes cidr (shoot manifest spec.networking.nodes) do not overlap with other existing segments of the selected tier-1 gateway.\nOption overwriteNSXTInfraVersion The option overwriteNSXTInfraVersion can be used to change the network objects created during the initial infrastructure creation. By default the infra-version is automatically selected according to the NSX-T version. The infra-version '1' is used for NSX-T 2.5, and infra-version '2' for NSX-T versions \u003e= 3.0. The difference is creation of the the logical DHCP server. For NSX-T 2.5, only the DHCP server of the “Advanced API” is usable. For NSX-T \u003e= 3.0 the new DHCP server is default, but for special purposes infra-version '1' is also allowed.\nControlPlaneConfig The control plane configuration mainly contains values for the vSphere-specific control plane components. Today, the only component deployed by the vSphere extension is the cloud-controller-manager.\nAn example ControlPlaneConfig for the vSphere extension looks as follows:\napiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1 kind: ControlPlaneConfig loadBalancerClasses:  - name: mypubliclbclass  - name: myprivatelbclass  ipPoolName: pool42 # optional overwrite loadBalancerSize: SMALL cloudControllerManager:  featureGates:  CustomResourceValidation: true The loadBalancerClasses optionally defines the load balancer classes to be used. The specified names must be defined in the constraints section of the cloud profile. If the list contains a load balancer named “default”, it is used as the default load balancer. Otherwise the first one is also the default. If no classes are specified the default load balancer class is used as defined in the cloud profile constraints section. If the ipPoolName is overwritten, the corresponding IP pool object in NSX-T must have a tag with scope authorized-shoots and its tag value consists of a comma-separated list of the allowed shoot names in the format shoot--\u003cproject\u003e--\u003cname\u003e (optionally with wildcard *). Additionally, it must have a tag with scope garden set to the garden ID.\nThe loadBalancerSize is optional and overwrites the default value specified in the cloud profile config. It must be one of the values SMALL, MEDIUM, or LARGE. SMALL can manage 10 service ports, MEDIUM 100, and LARGE 1000.\nThe cloudControllerManager.featureGates contains an optional map of explicitly enabled or disabled feature gates. For production usage it’s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability. If you don’t want to configure anything for the cloudControllerManager simply omit the key in the YAML specification.\nExample Shoot manifest (one availability zone) Please find below an example Shoot manifest for one availability zone:\napiVersion: core.gardener.cloud/v1alpha1 kind: Shoot metadata:  name: johndoe-vsphere  namespace: garden-dev spec:  cloudProfileName: vsphere  region: europe-1  secretBindingName: core-vsphere  provider:  type: vsphere   #infrastructureConfig:  # apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1  # kind: InfrastructureConfig  # overwriteNSXTInfraVersion: '1'   controlPlaneConfig:  apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1  kind: ControlPlaneConfig  # loadBalancerClasses:  # - name: mylbclass   workers:  - name: worker-xoluy  machine:  type: std-04  minimum: 2  maximum: 2  zones:  - europe-1a  networking:  nodes: 10.250.0.0/16  type: calico  kubernetes:  version: 1.16.1  maintenance:  autoUpdate:  kubernetesVersion: true  machineImageVersion: true  addons:  kubernetes-dashboard:  enabled: true  nginx-ingress:  enabled: true Kubernetes Versions per Worker Pool This extension supports gardener/gardener’s WorkerPoolKubernetesVersion feature gate, i.e., having worker pools with overridden Kubernetes versions since gardener-extension-provider-vsphere@v0.12.\nShoot CA Certificate and ServiceAccount Signing Key Rotation This extension supports gardener/gardener’s ShootCARotation feature gate since gardener-extension-provider-vsphere@v0.13 and ShootSARotation feature gate since gardener-extension-provider-vsphere@v0.14.\n","categories":"","description":"","excerpt":"Using the vSphere provider extension with Gardener as end-user The …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/docs/usage-as-end-user/","tags":"","title":"Usage As End User"},{"body":"Using the Networking Calico extension with Gardener as end-user The core.gardener.cloud/v1beta1.Shoot resource declares a networking field that is meant to contain network-specific configuration.\nIn this document we are describing how this configuration looks like for Calico and provide an example Shoot manifest with minimal configuration that you can use to create a cluster.\nCalico Typha Calico Typha is an optional component of Project Calico designed to offload the Kubernetes API server. The Typha daemon sits between the datastore (such as the Kubernetes API server which is the one used by Gardener managed Kubernetes) and many instances of Felix. Typha’s main purpose is to increase scale by reducing each node’s impact on the datastore. You can opt-out Typha via .spec.networking.providerConfig.typha.enabled=false of your Shoot manifest. By default the Typha is enabled.\nEBPF Dataplane Calico can be run in ebpf dataplane mode. This has several benefits, calico scales to higher troughput, uses less cpu per GBit and has native support for kubernetes services (without needing kube-proxy). To switch to a pure ebpf dataplane it is recommended to run without an overlay network. The following configuration can be used to run without an overlay and without kube-proxy.\nAn example ebpf dataplane NetworkingConfig manifest:\napiVersion: calico.networking.extensions.gardener.cloud/v1alpha1 kind: NetworkConfig ipv4:  mode: Never ebpfDataplane:  enabled: true backend: bird To disable kube-proxy set the enabled field to false in the shoot manifest.\napiVersion: core.gardener.cloud/v1alpha1 kind: Shoot metadata:  name: ebpf-shoot  namespace: garden-dev spec:  kubernetes:  kubeProxy:  enabled: false Example NetworkingConfig manifest An example NetworkingConfig for the Calico extension looks as follows:\napiVersion: calico.networking.extensions.gardener.cloud/v1alpha1 kind: NetworkConfig ipam:  type: host-local  cidr: usePodCIDR ipv4:  mode: Always vethMTU: 1440 backend: bird typha:  enabled: true Example Shoot manifest Please find below an example Shoot manifest with calico networking configratations:\napiVersion: core.gardener.cloud/v1alpha1 kind: Shoot metadata:  name: johndoe-azure  namespace: garden-dev spec:  cloudProfileName: azure  region: westeurope  secretBindingName: core-azure  provider:  type: azure  infrastructureConfig:  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureConfig  networks:  vnet:  cidr: 10.250.0.0/16  workers: 10.250.0.0/19  zoned: true  controlPlaneConfig:  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1  kind: ControlPlaneConfig  workers:  - name: worker-xoluy  machine:  type: Standard_D4_v3  minimum: 2  maximum: 2  volume:  size: 50Gi  type: Standard_LRS  zones:  - \"1\"  - \"2\"  networking:  type: calico  nodes: 10.250.0.0/16  providerConfig:  apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1  kind: NetworkConfig  ipam:  type: host-local  vethMTU: 1440  backend: bird  typha:  enabled: false  kubernetes:  version: 1.16.1  maintenance:  autoUpdate:  kubernetesVersion: true  machineImageVersion: true  addons:  kubernetes-dashboard:  enabled: true  nginx-ingress:  enabled: true ","categories":"","description":"","excerpt":"Using the Networking Calico extension with Gardener as end-user The …","ref":"/docs/extensions/network-extensions/gardener-extension-networking-calico/docs/usage-as-end-user/","tags":"","title":"Usage As End User"},{"body":"Using the Networking Cilium extension with Gardener as end-user The core.gardener.cloud/v1beta1.Shoot resource declares a networking field that is meant to contain network-specific configuration.\nIn this document we are describing how this configuration looks like for Cilium and provide an example Shoot manifest with minimal configuration that you can use to create a cluster.\nCilium Hubble Hubble is a fully distributed networking and security observability platform build on top of Cilium and BPF. It is optional and is deployed to the cluster when enabled in the NetworkConfig. If the dashboard is not externally exposed\nkubectl port-forward -n kube-system deployment/hubble-ui 8081 can be used to acess it locally.\nExample NetworkingConfig manifest An example NetworkingConfig for the Cilium extension looks as follows:\napiVersion: cilium.networking.extensions.gardener.cloud/v1alpha1 kind: NetworkConfig hubble:  enabled: true #debug: false #psp: true #tunnel: vxlan #store: kubernetes NetworkingConfig options The hubble.enabled field describes whether hubble should be deployed into the cluster or not (default).\nThe debug field describes whether you want to run cilium in debug mode or not (default), change this value to true to use debug mode.\nThe psp field describes whether cilium-operator and cilium-agent shall be deployed with pod security policies or not (default).\nThe tunnel field describes the encapsulation mode for communication between nodes. Possible values are vxlan (default), geneve or disabled.\nThe store field describes which backend to use to store the identities. Can be either etcd (kvstore) or kubernetes (crd) (default).\nThe bpfSocketLBHostnsOnly.enabled field describes wheter socket LB will be skipped for services when inside a pod namespace (default), in favor of service LB at the pod interface. Socket LB is still used when in the host namespace. This feature is required when using cilium with a service mesh like istio or linkerd.\nThe egressGateway.enabled field describes wheter egress gateways are enabled or not (default). To use this feature kube-proxy must be disabled. This can be done with the following configuration in the shoot.yaml file:\nspec: kubernetes: kubeProxy: enabled: false Example Shoot manifest Please find below an example Shoot manifest with cilium networking configuration:\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata:  name: aws-cilium  namespace: garden-dev spec:  networking:  type: cilium  providerConfig:  apiVersion: cilium.networking.extensions.gardener.cloud/v1alpha1  kind: NetworkConfig  hubble:  enabled: true  pods: 100.96.0.0/11  nodes: 10.250.0.0/16  services: 100.64.0.0/13  ... If you would like to see a provider specific shoot example, please check out the documentation of the well-known extensions. A list of them can be found here.\n","categories":"","description":"","excerpt":"Using the Networking Cilium extension with Gardener as end-user The …","ref":"/docs/extensions/network-extensions/gardener-extension-networking-cilium/docs/usage-as-end-user/","tags":"","title":"Usage As End User"},{"body":"Using the CoreOS extension with Gardener as end-user The core.gardener.cloud/v1beta1.Shoot resource declares a few fields that must be considered when this OS extension is used.\nIn this document we describe how this configuration looks like and under which circumstances your attention may be required.\nAWS VPC settings for CoreOS workers Gardener allows you to create CoreOS based worker nodes by:\n Using a Gardener managed VPC Reusing a VPC that already exists (VPC id specified in InfrastructureConfig]  If the second option applies to your use-case please make sure that your VPC has enabled DNS Support. Otherwise CoreOS based nodes aren’t able to join or operate in your cluster properly.\nDNS settings (required):\n enableDnsHostnames: true (necessary for collecting node metrics) enableDnsSupport: true  ","categories":"","description":"","excerpt":"Using the CoreOS extension with Gardener as end-user The …","ref":"/docs/extensions/os-extensions/gardener-extension-os-coreos/docs/usage-as-end-user/","tags":"","title":"Usage As End User"},{"body":"Using the SuSE CHost extension with Gardener as end-user The core.gardener.cloud/v1beta1.Shoot resource declares a few fields that must be considered when this OS extension is used.\nIn this document we describe how this configuration looks like and under which circumstances your attention may be required.\nAWS VPC settings for SuSE CHost workers Gardener allows you to create SuSE CHost based worker nodes by:\n Using a Gardener managed VPC Reusing a VPC that already exists (VPC id specified in InfrastructureConfig]  If the second option applies to your use-case please make sure that your VPC has enabled DNS Support. Otherwise SuSE CHost based nodes aren’t able to join or operate in your cluster properly.\nDNS settings (required):\n enableDnsHostnames: true enableDnsSupport: true  Support for vSMP MemoryOne This extension controller is also capable of generating user-data for the vSMP MemoryOne operating system in conjunction with SuSE CHost. It reacts on the memoryone-chost extension type. Additionally, it allows certain customizations with the following configuration:\napiVersion: memoryone-chost.os.extensions.gardener.cloud/v1alpha1 kind: OperatingSystemConfiguration memoryTopology: \"3\" systemMemory: \"7x\"  The memoryTopology field controls the mem_topology setting. If it’s not provided then it will default to 2. The systemMemory field controls the system_memory setting. If it’s not provided then it defaults to 6x.  Please note that it was only e2e-tested on AWS. Additionally, you need a snapshot ID of a SuSE CHost/CHost volume (see below how to create it).\nAn exemplary worker pool configuration inside a Shoot resource using for the vSMP MemoryOne operating system would look as follows:\napiVersion: core.gardener.cloud/v1beta1 kind: Shoot metadata:  name: vsmp-memoryone  namespace: garden-foo spec:  ...  workers:  - name: cpu-worker3  minimum: 1  maximum: 1  maxSurge: 1  maxUnavailable: 0  machine:  image:  name: memoryone-chost  version: 9.5.195  providerConfig:  apiVersion: memoryone-chost.os.extensions.gardener.cloud/v1alpha1  kind: OperatingSystemConfiguration  memoryTopology: \"2\"  systemMemory: \"6x\"  type: c5d.metal  volume:  size: 20Gi  type: gp2  dataVolumes:  - name: chost  size: 50Gi  type: gp2  providerConfig:  apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1  kind: WorkerConfig  dataVolumes:  - name: chost  snapshotID: snap-123456  zones:  - eu-central-1b Please note that vSMP MemoryOne only works for EC2 bare-metal instance types such as M5d, R5, C5, C5d, etc. - please consult the EC2 instance types overview page and the documentation of vSMP MemoryOne to find out whether the instance type in question is eligible.\nGenerating an AWS snapshot ID for the CHost/CHost operating system The following script will help to generate the snapshot ID on AWS. It runs in the region that is selected in your $HOME/.aws/config file. Consequently, if you want to generate the snapshot in multiple regions, you have to run in multiple times after configuring the respective region using aws configure.\nami=\"ami-1234\" #Replace the ami with the intended one.  name=`aws ec2 describe-images --image-ids $ami --query=\"Images[].Name\" --output=text` cur=`aws ec2 describe-snapshots --filter=\"Name=description,Values=snap-$name\" --query=\"Snapshots[].Description\" --output=text` if [ -n \"$cur\" ]; then  echo \"AMI $nameexists as snapshot $cur\"  continue fi echo \"AMI $name... creating private snapshot\" inst=`aws ec2 run-instances --instance-type t3.nano --image-id $ami --query 'Instances[0].InstanceId' --output=text --subnet-id subnet-1234 --tag-specifications 'ResourceType=instance,Tags=[{Key=scalemp-test,Value=scalemp-test}]'` #Replace the subnet-id with the intended one. aws ec2 wait instance-running --instance-ids $inst vol=`aws ec2 describe-instances --instance-ids $inst --query \"Reservations[].Instances[].BlockDeviceMappings[0].Ebs.VolumeId\" --output=text` snap=`aws ec2 create-snapshot --description \"snap-$name\" --volume-id $vol --query='SnapshotId' --tag-specifications \"ResourceType=snapshot,Tags=[{Key=Name,Value=\\\"$name\\\"}]\" --output=text` aws ec2 wait snapshot-completed --snapshot-ids $snap aws ec2 terminate-instances --instance-id $inst \u003e /dev/null echo $snap ","categories":"","description":"","excerpt":"Using the SuSE CHost extension with Gardener as end-user The …","ref":"/docs/extensions/os-extensions/gardener-extension-os-suse-chost/docs/usage-as-end-user/","tags":"","title":"Usage As End User"},{"body":"Using the Ubuntu extension with Gardener as end-user The core.gardener.cloud/v1beta1.Shoot resource declares a few fields that must be considered when this OS extension is used.\nIn this document we describe how this configuration looks like and under which circumstances your attention may be required.\nAWS VPC settings for Ubuntu workers Gardener allows you to create Ubuntu based worker nodes by:\n Using a Gardener managed VPC Reusing a VPC that already exists (VPC id specified in InfrastructureConfig]  If the second option applies to your use-case please make sure that your VPC has enabled DNS Support. Otherwise Ubuntu based nodes aren’t able to join or operate in your cluster properly.\nDNS settings (required):\n enableDnsHostnames: true enableDnsSupport: true  ","categories":"","description":"","excerpt":"Using the Ubuntu extension with Gardener as end-user The …","ref":"/docs/extensions/os-extensions/gardener-extension-os-ubuntu/docs/usage-as-end-user/","tags":"","title":"Usage As End User"},{"body":"Using the Alicloud provider extension with Gardener as operator The core.gardener.cloud/v1beta1.CloudProfile resource declares a providerConfig field that is meant to contain provider-specific configuration. The core.gardener.cloud/v1beta1.Seed resource is structured similarly. Additionally, it allows configuring settings for the backups of the main etcds’ data of shoot clusters control planes running in this seed cluster.\nThis document explains the necessary configuration for this provider extension. In addition, this document also describes how to enable the use of customized machine images for Alicloud.\nCloudProfile resource This section describes, how the configuration for CloudProfile looks like for Alicloud by providing an example CloudProfile manifest with minimal configuration that can be used to allow the creation of Alicloud shoot clusters.\nCloudProfileConfig The cloud profile configuration contains information about the real machine image IDs in the Alicloud environment (AMIs). You have to map every version that you specify in .spec.machineImages[].versions here such that the Alicloud extension knows the AMI for every version you want to offer.\nAn example CloudProfileConfig for the Alicloud extension looks as follows:\napiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1 kind: CloudProfileConfig machineImages: - name: coreos  versions:  - version: 2023.4.0  regions:  - name: eu-central-1  id: coreos_2023_4_0_64_30G_alibase_20190319.vhd Example CloudProfile manifest Please find below an example CloudProfile manifest:\napiVersion: core.gardener.cloud/v1beta1 kind: CloudProfile metadata:  name: alicloud spec:  type: alicloud  kubernetes:  versions:  - version: 1.16.1  - version: 1.16.0  expirationDate: \"2020-04-05T01:02:03Z\"  machineImages:  - name: coreos  versions:  - version: 2023.4.0  machineTypes:  - name: ecs.sn2ne.large  cpu: \"2\"  gpu: \"0\"  memory: 8Gi  volumeTypes:  - name: cloud_efficiency  class: standard  - name: cloud_ssd  class: premium  regions:  - name: eu-central-1  zones:  - name: eu-central-1a  - name: eu-central-1b  providerConfig:  apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1  kind: CloudProfileConfig  machineImages:  - name: coreos  versions:  - version: 2023.4.0  regions:  - name: eu-central-1  id: coreos_2023_4_0_64_30G_alibase_20190319.vhd Enable customized machine images for the Alicloud extension Customized machine images can be created for an Alicloud account and shared with other Alicloud accounts. The same customized machine image has different image ID in different regions on Alicloud. If you need to enable encrypted system disk, you must provide customized machine images. Administrators/Operators need to explicitly declare them per imageID per region as below:\nmachineImages: - name: customized_coreos  regions:  - imageID: \u003cimage_id_in_eu_central_1\u003e  region: eu-central-1  - imageID: \u003cimage_id_in_cn_shanghai\u003e  region: cn-shanghai  ...  version: 2191.4.1 ... End-users have to have the permission to use the customized image from its creator Alicloud account. To enable end-users to use customized images, the images are shared from Alicloud account of Seed operator with end-users’ Alicloud accounts. Administrators/Operators need to explicitly provide Seed operator’s Alicloud account access credentials (base64 encoded) as below:\nmachineImageOwnerSecret:  name: machine-image-owner  accessKeyID: \u003cbase64_encoded_access_key_id\u003e  accessKeySecret: \u003cbase64_encoded_access_key_secret\u003e As a result, a Secret named machine-image-owner by default will be created in namespace of Alicloud provider extension.\nOperators should also maintain custom image IDs which are to be shared with end-users as below:\ntoBeSharedImageIDs: - \u003cimage_id_1\u003e - \u003cimage_id_2\u003e - \u003cimage_id_3\u003e Example ControllerDeployment manifest for enabling customized machine images apiVersion: core.gardener.cloud/v1beta1 kind: ControllerDeployment metadata:  name: extension-provider-alicloud spec:  type: helm  providerConfig:  chart: | H4sIFAAAAAAA/yk...  values:  config:  machineImageOwnerSecret:  accessKeyID: \u003cbase64_encoded_access_key_id\u003e  accessKeySecret: \u003cbase64_encoded_access_key_secret\u003e  toBeSharedImageIDs:  - \u003cimage_id_1\u003e  - \u003cimage_id_2\u003e  ...  machineImages:  - name: customized_coreos  regions:  - imageID: \u003cimage_id_in_eu_central_1\u003e  region: eu-central-1  - imageID: \u003cimage_id_in_cn_shanghai\u003e  region: cn-shanghai  ...  version: 2191.4.1  ...  csi:  enableADController: true  resources:  limits:  cpu: 500m  memory: 1Gi  requests:  memory: 128Mi Seed resource This provider extension does not support any provider configuration for the Seed’s .spec.provider.providerConfig field. However, it supports to managing of backup infrastructure, i.e., you can specify a configuration for the .spec.backup field.\nBackup configuration A Seed of type alicloud can be configured to perform backups for the main etcds’ of the shoot clusters control planes using Alicloud Object Storage Service.\nThe location/region where the backups will be stored defaults to the region of the Seed (spec.provider.region).\nPlease find below an example Seed manifest (partly) that configures backups using Alicloud Object Storage Service.\n--- apiVersion: core.gardener.cloud/v1beta1 kind: Seed metadata:  name: my-seed spec:  provider:  type: alicloud  region: cn-shanghai  backup:  provider: alicloud  secretRef:  name: backup-credentials  namespace: garden  ... An example of the referenced secret containing the credentials for the Alicloud Object Storage Service can be found in the example folder.\nPermissions for Alicloud Object Storage Service Please make sure the RAM user associated with the provided AccessKey pair has the following permission.\n AliyunOSSFullAccess  ","categories":"","description":"","excerpt":"Using the Alicloud provider extension with Gardener as operator The …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-alicloud/docs/usage-as-operator/","tags":"","title":"Usage As Operator"},{"body":"Using the AWS provider extension with Gardener as operator The core.gardener.cloud/v1beta1.CloudProfile resource declares a providerConfig field that is meant to contain provider-specific configuration. Similarly, the core.gardener.cloud/v1beta1.Seed resource is structured. Additionally, it allows to configure settings for the backups of the main etcds’ data of shoot clusters control planes running in this seed cluster.\nThis document explains what is necessary to configure for this provider extension.\nCloudProfile resource In this section we are describing how the configuration for CloudProfiles looks like for AWS and provide an example CloudProfile manifest with minimal configuration that you can use to allow creating AWS shoot clusters.\nCloudProfileConfig The cloud profile configuration contains information about the real machine image IDs in the AWS environment (AMIs). You have to map every version that you specify in .spec.machineImages[].versions here such that the AWS extension knows the AMI for every version you want to offer. For each AMI an architecture field can be specified which specifies the CPU architecture of the machine on which given machine image can be used.\nAn example CloudProfileConfig for the AWS extension looks as follows:\napiVersion: aws.provider.extensions.gardener.cloud/v1alpha1 kind: CloudProfileConfig machineImages: - name: coreos  versions:  - version: 2135.6.0  regions:  - name: eu-central-1  ami: ami-034fd8c3f4026eb39  # architecture: amd64 # optional Example CloudProfile manifest Please find below an example CloudProfile manifest:\napiVersion: core.gardener.cloud/v1beta1 kind: CloudProfile metadata:  name: aws spec:  type: aws  kubernetes:  versions:  - version: 1.24.3  - version: 1.23.8  expirationDate: \"2022-10-31T23:59:59Z\"  machineImages:  - name: coreos  versions:  - version: 2135.6.0  machineTypes:  - name: m5.large  cpu: \"2\"  gpu: \"0\"  memory: 8Gi  usable: true  volumeTypes:  - name: gp2  class: standard  usable: true  - name: io1  class: premium  usable: true  regions:  - name: eu-central-1  zones:  - name: eu-central-1a  - name: eu-central-1b  - name: eu-central-1c  providerConfig:  apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1  kind: CloudProfileConfig  machineImages:  - name: coreos  versions:  - version: 2135.6.0  regions:  - name: eu-central-1  ami: ami-034fd8c3f4026eb39  # architecture: amd64 # optional Seed resource This provider extension does not support any provider configuration for the Seed’s .spec.provider.providerConfig field. However, it supports to manage backup infrastructure, i.e., you can specify configuration for the .spec.backup field.\nBackup configuration Please find below an example Seed manifest (partly) that configures backups. As you can see, the location/region where the backups will be stored can be different to the region where the seed cluster is running.\napiVersion: v1 kind: Secret metadata:  name: backup-credentials  namespace: garden type: Opaque data:  accessKeyID: base64(access-key-id)  secretAccessKey: base64(secret-access-key) --- apiVersion: core.gardener.cloud/v1beta1 kind: Seed metadata:  name: my-seed spec:  provider:  type: aws  region: eu-west-1  backup:  provider: aws  region: eu-central-1  secretRef:  name: backup-credentials  namespace: garden  ... Please look up https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys as well.\nPermissions for AWS IAM user Please make sure that the provided credentials have the correct privileges. You can use the following AWS IAM policy document and attach it to the IAM user backed by the credentials you provided (please check the official AWS documentation as well):\n Click to expand the AWS IAM policy document! {  \"Version\": \"2012-10-17\",  \"Statement\": [  {  \"Effect\": \"Allow\",  \"Action\": \"s3:*\",  \"Resource\": \"*\"  }  ] }  ","categories":"","description":"","excerpt":"Using the AWS provider extension with Gardener as operator The …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/usage-as-operator/","tags":"","title":"Usage As Operator"},{"body":"Using the Azure provider extension with Gardener as an operator The core.gardener.cloud/v1beta1.CloudProfile resource declares a providerConfig field that is meant to contain provider-specific configuration. The core.gardener.cloud/v1beta1.Seed resource is structured similarly. Additionally, it allows configuring settings for the backups of the main etcds’ data of shoot clusters control planes running in this seed cluster.\nThis document explains the necessary configuration for the Azure provider extension.\nCloudProfile resource This section describes, how the configuration for CloudProfiles looks like for Azure by providing an example CloudProfile manifest with minimal configuration that can be used to allow the creation of Azure shoot clusters.\nCloudProfileConfig The cloud profile configuration contains information about the real machine image IDs in the Azure environment (image urn, id, communityGalleryImageID or sharedGalleryImageID). You have to map every version that you specify in .spec.machineImages[].versions to an available VM image in your subscription. The VM image can be either from the Azure Marketplace and will then get identified via a urn, it can be a custom VM image from a shared image gallery and is then identified sharedGalleryImageID, or it can be from a community image gallery and is then identified by its communityGalleryImageID. You can use id field also to specifiy the image location in the azure compute gallery (in which case it would have a different kind of path) but it is not recommended as it sometimes faces problems in cross subscription image sharing. For each machine image version an architecture field can be specified which specifies the CPU architecture of the machine on which given machine image can be used.\nAn example CloudProfileConfig for the Azure extension looks as follows:\napiVersion: azure.provider.extensions.gardener.cloud/v1alpha1 kind: CloudProfileConfig countUpdateDomains: - region: westeurope  count: 5 countFaultDomains: - region: westeurope  count: 3 machineTypes: - name: Standard_D3_v2  acceleratedNetworking: true - name: Standard_X machineImages: - name: coreos  versions:  - version: 2135.6.0  urn: \"CoreOS:CoreOS:Stable:2135.6.0\"  # architecture: amd64 # optional  acceleratedNetworking: true - name: myimage  versions:  - version: 1.0.0  id: \"/subscriptions/\u003csubscription ID where the gallery is located\u003e/resourceGroups/myGalleryRG/providers/Microsoft.Compute/galleries/myGallery/images/myImageDefinition/versions/1.0.0\" - name: GardenLinuxCommunityImage  versions:  - version: 1.0.0  communityGalleryImageID: \"/CommunityGalleries/gardenlinux-567905d8-921f-4a85-b423-1fbf4e249d90/Images/gardenlinux/Versions/576.1.1\" - name: SharedGalleryImageName  versions:  - version: 1.0.0  sharedGalleryImageID: \"/SharedGalleries/sharedGalleryName/Images/sharedGalleryImageName/Versions/sharedGalleryImageVersionName\" The cloud profile configuration contains information about the update via .countUpdateDomains[] and failure domain via .countFaultDomains[] counts in the Azure regions you want to offer.\nThe .machineTypes[] list contain provider specific information to the machine types e.g. if the machine type support Azure Accelerated Networking, see .machineTypes[].acceleratedNetworking.\nAdditionally, it contains the real machine image identifiers in the Azure environment. You can provide either URN for Azure Market Place images or id of Shared Image Gallery images. When Shared Image Gallery is used, you have to ensure that the image is available in the desired regions and the end-user subscriptions have access to the image or to the whole gallery. You have to map every version that you specify in .spec.machineImages[].versions here such that the Azure extension knows the machine image identifiers for every version you want to offer. Furthermore, you can specify for each image version via .machineImages[].versions[].acceleratedNetworking if Azure Accelerated Networking is supported.\nExample CloudProfile manifest The possible values for .spec.volumeTypes[].name on Azure are Standard_LRS, StandardSSD_LRS and Premium_LRS. There is another volume type called UltraSSD_LRS but this type is not supported to use as os disk. If an end user select a volume type whose name is not equal to one of the valid values then the machine will be created with the default volume type which belong to the selected machine type. Therefore it is recommended to configure only the valid values for the .spec.volumeType[].name in the CloudProfile.\nPlease find below an example CloudProfile manifest:\napiVersion: core.gardener.cloud/v1beta1 kind: CloudProfile metadata:  name: azure spec:  type: azure  kubernetes:  versions:  - version: 1.24.3  - version: 1.23.8  expirationDate: \"2022-10-31T23:59:59Z\"  machineImages:  - name: coreos  versions:  - version: 2135.6.0  machineTypes:  - name: Standard_D3_v2  cpu: \"4\"  gpu: \"0\"  memory: 14Gi  - name: Standard_D4_v3  cpu: \"4\"  gpu: \"0\"  memory: 16Gi  volumeTypes:  - name: Standard_LRS  class: standard  usable: true  - name: StandardSSD_LRS  class: premium  usable: false  - name: Premium_LRS  class: premium  usable: false  regions:  - name: westeurope  providerConfig:  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1  kind: CloudProfileConfig  machineTypes:  - name: Standard_D3_v2  acceleratedNetworking: true  - name: Standard_D4_v3  countUpdateDomains:  - region: westeurope  count: 5  countFaultDomains:  - region: westeurope  count: 3  machineImages:  - name: coreos  versions:  - version: 2303.3.0  urn: CoreOS:CoreOS:Stable:2303.3.0  # architecture: amd64 # optional  acceleratedNetworking: true  - version: 2135.6.0  urn: \"CoreOS:CoreOS:Stable:2135.6.0\"  # architecture: amd64 # optional Seed resource This provider extension does not support any provider configuration for the Seed’s .spec.provider.providerConfig field. However, it supports managing of backup infrastructure, i.e., you can specify a configuration for the .spec.backup field.\nBackup configuration A Seed of type azure can be configured to perform backups for the main etcds’ of the shoot clusters control planes using Azure Blob storage.\nThe location/region where the backups will be stored defaults to the region of the Seed (spec.provider.region), but can also be explicitly configured via the field spec.backup.region. The region of the backup can be different from where the Seed cluster is running. However, usually it makes sense to pick the same region for the backup bucket as used for the Seed cluster.\nPlease find below an example Seed manifest (partly) that configures backups using Azure Blob storage.\n--- apiVersion: core.gardener.cloud/v1beta1 kind: Seed metadata:  name: my-seed spec:  provider:  type: azure  region: westeurope  backup:  provider: azure  region: westeurope # default region  secretRef:  name: backup-credentials  namespace: garden  ... The referenced secret has to contain the provider credentials of the Azure subscription. Please take a look here on how to create an Azure Application, Service Principle and how to obtain credentials. The example below demonstrates how the secret has to look like.\napiVersion: v1 kind: Secret metadata:  name: core-azure  namespace: garden-dev type: Opaque data:  clientID: base64(client-id)  clientSecret: base64(client-secret)  subscriptionID: base64(subscription-id)  tenantID: base64(tenant-id) Permissions for Azure Blob storage Please make sure the Azure application has the following IAM roles.\n Contributor  Miscellaneous Gardener managed Service Principals The operators of the Gardener Azure extension can provide a list of managed service principals (technical users) that can be used for Azure Shoots. This eliminates the need for users to provide own service principals for their clusters.\nThe user would need to grant the managed service principal access to their subscription with proper permissions.\nAs service principals are managed in an Azure Active Directory for each supported Active Directory, an own service principal needs to be provided.\nIn case the user provides an own service principal in the Shoot secret, this one will be used instead of the managed one provided by the operator.\nEach managed service principal will be maintained in a Secret like that:\napiVersion: v1 kind: Secret metadata:  name: service-principal-my-tenant  namespace: extension-provider-azure  labels:  azure.provider.extensions.gardener.cloud/purpose: tenant-service-principal-secret data:  tenantID: base64(my-tenant)  clientID: base64(my-service-princiapl-id)  clientSecret: base64(my-service-princiapl-secret) type: Opaque The user needs to provide in its Shoot secret a tenantID and subscriptionID.\nThe managed service principal will be assigned based on the tenantID. In case there is a managed service principal secret with a matching tenantID, this one will be used for the Shoot. If there is no matching managed service principal secret then the next Shoot operation will fail.\nOne of the benefits of having managed service principals is that the operator controls the lifecycle of the service principal and can rotate its secrets.\nAfter the service principal secret has been rotated and the corresponding secret is updated, all Shoot clusters using it need to be reconciled or the last operation to be retried.\n","categories":"","description":"","excerpt":"Using the Azure provider extension with Gardener as an operator The …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-azure/docs/usage-as-operator/","tags":"","title":"Usage As Operator"},{"body":"Using the Equinix Metal provider extension with Gardener as operator The core.gardener.cloud/v1alpha1.CloudProfile resource declares a providerConfig field that is meant to contain provider-specific configuration.\nIn this document we are describing how this configuration looks like for Equinix Metal and provide an example CloudProfile manifest with minimal configuration that you can use to allow creating Equinix Metal shoot clusters.\nExample CloudProfile manifest Please find below an example CloudProfile manifest:\napiVersion: core.gardener.cloud/v1beta1 kind: CloudProfile metadata:  name: equinix-metal spec:  type: equinixmetal  kubernetes:  versions:  - version: 1.20.2  - version: 1.19.7  - version: 1.18.15  #expirationDate: \"2020-04-05T01:02:03Z\"  machineImages:  - name: flatcar  versions:  - version: 0.0.0-stable  machineTypes:  - name: t1.small  cpu: \"4\"  gpu: \"0\"  memory: 8Gi  usable: true  regions: # List of offered metros  - name: ny  zones: # List of offered facilities within the respective metro  - name: ewr1  - name: ny5  - name: ny7  providerConfig:  apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1  kind: CloudProfileConfig  machineImages:  - name: flatcar  versions:  - version: 0.0.0-stable  id: flatcar_stable CloudProfileConfig The cloud profile configuration contains information about the real machine image IDs in the Equinix Metal environment (IDs). You have to map every version that you specify in .spec.machineImages[].versions here such that the Equinix Metal extension knows the ID for every version you want to offer.\nAn example CloudProfileConfig for the Equinix Metal extension looks as follows:\napiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1 kind: CloudProfileConfig machineImages: - name: flatcar  versions:  - version: 0.0.0-stable  id: flatcar_stable  NOTE: CloudProfileConfig is not a Custom Resource, so you cannot create it directly.\n ","categories":"","description":"","excerpt":"Using the Equinix Metal provider extension with Gardener as operator …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-equinix-metal/docs/usage-as-operator/","tags":"","title":"Usage As Operator"},{"body":"Using the GCP provider extension with Gardener as operator The core.gardener.cloud/v1beta1.CloudProfile resource declares a providerConfig field that is meant to contain provider-specific configuration. The core.gardener.cloud/v1beta1.Seed resource is structured similarly. Additionally, it allows configuring settings for the backups of the main etcds’ data of shoot clusters control planes running in this seed cluster.\nThis document explains the necessary configuration for this provider extension.\nCloudProfile resource This section describes, how the configuration for CloudProfiles looks like for GCP by providing an example CloudProfile manifest with minimal configuration that can be used to allow the creation of GCP shoot clusters.\nCloudProfileConfig The cloud profile configuration contains information about the real machine image IDs in the GCP environment (image URLs). You have to map every version that you specify in .spec.machineImages[].versions here such that the GCP extension knows the image URL for every version you want to offer. For each machine image version an architecture field can be specified which specifies the CPU architecture of the machine on which given machine image can be used.\nAn example CloudProfileConfig for the GCP extension looks as follows:\napiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1 kind: CloudProfileConfig machineImages: - name: coreos  versions:  - version: 2135.6.0  image: projects/coreos-cloud/global/images/coreos-stable-2135-6-0-v20190801  # architecture: amd64 # optional Example CloudProfile manifest If you want to allow that shoots can create VMs with local SSDs volumes then you have to specify the type of the disk with SCRATCH in the .spec.volumeTypes[] list. Please find below an example CloudProfile manifest:\napiVersion: core.gardener.cloud/v1beta1 kind: CloudProfile metadata:  name: gcp spec:  type: gcp  kubernetes:  versions:  - version: 1.24.3  - version: 1.23.8  expirationDate: \"2022-10-31T23:59:59Z\"  machineImages:  - name: coreos  versions:  - version: 2135.6.0  machineTypes:  - name: n1-standard-4  cpu: \"4\"  gpu: \"0\"  memory: 15Gi  volumeTypes:  - name: pd-standard  class: standard  - name: pd-ssd  class: premium  - name: SCRATCH  class: standard  regions:  - region: europe-west1  names:  - europe-west1-b  - europe-west1-c  - europe-west1-d  providerConfig:  apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1  kind: CloudProfileConfig  machineImages:  - name: coreos  versions:  - version: 2135.6.0  image: projects/coreos-cloud/global/images/coreos-stable-2135-6-0-v20190801  # architecture: amd64 # optional Seed resource This provider extension does not support any provider configuration for the Seed’s .spec.provider.providerConfig field. However, it supports to managing of backup infrastructure, i.e., you can specify a configuration for the .spec.backup field.\nBackup configuration A Seed of type gcp can be configured to perform backups for the main etcds’ of the shoot clusters control planes using Google Cloud Storage buckets.\nThe location/region where the backups will be stored defaults to the region of the Seed (spec.provider.region), but can also be explicitly configured via the field spec.backup.region. The region of the backup can be different from where the seed cluster is running. However, usually it makes sense to pick the same region for the backup bucket as used for the Seed cluster.\nPlease find below an example Seed manifest (partly) that configures backups using Google Cloud Storage buckets.\n--- apiVersion: core.gardener.cloud/v1beta1 kind: Seed metadata:  name: my-seed spec:  provider:  type: gcp  region: europe-west1  backup:  provider: gcp  region: europe-west1 # default region  secretRef:  name: backup-credentials  namespace: garden  ... An example of the referenced secret containing the credentials for the GCP Cloud storage can be found in the example folder.\nPermissions for GCP Cloud Storage Please make sure the service account associated with the provided credentials has the following IAM roles.\n Storage Admin  ","categories":"","description":"","excerpt":"Using the GCP provider extension with Gardener as operator The …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-gcp/docs/usage-as-operator/","tags":"","title":"Usage As Operator"},{"body":"Using the OpenStack provider extension with Gardener as operator The core.gardener.cloud/v1alpha1.CloudProfile resource declares a providerConfig field that is meant to contain provider-specific configuration.\nIn this document we are describing how this configuration looks like for OpenStack and provide an example CloudProfile manifest with minimal configuration that you can use to allow creating OpenStack shoot clusters.\nCloudProfileConfig The cloud profile configuration contains information about the real machine image IDs in the OpenStack environment (image names). You have to map every version that you specify in .spec.machineImages[].versions here such that the OpenStack extension knows the image ID for every version you want to offer.\nIt also contains optional default values for DNS servers that shall be used for shoots. In the dnsServers[] list you can specify IP addresses that are used as DNS configuration for created shoot subnets.\nAlso, you have to specify the keystone URL in the keystoneURL field to your environment.\nAdditionally, you can influence the HTTP request timeout when talking to the OpenStack API in the requestTimeout field. This may help when you have for example a long list of load balancers in your environment.\nIn case your OpenStack system uses Octavia for network load balancing then you have to set the useOctavia field to true such that the cloud-controller-manager for OpenStack gets correctly configured (it defaults to false).\nSome hypervisors (especially those which are VMware-based) don’t automatically send a new volume size to a Linux kernel when a volume is resized and in-use. For those hypervisors you can enable the storage plugin interacting with Cinder to telling the SCSI block device to refresh its information to provide information about it’s updated size to the kernel. You might need to enable this behavior depending on the underlying hypervisor of your OpenStack installation. The rescanBlockStorageOnResize field controls this. Please note that it only applies for Kubernetes versions where CSI is used.\nSome openstack configurations do not allow to attach more volumes than a specific amount to a single node. To tell the k8s scheduler to not over schedule volumes on a node, you can set nodeVolumeAttachLimit which defaults to 256. Some openstack configurations have different names for volume and compute availability zones, which might cause pods to go into pending state as there are no nodes available in the detected volume AZ. To ignore the volume AZ when scheduling pods, you can set ignoreVolumeAZ to true, which is only supported for shoot kubernetes version 1.20.x and newer (it defaults to false). See CSI Cinder driver.\nThe cloud profile config also contains constraints for floating pools and load balancer providers that can be used in shoots.\nIf your OpenStack system supports server groups, the serverGroupPolicies property will enable your end-users to create shoots with workers where the nodes are managed by Nova’s server groups. Specifying serverGroupPolicies is optional and can be omitted. If enabled, the end-user can choose whether or not to use this feature for a shoot’s workers. Gardener will handle the creation of the server group and node assignment.\nTo enable this feature, an operator should:\n specify the allowed policy values (e.g. affintity, anti-affinity) in this section. Only the policies in the allow-list will be available for end-users. make sure that your OpenStack project has enough server group capacity. Otherwise, shoot creation will fail.  An example CloudProfileConfig for the OpenStack extension looks as follows:\napiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1 kind: CloudProfileConfig machineImages: - name: coreos  versions:  - version: 2135.6.0  image: coreos-2135.6.0 # keystoneURL: https://url-to-keystone/v3/ # keystoneURLs: # - region: europe # url: https://europe.example.com/v3/ # - region: asia # url: https://asia.example.com/v3/ # dnsServers: # - 10.10.10.11 # - 10.10.10.12 # requestTimeout: 60s # useOctavia: true # useSNAT: true # rescanBlockStorageOnResize: true # ignoreVolumeAZ: true # nodeVolumeAttachLimit: 30 # serverGroupPolicies: # - soft-anti-affinity # - anti-affinity # resolvConfOptions: # - rotate # - timeout:1 constraints:  floatingPools:  - name: fp-pool-1 # region: europe # loadBalancerClasses: # - name: lb-class-1 # floatingSubnetID: \"1234\" # floatingNetworkID: \"4567\" # subnetID: \"7890\" # - name: \"fp-pool-*\" # region: europe # loadBalancerClasses: # - name: lb-class-1 # floatingSubnetID: \"1234\" # floatingNetworkID: \"4567\" # subnetID: \"7890\" # - name: \"fp-pool-eu-demo\" # region: europe # domain: demo # loadBalancerClasses: # - name: lb-class-1 # floatingSubnetID: \"1234\" # floatingNetworkID: \"4567\" # subnetID: \"7890\" # - name: \"fp-pool-eu-dev\" # region: europe # domain: dev # nonConstraining: true # loadBalancerClasses: # - name: lb-class-1 # floatingSubnetID: \"1234\" # floatingNetworkID: \"4567\" # subnetID: \"7890\"  loadBalancerProviders:  - name: haproxy # region: europe # - name: f5 # region: asia Please note that it is possible to configure a region mapping for keystone URLs, floating pools, and load balancer providers. Additionally, floating pools can be constrainted to a keystone domain by specifying the domain field. Floating pool names may also contains simple wildcard expressions, like * or fp-pool-* or *-fp-pool. Please note that the * must be either single or at the beginning or at the end. Consequently, fp-*-pool is not possible/allowed. The default behavior is that, if found, the regional (and/or domain restricted) entry is taken. If no entry for the given region exists then the fallback value is the most matching entry (w.r.t. wildcard matching) in the list without a region field (or the keystoneURL value for the keystone URLs). If an additional floating pool should be selectable for a region and/or domain, you can mark it as non constraining with setting the optional field nonConstraining to true.\nThe loadBalancerClasses field is an optional list of load balancer classes which can be when the corresponding floating pool network is choosen. The load balancer classes can be configured in the same way as in the ControlPlaneConfig in the Shoot resource, therefore see here for more details.\nSome OpenStack environments don’t need these regional mappings, hence, the region and keystoneURLs fields are optional. If your OpenStack environment only has regional values and it doesn’t make sense to provide a (non-regional) fallback then simply omit keystoneURL and always specify region.\nIf Gardener creates and manages the router of a shoot cluster, it is additionally possible to specify that the enable_snat field is set to true via useSNAT: true in the CloudProfileConfig.\nOn some OpenStack enviroments, there may be the need to set options in the file /etc/resolv.conf on worker nodes. If the field resolvConfOptions is set, a systemd service will be installed which copies /run/systemd/resolve/resolv.conf on every change to /etc/resolv.conf and appends the given options.\nExample CloudProfile manifest Please find below an example CloudProfile manifest:\napiVersion: core.gardener.cloud/v1beta1 kind: CloudProfile metadata:  name: openstack spec:  type: openstack  kubernetes:  versions:  - version: 1.24.3  - version: 1.23.8  expirationDate: \"2022-10-31T23:59:59Z\"  machineImages:  - name: coreos  versions:  - version: 2135.6.0  machineTypes:  - name: medium_4_8  cpu: \"4\"  gpu: \"0\"  memory: 8Gi  storage:  class: standard  type: default  size: 40Gi  regions:  - name: europe-1  zones:  - name: europe-1a  - name: europe-1b  - name: europe-1c  providerConfig:  apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1  kind: CloudProfileConfig  machineImages:  - name: coreos  versions:  - version: 2135.6.0  image: coreos-2135.6.0  keystoneURL: https://url-to-keystone/v3/  constraints:  floatingPools:  - name: fp-pool-1  loadBalancerProviders:  - name: haproxy ","categories":"","description":"","excerpt":"Using the OpenStack provider extension with Gardener as operator The …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-openstack/docs/usage-as-operator/","tags":"","title":"Usage As Operator"},{"body":"Using the vSphere provider extension with Gardener as operator The core.gardener.cloud/v1alpha1.CloudProfile resource declares a providerConfig field that is meant to contain provider-specific configuration.\nIn this document we are describing how this configuration looks like for VMware vSphere and provide an example CloudProfile manifest with minimal configuration that you can use to allow creating vSphere shoot clusters.\nCloudProfileConfig The cloud profile configuration contains information about the real machine image paths in the vSphere environment (image names). You have to map every version that you specify in .spec.machineImages[].versions here such that the vSphere extension knows the image ID for every version you want to offer.\nIt also contains optional default values for DNS servers that shall be used for shoots. In the dnsServers[] list you can specify IP addresses that are used as DNS configuration for created shoot subnets.\nThe dhcpOptions list allows to specify DHCP options. See BOOTP Vendor Extensions and DHCP Options for valid codes (tags) and details about values. The code 15 (domain name) is only allowed for when using NSX-T 2.5. For NSX-T \u003e= 3.0 use 119 (search domain).\nThe dockerDaemonOptions allow to adjust the docker daemon configuration.\n with dockerDaemonOptions.httpProxyConf the content of the proxy configuration file can be set. See Docker HTTP/HTTPS proxy for more details with dockerDaemonOptions.insecureRegistries insecure registries can be specified. This should only be used for development or evaluation purposes.  Also, you have to specify several name of NSX-T objects in the constraints.\nAn example CloudProfileConfig for the vSphere extension looks as follows:\napiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1 kind: CloudProfileConfig namePrefix: my_gardener defaultClassStoragePolicyName: \"vSAN Default Storage Policy\" folder: my-vsphere-vm-folder regions: - name: region1  vsphereHost: my.vsphere.host  vsphereInsecureSSL: true  nsxtHost: my.vsphere.host  nsxtInsecureSSL: true  transportZone: \"my-tz\"  logicalTier0Router: \"my-tier0router\"  edgeCluster: \"my-edgecluster\"  snatIpPool: \"my-snat-ip-pool\"  datacenter: my-vsphere-dc  zones:  - name: zone1  computeCluster: my-vsphere-computecluster1  # resourcePool: my-resource-pool1 # provide either computeCluster or resourcePool or hostSystem  # hostSystem: my-host1 # provide either computeCluster or resourcePool or hostSystem  datastore: my-vsphere-datastore1  #datastoreCluster: my-vsphere-datastore-cluster # provide either datastore or datastoreCluster  - name: zone2  computeCluster: my-vsphere-computecluster2  # resourcePool: my-resource-pool2 # provide either computeCluster or resourcePool or hostSystem  # hostSystem: my-host2 # provide either computeCluster or resourcePool or hostSystem  datastore: my-vsphere-datastore2  #datastoreCluster: my-vsphere-datastore-cluster # provide either datastore or datastoreCluster constraints:  loadBalancerConfig:  size: MEDIUM  classes:  - name: default  ipPoolName: gardener_lb_vip # optional DHCP options like 119 (search domain), 42 (NTP), 15 (domain name (only NSX-T 2.5)) #dhcpOptions: #- code: 15 # values: # - foo.bar.com #- code: 42 # values: # - 136.243.202.118 # - 80.240.29.124 # - 78.46.53.8 # - 162.159.200.123 dnsServers: - 10.10.10.11 - 10.10.10.12 machineImages: - name: flatcar  versions:  - version: 3139.2.3  path: gardener/templates/flatcar-3139.2.3  guestId: other4xLinux64Guest #dockerDaemonOptions: # httpProxyConf: | # [Service] # Environment=\"HTTPS_PROXY=https://proxy.example.com:443\" # insecureRegistries: # - myregistrydomain.com:5000 # - blabla.mycompany.local Example CloudProfile manifest Please find below an example CloudProfile manifest:\napiVersion: core.gardener.cloud/v1beta1 kind: CloudProfile metadata:  name: vsphere spec:  type: vsphere  providerConfig:  apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1  kind: CloudProfileConfig  namePrefix: my_gardener  defaultClassStoragePolicyName: \"vSAN Default Storage Policy\"  folder: my-vsphere-vm-folder  regions:  - name: region1  vsphereHost: my.vsphere.host  vsphereInsecureSSL: true  nsxtHost: my.vsphere.host  nsxtInsecureSSL: true  transportZone: \"my-tz\"  logicalTier0Router: \"my-tier0router\"  edgeCluster: \"my-edgecluster\"  snatIpPool: \"my-snat-ip-pool\"  datacenter: my-vsphere-dc  zones:  - name: zone1  computeCluster: my-vsphere-computecluster1  # resourcePool: my-resource-pool1 # provide either computeCluster or resourcePool or hostSystem  # hostSystem: my-host1 # provide either computeCluster or resourcePool or hostSystem  datastore: my-vsphere-datastore1  #datastoreCluster: my-vsphere-datastore-cluster # provide either datastore or datastoreCluster  - name: zone2  computeCluster: my-vsphere-computecluster2  # resourcePool: my-resource-pool2 # provide either computeCluster or resourcePool or hostSystem  # hostSystem: my-host2 # provide either computeCluster or resourcePool or hostSystem  datastore: my-vsphere-datastore2  #datastoreCluster: my-vsphere-datastore-cluster # provide either datastore or datastoreCluster  constraints:  loadBalancerConfig:  size: MEDIUM  classes:  - name: default  ipPoolName: gardener_lb_vip  dnsServers:  - 10.10.10.11  - 10.10.10.12  machineImages:  - name: coreos  versions:  - version: 3139.2.3  path: gardener/templates/flatcar-3139.2.3  guestId: other4xLinux64Guest  kubernetes:  versions:  - version: 1.15.4  - version: 1.16.0  - version: 1.16.1  machineImages:  - name: flatcar  versions:  - version: 3139.2.3  machineTypes:  - name: std-02  cpu: \"2\"  gpu: \"0\"  memory: 8Gi  usable: true  - name: std-04  cpu: \"4\"  gpu: \"0\"  memory: 16Gi  usable: true  - name: std-08  cpu: \"8\"  gpu: \"0\"  memory: 32Gi  usable: true  regions:  - name: region1  zones:  - name: zone1  - name: zone2 Which versions of Kubernetes/vSphere are supported This extension targets Kubernetes \u003e= v1.15 and vSphere 6.7 U3 or later.\n vSphere CSI driver needs vSphere 6.7 U3 or later, and Kubernetes \u003e= v1.14 (see feature metrics for more details) vSpere CPI driver needs vSphere 6.7 U3 or later, and Kubernetes \u003e= v1.11 (see cloud-provider-vsphere CPI - Cloud Provider Interface)  Supported VM images Currently, only Gardenlinux and Flatcar (CoreOS fork) are supported. Virtual Machine Hardware must be version 15 or higher, but images are upgraded automatically if their hardware has an older version.\n","categories":"","description":"","excerpt":"Using the vSphere provider extension with Gardener as operator The …","ref":"/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/docs/usage-as-operator/","tags":"","title":"Usage As Operator"},{"body":"Using the Calico networking extension with Gardener as operator This document explains configuration options supported by the networking-calico extension.\nRun calico-node in non-privileged and non-root mode Feature State: Alpha\nMotivation Running containers in privileged mode is not recommended as privileged containers run with all linux capabilities enabled and can access the host’s resources. Running containers in privileged mode opens number of security threats such as breakout to underlying host OS.\nSupport for non-privileged and non-root mode The Calico project has a preliminary support for running the calico-node component in non-privileged mode (see this guide). Similar to Tigera Calico operator the networking-calico extension can also run calico-node in non-privileged and non-root mode. This feature is controller via feature gate named NonPrivilegedCalicoNode. The feature gates are configured in the ControllerConfiguration of networking-calico. The corresponding ControllerDeployment configuration that enables the NonPrivilegedCalicoNode would look like:\napiVersion: core.gardener.cloud/v1beta1 kind: ControllerDeployment metadata:  name: networking-calico type: helm providerConfig:  values:  chart: \u003comitted\u003e  config:  featureGates:  NonPrivilegedCalicoNode: false Limitations  The support for the non-privileged mode in the Calico project is not ready for productive usage. The upstream documentation states that in non-privileged mode the support for features added after Calico v3.21 is not guaranteed. Calico in non-privileged mode does not support eBPF dataplane. That’s why when eBPF dataplane is enabled, calico-node has to run in privileged mode (even when the NonPrivilegedCalicoNode feature gate is enabled). (At the time of writing this guide) there is the following issue projectcalico/calico#5348 that is not addressed. (At the time of writing this guide) the upstream adoptions seems to be low. The Calico charts and manifest in projectcalico/calico run calico-node in privileged mode.  ","categories":"","description":"","excerpt":"Using the Calico networking extension with Gardener as operator This …","ref":"/docs/extensions/network-extensions/gardener-extension-networking-calico/docs/usage-as-operator/","tags":"","title":"Usage As Operator"},{"body":"Basically, Helm Charts can be installed as described e.g. in the Helm QuickStart Guide. However, our clusters come with RBAC enabled by default, hence Helm must be installed as follows:\nCreate a Service Account Create a service account via the following command:\ncat \u003c\u003cEOF | kubectl create -f - apiVersion: v1 kind: ServiceAccount metadata: name: helm namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: helm roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: helm namespace: kube-system EOF Initialize Helm Initialise Helm via helm init --service-account helm. You can now use helm.\nIn case of failure In case you have already executed helm init, but without the above service account, you will get the following error: Error: User \"system:serviceaccount:kube-system:default\" cannot list configmaps in the namespace \"kube-system\". (get configmaps) (e.g. when you run helm list). You will now need to delete the Tiller deployment (Helm backend implicitly deployed to the Kubernetes cluster when you call helm init) as well as the local Helm files (usually $HELM_HOME is set to ~/.helm):\nkubectl delete deployment tiller-deploy --namespace=kube-system kubectl delete service tiller-deploy --namespace=kube-system rm -rf ~/.helm/ Now follow the instructions above. For more details see this Kubernetes Helm issue #2687.\n","categories":"","description":"","excerpt":"Basically, Helm Charts can be installed as described e.g. in the Helm …","ref":"/docs/guides/client_tools/helm/","tags":"","title":"Use a Helm Chart to Deploy an Application or Service"},{"body":"User Alerts    Alertname Severity Type Description     ApiServerUnreachableViaKubernetesService critical shoot The Api server has been unreachable for 15 minutes via the kubernetes service in the shoot.   KubeKubeletNodeDown warning shoot The kubelet {{ $labels.instance }} has been unavailable/unreachable for more than 1 hour. Workloads on the affected node may not be schedulable.   KubeletTooManyOpenFileDescriptorsShoot warning shoot Shoot-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubeletTooManyOpenFileDescriptorsShoot critical shoot Shoot-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubePodPendingShoot warning shoot Pod {{ $labels.pod }} is stuck in \"Pending\" state for more than 1 hour.   KubePodNotReadyShoot warning shoot Pod {{ $labels.pod }} is not ready for more than 1 hour.   NodeExporterDown warning shoot The NodeExporter has been down or unreachable from Prometheus for more than 1 hour.   K8SNodeOutOfDisk critical shoot Node {{ $labels.node }} has run out of disk space.   K8SNodeMemoryPressure warning shoot Node {{ $labels.node }} is under memory pressure.   K8SNodeDiskPressure warning shoot Node {{ $labels.node }} is under disk pressure   VMRootfsFull critical shoot Root filesystem device on instance {{ $labels.instance }} is almost full.   VMConntrackTableFull critical shoot The nf_conntrack table is {{ $value }}% full.   VPNProbeAPIServerProxyFailed critical shoot The API Server proxy functionality is not working. Probably the vpn connection from an API Server pod to the vpn-shoot endpoint on the Shoot workers does not work.    ","categories":"","description":"","excerpt":"User Alerts    Alertname Severity Type Description …","ref":"/docs/gardener/monitoring/user_alerts/","tags":"","title":"User Alerts"},{"body":"Disclaimer This post is meant to give a basic end-to-end description for deploying and using Prometheus and Grafana. Both applications offer a wide range of flexibility which needs to be considered in case you have specific requirenments. Such advanced details are not in the scope of this post.\nIntroduction Prometheus is an open-source systems monitoring and alerting toolkit for recording numeric time series. It fits both machine-centric monitoring as well as monitoring of highly dynamic service-oriented architectures. In a world of microservices, its support for multi-dimensional data collection and querying is a particular strength.\nPrometheus graduates within CNCF second hosted project.\nThe following characteristics make Prometheus a good match for monitoring Kubernetes clusters:\n  Pull-based monitoring\nPrometheus is a pull-based monitoring system, which means that the Prometheus server dynamically discovers and pulls metrics from your services running in Kubernetes.\n  Labels Prometheus and Kubernetes share the same label (key-value) concept that can be used to select objects in the system.\nLabels are used to identify time series and sets of label matchers can be used in the query language ( PromQL ) to select the time series to be aggregated..\n  Exporters\nThere are many exporters available which enable integration of databases or even other monitoring systems not already providing a way to export metrics to Prometheus. One prominent exporter is the so called node-exporter, which allows to monitor hardware and OS related metrics of Unix systems.\n  Powerful query language\nThe Prometheus query language PromQL lets the user select and aggregate time series data in real time. Results can either be shown as a graph, viewed as tabular data in the Prometheus expression browser, or consumed by external systems via the HTTP API.\n  Find query examples on Prometheus Query Examples.\nOne very popular open-source visualization tool not only for Prometheus is Grafana. Grafana is a metric analytics and visualization suite. It is popular for for visualizing time series data for infrastructure and application analytics but many use it in other domains including industrial sensors, home automation, weather, and process control [see Grafana Documentation].\nGrafana accesses data via Data Sources. The continuously growing list of supported backends includes Prometheus.\nDashboards are created by combining panels, e.g. Graph and Dashlist.\nIn this example we describe an End-To-End scenario including the deployment of Prometheus and a basic monitoring configuration as the one provided for Kubernetes clusters created by Gardener.\nIf you miss elements on the Prometheus web page when accessing it via its service URL https://\u003cyour K8s FQN\u003e/api/v1/namespaces/\u003cyour-prometheus-namespace\u003e/services/prometheus-prometheus-server:80/proxy this is probably caused by Prometheus issue #1583 To workaround this issue setup a port forward kubectl port-forward -n \u003cyour-prometheus-namespace\u003e \u003cprometheus-pod\u003e 9090:9090 on your client and access the Prometheus UI from there with your locally installed web browser. This issue is not relevant in case you use the service type LoadBalancer.\nPreparation The deployment of Prometheus and Grafana is based on Helm charts.\nMake sure to implement the Helm settings before deploying the Helm charts.\nThe Kubernetes clusters provided by Gardener use role based access control (RBAC). To authorize the Prometheus node-exporter to access hardware and OS relevant metrics of your cluster’s worker nodes specific artifacts need to be deployed.\nBind the prometheus service account to the garden.sapcloud.io:monitoring:prometheus cluster role by running the command kubectl apply -f crbinding.yaml.\nContent of crbinding.yaml\napiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata:  name: \u003cyour-prometheus-name\u003e-server roleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: garden.sapcloud.io:monitoring:prometheus subjects: - kind: ServiceAccount  name: \u003cyour-prometheus-name\u003e-server  namespace: \u003cyour-prometheus-namespace\u003e Deployment of Prometheus and Grafana Only minor changes are needed to deploy Prometheus and Grafana based on Helm charts.\nCopy the following configuration into a file called values.yaml and deploy Prometheus: helm install \u003cyour-prometheus-name\u003e --namespace \u003cyour-prometheus-namespace\u003e stable/prometheus -f values.yaml\nTypically, Prometheus and Grafana are deployed into the same namespace. There is no technical reason behind this so feel free to choose different namespaces.\nContent of values.yaml for Prometheus:\nrbac:  create: false # Already created in Preparation step nodeExporter:  enabled: false # The node-exporter is already deployed by default  server:  global:  scrape_interval: 30s  scrape_timeout: 30s  serverFiles:  prometheus.yml:  rule_files:  - /etc/config/rules  - /etc/config/alerts  scrape_configs:  - job_name: 'kube-kubelet'  honor_labels: false  scheme: https   tls_config:  # This is needed because the kubelets' certificates are not generated  # for a specific pod IP  insecure_skip_verify: true  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token   kubernetes_sd_configs:  - role: node  relabel_configs:  - target_label: __metrics_path__  replacement: /metrics  - source_labels: [__meta_kubernetes_node_address_InternalIP]  target_label: instance  - action: labelmap  regex: __meta_kubernetes_node_label_(.+)   - job_name: 'kube-kubelet-cadvisor'  honor_labels: false  scheme: https   tls_config:  # This is needed because the kubelets' certificates are not generated  # for a specific pod IP  insecure_skip_verify: true  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token   kubernetes_sd_configs:  - role: node  relabel_configs:  - target_label: __metrics_path__  replacement: /metrics/cadvisor  - source_labels: [__meta_kubernetes_node_address_InternalIP]  target_label: instance  - action: labelmap  regex: __meta_kubernetes_node_label_(.+)   # Example scrape config for probing services via the Blackbox Exporter.  #  # Relabelling allows to configure the actual service scrape endpoint using the following annotations:  #  # * `prometheus.io/probe`: Only probe services that have a value of `true`  - job_name: 'kubernetes-services'  metrics_path: /probe  params:  module: [http_2xx]  kubernetes_sd_configs:  - role: service  relabel_configs:  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]  action: keep  regex: true  - source_labels: [__address__]  target_label: __param_target  - target_label: __address__  replacement: blackbox  - source_labels: [__param_target]  target_label: instance  - action: labelmap  regex: __meta_kubernetes_service_label_(.+)  - source_labels: [__meta_kubernetes_namespace]  target_label: kubernetes_namespace  - source_labels: [__meta_kubernetes_service_name]  target_label: kubernetes_name  # Example scrape config for pods  #  # Relabelling allows to configure the actual service scrape endpoint using the following annotations:  #  # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`  # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.  # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.  - job_name: 'kubernetes-pods'  kubernetes_sd_configs:  - role: pod  relabel_configs:  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]  action: keep  regex: true  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]  action: replace  target_label: __metrics_path__  regex: (.+)  - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]  action: replace  regex: (.+):(?:\\d+);(\\d+)  replacement: ${1}:${2}  target_label: __address__  - action: labelmap  regex: __meta_kubernetes_pod_label_(.+)  - source_labels: [__meta_kubernetes_namespace]  action: replace  target_label: kubernetes_namespace  - source_labels: [__meta_kubernetes_pod_name]  action: replace  target_label: kubernetes_pod_name  # Scrape config for service endpoints.  #  # The relabeling allows the actual service scrape endpoint to be configured  # via the following annotations:  #  # * `prometheus.io/scrape`: Only scrape services that have a value of `true`  # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need  # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.  # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.  # * `prometheus.io/port`: If the metrics are exposed on a different port to the  # service then set this appropriately.  - job_name: 'kubernetes-service-endpoints'  kubernetes_sd_configs:  - role: endpoints  relabel_configs:  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]  action: keep  regex: true  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]  action: replace  target_label: __scheme__  regex: (https?)  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]  action: replace  target_label: __metrics_path__  regex: (.+)  - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]  action: replace  target_label: __address__  regex: (.+)(?::\\d+);(\\d+)  replacement: $1:$2  - action: labelmap  regex: __meta_kubernetes_service_label_(.+)  - source_labels: [__meta_kubernetes_namespace]  action: replace  target_label: kubernetes_namespace  - source_labels: [__meta_kubernetes_service_name]  action: replace  target_label: kubernetes_name # Add your additional configuration here... Next, deploy Grafana. Since the deployment in this post is based on the Helm default values, the settings below are set explicitly in case the default changed. Deploy Grafana via helm install grafana --namespace \u003cyour-prometheus-namespace\u003e stable/grafana -f values.yaml. Here, the same namespace is chosen for Prometheus and for Grafana.\nContent of values.yaml for Grafana:\nserver:  ingress:  enabled: false  service:  type: ClusterIP Check the running state of the pods on the Kubernetes Dashboard or by running kubectl get pods -n \u003cyour-prometheus-namespace\u003e. In case of errors check the log files of the pod(s) in question.\nThe text output of Helm after the deployment of Prometheus and Grafana contains very useful information, e.g. the user and password of the Grafana Admin user. The credentials are stored as secrets in the namespace \u003cyour-prometheus-namespace\u003e and could be decoded via kubectl get secret --namespace \u003cmy-grafana-namespace\u003e grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo.\nBasic functional tests To access the web UI of both applications use port forwarding of port 9090.\nSetup port forwarding for port 9090:\nkubectl port-forward -n \u003cyour-prometheus-namespace\u003e \u003cyour-prometheus-server-pod\u003e 9090:9090 Open http://localhost:9090 in your web browser. Select Graph from the top tab and enter the following expressing to show the overall CPU usage for a server (see Prometheus Query Examples)\n100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m]))) This should show some data in a graph.\nTo show the same data in Grafana setup port forwarding for port 3000 for the Grafana pod and open the Grafana Web UI by opening http://localhost:3000 in a browser. Enter the credentials of the admin user.\nNext, you need to enter the server name of your Prometheus deployment. This name is shown directly after the installation via helm.\nRun\nhelm status \u003cyour-prometheus-name\u003e to find this name. Below this server name is referenced by \u003cyour-prometheus-server-name\u003e.\nFirst, you need to add your Prometheus server as data source.\n select Dashboards → Data Sources select Add data source enter Name: \u003cyour-prometheus-datasource-name\u003e\nType: Prometheus\nURL: http://\u003cyour-prometheus-server-name\u003e\n_Access: proxy select Save \u0026 Test  In case of failure check the Prometheus URL in the Kubernetes Dashboard.\nTo add a Graph follow these steps:\n in the left corner, select Dashboards → New to create a new dashboard select Graph to create a new graph next, select the Panel Title → Edit select your Prometheus Data Source in the drop down list enter the expression 100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m]))) in the entry field A select the floppy disk symbol (Save) on top  Now you should have a very basic Prometheus and Grafana setup for your Kubernetes cluster.\nAs a next step you can implement monitoring for your applications by implementing the Prometheus client API.\nLinks  Prometheus Prometheus Helm Chart Prometheus and Kubernetes: A Perfect Match Grafana Grafana Helm Chart  ","categories":"","description":"How to deploy and configure Prometheus and Grafana to collect and monitor kubelet container metrics","excerpt":"How to deploy and configure Prometheus and Grafana to collect and …","ref":"/docs/guides/applications/prometheus/","tags":"","title":"Using Prometheus and Grafana to Monitor K8s"},{"body":"Using the Dashboard Terminal The dashboard features an integrated web-based terminal to your clusters. It allows you to use kubectl without the need to supply kubeconfig. There are several ways to access it and they’re described on this page.\nPrerequisites  You are logged on to the Gardener Dashboard. You have created a cluster and its status is operational. The landscape administrator has enabled the terminal feature The cluster you want to connect to is reachable from the dashboard  On this page:\n Open from cluster list Open from cluster details page Terminal  Open from cluster list   Choose your project from the menu on the left and choose CLUSTERS.\n  Locate a cluster for which you want to open a Terminal and choose the key icon.\n  In the dialog, choose the icon on the right of the Terminal label.\n  Open from cluster details page   Choose your project from the menu on the left and choose CLUSTERS.\n  Locate a cluster for which you want to open a Terminal and choose to display its details.\n  In the Access section, choose the icon on the right of the Terminal label.\n  Terminal Opening up the terminal in either of the ways discussed here results in the following screen:\nIt provides a bash environment and range of useful tools and an installed and configured kubectl (with alias k) to use right away with your cluster.\nTry to list the namespaces in the cluster.\n$ k get ns You get a result like this: ","categories":"","description":"","excerpt":"Using the Dashboard Terminal The dashboard features an integrated …","ref":"/docs/dashboard/usage/using-terminal/","tags":"","title":"Using Terminal"},{"body":"Utilize API Server Network Proxy to Invert Seed-to-Shoot Connectivity  Utilize API Server Network Proxy to Invert Seed-to-Shoot Connectivity  Problem Proposal  API Server Network Proxy   Challenges  Prometheus to Shoot connectivity  Possible Solutions Port-forwarder Sidecar Proxy Client Sidecar Proxy sub-resource   Proxy-server Loadbalancer Sharing and Re-advertising  Possible Solution   Summary      Problem Gardener’s architecture for Kubernetes clusters relies on having the control-plane (e.g., kube-apiserver, kube-scheduler, kube-controller-manager, etc.) and the data-plane (e.g., kube-proxy, kubelet, etc.) of the cluster residing in separate places, this provides many benefits but poses some challenges, especially when API-server to system components communication is required. This problem is solved today in Gardener by making use of OpenVPN to establish a VPN connection from the seed to the shoot. To do so, the following steps are required:\n Create a Loadbalancer service on the shoot. Add a sidecar to the API server pod which knows the address of the newly created Loadbalancer. Establish a connection over the internet to the VPN Loadbalancer Install additional iptables rules that would redirect all the IPs of the shoot (i.e., service, pod, node CIDRs) to the established VPN tunnel  There are however quite a few problems with the above approach, here are some:\n Every shoot would require an additional loadbalancer, this accounts for addition overhead in terms of both costs and troubleshooting efforts. Private access use-cases would not be possible without having a seed residing in the same private domain as a hard requirement. For example, have a look at this issue Providing a public endpoint to access components in the shoot poses a security risk.  Proposal There are mutliple ways to tackle the directional connectivity issue mentioned above, one way would be to invert the connection between the API server and the system components, i.e., instead of having the API server side-car establish a tunnel, we would have an agent residing in the shoot cluster initiate the connection itself. This way we don’t need a Loadbalancer for every shoot and from the security perspective, there is no ingress from outside, only controlled egress.\nWe want to replace this:\nAPIServer | VPN-seed ---\u003e internet ---\u003e LB --\u003e VPN-Shoot (4314) --\u003e Pods | Nodes | Services\nWith this:\nAPIServer \u003c-\u003e Proxy-Server \u003c--- internet \u003c--- Proxy-Agent --\u003e Pods | Nodes | Services\nAPI Server Network Proxy To solve this issue we can utilize the apiserver-network-proxy upstream implementation. Which provides a reference implementation for a reverse streaming server. The way it works is as follows:\n Proxy agent connects to proxy server to establish a sticky connection. Traffic to the proxy server (residing in the seed) gets then re-directed to the agent (residing in the shoot) which forwards the traffic to in-cluster components.  The initial motivation for the apiserver-network-proxy project is to get rid of provider-specific implementations that reside in the API-server (e.g., SSH), but it turns out that it has other interesting use-cases such as data-plane connection decoupling, which is the main use-case for this proposal.\nStarting with Kubernetes 1.18 it’s possible to make use of an --egress-selector-config-file flag, this helps point the API-server to traffic hook points based on traffic direction. For example, in the config below the API server would have to forward all cluster related traffic (e.g., logs, port-forward, exec, …etc.) to the proxy-server which then knows how to forward traffic to the shoot. For the rest of the traffic, e.g. API server to ETCD or other control-plane components direct is used which means legacy routing method, i.e., by-pass the proxy.\n egress-selector-configuration.yaml: |-apiVersion: apiserver.k8s.io/v1alpha1 kind: EgressSelectorConfiguration egressSelections: - name: cluster connection: proxyProtocol: httpConnect transport: tcp: url: https://proxy-server:8131 - name: master connection: proxyProtocol: direct - name: etcd connection: proxyProtocol: direct Challenges Prometheus to Shoot connectivity One challenge remains to completely eliminate the need for a VPN connection. In today’s Gardener setup, each control-plane has a Prometheus instance that directly scrapes cluster components such as CoreDNS, Kubelets, cadvisor, etc. This works because in addition to the VPN side car attached to the API server pod, we have another one attached to prometheus which knows how to forward traffic to these endpoints. Once the VPN is eliminated, it is required to find other means to forward traffic to these components.\nPossible Solutions There are currently two ways to solve this problem:\n Attach a port-forwarder side-car to prometheus. Utilize the proxy subresource on the API server.  Port-forwarder Sidecar With this solution each prometheus instance would have a side-car that has the kubeconfig of the shoot cluster, and which establishes a port-forward connection to the endpoints residing in the shoot.\nThere are a many problems with this approach:\n the port-forward connection is not reliable. the connection would break if the API server instance dies. requires an additional component. would need to expose every pod / service via port-forward.  Prom Pod (Prometheus -\u003e Port-forwarder) \u003c-\u003e APIServer -\u003e Proxy-server \u003c--- internet \u003c--- Proxy-Agent --\u003e Pods | Nodes | Services Proxy Client Sidecar Another solution would be to implement a proxy-client as a sidecar for every component that wishes to communicate with the shoot cluster. For this to work, means to re-direct / inject that proxy to handle the component’s traffic is necessary (e.g., additional IPtable rules).\nPrometheus Pod (Prometheus -\u003e Proxy) \u003c-\u003e Proxy-Server \u003c--- internet \u003c--- Proxy-Agent --\u003e Pods | Nodes | Services The problem with this approach is that it requires an additional sidecar (along with traffic redirection) to be attached to every client that wishes to communicate with the shoot cluster, this can cause:\n additional maintenance efforts (extra code). other side-effects (e.g., if istio sidecar injection is enabled)  Proxy sub-resource Kubernetes supports proxying requests to nodes, services, and pod endpoints in the shoot cluster. This proxy connection can be utilized for scraping the necessary endpoints in the shoot.\nThis approach requires less components and is more reliable than the port-forward solution, however, it relies on having the API server supporting proxied connection for the required endpoints.\nPrometheus \u003c-\u003e APIServer \u003c-\u003e Proxy-Server \u003c--- internet \u003c--- Proxy-Agent --\u003e Pods | Nodes | Services As simple as it is, it has a downside that it relies on the availability of the API server.\nProxy-server Loadbalancer Sharing and Re-advertising With the proxy-server in place, we need to provide means to enable the proxy-agent in the shoot to establish the connection with the server. As a result, we need to provide a public endpoint through which this channel of communication can be established, i.e., we need a Loadbalancer(s).\nPossible Solution Using a Loadbalancer / proxy server would not make sense since this is a pain-point we are trying to eliminate in the first-place, doing so just moves the costs to the control-plane. A possible solution is to communicate over a shared loadbalancer in the seed, similar to what has been proposed here, this way we can prevent the extra-costs for load-balancers.\nWith this in mind, we still have other pain-points, namely:\n Advertising Loadbalancer public IPs to the shoot. Directing the traffic to the corresponding shoot proxy-server.  For advertising the Loadbalancer IP, a DNS entry can be created for the proxy loadbalancer (or re-use the DNS entry for the SNI proxy), along with necessary certificates, which is then used to connect to the loadbalancer. At this point we can decide on either one of the two approaches:\n One Proxy / API server with a shared loadbalancer. Use one proxy server for all agents.  In the first case, we will probably need a proxy for the proxy-server that knows how to direct traffic to the correct proxy server based on the corresponding shoot cluster. In the second case, we don’t need another proxy if the proxy server is cluster-aware, i.e., can pool and identify connections coming from the same cluster and peer them with the correct API. Unfortunately, the second case is not supported today.\nSummary  API server proxy can be utilized to invert the connection (only for clusters \u003e= 1.18, for older clusters the old VPN solution will remain). This is achieved by utilizing the --egress-selector-config-file flag on the api-server. For monitoring endpoints, the proxy subresources would be the preferable methods to go, but in the future we can also support sidecar proxies that can communicate with the proxy-server. For Directing traffic to the correct proxy-server we will re-use the SNI proxy along with the load-balancer from the shoot API server via SNI GEP.  ","categories":"","description":"","excerpt":"Utilize API Server Network Proxy to Invert Seed-to-Shoot Connectivity …","ref":"/docs/gardener/proposals/11-apiserver-network-proxy/","tags":"","title":"Utilize API Server Network Proxy to Invert Seed-to-Shoot Connectivity"},{"body":"Version Skew Policy This document describes the maximum version skew supported between various Gardener components.\nSupported Gardener Versions Gardener versions are expressed as x.y.z, where x is the major version, y is the minor version, and z is the patch version, following Semantic Versioning terminology.\nThe Gardener project maintains release branches for the most recent three minor releases.\nApplicable fixes, including security fixes, may be backported to those three release branches, depending on severity and feasibility. Patch releases are cut from those branches at a regular cadence, plus additional urgent releases when required.\nFor more information, see this document.\nSupported Version Skew Technically, we follow the same policy as the Kubernetes project. However, given that our release cadence is much more frequent compared to Kubernetes (every 14d vs. every 120d), in many cases it is possible to skip a version. Still, to be on the safe side, it is highly recommended to follow the described policy.\ngardener-apiserver In multi-instance setups of Gardener, the newest and oldest gardener-apiserver instances must be within one minor version.\nExample:\n newest gardener-apiserver is at 1.37 other gardener-apiserver instances are supported at 1.37 and v1.36  gardener-controller-manager, gardener-scheduler, gardener-admission-controller, gardenlet gardener-controller-manager, gardener-scheduler, gardener-admission-controller, and gardenlet must not be newer than the gardener-apiserver instances they communicate with. They are expected to match the gardener-apiserver minor version, but may be up to one minor version older (to allow live upgrades).\nExample:\n gardener-apiserver is at v1.37 gardener-controller-manager, gardener-scheduler, gardener-admission-controller, and gardenlet are supported at 1.37 and v1.36  Supported Component Upgrade Order The supported version skew between components has implications on the order in which components must be upgraded. This section describes the order in which components must be upgraded to transition an existing Gardener installation from version 1.37 to version 1.38.\ngardener-apiserver Pre-requisites:\n In a single-instance setup, the existing gardener-apiserver instance is 1.37 In a multi-instance setup, all gardener-apiserver instances are at 1.37 or 1.38 (this ensures maximum skew of 1 minor version between the oldest and newest gardener-apiserver instance) The gardener-controller-manager, gardener-scheduler, gardener-admission-controller, and gardenlet instances that communicate with this gardener-apiserver are at version 1.37 (this ensures they are not newer than the existing API server version and are within 1 minor version of the new API server version)  Action:\n Upgrade gardener-apiserver to 1.38  gardener-controller-manager, gardener-scheduler, gardener-admission-controller, gardenlet Pre-requisites:\n The gardener-apiserver instances these components communicate with are at 1.38 (in multi-instance setups in which these components can communicate with any gardener-apiserver instance in the cluster, all gardener-apiserver instances must be upgraded before upgrading these components)  Action:\n Upgrade gardener-controller-manager, gardener-scheduler, gardener-admission-controller, and gardenlet to 1.38  Supported Kubernetes Versions Please refer to this document.\n","categories":"","description":"","excerpt":"Version Skew Policy This document describes the maximum version skew …","ref":"/docs/gardener/deployment/version_skew_policy/","tags":"","title":"Version Skew Policy"},{"body":"Webterminals Architecture Overview Motivation We want to give garden operators and “regular” users of the Gardener dashboard an easy way to have a preconfigured shell directly in the browser.\nThis has several advantages:\n no need to set up any tools locally no need to download / store kubeconfigs locally Each terminal session will have its own “access” service account created. This makes it easier to see “who” did “what” when using the web terminals. The “access” service account is deleted when the terminal session expires Easy “privileged” access to a node (privileged container, hostPID, and hostNetwork enabled, mounted host root fs) in case of troubleshooting node. If allowed by PSP.  How it’s done - TL;DR On the host cluster, we schedule a pod to which the dashboard frontend client attaches to (similar to kubectl attach). Usually the ops-toolbelt image is used, containing all relevant tools like kubectl. The Pod has a kubeconfig secret mounted with the necessary privileges for the target cluster - usually cluster-admin.\nTarget types There are currently three targets, where a user can open a terminal session to:\n The (virtual) garden cluster - Currently operator only The shoot cluster The control plane of the shoot cluster - operator only  Host There are different factors on where the host cluster (and namespace) is chosen by the dashboard:\n Depending on, the selected target and the role of the user (operator or “regular” user) the host is chosen. For performance / low latency reasons, we want to place the “terminal” pods as near as possible to the target kube-apiserver.  For example, the user wants to have a terminal for a shoot cluster. The kube-apiserver of the shoot is running in the seed-shoot-ns on the seed.\n If the user is an operator, we place the “terminal” pod directly in the seed-shoot-ns on the seed. However, if the user is a “regular” user, we don’t want to have “untrusted” workload scheduled on the seeds, that’s why the “terminal” pod is scheduled on the shoot itself, in a temporary namespace that is deleted afterwards.  Lifecycle of a Web Terminal Session 1. Browser / Dashboard Frontend - Open Terminal User chooses the target and clicks in the browser on Open terminal button. A POST request is made to the dashboard backend to request a new terminal session.\n2. Dashboard Backend - Create Terminal Resource According to the privileges of the user (operator / enduser) and the selected target, the dashboard backend creates a terminal resource on behalf of the user in the (virtual) garden and responds with a handle to the terminal session.\n3. Browser / Dashboard Frontend The frontend makes another POST request to the dashboard backend to fetch the terminal session. The Backend waits until the terminal resource is in a “ready” state (timeout 10s) before sending a response to the frontend. More to that later.\n4. Terminal Resource The terminal resource, among other things, holds the information of the desired host and target cluster. The credentials to these clusters are declared as references (secretRef / serviceAccountRef). The terminal resource itself doesn’t contain sensitive information.\n5. Admission A validating webhook is in place to ensure that the user, that created the terminal resource, has the permission to read the referenced credentials. There is also a mutating webhook in place. Both admission configurations have failurePolicy: Fail.\n6. Terminal-Controller-Manager - Apply Resources on Host \u0026 Target Cluster Sidenote: The terminal-controller-manager has no knowledge about the gardener, its shoots, and seeds. In that sense it can be considered as independent from the gardener.\nThe terminal-controller-manager watches terminal resources and ensures the desired state on the host and target cluster. The terminal-controller-manager needs the permission to read all secrets / service accounts in the virtual garden. As additional safety net, the terminal-controller-manager ensures that the terminal resource was not created before the admission configurations were created.\nThe terminal-controller-manager then creates the necessary resources in the host and target cluster.\n Target Cluster:  “Access” service account + (cluster)rolebinding usually to cluster-admin cluster role  used from within the “terminal” pod     Host Cluster:  “Attach” service Account + rolebinding to “attach” cluster role (privilege to attach and get pod)  will be used by the browser to attach to the pod   Kubeconfig secret, containing the “access” token from the target cluster The “terminal” pod itself, having the kubeconfig secret mounted    7. Dashboard Backend - Responds to Frontend As mentioned in step 3, the dashboard backend waits until the terminal resource is “ready”. It then reads the “attach” token from the host cluster on behalf of the user. It responds with:\n attach token hostname of the host cluster’s api server name of the pod and namespace  8. Browser / Dashboard Frontend - Attach to Pod Dashboard frontend attaches to the pod located on the host cluster by opening a WebSocket connection using the provided parameter and credentials. As long as the terminal window is open, the dashboard regularly annotates the terminal resource (heartbeat) to keep it alive.\n9. Terminal-Controller-Manager - Cleanup When there is no heartbeat on the terminal resource for a certain amount of time (default is 5m) the created resources in the host and target cluster are cleaned up again and the terminal resource will be deleted.\nBrowser Trusted Certificates for Kube-Apiservers Motivation The dashboard frontend opens up a secure WebSocket connection to the kube-apiserver. The certificate presented by the kube-apiserver must be browser trusted, otherwise the connection can’t be established (rejected by browser policy). Most kube-apiservers have self-signed certificates from a custom Root CA.\nBootstrapping Preferred Solution There is an issue on the gardener component, to have browser trusted certificates for shoot kube-apiservers using SNI and certmanager. However, this would solve the issue for shoots and shooted-seeds, but not for soil and plant kube-apiservers and potentially others.\nCurrent Solution We had to “workaround” it by creating ingress resources for the kube-apiservers and letting the certmanager (or the new shoot cert service) request browser trusted certificates.\n","categories":"","description":"","excerpt":"Webterminals Architecture Overview Motivation We want to give garden …","ref":"/docs/dashboard/concepts/webterminals/","tags":"","title":"Webterminals"},{"body":"Can you adapt a DNS configuration to be used by the workload on the cluster (CoreDNS configuration)? Yes, you can. Information on that can be found in Custom DNS Configuration.\nHow to use custom domain names using a DNS provider? Creating custom domain names for the Gardener infrastructure DNS records using DNSRecords resources With DNSRecords internal and external domain names of the kube-apiserver are set, as well as the deprecated ingress domain name and an “owner” DNS record for the owning seed.\nFor this purpose, you need either a provider extension supporting the needed resource kind DNSRecord/\u003cprovider-type\u003e or a special extension.\nAll main providers support their respective IaaS specific DNS servers:\n AWS =\u003e DNSRecord/aws-route53 GCP =\u003e DNSRecord/google-cloudns Azure =\u003e DNSRecord/azure-dns Openstack =\u003e DNSRecord/openstack-designate AliCloud =\u003e DNSRecord/alicloud-dns  For Cloudflare there is a community extension existing.\nFor other providers like Netlify and infoblox there is currently no known supporting extension, however, they are supported for shoot-dns-service.\nCreating domain names for cluster resources like ingress or services with services of type Loadbalancers and for TLS certificates For this purpose, the shoot-dns-service extension is used (DNSProvider and DNSEntry resources).\nYou can read more on it in these documents:\n Deployment of the Shoot DNS Service Extension Request DNS Names in Shoot Clusters DNS Providers Gardener DNS Management for Shoots Request X.509 Certificates Gardener Certificate Management  ","categories":"","description":"","excerpt":"Can you adapt a DNS configuration to be used by the workload on the …","ref":"/docs/faq/dns-config/","tags":"","title":"What are the meanings of different DNS configuration options?"},{"body":"Contract: Worker resource While the control plane of a shoot cluster is living in the seed and deployed as native Kubernetes workload, the worker nodes of the shoot clusters are normal virtual machines (VMs) in the end-users infrastructure account. The Gardener project features a sub-project called machine-controller-manager. This controller is extending the Kubernetes API using custom resource definitions to represent actual VMs as Machine objects inside a Kubernetes system. This approach unlocks the possibility to manage virtual machines in the Kubernetes style and benefit from all its design principles.\nWhat is the machine-controller-manager exactly doing? Generally, there are provider-specific MachineClass objects (AWSMachineClass, AzureMachineClass, etc.; similar to StorageClass), and MachineDeployment, MachineSet, and Machine objects (similar to Deployment, ReplicaSet, and Pod). A machine class describes where and how to create virtual machines (in which networks, region, availability zone, SSH key, user-data for bootstrapping, etc.) while a Machine results in an actual virtual machine. You can read up more information in the machine-controller-manager’s repository.\nBefore the introduction of the Worker extension resource Gardener was deploying the machine-controller-manager, the machine classes, and the machine deployments itself. Now, Gardener commissions an external, provider-specific controller to take over these tasks.\nWhat needs to be implemented to support a new worker provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: Worker metadata:  name: bar  namespace: shoot--foo--bar spec:  type: azure  region: eu-west-1  secretRef:  name: cloudprovider  namespace: shoot--foo--bar  infrastructureProviderStatus:  apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1  kind: InfrastructureStatus  ec2:  keyName: shoot--foo--bar-ssh-publickey  iam:  instanceProfiles:  - name: shoot--foo--bar-nodes  purpose: nodes  roles:  - arn: arn:aws:iam::0123456789:role/shoot--foo--bar-nodes  purpose: nodes  vpc:  id: vpc-0123456789  securityGroups:  - id: sg-1234567890  purpose: nodes  subnets:  - id: subnet-01234  purpose: nodes  zone: eu-west-1b  - id: subnet-56789  purpose: public  zone: eu-west-1b  - id: subnet-0123a  purpose: nodes  zone: eu-west-1c  - id: subnet-5678a  purpose: public  zone: eu-west-1c  pools:  - name: cpu-worker  minimum: 3  maximum: 5  maxSurge: 1  maxUnavailable: 0  machineType: m4.large  machineImage:  name: coreos  version: 1967.5.0  nodeTemplate:  capacity:  cpu: 2  gpu: 0  memory: 8Gi  userData: c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0K  volume:  size: 20Gi  type: gp2  zones:  - eu-west-1b  - eu-west-1c  machineControllerManager:  drainTimeout: 10m  healthTimeout: 10m  creationTimeout: 10m  maxEvictRetries: 30  nodeConditions:  - ReadonlyFilesystem  - DiskPressure  - KernelDeadlock The .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed virtual machines. Also, as you can see, Gardener copies the output of the infrastructure creation (.spec.infrastructureProviderStatus), see Infrastructure resource, into the .spec.\nIn the .spec.pools[] field the desired worker pools are listed. In the above example, one pool with machine type m4.large and min=3, max=5 machines shall be spread over two availability zones (eu-west-1b, eu-west-1c). This information together with the infrastructure status must be used to determine the proper configuration for the machine classes.\nThe spec.pools[].nodeTemplate.capacity field contains the resource information of the machine like cpu, gpu and memory. This info is used by Cluster Autoscaler to generate nodeTemplate during scaling the nodeGroup from zero.\nThe spec.pools[].machineControllerManager field allows to configure the settings for machine-controller-manager component. Providers must populate these settings on worker-pool to the related fields in MachineDeployment.\nWhen seeing such a resource your controller must make sure that it deploys the machine-controller-manager next to the control plane in the seed cluster. After that, it must compute the desired machine classes and the desired machine deployments. Typically, one class maps to one deployment, and one class/deployment is created per availability zone. Following this convention, the created resource would look like this:\napiVersion: v1 kind: Secret metadata:  name: shoot--foo--bar-cpu-worker-z1-3db65  namespace: shoot--foo--bar  labels:  gardener.cloud/purpose: machineclass type: Opaque data:  providerAccessKeyId: eW91ci1hd3MtYWNjZXNzLWtleS1pZAo=  providerSecretAccessKey: eW91ci1hd3Mtc2VjcmV0LWFjY2Vzcy1rZXkK  userData: c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0K --- apiVersion: machine.sapcloud.io/v1alpha1 kind: AWSMachineClass metadata:  name: shoot--foo--bar-cpu-worker-z1-3db65  namespace: shoot--foo--bar spec:  ami: ami-0123456789 # Your controller must map the stated version to the provider specific machine image information, in the AWS case the AMI.  blockDevices:  - ebs:  volumeSize: 20  volumeType: gp2  iam:  name: shoot--foo--bar-nodes  keyName: shoot--foo--bar-ssh-publickey  machineType: m4.large  networkInterfaces:  - securityGroupIDs:  - sg-1234567890  subnetID: subnet-01234  region: eu-west-1  secretRef:  name: shoot--foo--bar-cpu-worker-z1-3db65  namespace: shoot--foo--bar  tags:  kubernetes.io/cluster/shoot--foo--bar: \"1\"  kubernetes.io/role/node: \"1\" --- apiVersion: machine.sapcloud.io/v1alpha1 kind: MachineDeployment metadata:  name: shoot--foo--bar-cpu-worker-z1  namespace: shoot--foo--bar spec:  replicas: 2  selector:  matchLabels:  name: shoot--foo--bar-cpu-worker-z1  strategy:  type: RollingUpdate  rollingUpdate:  maxSurge: 1  maxUnavailable: 0  template:  metadata:  labels:  name: shoot--foo--bar-cpu-worker-z1  spec:  class:  kind: AWSMachineClass  name: shoot--foo--bar-cpu-worker-z1-3db65 for the first availability zone eu-west-1b, and\napiVersion: v1 kind: Secret metadata:  name: shoot--foo--bar-cpu-worker-z2-5z6as  namespace: shoot--foo--bar  labels:  gardener.cloud/purpose: machineclass type: Opaque data:  providerAccessKeyId: eW91ci1hd3MtYWNjZXNzLWtleS1pZAo=  providerSecretAccessKey: eW91ci1hd3Mtc2VjcmV0LWFjY2Vzcy1rZXkK  userData: c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0K --- apiVersion: machine.sapcloud.io/v1alpha1 kind: AWSMachineClass metadata:  name: shoot--foo--bar-cpu-worker-z2-5z6as  namespace: shoot--foo--bar spec:  ami: ami-0123456789 # Your controller must map the stated version to the provider specific machine image information, in the AWS case the AMI.  blockDevices:  - ebs:  volumeSize: 20  volumeType: gp2  iam:  name: shoot--foo--bar-nodes  keyName: shoot--foo--bar-ssh-publickey  machineType: m4.large  networkInterfaces:  - securityGroupIDs:  - sg-1234567890  subnetID: subnet-0123a  region: eu-west-1  secretRef:  name: shoot--foo--bar-cpu-worker-z2-5z6as  namespace: shoot--foo--bar  tags:  kubernetes.io/cluster/shoot--foo--bar: \"1\"  kubernetes.io/role/node: \"1\" --- apiVersion: machine.sapcloud.io/v1alpha1 kind: MachineDeployment metadata:  name: shoot--foo--bar-cpu-worker-z1  namespace: shoot--foo--bar spec:  replicas: 1  selector:  matchLabels:  name: shoot--foo--bar-cpu-worker-z1  strategy:  type: RollingUpdate  rollingUpdate:  maxSurge: 1  maxUnavailable: 0  template:  metadata:  labels:  name: shoot--foo--bar-cpu-worker-z1  spec:  class:  kind: AWSMachineClass  name: shoot--foo--bar-cpu-worker-z2-5z6as for the second availability zone eu-west-1c.\nAnother convention is the 5-letter hash at the end of the machine class names. Most controllers compute a checksum out of the specification of the machine class. This helps to trigger a rolling update of the worker nodes if, for example, the machine image version changes. In this case, a new checksum will be generated which results in the creation of a new machine class. The MachineDeployment’s machine class reference (.spec.template.spec.class.name) is updated which triggers the rolling update process in the machine-controller-manager. However, all of this is only a convention that eases writing the controller, but you can do it completely differently if you desire - as long as you make sure that the described behaviours are implemented correctly.\nAfter the machine classes and machine deployments have been created the machine-controller-manager will start talking to the provider’s IaaS API and create the virtual machines. Gardener makes sure that the content of the userData field that is used to bootstrap the machines contain the required configuration for installation of the kubelet and registering the VM as worker node in the shoot cluster. The Worker extension controller shall wait until all the created MachineDeployments indicate healthiness/readiness before it ends the control loop.\nDoes Gardener need some information that must be returned back? Another important benefit of the machine-controller-manager’s design principles (extending the Kubernetes API using CRDs) is that the cluster-autoscaler can be used without any provider-specific implementation. We have forked the upstream Kubernetes community’s cluster-autoscaler and extended it so that it understands the machine API. Definitely, we will merge it back into the community’s versions once it has been adapted properly.\nOur cluster-autoscaler only needs to know the minimum and maximum number of replicas per MachineDeployment and is ready to act without that it needs to talk to the provider APIs (it just modifies the .spec.replicas field in the MachineDeployment object). Gardener deploys this autoscaler if there is at least one worker pool that specifies max\u003emin. In order to know how it needs to configure it, the provider-specific Worker extension controller must expose which MachineDeployments it had created and how the min/max numbers should look like.\nConsequently, your controller should write this information into the Worker resource’s .status.machineDeployments field:\n--- apiVersion: extensions.gardener.cloud/v1alpha1 kind: Worker metadata:  name: worker  namespace: shoot--foo--bar spec:  ... status:  lastOperation: ...  machineDeployments:  - name: shoot--foo--bar-cpu-worker-z1  minimum: 2  maximum: 3  - name: shoot--foo--bar-cpu-worker-z2  minimum: 1  maximum: 2 In order to support a new worker provider you need to write a controller that watches all Workers with .spec.type=\u003cmy-provider-name\u003e. You can take a look at the below referenced example implementation for the AWS provider.\nThat sounds like a lot that needs to be done, can you help me? All of the described behaviour is mostly the same for every provider. The only difference is maybe the version/configuration of the machine-controller-manager, and the machine class specification itself. You can take a look at our extension library, especially the worker controller part where you will find a lot of utilities that you can use. Also, using the library you only need to implement your provider specifics - all the things that can be handled generically can be taken for free and do not need to be re-implemented. Take a look at the AWS worker controller for finding an example.\nNon-provider specific information required for worker creation All the providers require further information that is not provider specific but already part of the shoot resource. One example for such information is whether the shoot is hibernated or not. In this case all the virtual machines should be deleted/terminated, and after that the machine controller-manager should be scaled down. You can take a look at the AWS worker controller to see how it reads this information and how it is used. As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information that is not part of the Worker resource itself.\nReferences and additional resources  Worker API (Golang specification) Extension controller library Generic worker controller Exemplary implementation for the AWS provider  ","categories":"","description":"","excerpt":"Contract: Worker resource While the control plane of a shoot cluster …","ref":"/docs/gardener/extensions/worker/","tags":"","title":"Worker"},{"body":"Controlling the Kubernetes versions for specific worker pools Since Gardener v1.36, worker pools can have different Kubernetes versions specified than the control plane.\nIn earlier Gardener versions all worker pools inherited the Kubernetes version of the control plane. Once the Kubernetes version of the control plane was modified, all worker pools have been updated as well (either by rolling the nodes in case of a minor version change, or in-place for patch version changes).\nIn order to gracefully perform Kubernetes upgrades (triggering a rolling update of the nodes) with workloads sensitive to restarts (e.g., those dealing with lots of data), it might be required to be able to gradually perform the upgrade process. In such cases, the Kubernetes version for the worker pools can be pinned (.spec.provider.workers[].kubernetes.version) while the control plane Kubernetes version (.spec.kubernetes.version) is updated. This results in the nodes being untouched while the control plane is upgraded. Now a new worker pool (with the version equal to the control plane version) can be added. Administrators can then reschedule their workloads to the new worker pool according to their upgrade requirements and processes.\nExample Usage in a Shoot spec:  kubernetes:  version: 1.20.1  provider:  workers:  - name: data1  kubernetes:  version: 1.19.1  - name: data2  If .kubernetes.version is not specified in a worker pool, then the Kubernetes version of the kubelet is inherited from the control plane (.spec.kubernetes.version), i.e., in the above example, the data2 pool will use 1.20.1. If .kubernetes.version is specified in a worker pool then it must meet the following constraints:  It must be at most two minor versions lower than the control plane version. If it was not specified before, then no downgrade is possible (you cannot set it to 1.19.1 while .spec.kubernetes.version is already 1.20.1). The “two minor version skew” is only possible if the worker pool version is set to control plane version and then the control plane was updated gradually two minor versions. If the version is removed from the worker pool, only one minor version difference is allowed to the control plane (you cannot upgrade a pool from version 1.18.0 to 1.20.0 in one go).    Automatic updates of Kubernetes versions (see Shoot Maintenance) also apply to worker pool Kubernetes versions.\n","categories":"","description":"","excerpt":"Controlling the Kubernetes versions for specific worker pools Since …","ref":"/docs/gardener/usage/worker_pool_k8s_versions/","tags":"","title":"Worker Pool K8s Versions"},{"body":"Working with Projects Projects are used to group clusters, to onboard IaaS resources utilized by them and organize access control. To work with clusters, you need to create a project that they’ll belong to.\nPrerequisites  You have access to the Gardener dashboard and have permissions to create projects.  Procedure   Log on to the Gardener Dashboard and choose CREATE YOUR FIRST PROJECT.\n  Provide a project Name, and optionally a Description, and a Purpose, and choose CREATE.\nNote: You will not be able to change the project Name later. The rest of the details are editable.\nThe result is similar to the following:\nIf you need to create more projects, expand the projects list dropdown on the left. When expanded, it reveals a CREATE PROJECT button that brings up the same dialog as above.\nWhen you need to delete your project, go to ADMINISTRATON, choose the trash bin icon and, confirm the operation.   ","categories":"","description":"","excerpt":"Working with Projects Projects are used to group clusters, to onboard …","ref":"/docs/dashboard/usage/working-with-projects/","tags":"","title":"Working With Projects"},{"body":"Working with Service Accounts Prerequisites  You are logged on to the Gardener Dashboard You have created a project.  The cluster operations that are performed manually in the dashboard or via kubectl can be automated using the Gardener API. You need a service account to be authorized to perform them.\n The service account of a project has access to all Kubernetes resources in the project.\n Create a Service Account   Select your project and choose MEMBERS from the menu on the left.\n  Locate the section Service Accounts and choose +.\n  Enter the service account details.\nThe following Roles are available:\n   Role Granted Permissions     Admin Fully manage resources inside the project, except for member management. Also the delete/modify permissions for ServiceAccounts are now deprecated for this role and will be removed in a future version of Gardener, use the Service Account Manager role instead.   Viewer Read all resources inside the project except secrets.   UAM Manage human users or groups in the project member list. Service accounts can only be managed admins.   Service Account Manager This allows to fully manage service accounts inside the project namespace and request tokens for them. Please refer to this document. For security reasons this role should not be assigned to service accounts, especially it should be prevented that a service account can refresh tokens for itself.      Choose CREATE.\n  Use the Service Account To use the service account, download or copy its kubeconfig.\nDelete the Service Account Choose Delete Service Account to delete it.\n","categories":"","description":"","excerpt":"Working with Service Accounts Prerequisites  You are logged on to the …","ref":"/docs/dashboard/usage/working-with-service-accounts/","tags":"","title":"Working With Service Accounts"}]