[{"uri":"https://gardener.cloud/documentation/home/","title":"Home","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/concepts/core-components/api-server/","title":"Gardener API Server","tags":[],"description":"","content":"Gardener API server The Gardener API server is a Kubernetes-native extension based on its aggregation layer. It is registered via an APIService object and designed to run inside a Kubernetes cluster whose API it wants to extend.\nAfter registration, it exposes the following resources:\nCloudProfiles CloudProfiles are resources that describe a specific environment of an underlying infrastructure provider, e.g. AWS, Azure, etc. Each shoot has to reference a CloudProfile to declare the environment it should be created in. In a CloudProfile the gardener operator specifies certain constraints like available machine types, regions, which Kubernetes versions he wants to offer, etc. End-users can read CloudProfiles to see these values, but only operators can change the content or create/delete them. When a shoot is created or updated then an admission plugin checks that only values are used that are allowed via the referenced CloudProfile.\nAdditionally, a CloudProfile may contain a providerConfig which is a special configuration dedicated for the infrastructure provider. Gardener does not evaluate or understand this config, but extension controllers might need for declaration of provider-specific constraints, or global settings.\nPlease see this example manifest and consult the documentation of your provider extension controller to get information about its providerConfig.\nSeeds Seeds are resources that represent seed clusters. Gardener does not care about how a seed cluster got created - the only requirement is that it is of at least Kubernetes v1.11 and passes the Kubernetes conformance tests. The Gardener operator has to either deploy the Gardenlet into the cluster he wants to use as seed (recommended, then the Gardenlet will create the Seed object itself after bootstrapping), or he provides the kubeconfig to the cluster inside a secret (that is referenced by the Seed resource) and creates the Seed resource himself.\nPlease see this, this(, and optionally this) example manifests.\nQuotas In order to allow end-user not having their own dedicated infrastructure account to try out Gardener the operator can register an account owned by him that he allows to be used for trial clusters. Trial clusters can be put under quota such that they don\u0026rsquo;t consume too many resources (resulting in costs), and so that one user cannot consume all resources on his own. These clusters are automatically terminated after a specified time, but end-users may extend the lifetime manually if needed.\nPlease see this example manifest.\nProjects The first thing before creating a shoot cluster is to create a Project. A project is used to group multiple shoot clusters together. End-users can invite colleagues to the project to enable collaboration, and they can either make them admin or viewer. After an end-user has created a project he will get a dedicated namespace in the garden cluster for all his shoots.\nPlease see this example manifest.\nSecretBindings Now that the end-user has a namespace the next step is registering his infrastructure provider account.\nPlease see this example manifest and consult the documentation of the extension controller for the respective infrastructure provider to get information about which keys are required in this secret.\nAfter the secret has been created the end-user has to create a special SecretBinding resource that binds this secret. Later when creating shoot clusters he will reference such a binding.\nPlease see this example manifest.\nShoots Shoot cluster contain various settings that influence how end-user Kubernetes clusters will look like in the end. As Gardener heavily relies on extension controllers for operating system configuration, networking, and infrastructure specifics, the end-user has the possibility (and responsibility) to provide these provider-specific configurations as well. Such configurations are not evaluated by Gardener (because it doesn\u0026rsquo;t know/understand them), but they are only transported to the respective extension controller.\n:warning: This means that any configuration issues/mistake on the end-user side that relates to a provider-specific flag or setting cannot be caught during the update request itself but only later during the reconciliation (unless a validator webhook has been registered in the garden cluster by an operator).\nPlease see this example manifest and consult the documentation of the provider extension controller to get information about its spec.provider.controlPlaneConfig, .spec.provider.infrastructureConfig, and .spec.provider.workers[].providerConfig.\n(Cluster)OpenIDConnectPresets Please see this separate documentation file.\n"},{"uri":"https://gardener.cloud/documentation/concepts/architecture/","title":"Architecture","tags":[],"description":"","content":"Official Definition - What is Kubernetes?  \u0026ldquo;Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\u0026rdquo;\n Introduction - Basic Principle The foundation of the Gardener (providing Kubernetes Clusters as a Service) is Kubernetes itself, because Kubernetes is the go-to solution to manage software in the Cloud, even when it\u0026rsquo;s Kubernetes itself (see also OpenStack which is provisioned more and more on top of Kubernetes as well).\nWhile self-hosting, meaning to run Kubernetes components inside Kubernetes, is a popular topic in the community, we apply a special pattern catering to the needs of our cloud platform to provision hundreds or even thousands of clusters. We take a so-called \u0026ldquo;seed\u0026rdquo; cluster and seed the control plane (such as the API server, scheduler, controllers, etcd persistence and others) of an end-user cluster, which we call \u0026ldquo;shoot\u0026rdquo; cluster, as pods into the \u0026ldquo;seed\u0026rdquo; cluster. That means one \u0026ldquo;seed\u0026rdquo; cluster, of which we will have one per IaaS and region, hosts the control planes of multiple \u0026ldquo;shoot\u0026rdquo; clusters. That allows us to avoid dedicated hardware/virtual machines for the \u0026ldquo;shoot\u0026rdquo; cluster control planes. We simply put the control plane into pods/containers and since the \u0026ldquo;seed\u0026rdquo; cluster watches them, they can be deployed with a replica count of 1 and only need to be scaled out when the control plane gets under pressure, but no longer for HA reasons. At the same time, the deployments get simpler (standard Kubernetes deployment) and easier to update (standard Kubernetes rolling update). The actual \u0026ldquo;shoot\u0026rdquo; cluster consists only out of the worker nodes (no control plane) and therefore the users may get full administrative access to their clusters.\nSetting The Scene - Components and Procedure We provide a central operator UI, which we call the \u0026ldquo;Gardener Dashboard\u0026rdquo;. It talks to a dedicated cluster, which we call the \u0026ldquo;Garden\u0026rdquo; cluster and uses custom resources managed by an aggregated API server, one of the general extension concepts of Kubernetes) to represent \u0026ldquo;shoot\u0026rdquo; clusters. In this \u0026ldquo;Garden\u0026rdquo; cluster runs the \u0026ldquo;Gardener\u0026rdquo;, which is basically a Kubernetes controller that watches the custom resources and acts upon them, i.e. creates, updates/modifies, or deletes \u0026ldquo;shoot\u0026rdquo; clusters. The creation follows basically these steps:\n Create a namespace in the \u0026ldquo;seed\u0026rdquo; cluster for the \u0026ldquo;shoot\u0026rdquo; cluster which will host the \u0026ldquo;shoot\u0026rdquo; cluster control plane Generate secrets and credentials which the worker nodes will need to talk to the control plane Create the infrastructure (using Terraform), which basically consists out of the network setup) Deploy the \u0026ldquo;shoot\u0026rdquo; cluster control plane into the \u0026ldquo;shoot\u0026rdquo; namespace in the \u0026ldquo;seed\u0026rdquo; cluster, containing the \u0026ldquo;machine-controller-manager\u0026rdquo; pod Create machine CRDs in the \u0026ldquo;seed\u0026rdquo; cluster, describing the configuration and the number of worker machines for the \u0026ldquo;shoot\u0026rdquo; (the machine-controller-manager watches the CRDs and creates virtual machines out of it) Wait for the \u0026ldquo;shoot\u0026rdquo; cluster API server to become responsive (pods will be scheduled, persistent volumes and load balancers are created by Kubernetes via the respective cloud provider) Finally we deploy kube-system daemons like kube-proxy and further add-ons like the dashboard into the \u0026ldquo;shoot\u0026rdquo; cluster and the cluster becomes active  Overview Architecture Diagram Note: The kubelet as well as the pods inside the \u0026ldquo;shoot\u0026rdquo; cluster talk through the front-door (load balancer IP; public Internet) to its \u0026ldquo;shoot\u0026rdquo; cluster API server running in the \u0026ldquo;seed\u0026rdquo; cluster. The reverse communication from the API server to the pod, service, and node networks happens through a VPN connection that we deploy into \u0026ldquo;seed\u0026rdquo; and \u0026ldquo;shoot\u0026rdquo; clusters.\n"},{"uri":"https://gardener.cloud/documentation/concepts/core-components/scheduler/","title":"Gardener Scheduler","tags":[],"description":"","content":"Gardener Scheduler The Gardener Scheduler is in essence a controller that watches newly created shoots and assigns a seed cluster to them. Conceptually, the task of the Gardener Scheduler is very similar to the task of the Kubernetes Scheduler: finding a seed for a shoot instead of a node for a pod.\nEither the scheduling strategy or the shoot cluster purpose hereby determines how the scheduler is operating. The following sections explain the configuration and flow in greater detail.\nWhy is the Gardener Scheduler needed? 1. Decoupling Previously, an admission plugin in the Gardener API server conducted the scheduling decisions. This implies changes to the API server whenever adjustments of the scheduling are needed. Decoupling the API server and the scheduler comes with greater flexibility to develop these components independently from each other.\n2. Extensibility It should be possible to easily extend and tweak the scheduler in the future. Possibly, similar to the Kubernetes scheduler, hooks could be provided which influence the scheduling decisions. It should be also possible to completely replace the standard Gardener Scheduler with a custom implementation.\nConfiguration The Gardener Scheduler configuration has to be supplied on startup. It is a mandatory and also the only available flag. Here is an example scheduler configuration.\nMost of the configuration options are the same as in the Gardener Controller Manager (leader election, client connection, \u0026hellip;). However, the Gardener Scheduler on the other hand does not need a TLS configuration, because there are currently no webhooks configurable.\nThe scheduling strategy is defined in the candidateDeterminationStrategy and can have the possible values SameRegion and MinimalDistance. The SameRegion strategy is the default strategy.\n  Same Region strategy\nThe Gardener Scheduler reads the spec.provider.type and .spec.region fields from the Shoot resource. It tries to find a Seed that has the identical .spec.provider.type and .spec.provider.region fields set. If it cannot find a suitable Seed, it adds an event to the shoot stating, that it is unschedulable.\n  Minimal Distance strategy\nThe Gardener Scheduler tries to find a valid seed with minimal distance to the shoot\u0026rsquo;s intended region. The distance is calculated based on the Levenshtein distance of the region. Therefore the region name is split into a base name and an orientation. Possible orientations are north, south, east, west and central. The distance then is twice the Levenshtein distance of the region\u0026rsquo;s base name plus a correction value based on the orientation and the provider.\nIf the orientations of shoot and seed candidate match, the correction value is 0, if they differ it is 2 and if either the seed\u0026rsquo;s or the shoot\u0026rsquo;s region does not have an orientation it is 1. If the provider differs the correction value is additionally incremented by 2.\nBecause of this a matching region with a matching provider is always prefered.\n  In the last step, the scheduler picks the one seed having the least shoots currently deployed.\nIn order to put the scheduling decision into effect, the scheduler sends an update request for the Shoot resource to the API server. After validation, the Gardener Aggregated API server updates the shoot to have the spec.seedName field set. Subsequently, the Gardenlet picks up and starts to create the cluster on the specified seed.\nSpecial handling based on shoot cluster purpose Every shoot cluster can have a purpose that describes what the cluster is used for, and also influences how the cluster is setup (see this document for more information).\nIn case the shoot has the testing purpose then the scheduler only reads the .spec.provider.type from the Shoot resource and tries to find a Seed that has the identical .spec.provider.type. The region does not matter, i.e., testing shoots may also be scheduled on a seed in a complete different region if it is better for balancing the whole Gardener system.\nFiltering to determine the best candidate The section above has explained which strategies are used to determine the potential seed candidates. Once this list has been computed the scheduler tries to find the best one out of them to which, eventually, the shoot gets assigned to. It filters out Seeds\n whose networks have intersections with the Shoot's networks (due to the VPN connectivity between seeds and shoots their networks must be disjoint) that have .spec.settings.shootDNS.enabled=false (only if the shoot specifies a DNS domain or does not use the unmanaged DNS provider) whose labels don\u0026rsquo;t match the .spec.seedSelector field of the CloudProfile that is used in the Shoot (there might be multiple environments for the same provider type, e.g., you might have multiple OpenStack systems connected to Gardener) whose capacity for shoots would be exceeded if the shoot is scheduled onto the seed, see Ensuring seeds capacity for shoots is not exceeded  After this filtering process the least utilized seed, i.e., the one with the least number of shoot control planes, will be the winner and written to the .spec.seedName field of the Shoot.\nseedSelector field in the Shoot specification Similar to the .spec.nodeSelector field in Pods, the Shoot specification has an optional .spec.seedSelector field. It allows the user to provide a label selector that must match the labels of Seeds in order to be scheduled to one of them. The labels on Seeds are usually controlled by Gardener administrators/operators - end users cannot add arbitrary labels themselves. If provided, the Gardener Scheduler will only consider those seeds as \u0026ldquo;suitable\u0026rdquo; whose labels match those provided in the .spec.seedSelector of the Shoot.\nBy default only seeds with the same provider than the shoot are selected. By adding a providerTypes field to the seedSelector a dedicated set of possible providers (* means all provider types) can be selected.\nEnsuring seeds capacity for shoots is not exceeded Seeds have a practical limit of how many shoots they can accommodate. Exceeding this limit is undesirable as the system performance will be noticeably impacted. Therefore, the scheduler ensures that a seed\u0026rsquo;s capacity for shoots is not exceeded by taking into account a maximum number of shoots that can be scheduled onto a seed.\nThis mechanism works as follows:\n The gardenlet is configured with certain resources and their total capacity (and, for certain resources, the amount reserved for Gardener), see /example/20-componentconfig-gardenlet.yaml. Currently, the only such resource is the maximum number of shoots that can be scheduled onto a seed. The gardenlet seed controller updates the capacity and allocatable fields in Seed status with the capacity of each resource and how much of it is actually available to be consumed by shoots. The allocatable value of a resource is equal to capacity minus reserved. When scheduling shoots, the scheduler filters out all candidate seeds whose allocatable capacity for shoots would be exceeded if the shoot is scheduled onto the seed.  Failure to determine a suitable seed In case the scheduler fails to find a suitable seed, the operation is being retried with an exponential backoff - starting with the retrySyncPeriod (default of 15s).\nCurrent Limitation / Future Plans  Azure has unfortunately a geographically non-hierarchical naming pattern and does not start with the continent. This is the reason why we will exchange the implementation of the MinimalRegion strategy with a more suitable one in the future.  "},{"uri":"https://gardener.cloud/documentation/concepts/core-components/seed-admission-controller/","title":"Gardener Seed Admission Controller","tags":[],"description":"","content":"Gardener Seed Admission Controller The Gardener Seed admission controller is deployed by the Gardenlet as part of its seed bootstrapping phase and, consequently, running in every seed cluster. It\u0026rsquo;s main purpose is to serve webhooks (validating or mutating) in order to admit or deny certain requests to the seed\u0026rsquo;s API server.\nWhat is it doing concretely? Validating Webhooks Unconfirmed Deletion Prevention As part of Gardener\u0026rsquo;s extensibility concepts a lot of CustomResourceDefinitions are deployed to the seed clusters that serve as extension points for provider-specific controllers. For example, the Infrastructure CRD triggers the provider extension to prepare the IaaS infrastructure of the underlying cloud provider for a to-be-created shoot cluster. Consequently, these extension CRDs have a lot of power and control large portions of the end-user\u0026rsquo;s shoot cluster. Accidental or undesired deletions of those resource can cause tremendous and hard-to-recover-from outages and should be prevented.\nTogether with the deployment of the Gardener seed admission controller a ValidatingWebhookConfiguration for CustomResourceDefinitions and most (custom) resources in the extensions.gardener.cloud/v1alpha1 API group is registered. It prevents DELETE requests for those CustomResourceDefinitions labeled with gardener.cloud/deletion-protected=true, and for all mentioned custom resources if they were not previously annotated with the confirmation.gardener.cloud/deletion=true. This prevents that undesired kubectl delete \u0026lt;...\u0026gt; requests are accepted.\nMutating Webhooks It doesn\u0026rsquo;t serve any mutating webhooks yet.\n"},{"uri":"https://gardener.cloud/documentation/concepts/","title":"Concepts","tags":[],"description":"Explore the concepts on which Gardener is built","content":""},{"uri":"https://gardener.cloud/documentation/concepts/core-components/","title":"Core Components","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/concepts/core-components/gardenlet/","title":"gardenlet","tags":[],"description":"","content":"Gardenlet Gardener is implemented using the operator pattern: It uses custom controllers that act on our own custom resources, and apply Kubernetes principles to manage clusters instead of containers. Following this analogy, you can recognize components of the Gardener architecture as well-known Kubernetes components, for example, shoot clusters can be compared with pods, and seed clusters can be seen as worker nodes.\nThe following Gardener components play a similar role as the corresponding components in the Kubernetes architecture:\n   Gardener Component Kubernetes Component     gardener-apiserver kube-apiserver   gardener-controller-manager kube-controller-manager   gardener-scheduler kube-scheduler   gardenlet kubelet    Similar to how the kube-scheduler of Kubernetes finds an appropriate node for newly created pods, the gardener-scheduler of Gardener finds an appropriate seed cluster to host the control plane for newly ordered clusters. By providing multiple seed clusters for a region or provider, and distributing the workload, Gardener also reduces the blast radius of potential issues.\nKubernetes runs a primary \u0026ldquo;agent\u0026rdquo; on every node, the kubelet, which is responsible for managing pods and containers on its particular node. Decentralizing the responsibility to the kubelet has the advantage that the overall system is scalable. Gardener achieves the same for cluster management by using a gardenlet as primary \u0026ldquo;agent\u0026rdquo; on every seed cluster, and is only responsible for shoot clusters located in its particular seed cluster:\nThe gardener-controller-manager has control loops to manage resources of the Gardener API. However, instead of letting the gardener-controller-manager talk directly to seed clusters or shoot clusters, the responsibility isn‚Äôt only delegated to the gardenlet, but also managed using a reversed control flow: It\u0026rsquo;s up to the gardenlet to contact the Gardener API server, for example, to share a status for its managed seed clusters.\nReversing the control flow allows placing seed clusters or shoot clusters behind firewalls without the necessity of direct access via VPN tunnels anymore.\nTLS Bootstrapping Kubernetes doesn‚Äôt manage worker nodes itself, and it‚Äôs also not responsible for the lifecycle of the kubelet running on the workers. Similarly, Gardener doesn‚Äôt manage seed clusters itself, so Gardener is also not responsible for the lifecycle of the gardenlet running on the seeds. As a consequence, both the gardenlet and the kubelet need to prepare a trusted connection to the Gardener API server and the Kubernetes API server correspondingly.\nTo prepare a trusted connection between the gardenlet and the Gardener API server, the gardenlet initializes a bootstrapping process after you deployed it into your seed clusters:\n  The gardenlet starts up with a bootstrap kubeconfig having a bootstrap token that allows to create CertificateSigningRequest (CSR) resources.\n  After the CSR is signed, the gardenlet downloads the created client certificate, creates a new kubeconfig with it, and stores it inside a Secret in the seed cluster.\n  The gardenlet deletes the bootstrap kubeconfig secret, and starts up with its new kubeconfig.\n  The gardenlet starts normal operation.\n  The gardener-controller-manager runs a control loop that automatically signs CSRs created by gardenlets.\n The gardenlet bootstrapping process is based on the kubelet bootstrapping process. More information: Kubelet\u0026rsquo;s TLS bootstrapping.\n If you don\u0026rsquo;t want to run this bootstrap process you can create a kubeconfig pointing to the garden cluster for the gardenlet yourself, and use field gardenClientConnection.kubeconfig in the gardenlet configuration to share it with the gardenlet.\nGardenlet Certificate Rotation The certificate used to authenticate the gardenlet against the API server is valid for a year. After about 10 months, the gardenlet tries to automatically replace the current certificate with a new one (certificate rotation).\nTo use certificate rotation, you need to specify the secret to store the kubeconfig with the rotated certificate in field .gardenClientConnection.kubeconfigSecret of the gardenlet component configuration.\nRotate certificates using bootstrap kubeconfig If the gardenlet created the certificate during the initial TLS Bootstrapping using the Bootstrap kubeconfig, certificates can be rotated automatically. The same control loop in the gardener-controller-manager that signs the CSRs during the initial TLS Bootstrapping also automatically signs the CSR during a certificate rotation.\nRotate Certificate Using Custom kubeconfig When trying to rotate a custom certificate that wasn‚Äôt created by gardenlet as part of the TLS Bootstrap, the x509 certificate\u0026rsquo;s Subject field needs to conform to the following:\n the Common Name (CN) is prefixed with gardener.cloud:system:seed: the Organization (O) equals gardener.cloud:system:seeds  Otherwise, the gardener-controller-manager doesn‚Äôt automatically sign the CSR. In this case, an external component or user needs to approve the CSR manually, for example, using command kubectl certificate approve seed-csr-\u0026lt;...\u0026gt;). If that doesn‚Äôt happen within 15 minutes, the gardenlet repeats the process and creates another CSR.\nSeed Config versus Seed Selector The usage of the gardenlet is flexible:\n   Usage Description     seedConfig Run one gardenlet per seed cluster inside the seed cluster itself.   seedSelector Use one gardenlet to manage multiple seed clusters. The gardenlet can run outside of the seed cluster.     For production use it‚Äôs recommended to go for the seedConfig option, because it makes scaling easier and leads to a better distribution of responsibilities.\n   Provide a seedConfig that contains information about the seed cluster itself if you want the gardenlet in the standard way, see the example gardenlet configuration. Once bootstrapped, the gardenlet creates and updates its Seed object itself.\n  Provide a seedSelector that incorporates a label selector for the targeted seed clusters if you want the gardenlet to manage multiple seeds, see the example gardenlet configuration. In this case, you have to create the Seed objects together with a kubeconfig pointing to the cluster yourself.\n  Component Configuration In the component configuration for the gardenlet, it‚Äôs possible to define:\n settings for the Kubernetes clients interacting with the various clusters settings for the control loops inside the gardenlet settings for leader election and log levels, feature gates, and seed selection or seed configuration.  More information: Example Gardenlet Component Configuration.\nHeartbeats Similar to how Kubernetes is meanwhile using Lease objects for node heart beats (see KEP), the gardenlet is using Lease objects for heart beats of the seed cluster. Every two seconds, the gardenlet is checking its connectivity to the seed cluster and then it renews its lease. The status is reported in the GardenletReady condition in its Seed object list. Similarly to the node-lifecycle-controller inside the kube-controller-manager, the gardener-controller-manager features a seed-lifecycle-controller that sets the GardenletReady condition to Unknown in case the gardenlet stops sending its heartbeat signals. As a consequence, the gardener-scheduler doesn‚Äôt consider this seed cluster for newly created shoot clusters anymore.\n/healthz Endpoint The gardenlet includes an HTTPS server that serves a /healthz endpoint. It‚Äôs used as a liveness probe in the Deployment of the gardenlet. If the gardenlet fails trying to renew its lease, then the endpoint returns 500 Internal Server Error, otherwise it returns 200 OK.\n ‚ö†Ô∏è\nIn case the gardenlet is managing multiple seeds (that is, a seed selector is used) then the /healthz reports 500 Internal Server Error if there is at least one seed for which it couldn‚Äôt renew its lease. Only if it can renew the lease for all seeds it reports 200 OK.\n Shooted Seeds Gardener users can use shoot clusters as seed clusters, so-called \u0026ldquo;shooted seeds\u0026rdquo;, by using shoot cluster annotation shoot.gardener.cloud/use-as-seed. By default, the gardenlet that manages this shoot cluster then automatically creates a clone of itself with the same version and the same configuration that it currently has. Then it deploys the gardenlet clone into the shooted seed cluster.\nIf you want to prevent the automatic gardenlet deployment, use the no-gardenlet value in the shoot.gardener.cloud/use-as-seed annotation. In this case, you have to deploy the gardenlet on your own into the seed cluster.\n For example, if you annotate the shoot cluster with shoot.gardener.cloud/use-as-seed=\u0026quot;true,no-gardenlet,invisible\u0026quot; the shooted seed is created without gardenlet, and the garden-scheduler ignores it (it‚Äôs invisible).\n More information: Create Shooted Seed Cluster\nMigrating from Previous Gardener Versions If your Gardener version doesn‚Äôt support gardenlets yet, no special migration is required, but the following prerequisites must be met:\n Your Gardener version is at least 0.31 before upgrading to v1. You have to make sure that your garden cluster is exposed in a way that it‚Äôs reachable from all your seed clusters.  With previous Gardener versions, you had deployed the Gardener Helm chart (incorporating the API server, controller-manager, and scheduler). With v1, this stays the same, but you now have to deploy the gardenlet Helm chart as well into all of your seeds (if they aren‚Äôt shooted, as mentioned earlier).\nMore information: Deploy a Gardenlet for all instructions.\nRelated Links Gardener Architecture\nIssue #356: Implement Gardener Scheduler\nPR #2309: Add /healthz endpoint for Gardenlet\n"},{"uri":"https://gardener.cloud/documentation/concepts/networking/network_policies/","title":"Network Policies","tags":[],"description":"","content":"Network Policies in Gardener As Seed clusters can host the Kubernetes control planes of many Shoot clusters, it is necessary to isolate the control planes from each other for security reasons. Besides deploying each control plane in its own namespace, Gardener creates network policies to also isolate the networks. Essentially, network policies make sure that pods can only talk to other pods over the network they are supposed to. As such, network policies are an important part of Gardener\u0026rsquo;s tenant isolation.\nGardener deploys network policies into\n each namespace hosting the Kubernetes control plane of the Shoot cluster. the namespace dedicated to Gardener seed-wide global controllers. This namespace is often called garden and contains e.g. the Gardenlet. the kube-system namespace in the Shoot.  The aforementioned namespaces in the Seed contain a deny-all network policy that denies all ingress and egress traffic. This secure by default setting requires pods to whitelist network traffic. This is done by pods having labels matching to the selectors of the network policies deployed by Gardener.\nMore details on the deployed network policies can be found in the development and usage sections.\n"},{"uri":"https://gardener.cloud/documentation/concepts/backup-restore/backup-restore/","title":"Backup and Restore","tags":[],"description":"","content":"Backup and restore Kubernetes uses Etcd as the key-value store for its resource definitions. Gardener supports the backup and restore of etcd. It is the responsibility of the shoot owners to backup the workload data.\nGardener uses etcd-backup-restore component to backup the etcd backing the Shoot cluster regularly and restore in case of disaster. It is deployed as sidecar via etcd-druid. This doc mainly focuses on the backup and restore configuration used by Gardener when deploying these components. For more details on the design and internal implementation details, please refer GEP-06 and documentation on individual repository.\nBucket provisioning Refer the backup bucket extension document to know details about configuring backup bucket.\nBackup Policy etcd-backup-restore supports full snapshot and delta snapshots over full snapshot. In Gardener, this configuration is currently hard-coded to following parameters:\n Full Snapshot Schedule:  Daily, 24hr interval. For each Shoot, the schedule time in a day is randomized based on the configured Shoot maintenance window.   Delta Snapshot schedule:  At 5min interval. If aggregated events size since last snapshot goes beyond 100Mib.   Backup History / Garbage backup deletion policy:  Gardener configure backup restore to have Exponential garbage collection policy. As per policy, following backups are retained. All full backups and delta backups for the previous hour. Latest full snapshot of each previous hour for the day. Latest full snapshot of each previous day for 7 days. Latest full snapshot of the previous 4 weeks. Garbage Collection is configured at 12hr interval.   Listing:  Gardener don\u0026rsquo;t have any API to list out the backups. To find the backup list, admin can checkout the BackupEntry resource associated with Shoot which holds the bucket and prefix details on object store.    Restoration Restoration process of etcd is automated through the etcd-backup-restore component from latest snapshot. Gardener dosen\u0026rsquo;t support Point-In-Time-Recovery (PITR) of etcd. In case of etcd disaster, the etcd is recovered from latest backup automatically. For further details, please refer the doc. Post restoration of etcd, the Shoot reconciliation loop brings back the cluster to same state.\nAgain, Shoot owner is responsible for maintaining the backup/restore of his workload. Gardener does only take care of the cluster\u0026rsquo;s etcd.\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/","title":"Extensions","tags":[],"description":"","content":"Extensibility overview Initially, everything was developed in-tree in the Gardener project. All cloud providers and the configuration for all the supported operating systems were released together with the Gardener core itself. But as the project grew, it got more and more difficult to add new providers and maintain the existing code base. As a consequence and in order to become agile and flexible again, we proposed GEP-1 (Gardener Enhancement Proposal). The document describes an out-of-tree extension architecture that keeps the Gardener core logic independent of provider-specific knowledge (similar to what Kubernetes has achieved with out-of-tree cloud providers or with CSI volume plugins).\nBasic concepts Gardener keeps running in the \u0026ldquo;garden cluster\u0026rdquo; and implements the core logic of shoot cluster reconciliation/deletion. Extensions are Kubernetes controllers themselves (like Gardener) and run in the seed clusters. As usual, we try to use Kubernetes wherever applicable. We rely on Kubernetes extension concepts in order to enable extensibility for Gardener. The main ideas of GEP-1 are the following:\n  During the shoot reconciliation process Gardener will write CRDs into the seed cluster that are watched and managed by the extension controllers. They will reconcile (based on the .spec) and report whether everything went well or errors occurred in the CRD\u0026rsquo;s .status field.\n  Gardener keeps deploying the provider-independent control plane components (etcd, kube-apiserver, etc.). However, some of these components might still need little customization by providers, e.g., additional configuration, flags, etc. In this case, the extension controllers register webhooks in order to manipulate the manifests.\n  Example 1:\nGardener creates a new AWS shoot cluster and requires the preparation of infrastructure in order to proceed (networks, security groups, etc.). It writes the following CRD into the seed cluster:\napiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--core--aws-01spec:type:awsproviderConfig:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:vpc:cidr:10.250.0.0/16internal:- 10.250.112.0/22public:- 10.250.96.0/22workers:- 10.250.0.0/19zones:- eu-west-1adns:apiserver:api.aws-01.core.example.comregion:eu-west-1secretRef:name:my-aws-credentialssshPublicKey:| base64(key)Please note that the .spec.providerConfig is a raw blob and not evaluated or known in any way by Gardener. Instead, it was specified by the user (in the Shoot resource) and just \u0026ldquo;forwarded\u0026rdquo; to the extension controller. Only the AWS controller understands this configuration and will now start provisioning/reconciling the infrastructure. It reports in the .status field the result:\nstatus:observedGeneration:...state:...lastError:..lastOperation:...providerStatus:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusvpc:id:vpc-1234subnets:- id:subnet-acbd1234name:workerszone:eu-west-1securityGroups:- id:sg-xyz12345name:workersiam:nodesRoleARN:\u0026lt;some-arn\u0026gt; instanceProfileName: fooec2:keyName:barGardener waits until the .status.lastOperation/.status.lastError indicates that the operation reached a final state and either continuous with the next step or stops and reports the potential error. The extension-specific output in .status.providerStatus is - similar to .spec.providerConfig - not evaluated and simply forwarded to CRDs in subsequent steps.\nExample 2:\nGardener deploys the control plane components into the seed cluster, e.g. the kube-controller-manager deployment with the following flags:\napiVersion:apps/v1kind:Deployment...spec:template:spec:containers:- command:- /usr/local/bin/kube-controller-manager- --allocate-node-cidrs=true- --attach-detach-reconcile-sync-period=1m0s- --controllers=*,bootstrapsigner,tokencleaner- --cluster-cidr=100.96.0.0/11- --cluster-name=shoot--core--aws-01- --cluster-signing-cert-file=/srv/kubernetes/ca/ca.crt- --cluster-signing-key-file=/srv/kubernetes/ca/ca.key- --concurrent-deployment-syncs=10- --concurrent-replicaset-syncs=10...The AWS controller requires some additional flags in order to make the cluster functional. It needs to provide a Kubernetes cloud-config and also some cloud-specific flags. Consequently, it registers a MutatingWebhookConfiguration on Deployments and adds these flags to the container:\n- --cloud-provider=external- --external-cloud-volume-plugin=aws- --cloud-config=/etc/kubernetes/cloudprovider/cloudprovider.confOf course, it would have needed to create a ConfigMap containing the cloud config and to add the proper volume and volumeMounts to the manifest as well.\n(Please note for this special example: The Kubernetes community is also working on making the kube-controller-manager provider-independent. However, there will most probably be still components other than the kube-controller-manager which need to be adapted by extensions.)\nIf you are interested in writing an extension, or generally in digging deeper to find out the nitty-gritty details of the extension concepts please read GEP-1. We are truly looking forward to your feedback!\nCurrent status Meanwhile, the out-of-tree extension architecture of Gardener is in place and has been productively validated. We are tracking all internal and external extensions of Gardener in the repo: Gardener Extensions Library.\n"},{"uri":"https://gardener.cloud/documentation/concepts/networking/","title":"Networking","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/concepts/monitoring/","title":"Monitoring","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/contribute/","title":"Contribute","tags":[],"description":"Contributors guides for code and documentation","content":""},{"uri":"https://gardener.cloud/blog/2020-11/01/","title":"Case Study: Migrating ETCD Volumes in Production","tags":[],"description":"In this case study, our friends from metal-stack lead you through their journey of migrating Gardener ETCD volumes in their production environment.","content":" This is a guest commentary from metal-stack.\nmetal-stack is a software that provides an API for provisioning and managing physical servers in the data center. To categorize this product, the terms \u0026ldquo;Metal-as-a-Service\u0026rdquo; (MaaS) or \u0026ldquo;bare metal cloud\u0026rdquo; are commonly used.\n One reason you stumble upon this blog post could be that you saw errors like the following in your ETCD instances:\netcd-main-0 etcd 2020-09-03 06:00:07.556157 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;/registry/deployments/shoot--pwhhcd--devcluster2/kube-apiserver\\\u0026#34; \u0026#34; with result \u0026#34;range_response_count:1 size:9566\u0026#34; took too long (13.95374909s) to execute As it turns out, 14 seconds are way too slow for running Kubernetes API servers. It makes them go into the crash loop (leader election fails). Even worse, this whole thing is self-amplifying: The longer a response takes, the more requests queue up, leading to response times increasing further and further. The system is very unlikely to recover. üòû\nOn Github, you can easily find the reason for this problem. Most probably your disks are too slow (see etcd-io/etcd#10860). So, when you are (like in our case) on GKE and run your ETCD on their default persistent volumes, consider moving from standard disks to SSDs and the error messages should disappear. A guide on how to use SSD volumes on GKE can be found here.\nCase closed? Well. For some people it might. But when you are seeing this in your Gardener infrastructure, likely, there is something going wrong. The entire ETCD management is fully managed by the Gardener, which makes the problem a bit more interesting to look at. This blog post strives topics such as:\n Gardener operating principles Gardener architecture and ETCD management Pitfalls with multi-cloud environments Migrating GCP volumes to a new storage class  We from metal-stack learned quite a lot about the capabilities of Gardener through this problem. We are happy to share this experience with a broader audience. Gardener adopters and operators read on.\nHow Gardener Manages ETCDs In our infrastructure, we use the Gardener to provision Kubernetes clusters on bare metal machines in our own data centers using metal-stack. Even if the entire stack could be running on-premise, our initial seed cluster and the metal control plane are hosted on GKE. This way, we do not need to manage a single Kubernetes cluster in our entire landscape manually. As soon as we have Gardener deployed on this initial cluster, we can spin up further Seeds in our own data centers through the concept of shooted seeds.\nTo make this easier to understand, let us give you a simplified picture of how our Gardener production setup looks like:\nFigure 1: Simplified View on Our Production Setup For every shoot cluster, Gardener deploys an individual, standalone ETCD as a stateful set into a shoot namespace. The deployment of the ETCD stateful set is managed by a controller called etcd-druid, which reconciles a special resource of the kind etcds.druid.gardener.cloud. This Etcd resource is getting deployed during the shoot provisioning flow in the Gardenlet.\nFor failure-safety, the etcd-druid deploys the official ETCD container image along with a sidecar project called etcd-backup-restore. The sidecar automatically takes backups of the ETCD and stores them at a cloud provider, e.g. in S3 Buckets, Google Buckets, or similar. In case the ETCD comes up without or with corrupted data, the sidecar looks into the backup buckets and automatically restores the latest backup before ETCD starts up. This entire approach basically takes away the pain for operators to manually have to restore data in the event of data loss.\n We found the etcd-backup-restore project very intriguing. It was the inspiration for us to come up with a similar sidecar for the databases we use with metal-stack. This project is called backup-restore-sidecar. We can cope with postgres and rethinkdb database at the moment and more to come. Feel free to check it out when you are interested.\n As it\u0026rsquo;s the nature for multi-cloud applications to act upon a variety of cloud providers, with a single installation of Gardener, it is easily possible to spin up new Kubernetes clusters not only on GCP, but on other supported cloud platforms, too.\nWhen the Gardenlet deploys a resource like the Etcd resource into a shoot namespace, a provider-specific extension-controller has the chance to manipulate it through a mutating webhook. This way, a cloud provider can adjust the generic Gardener resource to fit his provider-specific needs. For every cloud that Gardener supports, there is such an extension-controller. For metal-stack, we also maintain one, it\u0026rsquo;s called gardener-extension-provider-metal.\n A side note for cloud providers: Meanwhile, new cloud providers can be added fully out-of-tree, i.e. without touching any of the Gardener sources. This works through API extensions and CRDs. The Gardener handles generic resources and backpacks provider-specific configuration through raw extensions. When you are a cloud provider on your own, this is really encouraging because you can integrate with Gardener without any burdens. You can find documentation on how to integrate your cloud into the Gardener here and here.\n The Mistake Is in the Deployment  This section contains code examples from Gardener v1.8.\n Now that we know how the ETCDs are managed by the Gardener, we can come back to the original problem from the beginning of this article. It turned out that the real problem was a misconfiguration in our deployment. The Gardener actually does use SSD-backed storage on GCP for ETCDs by default. During reconciliation, the gardener-extension-controller-gcp deploys a storage class called gardener.cloud-fast that enables accessing SSDs on GCP.\nBut for some reason, in our cluster we did not find such a storage class. And even more interesting, we did not use the gardener-extension-provider-gcp for any shoot reconciliation, only for ETCD backup purposes. And that was the big mistake we made: We reconciled the shoot control plane completely with gardener-extension-provider-metal even though our initial Seed actually runs on GKE and specific parts of the shoot control plane should be reconciled by the GCP extension-controller instead!\nThis is how the initial Seed resource looked like:\napiVersion: core.gardener.cloud/v1beta1 kind: Seed metadata: name: initial-seed spec: ... provider: region: gke type: metal ... ... Surprisingly, this configuration was working pretty well for a long time. The initial seed properly produced the Kubernetes control planes of our shooted seeds that looked like this:\n$ kubectl get controlplanes.extensions.gardener.cloud NAME TYPE PURPOSE STATUS AGE fra-equ01 metal Succeeded 85d fra-equ01-exposure metal exposure Succeeded 85d And this is another interesting observation: There are two ControlPlane resources. One regular resource and one with an exposure purpose. Gardener distinguishes between two types for this exact reason: Environments where the shoot control plane runs on a different cloud provider than the Kubernetes worker nodes. The regular ControlPlane resource gets reconciled by the provider configured in the Shoot resource, the exposure type ControlPlane by the provider configured in the Seed resource.\nWith the existing configuration the gardener-extension-provider-gcp does not kick in and hence, it neither deploys the gardener.cloud-fast storage class nor does it mutate the Etcd resource to point to it. And in the end, we are left with ETCD volumes using the default storage class (which is what we do for ETCD stateful sets in the metal-stack seeds, because our default storage class uses csi-lvm that writes into logical volumes on the SSD disks in our physical servers).\nThe correction we had to make was a one-liner: Setting the provider type of the initial Seed resource to gcp.\n$ kubectl get seed initial-seed -o yaml apiVersion: core.gardener.cloud/v1beta1 kind: Seed metadata: name: initial-seed spec: ... provider: region: gke type: gcp # \u0026lt;-- here ... ... This change moved over the control plane exposure reconciliation to the gardener-extension-provider-gcp:\n$ kubectl get -n \u0026lt;shoot-namespace\u0026gt; controlplanes.extensions.gardener.cloud NAME TYPE PURPOSE STATUS AGE fra-equ01 metal Succeeded 85d fra-equ01-exposure gcp exposure Succeeded 85d And boom, after some time of waiting for all sorts of magic reconciliations taking place in the background, the missing storage class suddenly appeared:\n$ kubectl get sc NAME PROVISIONER gardener.cloud-fast kubernetes.io/gce-pd standard (default) kubernetes.io/gce-pd Also, the Etcd resource was now configured properly to point to the new storage class:\n$ kubectl get -n \u0026lt;shoot-namespace\u0026gt; etcd etcd-main -o yaml apiVersion: druid.gardener.cloud/v1alpha1 kind: Etcd metadata: ... name: etcd-main spec: ... storageClass: gardener.cloud-fast # \u0026lt;-- was pointing to default storage class before! volumeClaimTemplate: main-etcd ...  Only the etcd-main storage class gets changed to gardener.cloud-fast. The etcd-events configuration will still point to standard disk storage because this ETCD is much less occupied as compared to the etcd-main stateful set.\n The Migration Now that the deployment was in place such that this mistake would not repeat in the future, we still had the ETCDs running on the default storage class. The reconciliation does not delete the existing persistent volumes (PVs) on its own.\nTo bring production back up quickly, we temporarily moved the ETCD pods to other nodes in the GKE cluster. These were nodes which were less occupied, such that the disk throughput was a little higher than before. But surely that was not a final solution.\nFor a proper solution we had to move the ETCD data out of the standard disk PV into a SSD-based PV.\nEven though we had the etcd-backup-restore sidecar, we did not want to fully rely on the restore mechanism to do the migration. The backup should only be there for emergency situations when something goes wrong. Thus, we came up with another approach to introduce the SSD volume: GCP disk snapshots. This is how we did the migration:\n Scale down etcd-druid to zero in order to prevent it from disturbing your migration Scale down the kube-apiservers deployment to zero, then wait for the ETCD stateful to take another clean snapshot Scale down the ETCD stateful set to zero as well (in order to prevent Gardener from trying to bring up the downscaled resources, we used small shell constructs like while true; do kubectl scale deploy etcd-druid --replicas 0 -n garden; sleep 1; done) Take a drive snapshot in GCP from the volume that is referenced by the ETCD PVC Create a new disk in GCP from the snapshot on a SSD disk Delete the existing PVC and PV of the ETCD (oops, data is now gone!) Manually deploy a PV into your Kubernetes cluster that references this new SSD disk Manually deploy a PVC with the name of the original PVC and let it reference the PV that you have just created Scale up the ETCD stateful set and check that ETCD is running properly (if something went terribly wrong, you still have the backup from the etcd-backup-restore sidecar, delete the PVC and PV again and let the sidecar bring up ETCD instead) Scale up the kube-apiserver deployment again Scale up etcd-druid again (stop your shell hacks ;D)  This approach worked very well for us and we were able to fix our production deployment issue. And what happened: We have never seen any crashing kube-apiservers again. üéâ\nConclusion As bad as problems in production are, they are the best way for learning from your mistakes. For new users of the Gardener it can be pretty overwhelming to understand the rich configuration possibilities that the Gardener brings. However, once you get a hang of how the Gardener works, the application offers an exceptional versatility that makes it very much suitable for production use-cases like ours.\nThis example has shown how Gardener:\n Can handle arbitrary layers of infrastructure hosted by different cloud providers. Allows provider-specific tweaks to gain ideal performance for every cloud you want to support. Leverages Kubernetes core principles across the entire project architecture, making it vastly extensible and resilient. Brings useful disaster recovery mechanisms to your infrastructure (e.g. with etcd-backup-restore).  We hope that you could take away something new through this blog post. With this article we also want to thank the SAP Gardener team for helping us to integrate Gardener with metal-stack. It\u0026rsquo;s been a great experience so far. üòÑ üòç\n"},{"uri":"https://gardener.cloud/blog/2020-11/00/","title":"Gardener v1.11 and v1.12 Released","tags":[],"description":"","content":"Two months after our last Gardener release update, we are happy again to present release v1.11 and v1.12 in this blog post. Control plane migration, load balancer consolidation, new security features are just a few topics we progressed with. As always, a detailed list of features, improvements, and bug fixes can be found in the release notes of each release. If you are going to update from a previous Gardener version, please take your time to go through the action items in the release notes.\nNotable Changes in v1.12 Release v1.12, fresh from the oven, is shipped with plenty of improvements, features and some API changes we want to pick up in the next sections.\nDrop Functionless DNS Providers (gardener/gardener#3036) This release drops the support for so-called functionless DNS providers. Those are providers in a shoot‚Äôs specification (.spec.dns.providers) which don‚Äôt serve the shoot‚Äôs domain (.spec.dns.domain), but are created by Gardener in the seed cluster to serve DNS requests coming from the shoot cluster. If such providers don‚Äôt specify a type or secretName the creation or update request for the corresponding shoot is denied.\nSeed Taints (gardener/gardener#2955) In an earlier release, we reserved a dedicated section in seed.spec.settings as a replacement for disable-capacity-reservation, disable-dns, invisible taints. These already deprecated taints were still considered and synced, which gave operators enough time to switch their integration to the new settings field. As of version v1.12, support for them has been discontinued and they are automatically removed from seed objects. You may use the actual taint names in a future release of Gardener again.\nLoad Balancer Events During Shoot Reconciliation (gardener/gardener#3028) As Gardener is capable of managing thousands of clusters, it is crucial to keep operation efforts at a minimum. This release demonstrates this endeavor by further improving error reporting to the end user. During a shoot‚Äôs reconciliation, Gardener creates Services of type LoadBalancer in the shoot cluster, e.g. for VPN or Nginx-Ingress addon, and waits for a successful creation. However, in the past we experienced that occurring issues caused by the party creating the load balancer (typically Cloud-Controller-Manager) are only exposed in the logs or as events. Gardener now fetches these event messages and propagates them to the shoot status in case of a failure. Users can then often fix the problem themselves, if for example the failure discloses an exhausted quota on the cloud provider.\nKonnectivityTunnel Feature Per Shoot(gardener/gardener#3007) Since release v1.6 Gardener has been capable of reversing the tunnel direction from the seed to the shoot via the KonnectivityTunnel feature gate (more information). With this release we make it possible to control the feature per shoot. We recommend to selectively enable the KonnectivityTunnel, as it is still in alpha state.\nReference Protection (gardener/gardener#2771, gardener/gardener 1708419) Shoot clusters may refer to external objects, like Secrets for specified DNS providers or they have a reference to an audit policy ConfigMap. Deleting those objects while any shoot still references them causes sever errors, often only recoverable by an immense amount of manual operations effort. To prevent such scenarios, Gardener now adds a new finalizer gardener.cloud/reference-protection to these objects and removes it as soon as the object itself becomes releasable. Due to compatibility reasons, we decided that the handling for audit policy ConfigMaps is delivered as an opt-in feature first, so please familiarize yourself with the necessary settings in the Gardener Controller Manager component config if you already plan to enable it.\nSupport For Resource Quotas (gardener/gardener#2627) After the Kubernetes upstream change (kubernetes/kubernetes#93537) for externalizing the backing admission plugin has been accepted, we are happy to announce the support of ResourceQuotas for Gardener offered resource kinds. ResourceQuotas allow you to specify a maximum number of objects per namespace, especially for end-user objects like Shoots or SecretBindings in a project namespace. Even though the admission plugin is enabled by default in the Gardener API Server, make sure the Kube Controller Manager runs the resourcequota controller as well.\nWatch Out Developers, Terraformer v2 Is Coming! (gardener/gardener#3034) Although not only related to Gardener core, but still an important milestone to mention, is the preparation towards Terraformer v2 in the extensions library. With Terraformer v2, Gardener extensions using Terraform scripts will benefit from great consistency improvements. Please check out #3034) which demonstrates necessary steps to transition to Terraformer v2 as soon as it‚Äôs been released.\nNotable Changes in v1.11 The Gardener community worked eagerly to deliver plenty of improvements with version v1.11. Those help us to further progress with topics like control plane migration, which is actively being worked on, or to harden our load balancer consolidation (APIServerSNI) feature. Besides improvements and fixes (full list available in release notes), this release as well contains major features and we don‚Äôt want to miss a chance to walk you through them.\nGardener Admission Controller (gardener/gardener#2832), (gardener/gardener#2781) In this release, all admission related HTTP handlers moved from the Gardener Controller Manager (GCM) to the new component Gardener Admission Controller. The admission controller is rather a small component as opposed to GCM with regards to memory footprint and CPU consumption, and thus allows you to run multiple replicas of it much cheaper than it was before. We certainly recommend specifying the admission controller deployment with more than one replica, since it reduces the odds of a system-wide outage and increases the performance of your Gardener service.\nBesides the already known Namespace and Kubeconfig Secret validation, a new admission handler Resource-Size-Validator was added to the admission controller. It allows operators to restrict the size for all kinds of Kubernetes objects, especially sent by end-users to the Kubernetes or Gardener API Server. We address a security concern with this feature to prevent denial of service attacks in which an attacker artificially increases the size of objects to exhaust your object store, API server caches, or to let Gardener and Kubernetes controllers run out-of-memory. The documentation reveals an approach of finding the right resource size for your setup and why you should create exceptions for technical users and operators.\nDeferring Shoot Progress Reporting (gardener/gardener#2909), Shoot progress reporting is the continuous update process of a shoot‚Äôs .status.lastOperation field while the shoot is being reconciled by Gardener. Many steps are involved during reconciliation and depending on the size of your setup, the updates might become an issue for the Gardener API Server which will refrain to process further requests for a certain period. With .controllers.shoot.progressReportPeriod in Gardenlet‚Äôs component configuration, you can now delay these updates for the specified period.\nNew Policy For Controller Registrations (gardener/gardener#2896), A while ago, we added support for different policies in ControllerRegistrations which determine under which circumstances the deployments of registration controllers happen in affected seed clusters. If you specify the new policy AlwaysExceptNoShoots, the respective extension controller will be deployed to all seed cluster hosting at least one shoot cluster. After all shoot clusters from a seed are gone, the extension deployment will be deleted again. A full list of supported policies can be found here.\n"},{"uri":"https://gardener.cloud/blog/2020-10/00/","title":"Gardener Integrates with KubeVirt","tags":[],"description":"","content":"The Gardener team is happy to announce that Gardener now offers support for an additional, often requested, infrastructure/virtualization technology, namely KubeVirt! Gardener can now provide Kubernetes-conformant clusters using KubeVirt managed Virtual Machines in the environment of your choice. This integration has been tested and works with any qualified Kubernetes (provider) cluster that is compatibly configured to host the required KubeVirt components, in particular for example Red Hat OpenShift Virtualization.\nGardener enables Kubernetes consumers to centralize and operate efficiently homogenous Kubernetes clusters across different IaaS providers and even private environments. This way the same cloud-based application version can be hosted and operated by its vendor or consumer on a variety of infrastructures. When a new customer or your development team demands for a new infrastructure provider, Gardener helps you to quickly and easily on-board your workload. Furthermore, on this new infrastructure, Gardener keeps the seamless Kubernetes management experience for your Kubernetes operators, while upholding the consistency of the CI/CD pipeline of your software development team.\nArchitecture and Workflow Gardener is based on the idea of three types of clusters ‚Äì Garden cluster, Seed cluster and Shoot cluster (see Figure 1). The Garden cluster is used to control the entire Kubernetes environment centrally in a highly scalable design. The highly available seed clusters are used to host the end users (shoot) clusters‚Äô control planes. Finally, the shoot clusters consist only of worker nodes to host the cloud native applications.\nFigure 1: Gardener Architecture An integration of the Gardener open source project with a new cloud provider follows a standard Gardener extensibility approach. The integration requires two new components: a provider extension and a Machine Controller Manager (MCM) extension. Both components together enable Gardener to instruct the new cloud provider. They run in the Gardener seed clusters that host the control planes of the shoots based on that cloud provider. The role of the provider extension is to manage the provider-specific aspects of the shoot clusters‚Äô lifecycle, including infrastructure, control plane, worker nodes, and others. It works in cooperation with the MCM extension, which in particular is responsible to handle machines that are provisioned as worker nodes for the shoot clusters. To get this job done, the MCM extension leverages the VM management/API capabilities available with the respective cloud provider.\nSetting up a Kubernetes cluster always involves a flow of interdependent steps (see Figure 2), beginning with the generation of certificates and preparation of the infrastructure, continuing with the provisioning of the control plane and the worker nodes, and ending with the deployment of system components. Gardener can be configured to utilize the KubeVirt extensions in its generic workflow at the right extension points, and deliver the desired outcome of a KubeVirt backed cluster.\nFigure 2: Generic cluster reconciliation flow with extension points Gardener Integration with KubeVirt in Detail Integration with KubeVirt follows the Gardener extensibility concept and introduces the two new components mentioned above: the KubeVirt Provider Extension and the KubeVirt Machine Controller Manager (MCM) Extension.\nFigure 3: Gardener integration with KubeVirt The KubeVirt Provider Extension consists of three separate controllers that handle respectively the infrastructure, the control plane, and the worker nodes of the shoot cluster.\nThe Infrastructure Controller configures the network communication between the shoot worker nodes. By default, shoot worker nodes only use the provider cluster‚Äôs pod network. To achieve higher level of network isolation and better performance, it is possible to add more networks and replace the default pod network with a different network using container network interface (CNI) plugins available in the provider cluster. This is currently based on Multus CNI and NetworkAttachmentDefinitions.\nExample infrastructure configuration in a shoot definition:\nprovider:type:kubevirtinfrastructureConfig:apiVersion:kubevirt.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:tenantNetworks:- name:network-1config:| {\u0026#34;cniVersion\u0026#34;: \u0026#34;0.4.0\u0026#34;,\u0026#34;name\u0026#34;: \u0026#34;bridge-firewall\u0026#34;,\u0026#34;plugins\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;,\u0026#34;isGateway\u0026#34;: true,\u0026#34;isDefaultGateway\u0026#34;: true,\u0026#34;ipMasq\u0026#34;: true,\u0026#34;ipam\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;,\u0026#34;subnet\u0026#34;: \u0026#34;10.100.0.0/16\u0026#34;}},{\u0026#34;type\u0026#34;: \u0026#34;firewall\u0026#34;}]}default:trueThe Control Plane Controller deploys a Cloud Controller Manager (CCM). This is a Kubernetes control plane component that embeds cloud-specific control logic. As any other CCM, it runs the Node controller that is responsible for initializing Node objects, annotating and labeling them with cloud-specific information, obtaining the node‚Äôs hostname and IP addresses, and verifying the node‚Äôs health. It also runs the Service controller that is responsible for setting up load balancers and other infrastructure components for Service resources that require them.\nFinally, the Worker Controller is responsible for managing the worker nodes of the Gardener shoot clusters.\nExample worker configuration in a shoot definition:\nprovider:type:kubevirtworkers:- name:cpu-workerminimum:1maximum:2machine:type:standard-1image:name:ubuntuversion:\u0026#34;18.04\u0026#34;volume:type:defaultsize:20Gizones:- europe-west1-cFor more information about configuring the KubeVirt Provider Extension as an end-user, see Using the KubeVirt provider extension with Gardener as end-user.\nEnabling Your Gardener Setup to Leverage a KubeVirt Compatible Environment The very first step required is to define the machine types (VM types) for VMs that will be available. This is achieved via the CloudProfile custom resource. The machine types configuration includes details such as CPU, GPU, memory, OS image, and more.\nExample CloudProfile custom resource:\napiVersion:core.gardener.cloud/v1beta1kind:CloudProfilemetadata:name:kubevirtspec:type:kubevirtproviderConfig:apiVersion:kubevirt.provider.extensions.gardener.cloud/v1alpha1kind:CloudProfileConfigmachineImages:- name:ubuntuversions:- version:\u0026#34;18.04\u0026#34;sourceURL:\u0026#34;https://cloud-images.ubuntu.com/bionic/current/bionic-server-cloudimg-amd64.img\u0026#34;kubernetes:versions:- version:\u0026#34;1.18.5\u0026#34;machineImages:- name:ubuntuversions:- version:\u0026#34;18.04\u0026#34;machineTypes:- name:standard-1cpu:\u0026#34;1\u0026#34;gpu:\u0026#34;0\u0026#34;memory:4GivolumeTypes:- name:defaultclass:defaultregions:- name:europe-west1zones:- name:europe-west1-b- name:europe-west1-c- name:europe-west1-dOnce a machine type is defined, it can be referenced in shoot definitions. This information is used by the KubeVirt Provider Extension to generate MachineDeployment and MachineClass custom resources required by the KubeVirt MCM extension for managing the worker nodes of the shoot clusters during the reconciliation process.\nFor more information about configuring the KubeVirt Provider Extension as an operator, see Using the KubeVirt provider extension with Gardener as operator.\nKubeVirt Machine Controller Manager (MCM) Extension The KubeVirt MCM Extension is responsible for managing the VMs that are used as worker nodes of the Gardener shoot clusters using the virtualization capabilities of KubeVirt. This extension handles all necessary lifecycle management activities, such as machines creation, fetching, updating, listing, and deletion.\nThe KubeVirt MCM Extension implements the Gardener‚Äôs common driver interface for managing VMs in different cloud providers. As already mentioned, the KubeVirt MCM Extension is using the MachineDeployments and MachineClasses ‚Äì an abstraction layer that follows the Kubernetes native declarative approach - to get instructions from the KubeVirt Provider Extension about the required machines for the shoot worker nodes. Also, the cluster austoscaler integrates with the scale subresource of the MachineDeployment resource. This way, Gardener offers a homogeneous autoscaling experience across all supported providers.\nWhen a new shoot cluster is created or when a new worker node is needed for an existing shoot cluster, a new Machine will be created, and at that time, the KubeVirt MCM extension will create a new KubeVirt VirtualMachine in the provider cluster. This VirtualMachine will be created based on a set of configurations in the MachineClass that follows the specification of the KubeVirt provider.\nThe KubeVirt MCM Extension has two main components. The MachinePlugin is responsible for handling the machine objects, and the PluginSPI is in charge of making calls to the cloud provider interface, to manage its resources.\nFigure 4: KubeVirt MCM extension workflow and architecture As shown in Figure 4, the MachinePlugin receives a machine request from the MCM and starts its processing by decoding the request, doing partial validation, extracting the relevant information, and sending it to the PluginSPI.\nThe PluginSPI then creates, gets, or deletes VirtualMachines depending on the method called by the MachinePlugin. It extracts the kubeconfig of the provider cluster and handles all other required KubeVirt resources such as the secret that holds the cloud-init configurations, and DataVolumes that are mounted as disks to the VMs.\nSupported Environments The Gardener KubeVirt support is currently qualified on:\n KubeVirt v0.32.0 (and later) Red Hat OpenShift Container Platform 4.4 (and later)  There are also plans for further improvements and new features, for example integration with CSI drivers for storage management. Details about the implementation progress can be found in the Gardener project on GitHub.\nYou can find further resources about the open source project Gardener at https://gardener.cloud.\n"},{"uri":"https://gardener.cloud/blog/2020-10/01/","title":"Shoot Reconciliation Details","tags":[],"description":"","content":"Do you want to understand how Gardener creates and updates Kubernetes clusters (Shoots)? Well, it\u0026rsquo;s complicated, but if you are not afraid of large diagrams and are a visual learner like me, this might be useful to you.\nIntroduction In this blog post I will share a technical diagram which attempts to tie together the various components involved when Gardener creates a Kubernetes cluster. I have created and curated the diagram, which visualizes the Shoot reconciliation flow since I started developing on Gardener. Aside from serving as a memory aid for myself, I created it in hopes that it may potentially help contributors to understand a core piece of the complex Gardener machinery. Please be advised that the diagram and components involved are large. Although it can be easily divided into multiple diagrams, I want to show all the components and connections in a single diagram to create an overview of the reconciliation flow.\nThe goal is to visualize the interactions of the components involved in the Shoot creation. It is not intended to serve as a documentation of every component involved.\nBackground Taking a step back, the Gardener READ.me states\n In essence, Gardener is an extension API server that comes along with a bundle of custom controllers. It introduces new API objects in an existing Kubernetes cluster (which is called garden cluster) in order to use them for the management of end-user Kubernetes clusters (which are called shoot clusters). These shoot clusters are described via declarative cluster specifications which are observed by the controllers. They will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.\n This means that Gardener, just like any Kubernetes controller, creates Kubernetes clusters (Shoots) using a reconciliation loop.\nThe Gardenlet contains the controller and reconciliation loop responsible for the creation, update, deletion and migration of Shoot cluster (there are more, but we spare them in this article). In addition, the Gardener Controller Manager also reconciles Shoot resources, but only for seed-independent functionality such as Shoot hibernation, Shoot maintenance or quota control.\nThis blog post is about the reconciliation loop in the Gardenlet responsible for creating and updating Shoot clusters. The code can be found here. The reconciliation loops of the extension controllers can be found in their individual repositories.\nShoot reconciliation flow diagram When Gardner creates a Shoot cluster, there are three conceptual layers involved: the Garden cluster, the Seed cluster and the Shoot cluster. Each layer represents a top-level section in the diagram (similar to a lane in a BPMN diagram).\nIt might seem confusing, that the Shoot cluster itself is a layer, because the whole flow in the first place is about creating the Shoot cluster. I decided to introduce this separate layer to make a clear distinction between which resources exist in the Seed API server (managed by Gardener) and which in the Shoot API server (accessible by the Shoot owner).\nEach section contains several components. Components are mostly Kubernetes resources in a Gardener installation (e.g. the gardenlet deployment in the Seed cluster).\nThis is the list of components:\n(Virtual) Garden Cluster\n Gardener Extension API server Validating Provider Webhooks Project Namespace  Seed Cluster\n Gardenlet Seed API server  every Shoot Control Plane has a dedicated namespace in the Seed.   Cloud Provider (owned by Stakeholder).  Arguably part of the Shoot cluster but used by components in the Seed cluster to create the infrastructure for the Shoot.   Gardener DNS extension Provider Extension (such as gardener-extension-provider-aws) Gardener Extension ETCD Druid Gardener Resource Manager Operating System Extension (such as gardener-extension-os-gardenlinux) Networking extension (such as gardener-extension-networking-cilium) Machine Controller Manager ContainerRuntime Extension (such as gardener-extension-runtime-gvisor) Shoot API server (in the Shoot Namespace in the Seed cluster)  Shoot Cluster\n Cloud Provider compute API (owned by Stakeholder) - for VM/Node creation. VM / Bare metal node hosted by Cloud Provider (in Stakeholder owned account).  How to use the diagram The diagram\n should be read from top to bottom - starting in the top left corner with the creation of the Shoot resource via the Gardener Extension API server. should not require an encompassing documentation / description. More detailed documentation on the components itself, can usually be found in the respective repository. does not show which activities execute in parallel (many) and also does not describe the exact dependencies between the steps. This can be found out by looking at the source code. It however tries to put the activities in a logical order of executing during the reconciliation flow.  Occasionally, there is an info box with additional information next to parts in the diagram that in my point of view require further explanation. Large example resource for the Gardener CRDs (e.g Worker CRD, Infrastructure CRD) are placed on the left side and are referenced by a dotted line (\u0026mdash;\u0026ndash;).\nBe aware, that Gardener is an evolving project, so the diagram will most likely be already outdated by the time you are reading this. Nevertheless, it should give a solid starting point for further explorations into the details of Gardener.\nFlow diagram The diagram can be found below and on Github.com. There are multiple formats available (svg, vsdx, draw.io, html).\nPlease open an issue or open a PR in the repository if information is missing or is incorrect. Thanks!\n\n"},{"uri":"https://gardener.cloud/blog/","title":"Blogs","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/","title":"Gardener","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/blog/2020-09/","title":"Gardener v1.9 and v1.10 Released","tags":[],"description":"","content":"Summer holidays aren\u0026rsquo;t over yet, still, the Gardener community was able to release two new minor versions in the past weeks. Despite being limited in capacity these days, we were able to reach some major milestones, like adding Kubernetes v1.19 support and the long-delayed automated gardenlet certificate rotation. Whilst we continue to work on topics related to scalability, robustness, and better observability, we agreed to adjust our focus a little more into the areas of development productivity, code quality and unit/integration testing for the upcoming releases.\nNotable Changes in v1.10 Gardener v1.10 was a comparatively small release (measured by the number of changes) but it comes with some major features!\nKubernetes 1.19 support (gardener/gardener#2799) The newest minor release of Kubernetes is now supported by Gardener (and all the maintained provider extensions)! Predominantly, we have enabled CSI migration for OpenStack now that it got promoted to beta, i.e. 1.19 shoots will no longer use the in-tree Cinder volume provisioner. The CSI migration enablement for Azure got postponed (to at least 1.20) due to some issues that the Kubernetes community is trying to fix in the 1.20 release cycle. As usual, the 1.19 release notes should be considered before upgrading your shoot clusters.\nAutomated certificate rotation for gardenlet (gardener/gardener#2542) Similar to the kubelet, the gardenlet supports TLS bootstrapping when deployed into a new seed cluster. It will request a client certificate for the garden cluster using the CertificateSigningRequest API of Kubernetes and store the generated results in a Secret object in the garden namespace of its seed. These certificates are usually valid for one year. We have now added support for automatic renewals if the expiration dates are approaching.\nImproved monitoring alerts (gardener/gardener#2776) We have worked on a larger refactoring to improve reliability and accuracy of our monitoring alerts for both shoot control planes in the seed as well as shoot system components running on worker nodes. The improvements are primarily for operators and should result in less false positive alerts. Also, the alerts should fire less frequently and are better grouped in order to reduce to overall amount of alerts.\nSeed deletion protection (gardener/gardener#2732) Our validation to improve robustness and countermeasures against accidental mistakes has been improved. Earlier, it was possible to remove the use-as-seed annotation for shooted seeds or directly set the deletionTimestamp on Seed objects, despite of the fact that they might still run shoot control planes. Seed deletion would not start in these cases, although, it would disrupt the system unnecessarily, and result in some unexpected behaviour. The Gardener API server is now forbidding such requests if the seeds are not completely empty yet.\nLogging improvements for Loki (multiple PRs) After we released our large logging stack refactoring (from EFK to Loki) with Gardener v1.8, we have continued to work on reliability, quality and user feedback in general. We aren\u0026rsquo;t done yet, though, Gardener v1.10 includes a bunch of improvements which will help to graduate the Logging feature gate to beta and GA, eventually.\nNotable Changes in v1.9 The v1.9 release contained tons of small improvements and adjustments in various areas of the code base and a little less new major features. However, we don\u0026rsquo;t want to miss the opportunity to highlight a few of them.\nCRI validation in CloudProfiles (gardener/gardener#2137) A couple of releases back we have introduced support for containerd and the ContainerRuntime extension API. The supported container runtimes are operating system specific, and until now it wasn\u0026rsquo;t possible for end-users to easily figure out whether they can enable containerd or other ContainerRuntime extensions for their shoots. With this change, Gardener administrators/operators can now provide that information in the .spec.machineImages section in the CloudProfile resource. This also allows for enhanced validation and prevents misconfigurations.\nNew shoot event controller (gardener/gardener#2649) The shoot controllers in both the gardener-controller-manager and gardenlet fire several Events for some important operations (e.g., automated hibernation/wake-up due to hibernation schedule, automated Kubernetes/machine image version update during maintenance, etc.). Earlier, the only way to prolong the lifetime of these events was to modify the --event-ttl command line parameter of the garden cluster\u0026rsquo;s kube-apiserver. This came with the disadvantage that all events were kept for a longer time (not only those related to Shoots that an operator is usually interested in and ideally wants to store for a couple of days). The new shoot event controller allows to achieve this by deleting non-shoot events. This helps operators and end-users to better understand which changes were applied to their shoots by Gardener.\nEarly deployment of the logging stack for new shoots (gardener/gardener#2750) Since the first introduction of the Logging feature gate two years back the logging stack was only deployed at the very end of the shoot creation. This had the disadvantage that control plane pod logs were not kept in case the shoot creation flow is interrupted before the logging stack could be deployed. In some situations, this was preventing fetching relevant information about why a certain control plane component crashed. We now deploy the logging stack very early in the shoot creation flow to always have access to such information.\n"},{"uri":"https://gardener.cloud/blog/2020-08/00/","title":"Gardener v1.8.0 Released","tags":[],"description":"","content":"Even if we are in the midst of the summer holidays, a new Gardener release came out yesterday: v1.8.0! It\u0026rsquo;s main themes are the large change of our logging stack to Loki (which was already explained in detail on a blog post on grafana.com), more configuration options to optimize the utilization of a shoot, node-local DNS, new project roles, and significant improvements for the Kubernetes client that Gardener uses to interact with the many different clusters.\nNotable Changes Logging 2.0: EFK stack replaced by Loki (gardener/gardener#2515) Since two years or so Gardener could optionally provision a dedicated logging stack per seed and per shoot which was based on fluent-bit, fluentd, ElasticSearch and Kibana. This feature was still hidden behind an alpha-level feature gate and never got promoted to beta so far. Due to various limitations of this solution we decided to replace the EFK stack with Loki. As we already have Prometheus and Grafana deployments for both users and operators by default for all clusters the choice was just natural. Please find out more on this topic at this dedicated blog post.\nCluster identities and DNSOwner objects (gardener/gardener#2471, gardener/gardener#2576) The shoot control plane migration topic is ongoing since a few months already, and we are very much progressing with it. A first alpha version will probably make it out soon. As part of these endeavors, we introduced cluster identities and the usage of DNSOwner objects in this release. Both are needed to gracefully migrate the DNSEntry extension objects from the old seed to the new seed as part of the control plane migration process. Please find out more on this topic at this blog post.\nNew uam role for Project members to limit user access management privileges (gardener/gardener#2611) In order to allow external user access management system to integrate with Gardener and to fulfil certain compliance aspects, we have introduced a new role called uam for Project members (next to admin and viewer). Only if a user has this role then he/she is allowed to add/remove other human users to the respective Project. By default, all newly created Projects assign this role only to the owner while, for backwards-compatibility reasons, it will be assigned for all members for existing projects. Project owners can steadily revoke this access as desired. Interestingly, the uam role is backed by a custom RBAC verb called manage-members, i.e., the Gardener API server is only admitting changes to the human Project members if the respective user is bound to this RBAC verb.\nNew node-local DNS feature for shoots (gardener/gardener#2528) By default, we are using CoreDNS as DNS plugin in shoot clusters which we auto-scale horizontally using HPA. However, in some situations we are discovering certain bottlenecks with it, e.g., unreliable UDP connections, unnecessary node hopping, inefficient load balancing, etc. To further optimize the DNS performance for shoot clusters, it is now possible to enable a new alpha-level feature gate in the gardenlet\u0026rsquo;s componentconfig: NodeLocalDNS. If enabled, all shoots will get a new DaemonSet to run a DNS server on each node.\nMore kubelet and API server configurability (gardener/gardener#2574, gardener/gardener#2668) One large benefit of Gardener is that it allows you to optimize the usage of your control plane as well as worker nodes by exposing relevant configuration parameters in the Shoot API. In this version, we are adding support to configure kubelet\u0026rsquo;s values for systemReserved and kubeReserved resources as well as the kube-apiserver\u0026rsquo;s watch cache sizes. This allows end-users to get to better node utilization and/or performance for their shoot clusters.\nConfigurable timeout settings for machine-controller-manager (gardener/gardener#2563) One very central component in Project Gardener is the machine-controller-manager for managing the worker nodes of shoot clusters. It has extensive qualities with respect to node lifecycle management and rolling updates. As such, it uses certain timeout values, e.g. when creating or draining nodes, or when checking their health. Earlier, those were not customizable by end-users, but we are adding this possibility now. You can fine-grain these settings per worker pool in the Shoot API such that you can optimize the lifecycle management of your worker nodes even more!\nImproved usage of cached client to reduce network I/O (gardener/gardener#2635, gardener/gardener#2637) In the last Gardener release v1.7 we have introduced a huge refactoring the clients that we use to interact with the many different Kubernetes clusters. This is to further optimize the network I/O performed by leveraging watches and caches as good as possible. It\u0026rsquo;s still an alpha-level feature that must be explicitly enabled in the Gardenlet\u0026rsquo;s component configuration, though, with this release we have improved certain things in order to pave the way for beta promotion. For example, we were initially also using a cached client when interacting with shoots. However, as the gardenlet runs in the seed as well (and thus can communicate cluster-internally with the kube-apiservers of the respective shoots) this cache is not necessary and just memory overhead. We have removed it again and saw the memory usage getting lower again. More to come!\nAWS EBS volume encryption by default (gardener/gardener-extension-provider-aws#147) The Shoot API already exposed the possibility to encrypt the root disks of worker nodes since quite a while, but it was disabled by default (for backwards-compatibility reasons). With this release we have change this default, so new shoot worker nodes will be provisioned with encrypted root disks out-of-the-box. However, the g4dn instance types of AWS don\u0026rsquo;t support this encryption, so when you use them you have to explicitly disable the encryption in the worker pool configuration.\nLiveness probe for Gardener API server deployment (gardener/gardener#2647) A small, but very valuable improvement is the introduction of a liveness probe for our Gardener API server. As it\u0026rsquo;s built with the same library like the Kubernetes API server, it exposes two endpoints at /livez and /readyz which were created exactly for the purpose of live- and readiness probes. With Gardener v1.8 the Helm chart contains a liveness probe configuration by default, and we are awaiting an upstream fix (kubernetes/kubernetes#93599) to also enable the readiness probe. This will help in a smoother rolling update of the Gardener API server pods, i.e., preventing clients from talking to a not yet initialized or already terminating API server instance.\nWebhook ports changed to enable OpenShift (gardener/gardener#2660) In order to make it possible to run Gardener on OpenShift clusters as well, we had to make a change in the port configuration for the webhooks we are using in both Gardener and the extension controllers. Earlier, all the webhook servers directly exposed port 443, i.e., a system port which is a security concern and disallowed in OpenShift. We have changed this port now across all places and also adapted our network policies accordingly. This is most likely not the last necessary change to enable this scenario, however, it\u0026rsquo;s a great improvement to push the project forward.\nIf you\u0026rsquo;re interested in more details and even more improvements you can find all release notes for Gardener v1.8.0 here: https://github.com/gardener/gardener/releases/v1.12.2/tag/v1.8.0\n"},{"uri":"https://gardener.cloud/blog/2020-05/00/","title":"PingCAP‚Äôs Experience in Implementing their Managed TiDB Service with Gardener","tags":[],"description":"","content":"Gardener is showing successful collaboration with its growing community of contributors and adopters. With this come some success stories, including PingCAP using Gardener to implement its managed service.\nAbout PingCAP and its TiDB Cloud PingCAP started in 2015, when three seasoned infrastructure engineers working at leading Internet companies got sick and tired of the way databases were managed, scaled and maintained. Seeing no good solution on the market, they decided to build their own - the open-source way. With the help of a first-class team and hundreds of contributors from around the globe, PingCAP is building a distributed NewSQL, hybrid transactional and analytical processing (HTAP) database.\nIts flagship project, TiDB, is a cloud-native distributed SQL database with MySQL compatibility, and one of the most popular open-source database projects - with 23.5K+ stars and 400+ contributors. Its sister project TiKV is a Cloud Native Interactive Landscape project.\nPingCAP envisioned their managed TiDB service, known as TiDB Cloud, to be multi-tenant, secure, cost-efficient, and to be compatible with different cloud providers. As a result, the company turned to Gardener to build their managed TiDB cloud service offering.\nTiDB Cloud Beta Preview Limitations with other public managed Kubernetes services Previously, PingCAP encountered issues while using other public managed K8s cluster services, to develop the first version of its TiDB Cloud. Their worst pain point was that they felt helpless when encountering certain malfunctions. PingCAP wasn‚Äôt able to do much to resolve these issues, except waiting for the providers‚Äô help. More specifically, they experienced problems due to cloud-provider specific Kubernetes system upgrades, delays in the support response (which could be avoided in exchange of a costly support fee), and no control over when things got fixed.\nThere was also a lot of cloud-specific integration work needed to follow a multi-cloud strategy, which proved to be expensive both to produce and maintain. With one of these managed K8s services, you would have to integrate the instance API, as opposed to a solution like Gardener, which provides a unified API for all clouds. Such a unified API eliminates the need to worry about cloud specific-integration work altogether.\nWhy PingCAP chose Gardener to build TiDB Cloud  ‚ÄúGardener has similar concepts to Kubernetes. Each Kubernetes cluster is just like a Kubernetes pod, so the similar concepts apply, and the controller pattern makes Gardener easy to manage. It was also easy to extend, as the team was already very familiar with Kubernetes, so it wasn‚Äôt hard for us to extend Gardener. We also saw that Gardener has a very active community, which is always a plus!‚Äù- Aylei Wu, (Cloud Engineer) at PingCAP\n At first glance, PingCAP had initial reservations about using Gardener - mainly due to its adoption level (still at the beginning) and an apparent complexity of use. However, these were soon eliminated as they learned more about the solution. As Aylei Wu mentioned during the last Gardener community meeting, ‚Äúa good product speaks for itself‚Äù, and once the company got familiar with Gardener, they quickly noticed that the concepts were very similar to Kubernetes, which they were already familiar with.\nThey recognized that Gardener would be their best option, as it is highly extensible and provides a unified abstraction API layer. In essence, the machines can be managed via a machine controller manager for different cloud providers - without having to worry about the individual cloud APIs.\nThey agreed that Gardener‚Äôs solution, although complex, was definitely worth it. Even though it is a relatively new solution, meaning they didn‚Äôt have access to other user testimonials, they decided to go with the service since it checked all the boxes (and as SAP was running it productively with a huge fleet). PingCAP also came to the conclusion that building a managed Kubernetes service themselves would not be easy. Even if they were to build a managed K8s service, they would have to heavily invest in development and would still end up with an even more complex platform than Gardener‚Äôs. For all these reasons combined, PingCAP decided to go with Gardener to build its TiDB Cloud.\nHere are certain features of Gardener that PingCAP found appealing:\n Cloud agnostic: Gardener‚Äôs abstractions for cloud-specific integrations dramatically reduce the investment in supporting more than one cloud infrastructure. Once the integration with Amazon Web Services was done, moving on to Google Cloud Platform proved to be relatively easy. (At the moment, TiDB Cloud has subscription plans available for both GCP and AWS, and they are planning to support Alibaba Cloud in the future.) Familiar concepts: Gardener is K8s native; its concepts are easily related to core Kubernetes concepts. As such, it was easy to onboard for a K8s experienced team like PingCAP‚Äôs SRE team. Easy to manage and extend: Gardener‚Äôs API and extensibility are easy to implement, which has a positive impact on the implementation, maintenance costs and time-to-market. Active community: Prompt and quality responses on Slack from the Gardener team tremendously helped to quickly onboard and produce an efficient solution.  How PingCAP built TiDB Cloud with Gardener On a technical level, PingCAP‚Äôs set-up overview includes the following:\n A Base Cluster globally, which is the top-level control plane of TiDB Cloud A Seed Cluster per cloud provider per region, which makes up the fundamental data plane of TiDB Cloud A Shoot Cluster is dynamically provisioned per tenant per cloud provider per region when requested A tenant may create one or more TiDB clusters in a Shoot Cluster  As a real world example, PingCAP sets up the Base Cluster and Seed Clusters in advance. When a tenant creates its first TiDB cluster under the us-west-2 region of AWS, a Shoot Cluster will be dynamically provisioned in this region, and will host all the TiDB clusters of this tenant under us-west-2. Nevertheless, if another tenant requests a TiDB cluster in the same region, a new Shoot Cluster will be provisioned. Since different Shoot Clusters are located in different VPCs and can even be hosted under different AWS accounts, TiDB Cloud is able to achieve hard isolation between tenants and meet the critical security requirements for our customers.\nTo automate these processes, PingCAP creates a service in the Base Cluster, known as the TiDB Cloud ‚ÄúCentral‚Äù service. The Central is responsible for managing shoots and the TiDB clusters in the Shoot Clusters. As shown in the following diagram, user operations go to the Central, being authenticated, authorized, validated, stored and then applied asynchronously in a controller manner. The Central will talk to the Gardener API Server to create and scale Shoot clusters. The Central will also access the Shoot API Service to deploy and reconcile components in the Shoot cluster, including control components (TiDB Operator, API Proxy, Usage Reporter for billing, etc.) and the TiDB clusters.\nTiDB Cloud on Gardener Architecture Overview What‚Äôs next for PingCAP and Gardener With the initial success of using the project to build TiDB Cloud, PingCAP is now working heavily on the stability and day-to-day operations of TiDB Cloud on Gardener. This includes writing Infrastructure-as-Code scripts/controllers with it to achieve GitOps, building tools to help diagnose problems across regions and clusters, as well as running chaos tests to identify and eliminate potential risks. After benefiting greatly from the community, PingCAP will continue to contribute back to Gardener.\nIn the future, PingCAP also plans to support more cloud providers like AliCloud and Azure. Moreover, PingCAP may explore the opportunity of running TiDB Cloud in on-premise data centers with the constantly expanding support this project provides. Engineers at PingCAP enjoy the ease of learning from Gardener‚Äôs kubernetes-like concepts and being able to apply them everywhere. Gone are the days of heavy integrations with different clouds and worrying about vendor stability. With this project, PingCAP now sees broader opportunities to land TiDB Cloud on various infrastructures to meet the needs of their global user group.\nStay tuned, more blog posts to come on how Gardener is collaborating with its contributors and adopters to bring fully-managed clusters at scale everywhere! If you want to join in on the fun, connect with our community.\n"},{"uri":"https://gardener.cloud/blog/2020_week_20/00/","title":"New Website, Same Green Flower","tags":[],"description":"","content":"The Gardener project website just received a serious facelift. Here are some of the highlights:\n A completely new landing page, emphasizing both on Gardener\u0026rsquo;s value proposition and the open community behind it. The Community page was reconstructed for quick access to the various community channels and will soon merge the Adopters page. It will provide a better insight into success stories from the communty. A completely new News section and content type available at /documentation/news. Use metadata such as publishdate and archivedate to schedule for news publish and archive automatically, regardless of when you contributed them. You can now track what\u0026rsquo;s happening from the landing page or in the dedicated News section on the website and share. Improved blogs layout. One-click sharing options are available starting with simple URL copy link and twitter button and others will closely follow up. While we are at it, give it a try. Spread the word.  Website builds also got to a new level with:\n Containerization. The whole build environment is containerized now, eliminating differences between local and CI/CD setup and reducing content developers focus only to the /documentation repository. Running a local server for live preview of changes as you make them when developing content for the website, is now as easy as runing make serve in your local /documentation clone. Numerous improvements to the buld scripts. More configuration options, authenticated requests, fault tollerance and performance. Good news for Windows WSL users who will now nejoy a significantly support. See the updated README for details on that. A number of improvements in layouts styles, site assets and hugo site-building techniques.  But hey, THAT\u0026rsquo;S NOT ALL!\nStay tuned for more improvements around the corner. The biggest ones are aligning the documentation with the new theme and restructuring it along, more emphasis on community success stories all around, more sharing options and more than a handful of shortcodes for content development and \u0026hellip; let\u0026rsquo;s cut the spoilers here.\nI hope you will like it. Let us know what you think about it. Feel free to leave comments and discuss on Twitter and Slack, or in case of issues - on GitHub.\nGo ahead and help us spread the word: https://gardener.cloud\n  "},{"uri":"https://gardener.cloud/blog/2019_week_21/","title":"Feature Flags in Kubernetes Applications","tags":[],"description":"","content":"Feature flags are used to change the behavior of a program at runtime without forcing a restart.\nAlthough they are essential in a native cloud environment, they cannot be implemented without significant effort on some platforms. Kubernetes has made this trivial. Here we will implement them through labels and annotations, but you can also implement them by connecting directly to the Kubernetes API Server.\nPossible Use Cases\n turn on/off a specific instance turn on/off profiling of a specific instance change the logging level, to capture detailed logs during a specific event change caching strategy at runtime change timeouts in production toggle on/off some special verification  ..read some more on Feature Flags for App.\n"},{"uri":"https://gardener.cloud/blog/2019_week_21_2/","title":"Feature Flags in Kubernetes Applications","tags":[],"description":"","content":"Feature flags are used to change the behavior of a program at runtime without forcing a restart.\nAlthough they are essential in a native cloud environment, they cannot be implemented without significant effort on some platforms. Kubernetes has made this trivial. Here we will implement them through labels and annotations, but you can also implement them by connecting directly to the Kubernetes API Server.\nPossible Use Cases\n turn on/off a specific instance turn on/off profiling of a specific instance change the logging level, to capture detailed logs during a specific event change caching strategy at runtime change timeouts in production toggle on/off some special verification  ..read some more on Feature Flags for App.\n"},{"uri":"https://gardener.cloud/blog/2019_week_06/","title":"Manually adding a node to an existing cluster","tags":[],"description":"","content":"Gardener has an excellent ability to automatically scale machines for the cluster. From the point of view of scalability, there is no need for manual intervention.\nThis tutorial is useful for those end-users who need specifically configured nodes, which are not yet supported by Gardener. For example: an end-user who wants some workload that requires runnc instead of runc as container runtime.\n..read some more on Adding Nodes to a Cluster.\n"},{"uri":"https://gardener.cloud/blog/2019_week_02/","title":"Organizing Access Using kubeconfig Files","tags":[],"description":"","content":"The kubectl command-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.\n What happens if your kubeconfig file of your production cluster is leaked or published by accident?\n Since there is no possibility to rotate or revoke the initial kubeconfig, there is only one way to protect your infrastructure or application if it is has leaked - delete the cluster.\n..learn more on Work with kubeconfig files.\n"},{"uri":"https://gardener.cloud/blog/2018_week_40/","title":"Hibernate a Cluster to save money","tags":[],"description":"","content":"You want to experiment with Kubernetes or have set up a customer scenario, but you don\u0026rsquo;t want to run the cluster 24 / 7 for reasons of cost?\nThe Gardener gives you the possibility to scale your cluster down to zero nodes.\n..read some more on Hibernate a Cluster.\n"},{"uri":"https://gardener.cloud/blog/2018_week_22/","title":"Anti Patterns","tags":[],"description":"","content":"Running as root user Whenever possible, do not run containers as root users. One could be tempted to say that Kubernetes Pods and Node are well separated. The host and the container share the same kernel. If the container is compromised, a root user can damage the underlying node. Use RUN groupadd -r anygroup \u0026amp;\u0026amp; useradd -r -g anygroup myuser to create a group and a user in it. Use the USER command to switch to this user.\nStoring data or logs in containers Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the container, as they are lost when the container is closed. If absolutely necessary, you can use persistence volumes instead to persist them outside the containers. However, an ELK stack is preferred for storing and processing log files.\n..read some more on Common Kubernetes Antipattern.\n"},{"uri":"https://gardener.cloud/blog/2018_week_46/","title":"Auditing Kubernetes for Secure Setup","tags":[],"description":"","content":"In summer 2018, the Gardener project team asked Kinvolk to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work is to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a Control-Plane-as-a-Service with a network air gap.\nAlong the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.\n..read some more on Auditing Kubernetes for Secure Setup.\n"},{"uri":"https://gardener.cloud/blog/2018_week_07/","title":"Big things come in small packages","tags":[],"description":"","content":"Microservices tend to use smaller runtimes but you can use what you have today - and this can be a problem in kubernetes.\nSwitching your architecture from a monolith to microservices has many advantages, both in the way you write software and the way it is used throughout its lifecycle. In this post, my attempt is to cover one problem which does not get as much attention and discussion - size of the technology stack.\nGeneral purpose technology stack There is a tendency to be more generalized in development and to apply this pattern to all services. One feels that a homogeneous image of the technology stack is good if it is the same for all services.\nOne forgets, however, that a large percentage of the integrated infrastructure is not used by all services in the same way, and is therefore only a burden. Thus, resources are wasted and the entire application becomes expensive in operation and scales very badly.\nLight technology stack Due to the lightweight nature of your service, you can run more containers on a physical server and virtual machines. The result is higher resource utilization.\nAdditionally, microservices are developed and deployed as containers independently of each another. This means that a development team can develop, optimize and deploy a microservice without impacting other subsystems.\n"},{"uri":"https://gardener.cloud/blog/2018_week_51/","title":"Cookies are dangerous...","tags":[],"description":"","content":"\u0026hellip;they mess up the figure.\nFor a team event during the Christmas season we decided to completely reinterpret the topic cookies\u0026hellip; since the vegetables have gone on a well-deserved vacation. :-)\nGet recipe on Gardener Cookies.\n"},{"uri":"https://gardener.cloud/blog/2018_week_17/","title":"Frontend HTTPS","tags":[],"description":"","content":"For encrypted communication between the client to the load balancer, you need to specify a TLS private key and certificate to be used by the ingress controller.\nCreate a secret in the namespace of the ingress containing the TLS private key and certificate. Then configure the secret name in the TLS configuration section of the ingress specification.\n..read on HTTPS - Self Signed Certificates how to configure it.\n"},{"uri":"https://gardener.cloud/blog/2018_week_50/","title":"Hardening the Gardener Community Setup","tags":[],"description":"","content":"The Gardener project team has analyzed the impact of the Gardener CVE-2018-2475 and the Kubernetes CVE-2018-1002105 on the Gardener Community Setup. Following some recommendations it is possible to mitigate both vulnerabilities.\nRead more on Hardening the Gardener Community Setup.\n"},{"uri":"https://gardener.cloud/blog/2018_week_06/","title":"Kubernetes is available in Docker for Mac 17.12 CE","tags":[],"description":"","content":"    Kubernetes is only available in Docker for Mac 17.12 CE and higher on the Edge channel. Kubernetes support is not included in Docker for Mac Stable releases. To find out more about Stable and Edge channels and how to switch between them, see general configuration.     Docker for Mac 17.12 CE (and higher) Edge includes a standalone Kubernetes server that runs on Mac, so that you can test deploying your Docker workloads on Kubernetes. The Kubernetes client command, kubectl, is included and configured to connect to the local Kubernetes server. If you have kubectl already installed and pointing to some other environment, such as minikube or a GKE cluster, be sure to change context so that kubectl is pointing to docker-for-desktop:\n\u0026hellip;see more on Docker.com\nI recommend to setup your shell to see which KUBECONFIG is active.\n"},{"uri":"https://gardener.cloud/blog/2018_week_09/","title":"Namespace Isolation","tags":[],"description":"","content":"\u0026hellip;or DENY all traffic from other namespaces\nYou can configure a NetworkPolicy to deny all traffic from other namespaces while allowing all traffic coming from the same namespace the pod is deployed to. There are many reasons why you may chose to configure Kubernetes network policies:\n Isolate multi-tenant deployments Regulatory compliance Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each another  ..read on Namespace Isolation how to configure it.\n"},{"uri":"https://gardener.cloud/blog/2018_week_08/","title":"Namespace Scope","tags":[],"description":"","content":"Should I use:\n‚ùå one namespace per user/developer? ‚ùå one namespace per team? ‚ùå one per service type? ‚ùå one namespace per application type? üòÑ one namespace per running instance of your application?  Apply the Principle of Least Privilege\nAll user accounts should run at all times as few privileges as possible, and also launch applications with as few privileges as possible. If you share a cluster for different user separated by a namespace, all user has access to all namespaces and services per default. It can happen that a user accidentally uses and destroys the namespace of a productive application or the namespace of another developer.\nKeep in mind: By default namespaces don\u0026rsquo;t provide:\n Network isolation Access Control Audit Logging on user level  "},{"uri":"https://gardener.cloud/blog/2018_week_08_2/","title":"Namespace Scope","tags":[],"description":"","content":"Should I use:\n‚ùå one namespace per user/developer? ‚ùå one namespace per team? ‚ùå one per service type? ‚ùå one namespace per application type? üòÑ one namespace per running instance of your application?  Apply the Principle of Least Privilege\nAll user accounts should run at all times as few privileges as possible, and also launch applications with as few privileges as possible. If you share a cluster for different user separated by a namespace, all user has access to all namespaces and services per default. It can happen that a user accidentally uses and destroys the namespace of a productive application or the namespace of another developer.\nKeep in mind: By default namespaces don\u0026rsquo;t provide:\n Network isolation Access Control Audit Logging on user level  "},{"uri":"https://gardener.cloud/blog/2018_week_27/","title":"ReadWriteMany - Dynamically Provisioned Persistent Volumes Using Amazon EFS","tags":[],"description":"","content":"The efs-provisioner allows you to mount EFS storage as PersistentVolumes in kubernetes. It consists of a container that has access to an AWS EFS resource. The container reads a configmap containing the EFS filesystem ID, the AWS region and the name identifying the efs-provisioner. This name will be used later when you create a storage class.\nWhy EFS  When you have application running on multiple nodes which require shared access to a file system When you have an application that requires multiple virtual machines to access the same file system at the same time, AWS EFS is a tool that you can use. EFS supports encryption. EFS is SSD based storage and its storage capacity and pricing will scale in or out as needed, so there is no need for the system administrator to do additional operations. It can grow to a petabyte scale. EFS now supports NFSv4 lock upgrading and downgrading, so yes, you can use sqlite with EFS‚Ä¶ even if it was possible before. Easy to setup  Why Not EFS  Sometimes when you think about using a service like EFS, you may also think about vendor lock-in and its negative sides Making an EFS backup may decrease your production FS performance; the throughput used by backup counts towards your total file system throughput. EFS is expensive compared to EBS (roughly twice the price of EBS storage) EFS is not the magical solution for all your distributed FS problems, it can be slow in many cases. Test, benchmark and measure to ensure your if EFS is a good solution for your use case. EFS distributed architecture results in a latency overhead for each file read/write operation. If you have the possibility to use a CDN, don‚Äôt use EFS, use it for the files which can\u0026rsquo;t be stored in a CDN. Don‚Äôt use EFS as a caching system, sometimes you could be doing this unintentionally. Last but not least, even if EFS is a fully managed NFS, you will face performance problems in many cases, resolving them takes time and needs effort.  "},{"uri":"https://gardener.cloud/blog/2018_week_10/","title":"Shared storage with S3 backend","tags":[],"description":"","content":"The storage is definitely the most complex and important part of an application setup, once this part is completed, one of the most problematic parts could be solved.\nMounting a S3 bucket into a pod using FUSE allows to access data stored in S3 via the filesystem. The mount is a pointer to an S3 location, so the data is never synced locally. Once mounted, any pod can read or even write from that directory without the need for explicit keys.\nHowever, it can be used to import and parse large amounts of data into a database.\n..read on Shared S3 Storage how to configure it.\n"},{"uri":"https://gardener.cloud/adopter/","title":"Adopters","tags":[],"description":"","content":"See who is using Gardener Gardener adopters in production environments that have publicly shared details of their usage.       b‚Äônerd uses Gardener as the core technology for its own managed Kubernetes as a Service solution and operates multiple Gardener installations for several cloud hosting service providers.    SAP uses Gardener to deploy and manage Kubernetes clusters at scale in a uniform way across infrastructures (AWS, Azure, GCP, Alicloud, OpenStack). Workloads include databases (SAP HANA), Big Data (SAP Data Hub), IoT, AI, and Machine Learning (SAP Leonardo), Serverless and diverse business workloads.    ScaleUp Technologies runs Gardener within their public Openstack Clouds (Hamburg, Berlin, D√ºsseldorf). Their clients run all kinds of workloads on top of Gardener maintained Kubernetes clusters ranging from databases to Software-as-a-Service applications.    Finanz Informatik Technologie Services GmbH uses Gardener to offer k8s as a service for customers in the financial industry in Germany. It is built on top of a \"metal as a service\" infrastructure implemented from scratch for k8s workloads in mind. The result is k8s on top of bare metal in minutes.    PingCAP TiDB, is a cloud-native distributed SQL database with MySQL compatibility, and one of the most popular open-source database projects - with 23.5K+ stars and 400+ contributors. Its sister project TiKV is a Cloud Native Interactive Landscape project. PingCAP envisioned their managed TiDB service, known as TiDB Cloud, to be multi-tenant, secure, cost-efficient, and to be compatible with different cloud providers and they chose Gardener.    Beezlabs uses Gardener to deliver Intelligent Process Automation platform, on multiple cloud providers and reduce costs and lock-in risks.   If you‚Äôre using Gardener and you aren‚Äôt on this list, submit a pull request!    "},{"uri":"https://gardener.cloud/documentation/concepts/monitoring/alerting/","title":"Alerting","tags":[],"description":"","content":"Alerting Gardener uses Prometheus to gather metrics from each component. A Prometheus is deployed in each shoot control plane (on the seed) which is responsible for gathering control plane and cluster metrics. Prometheus can be configured to fire alerts based on these metrics and send them to an alertmanager. The alertmanager is responsible for sending the alerts to users and operators. This document describes how to setup alerting for:\n end-users/stakeholders/customers operators/administrators  Alerting for Users To receive email alerts as a user set the following values in the shoot spec:\nspec:monitoring:alerting:emailReceivers:- john.doe@example.comemailReceivers is a list of emails that will receive alerts if something is wrong with the shoot cluster. A list of alerts for users can be found here.\nAlerting for Operators Currently, Gardener supports two options for alerting:\n Email Alerting Sending Alerts to an external alertmanager  A list of operator alerts can be found here.\nEmail Alerting Gardener provides the option to deploy an alertmanager into each seed. This alertmanager is responsible for sending out alerts to operators for each shoot cluster in the seed. Only email alerts are supported by the alertmanager managed by Gardener. This is configurable by setting the Gardener controller manager configuration values alerting. See this on how to configure the Gardener\u0026rsquo;s SMTP secret. If the values are set, a secret with the label gardener.cloud/role: alerting will be created in the garden namespace of the garden cluster. This secret will be used by each alertmanager in each seed.\nExternal Alertmanager The alertmanager supports different kinds of alerting configurations. The alertmanager provided by Gardener only supports email alerts. If email is not sufficient, then alerts can be sent to an external alertmanager. Prometheus will send alerts to a URL and then alerts will be handled by the external alertmanager. This external alertmanager is operated and configured by the operator (i.e. Gardener does not configure or deploy this alertmanager). To configure sending alerts to an external alertmanager, create a secret in the virtual garden cluster in the garden namespace with the label: gardener.cloud/role: alerting. This secret needs to contain a URL to the the external alertmanager and information regarding authentication. Supported authentication types are:\n No Authentication (none) Basic Authentication (basic) Mutual TLS (certificate)  Remote Alertmanager Examples Note: the url value cannot be prepended with http or https.\n# No AuthenticationapiVersion:v1kind:Secretmetadata:labels:gardener.cloud/role:alertingname:alerting-authnamespace:gardendata:# No Authenticationauth_type:base64(none)url:base64(external.alertmanager.foo)# Basic Authauth_type:base64(basic)url:base64(extenal.alertmanager.foo)username:base64(admin)password:base64(password)# Mutual TLSauth_type:base64(certificate)url:base64(external.alertmanager.foo)ca.crt:base64(ca)tls.crt:base64(certificate)tls.key:base64(key)# Email Alerts (internal alertmanager)auth_type:base64(smtp)auth_identity:base64(internal.alertmanager.auth_identity)auth_password:base64(internal.alertmanager.auth_password)auth_username:base64(internal.alertmanager.auth_username)from:base64(internal.alertmanager.from)smarthost:base64(internal.alertmanager.smarthost)to:base64(internal.alertmanager.to)type:OpaqueConfiguring your External Alertmanager Please refer to the alertmanager documentation on how to configure an alertmanager.\nWe recommend you use at least the following inhibition rules in your alertmanager configuration to prevent excessive alerts:\ninhibit_rules:# Apply inhibition if the alert name is the same.- source_match:severity:criticaltarget_match:severity:warningequal:[\u0026#39;alertname\u0026#39;,\u0026#39;service\u0026#39;,\u0026#39;cluster\u0026#39;]# Stop all alerts for type=shoot if there are VPN problems.- source_match:service:vpntarget_match_re:type:shootequal:[\u0026#39;type\u0026#39;,\u0026#39;cluster\u0026#39;]# Stop warning and critical alerts if there is a blocker- source_match:severity:blockertarget_match_re:severity:^(critical|warning)$equal:[\u0026#39;cluster\u0026#39;]# If the API server is down inhibit no worker nodes alert. No worker nodes depends on kube-state-metrics which depends on the API server.- source_match:service:kube-apiservertarget_match_re:service:nodesequal:[\u0026#39;cluster\u0026#39;]# If API server is down inhibit kube-state-metrics alerts.- source_match:service:kube-apiservertarget_match_re:severity:infoequal:[\u0026#39;cluster\u0026#39;]# No Worker nodes depends on kube-state-metrics. Inhibit no worker nodes if kube-state-metrics is down.- source_match:service:kube-state-metrics-shoottarget_match_re:service:nodesequal:[\u0026#39;cluster\u0026#39;]Below is a graph visualizing the inhibition rules:\n"},{"uri":"https://gardener.cloud/documentation/concepts/deployment/deploy_gardenlet_automatically/","title":"Automatic Deployment of Gardenlets","tags":[],"description":"","content":"Automatic Deployment of Gardenlets The gardenlet can automatically deploy itself into shoot clusters, and register this cluster as a seed cluster. These clusters are called shooted seeds. This procedure is the preferred way to add additional seed clusters, because shoot clusters already come with production-grade qualities that are also demanded for seed clusters.\nPrerequisites The only prerequisite is to register an initial cluster as a seed cluster that has already a gardenlet deployed:\n This gardenlet was either deployed as part of a Gardener installation using an installation tool (for example, gardener/garden-setup) or the gardenlet was deployed manually (more information: Deploy a Gardenlet Manually)   The initial cluster can be the garden cluster itself.\n Self-Deployment of Gardenlets in Additional Shooted Seed Clusters For a better scalability, you usually need more seed clusters that you can create as follows:\n Use the initial cluster as the seed cluster for other shooted seed clusters. It hosts the control planes of the other seed clusters. The gardenlet deployed in the initial cluster deploys itself automatically into the shooted seed clusters.  The advantage of this approach is that there‚Äôs only one initial gardenlet installation required. Every other shooted seed cluster has a gardenlet deployed automatically.\nRelated Links Create Shooted Seed Cluster\ngarden-setup\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/backupbucket/","title":"BackupBucket resource","tags":[],"description":"","content":"Contract: BackupBucket resource The Gardener project features a sub-project called etcd-backup-restore to take periodic backups of etcd backing Shoot clusters. It demands the bucket (or its equivalent in different object store providers) to be created and configured externally with appropriate credentials. The BackupBucket resource takes this responsibility in Gardener.\nBefore introducing the BackupBucket extension resource Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see here). Now, Gardener commissions an external, provider-specific controller to take over this task. You can also refer to backupInfra proposal documentation to get idea about how the transition was done and understand the resource in broader scope.\nWhat is the scope of bucket? A bucket will be provisioned per Seed. So, backup of every Shoot created on that Seed will be stored under different shoot specific prefix under the bucket. For the backup of the Shoot rescheduled on different Seed it will continue to use the same bucket.\nWhat is the lifespan of BackupBucket? The bucket associated with BackupBucket will be created at creation of Seed. And as per current implementation, it will be deleted on deletion of Seed and there isn\u0026rsquo;t any BackupEntry resource associated with it.\nIn the future, we plan to introduce schedule for BackupBucket the deletion logic for BackupBucket resource, which will reschedule the it on different available Seed, on deletion or failure of health check for current associated seed. In that case, BackupBucket will be deleted only if there isn\u0026rsquo;t any schedulable Seed available and there isn\u0026rsquo;t any associated BackupEntry resource.\nWhat needs to be implemented to support a new infrastructure provider? As part of the seed flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:BackupBucketmetadata:name:foospec:type:azureproviderConfig:\u0026lt;some-optional-provider-specific-backupbucket-configuration\u0026gt; region: eu-west-1secretRef:name:backupprovidernamespace:shoot--foo--barThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. This provider secret will be configured by Gardener operator in the Seed resource and propagated over there by seed controller.\nAfter your controller has created the required bucket, if required it generates the secret to access the objects in buckets and put reference to it in status. This secret is supposed to be used by Gardener or eventually BackupEntry resource and etcd-backup-restore component to backup the etcd.\nIn order to support a new infrastructure provider you need to write a controller that watches all BackupBuckets with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the Azure provider.\nReferences and additional resources  BackupBucket API Reference Exemplary implementation for the Azure provider BackupEntry resource documentation Shared bucket proposal  "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/backupentry/","title":"BackupEntry resource","tags":[],"description":"","content":"Contract: BackupEntry resource The Gardener project features a sub-project called etcd-backup-restore to take periodic backups of etcd backing Shoot clusters. It demands the bucket (or its equivalent in different object store providers) access credentials to be created and configured externally with appropriate credentials. The BackupEntry resource takes this responsibility in Gardener to provide this information by creating a secret specific to the component. Said that, the core motivation for introducing this resource was to support retention of backups post deletion of Shoot. The etcd-backup-restore components takes responsibility of garbage collecting old backups out of the defined period. Once a shoot is deleted, we need to persist the backups for few days. Hence, Gardener uses the BackupEntry resource for this housekeeping work post deletion of a Shoot. The BackupEntry resource is responsible for shoot specific prefix under referred bucket.\nBefore introducing the BackupEntry extension resource Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see here). Now, Gardener commissions an external, provider-specific controller to take over this task. You can also refer to backupInfra proposal documentation to get idea about how the transition was done and understand the resource in broader scope.\nWhat is the lifespan of BackupEntry? The bucket associated with BackupEntry will be created at using BackupBucket resource. The BackupEntry resource will be created as a part of a Shoot creation. But resource continue to exists post deletion of a Shoot. The deletionGracePeriod of a BackupEntry resource i.e. time after the shoot deletion is configurable globally for Gardener via gardener-controller-manager component config. You can find the configuration option in reference.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:BackupEntrymetadata:name:shoot--foo--barspec:type:azureproviderConfig:\u0026lt;some-optional-provider-specific-backup-bucket-configuration\u0026gt; backupBucketProviderStatus:\u0026lt;some-optional-provider-specific-backup-bucket-status\u0026gt; region: eu-west-1bucketName:foosecretRef:name:backupprovidernamespace:shoot--foo--barThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. This provider secret will be propagated from BackupBucket resource by Shoot controller.\nYour controller is supposed to create the etcd-backup secret in control-plane namespace of a shoot. This secret is supposed to be used by Gardener or eventually the etcd-backup-restore component to backup the etcd. The controller implementation should cleanup the objects created under shoot specific prefix in bucket equivalent to name of BackupEntry resource.\nIn order to support a new infrastructure provider you need to write a controller that watches all BackupBuckets with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the Azure provider.\nReferences and additional resources  BackupEntry API Reference Exemplary implementation for the Azure provider BackupBucket resource documentation Shared bucket proposal Gardener-controller-manager-component-config API specification  "},{"uri":"https://gardener.cloud/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/concepts/extensions/cluster/","title":"Cluster resource","tags":[],"description":"","content":"Cluster resource As part of the extensibility epic a lot of responsibility that was previously taken over by Gardener directly has now been shifted to extension controllers running in the seed clusters. These extensions often serve a well-defined purpose, e.g. the management of DNS records, infrastructure, etc. We have introduced a couple of extension CRDs in the seeds whose specification is written by Gardener, and which are acted up by the extensions.\nHowever, the extensions sometimes require more information that is not directly part of the specification. One example of that is the GCP infrastructure controller which needs to know the shoot\u0026rsquo;s pod and service network. Another example is the Azure infrastructure controller which requires some information out of the CloudProfile resource. The problem is that Gardener does not know which extension requires which information so that it can write it into their specific CRDs.\nIn order to deal with this problem we have introduced the Cluster extension resource. This CRD is written into the seeds, however, it does not contain a status, so it is not expected that something acts upon it. Instead, you can treat it like a ConfigMap which contains data that might be interesting for you. In the context of Gardener, seeds and shoots, and extensibility the Cluster resource contains the CloudProfile, Seed, and Shoot manifest. Extension controllers can take whatever information they want out of it that might help completing their individual tasks.\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Clustermetadata:name:shoot--foo--barspec:cloudProfile:apiVersion:core.gardener.cloud/v1beta1kind:CloudProfile...seed:apiVersion:core.gardener.cloud/v1beta1kind:Seed...shoot:apiVersion:core.gardener.cloud/v1beta1kind:Shoot...The resource is written by Gardener before it starts the reconciliation flow of the shoot.\n:warning: All Gardener components use the core.gardener.cloud/v1beta1 version, i.e., the Cluster resource will contain the objects in this version.\nImportant information that should be taken into account There are some fields in the Shoot specification that might be interesting to take into account.\n .spec.hibernation.enabled={true,false}: Extension controllers might want to behave differently if the shoot is hibernated or not (probably they might want to scale down their control plane components, for example). .status.lastOperation.state=Failed: If Gardener sets the shoot\u0026rsquo;s last operation state to Failed it means that Gardener won\u0026rsquo;t automatically retry to finish the reconciliation/deletion flow because an error occurred that could not be resolved within the last 24h (default). In this case end-users are expected to manually re-trigger the reconciliation flow in case they want Gardener to try again. Extension controllers are expected to follow the same principle. This means they have to read the shoot state out of the Cluster resource.  References and additional resources  Cluster API (Golang specification)  "},{"uri":"https://gardener.cloud/community/","title":"Community","tags":[],"description":"","content":"Gardener Community Follow - Engage - Contribute\n@GardenerProject  Follow the latest project updates on Twitter  Community Meetings  You are welcome on our community meetings where you can engage with other contributors in person. See calendar for schedule or watch past recordings to get the idea.  GitHub  Eveyone is welcome to contribute with what they can - an issue or a pull request. Check Gardener project there and our contributors guide to help you get started.   Gardener Project  Watch videos and community meetings recordings on our YouTube channel  #gardener  Discuss Gardener on our Slack channel in the Kubernetes workspace   COMMUNITY The Gardener development process is an open process. Here are the general communication channels we use to communicate. We work with the wider community to create a strong, vibrant codebase. 60+ Committer  1300+ Merged Pull Requests  1400+ Github Stars  500+ Closed Community Issues   We are cordially inviting interested parties to join our weekly meetings. Here you can address questions regarding the direction of the project, technical problems and support.   Our Slack Channel is the best way to contact the experts in all questions about Kubernetes and the Gardener and share your ideas with them or ask for support.   Find out more about the project and consider making a contribution..     "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/shoot-health-status-conditions/","title":"Contributing to shoot health status conditions","tags":[],"description":"","content":"Contributing to shoot health status conditions Gardener checks regularly (every minute by default) the health status of all shoot clusters. It categorizes its checks into four different types:\n APIServerAvailable: This type indicates whether the shoot\u0026rsquo;s kube-apiserver is available or not. ControlPlaneHealthy: This type indicates whether all the control plane components deployed to the shoot\u0026rsquo;s namespace in the seed do exist and are running fine. EveryNodeReady: This type indicates whether all Nodes and all Machine objects report healthiness. SystemComponentsHealthy: This type indicates whether all system components deployed to the kube-system namespace in the shoot do exist and are running fine.  Every Shoot resource has a status.conditions[] list that contains the mentioned types, together with a status (True/False) and a descriptive message/explanation of the status.\nMost extension controllers are deploying components and resources as part of their reconciliation flows into the seed or shoot cluster. A prominent example for this is the ControlPlane controller that usually deploys a cloud-controller-manager or CSI controllers as part of the shoot control plane. Now that the extensions deploy resources into the cluster, especially resources that are essential for the functionality of the cluster, they might want to contribute to Gardener\u0026rsquo;s checks mentioned above.\nWhat can extensions do to contribute to Gardener\u0026rsquo;s health checks? Every extension resource in Gardener\u0026rsquo;s extensions.gardener.cloud/v1alpha1 API group also has a status.conditions[] list (like the Shoot). Extension controllers can write conditions to the resource they are acting on and use a type that also exist in the shoot\u0026rsquo;s conditions. One exception is that APIServerAvailable can\u0026rsquo;t be used as the Gardener clearly can identify the status of this condition and it doesn\u0026rsquo;t make sense for extensions to try to contribute/modify it.\nAs an example for the ControlPlane controller let\u0026rsquo;s take a look at the following resource:\napiVersion:extensions.gardener.cloud/v1alpha1kind:ControlPlanemetadata:name:control-planenamespace:shoot--foo--barspec:...status:conditions:- type:ControlPlaneHealthystatus:\u0026#34;False\u0026#34;reason:DeploymentUnhealthymessage: \u0026#39;Deployment cloud-controller-manager is unhealthy:condition\u0026#34;Available\u0026#34;hasinvalid status False (expected True) due to MinimumReplicasUnavailable:Deploymentdoesnothaveminimumavailability.\u0026#39;lastUpdateTime:\u0026#34;2014-05-25T12:44:27Z\u0026#34;- type:ConfigComputedSuccessfullystatus:\u0026#34;True\u0026#34;reason:ConfigCreatedmessage:Thecloud-provider-confighasbeensuccessfullycomputed.lastUpdateTime:\u0026#34;2014-05-25T12:43:27Z\u0026#34;The extension controller has declared in its extension resource that one of the deployments it is responsible for is unhealthy. Also, it has written a second condition using a type that is unknown by Gardener.\nGardener will pick the list of conditions and recognize that the there is one with a type ControlPlaneHealthy. It will merge it with its own ControlPlaneHealthy condition and report it back to the Shoot's status:\napiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:labels:shoot.garden.sapcloud.io/status:unhealthyshoot.garden.sapcloud.io/unhealthy:\u0026#34;true\u0026#34;name:some-shootnamespace:garden-corespec:status:conditions:- type:APIServerAvailablestatus:\u0026#34;True\u0026#34;reason:HealthzRequestSucceededmessage:APIserver/healthzendpointrespondedwithsuccessstatuscode.[response_time:31ms]lastUpdateTime:\u0026#34;2014-05-23T08:26:52Z\u0026#34;lastTransitionTime:\u0026#34;2014-05-25T12:45:13Z\u0026#34;- type:ControlPlaneHealthystatus:\u0026#34;False\u0026#34;reason:ControlPlaneUnhealthyReportmessage: \u0026#39;Deployment cloud-controller-manager is unhealthy:condition\u0026#34;Available\u0026#34;hasinvalid status False (expected True) due to MinimumReplicasUnavailable:Deploymentdoesnothaveminimumavailability.\u0026#39;lastUpdateTime:\u0026#34;2014-05-25T12:45:13Z\u0026#34;lastTransitionTime:\u0026#34;2014-05-25T12:45:13Z\u0026#34;...Hence, the only duty extensions have is to maintain the health status of their components in the extension resource they are managing. This can be accomplished using the health check library for extensions.\nError Codes The Gardener API includes some well-defined error codes, e.g., ERR_INFRA_UNAUTHORIZED, ERR_INFRA_DEPENDENCIES, etc. Extension may set these error codes in the .status.conditions[].codes[] list in case it makes sense. Gardener will pick them up and will similarly merge them into the .status.conditions[].codes[] list in the Shoot:\nstatus:conditions:- type:ControlPlaneHealthystatus:\u0026#34;False\u0026#34;reason:DeploymentUnhealthymessage: \u0026#39;Deployment cloud-controller-manager is unhealthy:condition\u0026#34;Available\u0026#34;hasinvalid status False (expected True) due to MinimumReplicasUnavailable:Deploymentdoesnothaveminimumavailability.\u0026#39;lastUpdateTime:\u0026#34;2014-05-25T12:44:27Z\u0026#34;codes:- ERR_INFRA_UNAUTHORIZED"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/controlplane-webhooks/","title":"Controlplane customization webhooks","tags":[],"description":"","content":"Controlplane customization webhooks Gardener creates the Shoot controlplane in several steps of the Shoot flow. At different point of this flow, it:\n deploys standard controlplane components such as kube-apiserver, kube-controller-manager, and kube-scheduler by creating the corresponding deployments, services, and other resources in the Shoot namespace. initiates the deployment of custom controlplane components by ControlPlane controllers by creating a ControlPlane resource in the Shoot namespace.  In order to apply any provider-specific changes to the configuration provided by Gardener for the standard controlplane components, cloud extension providers can install mutating admission webhooks for the resources created by Gardener in the Shoot namespace.\nWhat needs to be implemented to support a new cloud provider? In order to support a new cloud provider you should install \u0026ldquo;controlplane\u0026rdquo; mutating webhooks for any of the following resources:\n Deployment with name kube-apiserver, kube-controller-manager, or kube-scheduler StatefulSet with name etcd-main or etcd-events Service with name kube-apiserver OperatingSystemConfig with any name and purpose reconcile  See Contract Specification for more details on the contract that Gardener and webhooks should adhere to regarding the content of the above resources.\nYou can install 3 different kinds of controlplane webhooks:\n Shoot, or controlplane webhooks apply changes needed by the Shoot cloud provider, for example the --cloud-provider command line flag of kube-apiserver and kube-controller-manager. Such webhooks should only operate on Shoot namespaces labeled with shoot.gardener.cloud/provider=\u0026lt;provider\u0026gt;. Seed, or controlplaneexposure webhooks apply changes needed by the Seed cloud provider, for example annotations on the kube-apiserver service to ensure cloud-specific load balancers are correctly provisioned for a service of type LoadBalancer. Such webhooks should only operate on Shoot namespaces labeled with seed.gardener.cloud/provider=\u0026lt;provider\u0026gt;.  The labels shoot.gardener.cloud/provider and shoot.gardener.cloud/provider are added by Gardener when it creates the Shoot namespace.\nContract Specification This section specifies the contract that Gardener and webhooks should adhere to in order to ensure smooth interoperability. Note that this contract can\u0026rsquo;t be specified formally and is therefore easy to violate, especially by Gardener. The Gardener team will nevertheless do its best to adhere to this contract in the future and to ensure via additional measures (tests, validations) that it\u0026rsquo;s not unintentionally broken. If it needs to be changed intentionally, this can only happen after proper communication has taken place to ensure that the affected provider webhooks could be adapted to work with the new version of the contract.\nNote: The contract described below may not necessarily be what Gardener does currently (as of May 2019). Rather, it reflects the target state after changes for Gardener extensibility have been introduced.\nkube-apiserver To deploy kube-apiserver, Gardener shall create a deployment and a service both named kube-apiserver in the Shoot namespace. They can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of the kube-apiserver deployment shall contain a container named kube-apiserver.\nThe command field of the kube-apiserver container shall contain the kube-apiserver command line. It shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n admission plugins (--enable-admission-plugins, --disable-admission-plugins) secure communications (--etcd-cafile, --etcd-certfile, --etcd-keyfile, \u0026hellip;) audit log (--audit-log-*) ports (--insecure-port, --secure-port)  The kube-apiserver command line shall not contain any provider-specific flags, such as:\n --cloud-provider --cloud-config  These flags can be added by webhooks if needed.\nThe kube-apiserver command line may contain a number of additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-apiserver behavior for the specific provider. Among the flags to be considered are:\n --endpoint-reconciler-type --advertise-address --feature-gates  Gardener may use SNI to expose the apiserver (APIServerSNI feature gate). In this case, Gardener shall label the kube-apiserver's Deployment with core.gardener.cloud/apiserver-exposure: gardener-managed label and expects that the --endpoint-reconciler-type and --advertise-address flags are not modified.\nThe --enable-admission-plugins flag may contain admission plugins that are not compatible with CSI plugins such as PersistentVolumeLabel. Webhooks should therefore ensure that such admission plugins are either explicitly enabled (if CSI plugins are not used) or disabled (otherwise).\nThe env field of the kube-apiserver container shall not contain any provider-specific environment variables (so it will be empty). If any provider-specific environment variables are needed, they should be added by webhooks.\nThe volumes field of the pod template of the kube-apiserver deployment, and respectively the volumeMounts field of the kube-apiserver container shall not contain any provider-specific Secret or ConfigMap resources. If such resources should be mounted as volumes, this should be done by webhooks.\nThe kube-apiserver Service may be of type LoadBalancer, but shall not contain any provider-specific annotations that may be needed to actually provision a load balancer resource in the Seed provider\u0026rsquo;s cloud. If any such annotations are needed, they should be added by webhooks (typically controlplaneexposure webhooks).\nThe kube-apiserver Service shall be of type ClusterIP, if Gardener is using SNI to expose the apiserver (APIServerSNI feature gate). In this case, Gardener shall label this Service with core.gardener.cloud/apiserver-exposure: gardener-managed label and expects that no mutations happen.\nkube-controller-manager To deploy kube-controller-manager, Gardener shall create a deployment named kube-controller-manager in the Shoot namespace. It can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of the kube-controller-manager deployment shall contain a container named kube-controller-manager.\nThe command field of the kube-controller-manager container shall contain the kube-controller-manager command line. It shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n --kubeconfig, --authentication-kubeconfig, --authorization-kubeconfig --leader-elect secure communications (--tls-cert-file, --tls-private-key-file, \u0026hellip;) cluster CIDR and identity (--cluster-cidr, --cluster-name) sync settings (--concurrent-deployment-syncs, --concurrent-replicaset-syncs) horizontal pod autoscaler (--horizontal-pod-autoscaler-*) ports (--port, --secure-port)  The kube-controller-manager command line shall not contain any provider-specific flags, such as:\n --cloud-provider --cloud-config --configure-cloud-routes --external-cloud-volume-plugin  These flags can be added by webhooks if needed.\nThe kube-controller-manager command line may contain a number of additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-controller-manager behavior for the specific provider. Among the flags to be considered are:\n --feature-gates  The env field of the kube-controller-manager container shall not contain any provider-specific environment variables (so it will be empty). If any provider-specific environment variables are needed, they should be added by webhooks.\nThe volumes field of the pod template of the kube-controller-manager deployment, and respectively the volumeMounts field of the kube-controller-manager container shall not contain any provider-specific Secret or ConfigMap resources. If such resources should be mounted as volumes, this should be done by webhooks.\nkube-scheduler To deploy kube-scheduler, Gardener shall create a deployment named kube-scheduler in the Shoot namespace. It can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of the kube-scheduler deployment shall contain a container named kube-scheduler.\nThe command field of the kube-scheduler container shall contain the kube-scheduler command line. It shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n --config --authentication-kubeconfig, --authorization-kubeconfig secure communications (--tls-cert-file, --tls-private-key-file, \u0026hellip;) ports (--port, --secure-port)  The kube-scheduler command line may contain additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-controller-manager behavior for the specific provider. Among the flags to be considered are:\n --feature-gates  The kube-scheduler command line can\u0026rsquo;t contain provider-specific flags, and it makes no sense to specify provider-specific environment variables or mount provider-specific Secret or ConfigMap resources as volumes.\netcd-main and etcd-events To deploy etcd, Gardener shall create 2 StatefulSets named etcd-main and etcd-events in the Shoot namespace. They can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of these 2 deployments shall contain a container named etcd. It shall not contain a sidecar container for etcd backups. If such a container is needed, it should be added by webhooks, together with any volumes it may need to mount.\nThe command field of the etcd container shall contain the etcd command line. It shall contain only provider-independent flags that should be ignored by webhooks. It can\u0026rsquo;t contain provider-specific flags, and it makes no sense to specify provider-specific environment variables or mount provider-specific Secret or ConfigMap resources as volumes.\nThe volumeClaimTemplates section of these 2 StatefulSets shall contain a template named etcd-main or etcd-events. This template shall use the default storage class. The corresponding claim is mounted into the etcd container at /var/etcd/data. If it is desirable to use a non-default storage class, this should be done by webhooks.\ncloud-controller-manager Gardener shall not deploy a cloud-controller-manager. If it is needed, it should be added by a ControlPlane controller\nCSI controllers Gardener shall not deploy a CSI controller. If it is needed, it should be added by a ControlPlane controller\nkubelet To specify the kubelet configuration, Gardener shall create a OperatingSystemConfig resource with any name and purpose reconcile in the Shoot namespace. It can therefore also be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener. Gardener may write multiple such resources with different type to the same Shoot namespaces if multiple OSs are used.\nThe OSC resource shall contain a unit named kubelet.service, containing the corresponding systemd unit configuration file. The [Service] section of this file shall contain a single ExecStart option having the kubelet command line as its value.\nThe OSC resource shall contain a file with path /var/lib/kubelet/config/kubelet, which contains a KubeletConfiguration resource in YAML format. Most of the flags that can be specified in the kubelet command line can alternatively be specified as options in this configuration as well.\nThe kubelet command line shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n --config --bootstrap-kubeconfig, --kubeconfig --network-plugin (and, if it equals cni, also --cni-bin-dir and --cni-conf-dir) --node-labels  The kubelet command line shall not contain any provider-specific flags, such as:\n --cloud-provider --cloud-config --provider-id  These flags can be added by webhooks if needed.\nThe kubelet command line / configuration may contain a number of additional provider-independent flags / options. In general, webhooks should ignore these unless they are known to interfere with the desired kubelet behavior for the specific provider. Among the flags / options to be considered are:\n --enable-controller-attach-detach (enableControllerAttachDetach) - should be set to true if CSI plugins are used, but in general can also be ignored since its default value is also true, and this should work both with and without CSI plugins. --feature-gates (featureGates) - should contain a list of specific feature gates if CSI plugins are used. If CSI plugins are not used, the corresponding feature gates can be ignored since enabling them should not harm in any way.  "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/controlplane/","title":"ControlPlane resource","tags":[],"description":"","content":"Contract: ControlPlane resource Most Kubernetes clusters require a cloud-controller-manager or CSI drivers in order to work properly. Before introducing the ControlPlane extension resource Gardener was having several different Helm charts for the cloud-controller-manager deployments for the various providers. Now, Gardener commissions an external, provider-specific controller to take over this task.\nWhich control plane resources are required? As mentioned in the controlplane customization webhooks document Gardener shall not deploy any cloud-controller-manager or any other provider-specific component. Instead, it creates a ControlPlane CRD that should be picked up by provider extensions. Its purpose is to trigger the deployment of such provider-specific components in the shoot namespace in the seed cluster.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:ControlPlanemetadata:name:control-planenamespace:shoot--foo--barspec:type:openstackregion:europe-west1secretRef:name:cloudprovidernamespace:shoot--foo--barproviderConfig:apiVersion:openstack.provider.extensions.gardener.cloud/v1alpha1kind:ControlPlaneConfigloadBalancerProvider:providerzone:eu-1acloudControllerManager:featureGates:CustomResourceValidation:trueinfrastructureProviderStatus:apiVersion:openstack.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusnetworks:floatingPool:id:vpc-1234subnets:- purpose:nodesid:subnetidThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used for the shoot cluster. However, the most important section is the .spec.providerConfig and the .spec.infrastructureProviderStatus. The first one contains an embedded declaration of the provider specific configuration for the control plane (that cannot be known by Gardener itself). You are responsible for designing how this configuration looks like. Gardener does not evaluate it but just copies this part from what has been provided by the end-user in the Shoot resource. The second one contains the output of the Infrastructure resource (that might be relevant for the CCM config).\nIn order to support a new control plane provider you need to write a controller that watches all ControlPlanes with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the Alicloud provider.\nThe control plane controller as part of the ControlPlane reconciliation, often deploys resources (e.g. pods/deployments) into the Shoot namespace in the Seed as part of its ControlPlane reconciliation loop. Because the namespace contains network policies that per default deny all ingress and egress traffic, the pods may need to have proper labels matching to the selectors of the network policies in order to allow the required network traffic. Otherwise, they won\u0026rsquo;t be allowed to talk to certain other components (e.g., the kube-apiserver of the shoot). Please see this document for more information.\nNon-provider specific information required for infrastructure creation Most providers might require further information that is not provider specific but already part of the shoot resource. One example for this is the GCP control plane controller which needs the Kubernetes version of the shoot cluster (because it already uses the in-tree Kubernetes cloud-controller-manager). As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information that is not part of the Infrastructure resource itself.\nReferences and additional resources  ControlPlane API (Golang specification) Exemplary implementation for the Alicloud provider  "},{"uri":"https://gardener.cloud/components/dashboard/","title":"Dashboard","tags":[],"description":"","content":"Gardener Dashboard  \nDemo Development Setup Install Install all dependencies\nyarn Configuration KUBECONFIG If the dashboard is not running in the Garden Cluster you have to point the kubeconfig to Garden Cluster. This can be done in the default kubeconfig file in ${HOME}/.kube/config or by the KUBECONFIG environment variable.\nGARDENER_CONFIG The configuration file of the Gardener Dashboard can be specified as first command line argument or as environment variable GARDENER_CONFIG at the server process. If nothing is specified the default location is ${HOME}/.gardener/config.yaml.\nA local configuration example for minikube and dex could look like follows:\nport:3030logLevel:debuglogFormat:textapiServerUrl:https://minkube# garden cluster kube-apiserver urlsessionSecret:c2VjcmV0# symetric key used for encryptionoidc:issuer:https://minikube:32001client_id:dashboardclient_secret:c2VjcmV0# oauth client secretredirect_uri:http://localhost:8080/auth/callbackscope:\u0026#39;openid email profile groups audience:server:client_id:dashboard audience:server:client_id:kube-kubectl\u0026#39;clockTolerance:15frontend:dashboardUrl:pathname:/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy/defaultHibernationSchedule:evaluation:- start:0017**1,2,3,4,5development:- start:0017**1,2,3,4,5end:0008**1,2,3,4,5production:~Run locally (during development) Concurrently run the backend server (port 3030) and the frontend server (port 8080) both with hot reload enabled.\nyarn serve All request to /api, /auth and /config.json will be proxied by default to the backend server.\nBuild Build docker image locally.\nmake build Push Push docker image to Google Container Registry.\nmake push This command expects a valid gcloud configuration named gardener.\ngcloud config configurations describe gardener is_active: true name: gardener properties: core: account: john.doe@example.org project: johndoe-1008 People The following SAP developers contributed to this project until this initial contribution was published as open source.\n   contributor commits (%) +lines -lines first commit last commit     Holger Koser 313 (42%) 57878 18562 2017-07-13 2018-01-23   Andreas Herz 307 (41%) 13666 11099 2017-07-14 2017-10-27   Peter Sutter 99 (13%) 4838 3967 2017-11-07 2018-01-23   Gross, Lukas 31 (4%) 400 267 2018-01-10 2018-01-23    It is derived from the historical, internal gardener-ui repository at commit eeb623d60c86e6037c0e1dc2bdd9e54663bf41a8.\nLicense Apache License 2.0\nCopyright 2020 The Gardener Authors\n"},{"uri":"https://gardener.cloud/documentation/concepts/deployment/deploy_gardenlet_manually/","title":"Deploy a Gardenlet Manually","tags":[],"description":"","content":"Deploy a Gardenlet Manually Manually deploying a gardenlet is required in the following cases:\n  The Kubernetes cluster to be registered as a seed cluster has no public endpoint, because it is behind a firewall. The gardenlet must then be deployed into the cluster itself.\n  The Kubernetes cluster to be registered as a seed cluster is managed externally (the Kubernetes cluster is not a shoot cluster, so Automatic Deployment of Gardenlets cannot be used).\n  The gardenlet runs outside of the Kubernetes cluster that should be registered as a seed cluster. (The gardenlet is not restricted to run in the seed cluster or to be deployed into a Kubernetes cluster at all).\n   Once you‚Äôve deployed a gardenlet manually, for example, behind a firewall, you can deploy new gardenlets automatically. The manually deployed gardenlet is then used as a template for the new gardenlets. More information: Automatic Deployment of Gardenlets.\n Prerequisites Kubernetes cluster that should be registered as a seed cluster   Verify that the cluster has a supported Kubernetes version.\n  Determine the nodes, pods, and services CIDR of the cluster. You need to configure this information in the Seed configuration. Gardener uses this information to check that the shoot cluster isn‚Äôt created with overlapping CIDR ranges.\n  You need to have a predeployed Ingress controller. The ingress controller is preinstalled for shooted seed clusters. An ingress controller is required to:\n  Configure the Seed cluster resource (tells the gardenlet the ingress domain to use to deploy ingress resources for seed components like grafana and prometheus).\n  Handle ingress resources that are deployed during the Seed bootstrap by the gardenlet.\n   :warning:\nThere must be a DNS record *.ingress.\u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; where \u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; is the value of the .dns.ingressDomain field of a seed cluster resource (or configured using the seedConfig{} in the gardenlet configuration).\n   Example Nginx Ingress Controller Here is an example for the Nginx ingress controller:\n  Deploy Nginx into the kube-system namespace in the Kubernetes cluster that you want to be registered as a Seed.\nOn most cloud providers, Nginx creates the service with type LoadBalancer with an external IP.\nNAME TYPE CLUSTER-IP EXTERNAL-IP nginx-ingress-controller LoadBalancer 10.0.15.46 34.200.30.30   Create a wildcard A record (for example, *.ingress.sweet-seed.\u0026lt;my-domain\u0026gt;. IN A 34.200.30.30) with your DNS provider and point it to the external IP of the ingress service. This ingress domain is later required to register the Seed cluster.\n  kubeconfig for the Seed Cluster The kubeconfig is required to deploy the gardenlet Helm chart to the seed cluster. This deployment requires admin privileges. The Helm chart contains a service account gardenlet that the gardenlet deployment uses by default to talk to the Seed API server.\n If the gardenlet isn‚Äôt deployed in the seed cluster, the gardenlet can be configured to use a kubeconfig, which also requires full admin rights, from a mounted directory. The kubeconfig is specified in section seedClientConnection.kubeconfig of the Gardenlet configuration. This configuration option isn‚Äôt used in the following,\nas this procedure only describes the recommended setup option where the gardenlet is running in the seed cluster itself.\n Procedure Overview   Prepare the garden cluster:\n Create a bootstrap token secret in the kube-system namespace of the garden cluster Create RBAC roles for the gardenlet to allow bootstrapping in the garden cluster    Prepare the gardenlet Helm chart.\n  Automatically register shoot cluster as a seed cluster.\n  Deploy the gardenlet\n  Check that the gardenlet is successfully deployed\n  Create a bootstrap token secret in the kube-system namespace of the garden cluster The gardenlet needs to talk to the Gardener API server residing in the garden cluster.\nThe gardenlet can be configured with an already existing garden cluster kubeconfig in one of the following ways:\n  Either by specifying gardenClientConnection.kubeconfig in the Gardenlet configuration or\n  by supplying the environment variable GARDEN_KUBECONFIG pointing to a mounted kubeconfig file).\n  The preferred way however, is to use the gardenlets ability to request a signed certificate for the garden cluster by leveraging Kubernetes Certificate Signing Requests. The gardenlet performs a TLS bootstrapping process that is similar to the Kubelet TLS Bootstrapping. Make sure that the API server of the garden cluster has bootstrap token authentication enabled.\nThe client credentials required for the gardenlets TLS bootstrapping process, need to be either token or certificate (OIDC isn‚Äôt supported) and have permissions to create a Certificate Signing Request (CSR). It‚Äôs recommended to use bootstrap tokens due to their desirable security properties (such as a limited token lifetime).\nTherefore, first create a bootstrap token secret for the garden cluster:\napiVersion:v1kind:Secretmetadata:# Name MUST be of form \u0026#34;bootstrap-token-\u0026lt;token id\u0026gt;\u0026#34;name:bootstrap-token-07401bnamespace:kube-system# Type MUST be \u0026#39;bootstrap.kubernetes.io/token\u0026#39;type:bootstrap.kubernetes.io/tokenstringData:# Human readable description. Optional.description:\u0026#34;Token to be used by the gardenlet for Seed `sweet-seed`.\u0026#34;# Token ID and secret. Required.token-id:07401b# 6 characterstoken-secret:f395accd246ae52d# 16 characters# Expiration. Optional.# expiration: 2017-03-10T03:22:11Z# Allowed usages.usage-bootstrap-authentication:\u0026#34;true\u0026#34;usage-bootstrap-signing:\u0026#34;true\u0026#34;When you later prepare the gardenlet Helm chart, a kubeconfig based on this token is shared with the gardenlet upon deployment.\nCreate RBAC roles for the gardenlet to allow bootstrapping in the garden cluster This step is only required if the gardenlet you deploy is the first gardenlet in the Gardener installation. Additionally, when using the control plane chart, the following resources are already contained in the Helm chart, that is, if you use it you can skip these steps as the needed RBAC roles already exist.\nThe gardenlet uses the configured bootstrap kubeconfig in gardenClientConnection.bootstrapKubeconfig to request a signed certificate for the user gardener.cloud:system:seed:\u0026lt;seed-name\u0026gt; in the group gardener.cloud:system:seeds.\nCreate a ClusterRole and ClusterRoleBinding that grant full admin permissions to authenticated gardenlets.\nCreate the following resources in the garden cluster:\n---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:gardener.cloud:system:seedsrules:- apiGroups:- \u0026#39;*\u0026#39;resources:- \u0026#39;*\u0026#39;verbs:- \u0026#39;*\u0026#39;---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:gardener.cloud:system:seedsroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:gardener.cloud:system:seedssubjects:- kind:Groupname:gardener.cloud:system:seedsapiGroup:rbac.authorization.k8s.ioPrepare the gardenlet Helm chart This section only describes the minimal configuration, using the global configuration values of the gardenlet Helm chart. For an overview over all values, see the configuration values. We refer to the global configuration values as gardenlet configuration in the remaining procedure.\n  Create a gardenlet configuration gardenlet-values.yaml based on this template.\n  Create a bootstrap kubeconfig based on the bootstrap token created in the garden cluster.\nReplace the \u0026lt;bootstrap-token\u0026gt; with token-id.token-secret (from our previous example: 07401b.f395accd246ae52d) from the bootstrap token secret.\napiVersion:v1kind:Configcurrent-context:gardenlet-bootstrap@defaultclusters:- cluster:certificate-authority-data:\u0026lt;ca-of-garden-cluster\u0026gt; server: https://\u0026lt;endpoint-of-garden-cluster\u0026gt;name:defaultcontexts:- context:cluster:defaultuser:gardenlet-bootstrapname:gardenlet-bootstrap@defaultusers:- name:gardenlet-bootstrapuser:token:\u0026lt;bootstrap-token\u0026gt;  In section gardenClientConnection.bootstrapKubeconfig of your gardenlet configuration, provide the bootstrap kubeconfig together with a name and namespace to the gardenlet Helm chart.\ngardenClientConnection:bootstrapKubeconfig:name:gardenlet-kubeconfig-bootstrapnamespace:gardenkubeconfig:| \u0026lt;bootstrap-kubeconfig\u0026gt; # will be base64 encoded by helmThe bootstrap kubeconfig is stored in the specified secret.\n  In section gardenClientConnection.kubeconfigSecret of your gardenlet configuration, define a name and a namespace where the gardenlet stores the real kubeconfig that it creates during the bootstrap process. If the secret doesn\u0026rsquo;t exist, the gardenlet creates it for you.\ngardenClientConnection:kubeconfigSecret:name:gardenlet-kubeconfignamespace:garden  Automatically register shoot cluster as a seed cluster A seed cluster can either be registered by manually creating the Seed resource or automatically by the gardenlet.\nThis functionality is useful for shooted seed clusters, as the gardenlet in the garden cluster deploys a copy of itself into the cluster with automatic registration of the Seed configured.\nHowever, it can also be used to have a streamlined seed cluster registration process when manually deploying the gardenlet.\n This procedure doesn‚Äôt describe all the possible configurations for the Seed resource. For more information, see:\n Example Seed resource Configurable Seed settings.   Adjust the gardenlet component configuration   Supply the Seed resource in section seedConfig of your gardenlet configuration gardenlet-values.yaml.\n Note that with the seedConfig supplied, the gardenlet is only responsible to create and reconcile this one configured seed (in the previous example: sweet-seed). The gardenlet can also be configured to reconcile (but not create!) multiple Seeds based on a label selector, which is only recommended for a development setup.\n   Add the seedConfig to your gardenlet configuration gardenlet-values.yaml. The field seedConfig.spec.provider.type specifies the infrastructure provider type (for example, aws) of the seed cluster. For all supported infrastructure providers, see Known Extension Implementations.\n....seedConfig:metadata:name:sweet-seedspec:dns:ingressDomain:ingress.sweet-seed.\u0026lt;my-domain\u0026gt;# see prerequisitesnetworks:# see prerequisitesnodes:10.240.0.0/16pods:100.244.0.0/16services:100.32.0.0/13shootDefaults: # optional:non-overlappingdefaultCIDRsforshootclustersofthatSeedpods:100.96.0.0/11services:100.64.0.0/13provider:region:eu-west-1type:\u0026lt;provider\u0026gt;  Optional: Enable backup and restore The seed cluster can be set up with backup and restore for the main etcds of shoot clusters.\nGardener uses etcd-backup-restore that integrates with different storage providers to store the shoot cluster\u0026rsquo;s main etcd backups. Make sure to obtain client credentials that have sufficient permissions with the chosen storage provider.\nCreate a secret in the garden cluster with client credentials for the storage provider. The format of the secret is cloud provider specific and can be found in the repository of the respective Gardener extension. For example, the secret for AWS S3 can be found in the AWS provider extension (30-etcd-backup-secret.yaml).\napiVersion:v1kind:Secretmetadata:name:sweet-seed-backupnamespace:gardentype:Opaquedata:# client credentials format is provider specificConfigure the Seed resource in section seedConfig of your gardenlet configuration to use backup and restore:\n...seedConfig:metadata:name:sweet-seedspec:backup:provider:\u0026lt;provider\u0026gt; secretRef:name:sweet-seed-backupnamespace:gardenDeploy the gardenlet  The gardenlet doesn‚Äôt have to run in the same Kubernetes cluster as the seed cluster it‚Äôs registering and reconciling, but it is in most cases advantageous to use in-cluster communication to talk to the Seed API server. Running a gardenlet outside of the cluster is mostly used for local development.\n The gardenlet-values.yaml looks something like this (with automatic Seed registration and backup for shoot clusters enabled):\nglobal:# Gardenlet configuration valuesgardenlet:enabled:true...\u0026lt;defaultconfig\u0026gt; ...config:gardenClientConnection:...bootstrapKubeconfig:name:gardenlet-bootstrap-kubeconfignamespace:gardenkubeconfig:| apiVersion: v1clusters:- cluster:certificate-authority-data:\u0026lt;dummy\u0026gt; server: \u0026lt;my-garden-cluster-endpoint\u0026gt;name:my-kubernetes-cluster....kubeconfigSecret:name:gardenlet-kubeconfignamespace:garden...\u0026lt;defaultconfig\u0026gt; ...seedConfig:metadata:name:sweet-seedspec:dns:ingressDomain:ingress.sweet-seed.\u0026lt;my-domain\u0026gt; networks:nodes:10.240.0.0/16pods:100.244.0.0/16services:100.32.0.0/13shootDefaults:pods:100.96.0.0/11services:100.64.0.0/13provider:region:eu-west-1type:\u0026lt;provider\u0026gt; backup:provider:\u0026lt;provider\u0026gt; secretRef:name:sweet-seed-backupnamespace:gardenDeploy the gardenlet Helm chart to the Kubernetes cluster.\nhelm install gardenlet charts/gardener/gardenlet \\  --namespace garden \\  -f gardenlet-values.yaml \\  --wait This helm chart creates:\n A service account gardenlet that the gardenlet can use to talk to the Seed API server. RBAC roles for the service account (full admin rights at the moment). The secret (garden/gardenlet-bootstrap-kubeconfig) containing the bootstrap kubeconfig. The gardenlet deployment in the garden namespace.  Check that the gardenlet is successfully deployed   Check that the gardenlets certificate bootstrap was successful.\nCheck if the secret gardenlet-kubeconfig in the namespace garden in the seed cluster is created and contains a kubeconfig with a valid certificate.\n  Get the kubeconfig from the created secret.\n$ kubectl -n garden get secret gardenlet-kubeconfig -o json | jq -r .data.kubeconfig | base64 -d   Test against the garden cluster and verify it‚Äôs working.\n  Extract the client-certificate-data from the user gardenlet.\n  View the certificate:\n$ openssl x509 -in ./gardenlet-cert -noout -text Check that the certificate is valid for a year (that is the lifetime of new certificates).\n    Check that the bootstrap secret gardenlet-bootstrap-kubeconfig has been deleted from the seed cluster in namespace garden.\n  Check that the seed cluster is registered and READY in the garden cluster.\nCheck that the seed cluster sweet-seed exists and all conditions indicate that it‚Äôs available. If so, the Gardenlet is sending regular heartbeats and the seed bootstrapping was successful.\nCheck that the conditions on the Seed resource look similar to the following:\n$ kubectl get seed sweet-seed -o json | jq .status.conditions [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2020-07-17T09:17:29Z\u0026#34;, \u0026#34;lastUpdateTime\u0026#34;: \u0026#34;2020-07-17T09:17:29Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Gardenlet is posting ready status.\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;GardenletReady\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GardenletReady\u0026#34; }, { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2020-07-17T09:17:49Z\u0026#34;, \u0026#34;lastUpdateTime\u0026#34;: \u0026#34;2020-07-17T09:53:17Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Seed cluster has been bootstrapped successfully.\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;BootstrappingSucceeded\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Bootstrapped\u0026#34; } ]   Related Links Issue #1724: Harden Gardenlet RBAC privileges.\nBackup and Restore.\n"},{"uri":"https://gardener.cloud/documentation/concepts/deployment/deploy_gardenlet/","title":"Deploying Gardenlets","tags":[],"description":"","content":"Deploying Gardenlets Gardenlets act as decentral \u0026ldquo;agents\u0026rdquo; to manage shoot clusters of a seed cluster.\nTo support scaleability in an automated way, gardenlets are deployed automatically. However, you can still deploy gardenlets manually to be more flexible, for example, when shoot clusters that need to be managed by Gardener are behind a firewall. The gardenlet only requires network connectivity from the gardenlet to the Garden cluster (not the other way round), so it can be used to register Kubernetes clusters with no public endpoint.\nProcedure   First, an initial gardenlet needs to be deployed:\n Deploy it manually if you have special requirements. More information: Deploy a Gardenlet Manually Let the Gardener installer deploy it automatically otherwise. More information: Automatic Deployment of Gardenlets    To add additional seed clusters, it is recommended to use regular shoot clusters. The gardenlet automatically installs itself into these so-called shooted seed clusters.\n  "},{"uri":"https://gardener.cloud/documentation/concepts/deployment/setup_gardener/","title":"Deploying the Gardener into a Kubernetes cluster","tags":[],"description":"","content":"Deploying the Gardener into a Kubernetes cluster Similar to Kubernetes, Gardener consists out of control plane components (Gardener API server, Gardener controller manager, Gardener scheduler), and an agent component (Gardenlet). The control plane is deployed in the so-called garden cluster while the agent is installed into every seed cluster. Please note that it is possible to use the garden cluster as seed cluster by simply deploying the Gardenlet into it.\nWe are providing Helm charts in order to manage the various resources of the components. Please always make sure that you use the Helm chart version that matches the Gardener version you want to deploy.\nDeploying the Gardener control plane (API server, admission controller, controller manager, scheduler) The configuration values depict the various options to configure the different components. Please consult this document to get a detailed explanation of what can be configured for which component.\nAlso note that all resources and deployments need to be created in the garden namespace (not overrideable). If you enable the Gardener admission controller as part of you setup, please make sure the garden namespace is labelled with app: gardener. Otherwise, the backing service account for the admission controller Pod might not be created successfully. No action is necessary, if you deploy the garden namespace with the Gardener control plane Helm chart.\nAfter preparing your values in a separate controlplane-values.yaml file, you can run the following command against your garden cluster:\nhelm install charts/gardener/controlplane \\  --namespace garden \\  --name gardener-controlplane \\  -f gardener-values.yaml \\  --wait Deploying Gardener extensions Gardener is an extensible system that does not contain the logic for provider-specific things like DNS management, cloud infrastructures, network plugins, operating system configs, and many more.\nYou have to install extension controllers for these parts. Please consult the documentation regarding extensions to get more information.\nDeploying the Gardener agent (Gardenlet) Please refer to this document on how to deploy a Gardenlet.\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/dns/","title":"DNSProvider and DNSEntry resources","tags":[],"description":"","content":"Contract: DNSProvider and DNSEntry resources Every shoot cluster requires external DNS records that are publicly resolvable. The management of these DNS records requires provider-specific knowledge which is to be developed outside of the Gardener\u0026rsquo;s core repository.\nWhat does Gardener create DNS records for? Internal domain name Every shoot cluster\u0026rsquo;s kube-apiserver running in the seed is exposed via a load balancer that has a public endpoint (IP or hostname). This endpoint is used by end-users and also by system components (that are running in another network, e.g., the kubelet or kube-proxy) to talk to the cluster. In order to be robust against changes of this endpoint (e.g., caused due to re-creation of the load balancer or move of the control plane to another seed cluster) Gardener creates a so-called internal domain name for every shoot cluster. The internal domain name is a publicly resolvable DNS record that points to the load balancer of the kube-apiserver. Gardener uses this domain name in the kubeconfigs of all system components (instead of writing the load balancer endpoint directly into it. This way Gardener does not need to recreate all the kubeconfigs if the endpoint changes - it just needs to update the DNS record.\nExternal domain name The internal domain name is not configurable by end-users directly but dictated by the Gardener administrator. However, end-users usually prefer to have another DNS name, maybe even using their own domain sometimes to access their Kubernetes clusters. Gardener supports that by creating another DNS record, named external domain name, that actually points to the internal domain name. The kubeconfig handed out to end-users does contain this external domain name, i.e., users can access their clusters with the DNS name they like to.\nAs not every end-user has an own domain it is possible for Gardener administrators to configure so-called default domains. If configured, shoots that do not specify a domain explicitly get an external domain name based on a default domain (unless explicitly stated that this shoot should not get an external domain name (.spec.dns.provider=unmanaged).\nDomain name for ingress (deprecated) Gardener allows to deploy a nginx-ingress-controller into a shoot cluster (deprecated). This controller is exposed via a public load balancer (again, either IP or hostname). Gardener creates a wildcard DNS record pointing to this load balancer. Ingress resources can later use this wildcard DNS record to expose underlying applications.\nWhat needs to be implemented to support a new DNS provider? As part of the shoot flow Gardener will create two special resources in the seed cluster that need to be reconciled by an extension controller. The first resource (DNSProvider) is a declaration of a DNS provider (e.g., aws-route53, google-clouddns, \u0026hellip;) with a reference to a Secret object that contains the provider-specific credentials in order to talk to the provider\u0026rsquo;s API. It also allows to specify two lists of domains that shall be allowed or disallowed to be used for DNS entries:\n---apiVersion:v1kind:Secretmetadata:name:aws-credentialsnamespace:defaulttype:Opaquedata:# aws-route53 specific credentials here---apiVersion:dns.gardener.cloud/v1alpha1kind:DNSProvidermetadata:name:my-aws-accountnamespace:defaultspec:type:aws-route53secretRef:name:aws-credentialsdomains:include:- dev.my-fancy-domain.comexclude:- staging.my-fancy-domain.com- prod.my-fancy-domain.comWhen reconciling this resource the DNS controller has to read information about available DNS zones to figure out which domains can actually be supported by the provided credentials. Based on the constraints given in the DNSProvider resources .spec.domains.{include|exclude} fields it shall later only allow certain DNS entries. Gardener waits until the status indicates that the registration went well:\napiVersion:dns.gardener.cloud/v1alpha1kind:DNSProvider...status:state:Readymessage:everythingokOther possible states are Pending, Error, and Invalid. The DNS controller may provide an explanation of the .status.state in the .status.message field.\nNow Gardener may create DNSEntry objects that represent the ask to create an actual external DNS record:\n---apiVersion:dns.gardener.cloud/v1alpha1kind:DNSEntrymetadata:name:dnsnamespace:defaultspec:dnsName:apiserver.cluster1.dev.my-fancy-domain.comttl:600targets:- 8.8.8.8It has to be automatically determined whether the to-be-created DNS record is of type A or CNAME. The spec shall also allow the creation of TXT records, e.g.:\n---apiVersion:dns.gardener.cloud/v1alpha1kind:DNSEntrymetadata:name:dnsnamespace:defaultspec:dnsName:data.apiserver.cluster1.dev.my-fancy-domain.comttl:120text:| content for the DNS TXT recordThe status section of this resource looks similar like the DNSProvider's. Gardener is (as of today) only evaluating the .status.state and .status.message fields.\nReferences and additional resources  DNSProvider and DNSEntry API (Golang specification) external-dns-management project in Gardener\u0026rsquo;s GitHub organization  "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/project-roles/","title":"Extending project roles","tags":[],"description":"","content":"Extending project roles The Project resource allows to specify a list of roles for every member (.spec.members[*].roles). There are a few standard roles defined by Gardener itself. Please consult this document for further information.\nHowever, extension controllers running in the garden cluster may also create CustomResourceDefinitions that project members might be able to CRUD. For this purpose Gardener also allows to specify extension roles.\nAn extension role is prefixed with extension:, e.g.\napiVersion:core.gardener.cloud/v1beta1kind:Projectmetadata:name:devspec:members:- apiGroup:rbac.authorization.k8s.iokind:Username:alice.doe@example.comrole:adminroles:- owner- extension:fooThe project controller will, for every extension role, create a ClusterRole with name name: gardener.cloud:extension:project:\u0026lt;projectName\u0026gt;:\u0026lt;roleName\u0026gt;, i.e., for above example: name: gardener.cloud:extension:project:dev:foo. This ClusterRole aggregates other ClusterRoles that are labeled with rbac.gardener.cloud/aggregate-to-extension-role=foo which might be created by extension controllers.\nExtension that might want to contribute to the core admin or viewer roles can use the labels rbac.gardener.cloud/aggregate-to-project-member=true or rbac.gardener.cloud/aggregate-to-project-viewer=true, respectively.\nPlease note that the names of the extension roles are restricted to 20 characters!\nMoreover, the project controller will also create a corresponding RoleBinding with the same name in the project namespace. It will automatically assign all members that are assigned to this extension role.\n"},{"uri":"https://gardener.cloud/documentation/concepts/monitoring/extending/","title":"Extending the Monitoring Stack","tags":[],"description":"","content":"Extending the Monitoring Stack This document provides instructions to extend the Shoot cluster monitoring stack by integrating new scrape targets, alerts and dashboards.\nPlease ensure that you have understood the basic principles of Prometheus and its ecosystem before you continue.\n:bangbang: The purpose of the monitoring stack is to observe the behaviour of the control plane and the system components deployed by Gardener onto the worker nodes. Monitoring of custom workloads running in the cluster is out of scope.\nOverview Each Shoot cluster comes with its own monitoring stack. The following components are deployed into the seed and shoot:\n Seed  Prometheus Grafana blackbox-exporter kube-state-metrics (Seed metrics) kube-state-metrics (Shoot metrics) Alertmanager (Optional)   Shoot  node-exporter(s) kube-state-metrics blackbox-exporter    In each Seed cluster there is a Prometheus in the garden namespace responsible for collecting metrics from the Seed kubelets and cAdvisors. These metrics are provided to each Shoot Prometheus via federation.\nThe alerts for all Shoot clusters hosted on a Seed are routed to a central Alertmanger running in the garden namespace of the Seed. The purpose of this central alertmanager is to forward all important alerts to the operators of the Gardener setup.\nThe Alertmanager in the Shoot namespace on the Seed is only responsible for forwarding alerts from its Shoot cluster to a cluster owner/cluster alert receiver via email. The Alertmanager is optional and the conditions for a deployment are already described here.\nAdding New Monitoring Targets After exploring the metrics which your component provides or adding new metrics, you should be aware which metrics are required to write the needed alerts and dashboards.\nPrometheus prefers a pull based metrics collection approach and therefore the targets to observe need to be defined upfront. The targets are defined in charts/seed-monitoring/charts/prometheus/templates/config.yaml. New scrape jobs can be added in the section scrape_configs. Detailed information how to configure scrape jobs and how to use the kubernetes service discovery are available in the Prometheus documentation.\nThe job_name of a scrape job should be the name of the component e.g. kube-apiserver or vpn. The collection interval should be the default of 30s. You do not need to specify this in the configuration.\nPlease do not ingest all metrics which are provided by a component. Rather collect only those metrics which are needed to define the alerts and dashboards (i.e. whitelist). This can be achieved by adding the following metric_relabel_configs statement to your scrape jobs (replace exampleComponent with component name).\n- job_name:example-component...metric_relabel_configs:{{include\u0026#34;prometheus.keep-metrics.metric-relabel-config\u0026#34;.Values.allowedMetrics.exampleComponent|indent6}}The whitelist for the metrics of your job can be maintained in charts/seed-monitoring/charts/prometheus/values.yaml in section allowedMetrics.exampleComponent (replace exampleComponent with component name). Check the following example:\nallowedMetrics:...exampleComponent:*metrics_name_1*metrics_name_2...Adding Alerts The alert definitons are located in charts/seed-monitoring/charts/prometheus/rules. There are two approaches for adding new alerts.\n Adding additional alerts for a component which already has a set of alerts. In this case you have to extend the existing rule file for the component. Adding alerts for a new component. In this case a new rule file with name scheme example-component.rules.yaml needs to be added. Add the new alert to alertInhibitionGraph.dot, add any required inhibition flows and render the new graph. To render the graph run:  dot -Tpng ./content/alertInhibitionGraph.dot -o ./content/alertInhibitionGraph.png  Create a test for the new alert. See Alert Tests.  Example alert:\ngroups:* name:example.rulesrules:* alert:ExampleAlertexpr:absent(up{job=\u0026#34;exampleJob\u0026#34;}==1)for:20mlabels:service:exampleseverity:critical# How severe is the alert? (blocker|critical|info|warning)type:shoot# For which topology is the alert relevant? (seed|shoot)visibility:all# Who should receive the alerts? (all|operator|owner)annotations:description:Alongerdescriptionoftheexamplealertthatshouldalsoexplaintheimpactofthealert.summary:Shortsummaryofanexamplealert.If the deployment of component is optional then the alert definitions needs to be added to charts/seed-monitoring/charts/prometheus/optional-rules instead. Furthermore the alerts for component need to be activatable in charts/seed-monitoring/charts/prometheus/values.yaml via rules.optional.example-component.enabled. The default should be true.\nBasic instruction how to define alert rules can be found in the Prometheus documentation.\nRouting tree The Alertmanager is grouping incoming alerts based on labels into buckets. Each bucket has its own configuration like alert receivers, initial delaying duration or resending frequency etc. You can find more information about Alertmanager routing in the Prometheus/Alertmanager documentation. The routing trees for the Alertmanagers deployed by Gardener are depicted below.\nCentral Seed Alertmanager\n‚àü main route (all alerts for all shoots on the seed will enter) ‚àü group by project and shoot name ‚àü group by visibility \u0026#34;all\u0026#34; and \u0026#34;operator\u0026#34; ‚àü group by severity \u0026#34;blocker\u0026#34;, \u0026#34;critical\u0026#34;, and \u0026#34;info\u0026#34; ‚Üí route to Garden operators ‚àü group by severity \u0026#34;warning\u0026#34; (dropped) ‚àü group by visibility \u0026#34;owner\u0026#34; (dropped) Shoot Alertmanager\n‚àü main route (only alerts for one Shoot will enter) ‚àü group by visibility \u0026#34;all\u0026#34; and \u0026#34;owner\u0026#34; ‚àü group by severity \u0026#34;blocker\u0026#34;, \u0026#34;critical\u0026#34;, and \u0026#34;info\u0026#34; ‚Üí route to cluster alert receiver ‚àü group by severity \u0026#34;warning\u0026#34; (dropped, will change soon ‚Üí route to cluster alert receiver) ‚àü group by visibility \u0026#34;operator\u0026#34; (dropped) Alert Inhibition All alerts related to components running on the Shoot workers are inhibited in case of an issue with the vpn connection, because those components can\u0026rsquo;t be scraped anymore and Prometheus will fire alerts in consequence. The components running on the workers are probably healthy and the alerts are presumably false positives. The inhibition flow is shown in the figure below. If you add a new alert make sure to add it to the diagram.\nAlert Attributes Each alert rule definition has to contain the following annotations:\n summary: A short description of the issue. description: A detailed explanation of the issue with hints to the possible root causes and the impact assessment of the issue.  In addtion each alert must contain the following labels:\n type  shoot: Components running on the Shoot worker nodes in the kube-system namespace. seed: Components running on the Seed in the Shoot namespace as part of/next to the control plane.   service  Name of the component (in lowercase) e.g. kube-apiserver, alertmanager or vpn.   severity  blocker: All issues which make the cluster entirely unusable e.g. KubeAPIServerDown or KubeSchedulerDown critical: All issues which affect single functionalities/components but not affect the cluster in its core functionality e.g. VPNDown or KubeletDown. info: All issues that do not affect the cluster or its core functionality, but if this component is down we cannot determine if a blocker alert is firing. (i.e. A component with an info level severity is a dependency for a component with a blocker severity) warning: No current existing issue, rather a hint for situations which could lead to real issue in the close future e.g. HighLatencyApiServerToWorkers or ApiServerResponseSlow.    Alert Tests Execute the tests in $GARDENERHOME/.ci/test or if you want to only test the Prometheus alerts:\n# Install promtool go get -u github.com/prometheus/prometheus/cmd/promtool # Move to seed-monitoring/prometheus chart cd $GARDENERHOME/charts/seed-monitoring/charts/prometheus/ # Execute tests promtool test rules rules-tests/*test.yaml If you want to add alert tests:\n  Create a new file in rules-tests in the form \u0026lt;alert-group-name\u0026gt;.rules.test.yaml or if the alerts are for an existing component with existing tests, simply add the tests to the appropriate files.\n  Make sure that newly added tests succeed. See above.\n  Adding Grafana Dashboards The dashboard definition files are located in charts/seed-monitoring/charts/grafana/dashboards. Every dashboard needs its own file.\nIf you are adding a new component dashboard please also update the overview dashboard by adding a chart for its current up/down status and with a drill down option to the component dashboard.\nDashboard Structure The dashboards should be structured in the following way. The assignment of the component dashboards to the categories should be handled via dashboard tags.\n Kubernetes control plane components (Tag: control-plane)  All components which are part of the Kubernetes control plane e. g. Kube API Server, Kube Controller Manager, Kube Scheduler and Cloud Controller Manager ETCD + Backup/Restore Kubernetes Addon Manager   Node/Machine components (Tag: node/machine)  All metrics which are related to the behaviour/control of the Kubernetes nodes and kubelets Machine-Controller-Manager + Cluster Autoscaler   Networking components (Tag: network)  CoreDNS, KubeProxy, Calico, VPN, Nginx Ingress   Addon components (Tag: addon)  Cert Broker   Monitoring components (Tag: monitoring) Logging components (Tag: logging)  Mandatory Charts for Component Dashboards For each new component, its corresponding dashboard should contain the following charts in the first row, before adding custom charts for the component in the subsequent rows.\n Pod up/down status up{job=\u0026quot;example-component\u0026quot;} Pod/containers cpu utilization Pod/containers memorty consumption Pod/containers network i/o  These information is provided by the cAdvisor metrics. These metrics are already integrated. Please check the other dashboards for detailed information on how to query.\nChart Requirements Each chart needs to contain:\n a meaningful name a detailed description (for non trivial charts) appropriate x/y axis descriptions appropriate scaling levels for the x/y axis proper units for the x/y axis  Dashboard Parameters The following parameters should be added to all dashboards to ensure a homogeneous experience across all dashboards.\nDashboards have to \u0026hellip;\n contain a title which refers to the component name(s) contain a timezone statement which should be the browser time contain tags which express where the component is running (seed or shoot) and to which category the component belong (see dashboard structure) contain a version statement with a value of 1 be immutable  Example dashboard configuration\n{ \u0026#34;title\u0026#34;: \u0026#34;example-component\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;browser\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;seed\u0026#34;, \u0026#34;control-plane\u0026#34; ], \u0026#34;version\u0026#34;: 1, \u0026#34;editable\u0026#34;: \u0026#34;false\u0026#34;, } Furthermore all dashboards should contain the following time options:\n{ \u0026#34;time\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;now-1h\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;now\u0026#34; }, \u0026#34;timepicker\u0026#34;: { \u0026#34;refresh_intervals\u0026#34;: [ \u0026#34;30s\u0026#34;, \u0026#34;1m\u0026#34;, \u0026#34;5m\u0026#34; ], \u0026#34;time_options\u0026#34;: [ \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;6h\u0026#34;, \u0026#34;12h\u0026#34;, \u0026#34;24h\u0026#34;, \u0026#34;2d\u0026#34;, \u0026#34;10d\u0026#34; ] }, } "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/extension/","title":"Extension resource","tags":[],"description":"","content":"Contract: Extension resource Gardener defines common procedures which must be passed to create a functioning shoot cluster. Well known steps are represented by special resources like Infrastructure, OperatingSystemConfig or DNS. These resources are typically reconciled by dedicated controllers setting up the infrastructure on the hyperscaler or managing DNS entries, etc..\nBut, some requirements don\u0026rsquo;t match with those special resources or don\u0026rsquo;t depend on being proceeded at a specific step in the creation / deletion flow of the shoot. They require a more generic hook. Therefore, Gardener offers the Extension resource.\nWhat is required to register and support an Extension type? Gardener creates one Extension resource per registered extension type in ControllerRegistration per shoot.\napiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationmetadata:name:extension-examplespec:resources:- kind:Extensiontype:examplegloballyEnabled:trueIf spec.resources[].globallyEnabled is true then the Extension resources of the given type is created for every shoot cluster. Set to false, the Extension resource is only created if configured in the Shoot manifest.\nThe Extension resources are created in the shoot namespace of the seed cluster.\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Extensionmetadata:name:examplenamespace:shoot--foo--barspec:type:exampleproviderConfig:{}Your controller needs to reconcile extensions.extensions.gardener.cloud. Since there can exist multiple Extension resources per shoot, each one holds a spec.type field to let controllers check their responsibility (similar to all other extension resources of Gardener).\nProviderConfig It is possible to provide data in the Shoot resource which is copied to spec.providerConfig of the Extension resource.\n---apiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:name:barnamespace:garden-foospec:extensions:- type:exampleproviderConfig:foo:bar...results in\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Extensionmetadata:name:examplenamespace:shoot--foo--barspec:type:exampleproviderConfig:foo:barShoot reconciliation flow and Extension status Gardener creates Extension resources as part of the Shoot reconciliation. Moreover, it is guaranteed that the Cluster resource exists before the Extension resource is created.\nFor an Extension controller it is crucial to maintain the Extension's status correctly. At the end Gardener checks the status of each Extension and only reports a successful shoot reconciliation if the state of the last operation is Succeeded.\napiVersion:extensions.gardener.cloud/v1alpha1kind:Extensionmetadata:generation:1name:examplenamespace:shoot--foo--barspec:type:examplestatus:lastOperation:state:SucceededobservedGeneration:1"},{"uri":"https://gardener.cloud/documentation/concepts/deployment/feature_gates/","title":"Feature Gates","tags":[],"description":"","content":"Feature Gates in Gardener This page contains an overview of the various feature gates an administrator can specify on different Gardener components.\nOverview Feature gates are a set of key=value pairs that describe Gardener features. You can turn these features on or off using the a component configuration file for a specific component.\nEach Gardener component lets you enable or disable a set of feature gates that are relevant to that component. For example this is the configuration of the gardenlet component.\nThe following tables are a summary of the feature gates that you can set on different Gardener components.\n The ‚ÄúSince‚Äù column contains the Gardener release when a feature is introduced or its release stage is changed. The ‚ÄúUntil‚Äù column, if not empty, contains the last Gardener release in which you can still use a feature gate. If a feature is in the Alpha or Beta state, you can find the feature listed in the Alpha/Beta feature gate table. If a feature is stable you can find all stages for that feature listed in the Graduated/Deprecated feature gate table. The Graduated/Deprecated feature gate table also lists deprecated and withdrawn features.  Feature gates for Alpha or Beta features    Feature Default Stage Since Until     Logging false Alpha 0.13    HVPA false Alpha 0.31    HVPAForShootedSeed false Alpha 0.32    ManagedIstio false Alpha 1.5    APIServerSNI false Alpha 1.7    MountHostCADirectories false Alpha 1.11.0    SeedChange false Alpha 1.12.0     Using a feature A feature can be in Alpha, Beta or GA stage. An Alpha feature means:\n Disabled by default. Might be buggy. Enabling the feature may expose bugs. Support for feature may be dropped at any time without notice. The API may change in incompatible ways in a later software release without notice. Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support.  A Beta feature means:\n Enabled by default. The feature is well tested. Enabling the feature is considered safe. Support for the overall feature will not be dropped, though details may change. The schema and/or semantics of objects may change in incompatible ways in a subsequent beta or stable release. When this happens, we will provide instructions for migrating to the next version. This may require deleting, editing, and re-creating API objects. The editing process may require some thought. This may require downtime for applications that rely on the feature. Recommended for only non-critical uses because of potential for incompatible changes in subsequent releases.   Please do try Beta features and give feedback on them! After they exit beta, it may not be practical for us to make more changes.\n A General Availability (GA) feature is also referred to as a stable feature. It means:\n The feature is always enabled; you cannot disable it. The corresponding feature gate is no longer needed. Stable versions of features will appear in released software for many subsequent versions.  List of feature gates  Logging enables logging stack for Seed clusters. HVPA enables simultaneous horizontal and vertical scaling in Seed Clusters. HVPAForShootedSeed enables simultaneous horizontal and vertical scaling in shooted Seed clusters. ManagedIstio enables a Gardener-tailored Istio in each Seed cluster. Disable this feature if Istio is already installed in the cluster. Istio is not automatically removed if this feature is disabled. See the detailed documentation for more information. APIServerSNI enables only one LoadBalancer to be used for every Shoot cluster API server in a Seed. Enable this feature when ManagedIstio is enabled or Istio is manually deployed in Seed cluster. See GEP-8 for more details. MountHostCADirectories enables mounting common CA certificate directories in the Shoot API server pod that might be required for webhooks or OIDC. SeedChange enables updating the spec.seedName field during shoot validation from a non-empty value in order to trigger shoot control plane migration.  "},{"uri":"https://gardener.cloud/components/gardenctl/","title":"gardenctl","tags":[],"description":"","content":"Gardenctl  \nWhat is gardenctl? gardenctl is a command-line client for administrative purposes for the Gardener. It facilitates the administration of one or many garden, seed and shoot clusters, e.g. to check for issues which occured in one of these clusters. Details about the concept behind the Gardener are described in the Gardener wiki.\nInstallation gardenctl is shipped for mac and linux in a binary format.\nOption 1: Install the latest release with Homebrew (macOS and Linux) as follows:\nbrew install gardener/tap/gardenctl Option 2: Manually download and install from gardenctl releases as follows:\n Download the latest release:  curl -LO https://github.com/gardener/gardenctl/releases/download/$(curl -s https://raw.githubusercontent.com/gardener/gardenctl/master/LATEST)/gardenctl-darwin-amd64 To download a specific version, replace the $(curl -s https://raw.githubusercontent.com/gardener/gardenctl/master/LATEST) portion of the command with the specific version.\nFor example, to download version 0.16.0 on macOS, type:\ncurl -LO https://github.com/gardener/gardenctl/releases/download/v0.16.0/gardenctl-darwin-amd64 Make the gardenctl binary executable.  chmod +x ./gardenctl-darwin-amd64 Move the binary in to your PATH.  sudo mv ./gardenctl-darwin-amd64 /usr/local/bin/gardenctl How to build it If no binary builds are available for your platform or architecture, you can build it from source, go get it or build the docker image from Dockerfile. Please keep in mind to use an up to date version of golang.\nPrerequisites To build gardenctl from sources you need to have a running Golang environment. Moreover, since gardenctl allows to execute kubectl as well as a running kubectl installation is recommended, but not required. Please check this description for further details.\nBuild gardenctl From source First, you need to clone the repository and build gardenctl.\ngit clone https://github.com/gardener/gardenctl.git cd gardenctl make build After successfully building gardenctl the executables are in the directory ~/go/src/github.com/gardener/gardenctl/bin/. Next, move the executable for your architecture to /usr/local/bin. In this case for darwin-amd64.\nsudo mv bin/darwin-amd64/gardenctl-darwin-amd64 /usr/local/bin/gardenctl gardenctl supports auto completion. This recommended feature is bound to gardenctl or the alias g. To configure it you can run:\nif you are using bash:\necho \u0026#34;source \u0026lt;(gardenctl completion bash)\u0026#34; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc if you are using zsh:\necho \u0026#34;source \u0026lt;(gardenctl completion zsh)\u0026#34; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc Via Dockerfile First clone the repository as described in the the build step \u0026ldquo;From source\u0026rdquo;. As next step add the garden \u0026ldquo;config\u0026rdquo; file and \u0026ldquo;clusters\u0026rdquo; folder with the corresponding kubeconfig files for the garden cluster. Then build the container image via docker build -t gardener/gardenctl:v1 . in the cloned repository and run a shell in the image with docker run -it gardener/gardenctl:v1 /bin/bash.\nConfigure gardenctl gardenctl requires a configuration file. The default location is in ~/.garden/config, but it can be overwritten with the environment variable GARDENCONFIG.\nHere an example file:\nemail:john.doe@example.comgithubURL:https://github.location.company.corpgardenClusters:- name:devkubeConfig:~/clusters/dev/kubeconfig.yamldashboardUrl:https://url_to_dashboardaccessRestrictions:- key:seed.gardener.cloud/eu-accessnotifyIf:truemsg:warningmsgoptions:- key:support.gardener.cloud/eu-access-for-cluster-addonsnotifyIf:truemsg:warningmsg- key:support.gardener.cloud/eu-access-for-cluster-nodesnotifyIf:truemsg:warningmsg- name:prodkubeConfig:~/clusters/prod/kubeconfig.yamlThe path to the kubeconfig files of a garden cluster can be relative by using the ~ (tilde) expansion or absolute.\ngardenctl caches some information, e.g. the garden project names. The location of this cache is per default $GARDENCTL_HOME/cache. If GARDENCTL_HOME is not set, ~/.garden is assumed.\ngardenctl supports multiple sessions. The session ID can be set via $GARDEN_SESSION_ID and the sessions are stored under $GARDENCTL_HOME/sessions.\ngardenctl makes it easy to get additional information of your IaaS provider by using the secrets stored in the corresponding projects in the Gardener. To use this functionality, the CLIs of the IaaS providers need to be available.\nPlease check the IaaS provider documentation for more details about their CLIs.\n aliyun aws az gcloud openstack  Moreover, gardenctl offers auto completion. To use it, the command\ngardenctl completion bash print on the standard output a completion script which can be sourced via\nsource \u0026lt;(gardenctl completion bash) Please keep in mind that the auto completion is bound to gardenctl or the alias g.\nUse gardenctl gardenctl requires the definition of a target, e.g. garden, project, seed or shoot. The following commands, e.g. gardenctl ls shoots uses the target definition as a context for getting the information.\nTargets represent a hierarchical structure of resources. On top, there is/are the garden/s. E.g. in case you setup a development and a production garden, you would have two entries in your ~/.garden/config. Via gardenctl ls gardens you get a list of the available gardens.\n gardenctl get target\nDisplays the current target gardenctl target [garden|project|seed|shoot]\nSet the target e.g. to a garden. It is as well possible to set the target directly to a element deeper in the hierarchy, e.g. to a shoot. gardenctl drop target\nDrop the deepest target.  Examples of basic usage:  List all seed cluster\ngardenctl ls seeds List all projects with shoot cluster\ngardenctl ls projects Target a seed cluster\ngardenctl target seed-gce-dev Target a project\ngardenctl target garden-vora Open prometheus ui for a targeted shoot-cluster\ngardenctl show prometheus Execute an aws command on a targeted aws shoot cluster\ngardenctl aws ec2 describe-instances or\ngardenctl aws ec2 describe-instances --no-cache without locally caching credentials Target a shoot directly and get all kube-dns pods in kube-system namespace\ngardenctl target myshoot\ngardenctl kubectl get pods -- -n kube-system -l k8s-app=kube-dns List all cluster with an issue\ngardenctl ls issues Drop an element from target stack\ngardenctl drop Open a shell to a cluster node\ngardenctl shell nodename Show logs from elasticsearch\ngardenctl logs etcd-main --elasticsearch Show last 100 logs from elasticsearch from the last 2 hours\ngardenctl logs etcd-main --elasticsearch --since=2h --tail=100 Show logs from seed nodes\ngardenctl target -g garden-name -s seed-name\ngardenctl logs tf infra shoot-name Show logs from shoot nodes\ngardenctl target -g garden-name -t shoot-name\ngardenctl logs api | scheduler | controller-manager | etcd-main -c etcd |etcd-main -c backup-restore | vpn-seed | vpn-shoot | machine-controller-manager | prometheus |grafana | cluster-autoscaler Show logs from garden nodes\ngardenctl target -g garden-name\ngardenctl logs gardener-apiserver | gardener-controller-manager SSH to shoot nodes (please unset any proxy env vars like HTTPS and HTTP before this command)\ngardenctl k get nodes\ngardenctl ssh node_name  Advanced usage based on JsonQuery The following examples are based on jq. The Json Query Playground offers a convenient environment to test the queries.\nBelow a list of examples:\n List the project name, shoot name and the state for all projects with issues  gardenctl ls issues -o json | jq \u0026#39;.issues[] | { project: .project, shoot: .shoot, state: .status.lastOperation.state }\u0026#39;  Print all issues of a single project e.g. garden-myproject  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.project==\u0026#34;garden-myproject\u0026#34;) then . else empty end\u0026#39;  Print all issues with error state \u0026ldquo;Error\u0026rdquo;  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state==\u0026#34;Error\u0026#34;) then . else empty end\u0026#39;  Print all issues with error state not equal \u0026ldquo;Succeded\u0026rdquo;  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state!=\u0026#34;Succeeded\u0026#34;) then . else empty end\u0026#39;  Print createdBy information (typically email addresses) of all shoots  gardenctl k get shoots -- -n garden-core -o json | jq -r \u0026#34;.items[].metadata | {email: .annotations.\\\u0026#34;garden.sapcloud.io/createdBy\\\u0026#34;, name: .name, namespace: .namespace}\u0026#34; Here a few on cluster analysis:\n Which states are there and how many clusters are in this state?  gardenctl ls issues -o json | jq \u0026#39;.issues | group_by( .status.lastOperation.state ) | .[] | {state:.[0].status.lastOperation.state, count:length}\u0026#39;  Get all clusters in state Failed  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state==\u0026#34;Failed\u0026#34;) then . else empty end\u0026#39; "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/containerruntime/","title":"Gardener Container Runtime Extension","tags":[],"description":"","content":"Gardener Container Runtime Extension At the lowest layers of a Kubernetes node is the software that, among other things, starts and stops containers. It is called ‚ÄúContainer Runtime‚Äù. The most widely known container runtime is Docker, but it is not alone in this space. In fact, the container runtime space has been rapidly evolving.\nKubernetes supports different container runtimes using Container Runtime Interface (CRI) ‚Äì a plugin interface which enables kubelet to use a wide variety of container runtimes.\nGardener supports creation of Worker machines using CRI, more information can be found here: CRI Support.\nMotivation Prior to the Container Runtime Extensibility concept, Gardener used Docker as the only container runtime to use in shoot worker machines. Because of the wide variety of different container runtimes offers multiple important features (for example enhanced security concepts) it is important to enable end users to use other container runtimes as well.\nThe ContainerRuntime Extension Resource Here is what a typical ContainerRuntime resource would look-like:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:ContainerRuntimemetadata:name:my-container-runtimespec:binaryPath:/var/bin/containerruntimestype:gvisorworkerPool:name:worker-ubuntuselector:matchLabels:worker.gardener.cloud/pool:worker-ubuntuGardener deploys one ContainerRuntime resource per worker pool per CRI. To exemplify this, consider a Shoot having two worker pools (worker-one, worker-two) using containerd as the CRI as well as gvisor and kata as enabled container runtimes. Gardener would deploy four ContainerRuntime resources. For worker-one: one ContainerRuntime for type gvisor and one for type kata. The same resource are being deployed for worker-two.\nSupporting a new Container Runtime Provider To add support for another container runtime (e.g., gvisor, kata-containers, etc.) a container runtime extension controller needs to be implemented. It should support Gardener\u0026rsquo;s supported CRI plugins.\nThe container runtime extension should install the necessary resources into the shoot cluster (e.g., RuntimeClasses), and it should copy the runtime binaries to the relevant worker machines in path: spec.binaryPath. Gardener labels the shoot nodes according to the CRI configured: worker.gardener.cloud/cri-name=\u0026lt;value\u0026gt; (e.g worker.gardener.cloud/cri-name=containerd) and multiple labels for each of the container runtimes configured for the shoot Worker machine: containerruntime.worker.gardener.cloud/\u0026lt;container-runtime-type-value\u0026gt;=true (e.g containerruntime.worker.gardener.cloud/gvisor=true). The way to install the binaries is by creating a daemon set which copies the binaries from an image in a docker registry to the relevant labeled Worker\u0026rsquo;s nodes (avoid downloading binaries from internet to also cater with isolated environments).\nFor additional reference, please have a look at the runtime-gvsior provider extension, which provides more information on how to configure the necessary charts as well as the actuators required to reconcile container runtime inside the Shoot cluster to the desired state.\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/conventions/","title":"General conventions","tags":[],"description":"","content":"General conventions All the extensions that are registered to Gardener are deployed to the seed clusters (at the moment, every extension is installed to every seed cluster, however, in the future Gardener will be more smart to determine which extensions needs to be placed into which seed).\nSome of these extensions might need to create global resources in the seed (e.g., ClusterRoles), i.e., it\u0026rsquo;s important to have a naming scheme to avoid conflicts as it cannot be checked or validated upfront that two extensions don\u0026rsquo;t use the same names.\nConsequently, this page should help answering some general questions that might come up when it comes to developing an extension.\nIs there a naming scheme for (global) resources? As there is no formal process to validate non-existence of conflicts between two extensions please follow these naming schemes when creating resources (especially, when creating global resources, but it\u0026rsquo;s in general a good idea for most created resources):\nThe resource name should be prefixed with extensions.gardener.cloud:\u0026lt;extension-type\u0026gt;-\u0026lt;extension-name\u0026gt;:\u0026lt;resource-name\u0026gt;, for example:\n extensions.gardener.cloud:provider-aws:machine-controller-manager extensions.gardener.cloud:extension-certificate-service:cert-broker  How to create resources in the shoot cluster? Some extensions might not only create resources in the seed cluster itself but also in the shoot cluster. Usually, every extension comes with a ServiceAccount and the required RBAC permissions when it gets installed to the seed. However, there are no credentials for the shoot for every extension.\nGardener creates a kubeconfig for itself that it uses to interact with the shoot cluster. This kubeconfig is stored as a Secret with name gardener in the shoot namespace. Extension controllers may use this kubeconfig to interact with the shoot cluster if desired (it has full administrator privileges and no further RBAC rules are required). Instead, they could also create their own kubeconfig for every shoot (which, of course, is better for auditing reasons, but not yet enforced at this point in time).\nIf you need to deploy a non-DaemonSet resource you need to ensure that it only runs on nodes that are allowed to host system components and extensions. To do that you need to configure a nodeSelector as following:\nnodeSelector:worker.gardener.cloud/system-components:\u0026#34;true\u0026#34;How to create certificates/kubeconfigs for the shoot cluster? Gardener creates several certificate authorities (CA) that are used to create server/client certificates for various components. For example, the shoot\u0026rsquo;s etcd has its own CA, the kube-aggregator has its own CA as well, and both are different to the actual cluster\u0026rsquo;s CA.\nThese CAs are stored as Secrets in the shoot namespace (see this for the actual names). Extension controllers may use them to create further certificates/kubeconfigs for potential other components they need to deploy to the seed or shoot. These utility functions should help with the creation and management.\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/healthcheck-library/","title":"Health Check Library","tags":[],"description":"","content":"Health Check Library Goal Typically an extension reconciles a specific resource (Custom Resource Definitions (CRDs)) and creates/modifies resources in the cluster (via helm, managed resources, kubectl, \u0026hellip;). We call these API Objects \u0026lsquo;dependent objects\u0026rsquo; - as they are bound to the lifecycle of the extension.\nThe goal of this library is to enable extensions to setup health checks for their \u0026lsquo;dependent objects\u0026rsquo; with minimal effort.\nUsage The library provides a generic controller with the ability to register any resource that satisfies the extension object interface. An example is the Worker CRD.\nHealth check functions for commonly used dependent objects can be reused and registered with the controller, such as:\n Deployment DaemonSet StatefulSet ManagedResource (Gardener specific)  See below example taken from the provider-aws.\nhealth.DefaultRegisterExtensionForHealthCheck( aws.Type, extensionsv1alpha1.SchemeGroupVersion.WithKind(extensionsv1alpha1.WorkerResource), func() runtime.Object { return \u0026amp;extensionsv1alpha1.Worker{} }, mgr, // controller runtime manager  opts, // options for the health check controller  nil, // custom predicates  map[extensionshealthcheckcontroller.HealthCheck]string{ general.CheckManagedResource(genericactuator.McmShootResourceName): string(gardencorev1beta1.ShootSystemComponentsHealthy), general.CheckSeedDeployment(aws.MachineControllerManagerName): string(gardencorev1beta1.ShootEveryNodeReady), worker.SufficientNodesAvailable(): string(gardencorev1beta1.ShootEveryNodeReady), }) This creates a health check controller that reconciles the extensions.gardener.cloud/v1alpha1.Worker resource with the spec.type \u0026lsquo;aws\u0026rsquo;. Three health check functions are registered that are executed during reconciliation. Each health check is mapped to a single HealthConditionType that results in conditions with the same condition.type (see below). To contribute to the Shoot\u0026rsquo;s health, the following can be used: SystemComponentsHealthy, EveryNodeReady, ControlPlaneHealthy. The Gardener/Gardenlet checks each extension for conditions matching these types. However extensions are free to choose any HealthConditionType. More information can be found here.\nA health check has to satisfy below interface. You can find implementation examples here.\ntype HealthCheck interface { // Check is the function that executes the actual health check  Check(context.Context, types.NamespacedName) (*SingleCheckResult, error) // InjectSeedClient injects the seed client  InjectSeedClient(client.Client) // InjectShootClient injects the shoot client  InjectShootClient(client.Client) // SetLoggerSuffix injects the logger  SetLoggerSuffix(string, string) // DeepCopy clones the healthCheck  DeepCopy() HealthCheck } The health check controller regularly (default: 30s) reconciles the extension resource and executes the registered health checks for the dependent objects. As a result, the controller writes condition(s) to the status of the extension containing the health check result. In our example, two checks are mapped to ShootEveryNodeReady and one to ShootSystemComponentsHealthy, leading to conditions with two distinct HealthConditionTypes (condition.type)\nstatus:conditions:- lastTransitionTime:\u0026#34;20XX-10-28T08:17:21Z\u0026#34;lastUpdateTime:\u0026#34;20XX-11-28T08:17:21Z\u0026#34;message:(1/1)Healthcheckssuccessfulreason:HealthCheckSuccessfulstatus:\u0026#34;True\u0026#34;type:SystemComponentsHealthy- lastTransitionTime:\u0026#34;20XX-10-28T08:17:21Z\u0026#34;lastUpdateTime:\u0026#34;20XX-11-28T08:17:21Z\u0026#34;message:(2/2)Healthcheckssuccessfulreason:HealthCheckSuccessfulstatus:\u0026#34;True\u0026#34;type:EveryNodeReadyPlease note that there are four statuses: True, False, Unknown, and Progressing.\n True should be used for successful health checks. False should be used for unsuccessful/failing health checks. Unknown should be used when there was an error trying to determine the health status. Progressing should be used to indicate that the health status did not succeed but for expected reasons (e.g., a cluster scale up/down could make the standard health check fail because something is wrong with the Machines, however, it\u0026rsquo;s actually an expected situation and known to be completed within a few minutes.)  Health checks that report Progressing should also provide a timeout after which this \u0026ldquo;progressing situation\u0026rdquo; is expected to be completed. The health check library will automatically transition the status to False if the timeout was exceeded.\nAdditional Considerations It is up to the extension to decide how to conduct health checks, though it is recommended to make use of the build-in health check functionality of managed-resources for trivial checks. By deploying the depending resources via managed resources, the gardener resource manager conducts basic checks for different API objects out-of-the-box (e.g Deployments, DaemonSets, \u0026hellip;) - and writes health conditions. In turn, the library contains a health check function to gather the health information from managed resources.\nMore sophisticated health checks should be implemented by the extension controller itself (implementing the HealthCheck interface).\n"},{"uri":"https://gardener.cloud/documentation/","title":"Home","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/concepts/deployment/image_vector/","title":"Image Vector","tags":[],"description":"","content":"Image Vector The Gardenlet is deploying several different container images into the seed and the shoot clusters. The image repositories and tags are defined in a central image vector file. Obviously, the image versions defined there must fit together with the deployment manifests (e.g., some command-line flags do only exist in certain versions).\nExample images:- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:gcr.io/google_containers/pause-amd64tag:\u0026#34;3.0\u0026#34;version:1.11.x- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:gcr.io/google_containers/pause-amd64tag:\u0026#34;3.1\u0026#34;version:\u0026#34;\u0026gt;= 1.12\u0026#34;...That means that the Gardenlet will use the pause-container in with tag 3.0 for all seed/shoot clusters with Kubernetes version 1.11.x, and tag 3.1 for all clusters with Kubernetes \u0026gt;= 1.12.\nOverwrite image vector In some environment it is not possible to use these \u0026ldquo;pre-defined\u0026rdquo; images that come with a Gardener release. A prominent example for that is Alicloud in China which does not allow access to Google\u0026rsquo;s GCR. In these cases you might want to overwrite certain images, e.g., point the pause-container to a different registry.\n:warning: If you specify an image that does not fit to the resource manifest then the seed/shoot reconciliation might fail.\nIn order to overwrite the images you must provide a similar file to Gardenlet:\nimages:- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:my-custom-image-registry/pause-amd64tag:\u0026#34;3.0\u0026#34;version:1.11.x- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:my-custom-image-registry/pause-amd64tag:\u0026#34;3.1\u0026#34;version:\u0026#34;\u0026gt;= 1.12\u0026#34;...During deployment of the gardenlet create a ConfigMap containing the above content and mount it as a volume into the gardenlet pod. Next, specify the environment variable IMAGEVECTOR_OVERWRITE whose value must be the path to the file you just mounted:\napiVersion:v1kind:ConfigMapmetadata:name:gardenlet-images-overwritenamespace:gardendata:images_overwrite.yaml:| images:- ...---apiVersion:apps/v1kind:Deploymentmetadata:name:gardenletnamespace:gardenspec:template:...spec:containers:- name:gardenletenv:- name:IMAGEVECTOR_OVERWRITEvalue:/charts-overwrite/images_overwrite.yamlvolumeMounts:- name:gardenlet-images-overwritemountPath:/charts-overwrite...volumes:- name:gardenlet-images-overwriteconfigMap:name:gardenlet-images-overwrite...Image vectors for dependent components The gardenlet is deploying a lot of different components that might deploy other images themselves. These components might use an image vector as well. Operators might want to customize the image locations for these transitive images as well, hence, they might need to specify an image vector overwrite for the components directly deployed by Gardener.\nIt is possible to specify the IMAGEVECTOR_OVERWRITE_COMPONENTS environment variable to the gardenlet that points to a file with the following content:\ncomponents:- name:etcd-druidimageVectorOverwrite:| images:- name:etcdtag:v1.2.3repository:etcd/etcd...The gardenlet will, if supported by the directly deployed component (etcd-druid in this example), inject the given imageVectorOverwrite into the Deployment manifest. The respective component is responsible for using the overwritten images instead of its defaults.\n"},{"uri":"https://gardener.cloud/documentation/concepts/core-components/api-server/apiserver_admission_plugins/","title":"In-Tree Admission Plugins","tags":[],"description":"","content":"Admission Plugins Similar to the kube-apiserver, the gardener-apiserver comes with a few in-tree managed admission plugins. If you want to get an overview of the what and why of admission plugins then this document might be a good start.\nThis document lists all existing admission plugins with a short explanation of what it is responsible for.\nClusterOpenIDConnectPreset, OpenIDConnectPreset (both enabled by default)\nThese admission controllers react on CREATE operations for Shoots. If the Shoot does not specify any OIDC configuration (.spec.kubernetes.kubeAPIServer.oidcConfig=nil) then it tries to find a matching ClusterOpenIDConnectPreset or OpenIDConnectPreset, respectively. If there are multiples that match then the one with the highest weight \u0026ldquo;wins\u0026rdquo;. In this case, the admission controller will default the OIDC configuration in the Shoot.\nControllerRegistrationResources (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for ControllerRegistrations. It validates that there exists only one ControllerRegistration in the system that is primarily responsible for a given kind/type resource combination. This prevents misconfiguration by the Gardener administrator/operator.\nCustomVerbAuthorizer (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Projects. It validates whether the user is bound to a RBAC role with the modify-spec-tolerations-whitelist verb in case the user tries to change the .spec.tolerations.whitelist field of the respective Project resource. Usually, regular project members are not bound to this custom verb, allowing the Gardener administrator to manage certain toleration whitelists on Project basis.\nDeletionConfirmation (enabled by default)\nThis admission controller reacts on DELETE operations for Projects and Shoots. It validates that the respective resource is annotated with a deletion confirmation annotation, namely confirmation.gardener.cloud/deletion=true. Only if this annotation is present it allows the DELETE operation to pass. This prevents users from accidental/undesired deletions.\nExtensionValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for BackupEntrys, BackupBuckets, Seeds, and Shoots. For all the various extension types in the specifications of these objects, it validates whether there exists a ControllerRegistration in the system that is primarily responsible for the stated extension type(s). This prevents misconfigurations that would otherwise allow users to create such resources with extension types that don\u0026rsquo;t exist in the cluster, effectively leading to failing reconciliation loops.\nPlantValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Plants. It sets the gardener.cloud/created-by annotation for newly created Plant resources. Also, it prevents creating new Plant resources in Projects that are already have a deletion timestamp.\nResourceQuota (enabled by default)\nThis admission controller enables object count ResourceQuotas for Gardener resources, e.g. Shoots, SecretBindings, Projects, etc..\n :warning: In addition to this admission plugin, the ResourceQuota controller must be enabled for the Kube-Controller-Manager of your Garden cluster.\n ResourceReferenceManager (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for CloudProfiles, Projects, SecretBindings, Seeds, and Shoots. Generally, it checks whether referred resources stated in the specifications of these objects exist in the system (e.g., if a referenced Secret exists). However, it also has some special behaviours for certain resources:\n CloudProfiles: It rejects removing Kubernetes or machine image versions if there is at least one Shoot that refers to them. Projects: It sets the .spec.createdBy field for newly created Project resources, and defaults the .spec.owner field in case it is empty (to the same value of .spec.createdBy). Seeds: It rejects changing the .spec.settings.shootDNS.enabled value if there is at least one Shoot that refers to this seed. Shoots: It sets the gardener.cloud/created-by=\u0026lt;username\u0026gt; annotation for newly created Shoot resources.  SeedValidator (enabled by default)\nThis admission controller reacts on DELETE operations for Seeds. Rejects the deletion if Shoot(s) reference the seed cluster.\nShootDNS (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It tries to assign a default domain to the Shoot if it gets scheduled to a seed that enables DNS for shoots (.spec.settings.shootDNS.enabled=true). It also validates that the DNS configuration (.spec.dns) is not set if the seed disables DNS for shoots.\nShootQuotaValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It validates the resource consumption declared in the specification against applicable Quota resources. Only if the applicable Quota resources admit the configured resources in the Shoot then it allows the request. Applicable Quotas are referred in the SecretBinding that is used by the Shoot.\nShootStateDeletionValidator (enabled by default)\nThis admission controller reacts on DELETE operations for ShootStates. It prevents the deletion of the respective ShootState resource in case the corresponding Shoot resource does still exist in the system. This prevents losing the shoot\u0026rsquo;s data required to recover it / migrate its control plane to a new seed cluster.\nShootTolerationRestriction (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It validates the .spec.tolerations used in Shoots against the whitelist of its Project, or against the whitelist configured in the admission controller\u0026rsquo;s configuration, respectively. Additionally, it defaults the .spec.tolerations in Shoots with those configured in its Project, and those configured in the admission controller\u0026rsquo;s configuration, respectively.\nShootValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It validates certain configurations in the specification against the referred CloudProfile (e.g., machine images, machine types, used Kubernetes version, \u0026hellip;). Generally, it performs validations that cannot be handled by the static API validation due to their dynamic nature (e.g., when something needs to be checked against referred resources). Additionally, it takes over certain defaulting tasks (e.g., default machine image for worker pools).\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/infrastructure/","title":"Infrastructure resource","tags":[],"description":"","content":"Contract: Infrastructure resource Every Kubernetes cluster requires some low-level infrastructure to be setup in order to work properly. Examples for that are networks, routing entries, security groups, IAM roles, etc. Before introducing the Infrastructure extension resource Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see here). Now, Gardener commissions an external, provider-specific controller to take over this task.\nWhich infrastructure resources are required? Unfortunately, there is no general answer to this question as it is highly provider specific. Consider the above mentioned resources, i.e. VPC, subnets, route tables, security groups, IAM roles, SSH key pairs. Most of the resources are required in order to create VMs (the shoot cluster worker nodes), load balancers, and volumes.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--foo--barspec:type:azureregion:eu-west-1secretRef:name:cloudprovidernamespace:shoot--foo--barproviderConfig:apiVersion:azure.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfigresourceGroup:name:mygroupnetworks:vnet:# specify either \u0026#39;name\u0026#39; or \u0026#39;cidr\u0026#39;# name: my-vnetcidr:10.250.0.0/16workers:10.250.0.0/19The .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. However, the most important section is the .spec.providerConfig. It contains an embedded declaration of the provider specific configuration for the infrastructure (that cannot be known by Gardener itself). You are responsible for designing how this configuration looks like. Gardener does not evaluate it but just copies this part from what has been provided by the end-user in the Shoot resource.\nAfter your controller has created the required resources in your provider\u0026rsquo;s infrastructure it needs to generate an output that can be used by other controllers in subsequent steps. An example for that is the Worker extension resource controller. It is responsible for creating virtual machines (shoot worker nodes) in this prepared infrastructure. Everything that it needs to know in order to do that (e.g., the network IDs, security group names, etc. (again: provider-specific)) needs to be provided as output in the Infrastructure resource:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--foo--barspec:...status:lastOperation:...providerStatus:apiVersion:azure.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusresourceGroup:name:mygroupnetworks:vnet:name:my-vnetsubnets:- purpose:nodesname:my-subnetavailabilitySets:- purpose:nodesid:av-set-idname:av-set-namerouteTables:- purpose:nodesname:route-table-namesecurityGroups:- purpose:nodesname:sec-group-nameIn order to support a new infrastructure provider you need to write a controller that watches all Infrastructures with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the Azure provider.\nDynamic nodes network for shoot clusters Some environments do not allow end-users to statically define a CIDR for the network that shall be used for the shoot worker nodes. In these cases it is possible for the extension controllers to dynamically provision a network for the nodes (as part of their reconciliation loops), and to provide the CIDR in the status of the Infrastructure resource:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--foo--barspec:...status:lastOperation:...providerStatus:...nodesCIDR:10.250.0.0/16Gardener will pick this nodesCIDR and use it to configure the VPN components to establish network connectivity between the control plane and the worker nodes. If the Shoot resource already specifies a nodes CIDR in .spec.networking.nodes and the extension controller provides also a value in .status.nodesCIDR in the Infrastructure resource then the latter one will always be considered with higher priority by Gardener.\nNon-provider specific information required for infrastructure creation Some providers might require further information that is not provider specific but already part of the shoot resource. One example for this is the GCP infrastructure controller which needs the pod and the service network of the cluster in order to prepare and configure the infrastructure correctly. As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information that is not part of the Infrastructure resource itself.\nReferences and additional resources  Infrastructure API (Golang specification) Exemplary implementation for the Azure provider  "},{"uri":"https://gardener.cloud/components/kubify/","title":"kubify","tags":[],"description":"","content":"Kubify Kubify is a Terraform based provisioning project for setting up production ready Kubernetes clusters on public and private Cloud infrastructures. Kubify currently supports:\n OpenStack AWS Azure  Key features of Kubify are:\n Kubernetes v1.10.12 Etcd v3.3.10 multi master node setup Etcd backup and restore Supports rolling updates   To start using or developing Kubify locally See our documentation in the /docs repository or find the main documentation here.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions about our Kubernetes clusters as such or the Kubify itself as GitHub issues or join our Slack channel #gardener (Invite yourself to the Kubernetes Slack workspace here).\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/logging-and-monitoring/","title":"Logging and Monitoring for Extensions","tags":[],"description":"","content":"Logging and Monitoring for Extensions Gardener provides an integrated logging and monitoring stack for alerting, monitoring and troubleshooting of its managed components by operators or end users. For further information how to make use of it in these roles, refer to the corresponding guides for exploring logs and for monitoring with Grafana.\nThe components that constitute the logging and monitoring stack are managed by Gardener. By default, it deploys Prometheus, Alertmanager and Grafana into the garden namespace of all seed clusters. If the Logging feature gate in the gardenlet configuration is enabled, it will deploy fluent-bit and Loki in the garden namespace too.\nEach shoot namespace hosts managed logging and monitoring components. As part of the shoot reconciliation flow, Gardener deploys a shoot-specific Prometheus, Grafana and, if configured, an Alertmanager into the shoot namespace, next to the other control plane components. If the Logging feature gate is enabled and the shoot purpose is not testing, it deploys a shoot-specific Loki in the shoot namespace too.\nThe logging and monitoring stack is extensible by configuration. Gardener extensions can take advantage of that and contribute configurations encoded in ConfigMaps for their own, specific dashboards, alerts, log parsers and other supported assets and integrate with it. As with other Gardener resources, they will be continuously reconciled.\nThis guide is about the roles and extensibility options of the logging and monitoring stack components, and how to integrate extensions with:\n Monitoring Logging  Monitoring The central Prometheus instance in the garden namespace fetches metrics and data from all seed cluster nodes and all seed cluster pods. It uses the federation concept to allow the shoot-specific instances to scrape only the metrics for the pods of the control plane they are responsible for. This mechanism allows to scrape the metrics for the nodes/pods once for the whole cluster, and to have them distributed afterwards.\nThe shoot-specific metrics are then made available to operators and users in the shoot Grafana, using the shoot Prometheus as data source.\nExtension controllers might deploy components as part of their reconciliation next to the shoot\u0026rsquo;s control plane. Examples for this would be a cloud-controller-manager or CSI controller deployments. Extensions that want to have their managed control plane components integrated with monitoring can contribute their per-shoot configuration for scraping Prometheus metrics, Alertmanager alerts or Grafana dashboards.\nExtensions monitoring integration Before deploying the shoot-specific Prometheus instance, Gardener will read all ConfigMaps in the shoot namespace, which are labeled with extensions.gardener.cloud/configuration=monitoring. Such ConfigMaps may contain four fields in their data:\n scrape_config: This field contains Prometheus scrape configuration for the component(s) and metrics that shall be scraped. alerting_rules: This field contains Alertmanager rules for alerts that shall be raised. dashboard_operators: This field contains a Grafana dashboard in JSON that is only relevant for Gardener operators. dashboard_users: This field contains a Grafana dashboard in JSON that is only relevant for Gardener users (shoot owners).  Example: A ControlPlane controller deploying a cloud-controller-manager into the shoot namespace wants to integrate monitoring configuration for scraping metrics, alerting rules and dashboards.\napiVersion:v1kind:ConfigMapmetadata:name:extension-controlplane-monitoring-ccmnamespace:shoot--project--namelabels:extensions.gardener.cloud/configuration:monitoringdata:scrape_config:| - job_name: cloud-controller-managerscheme:httpstls_config:insecure_skip_verify:truecert_file:/etc/prometheus/seed/prometheus.crtkey_file:/etc/prometheus/seed/prometheus.keyhonor_labels:falsekubernetes_sd_configs:- role:endpointsnamespaces:names:[shoot--project--name]relabel_configs:- source_labels:- __meta_kubernetes_service_name- __meta_kubernetes_endpoint_port_nameaction:keepregex:cloud-controller-manager;metrics# common metrics- action:labelmapregex:__meta_kubernetes_service_label_(.+)- source_labels:[__meta_kubernetes_pod_name]target_label:podmetric_relabel_configs:- process_max_fds- process_open_fdsalerting_rules:| cloud-controller-manager.rules.yaml: |groups:- name:cloud-controller-manager.rulesrules:- alert:CloudControllerManagerDownexpr:absent(up{job=\u0026#34;cloud-controller-manager\u0026#34;}==1)for:15mlabels:service:cloud-controller-managerseverity:criticaltype:seedvisibility:allannotations:description:Allinfrastructurespecificoperationscannotbecompleted(e.g.creatingloadbalancersorpersistentvolumes).summary:Cloudcontrollermanagerisdown.dashboard_operators:\u0026lt;some-json-describing-a-grafana-dashboard-for-operators\u0026gt; dashboard_users:\u0026lt;some-json-describing-a-grafana-dashboard-for-users\u0026gt;Logging In Kubernetes clusters, container logs are non-persistent and do not survive stopped and destroyed containers. Gardener addresses this problem for the components hosted in a seed cluster, by introducing its own managed logging solution. It is integrated with the Gardener monitoring stack to have all troubleshooting context in one place.\nGardener logging consists of components in three roles - log collectors and forwarders, log persistency and exploration/consumption interfaces. All of them live in the seed clusters in multiple instances:\n Logs are persisted by Loki instances deployed as StatefulSets - one per shoot namespace, if the Logging feature gate is enabled and the shoot purpose is not testing, and one in the garden namespace. The shoot instances store logs from the control plane components hosted there. The garden Loki instance is responsible for logs from the rest of the seed namespaces - kube-system, garden extension-* and others. Fluent-bit DaemonSets deployed on each seed node collect logs from it. A custom plugin takes care to distribute the collected log messages to the Loki instances that they are intended for. This allows to fetch the logs once for the whole cluster, and to distribute them afterwards. Grafana is the UI component used to explore monitoring and log data together for easier troubleshooting and in context. Grafana instances are configured to use the coresponding Loki instances, sharing the same namespace, as data providers. There is one Grafana Deployment in the garden namespace and two Deployments per shoot namespace (one exposed to the end users and one for the operators).  Logs can be produced from various sources, such as containers or systemd, and in different formats. The fluent-bit design supports configurable data pipeline to address that problem. Gardener provides such configuration for logs produced by all its core managed components as a ConfigMap. Extensions can contribute their own, specific configurations as ConfigMaps too. See for example the logging configuration for the Gardener AWS provider extension. The Gardener reconciliation loop watches such resources and updates the fluent-bit agents dynamically.\nExtensions logging integration Fluent-bit log parsers and filters To integrate with Gardener logging, extensions can and should specify how fluent-bit will handle the logs produced by the managed components that they contribute to Gardener. Normally, that would require to configure a parser for the specific logging format, if none of the available is applicable, and a filter defining how to apply it. For a complete reference for the configuration options, refer to fluent-bit\u0026rsquo;s documentation.\nNote: At the moment only parser and filter configurations are supported.\nTo contribute its own configuration to the fluent-bit agents data pipelines, an extension must provide it as a ConfigMap labeled extensions.gardener.cloud/configuration=logging and deployed in the seed\u0026rsquo;s garden namespace. Unlike the monitoring stack, where configurations are deployed per shoot, here a single configuration ConfigMap is sufficient and it applies to all fluent-bit agents in the seed. Its data field can have the following properties:\n filter-kubernetes.conf - configuration for data pipeline filters parser.conf - configuration for data pipeline parsers  Note: Take care to provide the correct data pipeline elements in the coresponding data field and not to mix them.\nExample: Logging configuration for provider-specific (OpenStack) worker controller deploying a machine-controller-manager component into a shoot namespace that reuses the kubeapiserverParser defined in fluent-bit-configmap.yaml to parse the component logs\napiVersion:v1kind:ConfigMapmetadata:name:gardener-extension-provider-openstack-logging-confignamespace:gardenlabels:extensions.gardener.cloud/configuration:loggingdata:filter-kubernetes.conf:| [FILTER]NameparserMatchkubernetes.machine-controller-manager*openstack-machine-controller-manager*Key_NamelogParserkubeapiserverParserReserve_DataTrueFurther details how to define parsers and use them with examples can be found in the following guide.\nGrafana The three types of Grafana instances found in a seed cluster are configured to expose logs of different origin in their dashboards:\n Garden Grafana dashboards expose logs from non-shoot namespaces of the seed clusters  Pod Logs Extensions Systemd Logs   Shoot User Grafana dashboards expose a subset of the logs shown to operators  Kube Apiserver Kube Controller Manager Kube Scheduler Cluster Autoscaler   Shoot Operator Grafana dashboards expose logs from the shoot cluster namespace where they belong  All user\u0026rsquo;s dashboards Kubernetes Pods    If the type of logs exposed in the Grafana instances needs to be changed, it is necessary to update the coresponding instance dashboard configurations.\nTips  Be careful to match exactly the log names that you need for a particular parser in your filters configuration. The regular expression you will supply will match names in the form kubernetes.pod_name.\u0026lt;metadata\u0026gt;.container_name. If there are extensions with the same container and pod names, they will all match the same parser in a filter. That may be a desired effect, if they all share the same log format. But it will be a problem if they don\u0026rsquo;t. To solve it, either the pod or container names must be unique, and the regular expression in the filter has to match that unique pattern. A recommended approach is to prefix containers with the extension name and tune the regular expression to match it. For example, using myextension-container as container name, and a regular expression kubernetes.mypod.*myextension-container will guarantee match of the right log name. Make sure that the regular expression does not match more than you expect. For example, kubernetes.systemd.*systemd.* will match both systemd-service and systemd-monitor-service. You will want to be as specific as possible. It\u0026rsquo;s a good idea to put the logging configuration into the Helm chart that also deploys the extension controller, while the monitoring configuration can be part of the Helm chart/deployment routine that deploys the component managed by the controller.  References and additional resources  GitHub issue describing the concept Exemplary implementation (monitoring) for the GCP provider Exemplary implementation (logging) for the OpenStack provider  "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/operatingsystemconfig/","title":"OperatingSystemConfig resource","tags":[],"description":"","content":"Contract: OperatingSystemConfig resource Gardener uses the machine API and leverages the functionalities of the machine-controller-manager (MCM) in order to manage the worker nodes of a shoot cluster. The machine-controller-manager itself simply takes a reference to an OS-image and (optionally) some user-data (a script or configuration that is executed when a VM is bootstrapped), and forwards both to the provider\u0026rsquo;s API when creating VMs. MCM does not have any restrictions regarding supported operating systems as it does not modify or influence the machine\u0026rsquo;s configuration in any way - it just creates/deletes machines with the provided metadata.\nConsequently, Gardener needs to provide this information when interacting with the machine-controller-manager. This means that basically every operating system is possible to be used as long as there is some implementation that generates the OS-specific configuration in order to provision/bootstrap the machines.\n:warning: Currently, there are a few requirements:\n The operating system must have built-in Docker support. The operating system must have systemd support. The operating system must have wget pre-installed. The operating system must have jq pre-installed.  The reasons for that will become evident later.\nWhat does the user-data bootstrapping the machines contain? Gardener installs a few components onto every worker machine in order to allow it to join the shoot cluster. There is the kubelet process, some scripts for continuously checking the health of kubelet and docker, but also configuration for log rotation, CA certificates, etc. The complete configuration you can find here. We are calling this the \u0026ldquo;original\u0026rdquo; user-data.\nHow does Gardener bootstrap the machines? Usually, you would submit all the components you want to install onto the machine as part of the user-data during creation time. However, some providers do have a size limitation (like ~16KB) for that user-data. That\u0026rsquo;s why we do not send the \u0026ldquo;original\u0026rdquo; user-data to the machine-controller-manager (who forwards it then to the provider\u0026rsquo;s API). Instead, we only send a small script that downloads the \u0026ldquo;original\u0026rdquo; data and applies it on the machine directly. This way we can extend the \u0026ldquo;original\u0026rdquo; user-data without any size restrictions - plus we can modify it without the necessity of re-creating the machine (because we run a script that downloads and updates it continuously).\nThe high-level flow is as follows:\n  For every worker pool X in the Shoot specification, Gardener creates a Secret named cloud-config-\u0026lt;X\u0026gt; in the kube-system namespace of the shoot cluster. The secret contains the \u0026ldquo;original\u0026rdquo; user-data.\n  Gardener generates a kubeconfig with minimal permissions just allowing reading these secrets. It is used by the downloader script later.\n  Gardener provides the downloader script, the kubeconfig, and the machine image stated in the Shoot specification to the machine-controller-manager.\n  Based on this information the machine-controller-manager creates the VM.\n  After the VM has been provisioned the downloader script starts and fetches the appropriate Secret for its worker pool (containing the \u0026ldquo;original\u0026rdquo; user-data) and applies it.\n  How does Gardener update the user-data on already existing machines? With ongoing development and new releases of Gardener some new components could be required to get installed onto every shoot worker VM, or existing components need to be changed. Gardener achieves that by simply updating the user-data inside the Secrets mentioned above (step 1). The downloader script is continuously (every 30s) reading the secret\u0026rsquo;s content (which might include an updated user-data) and storing it onto the disk. In order to re-apply the (new) downloaded data the secrets do not only contain the \u0026ldquo;original\u0026rdquo; user-data but also another short script (called \u0026ldquo;execution\u0026rdquo; script). This script checks whether the downloaded user-data differs from the one previously applied - and if required - re-applies it. After that it uses systemctl to restart the installed systemd units.\nWith the help of the execution script Gardener can centrally control how machines are updated without the need of OS providers to (re-)implement that logic. However, as stated in the mentioned requirements above, the execution script assumes existence of Docker and systemd.\nWhat needs to be implemented to support a new operating system? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:OperatingSystemConfigmetadata:name:pool-01-originalnamespace:defaultspec:type:\u0026lt;my-operating-system\u0026gt; purpose: reconcilereloadConfigFilePath:/var/lib/cloud-config-downloader/cloud-configunits:- name:docker.servicedropIns:- name:10-docker-opts.confcontent:| [Service]Environment=\u0026#34;DOCKER_OPTS=--log-opt max-size=60m --log-opt max-file=3\u0026#34;- name:docker-monitor.servicecommand:startenable:truecontent:| [Unit]Description=Docker-monitordaemonAfter=kubelet.service[Install]WantedBy=multi-user.target[Service]Restart=alwaysEnvironmentFile=/etc/environmentExecStart=/opt/bin/health-monitordockerfiles:- path:/var/lib/kubelet/ca.crtpermissions:0644encoding:b64content:secretRef:name:default-token-5dtjzdataKey:token- path:/etc/sysctl.d/99-k8s-general.confpermissions:0644content:inline:data:| # A higher vm.max_map_count is great for elasticsearch, mongo, or other mmap users# See https://github.com/kubernetes/kops/issues/1340vm.max_map_count=135217728In order to support a new operating system you need to write a controller that watches all OperatingSystemConfigs with .spec.type=\u0026lt;my-operating-system\u0026gt;. For those it shall generate a configuration blob that fits to your operating system. For example, a CoreOS controller might generate a CoreOS cloud-config or Ignition, SLES might generate cloud-init, and others might simply generate a bash script translating the .spec.units into systemd units, and .spec.files into real files on the disk.\nOperatingSystemConfigs can have two purposes which can be used (or ignored) by the extension controllers: either provision or reconcile.\n The provision purpose is used by Gardener for the user-data that it later passes to the machine-controller-manager (and then to the provider\u0026rsquo;s API) when creating new VMs. It contains the downloader unit. The reconcile purpose contains the \u0026ldquo;original\u0026rdquo; user-data (that is then stored in Secrets in the shoot\u0026rsquo;s kube-system namespace (see step 1). This is downloaded and applies late (see step 5).  As described above, the \u0026ldquo;original\u0026rdquo; user-data must be re-applicable to allow in-place updates. The way how this is done is specific to the generated operating system config (e.g., for CoreOS cloud-init the command is /usr/bin/coreos-cloudinit --from-file=\u0026lt;path\u0026gt;, whereas SLES would run cloud-init --file \u0026lt;path\u0026gt; single -n write_files --frequency=once). Consequently, besides the generated OS config, the extension controller must also provide a command for re-application an updated version of the user-data. As visible in the mentioned examples the command requires a path to the user-data file. Gardener will provide the path to the file in the OperatingSystemConfigs .spec.reloadConfigFilePath field (only if .spec.purpose=reconcile). As soon as Gardener detects that the user data has changed it will reload the systemd daemon and restart all the units provided in the .status.units[] list (see below example). The same logic applies during the very first application of the whole configuration.\nAfter generation extension controllers are asked to store their OS config inside a Secret (as it might contain confidential data) in the same namespace. The secret\u0026rsquo;s .data could look like this:\napiVersion:v1kind:Secretmetadata:name:osc-result-pool-01-originalnamespace:defaultownerReferences:- apiVersion:extensions.gardener.cloud/v1alpha1blockOwnerDeletion:truecontroller:truekind:OperatingSystemConfigname:pool-01-originaluid:99c0c5ca-19b9-11e9-9ebd-d67077b40f82data:cloud_config:base64(generated-user-data)Finally, the secret\u0026rsquo;s metadata, the OS-specific command to re-apply the configuration, and the list of systemd units that shall be considered to be restarted if an updated version of the user-data is re-applied must be provided in the OperatingSystemConfig's .status field:\n...status:cloudConfig:secretRef:name:osc-result-pool-01-originalnamespace:defaultcommand:/usr/bin/coreos-cloudinit--from-file=/var/lib/cloud-config-downloader/cloud-configlastOperation:description:SuccessfullygeneratedcloudconfiglastUpdateTime:\u0026#34;2019-01-23T07:45:23Z\u0026#34;progress:100state:Succeededtype:ReconcileobservedGeneration:5units:- docker-monitor.service(The .status.command field is optional and must only be provided if .spec.reloadConfigFilePath exists).\nOnce the .status indicates that the extension controller finished reconciling Gardener will continue with the next step of the shoot reconciliation flow.\nCRI Support Gardener supports specifying Container Runtime Interface (CRI) configuration in the OperatingSystemConfig resource. The only CRI supported at the moment is: \u0026ldquo;containerd\u0026rdquo;. For example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:OperatingSystemConfigmetadata:name:pool-01-originalnamespace:defaultspec:type:\u0026lt;my-operating-system\u0026gt; purpose: reconcilereloadConfigFilePath:/var/lib/cloud-config-downloader/cloud-configcri:name:containerd...If the .spec.cri section exists then the name property is mandatory. The only valid value at the moment is containerd. When the .spec.cri field is declared the kubelet will be configured by Gardener to work with ContainerD. Gardener expects that ContainerD service is running on the OS with the default socket path: unix:///run/containerd/containerd.sock.\nEach OS extension must support the CRI configurations by:\n The operating system must have built-in ContainerD and the Client CLI ContainerD service should be configure to work with the default configuration file in: /etc/containerd.config.toml (Created by Gardener).  If CRI configurations are not supported it is recommended create a validating webhook running in the garden cluster that prevents specifying the .spec.providers.workers[].cri section in the Shoot objects.\nReferences and additional resources  OperatingSystemConfig API (Golang specification) downloader script (fetching the \u0026ldquo;original\u0026rdquo; user-data and the execution script) Original user-data templates Execution script (applying the \u0026ldquo;original\u0026rdquo; user-data)  "},{"uri":"https://gardener.cloud/documentation/concepts/monitoring/operator_alerts/","title":"Operator Alerts","tags":[],"description":"","content":"Operator Alerts    Alertname Severity Type Description     ApiServerUnreachableViaKubernetesService critical shoot The Api server has been unreachable for 3 minutes via the kubernetes service in the shoot.   CoreDNSDown critical shoot CoreDNS could not be found. Cluster DNS resolution will not work.   ApiServerNotReachable blocker seed API server not reachable via external endpoint: {{ $labels.instance }}.   KubeApiserverDown blocker seed All API server replicas are down/unreachable, or all API server could not be found.   KubeApiServerTooManyAuditlogFailures warning seed The API servers cumulative failure rate in logging audit events is greater than 2%.   KubeletTooManyOpenFileDescriptorsSeed critical seed Seed-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubePersistentVolumeUsageCritical critical seed The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} is only {{ printf \u0026quot;%0.2f\u0026quot; $value }}% free.   KubePersistentVolumeFullInFourDays warning seed Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ printf \u0026quot;%0.2f\u0026quot; $value }}% is available.   KubePodPendingControlPlane warning seed Pod {{ $labels.pod }} is stuck in \u0026quot;Pending\u0026quot; state for more than 30 minutes.   KubePodNotReadyControlPlane warning  Pod {{ $labels.pod }} is not ready for more than 30 minutes.   KubeStateMetricsShootDown info seed There are no running kube-state-metric pods for the shoot cluster. No kubernetes resource metrics can be scraped.   KubeStateMetricsSeedDown critical seed There are no running kube-state-metric pods for the seed cluster. No kubernetes resource metrics can be scraped.   NoWorkerNodes blocker  There are no worker nodes in the cluster or all of the worker nodes in the cluster are not schedulable.   PrometheusCantScrape warning seed Prometheus failed to scrape metrics. Instance {{ $labels.instance }}, job {{ $labels.job }}.   PrometheusConfigurationFailure warning seed Latest Prometheus configuration is broken and Prometheus is using the previous one.   VPNShootNoPods critical shoot vpn-shoot deployment in Shoot cluster has 0 available pods. VPN won't work.   VPNProbeAPIServerProxyFailed critical shoot The API Server proxy functionality is not working. Probably the vpn connection from an API Server pod to the vpn-shoot endpoint on the Shoot workers does not work.    "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/referenced-resources/","title":"Referenced Resources","tags":[],"description":"","content":"Referenced Resources The Shoot resource can include a list of resources (usually secrets) that can be referenced by name in extension providerConfig and other Shoot sections, for example:\nkind:ShootapiVersion:core.gardener.cloud/v1beta1metadata:name:crazy-botanynamespace:garden-dev...spec:...extensions:- type:foobarproviderConfig:apiVersion:foobar.extensions.gardener.cloud/v1alpha1kind:FooBarConfigfoo:barsecretRef:foobar-secretresources:- name:foobar-secretresourceRef:apiVersion:v1kind:Secretname:my-foobar-secretGardener expects to find these referenced resources in the project namespace (e.g. garden-dev) and will copy them to the Shoot namespace in the Seed cluster when reconciling a Shoot, adding a prefix to their names to avoid naming collisions with Gardener\u0026rsquo;s own resources.\nExtension controllers can resolve the references to these resources by accessing the Shoot via the Cluster resource. To properly read a referenced resources, extension controllers should use the utility function GetObjectByReference from the extensions/pkg/controller package, for example:\n... ref = \u0026amp;autoscalingv1.CrossVersionObjectReference{ APIVersion: \u0026#34;v1\u0026#34;, Kind: \u0026#34;Secret\u0026#34;, Name: \u0026#34;foo\u0026#34;, } secret := \u0026amp;corev1.Secret{} if err := controller.GetObjectByReference(ctx, client, ref, \u0026#34;shoot--test--foo\u0026#34;, secret); err != nil { return err } // Use secret  ... "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/controllerregistration/","title":"Registering Extension Controllers","tags":[],"description":"","content":"Registering Extension Controllers Extensions are registered in the garden cluster via ControllerRegistration resources. Gardener is evaluating the registrations and creates ControllerInstallation resources which describe the request \u0026ldquo;please install this controller X to this seed Y\u0026rdquo;.\nSimilar to how CloudProfile or Seed resources get into the system, the Gardener administrator must deploy the ControllerRegistration resources (this does not happen automatically in any way - the administrator decides which extensions shall be enabled).\nThe specification mainly describes which of Gardener\u0026rsquo;s extension CRDs are managed, for example:\napiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationmetadata:name:os-coreosspec:resources:- kind:OperatingSystemConfigtype:coreosprimary:trueThis information tells Gardener that there is an extension controller that can handle OperatingSystemConfig resources of type coreos.\nAlso, it specifies that this controller is the primary one responsible for the lifecycle of the OperatingSystemConfig resource. Setting primary to false would allow to register additional, secondary controllers that may also watch/react on the OperatingSystemConfig/coreos resources, however, only the primary controller may change/update the main status of the extension object (that are used to \u0026ldquo;communicate\u0026rdquo; with the Gardenlet). Particularly, only the primary controller may set .status.lastOperation, .status.lastError, .status.observedGeneration, and .status.state. Secondary controllers may contribute to the .status.conditions[] if they like, of course.\nSecondary controllers might be helpful in scenarios where additional tasks need to be completed which are not part of the reconciliation logic of the primary controller but separated out into a dedicated extension.\n‚ö†Ô∏è There must be exactly one primary controller for every registered kind/type combination. Also, please note that the primary field cannot be changed after creation of the ControllerRegistration.\nDeploying Extension Controllers Submitting above ControllerRegistration will create a ControllerInstallation resource:\napiVersion:core.gardener.cloud/v1beta1kind:ControllerInstallationmetadata:name:os-coreosspec:registrationRef:apiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationname:os-coreosseedRef:apiVersion:core.gardener.cloud/v1beta1kind:Seedname:aws-eu1This resource expresses that Gardener requires the os-coreos extension controller to run on the aws-eu1 seed cluster.\nThe Gardener Controller Manager does automatically determine which extension is required on which seed cluster and will only create ControllerInstallation objects for those. Also, it will automatically delete ControllerInstallations referencing extension controllers that are no longer required on a seed (e.g., because all shoots on it have been deleted). There are additional configuration options, please see this section.\nHow do extension controllers get deployed to seeds? After Gardener has written the ControllerInstallation resource some component must satisfy this request and start deploying the extension controller to the seed. Depending on the complexity of the controllers lifecycle management, configuration, etc. there are two possible scenarios:\nScenario 1: Deployed by Gardener In many cases the extension controllers are easy to deploy and configure. It is sufficient to simply create a Helm chart (standardized way of packaging software in the Kubernetes context) and deploy it together with some static configuration values. Gardener supports this scenario and allows to provide arbitrary deployment information in the ControllerRegistration resource\u0026rsquo;s .spec section:\n...spec:...deployment:type:helmproviderConfig:chart:H4sIFAAAAAAA/yk...values:foo:barIf .spec.deployment.type=helm then Gardener itself will take over the responsibility the deployment. It base64-decodes the provided Helm chart (.spec.deployment.providerConfig.chart) and deploys it with the provided static configuration (.spec.deployment.providerConfig.values). The chart and the values can be updated at any time - Gardener will recognize and re-trigger the deployment process.\nIn order to allow extensions to get information about the garden and the seed cluster Gardener does mix-in certain properties into the values (root level) of every deployed Helm chart:\ngardener:garden:identifier:\u0026lt;uuid-of-gardener-installation\u0026gt; seed:identifier:\u0026lt;seed-name\u0026gt; region: europespec:\u0026lt;complete-seed-spec\u0026gt;Extensions can use this information in their Helm chart in case they require knowledge about the garden and the seed environment. The list might be extended in the future.\n:information_source: Gardener uses the UUID of the garden Namespace object in the .gardener.garden.identifier property.\nScenario 2: Deployed by a (non-human) Kubernetes operator Some extension controllers might be more complex and require additional domain-specific knowledge wrt. lifecycle or configuration. In this case, we encourage to follow the Kubernetes operator pattern and deploy a dedicated operator for this extension into the garden cluster. The ControllerResource's .spec.deployment.type field would then not be helm, and no Helm chart or values need to be provided there. Instead, the operator itself knows how to deploy the extension into the seed. It must watch ControllerInstallation resources and act one those referencing a ControllerRegistration the operator is responsible for.\nIn order to let Gardener know that the extension controller is ready and running in the seed the ControllerInstallation's .status field supports two conditions: RegistrationValid and InstallationSuccessful - both must be provided by the responsible operator:\n...status:conditions:- lastTransitionTime:\u0026#34;2019-01-22T11:51:11Z\u0026#34;lastUpdateTime:\u0026#34;2019-01-22T11:51:11Z\u0026#34;message:Chartcouldberenderedsuccessfully.reason:RegistrationValidstatus:\u0026#34;True\u0026#34;type:Valid- lastTransitionTime:\u0026#34;2019-01-22T11:51:12Z\u0026#34;lastUpdateTime:\u0026#34;2019-01-22T11:51:12Z\u0026#34;message:Installationofnewresourcessucceeded.reason:InstallationSuccessfulstatus:\u0026#34;True\u0026#34;type:InstalledAdditionally, the .status field has a providerStatus section into which the operator can (optionally) put any arbitrary data associated with this installation.\nExtensions in the garden cluster itself The Shoot resource itself will contain some provider-specific data blobs. As a result, some extensions might also want to run in the garden cluster, e.g., to provide ValidatingWebhookConfigurations for validating the correctness of their provider-specific blobs:\napiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:name:johndoe-awsnamespace:garden-devspec:...cloud:type:awsregion:eu-west-1providerConfig:apiVersion:aws.cloud.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:vpc:# specify either \u0026#39;id\u0026#39; or \u0026#39;cidr\u0026#39;# id: vpc-123456cidr:10.250.0.0/16internal:- 10.250.112.0/22public:- 10.250.96.0/22workers:- 10.250.0.0/19zones:- eu-west-1a...In the above example, Gardener itself does not understand the AWS-specific provider configuration for the infrastructure. However, if this part of the Shoot resource should be validated then you should run an AWS-specific component in the garden cluster that registers a webhook. You can do it similarly if you want to default some fields of a resource (by using a MutatingWebhookConfiguration).\nAgain, similar to how Gardener is deployed to the garden cluster, these components must be deployed and managed by the Gardener administrator.\nExtension resource configurations The Extension resource allows injecting arbitrary steps into the shoot reconciliation flow that are unknown to Gardener. Hence, it is slightly special and allows further configuration when registering it:\napiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationmetadata:name:extension-foospec:resources:- kind:Extensiontype:fooprimary:truegloballyEnabled:truereconcileTimeout:30sThe globallyEnabled=true option specifies that the Extension/foo object shall be created by default for all shoots (unless they opted out by setting .spec.extensions[].enabled=false in the Shoot spec).\nThe reconcileTimeout tells Gardener how long it should wait during its shoot reconciliation flow for the Extension/foo's reconciliation to finish.\nDeployment configuration options The .spec.deployment resource allows to configure a deployment policy. There are the following policies:\n OnDemand (default): Gardener will demand the deployment and deletion of the extension controller to/from seed clusters dynamically. It will automatically determine (based on other resources like Shoots) whether it is required and decide accordingly. Always: Gardener will demand the deployment of the extension controller to seed clusters independent of whether it is actually required or not. This might be helpful if you want to add a new component/controller to all seed clusters by default. Another use-case is to minimize the durations until extension controllers get deployed and ready in case you have highly fluctuating seed clusters. AlwaysExceptNoShoots: Similar to Always, but if the seed does not have any shoots then the extension is not being deployed. It will be deleted from a seed after the last shoot has been removed from it.  Also, the .spec.deployment.seedSelector allows to specify a label selector for seed clusters. Only if it matches the labels of a seed then it will be deployed to it. Please note that a seed selector can only be specified for secondary controllers (primary=false for all .spec.resources[]).\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/shoot-maintenance/","title":"Shoot maintenance","tags":[],"description":"","content":"Shoot maintenance There is a general document about shoot maintenance that you might want to read. Here, we describe how you can influence certain operations that happen during a shoot maintenance.\nRestart Control Plane Controllers As outlined in above linked document, Gardener offers to restart certain control plane controllers running in the seed during a shoot maintenance.\nExtension controllers can extend the amount of pods being affected by these restarts. If your Gardener extension manages pods of a shoot\u0026rsquo;s control plane (shoot namespace in seed) and it could potentially profit from a regular restart please consider labeling it with maintenance.gardener.cloud/restart=true.\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/shoot-webhooks/","title":"Shoot resource customization webhooks","tags":[],"description":"","content":"Shoot resource customization webhooks Gardener deploys several components/resources into the shoot cluster. Some of these resources are essential (like the kube-proxy), others are optional addons (like the kubernetes-dashboard or the nginx-ingress-controller). In either case, some provider extensions might need to mutate these resources and inject provider-specific bits into it.\nWhat\u0026rsquo;s the approach to implement such mutations? Similar to how control plane components in the seed are modified we are using MutatingWebhookConfigurations to achieve the same for resources in the shoot. Both, the provider extension and the kube-apiserver of the shoot cluster are running in the same seed. Consequently, the kube-apiserver can talk cluster-internally to the provider extension webhook which makes such operations even faster.\nHow is the MutatingWebhookConfiguration object created in the shoot? The preferred approach is to use a ManagedResource (see also this document) in the seed cluster. This way the gardener-resource-manager ensures that end-users cannot delete/modify the webhook configuration. The provider extension doesn\u0026rsquo;t need to care about the same.\nWhat else is needed? The shoot\u0026rsquo;s kube-apiserver must be allowed to talk to the provider extension. To achieve this you need to create a NetworkPolicy in the shoot namespace. Our extension controller library provides easy-to-use utilities and hooks to implement such a webhook. Please find an exemplary implementation here and here.\n"},{"uri":"https://gardener.cloud/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/concepts/monitoring/user_alerts/","title":"User Alerts","tags":[],"description":"","content":"User Alerts    Alertname Severity Type Description     ApiServerUnreachableViaKubernetesService critical shoot The Api server has been unreachable for 3 minutes via the kubernetes service in the shoot.   CoreDNSDown critical shoot CoreDNS could not be found. Cluster DNS resolution will not work.   ApiServerNotReachable blocker seed API server not reachable via external endpoint: {{ $labels.instance }}.   KubeApiServerTooManyOpenFileDescriptors warning seed The API server ({{ $labels.instance }}) is using {{ $value }}% of the available file/socket descriptors.   KubeApiServerTooManyOpenFileDescriptors critical seed The API server ({{ $labels.instance }}) is using {{ $value }}% of the available file/socket descriptors.   KubeApiServerLatency warning seed Kube API server latency for verb {{ $labels.verb }} is high. This could be because the shoot workers and the control plane are in different regions. 99th percentile of request latency is greater than 3 seconds.   KubeKubeletNodeDown warning shoot The kubelet {{ $labels.instance }} has been unavailable/unreachable for more than 1 hour. Workloads on the affected node may not be schedulable.   KubeletTooManyOpenFileDescriptorsShoot warning shoot Shoot-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubeletTooManyOpenFileDescriptorsShoot critical shoot Shoot-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubePodPendingShoot warning shoot Pod {{ $labels.pod }} is stuck in \u0026quot;Pending\u0026quot; state for more than 1 hour.   KubePodNotReadyShoot warning shoot Pod {{ $labels.pod }} is not ready for more than 1 hour.   NoWorkerNodes blocker  There are no worker nodes in the cluster or all of the worker nodes in the cluster are not schedulable.   NodeExporterDown warning shoot The NodeExporter has been down or unreachable from Prometheus for more than 1 hour.   K8SNodeOutOfDisk critical shoot Node {{ $labels.node }} has run out of disk space.   K8SNodeMemoryPressure warning shoot Node {{ $labels.node }} is under memory pressure.   K8SNodeDiskPressure warning shoot Node {{ $labels.node }} is under disk pressure   VMRootfsFull critical shoot Root filesystem device on instance {{ $labels.instance }} is almost full.   VMConntrackTableFull critical shoot The nf_conntrack table is {{ $value }}% full.   VPNProbeAPIServerProxyFailed critical shoot The API Server proxy functionality is not working. Probably the vpn connection from an API Server pod to the vpn-shoot endpoint on the Shoot workers does not work.    "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/worker/","title":"Worker resource","tags":[],"description":"","content":"Contract: Worker resource While the control plane of a shoot cluster is living in the seed and deployed as native Kubernetes workload, the worker nodes of the shoot clusters are normal virtual machines (VMs) in the end-users infrastructure account. The Gardener project features a sub-project called machine-controller-manager. This controller is extending the Kubernetes API using custom resource definitions to represent actual VMs as Machine objects inside a Kubernetes system. This approach unlocks the possibility to manage virtual machines in the Kubernetes style and benefit from all its design principles.\nWhat is the machine-controller-manager exactly doing? Generally, there are provider-specific MachineClass objects (AWSMachineClass, AzureMachineClass, etc.; similar to StorageClass), and MachineDeployment, MachineSet, and Machine objects (similar to Deployment, ReplicaSet, and Pod). A machine class describes where and how to create virtual machines (in which networks, region, availability zone, SSH key, user-data for bootstrapping, etc.) while a Machine results in an actual virtual machine. You can read up more information in the machine-controller-manager\u0026rsquo;s repository.\nBefore the introduction of the Worker extension resource Gardener was deploying the machine-controller-manager, the machine classes, and the machine deployments itself. Now, Gardener commissions an external, provider-specific controller to take over these tasks.\nWhat needs to be implemented to support a new worker provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Workermetadata:name:barnamespace:shoot--foo--barspec:type:azureregion:eu-west-1secretRef:name:cloudprovidernamespace:shoot--foo--barinfrastructureProviderStatus:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusec2:keyName:shoot--foo--bar-ssh-publickeyiam:instanceProfiles:- name:shoot--foo--bar-nodespurpose:nodesroles:- arn:arn:aws:iam::0123456789:role/shoot--foo--bar-nodespurpose:nodesvpc:id:vpc-0123456789securityGroups:- id:sg-1234567890purpose:nodessubnets:- id:subnet-01234purpose:nodeszone:eu-west-1b- id:subnet-56789purpose:publiczone:eu-west-1b- id:subnet-0123apurpose:nodeszone:eu-west-1c- id:subnet-5678apurpose:publiczone:eu-west-1cpools:- name:cpu-workerminimum:3maximum:5maxSurge:1maxUnavailable:0machineType:m4.largemachineImage:name:coreosversion:1967.5.0userData:c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0Kvolume:size:20Gitype:gp2zones:- eu-west-1b- eu-west-1cmachineControllerManager:drainTimeout:10mhealthTimeout:10mcreationTimeout:10mmaxEvictRetries:30nodeConditions:- ReadonlyFilesystem- DiskPressure- KernelDeadlockThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed virtual machines. Also, as you can see, Gardener copies the output of the infrastructure creation (.spec.infrastructureProviderStatus), see Infrastructure resource, into the .spec.\nIn the .spec.pools[] field the desired worker pools are listed. In the above example, one pool with machine type m4.large and min=3, max=5 machines shall be spread over two availability zones (eu-west-1b, eu-west-1c). This information together with the infrastructure status must be used to determine the proper configuration for the machine classes.\nThe spec.pools[].machineControllerManager field allows to configure the settings for machine-controller-manager component. Providers must populate these settings on worker-pool to the related fields in MachineDeployment.\nWhen seeing such a resource your controller must make sure that it deploys the machine-controller-manager next to the control plane in the seed cluster. After that, it must compute the desired machine classes and the desired machine deployments. Typically, one class maps to one deployment, and one class/deployment is created per availability zone. Following this convention, the created resource would look like this:\napiVersion:v1kind:Secretmetadata:name:shoot--foo--bar-cpu-worker-z1-3db65namespace:shoot--foo--barlabels:garden.sapcloud.io/purpose:machineclasstype:Opaquedata:providerAccessKeyId:eW91ci1hd3MtYWNjZXNzLWtleS1pZAo=providerSecretAccessKey:eW91ci1hd3Mtc2VjcmV0LWFjY2Vzcy1rZXkKuserData:c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0K---apiVersion:machine.sapcloud.io/v1alpha1kind:AWSMachineClassmetadata:name:shoot--foo--bar-cpu-worker-z1-3db65namespace:shoot--foo--barspec:ami:ami-0123456789# Your controller must map the stated version to the provider specific machine image information, in the AWS case the AMI.blockDevices:- ebs:volumeSize:20volumeType:gp2iam:name:shoot--foo--bar-nodeskeyName:shoot--foo--bar-ssh-publickeymachineType:m4.largenetworkInterfaces:- securityGroupIDs:- sg-1234567890subnetID:subnet-01234region:eu-west-1secretRef:name:shoot--foo--bar-cpu-worker-z1-3db65namespace:shoot--foo--bartags:kubernetes.io/cluster/shoot--foo--bar:\u0026#34;1\u0026#34;kubernetes.io/role/node:\u0026#34;1\u0026#34;---apiVersion:machine.sapcloud.io/v1alpha1kind:MachineDeploymentmetadata:name:shoot--foo--bar-cpu-worker-z1namespace:shoot--foo--barspec:replicas:2selector:matchLabels:name:shoot--foo--bar-cpu-worker-z1strategy:type:RollingUpdaterollingUpdate:maxSurge:1maxUnavailable:0template:metadata:labels:name:shoot--foo--bar-cpu-worker-z1spec:class:kind:AWSMachineClassname:shoot--foo--bar-cpu-worker-z1-3db65for the first availability zone eu-west-1b, and\napiVersion:v1kind:Secretmetadata:name:shoot--foo--bar-cpu-worker-z2-5z6asnamespace:shoot--foo--barlabels:garden.sapcloud.io/purpose:machineclasstype:Opaquedata:providerAccessKeyId:eW91ci1hd3MtYWNjZXNzLWtleS1pZAo=providerSecretAccessKey:eW91ci1hd3Mtc2VjcmV0LWFjY2Vzcy1rZXkKuserData:c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0K---apiVersion:machine.sapcloud.io/v1alpha1kind:AWSMachineClassmetadata:name:shoot--foo--bar-cpu-worker-z2-5z6asnamespace:shoot--foo--barspec:ami:ami-0123456789# Your controller must map the stated version to the provider specific machine image information, in the AWS case the AMI.blockDevices:- ebs:volumeSize:20volumeType:gp2iam:name:shoot--foo--bar-nodeskeyName:shoot--foo--bar-ssh-publickeymachineType:m4.largenetworkInterfaces:- securityGroupIDs:- sg-1234567890subnetID:subnet-0123aregion:eu-west-1secretRef:name:shoot--foo--bar-cpu-worker-z2-5z6asnamespace:shoot--foo--bartags:kubernetes.io/cluster/shoot--foo--bar:\u0026#34;1\u0026#34;kubernetes.io/role/node:\u0026#34;1\u0026#34;---apiVersion:machine.sapcloud.io/v1alpha1kind:MachineDeploymentmetadata:name:shoot--foo--bar-cpu-worker-z1namespace:shoot--foo--barspec:replicas:1selector:matchLabels:name:shoot--foo--bar-cpu-worker-z1strategy:type:RollingUpdaterollingUpdate:maxSurge:1maxUnavailable:0template:metadata:labels:name:shoot--foo--bar-cpu-worker-z1spec:class:kind:AWSMachineClassname:shoot--foo--bar-cpu-worker-z2-5z6asfor the second availability zone eu-west-1c.\nAnother convention is the 5-letter hash at the end of the machine class names. Most controllers compute a checksum out of the specification of the machine class. This helps to trigger a rolling update of the worker nodes if, for example, the machine image version changes. In this case, a new checksum will be generated which results in the creation of a new machine class. The MachineDeployment's machine class reference (.spec.template.spec.class.name) is updated which triggers the rolling update process in the machine-controller-manager. However, all of this is only a convention that eases writing the controller, but you can do it completely differently if you desire - as long as you make sure that the described behaviours are implemented correctly.\nAfter the machine classes and machine deployments have been created the machine-controller-manager will start talking to the provider\u0026rsquo;s IaaS API and create the virtual machines. Gardener makes sure that the content of the userData field that is used to bootstrap the machines contain the required configuration for installation of the kubelet and registering the VM as worker node in the shoot cluster. The Worker extension controller shall wait until all the created MachineDeployments indicate healthiness/readiness before it ends the control loop.\nDoes Gardener need some information that must be returned back? Another important benefit of the machine-controller-manager\u0026rsquo;s design principles (extending the Kubernetes API using CRDs) is that the cluster-autoscaler can be used without any provider-specific implementation. We have forked the upstream Kubernetes community\u0026rsquo;s cluster-autoscaler and extended it so that it understands the machine API. Definitely, we will merge it back into the community\u0026rsquo;s versions once it has been adapted properly.\nOur cluster-autoscaler only needs to know the minimum and maximum number of replicas per MachineDeployment and is ready to act without that it needs to talk to the provider APIs (it just modifies the .spec.replicas field in the MachineDeployment object). Gardener deploys this autoscaler if there is at least one worker pool that specifies max\u0026gt;min. In order to know how it needs to configure it, the provider-specific Worker extension controller must expose which MachineDeployments it had created and how the min/max numbers should look like.\nConsequently, your controller should write this information into the Worker resource\u0026rsquo;s .status.machineDeployments field:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Workermetadata:name:workernamespace:shoot--foo--barspec:...status:lastOperation:...machineDeployments:- name:shoot--foo--bar-cpu-worker-z1minimum:2maximum:3- name:shoot--foo--bar-cpu-worker-z2minimum:1maximum:2In order to support a new worker provider you need to write a controller that watches all Workers with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the AWS provider.\nThat sounds like a lot that needs to be done, can you help me? All of the described behaviour is mostly the same for every provider. The only difference is maybe the version/configuration of the machine-controller-manager, and the machine class specification itself. You can take a look at our extension library, especially the worker controller part where you will find a lot of utilities that you can use. Also, using the library you only need to implement your provider specifics - all the things that can be handled generically can be taken for free and do not need to be re-implemented. Take a look at the AWS worker controller for finding an example.\nNon-provider specific information required for worker creation All the providers require further information that is not provider specific but already part of the shoot resource. One example for such information is whether the shoot is hibernated or not. In this case all the virtual machines should be deleted/terminated, and after that the machine controller-manager should be scaled down. You can take a look at the AWS worker controller to see how it reads this information and how it is used. As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information that is not part of the Worker resource itself.\nReferences and additional resources  Worker API (Golang specification) Extension controller library Generic worker controller Exemplary implementation for the AWS provider  "}]