<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener â€“ Deployment</title><link>https://gardener.cloud/docs/other-components/dependency-watchdog/deployment/</link><description>Recent content in Deployment on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://gardener.cloud/docs/other-components/dependency-watchdog/deployment/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Configure</title><link>https://gardener.cloud/docs/other-components/dependency-watchdog/deployment/configure/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/dependency-watchdog/deployment/configure/</guid><description>
&lt;h1 id="configure-dependency-watchdog-components">Configure Dependency Watchdog Components&lt;/h1>
&lt;h2 id="prober">Prober&lt;/h2>
&lt;p>Dependency watchdog prober command takes command-line-flags which are meant to fine-tune the prober. In addition a &lt;code>ConfigMap&lt;/code> is also mounted to the container which provides tuning knobs for the all probes that the prober starts.&lt;/p>
&lt;h3 id="command-line-arguments">Command line arguments&lt;/h3>
&lt;p>Prober can be configured via the following flags:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Flag Name&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Required&lt;/th>
&lt;th>Default Value&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>kube-api-burst&lt;/td>
&lt;td>int&lt;/td>
&lt;td>No&lt;/td>
&lt;td>10&lt;/td>
&lt;td>Burst to use while talking with kubernetes API server. The number must be &amp;gt;= 0. If it is 0 then a default value of 10 will be used&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kube-api-qps&lt;/td>
&lt;td>float&lt;/td>
&lt;td>No&lt;/td>
&lt;td>5.0&lt;/td>
&lt;td>Maximum QPS (queries per second) allowed when talking with kubernetes API server. The number must be &amp;gt;= 0. If it is 0 then a default value of 5.0 will be used&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>concurrent-reconciles&lt;/td>
&lt;td>int&lt;/td>
&lt;td>No&lt;/td>
&lt;td>1&lt;/td>
&lt;td>Maximum number of concurrent reconciles&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>config-file&lt;/td>
&lt;td>string&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>NA&lt;/td>
&lt;td>Path of the config file containing the configuration to be used for all probes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>metrics-bind-addr&lt;/td>
&lt;td>string&lt;/td>
&lt;td>No&lt;/td>
&lt;td>&amp;ldquo;:9643&amp;rdquo;&lt;/td>
&lt;td>The TCP address that the controller should bind to for serving prometheus metrics&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>health-bind-addr&lt;/td>
&lt;td>string&lt;/td>
&lt;td>No&lt;/td>
&lt;td>&amp;ldquo;:9644&amp;rdquo;&lt;/td>
&lt;td>The TCP address that the controller should bind to for serving health probes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>enable-leader-election&lt;/td>
&lt;td>bool&lt;/td>
&lt;td>No&lt;/td>
&lt;td>false&lt;/td>
&lt;td>In case prober deployment has more than 1 replica for high availability, then it will be setup in a active-passive mode. Out of many replicas one will become the leader and the rest will be passive followers waiting to acquire leadership in case the leader dies.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>leader-election-namespace&lt;/td>
&lt;td>string&lt;/td>
&lt;td>No&lt;/td>
&lt;td>&amp;ldquo;garden&amp;rdquo;&lt;/td>
&lt;td>Namespace in which leader election resource will be created. It should be the same namespace where DWD pods are deployed&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>leader-elect-lease-duration&lt;/td>
&lt;td>time.Duration&lt;/td>
&lt;td>No&lt;/td>
&lt;td>15s&lt;/td>
&lt;td>The duration that non-leader candidates will wait after observing a leadership renewal until attempting to acquire leadership of a led but unrenewed leader slot. This is effectively the maximum duration that a leader can be stopped before it is replaced by another candidate. This is only applicable if leader election is enabled.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>leader-elect-renew-deadline&lt;/td>
&lt;td>time.Duration&lt;/td>
&lt;td>No&lt;/td>
&lt;td>10s&lt;/td>
&lt;td>The interval between attempts by the acting master to renew a leadership slot before it stops leading. This must be less than or equal to the lease duration. This is only applicable if leader election is enabled.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>leader-elect-retry-period&lt;/td>
&lt;td>time.Duration&lt;/td>
&lt;td>No&lt;/td>
&lt;td>2s&lt;/td>
&lt;td>The duration the clients should wait between attempting acquisition and renewal of a leadership. This is only applicable if leader election is enabled.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>You can view an example kubernetes prober &lt;a href="https://github.com/gardener/dependency-watchdog/blob/master/example/03-dwd-prober-deployment.yaml">deployment&lt;/a> YAML to see how these command line args are configured.&lt;/p>
&lt;h3 id="prober-configuration">Prober Configuration&lt;/h3>
&lt;p>A probe configuration is mounted as &lt;code>ConfigMap&lt;/code> to the container. The path to the config file is configured via &lt;code>config-file&lt;/code> command line argument as mentioned above. Prober will start one probe per Shoot control plane hosted within the Seed cluster. Each such probe will run asynchronously and will periodically connect to the Kube ApiServer of the Shoot. Configuration below will influence each such probe.&lt;/p>
&lt;p>You can view an example YAML configuration provided as &lt;code>data&lt;/code> in a &lt;code>ConfigMap&lt;/code> &lt;a href="https://github.com/gardener/dependency-watchdog/blob/master/example/01-dwd-prober-configmap.yaml">here&lt;/a>.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Required&lt;/th>
&lt;th>Default Value&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>kubeConfigSecretName&lt;/td>
&lt;td>string&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>NA&lt;/td>
&lt;td>Name of the kubernetes Secret which has the encoded KubeConfig required to connect to the Shoot control plane Kube ApiServer via an internal domain. This typically uses the local cluster DNS.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>probeInterval&lt;/td>
&lt;td>metav1.Duration&lt;/td>
&lt;td>No&lt;/td>
&lt;td>10s&lt;/td>
&lt;td>Interval with which each probe will run.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>initialDelay&lt;/td>
&lt;td>metav1.Duration&lt;/td>
&lt;td>No&lt;/td>
&lt;td>30s&lt;/td>
&lt;td>Initial delay for the probe to become active. Only applicable when the probe is created for the first time.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>probeTimeout&lt;/td>
&lt;td>metav1.Duration&lt;/td>
&lt;td>No&lt;/td>
&lt;td>30s&lt;/td>
&lt;td>In each run of the probe it will attempt to connect to the Shoot Kube ApiServer. probeTimeout defines the timeout after which a single run of the probe will fail.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>backoffJitterFactor&lt;/td>
&lt;td>float64&lt;/td>
&lt;td>No&lt;/td>
&lt;td>0.2&lt;/td>
&lt;td>Jitter with which a probe is run.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>dependentResourceInfos&lt;/td>
&lt;td>[]prober.DependentResourceInfo&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>NA&lt;/td>
&lt;td>Detailed below.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kcmNodeMonitorGraceDuration&lt;/td>
&lt;td>metav1.Duration&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>NA&lt;/td>
&lt;td>It is the node-monitor-grace-period set in the kcm flags. Used to determine whether a node lease can be considered expired.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>nodeLeaseFailureFraction&lt;/td>
&lt;td>float64&lt;/td>
&lt;td>No&lt;/td>
&lt;td>0.6&lt;/td>
&lt;td>is used to determine the maximum number of leases that can be expired for a lease probe to succeed.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="dependentresourceinfo">DependentResourceInfo&lt;/h3>
&lt;p>If a lease probe fails, then it scales down the dependent resources defined by this property. Similarly, if the lease probe is now successful, then it scales up the dependent resources defined by this property.&lt;/p>
&lt;p>Each dependent resource info has the following properties:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Required&lt;/th>
&lt;th>Default Value&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ref&lt;/td>
&lt;td>autoscalingv1.CrossVersionObjectReference&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>NA&lt;/td>
&lt;td>It is a collection of ApiVersion, Kind and Name for a kubernetes resource thus serving as an identifier.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>optional&lt;/td>
&lt;td>bool&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>NA&lt;/td>
&lt;td>It is possible that a dependent resource is optional for a Shoot control plane. This property enables a probe to determine the correct behavior in case it is unable to find the resource identified via &lt;code>ref&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>scaleUp&lt;/td>
&lt;td>prober.ScaleInfo&lt;/td>
&lt;td>No&lt;/td>
&lt;td>&lt;/td>
&lt;td>Captures the configuration to scale up this resource. Detailed below.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>scaleDown&lt;/td>
&lt;td>prober.ScaleInfo&lt;/td>
&lt;td>No&lt;/td>
&lt;td>&lt;/td>
&lt;td>Captures the configuration to scale down this resource. Detailed below.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>NOTE: Since each dependent resource is a target for scale up/down, therefore it is mandatory that the resource reference points a kubernetes resource which has a &lt;code>scale&lt;/code> subresource.&lt;/p>
&lt;/blockquote>
&lt;h3 id="scaleinfo">ScaleInfo&lt;/h3>
&lt;p>How to scale a &lt;code>DependentResourceInfo&lt;/code> is captured in &lt;code>ScaleInfo&lt;/code>. It has the following properties:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Required&lt;/th>
&lt;th>Default Value&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>level&lt;/td>
&lt;td>int&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>NA&lt;/td>
&lt;td>Detailed below.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>initialDelay&lt;/td>
&lt;td>metav1.Duration&lt;/td>
&lt;td>No&lt;/td>
&lt;td>0s (No initial delay)&lt;/td>
&lt;td>Once a decision is taken to scale a resource then via this property a delay can be induced before triggering the scale of the dependent resource.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>timeout&lt;/td>
&lt;td>metav1.Duration&lt;/td>
&lt;td>No&lt;/td>
&lt;td>30s&lt;/td>
&lt;td>Defines the timeout for the scale operation to finish for a dependent resource.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Determining target replicas&lt;/strong>&lt;/p>
&lt;p>Prober cannot assume any target replicas during a scale-up operation for the following reasons:&lt;/p>
&lt;ol>
&lt;li>Kubernetes resources could be set to provide highly availability and the number of replicas could wary from one shoot control plane to the other. In gardener the number of replicas of pods in shoot namespace are controlled by the &lt;a href="https://gardener.cloud/docs/gardener/shoot_high_availability/">shoot control plane configuration&lt;/a>.&lt;/li>
&lt;li>If Horizontal Pod Autoscaler has been configured for a kubernetes dependent resource then it could potentially change the &lt;code>spec.replicas&lt;/code> for a deployment/statefulset.&lt;/li>
&lt;/ol>
&lt;p>Given the above constraint lets look at how prober determines the target replicas during scale-down or scale-up operations.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;code>Scale-Up&lt;/code>: Primary responsibility of a probe while performing a scale-up is to restore the replicas of a kubernetes dependent resource prior to scale-down. In order to do that it updates the following for each dependent resource that requires a scale-up:&lt;/p>
&lt;ol>
&lt;li>&lt;code>spec.replicas&lt;/code>: Checks if &lt;code>dependency-watchdog.gardener.cloud/replicas&lt;/code> is set. If it is, then it will take the value stored against this key as the target replicas. To be a valid value it should always be greater than 0.&lt;/li>
&lt;li>If &lt;code>dependency-watchdog.gardener.cloud/replicas&lt;/code> annotation is not present then it falls back to the hard coded default value for scale-up which is set to 1.&lt;/li>
&lt;li>Removes the annotation &lt;code>dependency-watchdog.gardener.cloud/replicas&lt;/code> if it exists.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Scale-Down&lt;/code>: To scale down a dependent kubernetes resource it does the following:&lt;/p>
&lt;ol>
&lt;li>Adds an annotation &lt;code>dependency-watchdog.gardener.cloud/replicas&lt;/code> and sets its value to the current value of &lt;code>spec.replicas&lt;/code>.&lt;/li>
&lt;li>Updates &lt;code>spec.replicas&lt;/code> to 0.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Level&lt;/strong>&lt;/p>
&lt;p>Each dependent resource that should be scaled up or down is associated to a level. Levels are ordered and processed in ascending order (starting with 0 assigning it the highest priority). Consider the following configuration:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>dependentResourceInfos:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - ref:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: &lt;span style="color:#a31515">&amp;#34;Deployment&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: &lt;span style="color:#a31515">&amp;#34;kube-controller-manager&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: &lt;span style="color:#a31515">&amp;#34;apps/v1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleUp:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> level: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDown:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> level: 0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - ref:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: &lt;span style="color:#a31515">&amp;#34;Deployment&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: &lt;span style="color:#a31515">&amp;#34;machine-controller-manager&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: &lt;span style="color:#a31515">&amp;#34;apps/v1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleUp:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> level: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDown:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> level: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - ref:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: &lt;span style="color:#a31515">&amp;#34;Deployment&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: &lt;span style="color:#a31515">&amp;#34;cluster-autoscaler&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: &lt;span style="color:#a31515">&amp;#34;apps/v1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleUp:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> level: 0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDown:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> level: 2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let us order the dependent resources by their respective levels for both scale-up and scale-down. We get the following order:&lt;/p>
&lt;p>&lt;em>Scale Up Operation&lt;/em>&lt;/p>
&lt;p>Order of scale up will be:&lt;/p>
&lt;ol>
&lt;li>cluster-autoscaler&lt;/li>
&lt;li>kube-controller-manager and machine-controller-manager will be scaled up concurrently after cluster-autoscaler has been scaled up.&lt;/li>
&lt;/ol>
&lt;p>&lt;em>Scale Down Operation&lt;/em>&lt;/p>
&lt;p>Order of scale down will be:&lt;/p>
&lt;ol>
&lt;li>kube-controller-manager&lt;/li>
&lt;li>machine-controller-manager after (1) has been scaled down.&lt;/li>
&lt;li>cluster-autoscaler after (2) has been scaled down.&lt;/li>
&lt;/ol>
&lt;h3 id="disableignore-scaling">Disable/Ignore Scaling&lt;/h3>
&lt;p>A probe can be configured to ignore scaling of configured dependent kubernetes resources.
To do that one must set &lt;code>dependency-watchdog.gardener.cloud/ignore-scaling&lt;/code> annotation to &lt;code>true&lt;/code> on the scalable resource for which scaling should be ignored.&lt;/p>
&lt;h2 id="weeder">Weeder&lt;/h2>
&lt;p>Dependency watchdog weeder command also (just like the prober command) takes command-line-flags which are meant to fine-tune the weeder. In addition a &lt;code>ConfigMap&lt;/code> is also mounted to the container which helps in defining the dependency of pods on endpoints.&lt;/p>
&lt;h3 id="command-line-arguments-1">Command Line Arguments&lt;/h3>
&lt;p>Weeder can be configured with the same flags as that for prober described under &lt;a href="https://gardener.cloud/docs/other-components/dependency-watchdog/deployment/configure/#command-line-arguments">command-line-arguments&lt;/a> section
You can find an example weeder &lt;a href="https://github.com/gardener/dependency-watchdog/blob/master/example/04-dwd-weeder-deployment.yaml">deployment&lt;/a> YAML to see how these command line args are configured.&lt;/p>
&lt;h3 id="weeder-configuration">Weeder Configuration&lt;/h3>
&lt;p>Weeder configuration is mounted as &lt;code>ConfigMap&lt;/code> to the container. The path to the config file is configured via &lt;code>config-file&lt;/code> command line argument as mentioned above. Weeder will start one go routine per podSelector per endpoint on an endpoint event as described in &lt;a href="https://gardener.cloud/docs/other-components/dependency-watchdog/concepts/weeder/#internals">weeder internal concepts&lt;/a>.&lt;/p>
&lt;p>You can view the example YAML configuration provided as &lt;code>data&lt;/code> in a &lt;code>ConfigMap&lt;/code> &lt;a href="https://github.com/gardener/dependency-watchdog/blob/master/example/02-dwd-weeder-configmap.yaml">here&lt;/a>.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Required&lt;/th>
&lt;th>Default Value&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>watchDuration&lt;/td>
&lt;td>*metav1.Duration&lt;/td>
&lt;td>No&lt;/td>
&lt;td>5m0s&lt;/td>
&lt;td>The time duration for which watch is kept on dependent pods to see if anyone turns to &lt;code>CrashLoopBackoff&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>servicesAndDependantSelectors&lt;/td>
&lt;td>map[string]DependantSelectors&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>NA&lt;/td>
&lt;td>Endpoint name and its corresponding dependent pods. More info below.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="dependantselectors">DependantSelectors&lt;/h3>
&lt;p>If the service recovers from downtime, then weeder starts to watch for CrashLoopBackOff pods. These pods are identified by info stored in this property.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Required&lt;/th>
&lt;th>Default Value&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>podSelectors&lt;/td>
&lt;td>[]*metav1.LabelSelector&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>NA&lt;/td>
&lt;td>This is a list of &lt;a href="https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1@v0.24.3#LabelSelector">Label selector&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>Docs: Monitor</title><link>https://gardener.cloud/docs/other-components/dependency-watchdog/deployment/monitor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/dependency-watchdog/deployment/monitor/</guid><description>
&lt;h1 id="monitoring">Monitoring&lt;/h1>
&lt;h2 id="work-in-progress">&lt;em>Work In Progress&lt;/em>&lt;/h2>
&lt;p>We will be introducing metrics for &lt;code>Dependency-Watchdog-Prober&lt;/code> and &lt;code>Dependency-Watchdog-Weeder&lt;/code>. These metrics will be pushed to prometheus. Once that is completed we will provide details on all the metrics that will be supported here.&lt;/p></description></item></channel></rss>