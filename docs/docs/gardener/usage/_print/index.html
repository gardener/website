<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.95.0"><link rel=canonical type=text/html href=https://gardener.cloud/docs/gardener/usage/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/gardener/usage/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Usage | Gardener</title><meta name=description content><meta property="og:title" content="Usage"><meta property="og:description" content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/gardener/usage/"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="Usage"><meta itemprop=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="Usage"><meta name=twitter:description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><link rel=preload href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css as=style><link href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7N3XF5XLGV"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7N3XF5XLGV",{anonymize_ip:!1})}</script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.3e2054575c737d14f099dfe5a5bd26b3.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/gardener/usage/>Return to the regular view of this page</a>.</p></div><h1 class=title>Usage</h1><div class=content></div></div><div class=td-content><h1 id=pg-5092bba8dce7cbbc8af74c9d34288317>1 - Hibernate a Cluster</h1><h1 id=hibernate-a-cluster>Hibernate a Cluster</h1><p>Clusters are only needed 24 hours a day if they run productive workload. So whenever you do development in a cluster, or just use it for tests or demo purposes, you can save a lot of money if you scale-down your Kubernetes resources whenever you don&rsquo;t need them. However, scaling them down manually can become time-consuming the more resources you have.</p><p>Gardener offers a clever way to automatically scale-down all resources to zero: cluster hibernation. You can either hibernate a cluster by pushing a button, or by defining a hibernation schedule.</p><blockquote><p>To save costs, it&rsquo;s recommended to define a hibernation schedule before the creation of a cluster. You can hibernate your cluster or wake up your cluster manually even if there&rsquo;s a schedule for its hibernation.</p></blockquote><ul><li><a href=#hibernate-a-cluster>Hibernate a Cluster</a><ul><li><a href=#what-is-hibernation>What Is Hibernation?</a></li><li><a href=#what-isnt-affected-by-the-hibernation>What Isn’t Affected by the Hibernation?</a></li><li><a href=#hibernate-your-cluster-manually>Hibernate Your Cluster Manually</a></li><li><a href=#wake-up-your-cluster-manually>Wake Up Your Cluster Manually</a></li><li><a href=#create-a-schedule-to-hibernate-your-cluster>Create a Schedule to Hibernate Your Cluster</a></li></ul></li></ul><h2 id=what-is-hibernation>What Is Hibernation?</h2><p>When a cluster is hibernated, Gardener scales down the worker nodes and the cluster&rsquo;s control plane to free resources at the IaaS provider. This affects:</p><ul><li>Your workload, for example, pods, deployments, custom resources.</li><li>The virtual machines running your workload.</li><li>The resources of the control plane of your cluster.</li></ul><h2 id=what-isnt-affected-by-the-hibernation>What Isn’t Affected by the Hibernation?</h2><p>To scale up everything where it was before hibernation, Gardener doesn’t delete state-related information, that is, information stored in persistent volumes. The cluster state as persistent in <code>etcd</code> is also preserved.</p><h2 id=hibernate-your-cluster-manually>Hibernate Your Cluster Manually</h2><p>The <code>.spec.hibernation.enabled</code> field specifies whether the cluster needs to be hibernated or not. If the field is set to <code>true</code>, the cluster&rsquo;s desired state is to be hibernated. If it is set to <code>false</code> or not specified at all, the cluster&rsquo;s desired state is to be awakened.</p><p>To hibernate your cluster, you can run the following <code>kubectl</code> command:</p><pre tabindex=0><code>$ kubectl patch shoot -n $NAMESPACE $SHOOT_NAME -p &#39;{&#34;spec&#34;:{&#34;hibernation&#34;:{&#34;enabled&#34;: true}}}&#39;
</code></pre><h2 id=wake-up-your-cluster-manually>Wake Up Your Cluster Manually</h2><p>To wake up your cluster, you can run the following <code>kubectl</code> command:</p><pre tabindex=0><code>$ kubectl patch shoot -n $NAMESPACE $SHOOT_NAME -p &#39;{&#34;spec&#34;:{&#34;hibernation&#34;:{&#34;enabled&#34;: false}}}&#39;
</code></pre><h2 id=create-a-schedule-to-hibernate-your-cluster>Create a Schedule to Hibernate Your Cluster</h2><p>You can specify a hibernation schedule to automatically hibernate/wake up a cluster.</p><p>Let&rsquo;s have a look into the following example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  hibernation:
</span></span><span style=display:flex><span>    enabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>    schedules:
</span></span><span style=display:flex><span>    - start: <span style=color:#a31515>&#34;0 20 * * *&#34;</span> <span style=color:green># Start hibernation every day at 8PM</span>
</span></span><span style=display:flex><span>      end: <span style=color:#a31515>&#34;0 6 * * *&#34;</span>    <span style=color:green># Stop hibernation every day at 6AM</span>
</span></span><span style=display:flex><span>      location: <span style=color:#a31515>&#34;America/Los_Angeles&#34;</span> <span style=color:green># Specify a location for the cron to run in</span>
</span></span></code></pre></div><p>The above section configures a hibernation schedule that hibernates the cluster every day at 08:00 PM and wakes it up at 06:00 AM. The <code>start</code> or <code>end</code> fields can be omitted, though at least one of them has to be specified. Hence, it is possible to configure a hibernation schedule that only hibernates or wakes up a cluster. The <code>location</code> field is the time location used to evaluate the cron expressions.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5cc140bc78ef125e0e6636c2d7cd748a>2 - APIServer SNI Injection</h1><h1 id=apiserversni-environment-variable-injection>APIServerSNI Environment Variable Injection</h1><p>If the Gardener administrator has enabled the <code>APIServerSNI</code> feature gate for a particular Seed cluster, then in each Shoot cluster&rsquo;s <code>kube-system</code> namespace a <code>DaemonSet</code> called <code>apiserver-proxy</code> is deployed. It routes traffic to the upstream Shoot Kube APIServer. See the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md>APIServer SNI GEP</a> for more details.</p><p>To skip this extra network hop, a <a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook>mutating webhook</a> called <code>apiserver-proxy.networking.gardener.cloud</code> is deployed next to the API server in the Seed. It adds a <code>KUBERNETES_SERVICE_HOST</code> environment variable to each container and init container that do not specify it. See the webhook <a href=https://github.com/gardener/apiserver-proxy/>repository</a> for more information.</p><h2 id=opt-out-of-pod-injection>Opt-Out of Pod Injection</h2><p>In some cases it&rsquo;s desirable to opt-out of Pod injection:</p><ul><li>DNS is disabled on that individual Pod, but it still needs to talk to the kube-apiserver.</li><li>Want to test the <code>kube-proxy</code> and <code>kubelet</code> in-cluster discovery.</li></ul><h3 id=opt-out-of-pod-injection-for-specific-pods>Opt-Out of Pod Injection for Specific Pods</h3><p>To opt out of the injection, the Pod should be labeled with <code>apiserver-proxy.networking.gardener.cloud/inject: disable</code>, e.g.:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: nginx
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: nginx
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: nginx
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: nginx
</span></span><span style=display:flex><span>        apiserver-proxy.networking.gardener.cloud/inject: disable
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: nginx
</span></span><span style=display:flex><span>        image: nginx:1.14.2
</span></span><span style=display:flex><span>        ports:
</span></span><span style=display:flex><span>        - containerPort: 80
</span></span></code></pre></div><h3 id=opt-out-of-pod-injection-on-namespace-level>Opt-Out of Pod Injection on Namespace Level</h3><p>To opt out of the injection of <strong>all</strong> Pods in a namespace, you should label your namespace with <code>apiserver-proxy.networking.gardener.cloud/inject: disable</code>, e.g.:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Namespace
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    apiserver-proxy.networking.gardener.cloud/inject: disable
</span></span><span style=display:flex><span>  name: my-namespace
</span></span></code></pre></div><p>or via <code>kubectl</code> for existing namespace:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl label namespace my-namespace apiserver-proxy.networking.gardener.cloud/inject=disable
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> Please be aware that it&rsquo;s not possible to disable injection on a namespace level and enable it for individual pods in it.</p></blockquote><h3 id=opt-out-of-pod-injection-for-the-entire-cluster>Opt-Out of Pod Injection for the Entire Cluster</h3><p>If the injection is causing problems for different workloads and ignoring individual pods or namespaces is not possible, then the feature could be disabled for the entire cluster with the <code>alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector</code> annotation with value <code>disable</code> on the <code>Shoot</code> resource itself:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector: <span style=color:#a31515>&#39;disable&#39;</span>
</span></span><span style=display:flex><span>  name: my-cluster
</span></span></code></pre></div><p>or via <code>kubectl</code> for existing shoot cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl label shoot my-cluster alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector=disable
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> Please be aware that it&rsquo;s not possible to disable injection on a cluster level and enable it for individual pods in it.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-7e7022c7d80b227a07794be1c66352e7>3 - Configuration</h1><h1 id=gardener-configuration-and-usage>Gardener Configuration and Usage</h1><p>Gardener automates the full lifecycle of Kubernetes clusters as a service.
Additionally, it has several extension points allowing external controllers to plug-in to the lifecycle.
As a consequence, there are several configuration options for the various custom resources that are partially required.</p><p>This document describes the:</p><ol><li><a href=#configuration-and-usage-of-gardener-as-operatoradministrator>Configuration and usage of Gardener as operator/administrator</a>.</li><li><a href=#configuration-and-usage-of-gardener-as-end-userstakeholdercustomer>Configuration and usage of Gardener as end-user/stakeholder/customer</a>.</li></ol><h2 id=configuration-and-usage-of-gardener-as-operatoradministrator>Configuration and Usage of Gardener as Operator/Administrator</h2><p>When we use the terms &ldquo;operator/administrator&rdquo;, we refer to both the people deploying and operating Gardener.
Gardener consists of the following components:</p><ol><li><code>gardener-apiserver</code>, a Kubernetes-native API extension that serves custom resources in the Kubernetes-style (like <code>Seed</code>s and <code>Shoot</code>s), and a component that contains multiple admission plugins.</li><li><code>gardener-admission-controller</code>, an HTTP(S) server with several handlers to be used in a <a href=https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/charts/application/templates/validatingwebhook-admission-controller.yaml>ValidatingWebhookConfiguration</a>.</li><li><code>gardener-controller-manager</code>, a component consisting of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for maintaining <code>Shoot</code>s, reconciling <code>Project</code>s).</li><li><code>gardener-scheduler</code>, a component that assigns newly created <code>Shoot</code> clusters to appropriate <code>Seed</code> clusters.</li><li><code>gardenlet</code>, a component running in seed clusters and consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for reconciliation and deletion of <code>Shoot</code>s).</li></ol><p>Each of these components have various configuration options.
The <code>gardener-apiserver</code> uses the standard API server library maintained by the Kubernetes community, and as such it mainly supports command line flags.
Other components use so-called componentconfig files that describe their configuration in a Kubernetes-style versioned object.</p><h3 id=configuration-file-for-gardener-admission-controller>Configuration File for Gardener Admission Controller</h3><p>The Gardener admission controller only supports one command line flag, which should be a path to a valid admission-controller configuration file.
Please take a look at this <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-admission-controller.yaml>example configuration</a>.</p><h3 id=configuration-file-for-gardener-controller-manager>Configuration File for Gardener Controller Manager</h3><p>The Gardener controller manager only supports one command line flag, which should be a path to a valid controller-manager configuration file.
Please take a look at this <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-controller-manager.yaml>example configuration</a>.</p><h3 id=configuration-file-for-gardener-scheduler>Configuration File for Gardener Scheduler</h3><p>The Gardener scheduler also only supports one command line flag, which should be a path to a valid scheduler configuration file.
Please take a look at this <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-scheduler.yaml>example configuration</a>.
Information about the concepts of the Gardener scheduler can be found at <a href=/docs/gardener/concepts/scheduler/>Gardener Scheduler</a>.</p><h3 id=configuration-file-for-gardenlet>Configuration File for gardenlet</h3><p>The gardenlet also only supports one command line flag, which should be a path to a valid gardenlet configuration file.
Please take a look at this <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml>example configuration</a>.
Information about the concepts of the Gardenlet can be found at <a href=/docs/gardener/concepts/gardenlet/>gardenlet</a>.</p><h3 id=system-configuration>System Configuration</h3><p>After successful deployment of the four components, you need to setup the system.
Let&rsquo;s first focus on some &ldquo;static&rdquo; configuration.
When the <code>gardenlet</code> starts, it scans the <code>garden</code> namespace of the garden cluster for <code>Secret</code>s that have influence on its reconciliation loops, mainly the <code>Shoot</code> reconciliation:</p><ul><li><p><strong>Internal domain secret</strong> - contains the DNS provider credentials (having appropriate privileges) which will be used to create/delete the so-called &ldquo;internal&rdquo; DNS records for the Shoot clusters, please see this <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-internal-domain.yaml>yaml file</a> for an example.</p><ul><li>This secret is used in order to establish a stable endpoint for shoot clusters, which is used internally by all control plane components.</li><li>The DNS records are normal DNS records but called &ldquo;internal&rdquo; in our scenario because only the kubeconfigs for the control plane components use this endpoint when talking to the shoot clusters.</li><li>It is forbidden to change the internal domain secret if there are existing shoot clusters.</li></ul></li><li><p><strong>Default domain secrets</strong> (optional) - contain the DNS provider credentials (having appropriate privileges) which will be used to create/delete DNS records for a default domain for shoots (e.g., <code>example.com</code>), please see this <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-default-domain.yaml>yaml file</a> for an example.</p><ul><li>Not every end-user/stakeholder/customer has its own domain, however, Gardener needs to create a DNS record for every shoot cluster.</li><li>As landscape operator you might want to define a default domain owned and controlled by you that is used for all shoot clusters that don&rsquo;t specify their own domain.</li><li>If you have multiple default domain secrets defined you can add a priority as an annotation (<code>dns.gardener.cloud/domain-default-priority</code>) to select which domain should be used for new shoots during creation. The domain with the highest priority is selected during shoot creation. If there is no annotation defined, the default priority is <code>0</code>, also all non integer values are considered as priority <code>0</code>.</li></ul></li><li><p><strong>Alerting secrets</strong> (optional) - contain the alerting configuration and credentials for the <a href=https://prometheus.io/docs/alerting/alertmanager/>AlertManager</a> to send email alerts. It is also possible to configure the monitoring stack to send alerts to an AlertManager not deployed by Gardener to handle alerting. Please see this <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-alerting.yaml>yaml file</a> for an example.</p><ul><li>If email alerting is configured:<ul><li>An AlertManager is deployed into each seed cluster that handles the alerting for all shoots on the seed cluster.</li><li>Gardener will inject the SMTP credentials into the configuration of the AlertManager.</li><li>The AlertManager will send emails to the configured email address in case any alerts are firing.</li></ul></li><li>If an external AlertManager is configured:<ul><li>Each shoot has a <a href=https://prometheus.io/docs/introduction/overview/>Prometheus</a> responsible for monitoring components and sending out alerts. The alerts will be sent to a URL configured in the alerting secret.</li><li>This external AlertManager is not managed by Gardener and can be configured however the operator sees fit.</li><li>Supported authentication types are no authentication, basic, or mutual TLS.</li></ul></li></ul></li><li><p><strong>OpenVPN Diffie-Hellmann Key secret</strong> (optional) - contains the self-generated Diffie-Hellmann key used by OpenVPN in your landscape, please see this <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-openvpn-diffie-hellman.yaml>yaml file</a> for an example.</p><ul><li>If you don&rsquo;t specify a custom key, then a default key is used, but for productive landscapes it&rsquo;s recommend to create a landscape-specific key and define it.</li></ul></li><li><p><strong>Global monitoring secrets</strong> (optional) - contains basic authentication credentials for the Prometheus aggregating metrics for all clusters.</p><ul><li>These secrets are synced to each seed cluster and used to gain access to the aggregate monitoring components.</li></ul></li></ul><p>Apart from this &ldquo;static&rdquo; configuration there are several custom resources extending the Kubernetes API and used by Gardener.
As an operator/administrator, you have to configure some of them to make the system work.</p><h3 id=configuration-and-usage-of-gardener-as-end-userstakeholdercustomer>Configuration and Usage of Gardener as End-User/Stakeholder/Customer</h3><p>As an end-user/stakeholder/customer, you are using a Gardener landscape that has been setup for you by another team.
You don&rsquo;t need to care about how Gardener itself has to be configured or how it has to be deployed.
Take a look at <a href=/docs/gardener/concepts/apiserver/>Gardener API Server</a> - the topic describes which resources are offered by Gardener.
You may want to have a more detailed look for <code>Project</code>s, <code>SecretBinding</code>s, <code>Shoot</code>s, and <code>(Cluster)OpenIDConnectPreset</code>s.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d39ff1936115af2f29f4e58ff5cabc9d>4 - Control Plane Endpoints And Ports</h1><h1 id=endpoints-and-ports-of-a-shoot-control-plane>Endpoints and Ports of a Shoot Control-Plane</h1><p>With the <a href=/docs/gardener/usage/reversed-vpn-tunnel/>reversed VPN</a> tunnel, there are no endpoints with open ports in the shoot cluster required by Gardener.
In order to allow communication to the shoots control-plane in the seed cluster, there are endpoints shared by multiple shoots of a seed cluster.
Depending on the configured zones or <a href=/docs/gardener/usage/exposureclasses/>exposure classes</a>, there are different endpoints in a seed cluster. The IP address(es) can be determined by a DNS query for the API Server URL.
The main entry-point into the seed cluster is the load balancer of the Istio ingress-gateway service. Depending on the infrastructure provider, there can be one IP address per zone.</p><p>The load balancer of the Istio ingress-gateway service exposes the following TCP ports:</p><ul><li><strong>443</strong> for requests to the shoot API Server. The request is dispatched according to the set TLS SNI extension.</li><li><strong>8443</strong> for requests to the shoot API Server via <code>api-server-proxy</code>, dispatched based on the proxy protocol target, which is the IP address of <code>kubernetes.default.svc.cluster.local</code> in the shoot.</li><li><strong>8132</strong> to establish the reversed VPN connection. It&rsquo;s dispatched according to an HTTP header value.</li></ul><h2 id=kube-apiserver-via-sni><code>kube-apiserver</code> via SNI</h2><p><img src=/__resources/api-server-sni_feb16f.png alt="kube-apiserver via SNI"></p><p>DNS entries for <code>api.&lt;external-domain></code> and <code>api.&lt;shoot>.&lt;project>.&lt;internal-domain></code> point to the load balancer of an Istio ingress-gateway service.
The Kubernetes client sets the server name to <code>api.&lt;external-domain></code> or <code>api.&lt;shoot>.&lt;project>.&lt;internal-domain></code>.
Based on SNI, the connection is forwarded to the respective API Server at TCP layer. There is no TLS termination at the Istio ingress-gateway.
TLS termination happens on the shoots API Server. Traffic is end-to-end encrypted between the client and the API Server. The certificate authority and authentication are defined in the corresponding <code>kubeconfig</code>.
Details can be found in <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md>GEP-08</a>.</p><h2 id=kube-apiserver-via-apiserver-proxy><code>kube-apiserver</code> via <code>apiserver-proxy</code></h2><p><img src=/__resources/api-server-proxy_b419fc.png alt=apiserver-proxy></p><p>Inside the shoot cluster, the API Server can also be reached by the cluster internal name <code>kubernetes.default.svc.cluster.local</code>.
The pods <code>apiserver-proxy</code> are deployed in the host network as daemonset and intercept connections to the Kubernetes service IP address.
The destination address is changed to the cluster IP address of the service <code>kube-apiserver.&lt;shoot-namespace>.svc.cluster.local</code> in the seed cluster.
The connections are forwarded via the <a href=https://www.envoyproxy.io/docs/envoy/latest/configuration/listeners/listener_filters/proxy_protocol>HaProxy Proxy Protocol</a> to the Istio ingress-gateway in the seed cluster.
The Istio ingress-gateway forwards the connection to the respective shoot API Server by it&rsquo;s cluster IP address.
As TLS termination happens at the API Server, the traffic is end-to-end encrypted the same way as with SNI.</p><p>Details can be found in <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/11-apiserver-network-proxy.md>GEP-11</a>.</p><h2 id=reversed-vpn-tunnel>Reversed VPN Tunnel</h2><p><img src=/__resources/reversed-vpn_e89ad6.png alt="Reversed VPN"></p><p>As the API Server has to be able to connect to endpoints in the shoot cluster, a VPN connection is established.
This VPN connection is initiated from a VPN client in the shoot cluster.
The VPN client connects to the Istio ingress-gateway and is forwarded to the VPN server in the control-plane namespace of the shoot.
Once the VPN tunnel between the VPN client in the shoot and the VPN server in the seed cluster is established, the API Server can connect to nodes, services and pods in the shoot cluster.</p><p>More details can be found in the <a href=/docs/gardener/usage/reversed-vpn-tunnel/>usage document</a> and <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/14-reversed-cluster-vpn.md>GEP-14</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-557189bc540e3079a8a290a1db4e319f>5 - Control Plane Migration</h1><h1 id=control-plane-migration>Control Plane Migration</h1><h2 id=prerequisites>Prerequisites</h2><p>To be able to use this feature, the <code>SeedChange</code> feature gate has to be enabled on your <code>gardener-apiserver</code>.</p><p>Also, the involved Seeds need to have enabled <code>BackupBucket</code>s.</p><h2 id=shootstate>ShootState</h2><p><code>ShootState</code> is an API resource which stores non-reconstructible state and data required to completely recreate a <code>Shoot</code>&rsquo;s control plane on a new <code>Seed</code>. The <code>ShootState</code> resource is created on <code>Shoot</code> creation in its <code>Project</code> namespace and the required state/data is persisted during <code>Shoot</code> creation or reconciliation.</p><h2 id=shoot-control-plane-migration>Shoot Control Plane Migration</h2><p>Triggering the migration is done by changing the <code>Shoot</code>&rsquo;s <code>.spec.seedName</code> to a <code>Seed</code> that differs from the <code>.status.seedName</code>, we call this <code>Seed</code> a <code>"Destination Seed"</code>. This action can only be performed by an operator with the necessary RBAC. If the Destination <code>Seed</code> does not have a backup and restore configuration, the change to <code>spec.seedName</code> is rejected. Additionally, this Seed must not be set for deletion and must be healthy.</p><p>If the <code>Shoot</code> has different <code>.spec.seedName</code> and <code>.status.seedName</code>, a process is started to prepare the Control Plane for migration:</p><ol><li><code>.status.lastOperation</code> is changed to <code>Migrate</code>.</li><li>Kubernetes API Server is stopped and the extension resources are annotated with <code>gardener.cloud/operation=migrate</code>.</li><li>Full snapshot of the ETCD is created and terminating of the Control Plane in the <code>Source Seed</code> is initiated.</li></ol><p>If the process is successful, we update the status of the <code>Shoot</code> by setting the <code>.status.seedName</code> to the null value. That way, a restoration is triggered in the <code>Destination Seed</code> and <code>.status.lastOperation</code> is changed to <code>Restore</code>. The control plane migration is completed when the <code>Restore</code> operation has completed successfully.</p><p>When the <code>CopyEtcdBackupsDuringControlPlaneMigration</code> feature gate is enabled on the <code>gardenlet</code>, the etcd backups will be copied over to the <code>BackupBucket</code> of the <code>Destination Seed</code> during control plane migration and any future backups will be uploaded there. Otherwise, backups will continue to be uploaded to the <code>BackupBucket</code> of the <code>Source Seed</code>.</p><h2 id=triggering-the-migration>Triggering the Migration</h2><p>For controlplane migration, operators with the necessary RBAC can use the <a href=/docs/gardener/concepts/scheduler/#shootsbinding-subresource><code>shoots/binding</code></a> subresource to change the <code>.spec.seedName</code>, with the following commands:</p><pre tabindex=0><code>export NAMESPACE=my-namespace
export SHOOT_NAME=my-shoot
kubectl get --raw /apis/core.gardener.cloud/v1beta1/namespaces/${NAMESPACE}/shoots/${SHOOT_NAME} | jq -c &#39;.spec.seedName = &#34;&lt;destination-seed&gt;&#34;&#39; | kubectl replace --raw /apis/core.gardener.cloud/v1beta1/namespaces/${NAMESPACE}/shoots/${SHOOT_NAME}/binding -f - | jq -r &#39;.spec.seedName&#39;
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-3a01742bcc319ec4dfb0ee2cdce04deb>6 - CSI Components</h1><h1 id=custom-csi-components>(Custom) CSI Components</h1><p>Some provider extensions for Gardener are using CSI components to manage persistent volumes in the shoot clusters.
Additionally, most of the provider extensions are deploying controllers for taking volume snapshots (CSI snapshotter).</p><p>End-users can deploy their own CSI components and controllers into shoot clusters.
In such situations, there are multiple controllers acting on the <code>VolumeSnapshot</code> custom resources (each responsible for those instances associated with their respective driver provisioner types).</p><p>However, this might lead to operational conflicts that cannot be overcome by Gardener alone.
Concretely, Gardener cannot know which custom CSI components were installed by end-users which can lead to issues, especially during shoot cluster deletion.
You can add a label to your custom CSI components indicating that Gardener should not try to remove them during shoot cluster deletion. This means you have to take care of the lifecycle for these components yourself!</p><h2 id=recommendations>Recommendations</h2><p>Custom CSI components are typically regular <code>Deployment</code>s running in the shoot clusters.</p><p><strong>Please label them with the <code>shoot.gardener.cloud/no-cleanup=true</code> label.</strong></p><h2 id=background-information>Background Information</h2><p>When a shoot cluster is deleted, Gardener deletes most Kubernetes resources (<code>Deployment</code>s, <code>DaemonSet</code>s, <code>StatefulSet</code>s, etc.). Gardener will also try to delete CSI components if they are not marked with the above mentioned label.</p><p>This can result in <code>VolumeSnapshot</code> resources still having finalizers that will never be cleaned up.
Consequently, manual intervention is required to clean them up before the cluster deletion can continue.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2ff8f948af545c542d2859a2fd5223e4>7 - Custom containerd Configuration</h1><h1 id=custom-containerd-configuration>Custom <code>containerd</code> Configuration</h1><p>In case a <code>Shoot</code> cluster uses <code>containerd</code> (see <a href=/docs/gardener/usage/docker-shim-removal/>Kubernetes dockershim Removal</a>) for more information), it is possible to make the <code>containerd</code> process load custom configuration files.
Gardener initializes <code>containerd</code> with the following statement:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span>imports = [<span style=color:#a31515>&#34;/etc/containerd/conf.d/*.toml&#34;</span>]
</span></span></code></pre></div><p>This means that all <code>*.toml</code> files in the <code>/etc/containerd/conf.d</code> directory will be imported and merged with the default configuration.
To prevent unintended configuration overwrites, please be aware that containerd merges config sections, not individual keys (see <a href=https://github.com/containerd/containerd/issues/5837#issuecomment-894840240>here</a> and <a href=https://github.com/gardener/gardener/pull/7316>here</a>).
Please consult the <a href=https://github.com/containerd/containerd/blob/main/docs/man/containerd-config.toml.5.md#format>upstream <code>containerd</code> documentation</a> for more information.</p><blockquote><p>⚠️ Note that this only applies to nodes which were newly created after <code>gardener/gardener@v1.51</code> was deployed. Existing nodes are not affected.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-0886d3122ac4a522338baa665bcd836e>8 - Custom DNS Configuration</h1><h1 id=custom-dns-configuration>Custom DNS Configuration</h1><p>Gardener provides Kubernetes-Clusters-As-A-Service where all the system components (e.g., kube-proxy, networking, dns) are managed.
As a result, Gardener needs to ensure and auto-correct additional configuration to those system components to avoid unnecessary down-time.</p><p>In some cases, auto-correcting system components can prevent users from deploying applications on top of the cluster that requires bits of customization, DNS configuration can be a good example.</p><p>To allow for customizations for DNS configuration (that could potentially lead to downtime) while having the option to &ldquo;undo&rdquo;, we utilize the <code>import</code> plugin from CoreDNS [1].
which enables in-line configuration changes.</p><h2 id=how-to-use>How to use</h2><p>To customize your CoreDNS cluster config, you can simply edit a <code>ConfigMap</code> named <code>coredns-custom</code> in the <code>kube-system</code> namespace.
By editing, this <code>ConfigMap</code>, you are modifying CoreDNS configuration, therefore care is advised.</p><p>For example, to apply new config to CoreDNS that would point all <code>.global</code> DNS requests to another DNS pod, simply edit the configuration as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: coredns-custom
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  istio.server: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    global:8053 {
</span></span></span><span style=display:flex><span><span style=color:#a31515>            errors
</span></span></span><span style=display:flex><span><span style=color:#a31515>            cache 30
</span></span></span><span style=display:flex><span><span style=color:#a31515>            forward . 1.2.3.4
</span></span></span><span style=display:flex><span><span style=color:#a31515>        }</span>    
</span></span><span style=display:flex><span>  corefile.override: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>         # &lt;some-plugin&gt; &lt;some-plugin-config&gt;
</span></span></span><span style=display:flex><span><span style=color:#a31515>         debug
</span></span></span><span style=display:flex><span><span style=color:#a31515>         whoami</span>         
</span></span></code></pre></div><p>The port number 8053 in <code>global:8053</code> is the specific port that CoreDNS is bound to and cannot be changed to any other port if it should act on ordinary name resolution requests from pods. Otherwise, CoreDNS will open a second port, but you are responsible to direct the traffic to this port. <code>kube-dns</code> service in <code>kube-system</code> namespace will direct name resolution requests within the cluster to port 8053 on the CoreDNS pods.
Moreover, additional network policies are needed to allow corresponding ingress traffic to CoreDNS pods.
In order for the destination DNS server to be reachable, it must listen on port 53 as it is required by network policies. Other ports are only possible if additional network policies allow corresponding egress traffic from CoreDNS pods.</p><p>It is important to have the <code>ConfigMap</code> keys ending with <code>*.server</code> (if you would like to add a new server) or <code>*.override</code>
if you want to customize the current server configuration (it is optional setting both).</p><h2 id=optional-reload-coredns>[Optional] Reload CoreDNS</h2><p>As Gardener is configuring the <code>reload</code> <a href=https://coredns.io/plugins/reload/>plugin</a> of CoreDNS a restart of the CoreDNS components is typically not necessary to propagate <code>ConfigMap</code> changes. However, if you don&rsquo;t want to wait for the default (30s) to kick in, you can roll-out your CoreDNS deployment using:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n kube-system rollout restart deploy coredns
</span></span></code></pre></div><p>This will reload the config into CoreDNS.</p><p>The approach we follow here was inspired by AKS&rsquo;s approach [2].</p><h2 id=anti-pattern>Anti-Pattern</h2><p>Applying a configuration that is in-compatible with the running version of CoreDNS is an anti-pattern (sometimes plugin configuration changes,
simply applying a configuration can break DNS).</p><p>If incompatible changes are applied by mistake, simply delete the content of the <code>ConfigMap</code> and re-apply.
This should bring the cluster DNS back to functioning state.</p><h2 id=node-local-dns>Node Local DNS</h2><p>Custom DNS configuration] may not work as expected in conjunction with <code>NodeLocalDNS</code>.
With <code>NodeLocalDNS</code>, ordinary DNS queries targeted at the upstream DNS servers, i.e. non-kubernetes domains,
will not end up at CoreDNS, but will instead be directly sent to the upstream DNS server. Therefore, configuration
applying to non-kubernetes entities, e.g. the <code>istio.server</code> block in the
<a href=/docs/gardener/usage/custom-dns-config/>custom DNS configuration</a> example, may not have any effect with <code>NodeLocalDNS</code> enabled.
If this kind of custom configuration is required, forwarding to upstream DNS has to be disabled.
This can be done by setting the option (<code>spec.systemComponents.nodeLocalDNS.disableForwardToUpstreamDNS</code>) in the <code>Shoot</code> resource to <code>true</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  systemComponents:
</span></span><span style=display:flex><span>    nodeLocalDNS:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      disableForwardToUpstreamDNS: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=references>References</h2><p>[1] <a href=https://github.com/coredns/coredns/tree/master/plugin/import>Import plugin</a>
[2] <a href=https://docs.microsoft.com/en-us/azure/aks/coredns-custom>AKS Custom DNS</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-c5ac85b19cff8076310937381264d967>9 - Default Seccomp Profile</h1><h1 id=default-seccomp-profile-and-configuration>Default Seccomp Profile and Configuration</h1><p>This is a short guide describing how to enable the defaulting of seccomp profiles for Gardener managed workloads in the seed.</p><h2 id=default-kubernetes-behavior>Default Kubernetes Behavior</h2><p>The state of Kubernetes in versions &lt; 1.25 is such that all workloads by default run in <code>Unconfined</code> (seccomp disabled) mode. This is undesirable since this is the least restrictive profile. Also, mind that any privileged container will always run as <code>Unconfined</code>. More information about seccomp can be found in this <a href=https://kubernetes.io/docs/tutorials/security/seccomp/>Kubernetes tutorial</a>.</p><h2 id=setting-the-seccomp-profile-to-runtimedefault-for-seed-clusters>Setting the Seccomp Profile to RuntimeDefault for Seed Clusters</h2><p>To address the above issue, Gardener provides a webhook that is capable of mutating pods in the seed clusters, explicitly providing them with a seccomp profile type of <code>RuntimeDefault</code>. This profile is defined by the container runtime and represents a set of default syscalls that are allowed or not.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  securityContext:
</span></span><span style=display:flex><span>    seccompProfile:
</span></span><span style=display:flex><span>      type: RuntimeDefault
</span></span></code></pre></div><p>A <code>Pod</code> is mutated when all of the following preconditions are fulfilled:</p><ol><li>The <code>Pod</code> is created in a Gardener managed namespace.</li><li>The <code>Pod</code> is NOT labeled with <code>seccompprofile.resources.gardener.cloud/skip</code>.</li><li>The <code>Pod</code> does NOT explicitly specify <code>.spec.securityContext.seccompProfile.type</code>.</li></ol><h3 id=how-to-configure>How to Configure</h3><p>To enable this feature, the gardenlet <code>DefaultSeccompProfile</code> feature gate must be set to <code>true</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>featureGates:
</span></span><span style=display:flex><span>  DefaultSeccompProfile: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>Please refer to the examples in this <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml>yaml file</a> for more information.</p><p>Once the feature gate is enabled, the webhook will be registered and configured for the seed cluster. Newly created pods will be mutated to have their seccomp profile set to <code>RuntimeDefault</code>.</p><blockquote><p><strong>Note:</strong> Please note that this feature is still in Alpha, so you might see instabilities every now and then.</p></blockquote><h2 id=setting-the-seccomp-profile-to-runtimedefault-for-shoot-clusters>Setting the Seccomp Profile to RuntimeDefault for Shoot Clusters</h2><p>For Kubernetes shoot versions >= 1.25, you can enable the use of <code>RuntimeDefault</code> as the default seccomp profile for all workloads. If enabled, the kubelet will use the <code>RuntimeDefault</code> seccomp profile by default, which is defined by the container runtime, instead of using the <code>Unconfined</code> mode. More information for this feature can be found in the <a href=https://kubernetes.io/docs/tutorials/security/seccomp/#enable-the-use-of-runtimedefault-as-the-default-seccomp-profile-for-all-workloads>Kubernetes documentation</a>.</p><p>To use seccomp profile defaulting, you must run the kubelet with the <code>SeccompDefault</code> feature gate enabled (this is the default for k8s versions >= 1.25).</p><h3 id=how-to-configure-1>How to Configure</h3><p>To enable this feature, the kubelet <code>seccompDefault</code> configuration parameter must be set to <code>true</code> in the shoot&rsquo;s spec.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.25.0
</span></span><span style=display:flex><span>    kubelet:
</span></span><span style=display:flex><span>      seccompDefault: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>Please refer to the examples in this <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>yaml file</a> for more information.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-da5e6c1f64dc67e784358df56a1ec88d>10 - DNS Autoscaling</h1><h1 id=dns-autoscaling>DNS Autoscaling</h1><p>This is a short guide describing different options how to automatically scale CoreDNS in the shoot cluster.</p><h2 id=background>Background</h2><p>Currently, Gardener uses CoreDNS as DNS server. Per default, it is installed as a deployment into the shoot cluster that is auto-scaled horizontally to cover for QPS-intensive applications. However, doing so does not seem to be enough to completely circumvent DNS bottlenecks such as:</p><ul><li>Cloud provider limits for DNS lookups.</li><li>Unreliable UDP connections that forces a period of timeout in case packets are dropped.</li><li>Unnecessary node hopping since CoreDNS is not deployed on all nodes, and as a result DNS queries end-up traversing multiple nodes before reaching the destination server.</li><li>Inefficient load-balancing of services (e.g., round-robin might not be enough when using IPTables mode).</li><li>Overload of the CoreDNS replicas as the maximum amount of replicas is fixed.</li><li>and more &mldr;</li></ul><p>As an alternative with extended configuration options, Gardener provides cluster-proportional autoscaling of CoreDNS. This guide focuses on the configuration of cluster-proportional autoscaling of CoreDNS and its advantages/disadvantages compared to the horizontal
autoscaling.
Please note that there is also the option to use a <a href=/docs/gardener/usage/node-local-dns/>node-local DNS cache</a>, which helps mitigate potential DNS bottlenecks (see <a href=#trade-offs-in-conjunction-with-nodelocaldns>Trade-offs in conjunction with NodeLocalDNS</a> for considerations regarding using NodeLocalDNS together with one of the CoreDNS autoscaling approaches).</p><h2 id=configuring-cluster-proportional-dns-autoscaling>Configuring Cluster-Proportional DNS Autoscaling</h2><p>All that needs to be done to enable the usage of cluster-proportional autoscaling of CoreDNS is to set the corresponding option (<code>spec.systemComponents.coreDNS.autoscaling.mode</code>) in the <code>Shoot</code> resource to <code>cluster-proportional</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  systemComponents:
</span></span><span style=display:flex><span>    coreDNS:
</span></span><span style=display:flex><span>      autoscaling:
</span></span><span style=display:flex><span>        mode: cluster-proportional
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>To switch back to horizontal DNS autoscaling, you can set the <code>spec.systemComponents.coreDNS.autoscaling.mode</code> to <code>horizontal</code> (or remove the <code>coreDNS</code> section).</p><p>Once the cluster-proportional autoscaling of CoreDNS has been enabled and the Shoot cluster has been reconciled afterwards, a ConfigMap called <code>coredns-autoscaler</code> will be created in the <code>kube-system</code> namespace with the default settings. The content will be similar to the following:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>linear: <span style=color:#a31515>&#39;{&#34;coresPerReplica&#34;:256,&#34;min&#34;:2,&#34;nodesPerReplica&#34;:16}&#39;</span>
</span></span></code></pre></div><p>It is possible to adapt the ConfigMap according to your needs in case the defaults do not work as desired. The number of CoreDNS replicas is calculated according to the following formula:</p><pre tabindex=0><code>replicas = max( ceil( cores × 1 / coresPerReplica ) , ceil( nodes × 1 / nodesPerReplica ) )
</code></pre><p>Depending on your needs, you can adjust <code>coresPerReplica</code> or <code>nodesPerReplica</code>, but it is also possible to override <code>min</code> if required.</p><h2 id=trade-offs-of-horizontal-and-cluster-proportional-dns-autoscaling>Trade-Offs of Horizontal and Cluster-Proportional DNS Autoscaling</h2><p>The horizontal autoscaling of CoreDNS as implemented by Gardener is fully managed, i.e., you do not need to perform any configuration changes. It scales according to the CPU usage of CoreDNS replicas, meaning that it will create new replicas if the existing ones are under heavy load. This approach scales between 2 and 5 instances, which is sufficient for most workloads. In case this is not enough, the cluster-proportional autoscaling approach can be used instead, with its more flexible configuration options.</p><p>The cluster-proportional autoscaling of CoreDNS as implemented by Gardener is fully managed, but allows more configuration options to adjust the default settings to your individual needs. It scales according to the cluster size, i.e., if your cluster grows in terms of cores/nodes so will the amount of CoreDNS replicas. However, it does not take the actual workload, e.g., CPU consumption, into account.</p><p>Experience shows that the horizontal autoscaling of CoreDNS works for a variety of workloads. It does reach its limits if a cluster has a high amount of DNS requests, though. The cluster-proportional autoscaling approach allows to fine-tune the amount of CoreDNS replicas. It helps to scale in clusters of changing size. However, please keep in mind that you need to cater for the maximum amount of DNS requests as the replicas will not be adapted according to the workload, but only according to the cluster size (cores/nodes).</p><h2 id=trade-offs-in-conjunction-with-nodelocaldns>Trade-Offs in Conjunction with NodeLocalDNS</h2><p>Using a <a href=/docs/gardener/usage/node-local-dns/>node-local DNS cache</a> can mitigate a lot of the potential DNS related problems. It works fine with a DNS workload that can be handle through the cache and reduces the inter-node DNS communication. As <a href=/docs/gardener/usage/node-local-dns/>node-local DNS cache</a> reduces the amount of traffic being sent to the cluster&rsquo;s CoreDNS replicas, it usually works fine with horizontally scaled CoreDNS. Nevertheless, it also works with CoreDNS scaled in a cluster-proportional approach. In this mode, though, it might make sense to adapt the default settings as the CoreDNS workload is likely significantly reduced.</p><p>Overall, you can view the DNS options on a scale. Horizontally scaled DNS provides a small amount of DNS servers. Especially for bigger clusters, a cluster-proportional approach will yield more CoreDNS instances and hence may yield a more balanced DNS solution. By adapting the settings you can further increase the amount of CoreDNS replicas. On the other end of the spectrum, a <a href=/docs/gardener/usage/node-local-dns/>node-local DNS cache</a> provides DNS on every node and allows to reduce the amount of (backend) CoreDNS instances regardless if they are horizontally or cluster-proportionally scaled.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0de9e36d58f4dc635d78712b0907f162>11 - DNS Search Path Optimization</h1><h1 id=dns-search-path-optimization>DNS Search Path Optimization</h1><h2 id=dns-search-path>DNS Search Path</h2><p>Using fully qualified names has some downsides, e.g., it may become harder to move deployments from one landscape to the
next. It is far easier and simple to rely on short/local names, which may have different meaning depending on the context
they are used in.</p><p>The DNS search path allows for the usage of short/local names. It is an ordered list of DNS suffixes to append to short/local
names to create a fully qualified name.</p><p>If a short/local name should be resolved, each entry is appended to it one by one to check whether it can be resolved. The
process stops when either the name could be resolved or the DNS search path ends. As the last step after trying the search
path, the short/local name is attempted to be resolved on it own.</p><h2 id=dns-option-ndots>DNS Option <code>ndots</code></h2><p>As explained in the <a href=#dns-search-path>section above</a>, the DNS search path is used for short/local names to create fully
qualified names. The DNS option <code>ndots</code> specifies how many dots (<code>.</code>) a name needs to have to be considered fully qualified.
For names with less than <code>ndots</code> dots (<code>.</code>), the <a href=#dns-search-path>DNS search path</a> will be applied.</p><h2 id=dns-search-path-ndots-and-kubernetes>DNS Search Path, <code>ndots</code>, and Kubernetes</h2><p>Kubernetes tries to make it easy/convenient for developers to use name resolution. It provides several means to address a
service, most notably by its name directly, using the namespace as suffix, utilizing <code>&lt;namespace>.svc</code> as suffix or as a
fully qualified name as <code>&lt;service>.&lt;namespace>.svc.cluster.local</code> (assuming <code>cluster.local</code> to be the cluster domain).</p><p>This is why the DNS search path is fairly long in Kubernetes, usually consisting of <code>&lt;namespace>.svc.cluster.local</code>,
<code>svc.cluster.local</code>, <code>cluster.local</code>, and potentially some additional entries coming from the local network of the cluster.
For various reasons, the default <code>ndots</code> value in the context of Kubernetes is with <code>5</code>, also fairly large. See
<a href=https://github.com/kubernetes/kubernetes/issues/33554#issuecomment-266251056>this comment</a> for a more detailed description.</p><h2 id=dns-search-pathndots-problem-in-kubernetes>DNS Search Path/<code>ndots</code> Problem in Kubernetes</h2><p>As the DNS search path is long and <code>ndots</code> is large, a lot of DNS queries might traverse the DNS search path. This results
in an explosion of DNS requests.</p><p>For example, consider the name resolution of the default kubernetes service <code>kubernetes.default.svc.cluster.local</code>. As this
name has only four dots, it is not considered a fully qualified name according to the default <code>ndots=5</code> setting. Therefore,
the DNS search path is applied, resulting in the following queries being created</p><ul><li><code>kubernetes.default.svc.cluster.local.some-namespace.svc.cluster.local</code></li><li><code>kubernetes.default.svc.cluster.local.svc.cluster.local</code></li><li><code>kubernetes.default.svc.cluster.local.cluster.local</code></li><li><code>kubernetes.default.svc.cluster.local.network-domain</code></li><li>&mldr;</li></ul><p>In IPv4/IPv6 dual stack systems, the amount of DNS requests may even double as each name is resolved for IPv4 and IPv6.</p><h2 id=general-workaroundsmitigations>General Workarounds/Mitigations</h2><p>Kubernetes provides the capability to set the DNS options for each pod (see
<a href=https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-dns-config>Pod DNS config</a> for details).
However, this has to be applied for every pod (doing name resolution) to resolve the problem. A mutating webhook may be
useful in this regard. Unfortunately, the DNS requirements may be different depending on the workload. Therefore, a general
solution may difficult to impossible.</p><p>Another approach is to use always fully qualified names and append a dot (<code>.</code>) to the name to prevent the name resolution
system from using the DNS search path. This might be somewhat counterintuitive as most developers are not used to the
trailing dot (<code>.</code>). Furthermore, it makes moving to different landscapes more difficult/error-prone.</p><h2 id=gardener-specific-workaroundsmitigations>Gardener Specific Workarounds/Mitigations</h2><p>Gardener allows users to <a href=/docs/gardener/usage/custom-dns-config/>customize their DNS configuration</a>. CoreDNS allows several approaches to deal with
the requests generated by the DNS search path. <a href=https://coredns.io/plugins/cache/>Caching</a> is possible as well as
<a href=https://coredns.io/plugins/rewrite/>query rewriting</a>. There are also several other <a href=https://coredns.io/plugins/>plugins</a>
available, which may mitigate the situation.</p><h2 id=gardener-dns-query-rewriting>Gardener DNS Query Rewriting</h2><p>As explained <a href=#dns-search-path-ndots-and-kubernetes>above</a>, the application of the DNS search path may lead to the undesired
creation of DNS requests. Especially with the default setting of <code>ndots=5</code>, seemingly fully qualified names pointing to
services in the cluster may trigger the DNS search path application.</p><p>Gardener allows to automatically rewrite some obviously incorrect DNS names, which stem from an application of the DNS search
path to the most likely desired name. The feature can be enabled by setting the Gardenlet feature gate <code>CoreDNSQueryRewriting</code> to <code>true</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>featureGates:
</span></span><span style=display:flex><span>  CoreDNSQueryRewriting: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>In case the feature is enabled in the gardenlet, it can be disabled per shoot cluster by setting the annotation
<code>alpha.featuregates.shoot.gardener.cloud/core-dns-rewriting-disabled</code> to any value.</p><p>This will automatically rewrite requests like <code>service.namespace.svc.cluster.local.svc.cluster.local</code> to
<code>service.namespace.svc.cluster.local</code>.</p><p>In case the applications also target services for name resolution, which are outside of the cluster and have less than <code>ndots</code> dots,
it might be helpful to prevent search path application for them as well. One way to achieve it is by adding them to the
<code>commonSuffixes</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  systemComponents:
</span></span><span style=display:flex><span>    coreDNS:
</span></span><span style=display:flex><span>      rewriting:
</span></span><span style=display:flex><span>        commonSuffixes:
</span></span><span style=display:flex><span>        - gardener.cloud
</span></span><span style=display:flex><span>        - example.com
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>DNS requests containing a common suffix and ending in <code>.svc.cluster.local</code> are assumed to be incorrect application of the DNS
search path. Therefore, they are rewritten to everything ending in the common suffix. For example, <code>www.gardener.cloud.svc.cluster.local</code>
would be rewritten to <code>www.gardener.cloud</code>.</p><p>Please note that the common suffixes should be long enough and include enough dots (<code>.</code>) to prevent random overlap with
other DNS queries. For example, it would be a bad idea to simply put <code>com</code> on the list of common suffixes, as there may be
services/namespaces which have <code>com</code> as part of their name. The effect would be seemingly random DNS requests. Gardener
requires that common suffixes contain at least one dot (.) and adds a second dot at the beginning. For instance, a common
suffix of <code>example.com</code> in the configuration would match <code>*.example.com</code>.</p><p>Since some clients verify the host in the response of a DNS query, the host must also be rewritten.
For that reason, we can&rsquo;t rewrite a query for <code>service.dst-namespace.svc.cluster.local.src-namespace.svc.cluster.local</code> or
<code>www.example.com.src-namespace.svc.cluster.local</code>, as for an answer rewrite <code>src-namespace</code> would not be known.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2c8dcee576a95f02378e3b89966d6410>12 - Docker Shim Removal</h1><h1 id=kubernetes-dockershim-removal>Kubernetes dockershim Removal</h1><h2 id=whats-happening>What&rsquo;s Happening?</h2><p>With Kubernetes v1.20, the built-in dockershim <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#dockershim-deprecation>was deprecated</a> and is scheduled to be <a href=https://github.com/kubernetes/enhancements/issues/2221>removed with v1.24</a>.
Don&rsquo;t Panic! The Kubernetes community has <a href=https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/>published a blogpost</a> and an <a href=https://kubernetes.io/blog/2022/02/17/dockershim-faq/>FAQ</a> with more information.</p><p>Gardener also needs to switch from using the built-in dockershim to <code>containerd</code>.
Gardener will not change already running Shoot clusters. But changes to the container runtime will be coupled to the K8s version selected by the Shoot:</p><ul><li>Starting with K8s version 1.22, Shoots not explicitly selecting a container runtime will get <code>containerd</code> instead of <code>docker</code>. Shoots can still select <code>docker</code> explicitly if needed.</li><li>Starting with K8s version 1.23 <code>docker</code> can no longer be selected.</li></ul><p>At this point in time, we have no plans to support other container runtimes, such as <code>cri-o</code>.</p><h2 id=what-should-i-do>What Should I Do?</h2><p>As a Gardener operator:</p><ul><li>Add <code>containerd</code> and <code>docker</code> to <code>.spec.machineImages[].versions[].cri.name</code> in your CloudProfile to allow users selecting a container runtime for their Shoots (see below).</li></ul><blockquote><p><strong>Note:</strong> Please take a look at our detailed information regarding <a href=#container-runtime-support-in-gardener-operating-system-extensions>container runtime support in Gardener Operating System Extensions</a>.</p></blockquote><ul><li>Update your cloud provider extensions to avoid a node rollout when a Shoot is configured from <code>cri: nil</code> to <code>cri.name: docker</code>.</li></ul><blockquote><p><strong>Note:</strong> Please take a look at our detailed information regarding <a href=#stable-worker-node-hash-support-in-gardener-provider-extensions>stable Worker node hash support in Gardener Provider Extensions</a>.</p></blockquote><p>As a shoot owner:</p><ul><li><a href=https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/#find-docker-dependencies>Check if you have dependencies to the <code>docker</code> container runtime</a>.</li></ul><blockquote><p><strong>Note:</strong> This is not only about your actual workload, but also concerns ops tooling as well as logging, monitoring, and metric agents installed on the nodes.</p></blockquote><ul><li>Test with <code>containerd</code>:<ul><li>Create a new Shoot or add a Worker Pool to an existing one.</li><li><a href=/docs/gardener/api-reference/core/#cri>Set <code>.spec.provider.workers[].cri.name: containerd</code></a> for your Shoot.</li></ul></li><li>Once testing is successful, switch to <code>containerd</code> with your production workload. You don&rsquo;t need to wait for kubernetes v1.22, <code>containerd</code> is considered production ready as of today.</li><li>If you find dependencies to <code>docker</code>, set <code>.spec.provider.workers[].cri.name: docker</code> explicitly to avoid defaulting to <code>containerd</code> once you update your Shoot to kubernetes v1.22.</li></ul><h2 id=timeline>Timeline</h2><ul><li><strong>2021-08-04:</strong> Kubernetes v1.22 released. Shoots using this version get <code>containerd</code> as default container runtime. Shoots can still select <code>docker</code> explicitly if needed.</li><li><strong>2021-12-07:</strong> Kubernetes v1.23 released. Shoots using this version can no longer select <code>docker</code> as container runtime.</li><li><strong>2022-06-28:</strong> Kubernetes v1.21 goes out of maintenance. This is the last version not affected by these changes. Make sure you have tested thoroughly and set the correct configuration for your Shoots!</li><li><strong>2022-10-28:</strong> Kubernetes v1.22 goes out of maintenance. This is the last version that you can use with <code>docker</code> as container runtime. Make sure you have removed any dependencies to <code>docker</code> as container runtime!</li></ul><p>See <a href=https://kubernetes.io/releases/>the official kubernetes documentation</a> for the exact dates for all releases.</p><h2 id=container-runtime-support-in-gardener-operating-system-extensions>Container Runtime Support in Gardener Operating System Extensions</h2><table><thead><tr><th>Operating System</th><th>docker support</th><th>containerd support</th></tr></thead><tbody><tr><td>GardenLinux</td><td>✅</td><td>>= v0.3.0</td></tr><tr><td>Ubuntu</td><td>✅</td><td>>= v1.4.0</td></tr><tr><td>SuSE CHost</td><td>✅</td><td>>= v1.14.0</td></tr><tr><td>CoreOS/FlatCar</td><td>✅</td><td>>= v1.8.0</td></tr></tbody></table><blockquote><p><strong>Note</strong>: If you&rsquo;re using a different Operating System Extension, start evaluating now if it provides support for <code>containerd</code>. Please refer to <a href=/docs/gardener/extensions/operatingsystemconfig/#cri-support>our documentation of the <code>operatingsystemconfig</code> contract</a> to understand how to support <code>containerd</code> for an Operating System Extension.</p></blockquote><h2 id=stable-worker-node-hash-support-in-gardener-provider-extensions>Stable Worker Node Hash Support in Gardener Provider Extensions</h2><p>Upgrade to these versions to avoid a node rollout when a Shoot is configured from <code>cri: nil</code> to <code>cri.name: docker</code>.</p><table><thead><tr><th>Provider Extension</th><th>Stable worker hash support</th></tr></thead><tbody><tr><td>Alicloud</td><td>>= 1.26.0</td></tr><tr><td>AWS</td><td>>= 1.27.0</td></tr><tr><td>Azure</td><td>>= 1.21.0</td></tr><tr><td>GCP</td><td>>= 1.18.0</td></tr><tr><td>OpenStack</td><td>>= 1.21.0</td></tr><tr><td>vSphere</td><td>>= 0.11.0</td></tr></tbody></table><blockquote><p><strong>Note</strong>: If you&rsquo;re using a different Provider Extension, start evaluating now if it keeps the worker hash stable when switching from <code>.spec.provider.workers[].cri: nil</code> to <code>.spec.provider.workers[].cri.name: docker</code>. This doesn&rsquo;t impact functional correctness, however, a node rollout will be triggered when users decide to configure <code>docker</code> for their shoots.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-c846de8dc75ecb3d656ca901da206be3>13 - ExposureClasses</h1><h1 id=exposureclasses>ExposureClasses</h1><p>The Gardener API server provides a cluster-scoped <code>ExposureClass</code> resource.
This resource is used to allow exposing the control plane of a Shoot cluster in various network environments like restricted corporate networks, DMZ, etc.</p><h2 id=background>Background</h2><p>The <code>ExposureClass</code> resource is based on the concept for the <code>RuntimeClass</code> resource in Kubernetes.</p><p>A <code>RuntimeClass</code> abstracts the installation of a certain container runtime (e.g., gVisor, Kata Containers) on all nodes or a subset of the nodes in a Kubernetes cluster.
See <a href=https://kubernetes.io/docs/concepts/containers/runtime-class/>Runtime Class</a> for more information.</p><p>In contrast, an <code>ExposureClass</code> abstracts the ability to expose a Shoot clusters control plane in certain network environments (e.g., corporate networks, DMZ, internet) on all Seeds or a subset of the Seeds.</p><p>Example: <code>RuntimeClass</code> and <code>ExposureClass</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: node.k8s.io/v1
</span></span><span style=display:flex><span>kind: RuntimeClass
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gvisor
</span></span><span style=display:flex><span>handler: gvisorconfig
</span></span><span style=display:flex><span><span style=color:green># scheduling:</span>
</span></span><span style=display:flex><span><span style=color:green>#   nodeSelector:</span>
</span></span><span style=display:flex><span><span style=color:green>#     env: prod</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>kind: ExposureClass
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: internet
</span></span><span style=display:flex><span>handler: internet-config
</span></span><span style=display:flex><span><span style=color:green># scheduling:</span>
</span></span><span style=display:flex><span><span style=color:green>#   seedSelector:</span>
</span></span><span style=display:flex><span><span style=color:green>#     matchLabels:</span>
</span></span><span style=display:flex><span><span style=color:green>#       network/env: internet</span>
</span></span></code></pre></div><p>Similar to <code>RuntimeClasses</code>, <code>ExposureClasses</code> also define a <code>.handler</code> field reflecting the name reference for the corresponding CRI configuration of the <code>RuntimeClass</code> and the control plane exposure configuration for the <code>ExposureClass</code>.</p><p>The CRI handler for <code>RuntimeClasses</code> is usually installed by an administrator (e.g., via a <code>DaemonSet</code> which installs the corresponding container runtime on the nodes).
The control plane exposure configuration for <code>ExposureClasses</code> will be also provided by an administrator.
This exposure configuration is part of the gardenlet configuration, as this component is responsible to configure the control plane accordingly.
See the <a href=#gardenlet-configuration-exposureclass-handlers>gardenlet Configuration <code>ExposureClass</code> Handlers</a> section for more information.</p><p>The <code>RuntimeClass</code> also supports the selection of a node subset (which have the respective controller runtime binaries installed) for pod scheduling via its <code>.scheduling</code> section.
The <code>ExposureClass</code> also supports the selection of a subset of available Seed clusters whose gardenlet is capable of applying the exposure configuration for the Shoot control plane accordingly via its <code>.scheduling</code> section.</p><h2 id=usage-by-a-shoot>Usage by a <code>Shoot</code></h2><p>A <code>Shoot</code> can reference an <code>ExposureClass</code> via the <code>.spec.exposureClassName</code> field.</p><blockquote><p>⚠️ When creating a <code>Shoot</code> resource, the Gardener scheduler will try to assign the <code>Shoot</code> to a <code>Seed</code> which will host its control plane.</p></blockquote><p>The scheduling behaviour can be influenced via the <code>.spec.seedSelectors</code> and/or <code>.spec.tolerations</code> fields in the <code>Shoot</code>.
<code>ExposureClass</code>es can also contain scheduling instructions.
If a <code>Shoot</code> is referencing an <code>ExposureClass</code>, then the scheduling instructions of both will be merged into the <code>Shoot</code>.
Those unions of scheduling instructions might lead to a selection of a <code>Seed</code> which is not able to deal with the <code>handler</code> of the <code>ExposureClass</code> and the <code>Shoot</code> creation might end up in an error.
In such case, the <code>Shoot</code> scheduling instructions should be revisited to check that they are not interfering with the ones from the <code>ExposureClass</code>.
If this is not feasible, then the combination with the <code>ExposureClass</code> might not be possible and you need to contact your Gardener administrator.</p><details><summary>Example: Shoot and ExposureClass scheduling instructions merge flow</summary><ol><li>Assuming there is the following <code>Shoot</code> which is referencing the <code>ExposureClass</code> below:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: abc
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  exposureClassName: abc
</span></span><span style=display:flex><span>  seedSelectors:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      env: prod
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ExposureClass
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: abc
</span></span><span style=display:flex><span>handler: abc
</span></span><span style=display:flex><span>scheduling:
</span></span><span style=display:flex><span>  seedSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      network: internal
</span></span></code></pre></div><ol start=2><li>Both <code>seedSelectors</code> would be merged into the <code>Shoot</code>. The result would be the following:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: abc
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  exposureClassName: abc
</span></span><span style=display:flex><span>  seedSelectors:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      env: prod
</span></span><span style=display:flex><span>      network: internal
</span></span></code></pre></div><ol start=3><li>Now the Gardener Scheduler would try to find a <code>Seed</code> with those labels.</li></ol><ul><li>If there are <strong>no</strong> Seeds with matching labels for the seed selector, then the <code>Shoot</code> will be unschedulable.</li><li>If there are Seeds with matching labels for the seed selector, then the Shoot will be assigned to the best candidate after the scheduling strategy is applied, see <a href=/docs/gardener/concepts/scheduler/#algorithm-overview>Gardener Scheduler</a>.<ul><li>If the <code>Seed</code> is <strong>not</strong> able to serve the <code>ExposureClass</code> handler <code>abc</code>, then the Shoot will end up in error state.</li><li>If the <code>Seed</code> is able to serve the <code>ExposureClass</code> handler <code>abc</code>, then the <code>Shoot</code> will be created.</li></ul></li></ul></details><h2 id=gardenlet-configuration-exposureclass-handlers>gardenlet Configuration <code>ExposureClass</code> Handlers</h2><p>The gardenlet is responsible to realize the control plane exposure strategy defined in the referenced <code>ExposureClass</code> of a <code>Shoot</code>.</p><p>Therefore, the <code>GardenletConfiguration</code> can contain an <code>.exposureClassHandlers</code> list with the respective configuration.</p><p>Example of the <code>GardenletConfiguration</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>exposureClassHandlers:
</span></span><span style=display:flex><span>- name: internet-config
</span></span><span style=display:flex><span>  loadBalancerService:
</span></span><span style=display:flex><span>    annotations:
</span></span><span style=display:flex><span>      loadbalancer/network: internet
</span></span><span style=display:flex><span>- name: internal-config
</span></span><span style=display:flex><span>  loadBalancerService:
</span></span><span style=display:flex><span>    annotations:
</span></span><span style=display:flex><span>      loadbalancer/network: internal
</span></span><span style=display:flex><span>  sni:
</span></span><span style=display:flex><span>    ingress:
</span></span><span style=display:flex><span>      namespace: ingress-internal
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        network: internal
</span></span></code></pre></div><p>Each gardenlet can define how the handler of a certain <code>ExposureClass</code> needs to be implemented for the Seed(s) where it is responsible for.</p><p>The <code>.name</code> is the name of the handler config and it must match to the <code>.handler</code> in the <code>ExposureClass</code>.</p><p>All control planes on a <code>Seed</code> are exposed via a load balancer, either a dedicated one or a central shared one.
The load balancer service needs to be configured in a way that it is reachable from the target network environment.
Therefore, the configuration of load balancer service need to be specified, which can be done via the <code>.loadBalancerService</code> section.
The common way to influence load balancer service behaviour is via annotations where the respective cloud-controller-manager will react on and configure the infrastructure load balancer accordingly.</p><p>In case the gardenlet runs with activated <code>APIServerSNI</code> feature flag (default), the control planes on a <code>Seed</code> will be exposed via a central load balancer and with Envoy via TLS SNI passthrough proxy.
In this case, the gardenlet will install a dedicated ingress gateway (Envoy + load balancer + respective configuration) for each handler on the <code>Seed</code>.
The configuration of the ingress gateways can be controlled via the <code>.sni</code> section in the same way like for the default ingress gateways.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-455bea128db452349701d12b6d003975>14 - Ipv6</h1><h1 id=ipv6-in-gardener-clusters>IPv6 in Gardener Clusters</h1><blockquote><p>🚧 IPv6 networking is currently under development.</p></blockquote><h2 id=ipv6-single-stack-networking>IPv6 Single-Stack Networking</h2><p><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/21-ipv6-singlestack-local.md>GEP-21</a> proposes IPv6 Single-Stack Support in the local Gardener environment.
This documentation will be enhanced while implementing GEP-21, see <a href=https://github.com/gardener/gardener/issues/7051>gardener/gardener#7051</a>.</p><p>To use IPv6 single-stack networking, the <a href=/docs/gardener/deployment/feature_gates/>feature gate</a> <code>IPv6SingleStack</code> must be enabled on gardener-apiserver and gardenlet.</p><h2 id=development-setup>Development Setup</h2><p>Developing or testing IPv6-related features requires a Linux machine (docker only supports IPv6 on Linux) and native IPv6 connectivity to the internet.
If you&rsquo;re on a different OS or don&rsquo;t have IPv6 connectivity in your office environment or via your home ISP, make sure to check out <a href=https://github.com/gardener-community/dev-box-gcp>gardener-community/dev-box-gcp</a>, which allows you to circumvent these limitations.</p><p>You can follow the guide on <a href=/docs/gardener/deployment/getting_started_locally/>Deploying Gardener Locally</a> for setting up an IPv6 gardener for testing or development purposes.</p><h2 id=container-images>Container Images</h2><p>If you plan on using custom images, make sure your registry supports IPv6 access.</p><p>The <code>docker.io</code> registry doesn&rsquo;t support pulling images over IPv6 (see <a href=https://www.docker.com/blog/beta-ipv6-support-on-docker-hub-registry/>Beta IPv6 Support on Docker Hub Registry</a>).
There is a <a href=https://github.com/gardener/ci-infra/blob/92782bedd92815639abf4dc14b2c484f77c6e57d/config/jobs/ci-infra/copy-images.yaml>prow job</a> copying images from Docker Hub that are needed in gardener components to the gardener GCR under the prefix <code>eu.gcr.io/gardener-project/3rd/</code> (see the <a href=https://github.com/gardener/ci-infra/tree/master/config/images>documentation</a> or <a href=https://github.com/gardener/ci-infra/issues/619>gardener/ci-infra#619</a>).
If you want to use a new image from Docker Hub or upgrade an already used image to a newer tag, please open a PR to the ci-infra repository that modifies the job&rsquo;s list of images to copy: <a href=https://github.com/gardener/ci-infra/blob/master/config/images/images.yaml><code>images.yaml</code></a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e25b3c63361fa049e4ac90bb38e50ff1>15 - Istio</h1><h1 id=istio>Istio</h1><p><a href=https://istio.io>Istio</a> offers a service mesh implementation with focus on several important features - traffic, observability, security, and policy.</p><h2 id=prerequisites>Prerequisites</h2><ul><li>Third-party JWT is used, therefore each Seed cluster where this feature is enabled must have <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> enabled.</li><li>Kubernetes 1.16+</li></ul><h2 id=differences-with-istios-default-profile>Differences with Istio&rsquo;s Default Profile</h2><p>The <a href=https://istio.io/docs/setup/additional-setup/config-profiles/>default profile</a> which is recommended for production deployment, is not suitable for the Gardener use case, as it offers more functionality than desired. The current installation goes through heavy refactorings due to the <code>IstioOperator</code> and the mixture of Helm values + Kubernetes API specification makes configuring and fine-tuning it very hard. A more simplistic deployment is used by Gardener. The differences are the following:</p><ul><li>Telemetry is not deployed.</li><li><code>istiod</code> is deployed.</li><li><code>istio-ingress-gateway</code> is deployed in a separate <code>istio-ingress</code> namespace.</li><li><code>istio-egress-gateway</code> is not deployed.</li><li>None of the Istio addons are deployed.</li><li>Mixer (deprecated) is not deployed.</li><li>Mixer CDRs are not deployed.</li><li>Kubernetes <code>Service</code>, Istio&rsquo;s <code>VirtualService</code> and <code>ServiceEntry</code> are <strong>NOT</strong> advertised in the service mesh. This means that if a <code>Service</code> needs to be accessed directly from the Istio Ingress Gateway, it should have <code>networking.istio.io/exportTo: "*"</code> annotation. <code>VirtualService</code> and <code>ServiceEntry</code> must have <code>.spec.exportTo: ["*"]</code> set on them respectively.</li><li>Istio injector is not enabled.</li><li>mTLS is enabled by default.</li></ul><h2 id=handling-multiple-availability-zones-with-istio>Handling Multiple Availability Zones with Istio</h2><p>For various reasons, e.g., improved resiliency to certain failures, it may be beneficial to use multiple availability zones in a seed cluster. While availability zones have advantages in being able to cover some failure domains, they also come with some additional challenges. Most notably, the latency across availability zone boundaries is higher than within an availability zone. Furthermore, there might be additional cost implied by network traffic crossing an availability zone boundary. Therefore, it may be useful to try to keep traffic within an availability zone if possible. The istio deployment as part of Gardener has been adapted to allow this.</p><p>A seed cluster spanning multiple availability zones may be used for <a href=/docs/gardener/usage/shoot_high_availability/>highly-available shoot control planes</a>. Those control planes may use a single or multiple availability zones. In addition to that, ordinary non-highly-available shoot control planes may be scheduled to such a seed cluster as well. The result is that the seed cluster may have control planes spanning multiple availability zones and control planes that are pinned to exactly one availability zone. These two types need to be handled differently when trying to prevent unnecessary cross-zonal traffic.</p><p>The goal is achieved by using multiple istio ingress gateways. The default istio ingress gateway spans all availability zones. It is used for multi-zonal shoot control planes. For each availability zone, there is an additional istio ingress gateway, which is utilized only for single-zone shoot control planes pinned to this availability zone. This is illustrated in the following diagram.</p><p><img src=/__resources/multi-zonal-istio_bf47f6.png alt="Multi Availability Zone Handling in Istio"></p><p>Please note that operators may need to perform additional tuning to prevent cross-zonal traffic completely. The <a href=/docs/gardener/usage/seed_settings/#load-balancer-services>loadbalancer settings in the seed specification</a> offer various options, e.g., by setting the external traffic policy to <code>local</code> or using infrastructure specific loadbalancer annotations.</p><p>Furthermore, note that this approach is also taken in case <a href=/docs/gardener/usage/exposureclasses/><code>ExposureClass</code>es</a> are used. For each exposure class, additional zonal istio ingress gateways may be deployed to cover for single-zone shoot control planes using the exposure class.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c25983cd195ac0d27b90213377bdf169>16 - Logging</h1><h1 id=logging-stack>Logging Stack</h1><h2 id=motivation>Motivation</h2><p>Kubernetes uses the underlying container runtime logging, which does not persist logs for stopped and destroyed containers. This makes it difficult to investigate issues in the very common case of not running containers. Gardener provides a solution to this problem for the managed cluster components by introducing its own logging stack.</p><h2 id=components>Components</h2><ul><li>A Fluent-bit daemonset which works like a log collector and custom Golang plugin which spreads log messages to their Vali instances.</li><li>One Vali Statefulset in the <code>garden</code> namespace which contains logs for the seed cluster and one per shoot namespace which contains logs for shoot&rsquo;s controlplane.</li><li>One Plutono Deployment in <code>garden</code> namespace and two Deployments per shoot namespace (one exposed to the end users and one for the operators). Plutono is the UI component used in the logging stack.</li></ul><p><img src=/__resources/logging-architecture_c8dc32.png alt></p><h2 id=container-logs-rotation-and-retention>Container Logs Rotation and Retention</h2><p>Container <a href=https://kubernetes.io/docs/concepts/cluster-administration/logging/#log-rotation>log rotation</a> in Kubernetes describes a subtile but important implementation detail depending on the type of the used high-level container runtime. When the used container runtime is not CRI compliant (such as <code>dockershim</code>), then the <code>kubelet</code> does not provide any rotation or retention implementations, hence leaving those aspects to the downstream components. When the used container runtime is CRI compliant (such as <code>containerd</code>), then the <code>kubelet</code> provides the necessary implementation with two configuration options:</p><ul><li><code>ContainerLogMaxSize</code> for rotation</li><li><code>ContainerLogMaxFiles</code> for retention</li></ul><h3 id=docker-container-runtime>Docker Container Runtime</h3><p>In this case, the log rotation and retention is implemented by a <code>logrotate</code> service provisioned by Gardener, which rotates logs once <code>100M</code> size is reached. Logs are compressed on daily basis and retained for a maximum period of <code>14d</code>.</p><h3 id=containerd-runtime>ContainerD Runtime</h3><p>In this case, it is possible to configure the <code>containerLogMaxSize</code> and <code>containerLogMaxFiles</code> fields in the Shoot specification. Both fields are optional and if nothing is specified, then the <code>kubelet</code> rotates on the same size <code>100M</code> as in the <code>docker</code> container runtime. Those fields are part of provider&rsquo;s workers definition. Here is an example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>      - cri:
</span></span><span style=display:flex><span>          name: containerd
</span></span><span style=display:flex><span>        kubernetes:
</span></span><span style=display:flex><span>          kubelet:
</span></span><span style=display:flex><span>            <span style=color:green># accepted values are of resource.Quantity</span>
</span></span><span style=display:flex><span>            containerLogMaxSize: 150Mi
</span></span><span style=display:flex><span>            containerLogMaxFiles: 10
</span></span></code></pre></div><p>The values of the <code>containerLogMaxSize</code> and <code>containerLogMaxFiles</code> fields need to be considered with care since container log files claim disk space from the host. On the opposite side, log rotations on too small sizes may result in frequent rotations which can be missed by other components (log shippers) observing these rotations.</p><p>In the majority of the cases, the defaults should do just fine. Custom configuration might be of use under rare conditions.</p><h2 id=extension-of-the-logging-stack>Extension of the Logging Stack</h2><p>The logging stack is extended to scrape logs from the systemd services of each shoots&rsquo; nodes and from all Gardener components in the shoot <code>kube-system</code> namespace. These logs are exposed only to the Gardener operators.</p><p>Also, in the shoot control plane an <code>event-logger</code> pod is deployed, which scrapes events from the shoot <code>kube-system</code> namespace and shoot <code>control-plane</code> namespace in the seed. The <code>event-logger</code> logs the events to the standard output. Then the <code>fluent-bit</code> gets these events as container logs and sends them to the Vali in the shoot control plane (similar to how it works for any other control plane component).
<img src=/__resources/shoot-node-logging-architecture_23c018.png alt></p><h2 id=how-to-access-the-logs>How to Access the Logs</h2><p>The logs are accessible via Plutono. To access them:</p><ol><li><p>Authenticate via basic auth to gain access to Plutono.
The Plutono URL can be found in the <code>Logging and Monitoring</code> section of a cluster in the Gardener Dashboard alongside the credentials.
The secret containing the credentials is stored in the project namespace following the naming pattern <code>&lt;shoot-name>.monitoring</code>.
For Gardener operators, the credentials are also stored in the control-plane (<code>shoot--&lt;project-name>--&lt;shoot-name></code>) namespace in the <code>observability-ingress-users-&lt;hash></code> secret in the seed.</p></li><li><p>Plutono contains several dashboards that aim to facilitate the work of operators and users.
From the <code>Explore</code> tab, users and operators have unlimited abilities to extract and manipulate logs.</p></li></ol><blockquote><p><strong>Note:</strong> Gardener Operators are people part of the Gardener team with operator permissions, not operators of the end-user cluster!</p></blockquote><h3 id=how-to-use-the-explore-tab>How to Use the <code>Explore</code> Tab</h3><p>If you click on the <code>Log browser ></code> button, you will see all of the available labels.
Clicking on the label, you can see all of its available values for the given period of time you have specified.
If you are searching for logs for the past one hour, do not expect to see labels or values for which there were no logs for that period of time.
By clicking on a value, Plutono automatically eliminates all other labels and/or values with which no valid log stream can be made.
After choosing the right labels and their values, click on the <code>Show logs</code> button.
This will build <code>Log query</code> and execute it.
This approach is convenient when you don&rsquo;t know the labels names or they values.
<img src=/__resources/explore-button-usage_0dfdca.png alt></p><p>Once you feel comfortable, you can start to use the <a href=https://github.com/credativ/plutono>LogQL</a> language to search for logs.
Next to the <code>Log browser ></code> button is the place where you can type log queries.</p><p>Examples:</p><ol><li><p>If you want to get logs for <code>calico-node-&lt;hash></code> pod in the cluster <code>kube-system</code>:
The name of the node on which <code>calico-node</code> was running is known, but not the hash suffix of the <code>calico-node</code> pod.
Also we want to search for errors in the logs.</p><p><code>{pod_name=~"calico-node-.+", nodename="ip-10-222-31-182.eu-central-1.compute.internal"} |~ "error"</code></p><p>Here, you will get as much help as possible from the Plutono by giving you suggestions and auto-completion.</p></li><li><p>If you want to get the logs from <code>kubelet</code> systemd service of a given node and search for a pod name in the logs:</p><p><code>{unit="kubelet.service", nodename="ip-10-222-31-182.eu-central-1.compute.internal"} |~ "pod name"</code></p></li></ol><blockquote><p><strong>Note:</strong> Under <code>unit</code> label there is only the <code>docker</code>, <code>containerd</code>, <code>kubelet</code> and <code>kernel</code> logs.</p></blockquote><ol start=3><li><p>If you want to get the logs from <code>cloud-config-downloader</code> systemd service of a given node and search for a string in the logs:</p><p><code>{job="systemd-combine-journal",nodename="ip-10-222-31-182.eu-central-1.compute.internal"} | unpack | unit="cloud-config-downloader.service" |~ "last execution was"</code></p></li></ol><blockquote><p><strong>Note:</strong> <code>{job="systemd-combine-journal",nodename="&lt;node name>"}</code> stream <a href=https://github.com/credativ/plutono>pack</a> all logs from systemd services except <code>docker</code>, <code>containerd</code>, <code>kubelet</code>, and <code>kernel</code>. To filter those log by unit, you have to <a href=https://github.com/credativ/plutono>unpack</a> them first.</p></blockquote><ol start=4><li>Retrieving events:</li></ol><ul><li><p>If you want to get the events from the shoot <code>kube-system</code> namespace generated by <code>kubelet</code> and related to the <code>node-problem-detector</code>:</p><p><code>{job="event-logging"} | unpack | origin_extracted="shoot",source="kubelet",object=~".*node-problem-detector.*"</code></p></li><li><p>If you want to get the events generated by MCM in the shoot control plane in the seed:</p><p><code>{job="event-logging"} | unpack | origin_extracted="seed",source=~".*machine-controller-manager.*"</code></p><blockquote><p><strong>Note:</strong> In order to group events by origin, one has to specify <code>origin_extracted</code> because the <code>origin</code> label is reserved for all of the logs from the seed and the <code>event-logger</code> resides in the seed, so all of its logs are coming as they are only from the seed. The actual origin is embedded in the unpacked event. When unpacked, the embedded <code>origin</code> becomes <code>origin_extracted</code>.</p></blockquote></li></ul><h2 id=expose-logs-for-component-to-user-plutono>Expose Logs for Component to User Plutono</h2><p>Exposing logs for a new component to the User&rsquo;s Plutono is described in the <a href=/docs/gardener/extensions/logging-and-monitoring/#how-to-expose-logs-to-the-users>How to Expose Logs to the Users</a> section.</p><h2 id=configuration>Configuration</h2><h3 id=fluent-bit>Fluent-bit</h3><p>The Fluent-bit configurations can be found on <code>pkg/component/logging/fluentoperator/customresources</code>
There are six different specifications:</p><ul><li>FluentBit: Defines the fluent-bit DaemonSet specifications</li><li>ClusterFluentBitConfig: Defines the labelselectors of the resources which fluent-bit will match</li><li>ClusterInput: Defines the location of the input stream of the logs</li><li>ClusterOutput: Defines the location of the output source (Vali for example)</li><li>ClusterFilter: Defines filters which match specific keys</li><li>ClusterParser: Defines parsers which are used by the filters</li></ul><h3 id=vali>Vali</h3><p>The Vali configurations can be found on <code>charts/seed-bootstrap/charts/vali/templates/vali-configmap.yaml</code></p><p>The main specifications there are:</p><ul><li>Index configuration: Currently the following one is used:</li></ul><pre tabindex=0><code>    schema_config:
      configs:
      - from: 2018-04-15
        store: boltdb
        object_store: filesystem
        schema: v11
        index:
          prefix: index_
          period: 24h
</code></pre><ul><li><code>from</code>: Is the date from which logs collection is started. Using a date in the past is okay.</li><li><code>store</code>: The DB used for storing the index.</li><li><code>object_store</code>: Where the data is stored.</li><li><code>schema</code>: Schema version which should be used (v11 is currently recommended).</li><li><code>index.prefix</code>: The prefix for the index.</li><li><code>index.period</code>: The period for updating the indices.</li></ul><p><strong>Adding a new index happens with new config block definition. The <code>from</code> field should start from the current day + previous <code>index.period</code> and should not overlap with the current index. The <code>prefix</code> also should be different.</strong></p><pre tabindex=0><code>    schema_config:
      configs:
      - from: 2018-04-15
        store: boltdb
        object_store: filesystem
        schema: v11
        index:
          prefix: index_
          period: 24h
      - from: 2020-06-18
        store: boltdb
        object_store: filesystem
        schema: v11
        index:
          prefix: index_new_
          period: 24h
</code></pre><ul><li>chunk_store_config Configuration</li></ul><pre tabindex=0><code>    chunk_store_config:
      max_look_back_period: 336h
</code></pre><p><strong><code>chunk_store_config.max_look_back_period</code> should be the same as the <code>retention_period</code></strong></p><ul><li>table_manager Configuration</li></ul><pre tabindex=0><code>    table_manager:
      retention_deletes_enabled: true
      retention_period: 336h
</code></pre><p><code>table_manager.retention_period</code> is the living time for each log message. Vali will keep messages for (<code>table_manager.retention_period</code> - <code>index.period</code>) time due to specification in the Vali implementation.</p><h3 id=plutono>Plutono</h3><p>The Plutono configurations can be found on <code>charts/seed-bootstrap/charts/templates/plutono/plutono-datasources-configmap.yaml</code> and
<code>charts/seed-monitoring/charts/plutono/tempates/plutono-datasources-configmap.yaml</code></p><p>This is the Vali configuration that Plutono uses:</p><pre tabindex=0><code>    - name: vali
      type: vali
      access: proxy
      url: http://logging.{{ .Release.Namespace }}.svc:3100
      jsonData:
        maxLines: 5000
</code></pre><ul><li><code>name</code>: Is the name of the datasource.</li><li><code>type</code>: Is the type of the datasource.</li><li><code>access</code>: Should be set to proxy.</li><li><code>url</code>: Vali&rsquo;s url</li><li><code>svc</code>: Vali&rsquo;s port</li><li><code>jsonData.maxLines</code>: The limit of the log messages which Plutono will show to the users.</li></ul><p><strong>Decrease this value if the browser works slowly!</strong></p></div><div class=td-content style=page-break-before:always><h1 id=pg-b03cef45b6b25f644cf6ab64d3c5c66c>17 - Managed Seed</h1><h1 id=register-shoot-as-seed>Register Shoot as Seed</h1><p>An existing shoot can be registered as a seed by creating a <code>ManagedSeed</code> resource. This resource contains:</p><ul><li>The name of the shoot that should be registered as seed.</li><li>A <code>gardenlet</code> section that contains:<ul><li><code>gardenlet</code> deployment parameters, such as the number of replicas, the image, etc.</li><li>The <code>GardenletConfiguration</code> resource that contains controllers configuration, feature gates, and a <code>seedConfig</code> section that contains the <code>Seed</code> spec and parts of its metadata.</li><li>Additional configuration parameters, such as the garden connection bootstrap mechanism (see <a href=/docs/gardener/concepts/gardenlet/#tls-bootstrapping>TLS Bootstrapping</a>), and whether to merge the provided configuration with the configuration of the parent <code>gardenlet</code>.</li></ul></li></ul><p><code>gardenlet</code> is deployed to the shoot, and it registers a new seed upon startup based on the <code>seedConfig</code> section.</p><blockquote><p><strong>Note:</strong> Earlier Gardener allowed specifying a <code>seedTemplate</code> directly in the <code>ManagedSeed</code> resource. This feature is discontinued, any seed configuration must be via the <code>GardenletConfiguration</code>.</p></blockquote><p>Note the following important aspects:</p><ul><li>Unlike the <code>Seed</code> resource, the <code>ManagedSeed</code> resource is namespaced. Currently, managed seeds are restricted to the <code>garden</code> namespace.</li><li>The newly created <code>Seed</code> resource always has the same name as the <code>ManagedSeed</code> resource. Attempting to specify a different name in the <code>seedConfig</code> will fail.</li><li>The <code>ManagedSeed</code> resource must always refer to an existing shoot. Attempting to create a <code>ManagedSeed</code> referring to a non-existing shoot will fail.</li><li>A shoot that is being referred to by a <code>ManagedSeed</code> cannot be deleted. Attempting to delete such a shoot will fail.</li><li>You can omit practically everything from the <code>gardenlet</code> section, including all or most of the <code>Seed</code> spec fields. Proper defaults will be supplied in all cases, based either on the most common use cases or the information already available in the <code>Shoot</code> resource.</li><li>Also, if your seed is configured to host HA shoot control planes, then <code>gardenlet</code> will be deployed with multiple replicas across nodes or availability zones by default.</li><li>Some <code>Seed</code> spec fields, for example the provider type and region, networking CIDRs for pods, services, and nodes, etc., must be the same as the corresponding <code>Shoot</code> spec fields of the shoot that is being registered as seed. Attempting to use different values (except empty ones, so that they are supplied by the defaulting mechanims) will fail.</li></ul><h2 id=deploying-gardenlet-to-the-shoot>Deploying gardenlet to the Shoot</h2><p>To register a shoot as a seed and deploy <code>gardenlet</code> to the shoot using a default configuration, create a <code>ManagedSeed</code> resource similar to the following:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: seedmanagement.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ManagedSeed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-managed-seed
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  shoot:
</span></span><span style=display:flex><span>    name: crazy-botany
</span></span><span style=display:flex><span>  gardenlet: {}
</span></span></code></pre></div><p>For an example that uses non-default configuration, see <a href=https://github.com/gardener/gardener/blob/master/example/55-managedseed-gardenlet.yaml>55-managed-seed-gardenlet.yaml</a></p><h3 id=renewing-the-gardenlet-kubeconfig-secret>Renewing the Gardenlet Kubeconfig Secret</h3><p>In order to make the <code>ManagedSeed</code> controller renew the gardenlet&rsquo;s kubeconfig secret, annotate the <code>ManagedSeed</code> with <code>gardener.cloud/operation=renew-kubeconfig</code>. This will trigger a reconciliation during which the kubeconfig secret is deleted and the bootstrapping is performed again (during which gardenlet obtains a new client certificate).</p><p>It is also possible to trigger the renewal on the secret directly, see <a href=/docs/gardener/concepts/gardenlet/#rotate-certificates-using-bootstrap-kubeconfig>Rotate Certificates Using Bootstrap kubeconfig</a>.</p><h3 id=specifying-apiserver-replicas-and-autoscaler-options>Specifying <code>apiServer</code> <code>replicas</code> and <code>autoscaler</code> Options</h3><p>There are few configuration options that are not supported in a <code>Shoot</code> resource but due to backward compatibility reasons it is possible to specify them for a <code>Shoot</code> that is referred by a <code>ManagedSeed</code>. These options are:</p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>apiServer.autoscaler.minReplicas</code></td><td>Controls the minimum number of <code>kube-apiserver</code> replicas for the shoot registered as seed cluster.</td></tr><tr><td><code>apiServer.autoscaler.maxReplicas</code></td><td>Controls the maximum number of <code>kube-apiserver</code> replicas for the shoot registered as seed cluster.</td></tr><tr><td><code>apiServer.replicas</code></td><td>Controls how many <code>kube-apiserver</code> replicas the shoot registered as seed cluster gets by default.</td></tr></tbody></table><p>It is possible to specify these options via the <code>shoot.gardener.cloud/managed-seed-api-server</code> annotation on the Shoot resource. Example configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    shoot.gardener.cloud/managed-seed-api-server: <span style=color:#a31515>&#34;apiServer.replicas=3,apiServer.autoscaler.minReplicas=3,apiServer.autoscaler.maxReplicas=6&#34;</span>
</span></span></code></pre></div><h3 id=enforced-configuration-options>Enforced Configuration Options</h3><p>The following configuration options are enforced by Gardener API server for the ManagedSeed resources:</p><ol><li><p>The vertical pod autoscaler should be enabled from the Shoot specification.</p><p>The vertical pod autoscaler is a prerequisite for a Seed cluster. It is possible to enable the VPA feature for a Seed <a href=/docs/gardener/usage/seed_settings/#vertical-pod-autoscaler>(using the Seed spec)</a> and for a Shoot <a href=/docs/gardener/usage/shoot_autoscaling/#vertical-pod-auto-scaling>(using the Shoot spec)</a>. In context of <code>ManagedSeed</code>s, enabling the VPA in the Seed spec (instead of the Shoot spec) offers less flexibility and increases the network transfer and cost. Due to these reasons, the Gardener API server enforces the vertical pod autoscaler to be enabled from the Shoot specification.</p></li><li><p>The nginx-ingress addon should not be enabled for a Shoot referred by a ManagedSeed.</p><p>An Ingress controller is also a prerequisite for a Seed cluster. For a Seed cluster, it is possible to enable Gardener managed Ingress controller or to deploy self-managed Ingress controller. There is also the nginx-ingress addon that can be enabled for a Shoot (using the Shoot spec). However, the Shoot nginx-ingress addon is in deprecated mode and it is not recommended for production clusters. Due to these reasons, the Gardener API server does not allow the Shoot nginx-ingress addon to be enabled for ManagedSeeds.</p></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-41c26dd027fe9d2196cdfe967044e45a>18 - Network Policies</h1><h1 id=networkpolicys-in-garden-seed-shoot-clusters><code>NetworkPolicy</code>s In Garden, Seed, Shoot Clusters</h1><p>This document describes which <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/>Kubernetes <code>NetworkPolicy</code>s</a> deployed by Gardener into the various clusters.</p><h2 id=garden-cluster>Garden Cluster</h2><p><em>(via <code>gardener-operator</code> and <code>gardener-resource-manager</code>)</em></p><p>The <code>gardener-operator</code> runs a <a href=/docs/gardener/concepts/operator/#networkpolicy-controller-registrar><code>NetworkPolicy</code> controller</a> which is responsible for the following namespaces:</p><ul><li><code>garden</code></li><li><code>istio-system</code></li><li><code>*istio-ingress-*</code></li><li><code>shoot-*</code></li><li><code>extension-*</code> (only when the <a href=/docs/gardener/deployment/feature_gates/><code>FullNetworkPoliciesInRuntimeCluster</code> feature gate</a> is enabled (in case the garden cluster is a seed cluster at the same time))</li></ul><p>It deploys the following so-called &ldquo;general <code>NetworkPolicy</code>s&rdquo;:</p><table><thead><tr><th>Name</th><th>Purpose</th></tr></thead><tbody><tr><td><code>deny-all</code></td><td><a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-and-all-egress-traffic>Denies all ingress and egress traffic</a> for all pods in this namespace. Hence, all traffic must be explicitly allowed.</td></tr><tr><td><code>allow-to-dns</code></td><td>Allows egress traffic from pods labeled with <code>networking.gardener.cloud/to-dns=allowed</code> to DNS pods running in the <code>kube-sytem</code> namespace. In practice, most of the pods performing network egress traffic need this label.</td></tr><tr><td><code>allow-to-runtime-apiserver</code></td><td>Allows egress traffic from pods labeled with <code>networking.gardener.cloud/to-runtime-apiserver=allowed</code> to the API server of the runtime cluster.</td></tr><tr><td><code>allow-to-blocked-cidrs</code></td><td>Allows egress traffic from pods labeled with <code>networking.gardener.cloud/to-blocked-cidrs=allowed</code> to explicitly blocked addresses configured by human operators (configured via <code>.spec.networking.blockedCIDRs</code> in the <code>Seed</code>). For instance, this can be used to block the cloud provider&rsquo;s metadata service.</td></tr><tr><td><code>allow-to-public-networks</code></td><td>Allows egress traffic from pods labeled with <code>networking.gardener.cloud/allow-to-public-networks=allowed</code> to all public network IPs, except for private networks (RFC1918), carrier-grade NAT (RFC6598), and explicitly blocked addresses configured by human operators for all pods labeled with <code>networking.gardener.cloud/to-public-networks=allowed</code>. In practice, this blocks egress traffic to all networks in the cluster and only allows egress traffic to public IPv4 addresses.</td></tr><tr><td><code>allow-to-private-networks</code></td><td>Allows egress traffic from pods labeled with <code>networking.gardener.cloud/allow-to-private-networks=allowed</code> to the private networks (RFC1918) and carrier-grade NAT (RFC6598) except for cluster-specific networks (configured via <code>.spec.networks</code> in the <code>Seed</code>).</td></tr><tr><td><code>allow-to-shoot-networks</code></td><td>Allows egress traffic from pods labeled with <code>networking.gardener.cloud/to-shoot-networks=allowed</code> to IPv4 blocks belonging to the shoot networks (configured via <code>.spec.networking</code> in the <code>Shoot</code>). In practice, this should be used by components which use VPN tunnel to communicate to pods in the shoot cluster. Note that this policy only exists in <code>shoot-*</code> namespaces.</td></tr></tbody></table><p>Apart from those, the <code>gardener-operator</code> also enables the <a href=/docs/gardener/concepts/resource-manager/#networkpolicy-controller><code>NetworkPolicy</code> controller of <code>gardener-resource-manager</code></a>.
Please find more information in the linked document.
In summary, most of the pods that initiate connections with other pods will have labels with <code>networking.resources.gardener.cloud/</code> prefixes.
This way, they leverage the automatically created <code>NetworkPolicy</code>s by the controller.
As a result, in most cases no special/custom-crafted <code>NetworkPolicy</code>s must be created anymore.</p><h2 id=seed-cluster>Seed Cluster</h2><p><em>(via <code>gardenlet</code> and <code>gardener-resource-manager</code>)</em></p><p>In seed clusters it works the same way as in the garden cluster managed by <code>gardener-operator</code>.
When a seed cluster is the garden cluster at the same time, <code>gardenlet</code> does not enable the <code>NetworkPolicy</code> controller (since <code>gardener-operator</code> already runs it).
Otherwise, it uses the exact same controller and code like <code>gardener-operator</code>, resulting in the same behaviour in both garden and seed clusters.</p><h3 id=logging--monitoring>Logging & Monitoring</h3><h4 id=seed-system-namespaces>Seed System Namespaces</h4><p>As part of the seed reconciliation flow, the <code>gardenlet</code> deploys various Prometheus instances into the <code>garden</code> namespace.
See also <a href=/docs/gardener/development/monitoring-stack/>this document</a> for more information.
Each pod that should be scraped for metrics by these instances must have a <code>Service</code> which is annotated with</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>annotations:
</span></span><span style=display:flex><span>  networking.resources.gardener.cloud/from-all-seed-scrape-targets-allowed-ports: <span style=color:#a31515>&#39;[{&#34;port&#34;:&lt;metrics-port-on-pod&gt;,&#34;protocol&#34;:&#34;&lt;protocol, typically TCP&gt;&#34;}]&#39;</span>
</span></span></code></pre></div><p>If the respective pod is not running in the <code>garden</code> namespace, the <code>Service</code> needs these annotations in addition:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>annotations:
</span></span><span style=display:flex><span>  networking.resources.gardener.cloud/namespace-selectors: <span style=color:#a31515>&#39;[{&#34;matchLabels&#34;:{&#34;kubernetes.io/metadata.name&#34;:&#34;garden&#34;}}]&#39;</span>
</span></span></code></pre></div><p>If the respective pod is running in an <code>extension-*</code> namespace, the <code>Service</code> needs this annotation in addition:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>annotations:
</span></span><span style=display:flex><span>  networking.resources.gardener.cloud/pod-label-selector-namespace-alias: extensions
</span></span></code></pre></div><p>This automatically allows the needed network traffic from the respective Prometheus pods.</p><h4 id=shoot-namespaces>Shoot Namespaces</h4><p>As part of the shoot reconciliation flow, the <code>gardenlet</code> deploys a shoot-specific Prometheus into the shoot namespace.
Each pod that should be scraped for metrics must have a <code>Service</code> which is annotated with</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>annotations:
</span></span><span style=display:flex><span>  networking.resources.gardener.cloud/from-all-scrape-targets-allowed-ports: <span style=color:#a31515>&#39;[{&#34;port&#34;:&lt;metrics-port-on-pod&gt;,&#34;protocol&#34;:&#34;&lt;protocol, typically TCP&gt;&#34;}]&#39;</span>
</span></span></code></pre></div><p>This automatically allows the network traffic from the Prometheus pod.</p><h3 id=webhook-servers>Webhook Servers</h3><p>Components serving webhook handlers that must be reached by <code>kube-apiserver</code>s of the virtual garden cluster or shoot clusters just need to annotate their <code>Service</code> as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>annotations:
</span></span><span style=display:flex><span>  networking.resources.gardener.cloud/from-all-webhook-targets-allowed-ports: <span style=color:#a31515>&#39;[{&#34;port&#34;:&lt;server-port-on-pod&gt;,&#34;protocol&#34;:&#34;&lt;protocol, typically TCP&gt;&#34;}]&#39;</span>
</span></span></code></pre></div><p>This automatically allows the network traffic from the API server pods.</p><h2 id=additional-namespace-coverage-in-gardenseed-cluster>Additional Namespace Coverage in Garden/Seed Cluster</h2><p>In some cases, garden or seed clusters might run components in dedicated namespaces which are not covered by the controller by default (see list above).
Still, it might(/should) be desired to also include such &ldquo;custom namespaces&rdquo; into the control of the <code>NetworkPolicy</code> controllers.</p><p>In order to do so, human operators can adapt the component configs of <code>gardener-operator</code> or <code>gardenlet</code> by providing label selectors for additional namespaces:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>controllers:
</span></span><span style=display:flex><span>  networkPolicy:
</span></span><span style=display:flex><span>    additionalNamespaceSelectors:
</span></span><span style=display:flex><span>    - matchLabels:
</span></span><span style=display:flex><span>        foo: bar
</span></span></code></pre></div><h3 id=communication-with-kube-apiserver-for-components-in-custom-namespaces>Communication With <code>kube-apiserver</code> For Components In Custom Namespaces</h3><h3 id=egress-traffic>Egress Traffic</h3><p>Component running in such custom namespaces might need to initiate the communication with the <code>kube-apiserver</code>s of the virtual garden cluster or a shoot cluster.
In order to achieve this, their custom namespace must be labeled with <code>networking.gardener.cloud/access-target-apiserver=allowed</code>.
This will make the <code>NetworkPolicy</code> controllers automatically provisioning the required policies into their namespace.</p><p>As a result, the respective component pods just need to be labeled with</p><ul><li><code>networking.resources.gardener.cloud/to-garden-virtual-garden-kube-apiserver-tcp-443=allowed</code> (virtual garden cluster)</li><li><code>networking.resources.gardener.cloud/to-all-shoots-kube-apiserver-tcp-443=allowed</code> (shoot clusters)</li></ul><h3 id=ingress-traffic>Ingress Traffic</h3><p>Components running in such custom namespaces might serve webhook handlers that must be reached by the <code>kube-apiservers</code> of the virtual garden cluster or a shoot cluster.
In order to achieve this, their <code>Service</code> must be annotated with <code>networking.resources.gardener.cloud/from-all-webhook-targets-allowed-ports=&lt;ports></code> as well as</p><ul><li><code>networking.resources.gardener.cloud/namespace-selectors: '[{"matchLabels":{"kubernetes.io/metadata.name":"garden"}}]</code> (virtual garden cluster)</li><li><code>networking.resources.gardener.cloud/namespace-selectors: '[{"matchLabels":{"gardener.cloud/role":"shoot"}}]</code> (shoot clusters)</li></ul><h2 id=shoot-cluster>Shoot Cluster</h2><p><em>(via <code>gardenlet</code>)</em></p><p>For shoot clusters, the concepts mentioned above don&rsquo;t apply and are not enabled.
Instead, <code>gardenlet</code> only deploys a few &ldquo;custom&rdquo; <code>NetworkPolicy</code>s for the shoot system components running in the <code>kube-system</code> namespace.
All other namespaces in the shoot cluster do not contain network policies deployed by <code>gardenlet</code>.</p><p>As a best practice, every pod deployed into the <code>kube-system</code> namespace should use appropriate <code>NetworkPolicy</code> in order to only allow <strong>required</strong> network traffic.
Therefore, pods should have labels matching to the selectors of the available network policies.</p><p><code>gardenlet</code> deploys the following <code>NetworkPolicy</code>s:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME                                       POD-SELECTOR
</span></span><span style=display:flex><span>gardener.cloud--allow-dns                  k8s-app in (kube-dns)
</span></span><span style=display:flex><span>gardener.cloud--allow-from-seed            networking.gardener.cloud/from-seed=allowed
</span></span><span style=display:flex><span>gardener.cloud--allow-to-dns               networking.gardener.cloud/to-dns=allowed
</span></span><span style=display:flex><span>gardener.cloud--allow-to-apiserver         networking.gardener.cloud/to-apiserver=allowed
</span></span><span style=display:flex><span>gardener.cloud--allow-to-from-nginx        app=nginx-ingress
</span></span><span style=display:flex><span>gardener.cloud--allow-to-kubelet           networking.gardener.cloud/to-kubelet=allowed
</span></span><span style=display:flex><span>gardener.cloud--allow-to-public-networks   networking.gardener.cloud/to-public-networks=allowed
</span></span><span style=display:flex><span>gardener.cloud--allow-vpn                  app=vpn-shoot
</span></span></code></pre></div><p>Note that a <code>deny-all</code> policy will not be created by <code>gardenlet</code>.
Shoot owners can create it manually if needed/desired.
Above listed <code>NetworkPolicy</code>s ensure that the traffic for the shoot system components is allowed in case such <code>deny-all</code> policies is created.</p><h2 id=implications-for-gardener-extensions>Implications for Gardener Extensions</h2><p>Gardener extensions sometimes need to deploy additional components into the shoot namespace in the seed cluster hosting the control plane.
For example, the <a href=https://github.com/gardener/gardener-extension-provider-aws><code>gardener-extension-provider-aws</code></a> deploys the <code>cloud-controller-manager</code> into the shoot namespace.
In most cases, such pods require network policy labels to allow the traffic they are initiating.</p><p>For components deployed in the <code>kube-system</code> namespace of the shoots (e.g., CNI plugins or CSI drivers, etc.), custom <code>NetworkPolicy</code>s might be required to ensure the respective components can still communicate in case the user creates a <code>deny-all</code> policy.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-34837cc04520de863b3e8452aabb4fe7>19 - Node Readiness</h1><h1 id=readiness-of-shoot-worker-nodes>Readiness of Shoot Worker Nodes</h1><h2 id=background>Background</h2><p>When registering new <code>Nodes</code>, kubelet adds the <code>node.kubernetes.io/not-ready</code> taint to prevent scheduling workload Pods to the <code>Node</code> until the <code>Ready</code> condition gets <code>True</code>.
However, the kubelet does not consider the readiness of node-critical Pods.
Hence, the <code>Ready</code> condition might get <code>True</code> and the <code>node.kubernetes.io/not-ready</code> taint might get removed, for example, before the CNI daemon Pod (e.g., <code>calico-node</code>) has successfully placed the CNI binaries on the machine.</p><p>This problem has been discussed extensively in kubernetes, e.g., in <a href=https://github.com/kubernetes/kubernetes/issues/75890>kubernetes/kubernetes#75890</a>.
However, several proposals have been rejected because the problem can be solved by using the <code>--register-with-taints</code> kubelet flag and dedicated controllers (<a href=https://github.com/kubernetes/enhancements/pull/1003#issuecomment-619087019>ref</a>).</p><h2 id=implementation-in-gardener>Implementation in Gardener</h2><p>Gardener makes sure that workload Pods are only scheduled to <code>Nodes</code> where all node-critical components required for running workload Pods are ready.
For this, Gardener follows the proposed solution by the Kubernetes community and registers new <code>Node</code> objects with the <code>node.gardener.cloud/critical-components-not-ready</code> taint (effect <code>NoSchedule</code>).
gardener-resource-manager&rsquo;s <a href=/docs/gardener/concepts/resource-manager/#node-controller><code>Node</code> controller</a> reacts on newly created <code>Node</code> objects that have this taint.
The controller removes the taint once all node-critical Pods are ready (determined by checking the Pods&rsquo; <code>Ready</code> conditions).</p><p>The <code>Node</code> controller considers all <code>DaemonSets</code> and <code>Pods</code> with the label <code>node.gardener.cloud/critical-component=true</code> as node-critical.
If there are <code>DaemonSets</code> that contain the <code>node.gardener.cloud/critical-component=true</code> label in their metadata and in their Pod template, the <code>Node</code> controller waits for corresponding daemon Pods to be scheduled and to get ready before removing the taint.</p><p>Additionally, the <code>Node</code> controller checks for the readiness of <code>csi-driver-node</code> components if a respective Pod indicates that it uses such a driver.
This is achieved through a well-defined annotation prefix (<code>node.gardener.cloud/wait-for-csi-node-</code>).
For example, the <code>csi-driver-node</code> Pod for Openstack Cinder is annotated with <code>node.gardener.cloud/wait-for-csi-node-cinder=cinder.csi.openstack.org</code>.
A key prefix is used instead of a &ldquo;regular&rdquo; annotation to allow for multiple CSI drivers being registered by one <code>csi-driver-node</code> Pod.
The annotation key&rsquo;s suffix can be chosen arbitrarily (in this case <code>cinder</code>) and the annotation value needs to match the actual driver name as specified in the <code>CSINode</code> object.
The <code>Node</code> controller will verify that the used driver is properly registered in this object before removing the <code>node.gardener.cloud/critical-components-not-ready</code> taint.
Note that the <code>csi-driver-node</code> Pod still needs to be labelled and tolerate the taint as described above to be considered in this additional check.</p><h2 id=marking-node-critical-components>Marking Node-Critical Components</h2><p>To make use of this feature, node-critical DaemonSets and Pods need to:</p><ul><li>Tolerate the <code>node.gardener.cloud/critical-components-not-ready</code> <code>NoSchedule</code> taint.</li><li>Be labelled with <code>node.gardener.cloud/critical-component=true</code>.</li></ul><p><code>csi-driver-node</code> Pods additionally need to:</p><ul><li>Be annotated with <code>node.gardener.cloud/wait-for-csi-node-&lt;name>=&lt;full-driver-name></code>.
It&rsquo;s required that these Pods fulfill the above criteria (label and toleration) as well.</li></ul><p>Gardener already marks components like kube-proxy, apiserver-proxy and node-local-dns as node-critical.
Provider extensions mark components like csi-driver-node as node-critical and add the <code>wait-for-csi-node</code> annotation.
Network extensions mark components responsible for setting up CNI on worker Nodes (e.g., <code>calico-node</code>) as node-critical.
If shoot owners manage any additional node-critical components, they can make use of this feature as well.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-8d5f560b3aa48903f7142745f2944770>20 - NodeLocalDNS Configuration</h1><h1 id=nodelocaldns-configuration>NodeLocalDNS Configuration</h1><p>This is a short guide describing how to enable DNS caching on the shoot cluster nodes.</p><h2 id=background>Background</h2><p>Currently in Gardener we are using CoreDNS as a deployment that is auto-scaled horizontally to cover for QPS-intensive applications. However, doing so does not seem to be enough to completely circumvent DNS bottlenecks such as:</p><ul><li>Cloud provider limits for DNS lookups.</li><li>Unreliable UDP connections that forces a period of timeout in case packets are dropped.</li><li>Unnecessary node hopping since CoreDNS is not deployed on all nodes, and as a result DNS queries end-up traversing multiple nodes before reaching the destination server.</li><li>Inefficient load-balancing of services (e.g., round-robin might not be enough when using IPTables mode)</li><li>and more &mldr;</li></ul><p>To workaround the issues described above, <code>node-local-dns</code> was introduced. The architecture is described below. The idea is simple:</p><ul><li>For new queries, the connection is upgraded from UDP to TCP and forwarded towards the cluster IP for the original CoreDNS server.</li><li>For previously resolved queries, an immediate response from the same node where the requester workload / pod resides is provided.</li></ul><p><img src=/__resources/node-local-dns_6d452a.png alt=node-local-dns-architecture></p><h2 id=configuring-nodelocaldns>Configuring NodeLocalDNS</h2><p>All that needs to be done to enable the usage of the <code>node-local-dns</code> feature is to set the corresponding option (<code>spec.systemComponents.nodeLocalDNS.enabled</code>) in the <code>Shoot</code> resource to <code>true</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  systemComponents:
</span></span><span style=display:flex><span>    nodeLocalDNS:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>It is worth noting that:</p><ul><li>When migrating from IPVS to IPTables, existing pods will continue to leverage the node-local-dns cache.</li><li>When migrating from IPtables to IPVS, only newer pods will be switched to the node-local-dns cache.</li><li>The annotation will take effect during the next shoot reconciliation. This happens automatically once per day in the maintenance period (unless you have disabled it).</li><li>During the reconfiguration of the node-local-dns there might be a short disruption in terms of domain name resolution depending on the setup. Usually, DNS requests are repeated for some time as UDP is an unreliable protocol, but that strictly depends on the application/way the domain name resolution happens. It is recommended to let the shoot be reconciled during the next maintenance period.</li><li>If a short DNS outage is not a big issue, you can <a href=/docs/gardener/usage/shoot_operations/#immediate-reconciliation>trigger reconciliation</a> directly after setting the annotation.</li><li>Switching node-local-dns off by removing the annotation can be a rather destructive operation that will result in pods without a working DNS configuration.</li></ul><p>For more information about <code>node-local-dns</code>, please refer to the <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/1024-nodelocal-cache-dns/README.md>KEP</a> or to the <a href=https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/>usage documentation</a>.</p><h2 id=known-issues>Known Issues</h2><p>Custom DNS configuration may not work as expected in conjunction with <code>NodeLocalDNS</code>.
Please refer to <a href=/docs/gardener/usage/custom-dns-config/#node-local-dns>Custom DNS Configuration</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-6aba6b44b714cf2748c3009e8eadd0c0>21 - OpenIDConnect Presets</h1><h1 id=clusteropenidconnectpreset-and-openidconnectpreset>ClusterOpenIDConnectPreset and OpenIDConnectPreset</h1><p>This page provides an overview of ClusterOpenIDConnectPresets and OpenIDConnectPresets, which are objects for injecting <a href=https://openid.net/connect/>OpenIDConnect Configuration</a> into <code>Shoot</code> at creation time. The injected information contains configuration for the Kube API Server and optionally configuration for kubeconfig generation using said configuration.</p><h2 id=openidconnectpreset>OpenIDConnectPreset</h2><p>An OpenIDConnectPreset is an API resource for injecting additional runtime OIDC requirements into a Shoot at creation time. You use label selectors to specify the <code>Shoot</code> to which a given OpenIDConnectPreset applies.</p><p>Using a OpenIDConnectPresets allows project owners to not have to explicitly provide the same OIDC configuration for every <code>Shoot</code> in their <code>Project</code>.</p><p>For more information about the background, see the <a href=https://github.com/gardener/gardener/issues/1161>issue</a> for OpenIDConnectPreset.</p><h3 id=how-openidconnectpreset-works>How OpenIDConnectPreset Works</h3><p>Gardener provides an admission controller (OpenIDConnectPreset) which, when enabled, applies OpenIDConnectPresets to incoming <code>Shoot</code> creation requests. When a <code>Shoot</code> creation request occurs, the system does the following:</p><ul><li><p>Retrieve all OpenIDConnectPreset available for use in the <code>Shoot</code> namespace.</p></li><li><p>Check if the shoot label selectors of any OpenIDConnectPreset matches the labels on the Shoot being created.</p></li><li><p>If multiple presets are matched then only one is chosen and results are sorted based on:</p><ol><li><code>.spec.weight</code> value.</li><li>lexicographically ordering their names (e.g., <code>002preset</code> > <code>001preset</code>)</li></ol></li><li><p>If the <code>Shoot</code> already has a <code>.spec.kubernetes.kubeAPIServer.oidcConfig</code>, then no mutation occurs.</p></li></ul><h3 id=simple-openidconnectpreset-example>Simple OpenIDConnectPreset Example</h3><p>This is a simple example to show how a <code>Shoot</code> is modified by the OpenIDConnectPreset:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: settings.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OpenIDConnectPreset
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name:  test-1
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  shootSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      oidc: enabled
</span></span><span style=display:flex><span>  server:
</span></span><span style=display:flex><span>    clientID: test-1
</span></span><span style=display:flex><span>    issuerURL: https://foo.bar
</span></span><span style=display:flex><span>    <span style=color:green># caBundle: |</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   -----BEGIN CERTIFICATE-----</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   Li4u</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   -----END CERTIFICATE-----</span>
</span></span><span style=display:flex><span>    groupsClaim: groups-claim
</span></span><span style=display:flex><span>    groupsPrefix: groups-prefix
</span></span><span style=display:flex><span>    usernameClaim: username-claim
</span></span><span style=display:flex><span>    usernamePrefix: username-prefix
</span></span><span style=display:flex><span>    signingAlgs:
</span></span><span style=display:flex><span>    - RS256
</span></span><span style=display:flex><span>    requiredClaims:
</span></span><span style=display:flex><span>      key: value
</span></span><span style=display:flex><span>  client:
</span></span><span style=display:flex><span>    secret: oidc-client-secret
</span></span><span style=display:flex><span>    extraConfig:
</span></span><span style=display:flex><span>      extra-scopes: <span style=color:#a31515>&#34;email,offline_access,profile&#34;</span>
</span></span><span style=display:flex><span>      foo: bar
</span></span><span style=display:flex><span>  weight: 90
</span></span></code></pre></div><p>Create the OpenIDConnectPreset:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl apply -f preset.yaml
</span></span></code></pre></div><p>Examine the created OpenIDConnectPreset:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl get openidconnectpresets
</span></span><span style=display:flex><span>NAME     ISSUER            SHOOT-SELECTOR   AGE
</span></span><span style=display:flex><span>test-1   https://foo.bar   oidc=enabled     1s
</span></span></code></pre></div><p>Simple <code>Shoot</code> example:</p><p>This is a sample of a <code>Shoot</code> with some fields omitted:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: preset
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    oidc: enabled
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    allowPrivilegedContainers: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    version: 1.20.2
</span></span></code></pre></div><p>Create the Shoot:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl apply -f shoot.yaml
</span></span></code></pre></div><p>Examine the created Shoot:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl get shoot preset -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: preset
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    oidc: enabled
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeAPIServer:
</span></span><span style=display:flex><span>      oidcConfig:
</span></span><span style=display:flex><span>        clientAuthentication:
</span></span><span style=display:flex><span>          extraConfig:
</span></span><span style=display:flex><span>            extra-scopes: email,offline_access,profile
</span></span><span style=display:flex><span>            foo: bar
</span></span><span style=display:flex><span>          secret: oidc-client-secret
</span></span><span style=display:flex><span>        clientID: test-1
</span></span><span style=display:flex><span>        groupsClaim: groups-claim
</span></span><span style=display:flex><span>        groupsPrefix: groups-prefix
</span></span><span style=display:flex><span>        issuerURL: https://foo.bar
</span></span><span style=display:flex><span>        requiredClaims:
</span></span><span style=display:flex><span>          key: value
</span></span><span style=display:flex><span>        signingAlgs:
</span></span><span style=display:flex><span>        - RS256
</span></span><span style=display:flex><span>        usernameClaim: username-claim
</span></span><span style=display:flex><span>        usernamePrefix: username-prefix
</span></span><span style=display:flex><span>    version: 1.20.2
</span></span></code></pre></div><h3 id=disable-openidconnectpreset>Disable OpenIDConnectPreset</h3><p>The OpenIDConnectPreset admission control is enabled by default. To disable it, use the <code>--disable-admission-plugins</code> flag on the gardener-apiserver.</p><p>For example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>--disable-admission-plugins=OpenIDConnectPreset
</span></span></code></pre></div><h2 id=clusteropenidconnectpreset>ClusterOpenIDConnectPreset</h2><p>A ClusterOpenIDConnectPreset is an API resource for injecting additional runtime OIDC requirements into a Shoot at creation time. In contrast to OpenIDConnect, it&rsquo;s a cluster-scoped resource. You use label selectors to specify the <code>Project</code> and <code>Shoot</code> to which a given OpenIDCConnectPreset applies.</p><p>Using a OpenIDConnectPresets allows cluster owners to not have to explicitly provide the same OIDC configuration for every <code>Shoot</code> in specific <code>Project</code>.</p><p>For more information about the background, see the <a href=https://github.com/gardener/gardener/issues/1161>issue</a> for ClusterOpenIDConnectPreset.</p><h3 id=how-clusteropenidconnectpreset-works>How ClusterOpenIDConnectPreset Works</h3><p>Gardener provides an admission controller (ClusterOpenIDConnectPreset) which, when enabled, applies ClusterOpenIDConnectPresets to incoming <code>Shoot</code> creation requests. When a <code>Shoot</code> creation request occurs, the system does the following:</p><ul><li><p>Retrieve all ClusterOpenIDConnectPresets available.</p></li><li><p>Check if the project label selector of any ClusterOpenIDConnectPreset matches the labels of the <code>Project</code> in which the <code>Shoot</code> is being created.</p></li><li><p>Check if the shoot label selectors of any ClusterOpenIDConnectPreset matches the labels on the <code>Shoot</code> being created.</p></li><li><p>If multiple presets are matched then only one is chosen and results are sorted based on:</p><ol><li><code>.spec.weight</code> value.</li><li>lexicographically ordering their names ( e.g. <code>002preset</code> > <code>001preset</code> )</li></ol></li><li><p>If the <code>Shoot</code> already has a <code>.spec.kubernetes.kubeAPIServer.oidcConfig</code> then no mutation occurs.</p></li></ul><blockquote><p><strong>Note:</strong> Due to the previous requirement, if a <code>Shoot</code> is matched by both <code>OpenIDConnectPreset</code> and <code>ClusterOpenIDConnectPreset</code>, then <code>OpenIDConnectPreset</code> takes precedence over <code>ClusterOpenIDConnectPreset</code>.</p></blockquote><h3 id=simple-clusteropenidconnectpreset-example>Simple ClusterOpenIDConnectPreset Example</h3><p>This is a simple example to show how a <code>Shoot</code> is modified by the ClusterOpenIDConnectPreset:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: settings.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ClusterOpenIDConnectPreset
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name:  test
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  shootSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      oidc: enabled
</span></span><span style=display:flex><span>  projectSelector: {} <span style=color:green># selects all projects.</span>
</span></span><span style=display:flex><span>  server:
</span></span><span style=display:flex><span>    clientID: cluster-preset
</span></span><span style=display:flex><span>    issuerURL: https://foo.bar
</span></span><span style=display:flex><span>    <span style=color:green># caBundle: |</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   -----BEGIN CERTIFICATE-----</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   Li4u</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   -----END CERTIFICATE-----</span>
</span></span><span style=display:flex><span>    groupsClaim: groups-claim
</span></span><span style=display:flex><span>    groupsPrefix: groups-prefix
</span></span><span style=display:flex><span>    usernameClaim: username-claim
</span></span><span style=display:flex><span>    usernamePrefix: username-prefix
</span></span><span style=display:flex><span>    signingAlgs:
</span></span><span style=display:flex><span>    - RS256
</span></span><span style=display:flex><span>    requiredClaims:
</span></span><span style=display:flex><span>      key: value
</span></span><span style=display:flex><span>  client:
</span></span><span style=display:flex><span>    secret: oidc-client-secret
</span></span><span style=display:flex><span>    extraConfig:
</span></span><span style=display:flex><span>      extra-scopes: <span style=color:#a31515>&#34;email,offline_access,profile&#34;</span>
</span></span><span style=display:flex><span>      foo: bar
</span></span><span style=display:flex><span>  weight: 90
</span></span></code></pre></div><p>Create the ClusterOpenIDConnectPreset:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl apply -f preset.yaml
</span></span></code></pre></div><p>Examine the created ClusterOpenIDConnectPreset:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get clusteropenidconnectpresets
</span></span><span style=display:flex><span>NAME     ISSUER            PROJECT-SELECTOR   SHOOT-SELECTOR   AGE
</span></span><span style=display:flex><span>test     https://foo.bar   &lt;none&gt;             oidc=enabled     1s
</span></span></code></pre></div><p>This is a sample of a <code>Shoot</code>, with some fields omitted:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: preset
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    oidc: enabled
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    allowPrivilegedContainers: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    version: 1.20.2
</span></span></code></pre></div><p>Create the Shoot:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl apply -f shoot.yaml
</span></span></code></pre></div><p>Examine the created Shoot:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl get shoot preset -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: preset
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    oidc: enabled
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeAPIServer:
</span></span><span style=display:flex><span>      oidcConfig:
</span></span><span style=display:flex><span>        clientAuthentication:
</span></span><span style=display:flex><span>          extraConfig:
</span></span><span style=display:flex><span>            extra-scopes: email,offline_access,profile
</span></span><span style=display:flex><span>            foo: bar
</span></span><span style=display:flex><span>          secret: oidc-client-secret
</span></span><span style=display:flex><span>        clientID: cluster-preset
</span></span><span style=display:flex><span>        groupsClaim: groups-claim
</span></span><span style=display:flex><span>        groupsPrefix: groups-prefix
</span></span><span style=display:flex><span>        issuerURL: https://foo.bar
</span></span><span style=display:flex><span>        requiredClaims:
</span></span><span style=display:flex><span>          key: value
</span></span><span style=display:flex><span>        signingAlgs:
</span></span><span style=display:flex><span>        - RS256
</span></span><span style=display:flex><span>        usernameClaim: username-claim
</span></span><span style=display:flex><span>        usernamePrefix: username-prefix
</span></span><span style=display:flex><span>    version: 1.20.2
</span></span></code></pre></div><h3 id=disable-clusteropenidconnectpreset>Disable ClusterOpenIDConnectPreset</h3><p>The ClusterOpenIDConnectPreset admission control is enabled by default. To disable it, use the <code>--disable-admission-plugins</code> flag on the gardener-apiserver.</p><p>For example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>--disable-admission-plugins=ClusterOpenIDConnectPreset
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-3b70dcc12d5f6585d051e97f92c4a62e>22 - Pod Security</h1><h1 id=migrating-from-podsecuritypolicys-to-podsecurity-admission-controller>Migrating from <code>PodSecurityPolicy</code>s to PodSecurity Admission Controller</h1><p>Kubernetes has deprecated the <code>PodSecurityPolicy</code> API in <code>v1.21</code> and it will be removed in <code>v1.25</code>. With <code>v1.23</code>, a new feature called <a href=https://kubernetes.io/docs/concepts/security/pod-security-admission/><code>PodSecurity</code></a> was promoted to beta. From <code>v1.25</code> onwards, there will be no API serving <code>PodSecurityPolicy</code>s, so you have to cleanup all the existing PSPs before upgrading your cluster. Detailed migration steps are described in <a href=https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/>Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller</a>.</p><p>After migration, you should disable the <code>PodSecurityPolicy</code> admission plugin. To do so, you have to add:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>admissionPlugins:
</span></span><span style=display:flex><span>- name: PodSecurityPolicy
</span></span><span style=display:flex><span>  disabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>in <code>spec.kubernetes.kubeAPIServer.admissionPlugins</code> field in the <code>Shoot</code> resource. Please refer the example <code>Shoot</code> manifest in <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>90-shoot.yaml</a>.</p><p>Only if the <code>PodSecurityPolicy</code> admission plugin is disabled the cluster can be upgraded to <code>v1.25</code>.</p><blockquote><p>⚠️ You should disable the admission plugin and wait until Gardener finishes at least one <code>Shoot</code> reconciliation before upgrading to <code>v1.25</code>. This is to make sure all the <code>PodSecurityPolicy</code> related resources deployed by Gardener are cleaned up.</p></blockquote><h2 id=admission-configuration-for-the-podsecurity-admission-plugin>Admission Configuration for the <code>PodSecurity</code> Admission Plugin</h2><p>If you wish to add your custom configuration for the <code>PodSecurity</code> plugin and your cluster version is <code>v1.23+</code>, you can do so in the Shoot spec under <code>.spec.kubernetes.kubeAPIServer.admissionPlugins</code> by adding:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>admissionPlugins:
</span></span><span style=display:flex><span>- name: PodSecurity
</span></span><span style=display:flex><span>  config:
</span></span><span style=display:flex><span>    apiVersion: pod-security.admission.config.k8s.io/v1
</span></span><span style=display:flex><span>    kind: PodSecurityConfiguration
</span></span><span style=display:flex><span>    <span style=color:green># Defaults applied when a mode label is not set.</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># Level label values must be one of:</span>
</span></span><span style=display:flex><span>    <span style=color:green># - &#34;privileged&#34; (default)</span>
</span></span><span style=display:flex><span>    <span style=color:green># - &#34;baseline&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green># - &#34;restricted&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># Version label values must be one of:</span>
</span></span><span style=display:flex><span>    <span style=color:green># - &#34;latest&#34; (default) </span>
</span></span><span style=display:flex><span>    <span style=color:green># - specific version like &#34;v1.25&#34;</span>
</span></span><span style=display:flex><span>    defaults:
</span></span><span style=display:flex><span>      enforce: <span style=color:#a31515>&#34;privileged&#34;</span>
</span></span><span style=display:flex><span>      enforce-version: <span style=color:#a31515>&#34;latest&#34;</span>
</span></span><span style=display:flex><span>      audit: <span style=color:#a31515>&#34;privileged&#34;</span>
</span></span><span style=display:flex><span>      audit-version: <span style=color:#a31515>&#34;latest&#34;</span>
</span></span><span style=display:flex><span>      warn: <span style=color:#a31515>&#34;privileged&#34;</span>
</span></span><span style=display:flex><span>      warn-version: <span style=color:#a31515>&#34;latest&#34;</span>
</span></span><span style=display:flex><span>    exemptions:
</span></span><span style=display:flex><span>      <span style=color:green># Array of authenticated usernames to exempt.</span>
</span></span><span style=display:flex><span>      usernames: []
</span></span><span style=display:flex><span>      <span style=color:green># Array of runtime class names to exempt.</span>
</span></span><span style=display:flex><span>      runtimeClasses: []
</span></span><span style=display:flex><span>      <span style=color:green># Array of namespaces to exempt.</span>
</span></span><span style=display:flex><span>      namespaces: []
</span></span></code></pre></div><p>⚠️ Note that the <code>pod-security.admission.config.k8s.io/v1</code> configuration requires <code>v1.25</code>+. For <code>v1.23</code> and <code>v1.24</code>, use <code>pod-security.admission.config.k8s.io/v1beta1</code>. For <code>v1.22</code>, use <code>pod-security.admission.config.k8s.io/v1alpha1</code>.</p><p>Also note that in <code>v1.22</code>, the feature gate <code>PodSecurity</code> is not enabled by default. You have to add:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>featureGates:
</span></span><span style=display:flex><span>  PodSecurity: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>under <code>.spec.kubernetes.kubeAPIServer</code>.</p><p>For proper functioning of Gardener, <code>kube-system</code> namespace will also be automatically added to the <code>exemptions.namespaces</code> list.</p><h2 id=speckubernetesallowprivilegedcontainers-in-the-shoot-spec><code>.spec.kubernetes.allowPrivilegedContainers</code> in the Shoot Spec</h2><p>If this field is set to <code>true</code>, then all authenticated users can use the &ldquo;gardener.privileged&rdquo; <code>PodSecurityPolicy</code>, allowing full unrestricted access to Pod features. However, the <code>PodSecurityPolicy</code> admission plugin is removed in Kubernetes <code>v1.25</code> and <code>PodSecurity</code> has taken its place as its successor. Therefore, this field doesn&rsquo;t have any relevance in versions <code>>= v1.25</code> anymore. If you need to set a default pod admission level for your cluster, follow <a href=#admission-configuration-for-the-podsecurity-admission-plugin>this documentation</a>.</p><blockquote><p><strong>Note:</strong> You should remove this field from the <code>Shoot</code> spec for <code>v1.24</code> clusters only after migrating to the new <code>PodSecurity</code> admission controller, i.e. after disabling the old <code>PodSecurityPolicy</code> admission plugin (see <a href=#migrating-from-podsecuritypolicys-to-podsecurity-admission-controller>Migrating from <code>PodSecurityPolicy</code>s to PodSecurity Admission Controller</a>). Otherwise the field is defaulted by the Gardener API Server. This is intentional because if we allow the field to be removed when <code>PodSecurityPolicy</code> admission plugin is active, the existing pods running in the cluster which need privileges can fail.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-5312b08cd06fa7b827ab36d682e935b0>23 - Projects</h1><h1 id=projects>Projects</h1><p>The Gardener API server supports a cluster-scoped <code>Project</code> resource which is used for data isolation between individual Gardener consumers. For example, each development team has its own project to manage its own shoot clusters.</p><p>Each <code>Project</code> is backed by a Kubernetes <code>Namespace</code> that contains the actual related Kubernetes resources, like <code>Secret</code>s or <code>Shoot</code>s.</p><p><strong>Example resource:</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Project
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>  description: <span style=color:#a31515>&#34;This is my first project&#34;</span>
</span></span><span style=display:flex><span>  purpose: <span style=color:#a31515>&#34;Experimenting with Gardener&#34;</span>
</span></span><span style=display:flex><span>  owner:
</span></span><span style=display:flex><span>    apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>    kind: User
</span></span><span style=display:flex><span>    name: john.doe@example.com
</span></span><span style=display:flex><span>  members:
</span></span><span style=display:flex><span>  - apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>    kind: User
</span></span><span style=display:flex><span>    name: alice.doe@example.com
</span></span><span style=display:flex><span>    role: admin
</span></span><span style=display:flex><span>  <span style=color:green># roles:</span>
</span></span><span style=display:flex><span>  <span style=color:green># - viewer </span>
</span></span><span style=display:flex><span>  <span style=color:green># - uam</span>
</span></span><span style=display:flex><span>  <span style=color:green># - serviceaccountmanager</span>
</span></span><span style=display:flex><span>  <span style=color:green># - extension:foo</span>
</span></span><span style=display:flex><span>  - apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>    kind: User
</span></span><span style=display:flex><span>    name: bob.doe@example.com
</span></span><span style=display:flex><span>    role: viewer
</span></span><span style=display:flex><span><span style=color:green># tolerations:</span>
</span></span><span style=display:flex><span><span style=color:green>#   defaults:</span>
</span></span><span style=display:flex><span><span style=color:green>#   - key: &lt;some-key&gt;</span>
</span></span><span style=display:flex><span><span style=color:green>#   whitelist:</span>
</span></span><span style=display:flex><span><span style=color:green>#   - key: &lt;some-key&gt;</span>
</span></span></code></pre></div><p>The <code>.spec.namespace</code> field is optional and is initialized if unset.
The name of the resulting namespace will be determined based on the <code>Project</code> name and UID, e.g., <code>garden-dev-5aef3</code>.
It&rsquo;s also possible to adopt existing namespaces by labeling them <code>gardener.cloud/role=project</code> and <code>project.gardener.cloud/name=dev</code> beforehand (otherwise, they cannot be adopted).</p><p>When deleting a Project resource, the corresponding namespace is also deleted.
To keep a namespace after project deletion, an administrator/operator (not Project members!) can annotate the project-namespace with <code>namespace.gardener.cloud/keep-after-project-deletion</code>.</p><p>The <code>spec.description</code> and <code>.spec.purpose</code> fields can be used to describe to fellow team members and Gardener operators what this project is used for.</p><p>Each project has one dedicated owner, configured in <code>.spec.owner</code> using the <code>rbac.authorization.k8s.io/v1.Subject</code> type.
The owner is the main contact person for Gardener operators.
Please note that the <code>.spec.owner</code> field is deprecated and will be removed in future API versions in favor of the <code>owner</code> role, see below.</p><p>The list of members (again a list in <code>.spec.members[]</code> using the <code>rbac.authorization.k8s.io/v1.Subject</code> type) contains all the people that are associated with the project in any way.
Each project member must have at least one role (currently described in <code>.spec.members[].role</code>, additional roles can be added to <code>.spec.members[].roles[]</code>). The following roles exist:</p><ul><li><code>admin</code>: This allows to fully manage resources inside the project (e.g., secrets, shoots, configmaps, and similar). Mind that the <code>admin</code> role has read only access to service accounts.</li><li><code>serviceaccountmanager</code>: This allows to fully manage service accounts inside the project namespace and request tokens for them. The permissions of the created service accounts are instead managed by the <code>admin</code> role. Please refer to <a href=/docs/gardener/usage/service-account-manager/>Service Account Manager</a>.</li><li><code>uam</code>: This allows to add/modify/remove human users or groups to/from the project member list.</li><li><code>viewer</code>: This allows to read all resources inside the project except secrets.</li><li><code>owner</code>: This combines the <code>admin</code>, <code>uam</code>, and <code>serviceaccountmanager</code> roles.</li><li>Extension roles (prefixed with <code>extension:</code>): Please refer to <a href=/docs/gardener/extensions/project-roles/>Extending Project Roles</a>.</li></ul><p>The <a href=/docs/gardener/concepts/controller-manager/#project-controller>project controller</a> inside the Gardener Controller Manager is managing RBAC resources that grant the described privileges to the respective members.</p><p>There are three central <code>ClusterRole</code>s <code>gardener.cloud:system:project-member</code>, <code>gardener.cloud:system:project-viewer</code>, and <code>gardener.cloud:system:project-serviceaccountmanager</code> that grant the permissions for namespaced resources (e.g., <code>Secret</code>s, <code>Shoot</code>s, <code>ServiceAccount</code>s).
Via referring <code>RoleBinding</code>s created in the respective namespace the project members get bound to these <code>ClusterRole</code>s and, thus, the needed permissions.
There are also project-specific <code>ClusterRole</code>s granting the permissions for cluster-scoped resources, e.g., the <code>Namespace</code> or <code>Project</code> itself.<br>For each role, the following <code>ClusterRole</code>s, <code>ClusterRoleBinding</code>s, and <code>RoleBinding</code>s are created:</p><table><thead><tr><th>Role</th><th><code>ClusterRole</code></th><th><code>ClusterRoleBinding</code></th><th><code>RoleBinding</code></th></tr></thead><tbody><tr><td><code>admin</code></td><td><code>gardener.cloud:system:project-member:&lt;projectName></code></td><td><code>gardener.cloud:system:project-member:&lt;projectName></code></td><td><code>gardener.cloud:system:project-member</code></td></tr><tr><td><code>serviceaccountmanager</code></td><td></td><td></td><td><code>gardener.cloud:system:project-serviceaccountmanager</code></td></tr><tr><td><code>uam</code></td><td><code>gardener.cloud:system:project-uam:&lt;projectName></code></td><td><code>gardener.cloud:system:project-uam:&lt;projectName></code></td><td></td></tr><tr><td><code>viewer</code></td><td><code>gardener.cloud:system:project-viewer:&lt;projectName></code></td><td><code>gardener.cloud:system:project-viewer:&lt;projectName></code></td><td><code>gardener.cloud:system:project-viewer</code></td></tr><tr><td><code>owner</code></td><td><code>gardener.cloud:system:project:&lt;projectName></code></td><td><code>gardener.cloud:system:project:&lt;projectName></code></td><td></td></tr><tr><td><code>extension:*</code></td><td><code>gardener.cloud:extension:project:&lt;projectName>:&lt;extensionRoleName></code></td><td></td><td><code>gardener.cloud:extension:project:&lt;projectName>:&lt;extensionRoleName></code></td></tr></tbody></table><h2 id=user-access-management>User Access Management</h2><p>For <code>Project</code>s created before Gardener v1.8, all admins were allowed to manage other members.
Beginning with v1.8, the new <code>uam</code> role is being introduced.
It is backed by the <code>manage-members</code> custom RBAC verb which allows to add/modify/remove human users or groups to/from the project member list.
Human users are subjects with <code>kind=User</code> and <code>name!=system:serviceaccount:*</code>, and groups are subjects with <code>kind=Group</code>.
The management of service account subjects (<code>kind=ServiceAccount</code> or <code>name=system:serviceaccount:*</code>) is not controlled via the <code>uam</code> custom verb but with the standard <code>update</code>/<code>patch</code> verbs for projects.</p><p>All newly created projects will only bind the owner to the <code>uam</code> role.
The owner can still grant the <code>uam</code> role to other members if desired.
For projects created before Gardener v1.8, the Gardener Controller Manager will migrate all projects to also assign the <code>uam</code> role to all <code>admin</code> members (to not break existing use-cases). The corresponding migration logic is present in Gardener Controller Manager from v1.8 to v1.13.
The project owner can gradually remove these roles if desired.</p><h2 id=stale-projects>Stale Projects</h2><p>When a project is not actively used for some period of time, it is marked as &ldquo;stale&rdquo;. This is done by a controller called <a href=/docs/gardener/concepts/controller-manager/#stale-projects-reconciler>&ldquo;Stale Projects Reconciler&rdquo;</a>. Once the project is marked as stale, there is a time frame in which if not used it will be deleted by that controller.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-51f43840ca475366ea7f9ff5a443fac5>24 - Reversed VPN Tunnel</h1><h1 id=reversed-vpn-tunnel-setup-and-configuration>Reversed VPN Tunnel Setup and Configuration</h1><p>The Reversed VPN Tunnel is enabled by default.
A highly available VPN connection is automatically deployed in all shoots that configure an HA control-plane.</p><h2 id=reversed-vpn-tunnel>Reversed VPN Tunnel</h2><p>In the first VPN solution, connection establishment was initiated by a VPN client in the seed cluster.
Due to several issues with this solution, the tunnel establishment direction has been reverted.
The client is deployed in the shoot and initiates the connection from there. This way, there is no need to deploy a special purpose
loadbalancer for the sake of addressing the data-plane, in addition to saving costs, this is considered the more secure alternative.
For more information on how this is achieved, please have a look at the following <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/14-reversed-cluster-vpn.md>GEP</a>.</p><p>Connection establishment with a reversed tunnel:</p><p><code>APIServer --> Envoy-Proxy | VPN-Seed-Server &lt;-- Istio/Envoy-Proxy &lt;-- SNI API Server Endpoint &lt;-- LB (one for all clusters of a seed) &lt;--- internet &lt;--- VPN-Shoot-Client --> Pods | Nodes | Services</code></p><h2 id=high-availability-for-reversed-vpn-tunnel>High Availability for Reversed VPN Tunnel</h2><p>Shoots which define <code>spec.controlPlane.highAvailability.failureTolerance: {node, zone}</code> get an HA control-plane, including a
highly available VPN connection by deploying redundant VPN servers and clients.</p><p>Please note that it is not possible to move an open connection to another VPN tunnel. Especially long-running
commands like <code>kubectl exec -it ...</code> or <code>kubectl logs -f ...</code> will still break if the routing path must be switched
because either VPN server or client are not reachable anymore. A new request should be possible within seconds.</p><h3 id=ha-architecture-for-vpn>HA Architecture for VPN</h3><p>Establishing a connection from the VPN client on the shoot to the server in the control plane works nearly the same
way as in the non-HA case. The only difference is that the VPN client targets one of two VPN servers, represented by two services
<code>vpn-seed-server-0</code> and <code>vpn-seed-server-1</code> with endpoints in pods with the same name.
The VPN tunnel is used by a <code>kube-apiserver</code> to reach nodes, services, or pods in the shoot cluster.
In the non-HA case, a kube-apiserver uses an HTTP proxy running as a side-car in the VPN server to address
the shoot networks via the VPN tunnel and the <code>vpn-shoot</code> acts as a router.
In the HA case, the setup is more complicated. Instead of an HTTP proxy in the VPN server, the kube-apiserver has
additional side-cars, one side-car for each VPN client to connect to the corresponding VPN server.
On the shoot side, there are now two <code>vpn-shoot</code> pods, each with two VPN clients for each VPN server.
With this setup, there would be four possible routes, but only one can be used. Switching the route kills all
open connections. Therefore, another layer is introduced: link aggregation, also named <a href=https://www.kernel.org/doc/Documentation/networking/bonding.txt>bonding</a>.
In Linux, you can create a network link by using several other links as slaves. Bonding here is used with
active-backup mode. This means the traffic only goes through the active sublink and is only changed if the active one
becomes unavailable. Switching happens in the bonding network driver without changing any routes. So with this layer,
vpn-seed-server pods can be rolled without disrupting open connections.</p><p><img src=/__resources/vpn-ha-architecture_2f71aa.png alt="VPN HA Architecture"></p><p>With bonding, there are 2 possible routing paths, ensuring that there is at least one routing path intact even if
one <code>vpn-seed-server</code> pod and one <code>vpn-shoot</code> pod are unavailable at the same time.</p><p>As it is not possible to use multi-path routing, one routing path must be configured explicitly.
For this purpose, the <code>path-controller</code> script is running in another side-car of the kube-apiserver pod.
It pings all shoot-side VPN clients regularly every few seconds. If the active routing path is not responsive anymore,
the routing is switched to the other responsive routing path.</p><p><img src=/__resources/vpn-ha-routing-paths_aa0d8d.png alt="Four possible routing paths"></p><p>For general information about HA control-plane, see <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/20-ha-control-planes.md>GEP-20</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-42e1bcea9f49e68fd1fe7ce23506a673>25 - Seed Bootstrapping</h1><h1 id=seed-bootstrapping>Seed Bootstrapping</h1><p>Whenever the gardenlet is responsible for a new <code>Seed</code> resource its &ldquo;seed controller&rdquo; is being activated.
One part of this controller&rsquo;s reconciliation logic is deploying certain components into the <code>garden</code> namespace of the seed cluster itself.
These components are required to spawn and manage control planes for shoot clusters later on.
This document is providing an overview which actions are performed during this bootstrapping phase, and it explains the rationale behind them.</p><h2 id=dependency-watchdog>Dependency Watchdog</h2><p>The dependency watchdog (abbreviation: DWD) is a component developed separately in the <a href=https://github.com/gardener/dependency-watchdog>gardener/dependency-watchdog</a> GitHub repository.
Gardener is using it for two purposes:</p><ol><li>Prevention of melt-down situations when the load balancer used to expose the kube-apiserver of shoot clusters goes down while the kube-apiserver itself is still up and running.</li><li>Fast recovery times for crash-looping pods when depending pods are again available.</li></ol><p>For the sake of separating these concerns, two instances of the DWD are deployed by the seed controller.</p><h3 id=prober>Prober</h3><p>The <code>dependency-watchdog-prober</code> deployment is responsible for above-mentioned first point.</p><p>The <code>kube-apiserver</code> of shoot clusters is exposed via a load balancer, usually with an attached public IP, which serves as the main entry point when it comes to interaction with the shoot cluster (e.g., via <code>kubectl</code>).
While end-users are talking to their clusters via this load balancer, other control plane components like the <code>kube-controller-manager</code> or <code>kube-scheduler</code> run in the same namespace/same cluster, so they can communicate via the in-cluster <code>Service</code> directly instead of using the detour with the load balancer.
However, the worker nodes of shoot clusters run in isolated, distinct networks.
This means that the <code>kubelet</code>s and <code>kube-proxy</code>s also have to talk to the control plane via the load balancer.</p><p>The <code>kube-controller-manager</code> has a special control loop called <a href=https://github.com/kubernetes/kubernetes/tree/master/pkg/controller/nodelifecycle><code>nodelifecycle</code></a> which will set the status of <code>Node</code>s to <code>NotReady</code> in case the kubelet stops to regularly renew its lease/to send its heartbeat.
This will trigger other self-healing capabilities of Kubernetes, for example, the eviction of pods from such &ldquo;unready&rdquo; nodes to healthy nodes.
Similarly, the <code>cloud-controller-manager</code> has a control loop that will disconnect load balancers from &ldquo;unready&rdquo; nodes, i.e., such workload would no longer be accessible until moved to a healthy node.
Furthermore, the <code>machine-controller-manager</code> removes &ldquo;unready&rdquo; nodes after <code>health-timeout</code> (default 10min).</p><p>While these are awesome Kubernetes features on their own, they have a dangerous drawback when applied in the context of Gardener&rsquo;s architecture:
When the <code>kube-apiserver</code> load balancer fails for whatever reason, then the <code>kubelet</code>s can&rsquo;t talk to the <code>kube-apiserver</code> to renew their lease anymore.
After a minute or so the <code>kube-controller-manager</code> will get the impression that all nodes have died and will mark them as <code>NotReady</code>.
This will trigger above mentioned eviction as well as detachment of load balancers.
As a result, the customer&rsquo;s workload will go down and become unreachable.</p><p>This is exactly the situation that the DWD prevents:
It regularly tries to talk to the <code>kube-apiserver</code>s of the shoot clusters, once by using their load balancer, and once by talking via the in-cluster <code>Service</code>.
If it detects that the <code>kube-apiserver</code> is reachable internally but not externally, it scales down <code>machine-controller-manager</code>, <code>cluster-autoscaler</code> (if enabled) and <code>kube-controller-manager</code> to <code>0</code>.
This will prevent it from marking the shoot worker nodes as &ldquo;unready&rdquo;. This will also prevent the <code>machine-controller-manager</code> from deleting potentially healthy nodes.
As soon as the <code>kube-apiserver</code> is reachable externally again, <code>kube-controller-manager</code>, <code>machine-controller-manager</code> and <code>cluster-autoscaler</code> are restored to the state prior to scale-down.</p><h3 id=weeder>Weeder</h3><p>The <code>dependency-watchdog-weeder</code> deployment is responsible for above mentioned second point.</p><p>Kubernetes is restarting failing pods with an exponentially increasing backoff time.
While this is a great strategy to prevent system overloads, it has the disadvantage that the delay between restarts is increasing up to multiple minutes very fast.</p><p>In the Gardener context, we are deploying many components that are depending on other components.
For example, the <code>kube-apiserver</code> is depending on a running <code>etcd</code>, or the <code>kube-controller-manager</code> and <code>kube-scheduler</code> are depending on a running <code>kube-apiserver</code>.
In case such a &ldquo;higher-level&rdquo; component fails for whatever reason, the dependent pods will fail and end-up in crash-loops.
As Kubernetes does not know anything about these hierarchies, it won&rsquo;t recognize that such pods can be restarted faster as soon as their dependents are up and running again.</p><p>This is exactly the situation in which the DWD will become active:
If it detects that a certain <code>Service</code> is available again (e.g., after the <code>etcd</code> was temporarily down while being moved to another seed node), then DWD will restart all crash-looping dependant pods.
These dependant pods are detected via a pre-configured label selector.</p><p>As of today, the DWD is configured to restart a crash-looping <code>kube-apiserver</code> after <code>etcd</code> became available again, or any pod depending on the <code>kube-apiserver</code> that has a <code>gardener.cloud/role=controlplane</code> label (e.g., <code>kube-controller-manager</code>, <code>kube-scheduler</code>).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a4713902bcc1223db8a52f790b8cd0ef>26 - Seed Settings</h1><h1 id=settings-for-seeds>Settings for <code>Seed</code>s</h1><p>The <code>Seed</code> resource offers a few settings that are used to control the behaviour of certain Gardener components.
This document provides an overview over the available settings:</p><h2 id=dependency-watchdog>Dependency Watchdog</h2><p>Gardenlet can deploy two instances of the <a href=https://github.com/gardener/dependency-watchdog>dependency-watchdog</a> into the <code>garden</code> namespace of the seed cluster.
One instance only activates the weeder while the second instance only activates the prober.</p><h3 id=weeder>Weeder</h3><p>The weeder helps to alleviate the delay where control plane components remain unavailable by finding the respective pods in <code>CrashLoopBackoff</code> status and restarting them once their dependants become ready and available again.
For example, if <code>etcd</code> goes down then also <code>kube-apiserver</code> goes down (and into a <code>CrashLoopBackoff</code> state). If <code>etcd</code> comes up again then (without the <code>endpoint</code> controller) it might take some time until <code>kube-apiserver</code> gets restarted as well.</p><p>⚠️ <code>.spec.settings.dependencyWatchdog.endpoint.enabled</code> is deprecated and will be removed in a future version of Gardener. Use <code>.spec.settings.dependencyWatchdog.weeder.enabled</code> instead.</p><p>It can be enabled/disabled via the <code>.spec.settings.dependencyWatchdog.endpoint.enabled</code> field.
It defaults to <code>true</code>.</p><h3 id=prober>Prober</h3><p>The <code>probe</code> controller scales down the <code>kube-controller-manager</code> of shoot clusters in case their respective <code>kube-apiserver</code> is not reachable via its external ingress.
This is in order to avoid melt-down situations, since the <code>kube-controller-manager</code> uses in-cluster communication when talking to the <code>kube-apiserver</code>, i.e., it wouldn&rsquo;t be affected if the external access to the <code>kube-apiserver</code> is interrupted for whatever reason.
The <code>kubelet</code>s on the shoot worker nodes, however, would indeed be affected since they typically run in different networks and use the external ingress when talking to the <code>kube-apiserver</code>.
Hence, without scaling down <code>kube-controller-manager</code>, the nodes might be marked as <code>NotReady</code> and eventually replaced (since the <code>kubelet</code>s cannot report their status anymore).
To prevent such unnecessary turbulences, <code>kube-controller-manager</code> is being scaled down until the external ingress becomes available again. In addition, as a precautionary measure, <code>machine-controller-manager</code> is also scaled down, along with <code>cluster-autoscaler</code> which depends on
<code>machine-controller-manager</code>.</p><p>⚠️ <code>.spec.settings.dependencyWatchdog.probe.enabled</code> is deprecated and will be removed in a future version of Gardener. Use <code>.spec.settings.dependencyWatchdog.prober.enabled</code> instead.</p><p>It can be enabled/disabled via the <code>.spec.settings.dependencyWatchdog.probe.enabled</code> field.
It defaults to <code>true</code>.</p><h2 id=reserve-excess-capacity>Reserve Excess Capacity</h2><p>If the excess capacity reservation is enabled, then the gardenlet will deploy a special <code>Deployment</code> into the <code>garden</code> namespace of the seed cluster.
This <code>Deployment</code>&rsquo;s pod template has only one container, the <code>pause</code> container, which simply runs in an infinite loop.
The priority of the deployment is very low, so any other pod will preempt these <code>pause</code> pods.
This is especially useful if new shoot control planes are created in the seed.
In case the seed cluster runs at its capacity, then there is no waiting time required during the scale-up.
Instead, the low-priority <code>pause</code> pods will be preempted and allow newly created shoot control plane pods to be scheduled fast.
In the meantime, the cluster-autoscaler will trigger the scale-up because the preempted <code>pause</code> pods want to run again.
However, this delay doesn&rsquo;t affect the important shoot control plane pods, which will improve the user experience.</p><p>It can be enabled/disabled via the <code>.spec.settings.excessCapacityReservation.enabled</code> field.
It defaults to <code>true</code>.</p><h2 id=scheduling>Scheduling</h2><p>By default, the Gardener Scheduler will consider all seed clusters when a new shoot cluster shall be created.
However, administrators/operators might want to exclude some of them from being considered by the scheduler.
Therefore, seed clusters can be marked as &ldquo;invisible&rdquo;.
In this case, the scheduler simply ignores them as if they wouldn&rsquo;t exist.
Shoots can still use the invisible seed but only by explicitly specifying the name in their <code>.spec.seedName</code> field.</p><p>Seed clusters can be marked visible/invisible via the <code>.spec.settings.scheduling.visible</code> field.
It defaults to <code>true</code>.</p><p>ℹ️ In previous Gardener versions (&lt; 1.5) these settings were controlled via taint keys (<code>seed.gardener.cloud/{disable-capacity-reservation,invisible}</code>).
The taint keys are no longer supported and removed in version 1.12.
The rationale behind it is the implementation of tolerations similar to Kubernetes tolerations.
More information about it can be found in <a href=https://github.com/gardener/gardener/issues/2193>#2193</a>.</p><h2 id=load-balancer-services>Load Balancer Services</h2><p>Gardener creates certain Kubernetes <code>Service</code> objects of type <code>LoadBalancer</code> in the seed cluster.
Most prominently, they are used for exposing the shoot control planes, namely the kube-apiserver of the shoot clusters.
In most cases, the cloud-controller-manager (responsible for managing these load balancers on the respective underlying infrastructure) supports certain customization and settings via annotations.
<a href=https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer>This document</a> provides a good overview and many examples.</p><p>By setting the <code>.spec.settings.loadBalancerServices.annotations</code> field the Gardener administrator can specify a list of annotations, which will be injected into the <code>Service</code>s of type <code>LoadBalancer</code>.</p><h3 id=external-traffic-policy>External Traffic Policy</h3><p>Setting the <a href=https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip>external traffic policy</a> to <code>Local</code> can be beneficial as it
preserves the source IP address of client requests. In addition to that, it removes one hop in the data path and hence reduces request latency. On some cloud infrastructures, it can furthermore be
used in conjunction with <code>Service</code> annotations as described above to prevent cross-zonal traffic from the load balancer to the backend pod.</p><p>The default external traffic policy is <code>Cluster</code>, meaning that all traffic from the load balancer will be sent to any cluster node, which then itself will redirect the traffic to the actual receiving pod.
This approach adds a node to the data path, may cross the zone boundaries twice, and replaces the source IP with one of the cluster nodes.</p><p><img src=/__resources/external-traffic-policy-cluster_350bbe.png alt="External Traffic Policy Cluster"></p><p>Using external traffic policy <code>Local</code> drops the additional node, i.e., only cluster nodes with corresponding backend pods will be in the list of backends of the load balancer. However, this has multiple implications.
The health check port in this scenario is exposed by <code>kube-proxy</code> , i.e., if <code>kube-proxy</code> is not working on a node a corresponding pod on the node will not receive traffic from
the load balancer as the load balancer will see a failing health check. (This is quite different from ordinary service routing where <code>kube-proxy</code> is only responsible for setup, but does not need to
run for its operation.) Furthermore, load balancing may become imbalanced if multiple pods run on the same node because load balancers will split the load equally among the nodes and not among the pods. This is mitigated by corresponding node anti affinities.</p><p><img src=/__resources/external-traffic-policy-local_b36f63.png alt="External Traffic Policy Local"></p><p>Operators need to take these implications into account when considering switching external traffic policy to <code>Local</code>.</p><h3 id=zone-specific-settings>Zone-Specific Settings</h3><p>In case a seed cluster is configured to use multiple zones via <code>.spec.provider.zones</code>, it may be necessary to configure the load balancers in individual zones in different way, e.g., by utilizing
different annotations. One reason may be to reduce cross-zonal traffic and have zone-specific load balancers in place. Zone-specific load balancers may then be bound to zone-specific subnets or
availability zones in the cloud infrastructure.</p><p>Besides the load balancer annotations, it is also possible to set the <a href=#external-traffic-policy>external traffic policy</a> for each zone-specific load balancer individually.</p><h2 id=vertical-pod-autoscaler>Vertical Pod Autoscaler</h2><p>Gardener heavily relies on the Kubernetes <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler><code>vertical-pod-autoscaler</code> component</a>.
By default, the seed controller deploys the VPA components into the <code>garden</code> namespace of the respective seed clusters.
In case you want to manage the VPA deployment on your own or have a custom one, then you might want to disable the automatic deployment of Gardener.
Otherwise, you might end up with two VPAs, which will cause erratic behaviour.
By setting the <code>.spec.settings.verticalPodAutoscaler.enabled=false</code>, you can disable the automatic deployment.</p><p>⚠️ In any case, there must be a VPA available for your seed cluster. Using a seed without VPA is not supported.</p><h2 id=owner-checks>Owner Checks</h2><blockquote><p>Note: The owner checks are deprecated. The &ldquo;bad-case&rdquo; control plane migration is being removed in favor of the HA Shoot control planes (see <a href=https://github.com/gardener/gardener/issues/6302>https://github.com/gardener/gardener/issues/6302</a>). The field will be locked to <code>false</code> in a future version of Gardener. In this way gardenlet will clean up all owner DNSRecords. Finally, the field will be removed from the API. Set this field to <code>false</code> to be prepared for the above-mentioned locking.</p></blockquote><p>When a shoot is scheduled to a seed and actually reconciled, Gardener appoints the seed as the current &ldquo;owner&rdquo; of the shoot by creating a special &ldquo;owner DNS record&rdquo; and checking against it if the seed still owns the shoot in order to guard against &ldquo;split brain scenario&rdquo; during control plane migration, as described in <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/17-shoot-control-plane-migration-bad-case.md>GEP-17 Shoot Control Plane Migration &ldquo;Bad Case&rdquo; Scenario</a>.
This mechanism relies on the DNS resolution of TXT DNS records being possible and highly reliable, since if the owner check fails, the shoot will be effectively disabled for the duration of the failure.
In environments where resolving TXT DNS records is either not possible or not considered reliable enough, it may be necessary to disable the owner check mechanism, in order to avoid shoots failing to reconcile or temporary outages due to transient DNS failures.
By setting the <code>.spec.settings.ownerChecks.enabled=false</code> (default is <code>true</code>), the creation and checking of owner DNS records can be disabled for all shoots scheduled on this seed. Note that if owner checks are disabled, migrating shoots scheduled on this seed to other seeds should be considered unsafe, and in the future will be disabled as well.</p><h2 id=topology-aware-traffic-routing>Topology-Aware Traffic Routing</h2><p>Refer to the <a href=/docs/gardener/usage/topology_aware_routing/>Topology-Aware Traffic Routing documentation</a> as this document contains the documentation for the topology-aware routing Seed setting.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ad3a46d969ae07bfe655009c5cfb0f36>27 - Service Account Manager</h1><h1 id=service-account-manager>Service Account Manager</h1><h2 id=overview>Overview</h2><p>With Gardener <code>v1.47</code>, a new role called <code>serviceaccountmanager</code> was introduced. This role allows to fully manage <code>ServiceAccount</code>&rsquo;s in the project namespace and request tokens for them. This is the preferred way of managing the access to a project namespace, as it aims to replace the usage of the default <code>ServiceAccount</code> secrets that will no longer be generated automatically with Kubernetes <code>v1.24+</code>.</p><h2 id=actions>Actions</h2><p>Once assigned the <code>serviceaccountmanager</code> role, a user can create/update/delete <code>ServiceAccount</code>s in the project namespace.</p><h3 id=create-a-service-account>Create a Service Account</h3><p>In order to create a <code>ServiceAccount</code> named &ldquo;robot-user&rdquo;, run the following <code>kubectl</code> command:</p><pre tabindex=0><code class=language-code data-lang=code>kubectl -n project-abc create sa robot-user
</code></pre><h3 id=request-a-token-for-a-service-account>Request a Token for a Service Account</h3><p>A token for the &ldquo;robot-user&rdquo; <code>ServiceAccount</code> can be requested via the <a href=https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-request-v1/>TokenRequest API</a> in several ways:</p><ul><li>using <code>kubectl</code> >= v1.24</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n project-abc create token robot-user --duration=3600s
</span></span></code></pre></div><ul><li>using <code>kubectl</code> &lt; v1.24</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat <span style=color:#a31515>&lt;&lt;EOF | kubectl create -f - --raw /api/v1/namespaces/project-abc/serviceaccounts/robot-user/token
</span></span></span><span style=display:flex><span><span style=color:#a31515>{
</span></span></span><span style=display:flex><span><span style=color:#a31515>  &#34;apiVersion&#34;: &#34;authentication.k8s.io/v1&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>  &#34;kind&#34;: &#34;TokenRequest&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>  &#34;spec&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#a31515>    &#34;expirationSeconds&#34;: 3600
</span></span></span><span style=display:flex><span><span style=color:#a31515>  }
</span></span></span><span style=display:flex><span><span style=color:#a31515>}
</span></span></span><span style=display:flex><span><span style=color:#a31515>EOF</span>
</span></span></code></pre></div><ul><li>directly calling the Kubernetes HTTP API</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl -X POST https://api.gardener/api/v1/namespaces/project-abc/serviceaccounts/robot-user/token <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    -H <span style=color:#a31515>&#34;Authorization: Bearer &lt;auth-token&gt;&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    -H <span style=color:#a31515>&#34;Content-Type: application/json&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    -d <span style=color:#a31515>&#39;{
</span></span></span><span style=display:flex><span><span style=color:#a31515>        &#34;apiVersion&#34;: &#34;authentication.k8s.io/v1&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>        &#34;kind&#34;: &#34;TokenRequest&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>        &#34;spec&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;expirationSeconds&#34;: 3600
</span></span></span><span style=display:flex><span><span style=color:#a31515>        }
</span></span></span><span style=display:flex><span><span style=color:#a31515>      }&#39;</span>
</span></span></code></pre></div><p>Mind that the returned token is not stored within the Kubernetes cluster, will be valid for <code>3600</code> seconds, and will be invalidated if the &ldquo;robot-user&rdquo; <code>ServiceAccount</code> is deleted. Although <code>expirationSeconds</code> can be modified depending on the needs, the returned token&rsquo;s validity will not exceed the configured <code>service-account-max-token-expiration</code> duration for the garden cluster. It is advised that the actual <code>expirationTimestamp</code> is verified so that expectations are met. This can be done by asserting the <code>expirationTimestamp</code> in the <code>TokenRequestStatus</code> or the <code>exp</code> claim in the token itself.</p><h3 id=delete-a-service-account>Delete a Service Account</h3><p>In order to delete the <code>ServiceAccount</code> named &ldquo;robot-user&rdquo;, run the following <code>kubectl</code> command:</p><pre tabindex=0><code class=language-code data-lang=code>kubectl -n project-abc delete sa robot-user
</code></pre><p>This will invalidate all existing tokens for the &ldquo;robot-user&rdquo; <code>ServiceAccount</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4d133339effd8d7bc8dfe83c9e64bb2e>28 - Shoot Access</h1><h1 id=accessing-shoot-clusters>Accessing Shoot Clusters</h1><p>After creation of a shoot cluster, end-users require a <code>kubeconfig</code> to access it. There are several options available to get to such <code>kubeconfig</code>.</p><h2 id=shootsadminkubeconfig-subresource><code>shoots/adminkubeconfig</code> Subresource</h2><p>The <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/16-adminkubeconfig-subresource.md><code>shoots/adminkubeconfig</code></a> subresource allows users to dynamically generate temporary <code>kubeconfig</code>s that can be used to access shoot cluster with <code>cluster-admin</code> privileges. The credentials associated with this <code>kubeconfig</code> are client certificates which have a very short validity and must be renewed before they expire (by calling the subresource endpoint again).</p><p>The username associated with such <code>kubeconfig</code> will be the same which is used for authenticating to the Gardener API. Apart from this advantage, the created <code>kubeconfig</code> will not be persisted anywhere.</p><p>In order to request such a <code>kubeconfig</code>, you can run the following commands:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export NAMESPACE=my-namespace
</span></span><span style=display:flex><span>export SHOOT_NAME=my-shoot
</span></span><span style=display:flex><span>kubectl create <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    -f &lt;path&gt;/&lt;to&gt;/kubeconfig-request.json <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --raw /apis/core.gardener.cloud/v1beta1/namespaces/<span style=color:#a31515>${</span>NAMESPACE<span style=color:#a31515>}</span>/shoots/<span style=color:#a31515>${</span>SHOOT_NAME<span style=color:#a31515>}</span>/adminkubeconfig | jq -r <span style=color:#a31515>&#34;.status.kubeconfig&#34;</span> | base64 -d
</span></span></code></pre></div><p>Here, the <code>kubeconfig-request.json</code> has the following content:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    &#34;apiVersion&#34;: <span style=color:#a31515>&#34;authentication.gardener.cloud/v1alpha1&#34;</span>,
</span></span><span style=display:flex><span>    &#34;kind&#34;: <span style=color:#a31515>&#34;AdminKubeconfigRequest&#34;</span>,
</span></span><span style=display:flex><span>    &#34;spec&#34;: {
</span></span><span style=display:flex><span>        &#34;expirationSeconds&#34;: 1000
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>You also can use controller-runtime <code>client</code> (>= v0.14.3) to create such a kubeconfig from your go code like so:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>expiration := 8*time.Hour
</span></span><span style=display:flex><span>expirationSeconds := int64(expiration.Seconds())
</span></span><span style=display:flex><span>adminKubeconfigRequest := &amp;authenticationv1alpha1.AdminKubeconfigRequest{
</span></span><span style=display:flex><span>  Spec: authenticationv1alpha1.AdminKubeconfigRequestSpec{
</span></span><span style=display:flex><span>    ExpirationSeconds: &amp;expirationSeconds,
</span></span><span style=display:flex><span>  },
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>err := client.SubResource(<span style=color:#a31515>&#34;adminkubeconfig&#34;</span>).Create(ctx, shoot, adminKubeconfigRequest)
</span></span><span style=display:flex><span><span style=color:#00f>if</span> err != <span style=color:#00f>nil</span> {
</span></span><span style=display:flex><span>  <span style=color:#00f>return</span> err
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>config = adminKubeconfigRequest.Status.Kubeconfig
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> The <a href=https://github.com/gardener/gardenctl-v2/><code>gardenctl-v2</code></a> tool makes it easy to target shoot clusters and automatically renews such <code>kubeconfig</code> when required.</p></blockquote><h2 id=openid-connect>OpenID Connect</h2><p>The <code>kube-apiserver</code> of shoot clusters can be provided with <a href=https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens>OpenID Connect configuration</a> via the Shoot spec:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    oidcConfig:
</span></span><span style=display:flex><span>      ...
</span></span></code></pre></div><p>It is the end-user&rsquo;s responsibility to incorporate the OpenID Connect configurations in <code>kubeconfig</code> for accessing the cluster (i.e., Gardener will not automatically generate <code>kubeconfig</code> based on these OIDC settings).
The recommended way is using the <code>kubectl</code> plugin called <a href=https://github.com/int128/kubelogin><code>kubectl oidc-login</code></a> for OIDC authentication.</p><p>If you want to use the same OIDC configuration for all your shoots by default, then you can use the <code>ClusterOpenIDConnectPreset</code> and <code>OpenIDConnectPreset</code> API resources. They allow defaulting the <code>.spec.kubernetes.kubeAPIServer.oidcConfig</code> fields for newly created <code>Shoot</code>s such that you don&rsquo;t have to repeat yourself every time (similar to <code>PodPreset</code> resources in Kubernetes).
<code>ClusterOpenIDConnectPreset</code> specified OIDC configuration applies to <code>Projects</code> and <code>Shoots</code> cluster-wide (hence, only available to Gardener operators) while <code>OpenIDConnectPreset</code> is <code>Project</code>-scoped.
Shoots have to &ldquo;opt-in&rdquo; for such defaulting by using the <code>oidc=enable</code> label.</p><p>For further information on <code>(Cluster)OpenIDConnectPreset</code>, refer to <a href=/docs/gardener/usage/openidconnect-presets/>ClusterOpenIDConnectPreset and OpenIDConnectPreset</a>.</p><h2 id=static-token-kubeconfig>Static Token kubeconfig</h2><blockquote><p><strong>Note:</strong> Static token kubeconfig is not available for Shoot clusters using Kubernetes version >= 1.27. The <a href=#shootsadminkubeconfig-subresource><code>shoots/adminkubeconfig</code> subresource</a> should be used instead.</p></blockquote><p>This <code>kubeconfig</code> contains a <a href=https://kubernetes.io/docs/reference/access-authn-authz/authentication/#static-token-file>static token</a> and provides <code>cluster-admin</code> privileges.
It is created by default and persisted in the <code>&lt;shoot-name>.kubeconfig</code> secret in the project namespace in the garden cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    enableStaticTokenKubeconfig: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>It is <strong>not</strong> the recommended method to access the shoot cluster, as the static token <code>kubeconfig</code> has some security flaws associated with it:</p><ul><li>The static token in the <code>kubeconfig</code> doesn&rsquo;t have any expiration date. Read <a href=/docs/gardener/usage/shoot_credentials_rotation/#kubeconfig>this document</a> to learn how to rotate the static token.</li><li>The static token doesn&rsquo;t have any user identity associated with it. The user in that token will always be <code>system:cluster-admin</code>, irrespective of the person accessing the cluster. Hence, it is impossible to audit the events in cluster.</li></ul><p>When <code>enableStaticTokenKubeconfig</code> field is not explicitly set in the Shoot spec:</p><ul><li>for Shoot clusters using Kubernetes version &lt; 1.26 the field is defaulted to <code>true</code>.</li><li>for Shoot clusters using Kubernetes version >= 1.26 the field is defaulted to <code>false</code>.</li></ul><blockquote><p><strong>Note:</strong> Starting with Kubernetes 1.27, the <code>enableStaticTokenKubeconfig</code> field will be locked to <code>false</code>.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-ac6a66f3733149171ad802e94f5dc7e7>29 - Shoot Auditpolicy</h1><h1 id=audit-a-kubernetes-cluster>Audit a Kubernetes Cluster</h1><p>The shoot cluster is a Kubernetes cluster and its <code>kube-apiserver</code> handles the audit events. In order to define which audit events must be logged, a proper audit policy file must be passed to the Kubernetes API server. You could find more information about auditing a kubernetes cluster in the <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/audit/>Auditing</a> topic.</p><h2 id=default-audit-policy>Default Audit Policy</h2><p>By default, the Gardener will deploy the shoot cluster with audit policy defined in the <a href=https://github.com/gardener/gardener/blob/master/pkg/component/kubeapiserver/secrets.go>kube-apiserver package</a>.</p><h2 id=custom-audit-policy>Custom Audit Policy</h2><p>If you need specific audit policy for your shoot cluster, then you could deploy the required audit policy in the garden cluster as <code>ConfigMap</code> resource and set up your shoot to refer this <code>ConfigMap</code>. Note that the policy must be stored under the key <code>policy</code> in the data section of the <code>ConfigMap</code>.</p><p>For example, deploy the auditpolicy <code>ConfigMap</code> in the same namespace as your <code>Shoot</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f example/95-configmap-custom-audit-policy.yaml
</span></span></code></pre></div><p>then set your shoot to refer that <code>ConfigMap</code> (only related fields are shown):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeAPIServer:
</span></span><span style=display:flex><span>      auditConfig:
</span></span><span style=display:flex><span>        auditPolicy:
</span></span><span style=display:flex><span>          configMapRef:
</span></span><span style=display:flex><span>            name: auditpolicy
</span></span></code></pre></div><p>Gardener validate the <code>Shoot</code> resource to refer only existing <code>ConfigMap</code> containing valid audit policy, and rejects the <code>Shoot</code> on failure.
If you want to switch back to the default audit policy, you have to remove the section</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>auditPolicy:
</span></span><span style=display:flex><span>  configMapRef:
</span></span><span style=display:flex><span>    name: &lt;configmap-name&gt;
</span></span></code></pre></div><p>from the shoot spec.</p><h2 id=rolling-out-changes-to-the-audit-policy>Rolling Out Changes to the Audit Policy</h2><p>Gardener is not automatically rolling out changes to the Audit Policy to minimize the amount of Shoot reconciliations in order to prevent cloud provider rate limits, etc.
Gardener will pick up the changes on the next reconciliation of Shoots referencing the Audit Policy ConfigMap.
If users want to immediately rollout Audit Policy changes, they can manually trigger a Shoot reconciliation as described in <a href=/docs/gardener/usage/shoot_operations/#immediate-reconciliation>triggering an immediate reconciliation</a>.
This is similar to changes to the cloud provider secret referenced by Shoots.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-fca47cfc40d1ae8a466ee0cce9f8b9e3>30 - Shoot Autoscaling</h1><h1 id=auto-scaling-in-shoot-clusters>Auto-Scaling in Shoot Clusters</h1><p>There are two parts that relate to auto-scaling in Kubernetes clusters in general:</p><ul><li>Horizontal node auto-scaling, i.e., dynamically adding and removing worker nodes.</li><li>Vertical pod auto-scaling, i.e., dynamically raising or shrinking the resource requests/limits of pods.</li></ul><p>This document provides an overview of both scenarios.</p><h2 id=horizontal-node-auto-scaling>Horizontal Node Auto-Scaling</h2><p>Every shoot cluster that has at least one worker pool with <code>minimum &lt; maximum</code> nodes configuration will get a <code>cluster-autoscaler</code> deployment.
Gardener is leveraging the upstream community Kubernetes <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler><code>cluster-autoscaler</code> component</a>.
We have forked it to <a href=https://github.com/gardener/autoscaler/>gardener/autoscaler</a> so that it supports the way how Gardener manages the worker nodes (leveraging <a href=https://github.com/gardener/machine-controller-manager>gardener/machine-controller-manager</a>).
However, we have not touched the logic how it performs auto-scaling decisions.
Consequently, please refer to the <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#faqdocumentation>offical documentation</a> for this component.</p><p>The <code>Shoot</code> API allows to configure a few flags of the <code>cluster-autoscaler</code>:</p><ul><li><code>.spec.kubernetes.clusterAutoscaler.scaleDownDelayAfterAdd</code> defines how long after scale up that scale down evaluation resumes (default: <code>1h</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.scaleDownDelayAfterDelete</code> defines how long after node deletion that scale down evaluation resumes (defaults to <code>ScanInterval</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.scaleDownDelayAfterFailure</code> defines how long after scale down failure that scale down evaluation resumes (default: <code>3m</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.scaleDownUnneededTime</code> defines how long a node should be unneeded before it is eligible for scale down (default: <code>30m</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.scaleDownUtilizationThreshold</code> defines the threshold under which a node is being removed (default: <code>0.5</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.scanInterval</code> defines how often cluster is reevaluated for scale up or down (default: <code>10s</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.ignoreTaints</code> specifies a list of taint keys to ignore in node templates when considering to scale a node group (default: <code>nil</code>).</li></ul><h2 id=vertical-pod-auto-scaling>Vertical Pod Auto-Scaling</h2><p>This form of auto-scaling is not enabled by default and must be explicitly enabled in the <code>Shoot</code> by setting <code>.spec.kubernetes.verticalPodAutoscaler.enabled=true</code>.
The reason is that it was only introduced lately, and some end-users might have already deployed their own VPA into their clusters, i.e., enabling it by default would interfere with such custom deployments and lead to issues, eventually.</p><p>Gardener is also leveraging an upstream community tool, i.e., the Kubernetes <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler><code>vertical-pod-autoscaler</code> component</a>.
If enabled, Gardener will deploy it as part of the control plane into the seed cluster.
It will also be used for the vertical autoscaling of Gardener&rsquo;s system components deployed into the <code>kube-system</code> namespace of shoot clusters, for example, <code>kube-proxy</code> or <code>metrics-server</code>.</p><p>You might want to refer to the <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md>official documentation</a> for this component to get more information how to use it.</p><p>The <code>Shoot</code> API allows to configure a few flags of the <code>vertical-pod-autoscaler</code>:</p><ul><li><code>.spec.kubernetes.verticalPodAutoscaler.evictAfterOOMThreshold</code> defines the threshold that will lead to pod eviction in case it OOMed in less than the given threshold since its start and if it has only one container (default: <code>10m0s</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.evictionRateBurst</code> defines the burst of pods that can be evicted (default: <code>1</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.evictionRateLimit</code> defines the number of pods that can be evicted per second. A rate limit set to 0 or -1 will disable the rate limiter (default: <code>-1</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.evictionTolerance</code> defines the fraction of replica count that can be evicted for update in case more than one pod can be evicted (default: <code>0.5</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.recommendationMarginFraction</code> is the fraction of usage added as the safety margin to the recommended request (default: <code>0.15</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.updaterInterval</code> is the interval how often the updater should run (default: <code>1m0s</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.recommenderInterval</code> is the interval how often metrics should be fetched (default: <code>1m0s</code>).</li></ul><p>⚠️ Please note that if you disable the VPA again, then the related <code>CustomResourceDefinition</code>s will remain in your shoot cluster (although, nobody will act on them).
This will also keep all existing <code>VerticalPodAutoscaler</code> objects in the system, including those that might be created by you. You can delete the <code>CustomResourceDefinition</code>s yourself using <code>kubectl delete crd</code> if you want to get rid of them.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cdc20eb6ababa209934b6e4bb03138d6>31 - Shoot Cleanup</h1><h1 id=cleanup-of-shoot-clusters-in-deletion>Cleanup of Shoot Clusters in Deletion</h1><p>When a shoot cluster is deleted then Gardener tries to gracefully remove most of the Kubernetes resources inside the cluster.
This is to prevent that any infrastructure or other artefacts remain after the shoot deletion.</p><p>The cleanup is performed in four steps.
Some resources are deleted with a grace period, and all resources are forcefully deleted (by removing blocking finalizers) after some time to not block the cluster deletion entirely.</p><p><strong>Cleanup steps:</strong></p><ol><li>All <code>ValidatingWebhookConfiguration</code>s and <code>MutatingWebhookConfiguration</code>s are deleted with a <code>5m</code> grace period. Forceful finalization happens after <code>5m</code>.</li><li>All <code>APIService</code>s and <code>CustomResourceDefinition</code>s are deleted with a <code>5m</code> grace period. Forceful finalization happens after <code>1h</code>.</li><li>All <code>CronJob</code>s, <code>DaemonSet</code>s, <code>Deployment</code>s, <code>Ingress</code>s, <code>Job</code>s, <code>Pod</code>s, <code>ReplicaSet</code>s, <code>ReplicationController</code>s, <code>Service</code>s, <code>StatefulSet</code>s, <code>PersistentVolumeClaim</code>s are deleted with a <code>5m</code> grace period. Forceful finalization happens after <code>5m</code>.<blockquote><p>If the <code>Shoot</code> is annotated with <code>shoot.gardener.cloud/skip-cleanup=true</code>, then only <code>Service</code>s and <code>PersistentVolumeClaim</code>s are considered.</p></blockquote></li><li>All <code>VolumeSnapshot</code>s and <code>VolumeSnapshotContent</code>s are deleted with a <code>5m</code> grace period. Forceful finalization happens after <code>1h</code>.</li></ol><p>It is possible to override the finalization grace periods via annotations on the <code>Shoot</code>:</p><ul><li><code>shoot.gardener.cloud/cleanup-webhooks-finalize-grace-period-seconds</code> (for the resources handled in step 1)</li><li><code>shoot.gardener.cloud/cleanup-extended-apis-finalize-grace-period-seconds</code> (for the resources handled in step 2)</li><li><code>shoot.gardener.cloud/cleanup-kubernetes-resources-finalize-grace-period-seconds</code> (for the resources handled in step 3)</li></ul><p>⚠️ If <code>"0"</code> is provided, then all resources are finalized immediately without waiting for any graceful deletion.
Please be aware that this might lead to orphaned infrastructure artefacts.</p><h2 id=infrastructure-cleanup-wait-period>Infrastructure Cleanup Wait Period</h2><p>After all above cleanup steps have been performed and the <code>Infrastructure</code> extension resource has been deleted, the gardenlet waits for a certain duration to allow controllers to properly cleanup infrastructure resources.</p><p>By default, this duration is set to <code>5m</code>. Only after this time has passed, the shoot deletion flow continues with the entire tear-down of the remaining control plane components (including <code>kube-apiserver</code>s, etc.).</p><p>It is also possible to override this wait period via an annotations on the <code>Shoot</code>:</p><ul><li><code>shoot.gardener.cloud/infrastructure-cleanup-wait-period-seconds</code></li></ul><blockquote><p>ℹ️️ All provided period values larger than the above mentioned defaults are ignored.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-be2e4b54fd8d4a9659ea3ba10c6cee2a>32 - Shoot Credentials Rotation</h1><h1 id=credentials-rotation-for-shoot-clusters>Credentials Rotation for Shoot Clusters</h1><p>There are a lot of different credentials for <code>Shoot</code>s to make sure that the various components can communicate with each other and to make sure it is usable and operable.</p><p>This page explains how the varieties of credentials can be rotated so that the cluster can be considered secure.</p><h2 id=user-provided-credentials>User-Provided Credentials</h2><h3 id=cloud-provider-keys>Cloud Provider Keys</h3><p>End-users must provide credentials such that Gardener and Kubernetes controllers can communicate with the respective cloud provider APIs in order to perform infrastructure operations.
For example, Gardener uses them to setup and maintain the networks, security groups, subnets, etc., while the <a href=https://kubernetes.io/docs/concepts/architecture/cloud-controller/>cloud-controller-manager</a> uses them to reconcile load balancers and routes, and the <a href=https://kubernetes-csi.github.io/docs/>CSI controller</a> uses them to reconcile volumes and disks.</p><p>Depending on the cloud provider, the required <a href=https://github.com/gardener/gardener/blob/master/example/70-secret-provider.yaml>data keys of the <code>Secret</code> differ</a>.
Please consult the documentation of the respective provider extension documentation to get to know the concrete data keys (e.g., <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/usage-as-end-user/#provider-secret-data>this document for AWS</a>).</p><p><strong>It is the responsibility of the end-user to regularly rotate those credentials.</strong>
The following steps are required to perform the rotation:</p><ol><li>Update the data in the <code>Secret</code> with new credentials.</li><li>⚠️ Wait until all <code>Shoot</code>s using the <code>Secret</code> are reconciled before you disable the old credentials in your cloud provider account! Otherwise, the <code>Shoot</code>s will no longer work as expected. Check out <a href=/docs/gardener/usage/shoot_operations/#immediate-reconciliation>this document</a> to learn how to trigger a reconciliation of your <code>Shoot</code>s.</li><li>After all <code>Shoot</code>s using the <code>Secret</code> were reconciled, you can go ahead and deactivate the old credentials in your provider account.</li></ol><h2 id=gardener-provided-credentials>Gardener-Provided Credentials</h2><p>The below credentials are generated by Gardener when shoot clusters are being created.
Those include:</p><ul><li>kubeconfig (if enabled)</li><li>certificate authorities (and related server and client certificates)</li><li>observability passwords for Plutono</li><li>SSH key pair for worker nodes</li><li>ETCD encryption key</li><li><code>ServiceAccount</code> token signing key</li><li>&mldr;</li></ul><p><strong>🚨 There is no auto-rotation of those credentials, and it is the responsibility of the end-user to regularly rotate them.</strong></p><p>While it is possible to rotate them one by one, there is also a convenient method to combine the rotation of all of those credentials.
The rotation happens in two phases since it might be required to update some API clients (e.g., when CAs are rotated).
In order to start the rotation (first phase), you have to annotate the shoot with the <code>rotate-credentials-start</code> operation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-credentials-start
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> You can check the <code>.status.credentials.rotation</code> field in the <code>Shoot</code> to see when the rotation was last initiated and last completed.</p></blockquote><p>Kindly consider the detailed descriptions below to learn how the rotation is performed and what your responsibilities are.
Please note that all respective individual actions apply for this combined rotation as well (e.g., worker nodes are rolled out in the first phase).</p><p>You can complete the rotation (second phase) by annotating the shoot with the <code>rotate-credentials-complete</code> operation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-credentials-complete
</span></span></code></pre></div><h3 id=kubeconfig>Kubeconfig</h3><p>If the <code>.spec.kubernetes.enableStaticTokenKubeconfig</code> field is set to <code>true</code> (default), then Gardener generates a <code>kubeconfig</code> with <code>cluster-admin</code> privileges for the <code>Shoot</code>s containing credentials for communication with the <code>kube-apiserver</code> (see <a href=/docs/gardener/usage/shoot_access/#static-token-kubeconfig>this document</a> for more information).</p><p>This <code>Secret</code> is stored with the name <code>&lt;shoot-name>.kubeconfig</code> in the project namespace in the garden cluster and has multiple data keys:</p><ul><li><code>kubeconfig</code>: the completed kubeconfig</li><li><code>ca.crt</code>: the CA bundle for establishing trust to the API server (same as in the <a href=#cluster-certificate-authority-bundle>Cluster CA bundle secret</a>)</li></ul><blockquote><p><code>Shoots</code> created with Gardener &lt;= 0.28 used to have a <code>kubeconfig</code> based on a client certificate instead of a static token. With the first kubeconfig rotation, such clusters will get a static token as well.</p><p>⚠️ This does not invalidate the old client certificate. In order to do this, you should perform a rotation of the CAs (see section below).</p></blockquote><p><strong>It is the responsibility of the end-user to regularly rotate those credentials (or disable this <code>kubeconfig</code> entirely).</strong>
In order to rotate the <code>token</code> in this <code>kubeconfig</code>, annotate the <code>Shoot</code> with <code>gardener.cloud/operation=rotate-kubeconfig-credentials</code>.
This operation is not allowed for <code>Shoot</code>s that are already marked for deletion.
Please note that only the token (and basic auth password, if enabled) are exchanged.
The CA certificate remains the same (see section below for information about the rotation).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-kubeconfig-credentials
</span></span></code></pre></div><blockquote><p>You can check the <code>.status.credentials.rotation.kubeconfig</code> field in the <code>Shoot</code> to see when the rotation was last initiated and last completed.</p></blockquote><h3 id=certificate-authorities>Certificate Authorities</h3><p>Gardener generates several certificate authorities (CAs) to ensure secured communication between the various components and actors.
Most of those CAs are used for internal communication (e.g., <code>kube-apiserver</code> talks to etcd, <code>vpn-shoot</code> talks to the <code>vpn-seed-server</code>, <code>kubelet</code> talks to <code>kube-apiserver</code>).
However, there is also the &ldquo;cluster CA&rdquo; which is part of all <code>kubeconfig</code>s and used to sign the server certificate exposed by the <code>kube-apiserver</code>.</p><p>Gardener populates a <code>Secret</code> with the name <code>&lt;shoot-name>.ca-cluster</code> in the project namespace in the garden cluster which contains the following data keys:</p><ul><li><code>ca.crt</code>: the CA bundle of the cluster</li></ul><p>This bundle contains one or multiple CAs which are used for signing serving certificates of the <code>Shoot</code>&rsquo;s API server.
Hence, the certificates contained in this <code>Secret</code> can be used to verify the API server&rsquo;s identity when communicating with its public endpoint (e.g., as <code>certificate-authority-data</code> in a <code>kubeconfig</code>).
This is the same certificate that is also contained in the <code>kubeconfig</code>&rsquo;s <code>certificate-authority-data</code> field.</p><blockquote><p><code>Shoot</code>s created with Gardener >= v1.45 have a dedicated client CA which verifies the legitimacy of client certificates. For older <code>Shoot</code>s, the client CA is equal to the cluster CA. With the first CA rotation, such clusters will get a dedicated client CA as well.</p></blockquote><p>All of the certificates are valid for 10 years.
Since it requires adaptation for the consumers of the <code>Shoot</code>, there is no automatic rotation and <strong>it is the responsibility of the end-user to regularly rotate the CA certificates.</strong></p><p>The rotation happens in three stages (see also <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/18-shoot-CA-rotation.md>GEP-18</a> for the full details):</p><ul><li>In stage one, new CAs are created and added to the bundle (together with the old CAs). Client certificates are re-issued immediately.</li><li>In stage two, end-users update all cluster API clients that communicate with the control plane.</li><li>In stage three, the old CAs are dropped from the bundle and server certificate are re-issued.</li></ul><p>Technically, the <code>Preparing</code> phase indicates stage one.
Once it is completed, the <code>Prepared</code> phase indicates readiness for stage two.
The <code>Completing</code> phase indicates stage three, and the <code>Completed</code> phase states that the rotation process has finished.</p><blockquote><p>You can check the <code>.status.credentials.rotation.certificateAuthorities</code> field in the <code>Shoot</code> to see when the rotation was last initiated, last completed, and in which phase it currently is.</p></blockquote><p>In order to start the rotation (stage one), you have to annotate the shoot with the <code>rotate-ca-start</code> operation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-ca-start
</span></span></code></pre></div><p>This will trigger a <code>Shoot</code> reconciliation and performs stage one.
After it is completed, the <code>.status.credentials.rotation.certificateAuthorities.phase</code> is set to <code>Prepared</code>.</p><p>Now you must update all API clients outside the cluster (such as the <code>kubeconfig</code>s on developer machines) to use the newly issued CA bundle in the <code>&lt;shoot-name>.ca-cluster</code> <code>Secret</code>.
Please also note that client certificates must be re-issued now.</p><p>After updating all API clients, you can complete the rotation by annotating the shoot with the <code>rotate-ca-complete</code> operation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-ca-complete
</span></span></code></pre></div><p>This will trigger another <code>Shoot</code> reconciliation and performs stage three.
After it is completed, the <code>.status.credentials.rotation.certificateAuthorities.phase</code> is set to <code>Completed</code>.
You could update your API clients again and drop the old CA from their bundle.</p><blockquote><p>Note that the CA rotation also rotates all internal CAs and signed certificates.
Hence, most of the components need to be restarted (including etcd and <code>kube-apiserver</code>).</p><p>⚠️ In stage one, all worker nodes of the <code>Shoot</code> will be rolled out to ensure that the <code>Pod</code>s as well as the <code>kubelet</code>s get the updated credentials as well.</p></blockquote><h3 id=observability-passwords-for-plutono-and-prometheus>Observability Password(s) For Plutono and Prometheus</h3><p>For <code>Shoot</code>s with <code>.spec.purpose!=testing</code>, Gardener deploys an observability stack with Prometheus for monitoring, Alertmanager for alerting (optional), Vali for logging, and Plutono for visualization.
The Plutono instance is exposed via <code>Ingress</code> and accessible for end-users via basic authentication credentials generated and managed by Gardener.</p><p>Those credentials are stored in a <code>Secret</code> with the name <code>&lt;shoot-name>.monitoring</code> in the project namespace in the garden cluster and has multiple data keys:</p><ul><li><code>username</code>: the user name</li><li><code>password</code>: the password</li><li><code>auth</code>: the user name with SHA-1 representation of the password</li></ul><p><strong>It is the responsibility of the end-user to regularly rotate those credentials.</strong>
In order to rotate the <code>password</code>, annotate the <code>Shoot</code> with <code>gardener.cloud/operation=rotate-observability-credentials</code>.
This operation is not allowed for <code>Shoot</code>s that are already marked for deletion.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-observability-credentials
</span></span></code></pre></div><blockquote><p>You can check the <code>.status.credentials.rotation.observability</code> field in the <code>Shoot</code> to see when the rotation was last initiated and last completed.</p></blockquote><h3 id=ssh-key-pair-for-worker-nodes>SSH Key Pair for Worker Nodes</h3><p>Gardener generates an SSH key pair whose public key is propagated to all worker nodes of the <code>Shoot</code>.
The private key can be used to establish an SSH connection to the workers for troubleshooting purposes.
It is recommended to use <a href=https://github.com/gardener/gardenctl-v2/><code>gardenctl-v2</code></a> and its <code>gardenctl ssh</code> command since it is required to first open up the security groups and create a bastion VM (no direct SSH access to the worker nodes is possible).</p><p>The private key is stored in a <code>Secret</code> with the name <code>&lt;shoot-name>.ssh-keypair</code> in the project namespace in the garden cluster and has multiple data keys:</p><ul><li><code>id_rsa</code>: the private key</li><li><code>id_rsa.pub</code>: the public key for SSH</li></ul><p>In order to rotate the keys, annotate the <code>Shoot</code> with <code>gardener.cloud/operation=rotate-ssh-keypair</code>.
This will propagate a new key to all worker nodes while keeping the old key active and valid as well (it will only be invalidated/removed with the next rotation).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-ssh-keypair
</span></span></code></pre></div><blockquote><p>You can check the <code>.status.credentials.rotation.sshKeypair</code> field in the <code>Shoot</code> to see when the rotation was last initiated or last completed.</p></blockquote><p>The old key is stored in a <code>Secret</code> with the name <code>&lt;shoot-name>.ssh-keypair.old</code> in the project namespace in the garden cluster and has the same data keys as the regular <code>Secret</code>.</p><h3 id=etcd-encryption-key>ETCD Encryption Key</h3><p>This key is used to encrypt the data of <code>Secret</code> resources inside etcd (see <a href=https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/>upstream Kubernetes documentation</a>).</p><p>The encryption key has no expiration date.
There is no automatic rotation and <strong>it is the responsibility of the end-user to regularly rotate the encryption key.</strong></p><p>The rotation happens in three stages:</p><ul><li>In stage one, a new encryption key is created and added to the bundle (together with the old encryption key).</li><li>In stage two, all <code>Secret</code>s in the cluster are rewritten by the <code>kube-apiserver</code> so that they become encrypted with the new encryption key.</li><li>In stage three, the old encryption is dropped from the bundle.</li></ul><p>Technically, the <code>Preparing</code> phase indicates the stages one and two.
Once it is completed, the <code>Prepared</code> phase indicates readiness for stage three.
The <code>Completing</code> phase indicates stage three, and the <code>Completed</code> phase states that the rotation process has finished.</p><blockquote><p>You can check the <code>.status.credentials.rotation.etcdEncryptionKey</code> field in the <code>Shoot</code> to see when the rotation was last initiated, last completed, and in which phase it currently is.</p></blockquote><p>In order to start the rotation (stage one), you have to annotate the shoot with the <code>rotate-etcd-encryption-key-start</code> operation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-etcd-encryption-key-start
</span></span></code></pre></div><p>This will trigger a <code>Shoot</code> reconciliation and performs the stages one and two.
After it is completed, the <code>.status.credentials.rotation.etcdEncryptionKey.phase</code> is set to <code>Prepared</code>.
Now you can complete the rotation by annotating the shoot with the <code>rotate-etcd-encryption-key-complete</code> operation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-etcd-encryption-key-complete
</span></span></code></pre></div><p>This will trigger another <code>Shoot</code> reconciliation and performs stage three.
After it is completed, the <code>.status.credentials.rotation.etcdEncryptionKey.phase</code> is set to <code>Completed</code>.</p><h3 id=serviceaccount-token-signing-key><code>ServiceAccount</code> Token Signing Key</h3><p>Gardener generates a key which is used to sign the tokens for <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/><code>ServiceAccount</code>s</a>.
Those tokens are typically used by workload <code>Pod</code>s running inside the cluster in order to authenticate themselves with the <code>kube-apiserver</code>.
This also includes system components running in the <code>kube-system</code> namespace.</p><p>The token signing key has no expiration date.
Since it might require adaptation for the consumers of the <code>Shoot</code>, there is no automatic rotation and <strong>it is the responsibility of the end-user to regularly rotate the signing key.</strong></p><p>The rotation happens in three stages, similar to how the <a href=#certificate-authorities>CA certificates</a> are rotated:</p><ul><li>In stage one, a new signing key is created and added to the bundle (together with the old signing key).</li><li>In stage two, end-users update all out-of-cluster API clients that communicate with the control plane via <code>ServiceAccount</code> tokens.</li><li>In stage three, the old signing key is dropped from the bundle.</li></ul><p>Technically, the <code>Preparing</code> phase indicates stage one.
Once it is completed, the <code>Prepared</code> phase indicates readiness for stage two.
The <code>Completing</code> phase indicates stage three, and the <code>Completed</code> phase states that the rotation process has finished.</p><blockquote><p>You can check the <code>.status.credentials.rotation.serviceAccountKey</code> field in the <code>Shoot</code> to see when the rotation was last initiated, last completed, and in which phase it currently is.</p></blockquote><p>In order to start the rotation (stage one), you have to annotate the shoot with the <code>rotate-serviceaccount-key-start</code> operation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-serviceaccount-key-start
</span></span></code></pre></div><p>This will trigger a <code>Shoot</code> reconciliation and performs stage one.
After it is completed, the <code>.status.credentials.rotation.serviceAccountKey.phase</code> is set to <code>Prepared</code>.</p><p>Now you must update all API clients outside the cluster using a <code>ServiceAccount</code> token (such as the <code>kubeconfig</code>s on developer machines) to use a token issued by the new signing key.
Gardener already generates new static token secrets for all <code>ServiceAccount</code>s in the cluster.
However, if you need to create it manually, you can check out <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#manually-create-a-service-account-api-token>this document</a> for instructions.</p><p>After updating all API clients, you can complete the rotation by annotating the shoot with the <code>rotate-serviceaccount-key-complete</code> operation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-serviceaccount-key-complete
</span></span></code></pre></div><p>This will trigger another <code>Shoot</code> reconciliation and performs stage three.
After it is completed, the <code>.status.credentials.rotation.serviceAccountKey.phase</code> is set to <code>Completed</code>.</p><blockquote><p>⚠️ In stage one, all worker nodes of the <code>Shoot</code> will be rolled out to ensure that the <code>Pod</code>s use a new token.</p></blockquote><h3 id=openvpn-tls-auth-keys>OpenVPN TLS Auth Keys</h3><p>This key is used to ensure encrypted communication for the VPN connection between the control plane in the seed cluster and the shoot cluster.
It is currently <strong>not</strong> rotated automatically and there is no way to trigger it manually.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-99132edc0ae6e88780688fe9c346c413>33 - Shoot High Availability</h1><h1 id=highly-available-shoot-control-plane>Highly Available Shoot Control Plane</h1><p>Shoot resource offers a way to request for a highly available control plane.</p><h2 id=failure-tolerance-types>Failure Tolerance Types</h2><p>A highly available shoot control plane can be setup with either a failure tolerance of <code>zone</code> or <code>node</code>.</p><h3 id=node-failure-tolerance><code>Node</code> Failure Tolerance</h3><p>The failure tolerance of a <code>node</code> will have the following characteristics:</p><ul><li>Control plane components will be spread across different nodes within a single availability zone. There will not be
more than one replica per node for each control plane component which has more than one replica.</li><li><code>Worker pool</code> should have a minimum of 3 nodes.</li><li>A multi-node etcd (quorum size of 3) will be provisioned, offering zero-downtime capabilities with each member in a
different node within a single availability zone.</li></ul><h3 id=zone-failure-tolerance><code>Zone</code> Failure Tolerance</h3><p>The failure tolerance of a <code>zone</code> will have the following characteristics:</p><ul><li>Control plane components will be spread across different availability zones. There will be at least
one replica per zone for each control plane component which has more than one replica.</li><li>Gardener scheduler will automatically select a <code>seed</code> which has a minimum of 3 zones to host the shoot control plane.</li><li>A multi-node etcd (quorum size of 3) will be provisioned, offering zero-downtime capabilities with each member in a
different zone.</li></ul><h2 id=shoot-spec>Shoot Spec</h2><p>To request for a highly available shoot control plane Gardener provides the following configuration in the shoot spec:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  controlPlane:
</span></span><span style=display:flex><span>    highAvailability:
</span></span><span style=display:flex><span>      failureTolerance:
</span></span><span style=display:flex><span>        type: &lt;node | zone&gt;
</span></span></code></pre></div><p><strong>Allowed Transitions</strong></p><p>If you already have a shoot cluster with non-HA control plane, then the following upgrades are possible:</p><ul><li>Upgrade of non-HA shoot control plane to HA shoot control plane with <code>node</code> failure tolerance.</li><li>Upgrade of non-HA shoot control plane to HA shoot control plane with <code>zone</code> failure tolerance. However, it is essential that the <code>seed</code> which is currently hosting the shoot control plane should be <code>multi-zonal</code>. If it is not, then the request to upgrade will be rejected.</li></ul><blockquote><p><strong>Note:</strong> There will be a small downtime during the upgrade, especially for etcd, which will transition from a single node etcd cluster to a multi-node etcd cluster.</p></blockquote><p><strong>Disallowed Transitions</strong></p><p>If you have already set-up an HA shoot control plane with <code>node</code> failure tolerance, then an upgrade to a <code>zone</code> failure tolerance is currently not supported, mainly because already existing volumes are bound to the zone they were created in originally.</p><h2 id=zone-outage-situation>Zone Outage Situation</h2><p>Implementing highly available software that can tolerate even a zone outage unscathed is no trivial task. You may find our <a href=/docs/gardener/usage/shoot_high_availability_best_practices/>HA Best Practices</a> helpful to get closer to that goal. In this document, we collected many options and settings for you that also Gardener internally uses to provide a highly available service.</p><p>During a zone outage, you may be forced to change your cluster setup on short notice in order to compensate for failures and shortages resulting from the outage.
For instance, if the shoot cluster has worker nodes across three zones where one zone goes down, the computing power from these nodes is also gone during that time.
Changing the worker pool (<code>shoot.spec.provider.workers[]</code>) and infrastructure (<code>shoot.spec.provider.infrastructureConfig</code>) configuration can eliminate this disbalance, having enough machines in healthy availability zones that can cope with the requests of your applications.</p><p>Gardener relies on a sophisticated reconciliation flow with several dependencies for which various flow steps wait for the <em>readiness</em> of prior ones.
During a zone outage, this can block the entire flow, e.g., because all three <code>etcd</code> replicas can never be ready when a zone is down, and required changes mentioned above can never be accomplished.
For this, a special one-off annotation <code>shoot.gardener.cloud/skip-readiness</code> helps to skip any readiness checks in the flow.</p><blockquote><p>The <code>shoot.gardener.cloud/skip-readiness</code> annotation serves as a last resort if reconciliation is stuck because of important changes during an AZ outage. Use it with caution, only in exceptional cases and after a case-by-case evaluation with your Gardener landscape administrator. If used together with other operations like Kubernetes version upgrades or credential rotation, the annotation may lead to a severe outage of your shoot control plane.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-0e751384315d5569017324b7d6c6616d>34 - Shoot High Availability Best Practices</h1><h1 id=implementing-high-availability-and-tolerating-zone-outages>Implementing High Availability and Tolerating Zone Outages</h1><p>Developing highly available workload that can tolerate a zone outage is no trivial task. You will find here various recommendations to get closer to that goal. While many recommendations are general enough, the examples are specific in how to achieve this in a Gardener-managed cluster and where/how to tweak the different control plane components. If you do not use Gardener, it may be still a worthwhile read.</p><p>First however, what is a zone outage? It sounds like a clear-cut &ldquo;thing&rdquo;, but it isn&rsquo;t. There are many things that can go haywire. Here are some examples:</p><ul><li>Elevated cloud provider API error rates for individual or multiple services</li><li>Network bandwidth reduced or latency increased, usually also effecting storage sub systems as they are network attached</li><li>No networking at all, no DNS, machines shutting down or restarting, &mldr;</li><li>Functional issues, of either the entire service (e.g. all block device operations) or only parts of it (e.g. LB listener registration)</li><li>All services down, temporarily or permanently (the proverbial burning down data center 🔥)</li></ul><p>This and everything in between make it hard to prepare for such events, but you can still do a lot. The most important recommendation is to not target specific issues exclusively - tomorrow another service will fail in an unanticipated way. Also, focus more on <a href=https://research.google/pubs/pub50828>meaningful availability</a> than on internal signals (useful, but not as relevant as the former). Always prefer automation over manual intervention (e.g. leader election is a pretty robust mechanism, auto-scaling may be required as well, etc.).</p><p>Also remember that HA is costly - you need to balance it against the cost of an outage as silly as this may sound, e.g. running all this excess capacity &ldquo;just in case&rdquo; vs. &ldquo;going down&rdquo; vs. a risk-based approach in between where you have means that will kick in, but they are not guaranteed to work (e.g. if the cloud provider is out of resource capacity). Maybe some of your components must run at the highest possible availability level, but others not - that&rsquo;s a decision only you can make.</p><h2 id=control-plane>Control Plane</h2><p>The Kubernetes cluster control plane is managed by Gardener (as pods in separate infrastructure clusters to which you have no direct access) and can be set up with no failure tolerance (control plane pods will be recreated best-effort when resources are available) or one of the <a href=/docs/gardener/usage/shoot_high_availability/>failure tolerance types <code>node</code> or <code>zone</code></a>.</p><p>Strictly speaking, static workload does not depend on the (high) availability of the control plane, but static workload doesn&rsquo;t rhyme with Cloud and Kubernetes and also means, that when you possibly need it the most, e.g. during a zone outage, critical self-healing or auto-scaling functionality won&rsquo;t be available to you and your workload, if your control plane is down as well. That&rsquo;s why, even though the resource consumption is significantly higher, we generally recommend to use the failure tolerance type <code>zone</code> for the control planes of productive clusters, at least in all regions that have 3+ zones. Regions that have only 1 or 2 zones don&rsquo;t support the failure tolerance type <code>zone</code> and then your second best option is the failure tolerance type <code>node</code>, which means a zone outage can still take down your control plane, but individual node outages won&rsquo;t.</p><p>In the <code>shoot</code> resource it&rsquo;s merely only this what you need to add:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  controlPlane:
</span></span><span style=display:flex><span>    highAvailability:
</span></span><span style=display:flex><span>      failureTolerance:
</span></span><span style=display:flex><span>        type: zone <span style=color:green># valid values are `node` and `zone` (only available if your control plane resides in a region with 3+ zones)</span>
</span></span></code></pre></div><p>This setting will scale out all control plane components for a Gardener cluster as necessary, so that no single zone outage can take down the control plane for longer than just a few seconds for the fail-over to take place (e.g. lease expiration and new leader election or readiness probe failure and endpoint removal). Components run highly available in either active-active (servers) or active-passive (controllers) mode at all times, the persistence (ETCD), which is consensus-based, will tolerate the loss of one zone and still maintain quorum and therefore remain operational. These are all patterns that we will revisit down below also for your own workload.</p><h2 id=worker-pools>Worker Pools</h2><p>Now that you have configured your Kubernetes cluster control plane in HA, i.e. spread it across multiple zones, you need to do the same for your own workload, but in order to do so, you need to spread your nodes across multiple zones first.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: ...
</span></span><span style=display:flex><span>      minimum: 6
</span></span><span style=display:flex><span>      maximum: 60
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - ...
</span></span></code></pre></div><p>Prefer regions with at least 2, better 3+ zones and list the zones in the <code>zones</code> section for each of your worker pools. Whether you need 2 or 3 zones at a minimum depends on your fail-over concept:</p><ul><li>Consensus-based software components (like ETCD) depend on maintaining a quorum of <code>(n/2)+1</code>, so you need at least 3 zones to tolerate the outage of 1 zone.</li><li>Primary/Secondary-based software components need just 2 zones to tolerate the outage of 1 zone.</li><li>Then there are software components that can scale out horizontally. They are probably fine with 2 zones, but you also need to think about the load-shift and that the remaining zone must then pick up the work of the unhealthy zone. With 2 zones, the remaining zone must cope with an increase of 100% load. With 3 zones, the remaining zones must only cope with an increase of 50% load (per zone).</li></ul><p>In general, the question is also whether you have the fail-over capacity already up and running or not. If not, i.e. you depend on re-scheduling to a healthy zone or auto-scaling, be aware that during a zone outage, you will see a resource crunch in the healthy zones. If you have no automation, i.e. only human operators (a.k.a. &ldquo;red button approach&rdquo;), you probably will not get the machines you need and even with automation, it may be tricky. But holding the capacity available at all times is costly. In the end, that&rsquo;s a decision only you can make. If you made that decision, please adapt the <code>minimum</code> and <code>maximum</code> settings for your worker pools accordingly.</p><p>Also, consider fall-back worker pools (with different/alternative machine types) and <a href=https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders>cluster autoscaler expanders</a> using a <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/expander/priority/readme.md>priority-based strategy</a>.</p><p>Gardener-managed clusters deploy the <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler>cluster autoscaler</a> or CA for short and you can <a href=https://gardener.cloud/docs/gardener/api-reference/core/#clusterautoscaler>tweak the general CA knobs</a> for Gardener-managed clusters like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    clusterAutoscaler:
</span></span><span style=display:flex><span>      expander: <span style=color:#a31515>&#34;least-waste&#34;</span>
</span></span><span style=display:flex><span>      scanInterval: 10s
</span></span><span style=display:flex><span>      scaleDownDelayAfterAdd: 60m
</span></span><span style=display:flex><span>      scaleDownDelayAfterDelete: 0s
</span></span><span style=display:flex><span>      scaleDownDelayAfterFailure: 3m
</span></span><span style=display:flex><span>      scaleDownUnneededTime: 30m
</span></span><span style=display:flex><span>      scaleDownUtilizationThreshold: 0.5
</span></span></code></pre></div><p>If you want to be ready for a sudden spike or have some buffer in general, <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-can-i-configure-overprovisioning-with-cluster-autoscaler>over-provision nodes by means of &ldquo;placeholder&rdquo; pods</a> with <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption>low priority</a> and appropriate resource requests. This way, they will demand nodes to be provisioned for them, but if any pod comes up with a regular/higher priority, the low priority pods will be evicted to make space for the more important ones. Strictly speaking, this is not related to HA, but it may be important to keep this in mind as you generally want critical components to be rescheduled as fast as possible and if there is no node available, it may take 3 minutes or longer to do so (depending on the cloud provider). Besides, not only zones can fail, but also individual nodes.</p><h2 id=replicas-horizontal-scaling>Replicas (Horizontal Scaling)</h2><p>Now let&rsquo;s talk about your workload. In most cases, this will mean to run multiple replicas. If you cannot do that (a.k.a. you have a singleton), that&rsquo;s a bad situation to be in. Maybe you can run a spare (secondary) as backup? If you cannot, you depend on quick detection and rescheduling of your singleton (more on that below).</p><p>Obviously, things get messier with persistence. If you have persistence, you should ideally replicate your data, i.e. let your spare (secondary) &ldquo;follow&rdquo; your main (primary). If your software doesn&rsquo;t support that, you have to deploy other means, e.g. <a href=https://kubernetes.io/docs/concepts/storage/volume-snapshots>volume snapshotting</a> or side-backups (specific to the software you deploy; keep the backups regional, so that you can switch to another zone at all times). If you have to do those, your HA scenario becomes more a DR scenario and terms like RPO and RTO become relevant to you:</p><ul><li><strong>Recovery Point Objective (RPO)</strong>: Potential data loss, i.e. how much data will you lose at most (time between backups)</li><li><strong>Recovery Time Objective (RTO)</strong>: Time until recovery, i.e. how long does it take you to be operational again (time to restore)</li></ul><p>Also, keep in mind that your persistent volumes are usually zonal, i.e. once you have a volume in one zone, it&rsquo;s bound to that zone and you cannot get up your pod in another zone w/o first recreating the volume yourself (Kubernetes won&rsquo;t help you here directly).</p><p>Anyway, best avoid that, if you can (from technical and cost perspective). The best solution (and also the most costly one) is to run multiple replicas in multiple zones and keep your data replicated at all times, so that your RPO is always 0 (best). That&rsquo;s what we do for Gardener-managed cluster HA control planes (ETCD) as any data loss may be disastrous and lead to orphaned resources (in addition, we deploy side cars that do side-backups for disaster recovery, with full and incremental snapshots with an RPO of 5m).</p><p>So, how to run with multiple replicas? That&rsquo;s the easiest part in Kubernetes and the two most important resources, <code>Deployments</code> and <code>StatefulSet</code>, support that out of the box:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment | StatefulSet
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: ...
</span></span></code></pre></div><p>The problem comes with the number of replicas. It&rsquo;s easy only if the number is static, e.g. 2 for active-active/passive or 3 for consensus-based software components, but what with software components that can scale out horizontally? Here you usually do not set the number of replicas statically, but make use of the <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale>horizontal pod autoscaler</a> or HPA for short (built-in; part of the kube-controller-manager). There are also other options like the <a href=https://github.com/kubernetes-sigs/cluster-proportional-autoscaler>cluster proportional autoscaler</a>, but while the former works based on metrics, the latter is more a guestimate approach that derives the number of replicas from the number of nodes/cores in a cluster. Sometimes useful, but often blind to the actual demand.</p><p>So, HPA it is then for most of the cases. However, what is the resource (e.g. CPU or memory) that drives the number of desired replicas? Again, this is up to you, but not always are CPU or memory the best choices. In some cases, <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#scaling-on-custom-metrics>custom metrics</a> may be more appropriate, e.g. requests per second (it was also for us).</p><p>You will have to create specific <code>HorizontalPodAutoscaler</code> resources for your scale target and can <a href=https://gardener.cloud/docs/gardener/api-reference/core/#horizontalpodautoscalerconfig>tweak the general HPA knobs</a> for Gardener-managed clusters like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeControllerManager:
</span></span><span style=display:flex><span>      horizontalPodAutoscaler:
</span></span><span style=display:flex><span>        syncPeriod: 15s
</span></span><span style=display:flex><span>        tolerance: 0.1
</span></span><span style=display:flex><span>        downscaleStabilization: 5m0s
</span></span><span style=display:flex><span>        initialReadinessDelay: 30s
</span></span><span style=display:flex><span>        cpuInitializationPeriod: 5m0s
</span></span></code></pre></div><h2 id=resources-vertical-scaling>Resources (Vertical Scaling)</h2><p>While it is important to set a sufficient number of replicas, it is also important to give the pods sufficient resources (CPU and memory). This is especially true when you think about HA. When a zone goes down, you might need to get up replacement pods, if you don&rsquo;t have them running already to take over the load from the impacted zone. Likewise, e.g. with active-active software components, you can expect the remaining pods to receive more load. If you cannot scale them out horizontally to serve the load, you will probably need to scale them out (or rather up) vertically. This is done by the <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler>vertical pod autoscaler</a> or VPA for short (not built-in; part of the <a href=https://github.com/kubernetes/autoscaler>kubernetes/autoscaler</a> repository).</p><p>A few caveats though:</p><ul><li>You cannot use HPA and VPA on the same metrics as they would influence each other, which would lead to pod trashing (more replicas require fewer resources; fewer resources require more replicas)</li><li>Scaling horizontally doesn&rsquo;t cause downtimes (at least not when out-scaling and only one replica is affected when in-scaling), but scaling vertically does (if the pod runs OOM anyway, but also when new recommendations are applied, resource requests for existing pods may be changed, which causes the pods to be rescheduled). Although the discussion is going on for a very long time now, that is still not supported in-place yet (see <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/1287-in-place-update-pod-resources/README.md>KEP 1287</a>, <a href=https://github.com/kubernetes/kubernetes/pull/102884>implementation in Kubernetes</a>, <a href=https://github.com/kubernetes/autoscaler/issues/4016>implementation in VPA</a>).</li></ul><p>VPA is a useful tool and Gardener-managed clusters deploy a VPA by default for you (HPA is supported anyway as it&rsquo;s built into the kube-controller-manager). You will have to create specific <code>VerticalPodAutoscaler</code> resources for your scale target and can <a href=https://gardener.cloud/docs/gardener/api-reference/core/#verticalpodautoscaler>tweak the general VPA knobs</a> for Gardener-managed clusters like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    verticalPodAutoscaler:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      evictAfterOOMThreshold: 10m0s
</span></span><span style=display:flex><span>      evictionRateBurst: 1
</span></span><span style=display:flex><span>      evictionRateLimit: -1
</span></span><span style=display:flex><span>      evictionTolerance: 0.5
</span></span><span style=display:flex><span>      recommendationMarginFraction: 0.15
</span></span><span style=display:flex><span>      updaterInterval: 1m0s
</span></span><span style=display:flex><span>      recommenderInterval: 1m0s
</span></span></code></pre></div><p>While horizontal pod autoscaling is relatively straight-forward, it takes a long time to master vertical pod autoscaling. We saw <a href=https://github.com/kubernetes/autoscaler/issues/4498>performance issues</a>, hard-coded behavior (on OOM, memory is bumped by +20% and it may take a few iterations to reach a good level), unintended pod disruptions by applying new resource requests (after 12h all targeted pods will receive new requests even though individually they would be fine without, which also drives active-passive resource consumption up), difficulties to deal with spiky workload in general (due to the algorithmic approach it takes), recommended requests may exceed node capacity, limit scaling is proportional and therefore often questionable, and more. VPA is a double-edged sword: useful and necessary, but not easy to handle.</p><p>For the Gardener-managed components, we mostly removed limits. Why?</p><ul><li>CPU limits have almost always only downsides. They cause needless CPU throttling, which is not even easily visible. CPU requests turn into <code>cpu shares</code>, so if the node has capacity, the pod may consume the freely available CPU, but not if you have set limits, which curtail the pod by means of <code>cpu quota</code>. There are only certain scenarios in which they may make sense, e.g. if you set requests=limits and thereby define a pod with <code>guaranteed</code> <a href=https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod>QoS</a>, which influences your <code>cgroup</code> placement. However, that is difficult to do for the components you implement yourself and practically impossible for the components you just consume, because what&rsquo;s the correct value for requests/limits and will it hold true also if the load increases and what happens if a zone goes down or with the next update/version of this component? If anything, CPU limits caused outages, not helped prevent them.</li><li>As for memory limits, they are slightly more useful, because CPU is compressible and memory is not, so if one pod runs berserk, it may take others down (with CPU, <code>cpu shares</code> make it as fair as possible), depending on which OOM killer strikes (a complicated topic by itself). You don&rsquo;t want the operating system OOM killer to strike as the result is unpredictable. Better, it&rsquo;s the cgroup OOM killer or even the <code>kubelet</code>&rsquo;s eviction, if the consumption is slow enough as <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#interactions-of-pod-priority-and-qos>it takes priorities into consideration</a> even. If your component is critical and a singleton (e.g. node daemon set pods), you are better off also without memory limits, because letting the pod go OOM because of artificial/wrong memory limits can mean that the node becomes unusable. Hence, such components also better run only with no or a very high memory limit, so that you can catch the occasional memory leak (bug) eventually, but under normal operation, if you cannot decide about a true upper limit, rather not have limits and cause endless outages through them or when you need the pods the most (during a zone outage) where all your assumptions went out of the window.</li></ul><p>The downside of having poor or no limits and poor and no requests is that nodes may &ldquo;die&rdquo; more often. Contrary to the expectation, even for managed services, the managed service is not responsible or cannot guarantee the health of a node under all circumstances, since the end user defines what is run on the nodes (shared responsibility). If the workload exhausts any resource, it will be the end of the node, e.g. by compressing the CPU too much (so that the <code>kubelet</code> fails to do its work), exhausting the main memory too fast, disk space, file handles, or any other resource.</p><p>The <code>kubelet</code> allows for <a href=https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources>explicit reservation of resources</a> for operating system daemons (<code>system-reserved</code>) and Kubernetes daemons (<code>kube-reserved</code>) that are subtracted from the actual node resources and become the allocatable node resources for your workload/pods. All managed services configure these settings &ldquo;by rule of thumb&rdquo; (a balancing act), but cannot guarantee that the values won&rsquo;t waste resources or always will be sufficient. You will have to fine-tune them eventually and adapt them to your needs. In addition, you can configure <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction>soft and hard eviction thresholds</a> to give the <code>kubelet</code> some headroom to evict &ldquo;greedy&rdquo; pods in a controlled way. These settings can be configured for Gardener-managed clusters like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubelet:
</span></span><span style=display:flex><span>      systemReserved:                          <span style=color:green># explicit resource reservation for operating system daemons</span>
</span></span><span style=display:flex><span>        cpu: 100m
</span></span><span style=display:flex><span>        memory: 1Gi
</span></span><span style=display:flex><span>        ephemeralStorage: 1Gi
</span></span><span style=display:flex><span>        pid: 1000
</span></span><span style=display:flex><span>      kubeReserved:                            <span style=color:green># explicit resource reservation for Kubernetes daemons</span>
</span></span><span style=display:flex><span>        cpu: 100m
</span></span><span style=display:flex><span>        memory: 1Gi
</span></span><span style=display:flex><span>        ephemeralStorage: 1Gi
</span></span><span style=display:flex><span>        pid: 1000
</span></span><span style=display:flex><span>      evictionSoft:                            <span style=color:green># soft, i.e. graceful eviction (used if the node is about to run out of resources, avoiding hard evictions)</span>
</span></span><span style=display:flex><span>        memoryAvailable: 200Mi
</span></span><span style=display:flex><span>        imageFSAvailable: 10%
</span></span><span style=display:flex><span>        imageFSInodesFree: 10%
</span></span><span style=display:flex><span>        nodeFSAvailable: 10%
</span></span><span style=display:flex><span>        nodeFSInodesFree: 10%
</span></span><span style=display:flex><span>      evictionSoftGracePeriod:                 <span style=color:green># caps pod&#39;s `terminationGracePeriodSeconds` value during soft evictions (specific grace periods)</span>
</span></span><span style=display:flex><span>        memoryAvailable: 1m30s
</span></span><span style=display:flex><span>        imageFSAvailable: 1m30s
</span></span><span style=display:flex><span>        imageFSInodesFree: 1m30s
</span></span><span style=display:flex><span>        nodeFSAvailable: 1m30s
</span></span><span style=display:flex><span>        nodeFSInodesFree: 1m30s
</span></span><span style=display:flex><span>      evictionHard:                            <span style=color:green># hard, i.e. immediate eviction (used if the node is out of resources, avoiding the OS generally run out of resources fail processes indiscriminately)</span>
</span></span><span style=display:flex><span>        memoryAvailable: 100Mi
</span></span><span style=display:flex><span>        imageFSAvailable: 5%
</span></span><span style=display:flex><span>        imageFSInodesFree: 5%
</span></span><span style=display:flex><span>        nodeFSAvailable: 5%
</span></span><span style=display:flex><span>        nodeFSInodesFree: 5%
</span></span><span style=display:flex><span>      evictionMinimumReclaim:                  <span style=color:green># additional resources to reclaim after hitting the hard eviction thresholds to not hit the same thresholds soon after again</span>
</span></span><span style=display:flex><span>        memoryAvailable: 0Mi
</span></span><span style=display:flex><span>        imageFSAvailable: 0Mi
</span></span><span style=display:flex><span>        imageFSInodesFree: 0Mi
</span></span><span style=display:flex><span>        nodeFSAvailable: 0Mi
</span></span><span style=display:flex><span>        nodeFSInodesFree: 0Mi
</span></span><span style=display:flex><span>      evictionMaxPodGracePeriod: 90            <span style=color:green># caps pod&#39;s `terminationGracePeriodSeconds` value during soft evictions (general grace periods)</span>
</span></span><span style=display:flex><span>      evictionPressureTransitionPeriod: 5m0s   <span style=color:green># stabilization time window to avoid flapping of node eviction state</span>
</span></span></code></pre></div><p>You can tweak these settings also individually per worker pool (<code>spec.provider.workers.kubernetes.kubelet...</code>), which makes sense especially with different machine types (and also workload that you may want to schedule there).</p><p>Physical memory is not compressible, but you can overcome this issue to some degree (alpha since Kubernetes <code>v1.22</code> in combination with the feature gate <code>NodeSwap</code> on the <code>kubelet</code>) with swap memory. You can read more in this <a href=https://kubernetes.io/blog/2021/08/09/run-nodes-with-swap-alpha>introductory blog</a> and the <a href=https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory>docs</a>. If you chose to use it (still only alpha at the time of this writing) you may want to consider also the risks associated with swap memory:</p><ul><li>Reduced performance predictability</li><li>Reduced performance up to page trashing</li><li>Reduced security as secrets, normally held only in memory, could be swapped out to disk</li></ul><p>That said, the various options mentioned above are only remotely related to HA and will not be further explored throughout this document, but just to remind you: if a zone goes down, load patterns will shift, existing pods will probably receive more load and will require more resources (especially because it is often practically impossible to set &ldquo;proper&rdquo; resource requests, which drive node allocation - limits are always ignored by the scheduler) or more pods will/must be placed on the existing and/or new nodes and then these settings, which are generally critical (especially if you switch on <a href=/docs/gardener/usage/shoot_scheduling_profiles/>bin-packing for Gardener-managed clusters</a> as a cost saving measure), will become even more critical during a zone outage.</p><h2 id=probes>Probes</h2><p>Before we go down the rabbit hole even further and talk about how to spread your replicas, we need to talk about probes first, as they will become relevant later. Kubernetes supports three kinds of probes: <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes>startup, liveness, and readiness probes</a>. If you are a <a href=https://twitter.com/thockin/status/1615468485987143682>visual thinker</a>, also check out this <a href=https://speakerdeck.com/thockin/kubernetes-pod-probes>slide deck</a> by <a href=https://www.linkedin.com/in/tim-hockin-6501072>Tim Hockin</a> (Kubernetes networking SIG chair).</p><p>Basically, the <code>startupProbe</code> and the <code>livenessProbe</code> help you restart the container, if it&rsquo;s unhealthy for whatever reason, by letting the <code>kubelet</code> that orchestrates your containers on a node know, that it&rsquo;s unhealthy. The former is a special case of the latter and only applied at the startup of your container, if you need to handle the startup phase differently (e.g. with very slow starting containers) from the rest of the lifetime of the container.</p><p>Now, the <code>readinessProbe</code> helps you manage the ready status of your container and thereby pod (any container that is not ready turns the pod not ready). This again has impact on endpoints and pod disruption budgets:</p><ul><li>If the pod is not ready, the endpoint will be removed and the pod will not receive traffic anymore</li><li>If the pod is not ready, the pod counts into the pod disruption budget and if the budget is exceeded, no further voluntary pod disruptions will be permitted for the remaining ready pods (e.g. no eviction, no voluntary horizontal or vertical scaling, if the pod runs on a node that is about to be drained or in draining, draining will be paused until the max drain timeout passes)</li></ul><p>As you can see, all of these probes are (also) related to HA (mostly the <code>readinessProbe</code>, but depending on your workload, you can also leverage <code>livenessProbe</code> and <code>startupProbe</code> into your HA strategy). If Kubernetes doesn&rsquo;t know about the individual status of your container/pod, it won&rsquo;t do anything for you (right away). That said, later/indirectly something might/will happen via the node status that can also be ready or not ready, which influences the pods and load balancer listener registration (a not ready node will not receive cluster traffic anymore), but this process is worker pool global and reacts delayed and also doesn&rsquo;t discriminate between the containers/pods on a node.</p><p>In addition, Kubernetes also offers <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-readiness-gate>pod readiness gates</a> to amend your pod readiness with additional custom conditions (normally, only the sum of the container readiness matters, but pod readiness gates additionally count into the overall pod readiness). This may be useful if you want to block (by means of pod disruption budgets that we will talk about next) the roll-out of your workload/nodes in case some (possibly external) condition fails.</p><h2 id=pod-disruption-budgets>Pod Disruption Budgets</h2><p>One of the most important resources that help you on your way to HA are <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb>pod disruption budgets</a> or PDB for short. They tell Kubernetes how to deal with voluntary pod disruptions, e.g. during the deployment of your workload, when the nodes are rolled, or just in general when a pod shall be evicted/terminated. Basically, if the budget is reached, they block all voluntary pod disruptions (at least for a while until possibly other timeouts act or things happen that leave Kubernetes no choice anymore, e.g. the node is forcefully terminated). You should always define them for your workload.</p><p>Very important to note is that they are based on the <code>readinessProbe</code>, i.e. even if all of your replicas are <code>lively</code>, but not enough of them are <code>ready</code>, this blocks voluntary pod disruptions, so they are very critical and useful. Here an example (you can specify either <code>minAvailable</code> or <code>maxUnavailable</code> in absolute numbers or as percentage):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: policy/v1
</span></span><span style=display:flex><span>kind: PodDisruptionBudget
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  maxUnavailable: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      ...
</span></span></code></pre></div><p>And please do not specify a PDB of <code>maxUnavailable</code> being 0 or similar. That&rsquo;s pointless, even detrimental, as it blocks then even useful operations, forces always the hard timeouts that are less graceful and it doesn&rsquo;t make sense in the context of HA. You cannot &ldquo;force&rdquo; HA by preventing voluntary pod disruptions, you must work with the pod disruptions in a resilient way. Besides, PDBs are really only about voluntary pod disruptions - something bad can happen to a node/pod at any time and PDBs won&rsquo;t make this reality go away for you.</p><p>PDBs will not always work as expected and can also get in your way, e.g. if the PDB is violated or would be violated, it may possibly block whatever you are trying to do to salvage the situation, e.g. drain a node or deploy a patch version (if the PDB is or would be violated, not even unhealthy pods would be evicted as they could theoretically become healthy again, which Kubernetes doesn&rsquo;t know). In order to overcome this issue, it is now possible (alpha since Kubernetes <code>v1.26</code> in combination with the feature gate <code>PDBUnhealthyPodEvictionPolicy</code> on the API server) to configure the so-called <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy>unhealthy pod eviction policy</a>. The default is still <code>IfHealthyBudget</code> as a change in default would have changed the behavior (as described above), but you can now also set <code>AlwaysAllow</code> at the PDB (<code>spec.unhealthyPodEvictionPolicy</code>). For more information, please check out <a href=https://github.com/kubernetes/kubernetes/issues/72320>this discussion</a>, <a href=https://github.com/kubernetes/kubernetes/pull/105296>the PR</a> and <a href="https://groups.google.com/g/kubernetes-sig-apps/c/_joO4swogKY?pli=1">this document</a> and balance the pros and cons for yourself. In short,
the new <code>AlwaysAllow</code> option is probably the better choice in most of the cases while <code>IfHealthyBudget</code> is useful only if you have frequent temporary transitions or for special cases where you have already implemented controllers that depend on the old behavior.</p><h2 id=pod-topology-spread-constraints>Pod Topology Spread Constraints</h2><p><a href=https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints>Pod topology spread constraints</a> or PTSC for short (no official abbreviation exists, but we will use this in the following) are enormously helpful to distribute your replicas across multiple zones, nodes, or any other user-defined topology domain. They complement and improve on <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>pod (anti-)affinities</a> that still exist and can be used in combination.</p><p>PTSCs are an improvement, because they allow for <code>maxSkew</code> and <code>minDomains</code>. You can steer the &ldquo;level of tolerated imbalance&rdquo; with <code>maxSkew</code>, e.g. you probably want that to be at least 1, so that you can perform a rolling update, but this all depends on your <a href=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment>deployment</a> (<code>maxUnavailable</code> and <code>maxSurge</code>), etc. <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#rolling-updates>Stateful sets</a> are a bit different (<code>maxUnavailable</code>) as they are bound to volumes and depend on them, so there usually cannot be 2 pods requiring the same volume. <code>minDomains</code> is a hint to tell the scheduler how far to spread, e.g. if all nodes in one zone disappeared because of a zone outage, it may &ldquo;appear&rdquo; as if there are only 2 zones in a 3 zones cluster and the scheduling decisions may end up wrong, so a <code>minDomains</code> of 3 will tell the scheduler to spread to 3 zones before adding another replica in one zone. Be careful with this setting as it also means, if one zone is down the &ldquo;spread&rdquo; is already at least 1, if pods run in the other zones. This is useful where you have exactly as many replicas as you have zones and you do not want any imbalance. Imbalance is critical as if you end up with one, nobody is going to do the (active) re-balancing for you (unless you deploy and configure additional non-standard components such as the <a href=https://github.com/kubernetes-sigs/descheduler>descheduler</a>). So, for instance, if you have something like a DBMS that you want to spread across 2 zones (active-passive) or 3 zones (consensus-based), you better specify <code>minDomains</code> of 2 respectively 3 to force your replicas into at least that many zones before adding more replicas to another zone (if supported).</p><p>Anyway, PTSCs are critical to have, but not perfect, so we saw (unsurprisingly, because that&rsquo;s how the scheduler works), that the scheduler may block the deployment of new pods because it takes the decision pod-by-pod (see for instance <a href=https://github.com/kubernetes/kubernetes/issues/109364>#109364</a>).</p><h2 id=pod-affinities-and-anti-affinities>Pod Affinities and Anti-Affinities</h2><p>As said, you can combine PTSCs with <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>pod affinities and/or anti-affinities</a>. Especially <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity>inter-pod (anti-)affinities</a> may be helpful to place pods <em>apart</em>, e.g. because they are fall-backs for each other or you do not want multiple potentially resource-hungry <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#besteffort>&ldquo;best-effort&rdquo;</a> or <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#burstable>&ldquo;burstable&rdquo;</a> pods side-by-side (noisy neighbor problem), or <em>together</em>, e.g. because they form a unit and you want to reduce the failure domain, reduce the network latency, and reduce the costs.</p><h2 id=topology-aware-hints>Topology Aware Hints</h2><p>While <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints>topology aware hints</a> are not directly related to HA, they are very relevant in the HA context. Spreading your workload across multiple zones may increase network latency and cost significantly, if the traffic is not shaped. Topology aware hints (beta since Kubernetes <code>v1.23</code>, replacing the now deprecated topology aware traffic routing with topology keys) help to route the traffic within the originating zone, if possible. Basically, they tell <code>kube-proxy</code> how to setup your routing information, so that clients can talk to endpoints that are located within the same zone.</p><p>Be aware however, that there are some limitations. Those are called <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints/#safeguards>safeguards</a> and if they strike, the hints are off and traffic is routed again randomly. Especially controversial is the balancing limitation as there is the assumption, that the load that hits an endpoint is determined by the allocatable CPUs in that topology zone, but that&rsquo;s not always, if even often, the case (see for instance <a href=https://github.com/kubernetes/kubernetes/issues/113731>#113731</a> and <a href=https://github.com/kubernetes/kubernetes/issues/110714>#110714</a>). So, this limitation hits far too often and your hints are off, but then again, it&rsquo;s about network latency and cost optimization first, so it&rsquo;s better than nothing.</p><h2 id=networking>Networking</h2><p>We have talked about networking only to some small degree so far (<code>readiness</code> probes, pod disruption budgets, topology aware hints). The most important component is probably your ingress load balancer - everything else is managed by Kubernetes. AWS, Azure, GCP, and also OpenStack offer multi-zonal load balancers, so make use of them. In Azure and GCP, LBs are regional whereas in AWS and OpenStack, they need to be bound to a zone, which the cloud-controller-manager does by observing the zone labels at the nodes (please note that this behavior is not always working as expected, see <a href=https://github.com/kubernetes/cloud-provider-aws/issues/569>#570</a> where the AWS cloud-controller-manager is not readjusting to newly observed zones).</p><p>Please be reminded that even if you use a service mesh like <a href=https://istio.io>Istio</a>, the off-the-shelf installation/configuration usually never comes with productive settings (to simplify first-time installation and improve first-time user experience) and you will have to fine-tune your installation/configuration, much like the rest of your workload.</p><h2 id=relevant-cluster-settings>Relevant Cluster Settings</h2><p>Following now a summary/list of the more relevant settings you may like to tune for Gardener-managed clusters:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  controlPlane:
</span></span><span style=display:flex><span>    highAvailability:
</span></span><span style=display:flex><span>      failureTolerance:
</span></span><span style=display:flex><span>        type: zone <span style=color:green># valid values are `node` and `zone` (only available if your control plane resides in a region with 3+ zones)</span>
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeAPIServer:
</span></span><span style=display:flex><span>      defaultNotReadyTolerationSeconds: 300
</span></span><span style=display:flex><span>      defaultUnreachableTolerationSeconds: 300
</span></span><span style=display:flex><span>    kubelet:
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>    kubeScheduler:
</span></span><span style=display:flex><span>      featureGates:
</span></span><span style=display:flex><span>        MinDomainsInPodTopologySpread: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    kubeControllerManager:
</span></span><span style=display:flex><span>      nodeMonitorPeriod: 10s
</span></span><span style=display:flex><span>      nodeMonitorGracePeriod: 40s
</span></span><span style=display:flex><span>      horizontalPodAutoscaler:
</span></span><span style=display:flex><span>        syncPeriod: 15s
</span></span><span style=display:flex><span>        tolerance: 0.1
</span></span><span style=display:flex><span>        downscaleStabilization: 5m0s
</span></span><span style=display:flex><span>        initialReadinessDelay: 30s
</span></span><span style=display:flex><span>        cpuInitializationPeriod: 5m0s
</span></span><span style=display:flex><span>    verticalPodAutoscaler:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      evictAfterOOMThreshold: 10m0s
</span></span><span style=display:flex><span>      evictionRateBurst: 1
</span></span><span style=display:flex><span>      evictionRateLimit: -1
</span></span><span style=display:flex><span>      evictionTolerance: 0.5
</span></span><span style=display:flex><span>      recommendationMarginFraction: 0.15
</span></span><span style=display:flex><span>      updaterInterval: 1m0s
</span></span><span style=display:flex><span>      recommenderInterval: 1m0s
</span></span><span style=display:flex><span>    clusterAutoscaler:
</span></span><span style=display:flex><span>      expander: <span style=color:#a31515>&#34;least-waste&#34;</span>
</span></span><span style=display:flex><span>      scanInterval: 10s
</span></span><span style=display:flex><span>      scaleDownDelayAfterAdd: 60m
</span></span><span style=display:flex><span>      scaleDownDelayAfterDelete: 0s
</span></span><span style=display:flex><span>      scaleDownDelayAfterFailure: 3m
</span></span><span style=display:flex><span>      scaleDownUnneededTime: 30m
</span></span><span style=display:flex><span>      scaleDownUtilizationThreshold: 0.5
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: ...
</span></span><span style=display:flex><span>      minimum: 6
</span></span><span style=display:flex><span>      maximum: 60
</span></span><span style=display:flex><span>      maxSurge: 3
</span></span><span style=display:flex><span>      maxUnavailable: 0
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - ... <span style=color:green># list of zones you want your worker pool nodes to be spread across, see above</span>
</span></span><span style=display:flex><span>      kubernetes:
</span></span><span style=display:flex><span>        kubelet:
</span></span><span style=display:flex><span>          ... <span style=color:green># similar to `kubelet` above (cluster-wide settings), but here per worker pool (pool-specific settings), see above</span>
</span></span><span style=display:flex><span>      machineControllerManager: <span style=color:green># optional, it allows to configure the machine-controller settings.</span>
</span></span><span style=display:flex><span>        machineCreationTimeout: 20m
</span></span><span style=display:flex><span>        machineHealthTimeout: 10m
</span></span><span style=display:flex><span>        machineDrainTimeout: 60h
</span></span><span style=display:flex><span>  systemComponents:
</span></span><span style=display:flex><span>    coreDNS:
</span></span><span style=display:flex><span>      autoscaling:
</span></span><span style=display:flex><span>        mode: horizontal <span style=color:green># valid values are `horizontal` (driven by CPU load) and `cluster-proportional` (driven by number of nodes/cores)</span>
</span></span></code></pre></div><h4 id=on-speccontrolplanehighavailabilityfailuretolerancetype>On <code>spec.controlPlane.highAvailability.failureTolerance.type</code></h4><p>If set, determines the degree of failure tolerance for your control plane. <code>zone</code> is preferred, but only available if your control plane resides in a region with 3+ zones. See <a href=#control-plane>above</a> and the <a href=/docs/gardener/usage/shoot_high_availability/>docs</a>.</p><h4 id=on-speckuberneteskubeapiserverdefaultunreachabletolerationseconds-and-defaultnotreadytolerationseconds>On <code>spec.kubernetes.kubeAPIServer.defaultUnreachableTolerationSeconds</code> and <code>defaultNotReadyTolerationSeconds</code></h4><p>This is a very interesting <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver>API server setting</a> that lets Kubernetes decide how fast to evict pods from nodes whose status condition of type <code>Ready</code> is either <code>Unknown</code> (node status unknown, a.k.a unreachable) or <code>False</code> (<code>kubelet</code> not ready) (see <a href=https://kubernetes.io/docs/concepts/architecture/nodes/#condition>node status conditions</a>; please note that <code>kubectl</code> shows both values as <code>NotReady</code> which is a somewhat &ldquo;simplified&rdquo; visualization).</p><p>You can also override the cluster-wide API server settings <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions>individually per pod</a>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  tolerations:
</span></span><span style=display:flex><span>  - key: <span style=color:#a31515>&#34;node.kubernetes.io/unreachable&#34;</span>
</span></span><span style=display:flex><span>    operator: <span style=color:#a31515>&#34;Exists&#34;</span>
</span></span><span style=display:flex><span>    effect: <span style=color:#a31515>&#34;NoExecute&#34;</span>
</span></span><span style=display:flex><span>    tolerationSeconds: 0
</span></span><span style=display:flex><span>  - key: <span style=color:#a31515>&#34;node.kubernetes.io/not-ready&#34;</span>
</span></span><span style=display:flex><span>    operator: <span style=color:#a31515>&#34;Exists&#34;</span>
</span></span><span style=display:flex><span>    effect: <span style=color:#a31515>&#34;NoExecute&#34;</span>
</span></span><span style=display:flex><span>    tolerationSeconds: 0
</span></span></code></pre></div><p>This will evict pods on unreachable or not-ready nodes immediately, but be cautious: <code>0</code> is very aggressive and may lead to unnecessary disruptions. Again, you must decide for your own workload and balance out the pros and cons (e.g. long startup time).</p><p>Please note, these settings replace <code>spec.kubernetes.kubeControllerManager.podEvictionTimeout</code> that was deprecated with Kubernetes <code>v1.26</code> (and acted as an upper bound).</p><h4 id=on-speckuberneteskubeschedulerfeaturegatesmindomainsinpodtopologyspread>On <code>spec.kubernetes.kubeScheduler.featureGates.MinDomainsInPodTopologySpread</code></h4><p>Required to be enabled for <code>minDomains</code> to work with PTSCs (beta since Kubernetes <code>v1.25</code>, but off by default). See <a href=#pod-topology-spread-constraints>above</a> and the <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#topologyspreadconstraints-field>docs</a>. This tells the scheduler, how many topology domains to expect (=zones in the context of this document).</p><h4 id=on-speckuberneteskubecontrollermanagernodemonitorperiod-and-nodemonitorgraceperiod>On <code>spec.kubernetes.kubeControllerManager.nodeMonitorPeriod</code> and <code>nodeMonitorGracePeriod</code></h4><p>This is another very interesting <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager>kube-controller-manager setting</a> that can help you speed up or slow down how fast a node shall be considered <code>Unknown</code> (node status unknown, a.k.a unreachable) when the <code>kubelet</code> is not updating its status anymore (see <a href=https://kubernetes.io/docs/concepts/architecture/nodes/#condition>node status conditions</a>), which effects eviction (see <code>spec.kubernetes.kubeAPIServer.defaultUnreachableTolerationSeconds</code> and <code>defaultNotReadyTolerationSeconds</code> above). The shorter the time window, the faster Kubernetes will act, but the higher the chance of flapping behavior and pod trashing, so you may want to balance that out according to your needs, otherwise stick to the default which is a reasonable compromise.</p><h4 id=on-speckuberneteskubecontrollermanagerhorizontalpodautoscaler>On <code>spec.kubernetes.kubeControllerManager.horizontalPodAutoscaler...</code></h4><p>This configures horizontal pod autoscaling in Gardener-managed clusters. See <a href=#replicas-horizontal-scaling>above</a> and the <a href=https://kubernetes.io/de/docs/tasks/run-application/horizontal-pod-autoscale>docs</a> for the detailed fields.</p><h4 id=on-speckubernetesverticalpodautoscaler>On <code>spec.kubernetes.verticalPodAutoscaler...</code></h4><p>This configures vertical pod autoscaling in Gardener-managed clusters. See <a href=#resources-vertical-scaling>above</a> and the <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/FAQ.md>docs</a> for the detailed fields.</p><h4 id=on-speckubernetesclusterautoscaler>On <code>spec.kubernetes.clusterAutoscaler...</code></h4><p>This configures node auto-scaling in Gardener-managed clusters. See <a href=#worker-pools>above</a> and the <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md>docs</a> for the detailed fields, especially about <a href=https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders>expanders</a>, which may become life-saving in case of a zone outage when a resource crunch is setting in and everybody rushes to get machines in the healthy zones.</p><p>In case of a zone outage, it may be interesting to understand how the cluster autoscaler will put a worker pool in one zone into &ldquo;back-off&rdquo;. Unfortunately, the official cluster autoscaler documentation does not explain these details, but you can find hints in the <a href=https://github.com/kubernetes/autoscaler/blob/b94f340af58eb063df9ebfcd65835f9a499a69a2/cluster-autoscaler/config/autoscaling_options.go#L214-L219>source code</a>:</p><p>If a node fails to come up, the node group (worker pool in that zone) will go into &ldquo;back-off&rdquo;, at first 5m, then <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/utils/backoff/exponential_backoff.go#L77-L82>exponentially longer</a> until the maximum of 30m is reached. The &ldquo;back-off&rdquo; is reset after 3 hours. This in turn means, that nodes must be first considered <code>Unknown</code>, which happens when <code>spec.kubernetes.kubeControllerManager.nodeMonitorPeriod.nodeMonitorGracePeriod</code> lapses. Then they must either remain in this state until <code>spec.provider.workers.machineControllerManager.machineHealthTimeout</code> lapses for them to be recreated, which will fail in the unhealthy zone, or <code>spec.kubernetes.kubeAPIServer.defaultUnreachableTolerationSeconds</code> lapses for the pods to be evicted (usually faster than node replacements, depending on your configuration), which will trigger the cluster autoscaler to create more capacity, but very likely in the same zone as it tries to balance its node groups at first, which will also fail in the unhealthy zone. It will be considered failed only when <code>maxNodeProvisionTime</code> lapses (usually close to <code>spec.provider.workers.machineControllerManager.machineCreationTimeout</code>) and only then put the node group into &ldquo;back-off&rdquo; and not retry for 5m at first and then exponentially longer. It&rsquo;s critical to keep that in mind and accommodate for it. If you have already capacity up and running, the reaction time is usually much faster with leases (whatever you set) or endpoints (<code>spec.kubernetes.kubeControllerManager.nodeMonitorPeriod.nodeMonitorGracePeriod</code>), but if you depend on new/fresh capacity, the above should inform you how long you will have to wait for it.</p><h4 id=on-specproviderworkersminimum-maximum-maxsurge-maxunavailable-zones-and-machinecontrollermanager>On <code>spec.provider.workers.minimum</code>, <code>maximum</code>, <code>maxSurge</code>, <code>maxUnavailable</code>, <code>zones</code>, and <code>machineControllerManager</code></h4><p>Each worker pool in Gardener may be configured differently. Among many other settings like machine type, root disk, Kubernetes version, <code>kubelet</code> settings, and many more you can also specify the lower and upper bound for the number of machines (<code>minimum</code> and <code>maximum</code>), how many machines may be added additionally during a rolling update (<code>maxSurge</code>) and how many machines may be in termination/recreation during a rolling update (<code>maxUnavailable</code>), and of course across how many zones the nodes shall be spread (<code>zones</code>).</p><p>Interesting is also the configuration for Gardener&rsquo;s machine-controller-manager or MCM for short that provisions, monitors, terminates, replaces, or updates machines that back your nodes:</p><ul><li>The shorter <code>machineCreationTimeout</code> is, the faster MCM will retry to create a machine/node, if the process is stuck on cloud provider side. It is set to useful/practical timeouts for the different cloud providers and you probably don&rsquo;t want to change those (in the context of HA at least). Please align with the cluster autoscaler&rsquo;s <code>maxNodeProvisionTime</code>.</li><li>The shorter <code>machineHealthTimeout</code> is, the faster MCM will replace machines/nodes in case the kubelet isn&rsquo;t reporting back, which translates to <code>Unknown</code>, or reports back with <code>NotReady</code>, or the <a href=https://github.com/kubernetes/node-problem-detector>node-problem-detector</a> that Gardener deploys for you reports a non-recoverable issue/condition (e.g. read-only file system). If it is too short however, you risk node and pod trashing, so be careful.</li><li>The shorter <code>machineDrainTimeout</code> is, the faster you can get rid of machines/nodes that MCM decided to remove, but this puts a cap on the grace periods and PDBs. They are respected up until the drain timeout lapses - then the machine/node will be forcefully terminated, whether or not the pods are still in termination or not even terminated because of PDBs. Those PDBs will then be violated, so be careful here as well. Please align with the cluster autoscaler&rsquo;s <code>maxGracefulTerminationSeconds</code>.</li></ul><p>Especially the last two settings may help you recover faster from cloud provider issues.</p><h4 id=on-specsystemcomponentscorednsautoscaling>On <code>spec.systemComponents.coreDNS.autoscaling</code></h4><p>DNS is critical, in general and also within a Kubernetes cluster. Gardener-managed clusters deploy <a href=https://coredns.io>CoreDNS</a>, a graduated CNCF project. Gardener supports 2 auto-scaling modes for it, <code>horizontal</code> (using HPA based on CPU) and <code>cluster-proportional</code> (using <a href=https://github.com/kubernetes-sigs/cluster-proportional-autoscaler>cluster proportional autoscaler</a> that scales the number of pods based on the number of nodes/cores, not to be confused with the cluster autoscaler that scales nodes based on their utilization). Check out the <a href=/docs/gardener/usage/dns-autoscaling/>docs</a>, especially the <a href=/docs/gardener/usage/dns-autoscaling/#trade-offs-of-horizontal-and-cluster-proportional-dns-autoscaling>trade-offs</a> why you would chose one over the other (<code>cluster-proportional</code> gives you more configuration options, if CPU-based horizontal scaling is insufficient to your needs). Consider also Gardener&rsquo;s feature <a href=/docs/gardener/usage/node-local-dns/>node-local DNS</a> to decouple you further from the DNS pods and stabilize DNS. Again, that&rsquo;s not strictly related to HA, but may become important during a zone outage, when load patterns shift and pods start to initialize/resolve DNS records more frequently in bulk.</p><h2 id=more-caveats>More Caveats</h2><p>Unfortunately, there are a few more things of note when it comes to HA in a Kubernetes cluster that may be &ldquo;surprising&rdquo; and hard to mitigate:</p><ul><li>If the <code>kubelet</code> restarts, it will report all pods as <code>NotReady</code> on startup until it reruns its probes (<a href=https://github.com/kubernetes/kubernetes/issues/100277>#100277</a>), which leads to temporary endpoint and load balancer target removal (<a href=https://github.com/kubernetes/kubernetes/issues/102367>#102367</a>). This topic is somewhat controversial. Gardener uses rolling updates and a jitter to spread necessary <code>kubelet</code> restarts as good as possible.</li><li>If a <code>kube-proxy</code> pod on a node turns <code>NotReady</code>, all load balancer traffic to all pods (on this node) under services with <code>externalTrafficPolicy</code> <code>local</code> will cease as the load balancer will then take this node out of serving. This topic is somewhat controversial as well. So, please remember that <code>externalTrafficPolicy</code> <code>local</code> not only has the disadvantage of imbalanced traffic spreading, but also a dependency to the kube-proxy pod that may and will be unavailable during updates. Gardener uses rolling updates to spread necessary <code>kube-proxy</code> updates as good as possible.</li></ul><p>These are just a few additional considerations. They may or may not affect you, but other intricacies may. It&rsquo;s a reminder to be watchful as Kubernetes may have one or two relevant quirks that you need to consider (and will probably only find out over time and with extensive testing).</p><h2 id=meaningful-availability>Meaningful Availability</h2><p>Finally, let&rsquo;s go back to where we started. We recommended to measure <a href=https://research.google/pubs/pub50828>meaningful availability</a>. For instance, in Gardener, we do not trust only internal signals, but track also whether Gardener or the control planes that it manages are externally available through the external DNS records and load balancers, SNI-routing Istio gateways, etc. (the same path all users must take). It&rsquo;s a huge difference whether the API server&rsquo;s internal readiness probe passes or the user can actually reach the API server and it does what it&rsquo;s supposed to do. Most likely, you will be in a similar spot and can do the same.</p><p>What you do with these signals is another matter. Maybe there are some actionable metrics and you can trigger some active fail-over, maybe you can only use it to improve your HA setup altogether. In our case, we also use it to deploy mitigations, e.g. via our <a href=https://github.com/gardener/dependency-watchdog>dependency-watchdog</a> that watches, for instance, Gardener-managed API servers and shuts down components like the controller managers to avert cascading knock-off effects (e.g. melt-down if the <code>kubelets</code> cannot reach the API server, but the controller managers can and start taking down nodes and pods).</p><p>Either way, understanding how users perceive your service is key to the improvement process as a whole. Even if you are not struck by a zone outage, the measures above and tracking the meaningful availability will help you improve your service.</p><p>Thank you for your interest.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-6e28d817b7a970b189d7b89840fa8925>35 - Shoot Info Configmap</h1><h1 id=shoot-info-configmap>Shoot Info <code>ConfigMap</code></h1><h2 id=overview>Overview</h2><p>The gardenlet maintains a <a href=https://kubernetes.io/docs/concepts/configuration/configmap/>ConfigMap</a> inside the Shoot cluster that contains information about the cluster itself. The ConfigMap is named <code>shoot-info</code> and located in the <code>kube-system</code> namespace.</p><h2 id=fields>Fields</h2><p>The following fields are provided:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: shoot-info
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  domain: crazy-botany.core.my-custom-domain.com     <span style=color:green># .spec.dns.domain field from the Shoot resource</span>
</span></span><span style=display:flex><span>  extensions: foobar,foobaz                          <span style=color:green># List of extensions that are enabled</span>
</span></span><span style=display:flex><span>  kubernetesVersion: 1.20.1                          <span style=color:green># .spec.kubernetes.version field from the Shoot resource</span>
</span></span><span style=display:flex><span>  maintenanceBegin: 220000+0100                      <span style=color:green># .spec.maintenance.timeWindow.begin field from the Shoot resource</span>
</span></span><span style=display:flex><span>  maintenanceEnd: 230000+0100                        <span style=color:green># .spec.maintenance.timeWindow.end field from the Shoot resource</span>
</span></span><span style=display:flex><span>  nodeNetwork: 10.250.0.0/16                         <span style=color:green># .spec.networking.nodes field from the Shoot resource</span>
</span></span><span style=display:flex><span>  podNetwork: 100.96.0.0/11                          <span style=color:green># .spec.networking.pods field from the Shoot resource</span>
</span></span><span style=display:flex><span>  projectName: dev                                   <span style=color:green># .metadata.name of the Project</span>
</span></span><span style=display:flex><span>  provider: &lt;some-provider-name&gt;                     <span style=color:green># .spec.provider.type field from the Shoot resource</span>
</span></span><span style=display:flex><span>  region: europe-central-1                           <span style=color:green># .spec.region field from the Shoot resource</span>
</span></span><span style=display:flex><span>  serviceNetwork: 100.64.0.0/13                      <span style=color:green># .spec.networking.services field from the Shoot resource</span>
</span></span><span style=display:flex><span>  shootName: crazy-botany                            <span style=color:green># .metadata.name from the Shoot resource</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-530324ec62b83dd4fb4dbc1c64e21ea4>36 - Shoot Maintenance</h1><h1 id=shoot-maintenance>Shoot Maintenance</h1><p>Shoots configure a maintenance time window in which Gardener performs certain operations that may restart the control plane, roll out the nodes, result in higher network traffic, etc. A summary of what was changed in the last maintenance time window in shoot specification is kept in the shoot status <code>.status.lastMaintenance</code> field.</p><p>This document outlines what happens during a shoot maintenance.</p><h2 id=time-window>Time Window</h2><p>Via the <code>.spec.maintenance.timeWindow</code> field in the shoot specification, end-users can configure the time window in which maintenance operations are executed.
Gardener runs one maintenance operation per day in this time window:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    timeWindow:
</span></span><span style=display:flex><span>      begin: 220000+0100
</span></span><span style=display:flex><span>      end: 230000+0100
</span></span></code></pre></div><p>The offset (<code>+0100</code>) is considered with respect to UTC time.
The minimum time window is <code>30m</code> and the maximum is <code>6h</code>.</p><p>⚠️ Please note that there is no guarantee that a maintenance operation that, e.g., starts a node roll-out will finish <em>within</em> the time window.
Especially for large clusters, it may take several hours until a graceful rolling update of the worker nodes succeeds (also depending on the workload and the configured pod disruption budgets/termination grace periods).</p><p>Internally, Gardener is subtracting <code>15m</code> from the end of the time window to (best-effort) try to finish the maintenance until the end is reached, however, this might not work in all cases.</p><p>If you don&rsquo;t specify a time window, then Gardener will randomly compute it.
You can change it later, of course.</p><h2 id=automatic-version-updates>Automatic Version Updates</h2><p>The <code>.spec.maintenance.autoUpdate</code> field in the shoot specification allows you to control how/whether automatic updates of Kubernetes patch and machine image versions are performed.
Machine image versions are updated per worker pool.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>During the daily maintenance, the Gardener Controller Manager updates the Shoot&rsquo;s Kubernetes and machine image version if any of the following criteria applies:</p><ul><li>There is a higher version available and the Shoot opted-in for automatic version updates.</li><li>The currently used version is <code>expired</code>.</li></ul><p>Gardener creates events with the type <code>MaintenanceDone</code> on the Shoot describing the action performed during maintenance, including the reason why an update has been triggered.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>MaintenanceDone  Updated image of worker-pool &#39;coreos-xy&#39; from &#39;coreos&#39; version &#39;xy&#39; to version &#39;abc&#39;. Reason: AutoUpdate of MachineImage configured.
</span></span><span style=display:flex><span>MaintenanceDone  Updated Kubernetes version &#39;0.0.1&#39; to version &#39;0.0.5&#39;. This is an increase in the patch level. Reason: AutoUpdate of Kubernetes version configured.
</span></span><span style=display:flex><span>MaintenanceDone  Updated Kubernetes version &#39;0.0.5&#39; to version &#39;0.1.5&#39;. This is an increase in the minor level. Reason: Kubernetes version expired - force update required.
</span></span></code></pre></div><p>Please refer to the <a href=/docs/gardener/usage/shoot_versions/>Shoot Kubernetes and Operating System Versioning in Gardener</a> topic for more information about Kubernetes and machine image versions in Gardener.</p><h2 id=cluster-reconciliation>Cluster Reconciliation</h2><p>Gardener administrators/operators can configure the gardenlet in a way that it only reconciles shoot clusters during their maintenance time windows.
This behaviour is not controllable by end-users but might make sense for large Gardener installations.
Concretely, your shoot will be reconciled regularly during its maintenance time window.
Outside of the maintenance time window it will only reconcile if you change the specification or if you explicitly trigger it, see also <a href=/docs/gardener/usage/shoot_operations/>Trigger Shoot Operations</a>.</p><h2 id=confine-specification-changesupdates-roll-out>Confine Specification Changes/Updates Roll Out</h2><p>Via the <code>.spec.maintenance.confineSpecUpdateRollout</code> field you can control whether you want to make Gardener roll out changes/updates to your shoot specification only during the maintenance time window.
It is <code>false</code> by default, i.e., any change to your shoot specification triggers a reconciliation (even outside of the maintenance time window).
This is helpful if you want to update your shoot but don&rsquo;t want the changes to be applied immediately. One example use-case would be a Kubernetes version upgrade that you want to roll out during the maintenance time window.
Any update to the specification will not increase the <code>.metadata.generation</code> of the <code>Shoot</code>, which is something you should be aware of.
Also, even if Gardener administrators/operators have not enabled the &ldquo;reconciliation in maintenance time window only&rdquo; configuration (as mentioned above), then your shoot will only reconcile in the maintenance time window.
The reason is that Gardener cannot differentiate between create/update/reconcile operations.</p><p>⚠️ If <code>confineSpecUpdateRollout=true</code>, please note that if you change the maintenance time window itself, then it will only be effective after the upcoming maintenance.</p><p>⚠️ As exceptions to the above rules, <a href=/docs/gardener/usage/shoot_operations/#immediate-reconciliation>manually triggered reconciliations</a> and changes to the <code>.spec.hibernation.enabled</code> field trigger immediate rollouts.
I.e., if you hibernate or wake-up your shoot, or you explicitly tell Gardener to reconcile your shoot, then Gardener gets active right away.</p><h2 id=shoot-operations>Shoot Operations</h2><p>In case you would like to perform a <a href=/docs/gardener/usage/shoot_operations/#credentials-rotation-operations>shoot credential rotation</a> or a <code>reconcile</code> operation during your maintenance time window, you can annotate the <code>Shoot</code> with</p><pre tabindex=0><code>maintenance.gardener.cloud/operation=&lt;operation&gt;
</code></pre><p>This will execute the specified <code>&lt;operation></code> during the next maintenance reconciliation.
Note that Gardener will remove this annotation after it has been performed in the maintenance reconciliation.</p><blockquote><p>⚠️ This is skipped when the <code>Shoot</code>&rsquo;s <code>.status.lastOperation.state=Failed</code>. Make sure to <a href=/docs/gardener/usage/shoot_operations/#retry-failed-reconciliation>retry</a> your shoot reconciliation beforehand.</p></blockquote><h2 id=special-operations-during-maintenance>Special Operations During Maintenance</h2><p>The shoot maintenance controller triggers special operations that are performed as part of the shoot reconciliation.</p><h3 id=infrastructure-and-dnsrecord-reconciliation><code>Infrastructure</code> and <code>DNSRecord</code> Reconciliation</h3><p>The reconciliation of the <code>Infrastructure</code> and <code>DNSRecord</code> extension resources is only demanded during the shoot&rsquo;s maintenance time window.
The rationale behind it is to prevent sending too many requests against the cloud provider APIs, especially on large landscapes or if a user has many shoot clusters in the same cloud provider account.</p><h3 id=restart-control-plane-controllers>Restart Control Plane Controllers</h3><p>Gardener operators can make Gardener restart/delete certain control plane pods during a shoot maintenance.
This feature helps to automatically solve service denials of controllers due to stale caches, dead-locks or starving routines.</p><p>Please note that these are exceptional cases but they are observed from time to time.
Gardener, for example, takes this precautionary measure for <code>kube-controller-manager</code> pods.</p><p>See <a href=/docs/gardener/extensions/shoot-maintenance/>Shoot Maintenance</a> to see how extension developers can extend this behaviour.</p><h3 id=restart-some-core-addons>Restart Some Core Addons</h3><p>Gardener operators can make Gardener restart some core addons (at the moment only CoreDNS) during a shoot maintenance.</p><p>CoreDNS benefits from this feature as it automatically solve problems with clients stuck to single replica of the deployment and thus overloading it.
Please note that these are exceptional cases but they are observed from time to time.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ee4728cf7f71a3c0ae90d16a7c9cf79a>37 - Shoot Networking</h1><h1 id=shoot-networking>Shoot Networking</h1><p>This document contains network related information for Shoot clusters.</p><h2 id=pod-network>Pod Network</h2><p>A Pod network is imperative for any kind of cluster communication with Pods not started within the Node&rsquo;s host network.
More information about the Kubernetes network model can be found in the <a href=https://kubernetes.io/docs/concepts/cluster-administration/networking/>Cluster Networking</a> topic.</p><p>Gardener allows users to configure the Pod network&rsquo;s CIDR during Shoot creation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: &lt;some-network-extension-name&gt; <span style=color:green># {calico,cilium}</span>
</span></span><span style=display:flex><span>    pods: 100.96.0.0/16
</span></span><span style=display:flex><span>    nodes: ...
</span></span><span style=display:flex><span>    services: ...
</span></span></code></pre></div><blockquote><p>⚠️ The <code>networking.pods</code> IP configuration is immutable and cannot be changed afterwards.
Please consider the following paragraph to choose a configuration which will meet your demands.</p></blockquote><p>One of the network plugin&rsquo;s (CNI) tasks is to assign IP addresses to Pods started in the Pod network.
Different network plugins come with different IP address management (IPAM) features, so we can&rsquo;t give any definite advice how IP ranges should be configured.
Nevertheless, we want to outline the standard configuration.</p><p>Information in <code>.spec.networking.pods</code> matches the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/>&ndash;cluster-cidr flag</a> of the Kube-Controller-Manager of your Shoot cluster.
This IP range is divided into smaller subnets, also called <code>podCIDRs</code> (default mask <code>/24</code>) and assigned to Node objects <code>.spec.podCIDR</code>.
Pods get their IP address from this smaller node subnet in a default IPAM setup.
Thus, it must be guaranteed that enough of these subnets can be created for the maximum amount of nodes you expect in the cluster.</p><p><em><strong>Example 1</strong></em></p><pre tabindex=0><code>Pod network: 100.96.0.0/16
nodeCIDRMaskSize: /24
-------------------------

Number of podCIDRs: 256 --&gt; max. Node count 
Number of IPs per podCIDRs: 256
</code></pre><p>With the configuration above a Shoot cluster can at most have <strong>256 nodes</strong> which are ready to run workload in the Pod network.</p><p><em><strong>Example 2</strong></em></p><pre tabindex=0><code>Pod network: 100.96.0.0/20
nodeCIDRMaskSize: /24
-------------------------

Number of podCIDRs: 16 --&gt; max. Node count 
Number of IPs per podCIDRs: 256
</code></pre><p>With the configuration above a Shoot cluster can at most have <strong>16 nodes</strong> which are ready to run workload in the Pod network.</p><p>Beside the configuration in <code>.spec.networking.pods</code>, users can tune the <code>nodeCIDRMaskSize</code> used by Kube-Controller-Manager on shoot creation.
A smaller IP range per node means more <code>podCIDRs</code> and thus the ability to provision more nodes in the cluster, but less available IPs for Pods running on each of the nodes.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubeControllerManager:
</span></span><span style=display:flex><span>    nodeCIDRMaskSize: 24 (default)
</span></span></code></pre></div><blockquote><p>⚠️ The <code>nodeCIDRMaskSize</code> configuration is immutable and cannot be changed afterwards.</p></blockquote><p><em><strong>Example 3</strong></em></p><pre tabindex=0><code>Pod network: 100.96.0.0/20
nodeCIDRMaskSize: /25
-------------------------

Number of podCIDRs: 32 --&gt; max. Node count 
Number of IPs per podCIDRs: 128
</code></pre><p>With the configuration above, a Shoot cluster can at most have <strong>32 nodes</strong> which are ready to run workload in the Pod network.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-aa95d2c4d5739a992f6e2fe3ce04db73>38 - Shoot Operations</h1><h1 id=trigger-shoot-operations>Trigger Shoot Operations</h1><p>You can trigger a few explicit operations by annotating the <code>Shoot</code> with an operation annotation.
This might allow you to induct certain behavior without the need to change the <code>Shoot</code> specification.
Some of the operations can also not be caused by changing something in the shoot specification because they can&rsquo;t properly be reflected here.
Note that once the triggered operation is considered by the controllers, the annotation will be automatically removed and you have to add it each time you want to trigger the operation.</p><p>Please note: If <code>.spec.maintenance.confineSpecUpdateRollout=true</code>, then the only way to trigger a shoot reconciliation is by setting the <code>reconcile</code> operation, see below.</p><h2 id=immediate-reconciliation>Immediate Reconciliation</h2><p>Annotate the shoot with <code>gardener.cloud/operation=reconcile</code> to make the <code>gardenlet</code> start a reconciliation operation without changing the shoot spec and possibly without being in its maintenance time window:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=reconcile
</span></span></code></pre></div><h2 id=immediate-maintenance>Immediate Maintenance</h2><p>Annotate the shoot with <code>gardener.cloud/operation=maintain</code> to make the <code>gardener-controller-manager</code> start maintaining your shoot immediately (possibly without being in its maintenance time window).
If no reconciliation starts, then nothing needs to be maintained:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=maintain
</span></span></code></pre></div><h2 id=retry-failed-reconciliation>Retry Failed Reconciliation</h2><p>Annotate the shoot with <code>gardener.cloud/operation=retry</code> to make the <code>gardenlet</code> start a new reconciliation loop on a failed shoot.
Failed shoots are only reconciled again if a new Gardener version is deployed, the shoot specification is changed or this annotation is set:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=retry
</span></span></code></pre></div><h2 id=credentials-rotation-operations>Credentials Rotation Operations</h2><p>Please consult <a href=/docs/gardener/usage/shoot_credentials_rotation/>Credentials Rotation for Shoot Clusters</a> for more information.</p><h2 id=restart-systemd-services-on-particular-worker-nodes>Restart <code>systemd</code> Services on Particular Worker Nodes</h2><p>It is possible to make Gardener restart particular systemd services on your shoot worker nodes if needed.
The annotation is not set on the <code>Shoot</code> resource but directly on the <code>Node</code> object you want to target.
For example, the following will restart both the <code>kubelet</code> and the <code>docker</code> services:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl annotate node &lt;node-name&gt; worker.gardener.cloud/restart-systemd-services=kubelet,docker
</span></span></code></pre></div><p>It may take up to a minute until the service is restarted.
The annotation will be removed from the <code>Node</code> object after all specified systemd services have been restarted.
It will also be removed even if the restart of one or more services failed.</p><blockquote><p>ℹ️ In the example mentioned above, you could additionally verify when/whether the kubelet restarted by using <code>kubectl describe node &lt;node-name></code> and looking for such a <code>Starting kubelet</code> event.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-ce24e382e35468e5c98ad64130e0653b>39 - Shoot Purposes</h1><h1 id=shoot-cluster-purpose>Shoot Cluster Purpose</h1><p>The <code>Shoot</code> resource contains a <code>.spec.purpose</code> field indicating how the shoot is used, whose allowed values are as follows:</p><ul><li><code>evaluation</code> (default): Indicates that the shoot cluster is for evaluation scenarios.</li><li><code>development</code>: Indicates that the shoot cluster is for development scenarios.</li><li><code>testing</code>: Indicates that the shoot cluster is for testing scenarios.</li><li><code>production</code>: Indicates that the shoot cluster is for production scenarios.</li><li><code>infrastructure</code>: Indicates that the shoot cluster is for infrastructure scenarios (only allowed for shoots in the <code>garden</code> namespace).</li></ul><h2 id=behavioral-differences>Behavioral Differences</h2><p>The following enlists the differences in the way the shoot clusters are set up based on the selected purpose:</p><ul><li><code>testing</code> shoot clusters <strong>do not</strong> get a monitoring or a logging stack as part of their control planes.</li><li><code>production</code> shoot clusters get at least two replicas of the <code>kube-apiserver</code> for their control planes.
Auto-scaling scale down of the main ETCD is disabled for such clusters.</li></ul><p>There are also differences with respect to how <code>testing</code> shoots are scheduled after creation, please consult the <a href=/docs/gardener/concepts/scheduler/>Scheduler documentation</a>.</p><h2 id=future-steps>Future Steps</h2><p>We might introduce more behavioral difference depending on the shoot purpose in the future.
As of today, there are no plans yet.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5e55fb76047a446cc19292d575f3a524>40 - Shoot Scheduling Profiles</h1><h1 id=shoot-scheduling-profiles>Shoot Scheduling Profiles</h1><p>This guide describes the available scheduling profiles and how they can be configured in the Shoot cluster. It also clarifies how a custom scheduling profile can be configured.</p><h2 id=scheduling-profiles>Scheduling Profiles</h2><p>The scheduling process in the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/>kube-scheduler</a> happens in a series of stages. A <a href=https://kubernetes.io/docs/reference/scheduling/config/#profiles>scheduling profile</a> allows configuring the different stages of the scheduling.</p><p>As of today, Gardener supports two predefined scheduling profiles:</p><ul><li><p><code>balanced</code> (default)</p><p><strong>Overview</strong></p><p>The <code>balanced</code> profile attempts to spread Pods evenly across Nodes to obtain a more balanced resource usage. This profile provides the default kube-scheduler behavior.</p><p><strong>How it works?</strong></p><p>The kube-scheduler is started without any profiles. In such case, by default, one profile with the scheduler name <code>default-scheduler</code> is created. This profile includes the default plugins. If a Pod doesn&rsquo;t specify the <code>.spec.schedulerName</code> field, kube-apiserver sets it to <code>default-scheduler</code>. Then, the Pod gets scheduled by the <code>default-scheduler</code> accordingly.</p></li><li><p><code>bin-packing</code></p><p><strong>Overview</strong></p><p>The <code>bin-packing</code> profile scores Nodes based on the allocation of resources. It prioritizes Nodes with the most allocated resources. By favoring the Nodes with the most allocation, some of the other Nodes become under-utilized over time (because new Pods keep being scheduled to the most allocated Nodes). Then, the cluster-autoscaler identifies such under-utilized Nodes and removes them from the cluster. In this way, this profile provides a greater overall resource utilization (compared to the <code>balanced</code> profile).</p><blockquote><p><strong>Note:</strong> The decision of when to remove a Node is a trade-off between optimizing for utilization or the availability of resources. Removing under-utilized Nodes improves cluster utilization, but new workloads might have to wait for resources to be provisioned again before they can run.</p></blockquote><p><strong>How it works?</strong></p><p>The kube-scheduler is configured with the following bin packing profile:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: kubescheduler.config.k8s.io/v1beta3
</span></span><span style=display:flex><span>kind: KubeSchedulerConfiguration
</span></span><span style=display:flex><span>profiles:
</span></span><span style=display:flex><span>- schedulerName: bin-packing-scheduler
</span></span><span style=display:flex><span>  pluginConfig:
</span></span><span style=display:flex><span>  - name: NodeResourcesFit
</span></span><span style=display:flex><span>    args:
</span></span><span style=display:flex><span>      scoringStrategy:
</span></span><span style=display:flex><span>        type: MostAllocated
</span></span><span style=display:flex><span>  plugins:
</span></span><span style=display:flex><span>    score:
</span></span><span style=display:flex><span>      disabled:
</span></span><span style=display:flex><span>      - name: NodeResourcesBalancedAllocation
</span></span></code></pre></div><p>To impose the new profile, a <code>MutatingWebhookConfiguration</code> is deployed in the Shoot cluster. The <code>MutatingWebhookConfiguration</code> intercepts <code>CREATE</code> operations for Pods and sets the <code>.spec.schedulerName</code> field to <code>bin-packing-scheduler</code>. Then, the Pod gets scheduled by the <code>bin-packing-scheduler</code> accordingly. Pods that specify a custom scheduler (i.e., having <code>.spec.schedulerName</code> different from <code>default-scheduler</code> and <code>bin-packing-scheduler</code>) are not affected.</p></li></ul><h2 id=configuring-the-scheduling-profile>Configuring the Scheduling Profile</h2><p>The scheduling profile can be configured via the <code>.spec.kubernetes.kubeScheduler.profile</code> field in the Shoot:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  <span style=color:green># ...</span>
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeScheduler:
</span></span><span style=display:flex><span>      profile: <span style=color:#a31515>&#34;balanced&#34;</span> <span style=color:green># or &#34;bin-packing&#34;</span>
</span></span></code></pre></div><h2 id=custom-scheduling-profiles>Custom Scheduling Profiles</h2><p>The kube-scheduler&rsquo;s component configs allows configuring custom scheduling profiles to match the cluster needs. As of today, Gardener supports only two predefined scheduling profiles. The profile configuration in the component config is quite expressive and it is not possible to easily define profiles that would match the needs of every cluster. Because of these reasons, there are no plans to add support for new predefined scheduling profiles. If a cluster owner wants to use a custom scheduling profile, then they have to deploy (and maintain) a dedicated kube-scheduler deployment in the cluster itself.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f031782d2dc6494fa8d3d01bba9374d9>41 - Shoot Serviceaccounts</h1><h1 id=serviceaccount-configurations-for-shoot-clusters><code>ServiceAccount</code> Configurations for Shoot Clusters</h1><p>The <code>Shoot</code> specification allows to configure some of the settings for the handling of <code>ServiceAccount</code>s:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeAPIServer:
</span></span><span style=display:flex><span>      serviceAccountConfig:
</span></span><span style=display:flex><span>        issuer: foo
</span></span><span style=display:flex><span>        acceptedIssuers:
</span></span><span style=display:flex><span>        - foo1
</span></span><span style=display:flex><span>        - foo2
</span></span><span style=display:flex><span>        extendTokenExpiration: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        maxTokenExpiration: 45d
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=issuer-and-accepted-issuers>Issuer and Accepted Issuers</h2><p>The <code>.spec.kubernetes.kubeAPIServer.serviceAccountConfig.{issuer,acceptedIssuers}</code> fields are translated to the <code>--service-account-issuer</code> flag for the <code>kube-apiserver</code>.
The issuer will assert its identifier in the <code>iss</code> claim of the issued tokens.
According to the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/>upstream specification</a>, values need to meet the following requirements:</p><blockquote><p>This value is a string or URI. If this option is not a valid URI per the OpenID Discovery 1.0 spec, the ServiceAccountIssuerDiscovery feature will remain disabled, even if the feature gate is set to true. It is highly recommended that this value comply with the OpenID spec: <a href=https://openid.net/specs/openid-connect-discovery-1_0.html>https://openid.net/specs/openid-connect-discovery-1_0.html</a>. In practice, this means that service-account-issuer must be an https URL. It is also highly recommended that this URL be capable of serving OpenID discovery documents at {service-account-issuer}/.well-known/openid-configuration.</p></blockquote><p>By default, Gardener uses the internal cluster domain as issuer (e.g., <code>https://api.foo.bar.example.com</code>).
If you specify the <code>issuer</code>, then this default issuer will always be part of the list of accepted issuers (you don&rsquo;t need to specify it yourself).</p><p>⚠️ Caution: If you change from the default issuer to a custom <code>issuer</code>, all previously issued tokens will still be valid/accepted.
However, if you change from a custom <code>issuer</code> <code>A</code> to another <code>issuer</code> <code>B</code> (custom or default), then you have to add <code>A</code> to the <code>acceptedIssuers</code> so that previously issued tokens are not invalidated.
Otherwise, the control plane components as well as system components and your workload pods might fail.
You can remove <code>A</code> from the <code>acceptedIssuers</code> when all currently active tokens have been issued solely by <code>B</code>.
This can be ensured by using <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>projected token volumes</a> with a short validity, or by rolling out all pods.
Additionally, all <a href=https://kubernetes.io/docs/concepts/configuration/secret/#service-account-token-secrets><code>ServiceAccount</code> token secrets</a> should be recreated.
Apart from this, you should wait for at least <code>12h</code> to make sure the control plane and system components have received a new token from Gardener.</p><h2 id=token-expirations>Token Expirations</h2><p>The <code>.spec.kubernetes.kubeAPIServer.serviceAccountConfig.extendTokenExpiration</code> configures the <code>--service-account-extend-token-expiration</code> flag of the <code>kube-apiserver</code>.
It is enabled by default and has the following specification:</p><blockquote><p>Turns on projected service account expiration extension during token generation, which helps safe transition from legacy token to bound service account token feature. If this flag is enabled, admission injected tokens would be extended up to 1 year to prevent unexpected failure during transition, ignoring value of service-account-max-token-expiration.</p></blockquote><p>The <code>.spec.kubernetes.kubeAPIServer.serviceAccountConfig.maxTokenExpiration</code> configures the <code>--service-account-max-token-expiration</code> flag of the <code>kube-apiserver</code>.
It has the following specification:</p><blockquote><p>The maximum validity duration of a token created by the service account token issuer. If an otherwise valid TokenRequest with a validity duration larger than this value is requested, a token will be issued with a validity duration of this value.</p></blockquote><p>⚠️ Note that the value for this field must be in the <code>[30d,90d]</code> range.
The background for this limitation is that all Gardener components rely on the <code>TokenRequest</code> API and the Kubernetes service account token projection feature with short-lived, auto-rotating tokens.
Any values lower than <code>30d</code> risk impacting the SLO for shoot clusters, and any values above <code>90d</code> violate security best practices with respect to maximum validity of credentials before they must be rotated.
Given that the field just specifies the upper bound, end-users can still use lower values for their individual workload by specifying the <code>.spec.volumes[].projected.sources[].serviceAccountToken.expirationSeconds</code> in the <code>PodSpec</code>s.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c9d857194e434604d822b4e8baf96323>42 - Shoot Status</h1><h1 id=shoot-status>Shoot Status</h1><p>This document provides an overview of the <a href=/docs/gardener/api-reference/core/#shootstatus>ShootStatus</a>.</p><h2 id=conditions>Conditions</h2><p>The Shoot status consists of a set of conditions. A <a href=/docs/gardener/api-reference/core/#condition>Condition</a> has the following fields:</p><table><thead><tr><th>Field name</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></td><td>Name of the condition.</td></tr><tr><td><code>status</code></td><td>Indicates whether the condition is applicable, with possible values <code>True</code>, <code>False</code>, <code>Unknown</code> or <code>Progressing</code>.</td></tr><tr><td><code>lastTransitionTime</code></td><td>Timestamp for when the condition last transitioned from one status to another.</td></tr><tr><td><code>lastUpdateTime</code></td><td>Timestamp for when the condition was updated. Usually changes when <code>reason</code> or <code>message</code> in condition is updated.</td></tr><tr><td><code>reason</code></td><td>Machine-readable, UpperCamelCase text indicating the reason for the condition&rsquo;s last transition.</td></tr><tr><td><code>message</code></td><td>Human-readable message indicating details about the last status transition.</td></tr><tr><td><code>codes</code></td><td>Well-defined error codes in case the condition reports a problem.</td></tr></tbody></table><p>Currently, the available Shoot condition types are:</p><ul><li><code>APIServerAvailable</code></li><li><code>ControlPlaneHealthy</code></li><li><code>EveryNodeReady</code></li><li><code>ObservabilityComponentsHealthy</code></li><li><code>SystemComponentsHealthy</code></li></ul><p>The Shoot conditions are maintained by the <a href=https://github.com/gardener/gardener/blob/master/pkg/gardenlet/controller/shoot/care/reconciler.go>shoot care reconciler</a> of the gardenlet.
Find more information in the <a href=/docs/gardener/concepts/gardenlet/#shoot-controller>gardelent documentation</a>.</p><h3 id=sync-period>Sync Period</h3><p>The condition checks are executed periodically at an interval which is configurable in the <code>GardenletConfiguration</code> (<code>.controllers.shootCare.syncPeriod</code>, defaults to <code>1m</code>).</p><h3 id=condition-thresholds>Condition Thresholds</h3><p>The <code>GardenletConfiguration</code> also allows configuring condition thresholds (<code>controllers.shootCare.conditionThresholds</code>). A condition threshold is the amount of time to consider a condition as <code>Processing</code> on condition status changes.</p><p>Let&rsquo;s check the following example to get a better understanding. Let&rsquo;s say that the <code>APIServerAvailable</code> condition of our Shoot is with status <code>True</code>. If the next condition check fails (for example kube-apiserver becomes unreachable), then the condition first goes to <code>Processing</code> state. Only if this state remains for condition threshold amount of time, then the condition is finally updated to <code>False</code>.</p><h3 id=constraints>Constraints</h3><p>Constraints represent conditions of a Shoot’s current state that constraint some operations on it.
The current constraints are:</p><p><strong><code>HibernationPossible</code></strong>:</p><p>This constraint indicates whether a Shoot is allowed to be hibernated.
The rationale behind this constraint is that a Shoot can have <code>ValidatingWebhookConfiguration</code>s or <code>MutatingWebhookConfiguration</code>s acting on resources that are critical for waking up a cluster.
For example, if a webhook has rules for <code>CREATE/UPDATE</code> Pods or Nodes and <code>failurePolicy=Fail</code>, the webhook will block joining <code>Nodes</code> and creating critical system component Pods and thus block the entire wakeup operation, because the server backing the webhook is not running.</p><p>Even if the <code>failurePolicy</code> is set to <code>Ignore</code>, high timeouts (<code>>15s</code>) can lead to blocking requests of control plane components.
That&rsquo;s because most control-plane API calls are made with a client-side timeout of <code>30s</code>, so if a webhook has <code>timeoutSeconds=30</code>
the overall request might still fail as there is overhead in communication with the API server and potential other webhooks.</p><p>Generally, it&rsquo;s <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#timeouts>best practice</a> to specify low timeouts in WebhookConfigs.</p><p>As an effort to correct this common problem, the webhook remediator has been created. This is enabled by setting <code>.controllers.shootCare.webhookRemediatorEnabled=true</code> in the <code>gardenlet</code>&rsquo;s configuration. This feature simply checks whether webhook configurations in shoot clusters match a set of rules described <a href=https://github.com/gardener/gardener/blob/master/pkg/operation/botanist/matchers/matcher.go>here</a>. If at least one of the rules matches, it will change set <code>status=False</code> for the <code>.status.constraints</code> of type <code>HibernationPossible</code> and <code>MaintenancePreconditionsSatisfied</code> in the <code>Shoot</code> resource. In addition, the <code>failurePolicy</code> in the affected webhook configurations will be set from <code>Fail</code> to <code>Ignore</code>. Gardenlet will also add an annotation to make it visible to end-users that their webhook configurations were mutated and should be fixed/adapted according to the rules and best practices.</p><p>In most cases, you can avoid this by simply excluding the <code>kube-system</code> namespace from your webhook via the <code>namespaceSelector</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: admissionregistration.k8s.io/v1
</span></span><span style=display:flex><span>kind: MutatingWebhookConfiguration
</span></span><span style=display:flex><span>webhooks:
</span></span><span style=display:flex><span>  - name: my-webhook.example.com
</span></span><span style=display:flex><span>    namespaceSelector:
</span></span><span style=display:flex><span>      matchExpressions:
</span></span><span style=display:flex><span>      - key: gardener.cloud/purpose
</span></span><span style=display:flex><span>        operator: NotIn
</span></span><span style=display:flex><span>        values:
</span></span><span style=display:flex><span>          - kube-system
</span></span><span style=display:flex><span>    rules:
</span></span><span style=display:flex><span>      - operations: [<span style=color:#a31515>&#34;*&#34;</span>]
</span></span><span style=display:flex><span>        apiGroups: [<span style=color:#a31515>&#34;&#34;</span>]
</span></span><span style=display:flex><span>        apiVersions: [<span style=color:#a31515>&#34;v1&#34;</span>]
</span></span><span style=display:flex><span>        resources: [<span style=color:#a31515>&#34;pods&#34;</span>]
</span></span><span style=display:flex><span>        scope: <span style=color:#a31515>&#34;Namespaced&#34;</span>
</span></span></code></pre></div><p>However, some other resources (some of them cluster-scoped) might still trigger the remediator, namely:</p><ul><li>endpoints</li><li>nodes</li><li>podsecuritypolicies</li><li>clusterroles</li><li>clusterrolebindings</li><li>customresourcedefinitions</li><li>apiservices</li><li>certificatesigningrequests</li><li>priorityclasses</li></ul><p>If one of the above resources triggers the remediator, the preferred solution is to remove that particular resource from your webhook&rsquo;s <code>rules</code>. You can also use the <code>objectSelector</code> to reduce the scope of webhook&rsquo;s <code>rules</code>. However, in special cases where a webhook is absolutely needed for the workload, it is possible to add the <code>remediation.webhook.shoot.gardener.cloud/exclude=true</code> label to your webhook so that the remediator ignores it. This label <strong>should not be used to silence an alert</strong>, but rather to confirm that a webhook won&rsquo;t cause problems. Note that all of this is no perfect solution and just done on a best effort basis, and only the owner of the webhook can know whether it indeed is problematic and configured correctly.</p><p>In a special case, if a webhook has a rule for <code>CREATE/UPDATE</code> lease resources in <code>kube-system</code> namespace, its <code>timeoutSeconds</code> is updated to 3 seconds. This is required to ensure the proper functioning of the leader election of essential control plane controllers.</p><p>You can also find more help from the <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#best-practices-and-warnings>Kubernetes documentation</a></p><p><strong><code>MaintenancePreconditionsSatisfied</code></strong>:</p><p>This constraint indicates whether all preconditions for a safe maintenance operation are satisfied (see <a href=/docs/gardener/usage/shoot_maintenance/>Shoot Maintenance</a> for more information about what happens during a shoot maintenance).
As of today, the same checks as in the <code>HibernationPossible</code> constraint are being performed (user-deployed webhooks that might interfere with potential rolling updates of shoot worker nodes).
There is no further action being performed on this constraint&rsquo;s status (maintenance is still being performed).
It is meant to make the user aware of potential problems that might occur due to his configurations.</p><p><strong><code>CACertificateValiditiesAcceptable</code></strong>:</p><p>This constraints indicates that there is at least one CA certificate which expires in less than <code>1y</code>.
It will not be added to the <code>.status.constraints</code> if there is no such CA certificate.
However, if it&rsquo;s visible, then a <a href=/docs/gardener/usage/shoot_credentials_rotation/#certificate-authorities>credentials rotation operation</a> should be considered.</p><h3 id=last-operation>Last Operation</h3><p>The Shoot status holds information about the last operation that is performed on the Shoot. The last operation field reflects overall progress and the tasks that are currently being executed. Allowed operation types are <code>Create</code>, <code>Reconcile</code>, <code>Delete</code>, <code>Migrate</code>, and <code>Restore</code>. Allowed operation states are <code>Processing</code>, <code>Succeeded</code>, <code>Error</code>, <code>Failed</code>, <code>Pending</code>, and <code>Aborted</code>. An operation in <code>Error</code> state is an operation that will be retried for a configurable amount of time (<code>controllers.shoot.retryDuration</code> field in <code>GardenletConfiguration</code>, defaults to <code>12h</code>). If the operation cannot complete successfully for the configured retry duration, it will be marked as <code>Failed</code>. An operation in <code>Failed</code> state is an operation that won&rsquo;t be retried automatically (to retry such an operation, see <a href=/docs/gardener/usage/shoot_operations/#retry-failed-operation>Retry failed operation</a>).</p><h3 id=last-errors>Last Errors</h3><p>The Shoot status also contains information about the last occurred error(s) (if any) during an operation. A <a href=/docs/gardener/api-reference/core/#lasterror>LastError</a> consists of identifier of the task returned error, human-readable message of the error and error codes (if any) associated with the error.</p><h3 id=error-codes>Error Codes</h3><p>Known error codes are:</p><ul><li><code>ERR_INFRA_UNAUTHENTICATED</code> - Indicates that the last error occurred due to the client request not being completed because it lacks valid authentication credentials for the requested resource. It is classified as a non-retryable error code.</li><li><code>ERR_INFRA_UNAUTHORIZED</code> - Indicates that the last error occurred due to the server understanding the request but refusing to authorize it. It is classified as a non-retryable error code.</li><li><code>ERR_INFRA_QUOTA_EXCEEDED</code> - Indicates that the last error occurred due to infrastructure quota limits. It is classified as a non-retryable error code.</li><li><code>ERR_INFRA_RATE_LIMITS_EXCEEDED</code> - Indicates that the last error occurred due to exceeded infrastructure request rate limits.</li><li><code>ERR_INFRA_DEPENDENCIES</code> - Indicates that the last error occurred due to dependent objects on the infrastructure level. It is classified as a non-retryable error code.</li><li><code>ERR_RETRYABLE_INFRA_DEPENDENCIES</code> - Indicates that the last error occurred due to dependent objects on the infrastructure level, but the operation should be retried.</li><li><code>ERR_INFRA_RESOURCES_DEPLETED</code> - Indicates that the last error occurred due to depleted resource in the infrastructure.</li><li><code>ERR_CLEANUP_CLUSTER_RESOURCES</code> - Indicates that the last error occurred due to resources in the cluster that are stuck in deletion.</li><li><code>ERR_CONFIGURATION_PROBLEM</code> - Indicates that the last error occurred due to a configuration problem. It is classified as a non-retryable error code.</li><li><code>ERR_RETRYABLE_CONFIGURATION_PROBLEM</code> - Indicates that the last error occurred due to a retryable configuration problem. &ldquo;Retryable&rdquo; means that the occurred error is likely to be resolved in a ungraceful manner after given period of time.</li><li><code>ERR_PROBLEMATIC_WEBHOOK</code> - Indicates that the last error occurred due to a webhook not following the <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#best-practices-and-warnings>Kubernetes best practices</a>.</li></ul><h3 id=status-label>Status Label</h3><p>Shoots will be automatically labeled with the <code>shoot.gardener.cloud/status</code> label.
Its value might either be <code>healthy</code>, <code>progressing</code>, <code>unhealthy</code> or <code>unknown</code> depending on the <code>.status.conditions</code>, <code>.status.lastOperation</code>, and <code>status.lastErrors</code> of the <code>Shoot</code>.
This can be used as an easy filter method to find shoots based on their &ldquo;health&rdquo; status.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3dee6f3ebf812a904f8258912bb1e764>43 - Shoot Supported Architectures</h1><h1 id=supported-cpu-architectures-for-shoot-worker-nodes>Supported CPU Architectures for Shoot Worker Nodes</h1><p>Users can create shoot clusters with worker groups having virtual machines of different architectures. CPU architecture of each worker pool can be specified in the <code>Shoot</code> specification as follows:</p><h2 id=example-usage-in-a-shoot>Example Usage in a <code>Shoot</code></h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: cpu-worker
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        architecture: &lt;some-cpu-architecture&gt; <span style=color:green># optional</span>
</span></span></code></pre></div><p>If no value is specified for the architecture field, it defaults to <code>amd64</code>. For a valid shoot object, a machine type should be present in the respective <code>CloudProfile</code> with the same CPU architecture as specified in the <code>Shoot</code> yaml. Also, a valid machine image should be present in the <code>CloudProfile</code> that supports the required architecture specified in the <code>Shoot</code> worker pool.</p><h2 id=example-usage-in-a-cloudprofile>Example Usage in a <code>CloudProfile</code></h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: test-image
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - architectures: <span style=color:green># optional</span>
</span></span><span style=display:flex><span>      - &lt;architecture-1&gt;
</span></span><span style=display:flex><span>      - &lt;architecture-2&gt;
</span></span><span style=display:flex><span>      version: 1.2.3
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - architecture: &lt;some-cpu-architecture&gt;
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 8Gi
</span></span><span style=display:flex><span>    name: test-machine
</span></span></code></pre></div><p>Currently, Gardener supports two of the most widely used CPU architectures:</p><ul><li><code>amd64</code></li><li><code>arm64</code></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-442c557a128e8fd2c4f787af32cd900f>44 - Shoot Updates</h1><h1 id=shoot-updates-and-upgrades>Shoot Updates and Upgrades</h1><p>This document describes what happens during shoot updates (changes incorporated in a newly deployed Gardener version) and during shoot upgrades (changes for version controllable by end-users).</p><h2 id=updates>Updates</h2><p>Updates to all aspects of the shoot cluster happen when the gardenlet reconciles the <code>Shoot</code> resource.</p><h3 id=when-are-reconciliations-triggered>When are Reconciliations Triggered</h3><p>Generally, when you change the specification of your <code>Shoot</code> the reconciliation will start immediately, potentially updating your cluster.
Please note that you can also confine the reconciliation triggered due to your specification updates to the cluster&rsquo;s maintenance time window. Please find more information in <a href=/docs/gardener/usage/shoot_maintenance/#confine-specification-changesupdates-roll-out>Confine Specification Changes/Updates Roll Out</a>.</p><p>You can also annotate your shoot with special operation annotations (for more information, see <a href=/docs/gardener/usage/shoot_operations/>Trigger Shoot Operations</a>), which will cause the reconciliation to start due to your actions.</p><p>There is also an automatic reconciliation by Gardener.
The period, i.e., how often it is performed, depends on the configuration of the Gardener administrators/operators.
In some Gardener installations the operators might enable &ldquo;reconciliation in maintenance time window only&rdquo; (for more information, see <a href=/docs/gardener/usage/shoot_maintenance/#cluster-reconciliation>Cluster Reconciliation</a>), which will result in at least one reconciliation during the time configured in the <code>Shoot</code>&rsquo;s <code>.spec.maintenance.timeWindow</code> field.</p><h3 id=which-updates-are-applied>Which Updates are Applied</h3><p>As end-users can only control the <code>Shoot</code> resource&rsquo;s specification but not the used Gardener version, they don&rsquo;t have any influence on which of the updates are rolled out (other than those settings configurable in the <code>Shoot</code>).
A Gardener operator can deploy a new Gardener version at any point in time.
Any subsequent reconciliation of <code>Shoot</code>s will update them by rolling out the changes incorporated in this new Gardener version.</p><p>Some examples for such shoot updates are:</p><ul><li>Add a new/remove an old component to/from the shoot&rsquo;s control plane running in the seed, or to/from the shoot&rsquo;s system components running on the worker nodes.</li><li>Change the configuration of an existing control plane/system component.</li><li>Restart of existing control plane/system components (this might result in a short unavailability of the Kubernetes API server, e.g., when etcd or a kube-apiserver itself is being restarted)</li></ul><h3 id=behavioural-changes>Behavioural Changes</h3><p>Generally, some of such updates (e.g., configuration changes) could theoretically result in different behaviour of controllers.
If such changes would be backwards-incompatible, then we usually follow one of those approaches (depends on the concrete change):</p><ul><li>Only apply the change for new clusters.</li><li>Expose a new field in the <code>Shoot</code> resource that lets users control this changed behaviour to enable it at a convenient point in time.</li><li>Put the change behind an alpha feature gate (disabled by default) in the gardenlet (only controllable by Gardener operators), which will be promoted to beta (enabled by default) in subsequent releases (in this case, end-users have no influence on when the behaviour changes - Gardener operators should inform their end-users and provide clear timelines when they will enable the feature gate).</li></ul><h2 id=upgrades>Upgrades</h2><p>We consider shoot upgrades to change either the:</p><ul><li>Kubernetes version (<code>.spec.kubernetes.version</code>)</li><li>Kubernetes version of the worker pool if specified (<code>.spec.provider.workers[].kubernetes.version</code>)</li><li>Machine image version of at least one worker pool (<code>.spec.provider.workers[].machine.image.version</code>)</li></ul><p>Generally, an upgrade is also performed through a reconciliation of the <code>Shoot</code> resource, i.e., the same concepts as for <a href=#updates>shoot updates</a> apply.
If an end-user triggers an upgrade (e.g., by changing the Kubernetes version) after a new Gardener version was deployed but before the shoot was reconciled again, then this upgrade might incorporate the changes delivered with this new Gardener version.</p><h3 id=in-place-vs-rolling-updates>In-Place vs. Rolling Updates</h3><p>If the Kubernetes patch version is changed, then the upgrade happens in-place.
This means that the shoot worker nodes remain untouched and only the <code>kubelet</code> process restarts with the new Kubernetes version binary.
The same applies for configuration changes of the kubelet.</p><p>If the Kubernetes minor version is changed, then the upgrade is done in a &ldquo;rolling update&rdquo; fashion, similar to how pods in Kubernetes are updated (when backed by a <code>Deployment</code>).
The worker nodes will be terminated one after another and replaced by new machines.
The existing workload is gracefully drained and evicted from the old worker nodes to new worker nodes, respecting the configured <code>PodDisruptionBudget</code>s (see <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb/>Specifying a Disruption Budget for your Application</a>).</p><h4 id=customize-rolling-update-behaviour-of-shoot-worker-nodes>Customize Rolling Update Behaviour of Shoot Worker Nodes</h4><p>The <code>.spec.provider.workers[]</code> list exposes two fields that you might configure based on your workload&rsquo;s needs: <code>maxSurge</code> and <code>maxUnavailable</code>.
The same concepts <a href=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment>like in Kubernetes</a> apply.
Additionally, you might customize how the machine-controller-manager (abbrev.: MCM; the component instrumenting this rolling update) is behaving. You can configure the following fields in <code>.spec.provider.worker[].machineControllerManager</code>:</p><ul><li><code>machineDrainTimeout</code>: Timeout (in duration) used while draining of machine before deletion, beyond which MCM forcefully deletes the machine (default: <code>10m</code>).</li><li><code>machineHealthTimeout</code>: Timeout (in duration) used while re-joining (in case of temporary health issues) of a machine before it is declared as failed (default: <code>10m</code>).</li><li><code>machineCreationTimeout</code>: Timeout (in duration) used while joining (during creation) of a machine before it is declared as failed (default: <code>10m</code>).</li><li><code>maxEvictRetries</code>: Maximum number of times evicts would be attempted on a pod before it is forcibly deleted during the draining of a machine (default: <code>10</code>).</li><li><code>nodeConditions</code>: List of case-sensitive node-conditions which will change a machine to a <code>Failed</code> state after the <code>machineHealthTimeout</code> duration. It may further be replaced with a new machine if the machine is backed by a machine-set object (defaults: <code>KernelDeadlock</code>, <code>ReadonlyFilesystem</code> , <code>DiskPressure</code>).</li></ul><h4 id=rolling-update-triggers>Rolling Update Triggers</h4><p>Apart from the above mentioned triggers, a rolling update of the shoot worker nodes is also triggered for some changes to your worker pool specification (<code>.spec.provider.workers[]</code>, even if you don&rsquo;t change the Kubernetes or machine image version).
The complete list of fields that trigger a rolling update:</p><ul><li><code>.spec.kubernetes.version</code> (except for patch version changes)</li><li><code>.spec.provider.workers[].machine.image.name</code></li><li><code>.spec.provider.workers[].machine.image.version</code></li><li><code>.spec.provider.workers[].machine.type</code></li><li><code>.spec.provider.workers[].volume.type</code></li><li><code>.spec.provider.workers[].volume.size</code></li><li><code>.spec.provider.workers[].providerConfig</code></li><li><code>.spec.provider.workers[].cri.name</code></li><li><code>.spec.provider.workers[].kubernetes.version</code> (except for patch version changes)</li><li><code>.status.credentials.rotation.certificateAuthorities.lastInitiationTime</code> (changed by Gardener when a shoot CA rotation is initiated)</li><li><code>.status.credentials.rotation.serviceAccountKey.lastInitiationTime</code> (changed by Gardener when a shoot service account signing key rotation is initiated)</li></ul><p>Generally, the provider extension controllers might have additional constraints for changes leading to rolling updates, so please consult the respective documentation as well.</p><h2 id=related-documentation>Related Documentation</h2><ul><li><a href=/docs/gardener/usage/shoot_operations/>Shoot Operations</a></li><li><a href=/docs/gardener/usage/shoot_maintenance/>Shoot Maintenance</a></li><li><a href=/docs/gardener/usage/shoot_maintenance/#confine-specification-changesupdates-roll-out>Confine Specification Changes/Updates Roll Out To Maintenance Time Window</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-76b0c7e5bbbfbafbbf70acec450a467b>45 - Shoot Versions</h1><h1 id=shoot-kubernetes-and-operating-system-versioning-in-gardener>Shoot Kubernetes and Operating System Versioning in Gardener</h1><h2 id=motivation>Motivation</h2><p>On the one hand-side, Gardener is responsible for managing the Kubernetes and the Operating System (OS) versions of its Shoot clusters.
On the other hand-side, Gardener needs to be configured and updated based on the availability and support of the Kubernetes and Operating System version it provides.
For instance, the Kubernetes community releases <strong>minor</strong> versions roughly every three months and usually maintains <strong>three minor</strong> versions (the current and the last two) with bug fixes and security updates.
Patch releases are done more frequently.</p><p>When using the term <code>Machine image</code> in the following, we refer to the OS version that comes with the machine image of the node/worker pool of a Gardener Shoot cluster.
As such, we are not referring to the <code>CloudProvider</code> specific machine image like the <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html><code>AMI</code></a> for AWS.
For more information on how Gardener maps machine image versions to <code>CloudProvider</code> specific machine images, take a look at the individual gardener extension providers, such as the <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/usage-as-operator/>provider for AWS</a>.</p><p>Gardener should be configured accordingly to reflect the &ldquo;logical state&rdquo; of a version.
It should be possible to define the Kubernetes or Machine image versions that still receive bug fixes and security patches, and also vice-versa to define the version that are out-of-maintenance and are potentially vulnerable.
Moreover, this allows Gardener to &ldquo;understand&rdquo; the current state of a version and act upon it (more information in the following sections).</p><h2 id=overview>Overview</h2><p><strong>As a Gardener operator</strong>:</p><ul><li>I can classify a version based on it&rsquo;s logical state (<code>preview</code>, <code>supported</code>, <code>deprecated</code>, and <code>expired</code>; see <a href=#version-classifications>Version Classification</a>).</li><li>I can define which Machine image and Kubernetes versions are eligible for the auto update of clusters during the maintenance time.</li><li>I can disallow the creation of clusters having a certain version (think of severe security issues).</li></ul><p><strong>As an end-user/Shoot owner of Gardener</strong>:</p><ul><li>I can get information about which Kubernetes and Machine image versions exist and their classification.</li><li>I can determine the time when my Shoot clusters Machine image and Kubernetes version will be forcefully updated to the next patch or minor version (in case the cluster is running a deprecated version with an expiration date).</li><li>I can get this information via API from the <code>CloudProfile</code>.</li></ul><h2 id=version-classifications>Version Classifications</h2><p>Administrators can classify versions into four distinct &ldquo;logical states&rdquo;: <code>preview</code>, <code>supported</code>, <code>deprecated</code>, and <code>expired</code>.
The version classification serves as a &ldquo;point-of-reference&rdquo; for end-users and also has implications during shoot creation and the maintenance time.</p><p>If a version is unclassified, Gardener cannot make those decision based on the &ldquo;logical state&rdquo;.
Nevertheless, Gardener can operate without version classifications and can be added at any time to the Kubernetes and machine image versions in the <code>CloudProfile</code>.</p><p>As a best practice, versions usually start with the classification <code>preview</code>, then are promoted to <code>supported</code>, eventually <code>deprecated</code> and finally <code>expired</code>.
This information is programmatically available in the <code>CloudProfiles</code> of the Garden cluster.</p><ul><li><p><strong>preview:</strong> A <code>preview</code> version is a new version that has not yet undergone thorough testing, possibly a new release, and needs time to be validated.
Due to its short early age, there is a higher probability of undiscovered issues and is therefore not yet recommended for production usage.
A Shoot does not update (neither <code>auto-update</code> or <code>force-update</code>) to a <code>preview</code> version during the maintenance time.
Also, <code>preview</code> versions are not considered for the defaulting to the highest available version when deliberately omitting the patch version during Shoot creation.
Typically, after a fresh release of a new Kubernetes (e.g., v1.25.0) or Machine image version (e.g., suse-chost 15.4.20220818), the operator tags it as <code>preview</code> until he has gained sufficient experience and regards this version to be reliable.
After the operator has gained sufficient trust, the version can be manually promoted to <code>supported</code>.</p></li><li><p><strong>supported:</strong> A <code>supported</code> version is the recommended version for new and existing Shoot clusters. This is the version that new Shoot clusters should use and existing clusters should update to.
Typically for Kubernetes versions, the latest Kubernetes patch versions of the actual (if not still in <code>preview</code>) and the last 3 minor Kubernetes versions are maintained by the community. An operator could define these versions as being <code>supported</code> (e.g., v1.24.6, v1.23.12, and v1.22.15).</p></li><li><p><strong>deprecated:</strong> A <code>deprecated</code> version is a version that approaches the end of its lifecycle and can contain issues which are probably resolved in a supported version.
New Shoots should not use this version anymore.
Existing Shoots will be updated to a newer version if <code>auto-update</code> is enabled (<code>.spec.maintenance.autoUpdate.kubernetesVersion</code> for Kubernetes version <code>auto-update</code>, or <code>.spec.maintenance.autoUpdate.machineImageVersion</code> for machine image version <code>auto-update</code>).
Using automatic upgrades, however, does not guarantee that a Shoot runs a non-deprecated version, as the latest version (overall or of the minor version) can be deprecated as well.
Deprecated versions <strong>should</strong> have an expiration date set for eventual expiration.</p></li><li><p><strong>expired:</strong> An <code>expired</code> versions has an expiration date (based on the <a href=https://golang.org/src/time/time.go>Golang time package</a>) in the past.
New clusters with that version cannot be created and existing clusters are forcefully migrated to a higher version during the maintenance time.</p></li></ul><p>Below is an example how the relevant section of the <code>CloudProfile</code> might look like:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: alicloud
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>      - classification: preview
</span></span><span style=display:flex><span>        version: 1.25.0
</span></span><span style=display:flex><span>      - classification: supported
</span></span><span style=display:flex><span>        version: 1.24.6
</span></span><span style=display:flex><span>      - classification: deprecated
</span></span><span style=display:flex><span>        expirationDate: <span style=color:#a31515>&#34;2022-11-30T23:59:59Z&#34;</span>
</span></span><span style=display:flex><span>        version: 1.24.5
</span></span><span style=display:flex><span>      - classification: supported
</span></span><span style=display:flex><span>        version: 1.23.12
</span></span><span style=display:flex><span>      - classification: deprecated
</span></span><span style=display:flex><span>        expirationDate: <span style=color:#a31515>&#34;2023-01-31T23:59:59Z&#34;</span>
</span></span><span style=display:flex><span>        version: 1.23.11
</span></span><span style=display:flex><span>      - classification: supported
</span></span><span style=display:flex><span>        version: 1.22.15
</span></span><span style=display:flex><span>      - classification: deprecated
</span></span><span style=display:flex><span>        version: 1.21.14
</span></span></code></pre></div><h2 id=version-requirements-kubernetes-and-machine-image>Version Requirements (Kubernetes and Machine Image)</h2><p>The Gardener API server enforces the following requirements for versions:</p><h3 id=deletion-of-a-version>Deletion of a Version</h3><ul><li>A version that is in use by a Shoot cannot be deleted from the <code>CloudProfile</code>.</li></ul><h3 id=adding-a-version>Adding a Version</h3><ul><li>A version must not have an expiration date in the past.</li><li>There can be only one <code>supported</code> version per minor version.</li><li>The latest Kubernetes version cannot have an expiration date.</li><li>The latest version for a machine image can have an expiration date. [*]</li></ul><p><sub>[*] Useful for cases in which support for A given machine image needs to be deprecated and removed (for example, the machine image reaches end of life).</sub></p><h2 id=forceful-migration-of-expired-versions>Forceful Migration of Expired Versions</h2><p>If a Shoot is running a version after its expiration date has passed, it will be forcefully migrated during its maintenance time.
This happens <strong>even if the owner has opted out of automatic cluster updates!</strong></p><p>For <strong>Machine images</strong>, the Shoots worker pools will be updated to the latest <code>non-preview</code> version of the pools respective image.</p><p>For <strong>Kubernetes versions</strong>, the forceful update picks the latest <code>non-preview</code> patch version of the current minor version.</p><p>If the cluster is already on the latest patch version and the latest patch version is also expired,
it will continue with the latest patch version of the <strong>next consecutive minor Kubernetes version</strong>,
so <strong>it will result in an update of a minor Kubernetes version!</strong></p><p>Please note that multiple consecutive minor version upgrades are possible.
This can occur if the Shoot is updated to a version that in turn is also <code>expired</code>.
In this case, the version is again upgraded in the <strong>next</strong> maintenance time.</p><p><strong>Depending on the circumstances described above, it can happen that the cluster receives multiple consecutive minor Kubernetes version updates!</strong></p><p>Kubernetes &ldquo;minor version jumps&rdquo; are not allowed - meaning to skip the update to the consecutive minor version and directly update to any version after that.
For instance, the version <code>1.20.x</code> can only update to a version <code>1.21.x</code>, not to <code>1.22.x</code> or any other version.
This is because Kubernetes does not guarantee upgradability in this case, leading to possibly broken Shoot clusters.
The administrator has to set up the <code>CloudProfile</code> in such a way that consecutive Kubernetes minor versions are available.
Otherwise, Shoot clusters will fail to upgrade during the maintenance time.</p><p>Consider the <code>CloudProfile</code> below with a Shoot using the Kubernetes version <code>1.20.12</code>.
Even though the version is <code>expired</code>, due to missing <code>1.21.x</code> versions, the Gardener Controller Manager cannot upgrade the Shoot&rsquo;s Kubernetes version.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.22.8
</span></span><span style=display:flex><span>    - version: 1.22.7
</span></span><span style=display:flex><span>    - version: 1.20.12
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;&lt;expiration date in the past&gt;&#34;</span>
</span></span></code></pre></div><p>The <code>CloudProfile</code> must specify versions <code>1.21.x</code> of the <strong>consecutive</strong> minor version.
Configuring the <code>CloudProfile</code> in such a way, the Shoot&rsquo;s Kubernetes version will be upgraded to version <code>1.21.10</code> in the next maintenance time.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.22.8
</span></span><span style=display:flex><span>    - version: 1.21.10
</span></span><span style=display:flex><span>    - version: 1.21.09
</span></span><span style=display:flex><span>    - version: 1.20.12
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;&lt;expiration date in the past&gt;&#34;</span>
</span></span></code></pre></div><h2 id=related-documentation>Related Documentation</h2><p>You might want to read about the <a href=/docs/gardener/usage/shoot_updates/>Shoot Updates and Upgrades</a> procedures to get to know the effects of such operations.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5bde288386d761b4f05e41e886f87a9c>46 - Shoot Workerless</h1><h1 id=workerless-shoots>Workerless <code>Shoot</code>s</h1><p>Starting from <code>v1.71</code>, users can create a <code>Shoot</code> without any workers, known as a &ldquo;workerless <code>Shoot</code>&rdquo;. Previously, worker nodes had to always be included even if users only needed the Kubernetes control plane. With workerless <code>Shoot</code>s, Gardener will not create any worker nodes or anything related to them.</p><p>Here&rsquo;s an example manifest for a local workerless <code>Shoot</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: local
</span></span><span style=display:flex><span>  namespace: garden-local
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: local
</span></span><span style=display:flex><span>  region: local
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: local
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.26.0
</span></span></code></pre></div><blockquote><p>⚠️ It&rsquo;s important to note that a workerless <code>Shoot</code> cannot be converted to a <code>Shoot</code> with workers or vice versa.</p></blockquote><p>As part of the control plane, the following components are deployed in the seed cluster for workerless <code>Shoot</code>:</p><ul><li>etcds</li><li>kube-apiserver</li><li>kube-controller-manager</li><li>gardener-resource-manager</li><li>logging and monitoring components</li><li>extension components (if they support workerless <code>Shoot</code>s, see <a href=/docs/gardener/extensions/extension/#what-is-required-to-register-and-support-an-extension-type>here</a>)</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-92a2153b69b5ad751225b35af070429c>47 - Shoot Workers Settings</h1><h1 id=shoot-worker-nodes-settings>Shoot Worker Nodes Settings</h1><p>Users can configure settings affecting all worker nodes via <code>.spec.provider.workersSettings</code> in the <code>Shoot</code> resource.</p><h2 id=ssh-access>SSH Access</h2><p><code>SSHAccess</code> indicates whether the <code>sshd.service</code> should be running on the worker nodes. This is ensured by a systemd service called <code>sshd-ensurer.service</code> which runs every 15 seconds on each worker node. When set to <code>true</code>, the systemd service ensures that the <code>sshd.service</code> is enabled and running. If it is set to <code>false</code>, the systemd service ensures that <code>sshd.service</code> is stopped and disabled. This also terminates all established SSH connections. In addition, when this value is set to <code>false</code>, existing <code>Bastion</code> resources are deleted during <code>Shoot</code> reconciliation and new ones are prevented from being created, SSH keypairs are not created/rotated, SSH keypair secrets are deleted from the Garden cluster, and the <code>gardener-user.service</code> is not deployed to the worker nodes.</p><p><code>sshAccess.enabled</code> is set to <code>true</code> by default.</p><h3 id=example-usage-in-a-shoot>Example Usage in a <code>Shoot</code></h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workersSettings:
</span></span><span style=display:flex><span>      sshAccess:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>false</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-ba6ea030a94e576907f8029c24109302>48 - Supported K8s Versions</h1><h1 id=supported-kubernetes-versions>Supported Kubernetes Versions</h1><p>Currently, Gardener supports the following Kubernetes versions:</p><h2 id=garden-clusters>Garden Clusters</h2><p>The minimum version of a garden cluster that can be used to run Gardener is <strong><code>1.20.x</code></strong>.</p><h2 id=seed-clusters>Seed Clusters</h2><p>The minimum version of a seed cluster that can be connected to Gardener is <strong><code>1.20.x</code></strong>.</p><h2 id=shoot-clusters>Shoot Clusters</h2><p>Gardener itself is capable of spinning up clusters with Kubernetes versions <strong><code>1.20</code></strong> up to <strong><code>1.27</code></strong>.
However, the concrete versions that can be used for shoot clusters depend on the installed provider extension.
Consequently, please consult the documentation of your provider extension to see which Kubernetes versions are supported for shoot clusters.</p><blockquote><p>👨🏼‍💻 Developers note: The <a href=/docs/gardener/development/new-kubernetes-version/>Adding Support For a New Kubernetes Version</a> topic explains what needs to be done in order to add support for a new Kubernetes version.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-f92740830de9285764daf47f392d76d6>49 - Tolerations</h1><h1 id=taints-and-tolerations-for-seeds-and-shoots>Taints and Tolerations for <code>Seed</code>s and <code>Shoot</code>s</h1><p>Similar to <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/>taints and tolerations</a> for <code>Node</code>s and <code>Pod</code>s in Kubernetes, the <code>Seed</code> resource supports specifying taints (<code>.spec.taints</code>, see <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml#L48-L55>this example</a>) while the <code>Shoot</code> resource supports specifying tolerations (<code>.spec.tolerations</code>, see <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L268-L269>this example</a>).
The feature is used to control scheduling to seeds as well as decisions whether a shoot can use a certain seed.</p><p>Compared to Kubernetes, Gardener&rsquo;s taints and tolerations are very much down-stripped right now and have some behavioral differences.
Please read the following explanations carefully if you plan to use them.</p><h2 id=scheduling>Scheduling</h2><p>When scheduling a new shoot, the gardener-scheduler will filter all seed candidates whose taints are not tolerated by the shoot.
As Gardener&rsquo;s taints/tolerations don&rsquo;t support <code>effect</code>s yet, you can compare this behaviour with using a <code>NoSchedule</code> effect taint in Kubernetes.</p><p>Be reminded that taints/tolerations are no means to define any affinity or selection for seeds - please use <code>.spec.seedSelector</code> in the <code>Shoot</code> to state such desires.</p><p>⚠️ Please note that - unlike how it&rsquo;s implemented in Kubernetes - a certain seed cluster <strong>may</strong> only be used when the shoot tolerates <strong>all</strong> the seed&rsquo;s taints.
This means that specifying <code>.spec.seedName</code> for a seed whose taints are not tolerated will make the gardener-apiserver reject the request.</p><p>Consequently, the taints/tolerations feature can be used as means to restrict usage of certain seeds.</p><h2 id=toleration-defaults-and-whitelist>Toleration Defaults and Whitelist</h2><p>The <code>Project</code> resource features a <code>.spec.tolerations</code> object that may carry <code>defaults</code> and a <code>whitelist</code> (see <a href=https://github.com/gardener/gardener/blob/master/example/05-project-dev.yaml#L33-L37>this example</a>).
The corresponding <code>ShootTolerationRestriction</code> admission plugin (cf. Kubernetes&rsquo; <code>PodTolerationRestriction</code> admission plugin) is responsible for evaluating these settings during creation/update of <code>Shoot</code>s.</p><h3 id=whitelist>Whitelist</h3><p>If a shoot gets created or updated with tolerations, then it is validated that only those tolerations may be used that were added to either a) the <code>Project</code>&rsquo;s <code>.spec.tolerations.whitelist</code>, or b) to the global whitelist in the <code>ShootTolerationRestriction</code>&rsquo;s admission config (see <a href=https://github.com/gardener/gardener/blob/master/example/20-admissionconfig.yaml#L7-L14>this example</a>).</p><p>⚠️ Please note that the tolerations whitelist of <code>Project</code>s can only be changed if the user trying to change it is bound to the <code>modify-spec-tolerations-whitelist</code> custom RBAC role, e.g., via the following <code>ClusterRole</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: rbac.authorization.k8s.io/v1
</span></span><span style=display:flex><span>kind: ClusterRole
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: full-project-modification-access
</span></span><span style=display:flex><span>rules:
</span></span><span style=display:flex><span>- apiGroups:
</span></span><span style=display:flex><span>  - core.gardener.cloud
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>  - projects
</span></span><span style=display:flex><span>  verbs:
</span></span><span style=display:flex><span>  - create
</span></span><span style=display:flex><span>  - patch
</span></span><span style=display:flex><span>  - update
</span></span><span style=display:flex><span>  - modify-spec-tolerations-whitelist
</span></span><span style=display:flex><span>  - delete
</span></span></code></pre></div><h3 id=defaults>Defaults</h3><p>If a shoot gets created, then the default tolerations specified in both the <code>Project</code>&rsquo;s <code>.spec.tolerations.defaults</code> and the global default list in the <code>ShootTolerationRestriction</code> admission plugin&rsquo;s configuration will be added to the <code>.spec.tolerations</code> of the <code>Shoot</code> (unless it already specifies a certain key).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-469f9faaaa43224f631cfa8aa470babf>50 - Topology Aware Routing</h1><h1 id=topology-aware-traffic-routing>Topology-Aware Traffic Routing</h1><h2 id=motivation>Motivation</h2><p>The enablement of <a href=/docs/gardener/usage/shoot_high_availability/>highly available shoot control-planes</a> requires multi-zone seed clusters. A garden runtime cluster can also be a multi-zone cluster. The topology-aware routing is introduced to reduce costs and to improve network performance by avoiding the cross availability zone traffic, if possible. The cross availability zone traffic is charged by the cloud providers and it comes with higher latency compared to the traffic within the same zone. The topology-aware routing feature enables topology-aware routing for <code>Service</code>s deployed in a seed or garden runtime cluster. For the clients consuming these topology-aware services, <code>kube-proxy</code> favors the endpoints which are located in the same zone where the traffic originated from. In this way, the cross availability zone traffic is avoided.</p><h2 id=how-it-works>How it works</h2><p>The topology-aware routing feature relies on the Kubernetes feature <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints/><code>TopologyAwareHints</code></a>.</p><h5 id=endpointslice-hints-mutating-webhook>EndpointSlice Hints Mutating Webhook</h5><p>The component that is responsible for providing hints in the EndpointSlices resources is the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/>kube-controller-manager</a>, in particular this is the <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints/>EndpointSlice controller</a>. However, there are several drawbacks with the TopologyAwareHints feature that don&rsquo;t allow us to use it in its native way:</p><ul><li><p>The algorithm in the EndpointSlice controller is based on a CPU-balance heuristic. From the TopologyAwareHints documentation:</p><blockquote><p>The controller allocates a proportional amount of endpoints to each zone. This proportion is based on the allocatable CPU cores for nodes running in that zone. For example, if one zone had 2 CPU cores and another zone only had 1 CPU core, the controller would allocate twice as many endpoints to the zone with 2 CPU cores.</p></blockquote><p>In case it is not possible to achieve a balanced distribution of the endpoints, as a safeguard mechanism the controller removes hints from the EndpointSlice resource.
In our setup, the clients and the servers are well-known and usually the traffic a component receives does not depend on the zone&rsquo;s allocatable CPU.
Many components deployed by Gardener are scaled automatically by VPA. In case of an overload of a replica, the VPA should provide and apply enhanced CPU and memory resources. Additionally, Gardener uses the cluster-autoscaler to upscale/downscale Nodes dynamically. Hence, it is not possible to ensure a balanced allocatable CPU across the zones.</p></li><li><p>The TopologyAwareHints feature does not work at low-endpoint counts. It falls apart for a Service with less than 10 Endpoints.</p></li><li><p>Hints provided by the EndpointSlice controller are not deterministic. With cluster-autoscaler running and load increasing, hints can be removed in the next moment. There is no option to enforce the zone-level topology.</p></li></ul><p>For more details, see the following issue <a href=https://github.com/kubernetes/kubernetes/issues/113731>kubernetes/kubernetes#113731</a>.</p><p>To circumvent these issues with the EndpointSlice controller, a mutating webhook in the gardener-resource-manager assigns hints to EndpointSlice resources. For each endpoint in the EndpointSlice, it sets the endpoint&rsquo;s hints to the endpoint&rsquo;s zone. The webhook overwrites the hints provided by the EndpointSlice controller in kube-controller-manager. For more details, see the <a href=/docs/gardener/concepts/resource-manager/#endpointslice-hints>webhook&rsquo;s documentation</a>.</p><h5 id=kube-proxy>kube-proxy</h5><p>By default, with kube-proxy running in <code>iptables</code> mode, traffic is distributed randomly across all endpoints, regardless of where it originates from. In a cluster with 3 zones, traffic is more likely to go to another zone than to stay in the current zone.
With the topology-aware routing feature, kube-proxy filters the endpoints it routes to based on the hints in the EndpointSlice resource. In most of the cases, kube-proxy will prefer the endpoint(s) in the same zone. For more details, see the <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints/#implementation-kube-proxy>Kubernetes documentation</a>.</p><h2 id=how-to-make-a-service-topology-aware>How to make a Service topology-aware?</h2><p>To make a Service topology-aware, the following annotation and label have to be added to the Service:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    service.kubernetes.io/topology-aware-hints: <span style=color:#a31515>&#34;auto&#34;</span>
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    endpoint-slice-hints.resources.gardener.cloud/consider: <span style=color:#a31515>&#34;true&#34;</span>
</span></span></code></pre></div><blockquote><p>Note: In Kubernetes 1.27 the <code>service.kubernetes.io/topology-aware-hints=auto</code> annotation is deprecated in favor of the newly introduced <code>service.kubernetes.io/topology-mode=auto</code>. When the runtime cluster&rsquo;s K8s version is >= 1.27, use the <code>service.kubernetes.io/topology-mode=auto</code> annotation. For more details, see the <a href=https://github.com/kubernetes/kubernetes/pull/116522>corresponding upstream PR</a>.</p></blockquote><p>The <code>service.kubernetes.io/topology-aware-hints=auto</code> annotation is needed for kube-proxy. One of the prerequisites on kube-proxy side for using topology-aware routing is the corresponding Service to be annotated with the <code>service.kubernetes.io/topology-aware-hints=auto</code>. For more details, see the following <a href=https://github.com/kubernetes/kubernetes/blob/b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d/pkg/proxy/topology.go#L140-L186>kube-proxy function</a>.
The <code>endpoint-slice-hints.resources.gardener.cloud/consider=true</code> label is needed for gardener-resource-manager to prevent the EndpointSlice hints mutating webhook from selecting all EndpointSlice resources but only the ones that are labeled with the consider label.</p><p>The Gardener extensions can use this approach to make a Service they deploy topology-aware.</p><p>Prerequisites for making a Service topology-aware:</p><ol><li>The Pods backing the Service should be spread on most of the available zones. This constraint should be ensured with appropriate scheduling constraints (topology spread constraints, (anti-)affinity). Enabling the feature for a Service with a single backing Pod or Pods all located in the same zone does not lead to a benefit.</li><li>The component should be scaled up by <code>VerticalPodAutoscaler</code>. In case of an overload (a large portion of the of the traffic is originating from a given zone), the <code>VerticalPodAutoscaler</code> should provide better resource recommendations for the overloaded backing Pods.</li><li>Consider the <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints/#constraints><code>TopologyAwareHints</code> constraints</a>.</li></ol><blockquote><p>Note: The topology-aware routing feature is considered as alpha feature. Use it only for evaluation purposes.</p></blockquote><h2 id=topology-aware-services-in-the-seed-cluster>Topology-aware Services in the Seed cluster</h2><h5 id=etcd-main-client-and-etcd-events-client>etcd-main-client and etcd-events-client</h5><p>The <code>etcd-main-client</code> and <code>etcd-events-client</code> Services are topology-aware. They are consumed by the kube-apiserver.</p><h5 id=kube-apiserver>kube-apiserver</h5><p>The <code>kube-apiserver</code> Service is topology-aware. It is consumed by the controllers running in the Shoot control plane.</p><blockquote><p>Note: The <code>istio-ingressgateway</code> component routes traffic in topology-aware manner - if possible, it routes traffic to the target <code>kube-apiserver</code> Pods in the same zone. If there is no healthy <code>kube-apiserver</code> Pod available in the same zone, the traffic is routed to any of the healthy Pods in the other zones. This behaviour is unconditionally enabled.</p></blockquote><h5 id=gardener-resource-manager>gardener-resource-manager</h5><p>The <code>gardener-resource-manager</code> Service that is part of the Shoot control plane is topology-aware. The resource-manager serves webhooks and the Service is consumed by the kube-apiserver for the webhook communication.</p><h5 id=vpa-webhook>vpa-webhook</h5><p>The <code>vpa-webhook</code> Service that is part of the Shoot control plane is topology-aware. It is consumed by the kube-apiserver for the webhook communication.</p><h2 id=topology-aware-services-in-the-garden-runtime-cluster>Topology-aware Services in the garden runtime cluster</h2><h5 id=virtual-garden-etcd-main-client-and-virtual-garden-etcd-events-client>virtual-garden-etcd-main-client and virtual-garden-etcd-events-client</h5><p>The <code>virtual-garden-etcd-main-client</code> and <code>virtual-garden-etcd-events-client</code> Services are topology-aware. <code>virtual-garden-etcd-main-client</code> is consumed by <code>virtual-garden-kube-apiserver</code> and <code>gardener-apiserver</code>, <code>virtual-garden-etcd-events-client</code> is consumed by <code>virtual-garden-kube-apiserver</code>.</p><h5 id=virtual-garden-kube-apiserver>virtual-garden-kube-apiserver</h5><p>The <code>virtual-garden-kube-apiserver</code> Service is topology-aware. It is consumed by <code>virtual-garden-kube-controller-manager</code>, <code>gardener-controller-manager</code>, <code>gardener-scheduler</code>, <code>gardener-admission-controller</code>, extension admission components, <code>gardener-dashboard</code> and other components.</p><blockquote><p>Note: Unlike the other Services, the <code>virtual-garden-kube-apiserver</code> Service is of type LoadBalancer. In-cluster components consuming the <code>virtual-garden-kube-apiserver</code> Service by its Service name will have benefit from the topology-aware routing. However, the TopologyAwareHints feature cannot help with external traffic routed to load balancer&rsquo;s address - such traffic won&rsquo;t be routed in a topology-aware manner and will be routed according to the cloud-provider specific implementation.</p></blockquote><h5 id=gardener-apiserver>gardener-apiserver</h5><p>The <code>gardener-apiserver</code> Service is topology-aware. It is consumed by <code>virtual-garden-kube-apiserver</code>. The aggregation layer in <code>virtual-garden-kube-apiserver</code> proxies requests sent for the Gardener API types to the <code>gardener-apiserver</code>.</p><h5 id=gardener-admission-controller>gardener-admission-controller</h5><p>The <code>gardener-admission-controller</code> Service is topology-aware. It is consumed by <code>virtual-garden-kube-apiserver</code> and <code>gardener-apiserver</code> for the webhook communication.</p><h2 id=how-to-enable-the-topology-aware-routing-for-a-seed-cluster>How to enable the topology-aware routing for a Seed cluster?</h2><p>For a Seed cluster the topology-aware routing functionality can be enabled in the Seed specification:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Seed
</span></span><span style=display:flex><span><span style=color:green># ...</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  settings:
</span></span><span style=display:flex><span>    topologyAwareRouting:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>The topology-aware routing setting can be only enabled for a Seed cluster with more than one zone.
gardenlet enables topology-aware Services only for Shoot control planes with failure tolerance type <code>zone</code> (<code>.spec.controlPlane.highAvailability.failureTolerance.type=zone</code>). Control plane Pods of non-HA Shoots and HA Shoots with failure tolerance type <code>node</code> are pinned to single zone. For more details, see <a href=/docs/gardener/development/high-availability/>High Availability Of Deployed Components</a>.</p><p>⚠️ For K8s &lt; 1.24 Seed clusters, the topology-aware routing setting requires the Kubernetes <code>TopologyAwareHints</code> feature gate to be enabled for kube-apiserver, kube-controller-manager and kube-proxy. This is required because the <code>TopologyAwareHints</code> feature gate is disabled by default in K8s &lt; 1.24. When <code>TopologyAwareHints</code> is disabled, the kube-apiserver does not allow anything to be persisted in the <code>.endpoints[].hints</code> field in the EndpointSlice resource. Also, the kube-controller-manager removes the hints, hence kube-proxy is not using topology-aware routing.</p><h2 id=how-to-enable-the-topology-aware-routing-for-a-garden-runtime-cluster>How to enable the topology-aware routing for a garden runtime cluster?</h2><p>For a garden runtime cluster the topology-aware routing functionality can be enabled in the Garden resource specification:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: operator.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Garden
</span></span><span style=display:flex><span><span style=color:green># ...</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  runtimeCluster:
</span></span><span style=display:flex><span>    settings:
</span></span><span style=display:flex><span>      topologyAwareRouting:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>The topology-aware routing setting can be only enabled for a garden runtime cluster with more than one zone.</p><p>⚠️ For K8s &lt; 1.24 garden runtime clusters, the topology-aware routing setting requires the Kubernetes <code>TopologyAwareHints</code> feature gate to be enabled for kube-apiserver, kube-controller-manager and kube-proxy. For more details, see the above section.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cf607bc71a1ae64925c002dc4829df51>51 - Trusted Tls For Control Planes</h1><h1 id=trusted-tls-certificate-for-shoot-control-planes>Trusted TLS Certificate for Shoot Control Planes</h1><p>Shoot clusters are composed of several control plane components deployed by Gardener and its corresponding extensions.</p><p>Some components are exposed via <code>Ingress</code> resources, which make them addressable under the HTTPS protocol.</p><p>Examples:</p><ul><li>Alertmanager</li><li>Plutono</li><li>Prometheus</li></ul><p>Gardener generates the backing TLS certificates, which are signed by the shoot cluster&rsquo;s CA by default (self-signed).</p><p>Unlike with a self-contained Kubeconfig file, common internet browsers or operating systems don&rsquo;t trust a shoot&rsquo;s cluster CA and adding it as a trusted root is often undesired in enterprise environments.</p><p>Therefore, Gardener operators can predefine trusted wildcard certificates under which the mentioned endpoints will be served instead.</p><h2 id=register-a-trusted-wildcard-certificate>Register a trusted wildcard certificate</h2><p>Since control plane components are published under the ingress domain (<code>core.gardener.cloud/v1beta1.Seed.spec.ingress.domain</code>) a wildcard certificate is required.</p><p>For example:</p><ul><li>Seed ingress domain: <code>dev.my-seed.example.com</code></li><li><code>CN</code> or <code>SAN</code> for a certificate: <code>*.dev.my-seed.example.com</code></li></ul><p>A wildcard certificate matches exactly one seed. It must be deployed as part of your landscape setup as a Kubernetes <code>Secret</code> inside the <code>garden</code> namespace of the corresponding seed cluster.</p><p>Please ensure that the secret has the <code>gardener.cloud/role</code> label shown below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  ca.crt: base64-encoded-ca.crt
</span></span><span style=display:flex><span>  tls.crt: base64-encoded-tls.crt
</span></span><span style=display:flex><span>  tls.key: base64-encoded-tls.key
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    gardener.cloud/role: controlplane-cert
</span></span><span style=display:flex><span>  name: seed-ingress-certificate
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>type: Opaque
</span></span></code></pre></div><p>Gardener copies the secret during the reconciliation of shoot clusters to the shoot namespace in the seed. Afterwards, the <code>Ingress</code> resources in that namespace for the mentioned components will refer to the wildcard certificate.</p><h2 id=best-practice>Best Practice</h2><p>While it is possible to create the wildcard certificates manually and deploy them to seed clusters, it is recommended to let certificate management components do this job. Often, a seed cluster is also a shoot cluster at the same time (ManagedSeed) and might already provide a certificate service extension.
Otherwise, a Gardener operator may use solutions like <a href=https://github.com/gardener/cert-management>Cert-Management</a> or <a href=https://github.com/jetstack/cert-manager>Cert-Manager</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2a827ed3506304fa29f2a71110704fb0>52 - Worker Pool K8s Versions</h1><h1 id=controlling-the-kubernetes-versions-for-specific-worker-pools>Controlling the Kubernetes Versions for Specific Worker Pools</h1><p>Since Gardener <code>v1.36</code>, worker pools can have different Kubernetes versions specified than the control plane.</p><p>In earlier Gardener versions, all worker pools inherited the Kubernetes version of the control plane. Once the Kubernetes version of the control plane was modified, all worker pools have been updated as well (either by rolling the nodes in case of a minor version change, or in-place for patch version changes).</p><p>In order to gracefully perform Kubernetes upgrades (triggering a rolling update of the nodes) with workloads sensitive to restarts (e.g., those dealing with lots of data), it might be required to be able to gradually perform the upgrade process.
In such cases, the Kubernetes version for the worker pools can be pinned (<code>.spec.provider.workers[].kubernetes.version</code>) while the control plane Kubernetes version (<code>.spec.kubernetes.version</code>) is updated.
This results in the nodes being untouched while the control plane is upgraded.
Now a new worker pool (with the version equal to the control plane version) can be added.
Administrators can then reschedule their workloads to the new worker pool according to their upgrade requirements and processes.</p><h2 id=example-usage-in-a-shoot>Example Usage in a <code>Shoot</code></h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.6
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: data1
</span></span><span style=display:flex><span>      kubernetes:
</span></span><span style=display:flex><span>        version: 1.23.13
</span></span><span style=display:flex><span>    - name: data2
</span></span></code></pre></div><ul><li>If <code>.kubernetes.version</code> is not specified in a worker pool, then the Kubernetes version of the kubelet is inherited from the control plane (<code>.spec.kubernetes.version</code>), i.e., in the above example, the <code>data2</code> pool will use <code>1.24.6</code>.</li><li>If <code>.kubernetes.version</code> is specified in a worker pool, then it must meet the following constraints:<ul><li>It must be at most two minor versions lower than the control plane version.</li><li>If it was not specified before, then no downgrade is possible (you cannot set it to <code>1.23.13</code> while <code>.spec.kubernetes.version</code> is already <code>1.24.6</code>). The &ldquo;two minor version skew&rdquo; is only possible if the worker pool version is set to the control plane version and then the control plane was updated gradually by two minor versions.</li><li>If the version is removed from the worker pool, only one minor version difference is allowed to the control plane (you cannot upgrade a pool from version <code>1.22.0</code> to <code>1.24.0</code> in one go).</li></ul></li></ul><p>Automatic updates of Kubernetes versions (see <a href=/docs/gardener/usage/shoot_maintenance/#automatic-version-updates>Shoot Maintenance</a>) also apply to worker pool Kubernetes versions.</p></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2023 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid@8.13.4/dist/mermaid.min.js integrity="sha512-JERecFUBbsm75UpkVheAuDOE8NdHjQBrPACfEQYPwvPG+fjgCpHAz1Jw2ci9EXmd3DdfiWth3O3CQvcfEg8gsA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.7b24c0fb082ffb2de6cb14d6c95e9f8053053709ffcf8c761ef8e9ad2f8021e4.js integrity="sha256-eyTA+wgv+y3myxTWyV6fgFMFNwn/z4x2HvjprS+AIeQ=" crossorigin=anonymous></script></body></html>