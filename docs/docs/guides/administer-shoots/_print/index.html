<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://gardener.cloud/docs/guides/administer-shoots/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/guides/administer-shoots/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Administer Client (Shoot) Clusters | Gardener</title>
<meta name=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:url" content="https://gardener.cloud/docs/guides/administer-shoots/"><meta property="og:site_name" content="Gardener"><meta property="og:title" content="Administer Client (Shoot) Clusters"><meta property="og:description" content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:locale" content="en_US"><meta property="og:type" content="website"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="Administer Client (Shoot) Clusters"><meta itemprop=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta itemprop=datePublished content="2023-05-23T00:00:00+00:00"><meta itemprop=dateModified content="2023-05-23T00:00:00+00:00"><meta itemprop=image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="Administer Client (Shoot) Clusters"><meta name=twitter:description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><link rel=preload href=/scss/main.min.26acfdfb147ebf2e823b350f7e183142a26f105f5e474d2e37605e9c55009e66.css as=style integrity="sha256-Jqz9+xR+vy6COzUPfhgxQqJvEF9eR00uN2BenFUAnmY=" crossorigin=anonymous><link href=/scss/main.min.26acfdfb147ebf2e823b350f7e183142a26f105f5e474d2e37605e9c55009e66.css rel=stylesheet integrity="sha256-Jqz9+xR+vy6COzUPfhgxQqJvEF9eR00uN2BenFUAnmY=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg width="90" height="90" viewBox="0 0 90 90" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=navbar-brand__name>Gardener</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://demo.gardener.cloud target=_blank rel=noopener><span>Demo</span></a></li><li class=nav-item><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class=nav-item><a class=nav-link href=/docs><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/blog><span>Blogs</span></a></li><li class=nav-item><a class=nav-link href=/community><span>Community</span></a></li><li class=nav-item><a class=nav-link href=https://join.slack.com/t/gardener-cloud/shared_invite/zt-33c9daems-3oOorhnqOSnldZPWqGmIBw target=_blank rel=noopener><span>Join us on</span></a></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.e284a7168da6d256c7837a634942e0bc.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/guides/administer-shoots/>Return to the regular view of this page</a>.</p></div><h1 class=title>Administer Client (Shoot) Clusters</h1><div class=content></div></div><div class=td-content><h1 id=pg-2bdac0ad92d87be01d23f48b7244c9fd>1 - Scalability of Gardener Managed Kubernetes Clusters</h1><div class=lead>Know the boundary conditions when scaling your workloads</div><p>Have you ever wondered how much more your Kubernetes cluster can scale before it breaks down?</p><p>Of course, the answer is heavily dependent on your workloads. But be assured, any cluster will break eventually. Therefore, the best mitigation is to plan for sharding early and run multiple clusters instead of trying to optimize everything hoping to survive with a single cluster.
Still, it is helpful to know when the time has come to scale out. This document aims at giving you the basic knowledge to keep a Gardener-managed Kubernetes cluster up and running while it scales according to your needs.</p><h2 id=welcome-to-planet-scale-please-mind-the-gap>Welcome to Planet Scale, Please Mind the Gap!<a class=td-heading-self-link href=#welcome-to-planet-scale-please-mind-the-gap aria-label="Heading self-link"></a></h2><p>For a complex, distributed system like Kubernetes it is impossible to give absolute thresholds for its scalability. Instead, the limit of a cluster&rsquo;s scalability is a combination of various, interconnected dimensions.</p><p>Let&rsquo;s take a rather simple example of two dimensions - the number of <code>Pods</code> per <code>Node</code> and number of <code>Nodes</code> in a cluster. According to the <a href=https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md>scalability thresholds documentation</a>, Kubernetes can scale up to 5000 <code>Nodes</code> and with default settings accommodate a maximum of 110 <code>Pods</code> on a single <code>Node</code>. Pushing only a single dimension towards its limit will likely harm the cluster. But if both are pushed simultaneously, any cluster will break way before reaching one dimension&rsquo;s limit.</p><p><img src=/__resources/pod-nodes_09aa76.png alt="Pods and Nodes"></p><p>What sounds rather straightforward in theory can be a bit trickier in reality. While 110 <code>Pods</code> is the default limit, we successfully pushed beyond that and in certain cases run up to 200 <code>Pods</code> per <code>Node</code> without breaking the cluster. This is possible in an environment where one knows and controls all workloads and cluster configurations. It still requires careful testing, though, and comes at the cost of limiting the scalability of other dimensions, like the number of <code>Nodes</code>.</p><p>Of course, a Kubernetes cluster has a plethora of dimensions. Thus, when looking at a simple questions like <em>&ldquo;How many resources can I store in ETCD?&rdquo;</em>, the only meaningful answer must be: <em>&ldquo;it depends&rdquo;</em></p><p>The following sections will help you to identify relevant dimensions and how they affect a Gardener-managed Kubernetes cluster&rsquo;s scalability.</p><h2 id=official-kubernetes-thresholds-and-scalability-considerations>&ldquo;Official&rdquo; Kubernetes Thresholds and Scalability Considerations<a class=td-heading-self-link href=#official-kubernetes-thresholds-and-scalability-considerations aria-label="Heading self-link"></a></h2><p>To get started with the topic, please check the basic guidance provided by the Kubernetes community (specifically SIG Scalability):</p><ul><li><a href=https://github.com/kubernetes/community/blob/master/sig-scalability/slos/slos.md#how-we-define-scalability>How we define scalability?</a></li><li><a href=https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md>Kubernetes Scalability Thresholds</a></li></ul><p>Furthermore, the problem space has been discussed in a <a href="https://www.youtube.com/watch?v=t_Ww6ELKl4Q">KubeCon talk</a>, the slides for which can be found <a href=https://docs.google.com/presentation/d/1aWjxpY4YJ4KJQUTqaVHdR4sbhwqDiW30EF4_hGCc-gI>here</a>. You should at least read the slides before continuing.</p><p>Essentially, it comes down to this:</p><blockquote><p>If you promise to:</p><ul><li>correctly configure your cluster</li><li>use extensibility features &ldquo;reasonably&rdquo;</li><li>keep the load in the cluster within recommended limits</li></ul><p>Then we promise that your cluster will function properly.</p></blockquote><p>With that knowledge in mind, let&rsquo;s look at Gardener and eventually pick up the question about the number of objects in ETCD raised above.</p><h2 id=gardener-specific-considerations>Gardener-Specific Considerations<a class=td-heading-self-link href=#gardener-specific-considerations aria-label="Heading self-link"></a></h2><p>The following considerations are based on experience with various large clusters that scaled in different dimensions. Just as explained above, pushing beyond even one of the limits is likely to cause issues at some point in time (but not guaranteed). Depending on the setup of your workloads however, it might work unexpectedly well. Nevertheless, we urge you take conscious decisions and rather think about sharding your workloads. Please keep in mind - your workload affects the overall stability and scalability of a cluster significantly.</p><h3 id=etcd>ETCD<a class=td-heading-self-link href=#etcd aria-label="Heading self-link"></a></h3><p><strong>The following section is based on a setup where ETCD <code>Pods</code> run on a dedicated <code>Node</code> pool and each <code>Node</code> has 8 vCPU and 32GB memory at least.</strong></p><p>ETCD has a practical space limit of 8 GB. It caps the number of objects one can technically have in a Kubernetes cluster.</p><p>Of course, the number is heavily influenced by each object&rsquo;s size, especially when considering that secrets and configmaps may store up to 1MB of data. Another dimension is a cluster&rsquo;s churn rate. Since ETCD stores a history of the keyspace, a higher churn rate reduces the number of objects. Gardener runs <a href=https://etcd.io/docs/v3.4/op-guide/maintenance/#history-compaction>compaction</a> every 30min and <a href=https://etcd.io/docs/v3.4/op-guide/maintenance/#defragmentation>defragmentation</a> once per day during a cluster&rsquo;s maintenance window to ensure proper ETCD operations. However, it is still possible to overload ETCD. If the space limit is reached, ETCD will only accept <code>READ</code> or <code>DELETE</code> requests and manual interaction by a Gardener operator is needed to disarm the alarm, once you got below the threshold.</p><p>To avoid such a situation, you can monitor the current ETCD usage via the &ldquo;ETCD&rdquo; dashboard of the monitoring stack. It gives you the current DB size, as well as historical data for the past 2 weeks. While there are improvements planned to trigger compaction and defragmentation based on DB size, an ETCD should not grow up to this threshold. A typical, healthy DB size is less than 3 GB.</p><p>Furthermore, the dashboard has a panel called &ldquo;Memory&rdquo;, which indicates the memory usage of the etcd pod(s). Using more than 16GB memory is a clear red flag, and you should reduce the load on ETCD.</p><p>Another dimension you should be aware of is the object count in ETCD. You can check it via the &ldquo;API Server&rdquo; dashboard, which features a &ldquo;ETCD Object Counts By Resource&rdquo; panel. The overall number of objects (excluding <code>events</code>, as they are stored in a different etcd instance) should not exceed 100k for most use cases.</p><h3 id=kube-api-server>Kube API Server<a class=td-heading-self-link href=#kube-api-server aria-label="Heading self-link"></a></h3><p><strong>The following section is based on a setup where <code>kube-apiserver</code> run as <code>Pods</code> and are scheduled to <code>Nodes</code> with at least 8 vCPU and 32GB memory.</strong></p><p>Gardener can scale the <code>Deployment</code> of a <code>kube-apiserver</code> horizontally and vertically. Horizontal scaling is limited to a certain number of replicas and should not concern a stakeholder much. However, the CPU / memory consumption of an individual <code>kube-apiserver</code> pod poses a potential threat to the overall availability of your cluster. The vertical scaling of any <code>kube-apiserver</code> is limited by the amount of resources available on a single <code>Node</code>. Outgrowing the resources of a <code>Node</code> will cause a downtime and render the cluster unavailable.</p><p>In general, continuous CPU usage of up to 3 cores and 16 GB memory per <code>kube-apiserver</code> pod is considered to be safe. This gives some room to absorb spikes, for example when the caches are initialized. You can check the resource consumption by selecting <code>kube-apiserver</code> <code>Pods</code> in the &ldquo;Kubernetes <code>Pods</code>&rdquo; dashboard. If these boundaries are exceeded constantly, you need to investigate and derive measures to lower the load.</p><p>Further information is also recorded and made available through the monitoring stack. The dashboard &ldquo;API Server Request Duration and Response Size&rdquo; provides insights into the request processing time of <code>kube-apiserver</code> <code>Pods</code>. Related information like request rates, dropped requests or termination codes (e.g., <code>429</code> for too many requests) can be obtained from the dashboards &ldquo;API Server&rdquo; and &ldquo;Kubernetes API Server Details&rdquo;. They provide a good indicator for how well the system is dealing with its current load.</p><p>Reducing the load on the API servers can become a challenge. To get started, you may try to:</p><ul><li>Use immutable <a href=https://kubernetes.io/docs/concepts/configuration/secret/#secret-immutable>secrets</a> and <a href=https://kubernetes.io/docs/concepts/configuration/configmap/#configmap-immutable>configmaps</a> where possible to save watches. This pays off, especially when you have a high number of <code>Nodes</code> or just lots of secrets in general.</li><li>Applications interacting with the K8s API: If you know an object by its name, use it. Using label selector queries is expensive, as the filtering happens only within the <code>kube-apiserver</code> and not <code>etcd</code>, hence all resources must first pass completely from <code>etcd</code> to <code>kube-apiserver</code>.</li><li>Use (single object) caches within your controllers. Check the <a href=https://github.com/gardener/gardener/issues/7593>&ldquo;Use cache for ShootStates in Gardenlet&rdquo; issue</a> for an example.</li></ul><h3 id=nodes><code>Nodes</code><a class=td-heading-self-link href=#nodes aria-label="Heading self-link"></a></h3><p>When talking about the scalability of a Kubernetes cluster, <code>Nodes</code> are probably mentioned in the first place&mldr; well, obviously not in this guide. While vanilla Kubernetes lists 5000 <code>Nodes</code> as its upper limit, pushing that dimension is not feasible. Most clusters should run with fewer than 300 <code>Nodes</code>. But of course, the actual limit depends on the workloads deployed and can be lower or higher. As you scale your cluster, be extra careful and closely monitor ETCD and <code>kube-apiserver</code>.</p><p>The scalability of <code>Nodes</code> is subject to a range of limiting factors. Some of them can only be defined upon cluster creation and remain immutable during a cluster lifetime. So let&rsquo;s discuss the most important dimensions.</p><p><strong>CIDR</strong>:</p><p>Upon cluster creation, you have to specify or use the default values for several network segments. There are dedicated CIDRs for services, <code>Pods</code>, and <code>Nodes</code>. Each defines a range of IP addresses available for the individual resource type. Obviously, the maximum of possible <code>Nodes</code> is capped by the CIDR for <code>Nodes</code>.
However, there is a second limiting factor, which is the pod CIDR combined with the <code>nodeCIDRMaskSize</code>. This mask is used to divide the pod CIDR into smaller subnets, where each blocks gets assigned to a node. With a <code>/16</code> pod network and a <code>/24</code> nodeCIDRMaskSize, a cluster can scale up to 256 <code>Nodes</code>. Please check <a href=/docs/gardener/networking/shoot_networking/>Shoot Networking</a> for details.</p><p>Even though a <code>/24</code> nodeCIDRMaskSize translates to a theoretical 256 pod IP addresses per <code>Node</code>, the <code>maxPods</code> setting should be less than 1/2 of this value. This gives the system some breathing room for churn and minimizes the risk for strange effects like mis-routed packages caused by immediate re-use of IPs.</p><p><strong>Cloud provider capacity</strong>:</p><p>Most of the time, <code>Nodes</code> in Kubernetes translate to virtual machines on a hyperscaler. An attempt to add more <code>Nodes</code> to a cluster might fail due to capacity issues resulting in an error message like this:</p><pre tabindex=0><code>Cloud provider message - machine codes error: code = [Internal] message = [InsufficientInstanceCapacity: We currently do not have sufficient &lt;instance type&gt; capacity in the Availability Zone you requested. Our system will be working on provisioning additional capacity. 
</code></pre><p>In heavily utilized regions, individual clusters are competing for scarce resources. So before choosing a region / zone, try to ensure that the hyperscaler supports your anticipated growth. This might be done through quota requests or by contacting the respective support teams.
To mitigate such a situation, you may configure a worker pool with a different <code>Node</code> type and a corresponding <a href=https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/expander/priority/readme.md>priority expander</a> as part of a <a href=/docs/gardener/api-reference/core/#clusterautoscaler>shoot&rsquo;s autoscaler section</a>. Please consult the <a href=https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders>Autoscaler FAQ</a> for more details.</p><p><strong>Rolling of <code>Node</code> pools</strong>:</p><p>The overall number of <code>Nodes</code> is affecting the duration of a cluster&rsquo;s maintenance. When upgrading a <code>Node</code> pool to a new OS image or Kubernetes version, all machines will be drained and deleted, and replaced with new ones. The more <code>Nodes</code> a cluster has, the longer this process will take, given that workloads are typically protected by <code>PodDisruptionBudgets</code>. Check <a href=/docs/gardener/shoot-operations/shoot_updates/>Shoot Updates and Upgrades</a> for details. Be sure to take this into consideration when planning maintenance.</p><p><strong>Root disk</strong>:</p><p>You should be aware that the <code>Node</code> configuration impacts your workload&rsquo;s performance too. Take the root disk of a <code>Node</code>, for example. While most hyperscalers offer the usage of HDD and SSD disks, it is strongly recommended to use SSD volumes as root disks. When there are lots of <code>Pods</code> on a <code>Node</code> or workloads making extensive use of <code>emptyDir</code> volumes, disk throttling becomes an issue. When a disk hits its IOPS limits, processes are stuck in IO-wait and slow down significantly. This can lead to a slow-down in the kubelet&rsquo;s heartbeat mechanism and result in <code>Nodes</code> being replaced automatically, as they appear to be unhealthy. To analyze such a situation, you might have to run tools like <code>iostat</code>, <code>sar</code> or <code>top</code> directly on a <code>Node</code>.</p><p>Switching to an I/O optimized instance type (if offered for your infrastructure) can help to resolve issue. Please keep in mind that disks used via <code>PersistentVolumeClaims</code> have I/O limits as well. Sometimes these limits are related to the size and/or can be increased for individual disks.</p><h3 id=cloud-provider-infrastructure-limits>Cloud Provider (Infrastructure) Limits<a class=td-heading-self-link href=#cloud-provider-infrastructure-limits aria-label="Heading self-link"></a></h3><p>In addition to the already mentioned capacity restrictions, a cloud provider may impose other limitations to a Kubernetes cluster&rsquo;s scalability. One category is the account quota defining the number of resources allowed globally or per region. Make sure to request appropriate values that suit your needs and contain a buffer, for example for having more <code>Nodes</code> during a rolling update.</p><p>Another dimension is the network throughput per VM or network interface. While you may be able to choose a network-optimized <code>Node</code> type for your workload to mitigate issues, you cannot influence the available bandwidth for control plane components. Therefore, please ensure that the traffic on the ETCD does not exceed 100MB/s. The ETCD dashboard provides data for monitoring this metric.</p><p>In some environments the upstream DNS might become an issue too and make your workloads subject to rate limiting. Given the heterogeneity of cloud providers incl. private data centers, it is not possible to give any thresholds. Still, the &ldquo;CoreDNS&rdquo; and &ldquo;NodeLocalDNS&rdquo; dashboards can help to derive a workload&rsquo;s usage pattern. Check the <a href=/docs/gardener/autoscaling/dns-autoscaling/>DNS autoscaling</a> and <a href=/docs/gardener/networking/node-local-dns/>NodeLocalDNS</a> documentations for available configuration options.</p><h3 id=webhooks>Webhooks<a class=td-heading-self-link href=#webhooks aria-label="Heading self-link"></a></h3><p>While webhooks provide powerful means to manage a cluster, they are equally powerful in breaking a cluster upon a malfunction or unavailability. Imagine using a policy enforcing system like <a href=https://kyverno.io/docs/>Kyverno</a> or <a href=https://open-policy-agent.github.io/gatekeeper/website/docs/>Open Policy Agent Gatekeeper</a>. As part of the stack, both will deploy webhooks which are invoked for almost everything that happens in a cluster. Now, if this webhook gets either overloaded or is simply not available, the cluster will stop functioning properly.</p><p>Hence, you have to ensure proper sizing, quick processing time, and availability of the webhook serving <code>Pods</code> when deploying webhooks. Please consult Dynamic Admission Control (<a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#availability>Availability</a> and <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#timeouts>Timeouts</a> sections) for details. You should also be aware of the time added to any request that has to go through a webhook, as the <code>kube-apiserver</code> sends the request for mutation / validation to another pod and waits for the response. The more resources being subject to an external webhook, the more likely this will become a bottleneck when having a high churn rate on resources. Within the Gardener monitoring stack, you can check the extra time per webhook via the &ldquo;API Server (Admission Details)&rdquo; dashboard, which has a panel for &ldquo;Duration per Webhook&rdquo;.</p><p>In Gardener, any webhook timeout should be less than 15 seconds. Due to the separation of Kubernetes data-plane (shoot) and control-plane (seed) in Gardener, the extra hop from <code>kube-apiserver</code> (control-plane) to webhook (data-plane) is more expensive. Please check <a href=/docs/gardener/shoot/shoot_status/>Shoot Status</a> for more details.</p><h3 id=custom-resource-definitions>Custom Resource Definitions<a class=td-heading-self-link href=#custom-resource-definitions aria-label="Heading self-link"></a></h3><p>Using Custom Resource Definitions (CRD) to extend a cluster&rsquo;s API is a common Kubernetes pattern and so is writing an operator to act upon custom resources. Writing an efficient controller reduces the load on the <code>kube-apiserver</code> and allows for better scaling. As a starting point, you might want to read Gardener&rsquo;s <a href=/docs/gardener/kubernetes-clients/>Kubernetes Clients Guide</a>.</p><p>Another problematic dimension is the usage of <a href=https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#webhook-conversion>conversion webhooks</a> when having resources stored in different versions. Not only do they add latency (see <a href=/docs/guides/administer-shoots/scalability/#webhooks>Webhooks</a>) but can also block the kube-controllermanager&rsquo;s garbage collection. If a conversion webhook is unavailable, the garbage collector fails to list all resources and does not perform any cleanup. In order to avoid such a situation, it is highly recommended to use conversion webhooks only when necessary and complete the migration to a new version as soon as possible.</p><h2 id=conclusion>Conclusion<a class=td-heading-self-link href=#conclusion aria-label="Heading self-link"></a></h2><p>As outlined by SIG Scalability, it is quite impossible to give limits or even recommendations fitting every individual use case. Instead, this guide outlines relevant dimensions and gives rather conservative recommendations based on usage patterns observed. By combining this information, it is possible to operate and scale a cluster in stable manner.</p><p>While going beyond is certainly possible for some dimensions, it significantly increases the risk of instability. Typically, limits on the control-plane are introduced by the availability of resources like CPU or memory on a single machine and can hardly be influenced by any user. Therefore, utilizing the existing resources efficiently is key. Other parameters are controlled by a user. In these cases, careful testing may reveal actual limits for a specific use case.</p><p>Please keep in mind that all aspects of a workload greatly influence the stability and scalability of a Kubernetes cluster.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-6695d22e43168eb0e7d4eafa56586373>2 - Authenticating with an Identity Provider</h1><div class=lead>Use OpenID Connect to authenticate users to access shoot clusters</div><h2 id=prerequisites>Prerequisites<a class=td-heading-self-link href=#prerequisites aria-label="Heading self-link"></a></h2><p>Please read the following background material on <a href=https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens>OIDC tokens</a> and <a href=https://kubernetes.io/docs/reference/access-authn-authz/authentication/#using-authentication-configuration>Structured Authentication</a>.</p><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>Kubernetes on its own doesn’t provide any user management. In other words, users aren’t managed through Kubernetes resources. Whenever you refer to a human user it’s sufficient to use a unique ID, for example, an email address. Nevertheless, Gardener project owners can use an identity provider to authenticate user access for shoot clusters in the following way:</p><ol><li><a href=/docs/guides/administer-shoots/oidc-login/#configure-an-identity-provider>Configure an Identity Provider</a> using <strong>OpenID Connect</strong> (OIDC).</li><li><a href=/docs/guides/administer-shoots/oidc-login/#configure-a-local-kubectl-oidc-login>Configure a local kubectl oidc-login</a> to enable <code>oidc-login</code>.</li><li><a href=/docs/guides/administer-shoots/oidc-login/#configure-the-shoot-cluster>Configure the shoot cluster</a> to share details of the OIDC-compliant identity provider with the Kubernetes API Server.</li><li><a href=/docs/guides/administer-shoots/oidc-login/#authorize-an-authenticated-user>Authorize an authenticated user</a> using role-based access control (RBAC).</li><li><a href=/docs/guides/administer-shoots/oidc-login/#verify-the-result>Verify the result</a></li></ol><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>Gardener allows administrators to modify aspects of the control plane setup.
It gives administrators full control of how the control plane is parameterized.
While this offers much flexibility, administrators need to ensure that they don’t configure a control plane that goes beyond the service level agreements of the responsible operators team.</p></blockquote><h2 id=configure-an-identity-provider>Configure an Identity Provider<a class=td-heading-self-link href=#configure-an-identity-provider aria-label="Heading self-link"></a></h2><p>Create a tenant in an OIDC compatible Identity Provider. For simplicity, we use <em>Auth0</em>, which has a free plan.</p><ol><li><p>In your tenant, create a client application to use authentication with <code>kubectl</code>:</p><p><img src=/__resources/Create-client-application_9d175d.png alt="Create client application"></p></li><li><p>Provide a <em>Name</em>, choose <em>Native</em> as application type, and choose <em>CREATE</em>.</p><p><img src=/__resources/Choose-application-type_bd5efb.png alt="Choose application type"></p></li><li><p>In the tab <em>Settings</em>, copy the following parameters to a local text file:</p><ul><li><p><em>Domain</em></p><p>Corresponds to the <strong>issuer</strong> in OIDC. It must be an <code>https</code>-secured endpoint (Auth0 requires a trailing <code>/</code> at the end). For more information, see <a href=https://openid.net/specs/openid-connect-core-1_0.html#Terminology>Issuer Identifier</a>.</p></li><li><p><em>Client ID</em></p></li><li><p><em>Client Secret</em></p><p><img src=/__resources/Basic-information_ff08dc.png alt="Basic information"></p></li></ul></li><li><p>Configure the client to have a callback url of <code>http://localhost:8000</code>. This callback connects to your local <code>kubectl oidc-login</code> plugin:</p><p><img src=/__resources/Configure-callback_fac02d.png alt="Configure callback"></p></li><li><p>Save your changes.</p></li><li><p>Verify that <code>https://&lt;Auth0 Domain>/.well-known/openid-configuration</code> is reachable.</p></li><li><p>Choose <em>Users & Roles</em> > <em>Users</em> > <em>CREATE USERS</em> to create a user with a user and password:</p><p><img src=/__resources/Create-user_6a8379.png alt="Create user"></p></li></ol><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>Users must have a <em>verified</em> email address.</p></blockquote><h2 id=configure-a-local-kubectl-oidc-login>Configure a Local <code>kubectl</code> <code>oidc-login</code><a class=td-heading-self-link href=#configure-a-local-kubectl-oidc-login aria-label="Heading self-link"></a></h2><ol><li><p>Install the <code>kubectl</code> plugin <a href=https://github.com/int128/kubelogin>oidc-login</a>. We highly recommend the <a href=https://github.com/kubernetes-sigs/krew>krew</a> installation tool, which also makes other plugins easily available.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl krew install oidc-login
</span></span></code></pre></div><p>The response looks like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>Updated the local copy of plugin index.
</span></span><span style=display:flex><span>Installing plugin: oidc-login
</span></span><span style=display:flex><span>CAVEATS:
</span></span><span style=display:flex><span>\
</span></span><span style=display:flex><span>|  You need to setup the OIDC provider, Kubernetes API server, role binding and kubeconfig.
</span></span><span style=display:flex><span>|  See https://github.com/int128/kubelogin for more.
</span></span><span style=display:flex><span>/
</span></span><span style=display:flex><span>Installed plugin: oidc-login
</span></span></code></pre></div></li><li><p>Prepare a <code>kubeconfig</code> for later use:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>cp ~/.kube/config ~/.kube/config-oidc
</span></span></code></pre></div></li><li><p>Modify the configuration of <code>~/.kube/config-oidc</code> as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: shoot--project--mycluster
</span></span><span style=display:flex><span>    user: my-oidc
</span></span><span style=display:flex><span>  name: shoot--project--mycluster
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: my-oidc
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    exec:
</span></span><span style=display:flex><span>      apiVersion: client.authentication.k8s.io/v1beta1
</span></span><span style=display:flex><span>      command: kubectl
</span></span><span style=display:flex><span>      args:
</span></span><span style=display:flex><span>      - oidc-login
</span></span><span style=display:flex><span>      - get-token
</span></span><span style=display:flex><span>      - --oidc-issuer-url=https://&lt;Issuer&gt;/ 
</span></span><span style=display:flex><span>      - --oidc-client-id=&lt;Client ID&gt;
</span></span><span style=display:flex><span>      - --oidc-client-secret=&lt;Client Secret&gt;
</span></span><span style=display:flex><span>      - --oidc-extra-scope=email,offline_access,profile
</span></span></code></pre></div></li></ol><p>To test our OIDC-based authentication, the context <code>shoot--project--mycluster</code> of <code>~/.kube/config-oidc</code> is used in a later step. For now, continue to use the configuration <code>~/.kube/config</code> with administration rights for your cluster.</p><h2 id=configure-the-shoot-cluster>Configure the Shoot Cluster<a class=td-heading-self-link href=#configure-the-shoot-cluster aria-label="Heading self-link"></a></h2><p>Create a <code>AuthenticationConfiguration</code> configmap in the project&rsquo;s namespace.
For more options, check out <a href=https://gardener.cloud/docs/gardener/shoot/shoot_access/#structured-authentication>Gardener Structured Authentication</a> and <a href=https://kubernetes.io/docs/reference/access-authn-authz/authentication/#using-authentication-configuration>Kubernetes Structured Authentication</a> documentation.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: authentication-config
</span></span><span style=display:flex><span>  namespace: garden-project
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  config.yaml: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    apiVersion: apiserver.config.k8s.io/v1beta1
</span></span></span><span style=display:flex><span><span style=color:#a31515>    kind: AuthenticationConfiguration
</span></span></span><span style=display:flex><span><span style=color:#a31515>    jwt:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    - issuer:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        url: https://&lt;Issuer&gt;/
</span></span></span><span style=display:flex><span><span style=color:#a31515>        audiences:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        - &lt;Client ID&gt;
</span></span></span><span style=display:flex><span><span style=color:#a31515>      claimMappings:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        username:
</span></span></span><span style=display:flex><span><span style=color:#a31515>          claim: &#39;email&#39;
</span></span></span><span style=display:flex><span><span style=color:#a31515>        prefix: &#39;unique-issuer-identifier:&#39;</span>    
</span></span></code></pre></div><p>Modify the shoot cluster YAML as follows, using the client ID and the domain (as issuer) from the settings of the client application you created in Auth0:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>apiVersion: garden.sapcloud.io/v1beta1
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: mycluster
</span></span><span style=display:flex><span>  namespace: garden-project
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeAPIServer:
</span></span><span style=display:flex><span>      structuredAuthentication:
</span></span><span style=display:flex><span>        configMapName: authentication-config
</span></span></code></pre></div><p>This change of the <code>Shoot</code> manifest triggers a reconciliation. Once the reconciliation is finished, your OIDC configuration is applied. It <strong>doesn&rsquo;t</strong> invalidate other certificate-based authentication methods. Wait for Gardener to reconcile the change. It can take up to 5 minutes.</p><h2 id=authorize-an-authenticated-user>Authorize an Authenticated User<a class=td-heading-self-link href=#authorize-an-authenticated-user aria-label="Heading self-link"></a></h2><p>In Auth0, you created a user with a verified email address, <code>test@test.com</code> in our example. For simplicity, we authorize a single user identified by this email address with the cluster role <code>view</code>:</p><blockquote class="alert alert-important"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>message-outline</title><path d="M20 2H4C2.9 2 2 2.9 2 4V22l4-4H20C21.1 18 22 17.1 22 16V4C22 2.9 21.1 2 20 2m0 14H5.2L4 17.2V4H20V16z"/></svg><p>Important</p></div><p>Do not forget to add the unique prefix to the name of the user.</p></blockquote><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: rbac.authorization.k8s.io/v1
</span></span><span style=display:flex><span>kind: ClusterRoleBinding
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: viewer-test
</span></span><span style=display:flex><span>roleRef:
</span></span><span style=display:flex><span>  apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>  kind: ClusterRole
</span></span><span style=display:flex><span>  name: view
</span></span><span style=display:flex><span>subjects:
</span></span><span style=display:flex><span>- apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>  kind: User
</span></span><span style=display:flex><span>  name: unique-issuer-identifier:test@test.com
</span></span></code></pre></div><p>As administrator, apply the cluster role binding in your shoot cluster.</p><h2 id=verify-the-result>Verify the Result<a class=td-heading-self-link href=#verify-the-result aria-label="Heading self-link"></a></h2><ol><li><p>To step into the shoes of your user, use the prepared <code>kubeconfig</code> file <code>~/.kube/config-oidc</code>, and switch to the context that uses <code>oidc-login</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>cd ~/.kube
</span></span><span style=display:flex><span>export KUBECONFIG=$(pwd)/config-oidc
</span></span><span style=display:flex><span>kubectl config use-context `shoot--project--mycluster`
</span></span></code></pre></div></li><li><p><code>kubectl</code> delegates the authentication to plugin <code>oidc-login</code> the first time the user uses <code>kubectl</code> to contact the API server, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl get all
</span></span></code></pre></div><p>The plugin opens a browser for an interactive authentication session with Auth0, and in parallel serves a local webserver for the configured callback.</p></li><li><p>Enter your login credentials.</p><p><img src=/__resources/Login-through-identity-provider_4ab5de.png alt="Login through identity provider"></p><p>You should get a successful response from the API server:</p><pre tabindex=0><code>Opening in existing browser session.
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   100.64.0.1   &lt;none&gt;        443/TCP   86m
</code></pre></li></ol><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>After a successful login, <code>kubectl</code> uses a token for authentication so that you don’t have to provide user and password for every new <code>kubectl</code> command. How long the token is valid can be configured. If you want to log in again earlier, reset plugin <code>oidc-login</code>:</p><ol><li>Delete directory <code>~/.kube/cache/oidc-login</code>.</li><li>Delete the browser cache.</li></ol></blockquote><ol><li><p>To see if your user uses the cluster role <code>view</code>, do some checks with <code>kubectl auth can-i</code>.</p><ul><li><p>The response for the following commands should be <code>no</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl auth can-i create clusterrolebindings
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl auth can-i get secrets
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl auth can-i describe secrets
</span></span></code></pre></div></li><li><p>The response for the following commands should be <code>yes</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl auth can-i list pods
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl auth can-i get pods
</span></span></code></pre></div></li></ul></li></ol><p>If the last step is successful, you’ve configured your cluster to authenticate against an identity provider using OIDC.</p><h2 id=related-links>Related Links<a class=td-heading-self-link href=#related-links aria-label="Heading self-link"></a></h2><ul><li><a href=https://auth0.com/pricing/>Auth0 Pricing</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-344c04e957430ce9a88323b6ebfc2cfd>3 - Backup and Restore of Kubernetes Objects</h1><div class=lead>Details about backup and recovery of Kubernetes objects based on the open source tool <a href=https://velero.io/>Velero</a>.</div><p><img src=/__resources/teaser_6f9405.png alt="Don&rsquo;t worry &mldr; have a backup"></p><h2 id=tldr>TL;DR<a class=td-heading-self-link href=#tldr aria-label="Heading self-link"></a></h2><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>Details of the description might change in the near future since Heptio was taken over by VMWare which might result in different GitHub repositories or other changes. Please don&rsquo;t hesitate to inform us in case you encounter any issues.</p></blockquote><p>In general, Backup and Restore (BR) covers activities enabling an organization to bring a system back in a consistent state, e.g., after a disaster or to setup a new system. These activities vary in a very broad way depending on the applications and its persistency.</p><p>Kubernetes objects like Pods, Deployments, NetworkPolicies, etc. configure Kubernetes internal components and might as well include external components like load balancer and persistent volumes of the cloud provider. The BR of external components and their configurations might be difficult to handle in case manual configurations were needed to prepare these components.</p><p>To set the expectations right from the beginning, this tutorial covers the BR of Kubernetes deployments which might use persistent volumes. The BR of any manual configuration of external components, e.g., via the cloud providers console, is not covered here, as well as the BR of a whole Kubernetes system.</p><p>This tutorial puts the focus on the open source tool <a href=https://velero.io/>Velero</a> (formerly Heptio Ark) and its functionality to explain the BR process.</p><style>#body-inner blockquote{border:0;padding:10px;margin-top:40px;margin-bottom:40px;border-radius:4px;background-color:rgba(0,0,0,5%);box-shadow:0 3px 6px rgba(0,0,0,.16),0 3px 6px rgba(0,0,0,.23);position:relative;padding-left:60px}#body-inner blockquote:before{content:"i";font-weight:700;position:absolute;top:0;bottom:0;left:0;background-color:#00a273;color:#fff;vertical-align:middle;margin:auto;width:36px;font-size:30px;text-align:center}</style><p>Basically, Velero allows you to:</p><ul><li>backup and restore your Kubernetes cluster resources and persistent volumes (on-demand or scheduled)</li><li>backup or restore all objects in your cluster, or filter resources by type, namespace, and/or label</li><li>by default, all persistent volumes are backed up (configurable)</li><li>replicate your production environment for development and testing environments</li><li>define an expiration date per backup</li><li>execute pre- and post-activities in a container of a pod when a backup is created (see <a href=https://velero.io/docs/main/backup-hooks/#docs>Hooks</a>)</li><li>extend Velero by Plugins, e.g., for Object and Block store (see <a href=https://velero.io/docs/main/custom-plugins/#docs>Plugins</a>)</li></ul><p>Velero consists of a server side component and a client tool. The server components consists of Custom Resource Definitions (CRD) and controllers to perform the activities. The client tool communicates with the K8s API server to, e.g., create objects like a Backup object.</p><p>The diagram below explains the backup process. When creating a backup, Velero client makes a call to the Kubernetes API server to create a Backup object (1). The BackupController notices the new Backup object, validates the object (2) and begins the backup process (3). Based on the filter settings provided by the Velero client it collects the resources in question (3). The BackupController creates a tar ball with the Kubernetes objects and stores it in the backup location, e.g., AWS S3 (4) as well as snapshots of persistent volumes (5).</p><p>The size of the backup tar ball corresponds to the number of objects in etcd. The gzipped archive contains the <code>Json</code> representations of the objects.</p><p><img src=/__resources/backup-process_0af76a.png alt="Backup process"></p><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>As of the writing of this tutorial, Velero or any other BR tool for Shoot clusters is not provided by Gardener.</p></blockquote><h2 id=getting-started>Getting Started<a class=td-heading-self-link href=#getting-started aria-label="Heading self-link"></a></h2><p>At first, clone the <a href=https://github.com/vmware-tanzu/velero-plugin-for-aws>Velero GitHub repository</a> and get the Velero client from the <a href=https://github.com/vmware-tanzu/velero-plugin-for-aws/releases>releases</a> or build it from source via <code>make all</code> in the main directory of the cloned GitHub repository.</p><p>To use an AWS S3 bucket as storage for the backup files and the persistent volumes, you need to:</p><ul><li>create a S3 bucket as the backup target</li><li>create an AWS IAM user for Velero</li><li>configure the Velero server</li><li>create a secret for your AWS credentials</li></ul><p>For details about this setup, check the <a href=https://github.com/vmware-tanzu/velero-plugin-for-aws#set-permissions-for-velero>Set Permissions for Velero</a> documentation. Moreover, it is possible to use other <a href=https://velero.io/docs/main/supported-providers/>supported storage providers</a>.</p><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>Per default, Velero is installed in the namespace <code>velero</code>. To change the namespace, check the <a href=https://velero.io/docs/main/namespace/#customize-the-namespace-during-install>documentation</a>.</p></blockquote><p>Velero offers a wide range of filter possibilities for Kubernetes resources, e.g filter by namespaces, labels or resource types. The filter settings can be combined and used as <em>include</em> or <em>exclude</em>, which gives a great flexibility for selecting resources.</p><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>Carefully set labels and/or use namespaces for your deployments to make the selection of the resources to be backed up easier. The best practice would be to check in advance which resources are selected with the defined filter.</p></blockquote><h2 id=exemplary-use-cases>Exemplary Use Cases<a class=td-heading-self-link href=#exemplary-use-cases aria-label="Heading self-link"></a></h2><p>Below are some use cases which could give you an idea on how to use Velero. You can also check <a href=https://velero.io/docs/main/>Velero&rsquo;s documentation</a> for other introductory examples.</p><h3 id=helm-based-deployments>Helm Based Deployments<a class=td-heading-self-link href=#helm-based-deployments aria-label="Heading self-link"></a></h3><p>To be able to use Helm charts in your Kubernetes cluster, you need to install the Helm client <code>helm</code> and the server component <code>tiller</code>. Per default the server component is installed in the namespace <code>kube-system</code>. Even if it is possible to select single deployments via the filter settings of Velero, you should consider to install <code>tiller</code> in a separate namespace via <code>helm init --tiller-namespace &lt;your namespace></code>. This approach applies as well for all Helm charts to be deployed - consider separate namespaces for your deployments as well by using the parameter <code>--namespace</code>.</p><p>To backup a Helm based deployment, you need to backup both Tiller <em>and</em> the deployment. Only then the deployments could be managed via Helm. As mentioned above, the selection of resources would be easier in case they are separated in namespaces.</p><h3 id=separate-backup-locations>Separate Backup Locations<a class=td-heading-self-link href=#separate-backup-locations aria-label="Heading self-link"></a></h3><p>In case you run all your Kubernetes clusters on a single cloud provider, there is probably no need to store the backups in a bucket of a different cloud provider. However, if you run Kubernetes clusters on different cloud provider, you might consider to use a bucket on just one cloud provider as the target for the backups, e.g., to benefit from a lower price tag for the storage.</p><p>Per default, Velero assumes that both the persistent volumes and the backup location are on the same cloud provider. During the setup of Velero, a secret is created using the credentials for a cloud provider user who has access to both objects (see the policies, e.g., for the <a href=https://github.com/vmware-tanzu/velero-plugin-for-aws#set-permissions-for-velero>AWS configuration</a>).</p><p>Now, since the backup location is different from the volume location, you need to follow these steps (described here for AWS):</p><ul><li><p>configure as documented the volume storage location in <code>examples/aws/06-volumesnapshotlocation.yaml</code> and provide the user credentials. In this case, the S3 related settings like the policies can be omitted</p></li><li><p>create the bucket for the backup in the cloud provider in question and a user with the appropriate credentials and store them in a separate file similar to <code>credentials-ark</code></p></li><li><p>create a secret which contains two credentials, one for the volumes and one for the backup target, e.g., by using the command <code>kubectl create secret generic cloud-credentials --namespace heptio-ark --from-file cloud=credentials-ark --from-file backup-target=backup-ark</code></p></li><li><p>configure in the deployment manifest <code>examples/aws/10-deployment.yaml</code> the entries in <code>volumeMounts</code>, <code>env</code> and <code>volumes</code> accordingly, e.g., for a cluster running on AWS and the backup target bucket on GCP a configuration could look similar to:</p><details>Some links might get broken in the near future since Heptio was taken over by VMWare which might result in different GitHub repositories or other changes. Please don't hesitate to inform us in case you encounter any issues.<summary>Example Velero deployment</summary><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green># Copyright 2017 the Heptio Ark contributors.</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span><span style=color:green># Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);</span>
</span></span><span style=display:flex><span><span style=color:green># you may not use this file except in compliance with the License.</span>
</span></span><span style=display:flex><span><span style=color:green># You may obtain a copy of the License at</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span><span style=color:green>#     http://www.apache.org/licenses/LICENSE-2.0</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span><span style=color:green># Unless required by applicable law or agreed to in writing, software</span>
</span></span><span style=display:flex><span><span style=color:green># distributed under the License is distributed on an &#34;AS IS&#34; BASIS,</span>
</span></span><span style=display:flex><span><span style=color:green># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
</span></span><span style=display:flex><span><span style=color:green># See the License for the specific language governing permissions and</span>
</span></span><span style=display:flex><span><span style=color:green># limitations under the License.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: apps/v1beta1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  namespace: velero
</span></span><span style=display:flex><span>  name: velero
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        component: velero
</span></span><span style=display:flex><span>      annotations:
</span></span><span style=display:flex><span>        prometheus.io/scrape: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>        prometheus.io/port: <span style=color:#a31515>&#34;8085&#34;</span>
</span></span><span style=display:flex><span>        prometheus.io/path: <span style=color:#a31515>&#34;/metrics&#34;</span>
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      restartPolicy: Always
</span></span><span style=display:flex><span>      serviceAccountName: velero
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>        - name: velero
</span></span><span style=display:flex><span>          image: gcr.io/heptio-images/velero:latest
</span></span><span style=display:flex><span>          command:
</span></span><span style=display:flex><span>            - /velero
</span></span><span style=display:flex><span>          args:
</span></span><span style=display:flex><span>            - server
</span></span><span style=display:flex><span>          volumeMounts:
</span></span><span style=display:flex><span>            - name: cloud-credentials
</span></span><span style=display:flex><span>              mountPath: /credentials
</span></span><span style=display:flex><span>            - name: plugins
</span></span><span style=display:flex><span>              mountPath: /plugins
</span></span><span style=display:flex><span>            - name: scratch
</span></span><span style=display:flex><span>              mountPath: /scratch
</span></span><span style=display:flex><span>          env:
</span></span><span style=display:flex><span>            - name: AWS_SHARED_CREDENTIALS_FILE
</span></span><span style=display:flex><span>              value: /credentials/cloud
</span></span><span style=display:flex><span>            - name: GOOGLE_APPLICATION_CREDENTIALS
</span></span><span style=display:flex><span>              value: /credentials/backup-target
</span></span><span style=display:flex><span>            - name: VELERO_SCRATCH_DIR
</span></span><span style=display:flex><span>              value: /scratch
</span></span><span style=display:flex><span>      volumes:
</span></span><span style=display:flex><span>        - name: cloud-credentials
</span></span><span style=display:flex><span>          secret:
</span></span><span style=display:flex><span>            secretName: cloud-credentials
</span></span><span style=display:flex><span>        - name: plugins
</span></span><span style=display:flex><span>          emptyDir: {}
</span></span><span style=display:flex><span>        - name: scratch
</span></span><span style=display:flex><span>          emptyDir: {}
</span></span></code></pre></div></details></li><li><p>finally, configure the backup storage location in <code>examples/aws/05-backupstoragelocation.yaml</code> to use, in this case, a GCP bucket</p></li></ul><h2 id=limitations>Limitations<a class=td-heading-self-link href=#limitations aria-label="Heading self-link"></a></h2><p>Below is a potentially incomplete list of limitations. You can also consult <a href=https://velero.io/docs/main/>Velero&rsquo;s documentation</a> to get up to date information.</p><ul><li>Only full backups of selected resources are supported. Incremental backups are not (yet) supported. However, by using filters it is possible to restrict the backup to specific resources</li><li>Inconsistencies might occur in case of changes during the creation of the backup</li><li>Application specific actions are not considered by default. However, they might be handled by using Velero&rsquo;s <a href=https://velero.io/docs/main/backup-hooks/#docs>Hooks</a> or <a href=https://velero.io/docs/main/custom-plugins/#docs>Plugins</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-9065562425e5b71395aea19a465cdabb>4 - Create / Delete a Shoot Cluster</h1><h2 id=create-a-shoot-cluster>Create a Shoot Cluster<a class=td-heading-self-link href=#create-a-shoot-cluster aria-label="Heading self-link"></a></h2><p>As you have already prepared an <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>example Shoot manifest</a> in the steps described in the development documentation, please open another Terminal pane/window with the <code>KUBECONFIG</code> environment variable pointing to the Garden development cluster and send the manifest to the Kubernetes API server:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f your-shoot-aws.yaml
</span></span></code></pre></div><p>You should see that Gardener has immediately picked up your manifest and has started to deploy the Shoot cluster.</p><p>In order to investigate what is happening in the Seed cluster, please download its proper Kubeconfig yourself (see next paragraph). The namespace of the Shoot cluster in the Seed cluster will look like that: <code>shoot-johndoe-johndoe-1</code>, whereas the first <code>johndoe</code> is your namespace in the Garden cluster (also called &ldquo;project&rdquo;) and the <code>johndoe-1</code> suffix is the actual name of the Shoot cluster.</p><p>To connect to the newly created Shoot cluster, you must download its Kubeconfig as well. Please connect to the proper Seed cluster, navigate to the Shoot namespace, and download the Kubeconfig from the <code>kubecfg</code> secret in that namespace.</p><h2 id=delete-a-shoot-cluster>Delete a Shoot Cluster<a class=td-heading-self-link href=#delete-a-shoot-cluster aria-label="Heading self-link"></a></h2><p>In order to delete your cluster, you have to set an annotation confirming the deletion first, and trigger the deletion after that. You can use the prepared <code>delete shoot</code> script which takes the Shoot name as first parameter. The namespace can be specified by the second parameter, but it is optional. If you don&rsquo;t state it, it defaults to your namespace (the username you are logged in with to your machine).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/usage/delete shoot johndoe-1 johndoe
</span></span></code></pre></div><p>(the <code>hack</code> bash script can be found at <a href=https://github.com/gardener/gardener/blob/master/hack/usage/delete>GitHub</a>)</p><h2 id=configure-a-shoot-cluster-alert-receiver>Configure a Shoot Cluster Alert Receiver<a class=td-heading-self-link href=#configure-a-shoot-cluster-alert-receiver aria-label="Heading self-link"></a></h2><p>The receiver of the Shoot alerts can be configured from the <code>.spec.monitoring.alerting.emailReceivers</code> section in the Shoot specification. The value of the field has to be a list of valid mail addresses.</p><p>The alerting for the Shoot clusters is handled by the Prometheus Alertmanager. The Alertmanager will be deployed next to the control plane when the <code>Shoot</code> resource specifies <code>.spec.monitoring.alerting.emailReceivers</code> and if a <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-alerting.yaml>SMTP secret</a> exists.</p><p>If the field gets removed then the Alertmanager will be also removed during the next reconcilation of the cluster. The opposite is also valid if the field is added to an existing cluster.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-457c612e72c97bcc552bc2ac30e85f80>5 - Create a Shoot Cluster Into an Existing AWS VPC</h1><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>Gardener can create a new VPC, or use an existing one for your shoot cluster. Depending on your needs, you may want to create shoot(s) into an already created VPC.
The tutorial describes how to create a shoot cluster into an existing AWS VPC. The steps are identical for Alicloud, Azure, and GCP. Please note that the existing VPC must be in the same region like the shoot cluster that you want to deploy into the VPC.</p><h2 id=tldr>TL;DR<a class=td-heading-self-link href=#tldr aria-label="Heading self-link"></a></h2><p>If <code>.spec.provider.infrastructureConfig.networks.vpc.cidr</code> is specified, Gardener will create a new VPC with the given CIDR block and respectively will delete it on shoot deletion.<br>If <code>.spec.provider.infrastructureConfig.networks.vpc.id</code> is specified, Gardener will use the existing VPC and respectively won&rsquo;t delete it on shoot deletion.</p><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>It&rsquo;s not recommended to create a shoot cluster into a VPC that is managed by Gardener (that is created for another shoot cluster). In this case the deletion of the initial shoot cluster will fail to delete the VPC because there will be resources attached to it.</p></blockquote><p>Gardener won&rsquo;t delete any manually created (unmanaged) resources in your cloud provider account.</p><h2 id=1-configure-the-aws-cli>1. Configure the AWS CLI<a class=td-heading-self-link href=#1-configure-the-aws-cli aria-label="Heading self-link"></a></h2><p>The <code>aws configure</code> command is a convenient way to setup your AWS CLI. It will prompt you for your credentials and settings which will be used in the following AWS CLI invocations:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>aws configure
</span></span><span style=display:flex><span>AWS Access Key ID [None]: &lt;ACCESS_KEY_ID&gt;
</span></span><span style=display:flex><span>AWS Secret Access Key [None]: &lt;SECRET_ACCESS_KEY&gt;
</span></span><span style=display:flex><span>Default region name [None]: &lt;DEFAULT_REGION&gt;
</span></span><span style=display:flex><span>Default output format [None]: &lt;DEFAULT_OUTPUT_FORMAT&gt;
</span></span></code></pre></div><h2 id=2-create-a-vpc>2. Create a VPC<a class=td-heading-self-link href=#2-create-a-vpc aria-label="Heading self-link"></a></h2><p>Create the VPC by running the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>aws ec2 create-vpc --cidr-block &lt;cidr-block&gt;
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#a31515>&#34;Vpc&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;VpcId&#34;</span>: <span style=color:#a31515>&#34;vpc-ff7bbf86&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;InstanceTenancy&#34;</span>: <span style=color:#a31515>&#34;default&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;Tags&#34;</span>: [],
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;CidrBlockAssociations&#34;</span>: [
</span></span><span style=display:flex><span>          {
</span></span><span style=display:flex><span>              <span style=color:#a31515>&#34;AssociationId&#34;</span>: <span style=color:#a31515>&#34;vpc-cidr-assoc-6e42b505&#34;</span>,
</span></span><span style=display:flex><span>              <span style=color:#a31515>&#34;CidrBlock&#34;</span>: <span style=color:#a31515>&#34;10.0.0.0/16&#34;</span>,
</span></span><span style=display:flex><span>              <span style=color:#a31515>&#34;CidrBlockState&#34;</span>: {
</span></span><span style=display:flex><span>                  <span style=color:#a31515>&#34;State&#34;</span>: <span style=color:#a31515>&#34;associated&#34;</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>          }
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;Ipv6CidrBlockAssociationSet&#34;</span>: [],
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;State&#34;</span>: <span style=color:#a31515>&#34;pending&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;DhcpOptionsId&#34;</span>: <span style=color:#a31515>&#34;dopt-38f7a057&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;CidrBlock&#34;</span>: <span style=color:#a31515>&#34;10.0.0.0/16&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;IsDefault&#34;</span>: false
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Gardener requires the VPC to have enabled <a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html>DNS support</a>, i.e the attributes <code>enableDnsSupport</code> and <code>enableDnsHostnames</code> must be set to <em>true</em>. <code>enableDnsSupport</code> attribute is enabled by default, <code>enableDnsHostnames</code> - not. Set the <code>enableDnsHostnames</code> attribute to <em>true</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>aws ec2 modify-vpc-attribute --vpc-id vpc-ff7bbf86 --enable-dns-hostnames
</span></span></code></pre></div><h2 id=3-create-an-internet-gateway>3. Create an Internet Gateway<a class=td-heading-self-link href=#3-create-an-internet-gateway aria-label="Heading self-link"></a></h2><p>Gardener also requires that an internet gateway is attached to the VPC. You can create one by using:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>aws ec2 create-internet-gateway
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;InternetGateway&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;Tags&#34;</span>: [],
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;InternetGatewayId&#34;</span>: <span style=color:#a31515>&#34;igw-c0a643a9&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;Attachments&#34;</span>: []
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>and attach it to the VPC using:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>aws ec2 attach-internet-gateway --internet-gateway-id igw-c0a643a9 --vpc-id vpc-ff7bbf86
</span></span></code></pre></div><h2 id=4-create-the-shoot>4. Create the Shoot<a class=td-heading-self-link href=#4-create-the-shoot aria-label="Heading self-link"></a></h2><p>Prepare your shoot manifest (you could check the <a href=https://github.com/gardener/gardener/tree/master/example>example manifests</a>). Please make sure that you choose the region in which you had created the VPC earlier (step 2). Also, put your VPC ID in the <code>.spec.provider.infrastructureConfig.networks.vpc.id</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  region: &lt;aws-region-of-vpc&gt;
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc:
</span></span><span style=display:flex><span>          id: vpc-ff7bbf86
</span></span><span style=display:flex><span>    <span style=color:green># ...</span>
</span></span></code></pre></div><p>Apply your shoot manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f your-shoot-aws.yaml
</span></span></code></pre></div><p>Ensure that the shoot cluster is properly created:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get shoot $SHOOT_NAME -n $SHOOT_NAMESPACE
</span></span><span style=display:flex><span>NAME           CLOUDPROFILE   VERSION   SEED   DOMAIN           OPERATION   PROGRESS   APISERVER   CONTROL   NODES   SYSTEM   AGE
</span></span><span style=display:flex><span>&lt;SHOOT_NAME&gt;   aws            1.15.0    aws    &lt;SHOOT_DOMAIN&gt;   Succeeded   100        True        True      True    True     20m
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-fdc3b00b4e039db6c5bb39051d46c73b>6 - Fix Problematic Conversion Webhooks</h1><h2 id=reasoning>Reasoning<a class=td-heading-self-link href=#reasoning aria-label="Heading self-link"></a></h2><p><strong>Custom Resource Definition (CRD)</strong> is what you use to define a <code>Custom Resource</code>. This is a powerful way to extend Kubernetes capabilities beyond the default installation, adding any kind of API objects useful for your application.</p><p>The CustomResourceDefinition API provides a workflow for introducing and upgrading to new versions of a CustomResourceDefinition. In a scenario where a CRD adds support for a new version and switches its <code>spec.versions.storage</code> field to it (i.e., from <code>v1beta1</code> to <code>v1)</code>, existing objects are not migrated in etcd. For more information, see <a href=https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#previous-storage-versions>Versions in CustomResourceDefinitions</a>.</p><p>This creates a mismatch between the requested and stored version for all clients (kubectl, KCM, etc.). When the CRD also declares the usage of a conversion webhook, it gets called whenever a client requests information about a resource that still exists in the old version. If the CRD is created by the end-user, the webhook runs on the shoot side, whereas controllers / kapi-servers run separated, as part of the control-plane. For the webhook to be reachable, a working VPN connection <code>seed -> shoot</code> is essential. In scenarios where the VPN connection is broken, the <strong>kube-controller-manager</strong> eventually stops its garbage collection, as that requires it to list <code>v1.PartialObjectMetadata</code> for everything to build a dependency graph. Without the kube-controller-manager&rsquo;s garbage collector, managed resources get stuck during update/rollout.</p><h2 id=breaking-situations>Breaking Situations<a class=td-heading-self-link href=#breaking-situations aria-label="Heading self-link"></a></h2><p>When a user upgrades to <code>failureTolerance: node|zone</code>, that will cause the VPN <strong>deployments</strong> to be <em>replaced</em> by <strong>statefulsets</strong>. However, as the VPN connection is broken upon teardown of the deployment, garbage collection will fail, leading to a situation that is stuck until an operator manually tackles it.</p><p>Such a situation can be avoided if the end-user has correctly configured CRDs containing conversion webhooks.</p><h2 id=checking-problematic-crds>Checking Problematic CRDs<a class=td-heading-self-link href=#checking-problematic-crds aria-label="Heading self-link"></a></h2><p>In order to make sure there are no version problematic CRDs, please run the script below in your shoot. It will return the name of the CRDs in case they have one of the 2 problems:</p><ul><li>the returned version of the CR is different than what is maintained in the <code>status.storedVersions</code> field of the CRD.</li><li>the <code>status.storedVersions</code> field of the CRD has more than 1 version defined.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#00f>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#00f></span>
</span></span><span style=display:flex><span>set -e -o pipefail
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#a31515>&#34;Checking all CRDs in the cluster...&#34;</span>
</span></span><span style=display:flex><span><span style=color:#00f>for</span> p in <span style=color:#00f>$(</span>kubectl get crd | awk <span style=color:#a31515>&#39;NR&gt;1&#39;</span> | awk <span style=color:#a31515>&#39;{print $1}&#39;</span><span style=color:#00f>)</span>; <span style=color:#00f>do</span>
</span></span><span style=display:flex><span>  strategy=<span style=color:#00f>$(</span>kubectl get crd <span style=color:#a31515>&#34;</span>$p<span style=color:#a31515>&#34;</span> -o json | jq -r .spec.conversion.strategy<span style=color:#00f>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#00f>if</span> [ <span style=color:#a31515>&#34;</span>$strategy<span style=color:#a31515>&#34;</span> == <span style=color:#a31515>&#34;Webhook&#34;</span> ]; <span style=color:#00f>then</span>
</span></span><span style=display:flex><span>     crd_name=<span style=color:#00f>$(</span>kubectl get crd <span style=color:#a31515>&#34;</span>$p<span style=color:#a31515>&#34;</span> -o json | jq -r .metadata.name<span style=color:#00f>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>     number_of_stored_versions=<span style=color:#00f>$(</span>kubectl get crd <span style=color:#a31515>&#34;</span>$crd_name<span style=color:#a31515>&#34;</span> -o json  | jq <span style=color:#a31515>&#39;.status.storedVersions | length&#39;</span><span style=color:#00f>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:#00f>if</span> [[ <span style=color:#a31515>&#34;</span>$number_of_stored_versions<span style=color:#a31515>&#34;</span> == 1 ]]; <span style=color:#00f>then</span>
</span></span><span style=display:flex><span>         returned_cr_version=<span style=color:#00f>$(</span>kubectl get <span style=color:#a31515>&#34;</span>$crd_name<span style=color:#a31515>&#34;</span> -A -o json |  jq -r <span style=color:#a31515>&#39;.items[] | .apiVersion&#39;</span>  | sed <span style=color:#a31515>&#39;s:.*/::&#39;</span><span style=color:#00f>)</span>
</span></span><span style=display:flex><span>         <span style=color:#00f>if</span> [ -z <span style=color:#a31515>&#34;</span>$returned_cr_version<span style=color:#a31515>&#34;</span> ]; <span style=color:#00f>then</span>
</span></span><span style=display:flex><span>           <span style=color:#00f>continue</span>
</span></span><span style=display:flex><span>         <span style=color:#00f>else</span>
</span></span><span style=display:flex><span>           variable=<span style=color:#00f>$(</span>echo <span style=color:#a31515>&#34;</span>$returned_cr_version<span style=color:#a31515>&#34;</span> | xargs -n1 | sort -u | xargs<span style=color:#00f>)</span>
</span></span><span style=display:flex><span>           present_version=<span style=color:#00f>$(</span>kubectl get crd <span style=color:#a31515>&#34;</span>$crd_name<span style=color:#a31515>&#34;</span> -o json  |  jq -cr <span style=color:#a31515>&#39;.status.storedVersions |.[]&#39;</span><span style=color:#00f>)</span>
</span></span><span style=display:flex><span>           <span style=color:#00f>if</span> [[ $variable != <span style=color:#a31515>&#34;</span>$present_version<span style=color:#a31515>&#34;</span> ]]; <span style=color:#00f>then</span>
</span></span><span style=display:flex><span>             echo <span style=color:#a31515>&#34;ERROR: Stored version differs from the version that CRs are being returned. </span>$crd_name<span style=color:#a31515> with conversion webhook needs to be fixed&#34;</span>
</span></span><span style=display:flex><span>           <span style=color:#00f>fi</span>
</span></span><span style=display:flex><span>         <span style=color:#00f>fi</span>
</span></span><span style=display:flex><span>      <span style=color:#00f>fi</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:#00f>if</span> [[ <span style=color:#a31515>&#34;</span>$number_of_stored_versions<span style=color:#a31515>&#34;</span> -gt 1 ]]; <span style=color:#00f>then</span>
</span></span><span style=display:flex><span>         returned_cr_version=<span style=color:#00f>$(</span>kubectl get <span style=color:#a31515>&#34;</span>$crd_name<span style=color:#a31515>&#34;</span> -A -o json |  jq -r <span style=color:#a31515>&#39;.items[] | .apiVersion&#39;</span>  | sed <span style=color:#a31515>&#39;s:.*/::&#39;</span><span style=color:#00f>)</span>
</span></span><span style=display:flex><span>         <span style=color:#00f>if</span> [ -z <span style=color:#a31515>&#34;</span>$returned_cr_version<span style=color:#a31515>&#34;</span> ]; <span style=color:#00f>then</span>
</span></span><span style=display:flex><span>           <span style=color:#00f>continue</span>
</span></span><span style=display:flex><span>         <span style=color:#00f>else</span>
</span></span><span style=display:flex><span>           echo <span style=color:#a31515>&#34;ERROR: Too many stored versions defined. </span>$crd_name<span style=color:#a31515> with conversion webhook needs to be fixed&#34;</span>
</span></span><span style=display:flex><span>         <span style=color:#00f>fi</span>
</span></span><span style=display:flex><span>      <span style=color:#00f>fi</span>
</span></span><span style=display:flex><span>  <span style=color:#00f>fi</span>
</span></span><span style=display:flex><span><span style=color:#00f>done</span>
</span></span><span style=display:flex><span>echo <span style=color:#a31515>&#34;Problematic CRDs are reported above.&#34;</span>
</span></span></code></pre></div><h2 id=resolve-crds>Resolve CRDs<a class=td-heading-self-link href=#resolve-crds aria-label="Heading self-link"></a></h2><p>Below we give the steps needed to be taken in order to fix the CRDs reported by the script above.</p><p>Inspect all your CRDs that have conversion webhooks in place. If you have more than 1 version defined in its <code>spec.status.storedVersions</code> field, then initiate migration as described in <strong>Option 2</strong> in the <a href=https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#upgrade-existing-objects-to-a-new-stored-version>Upgrade existing objects to a new stored version</a> guide.</p><p>For convenience, we have provided the necessary steps below.</p><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>Please test the following steps on a non-productive landscape to make sure that the new CR version doesn’t break any of your existing workloads.</p></blockquote><ol><li><p>Please check/set the old CR version to <code>storage:false</code> and set the new CR version to <code>storage:true</code>.</p><p>For the sake of an example, let’s consider the two versions <code>v1beta1</code> (old) and <code>v1</code> (new).</p><p>Before:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>versions:
</span></span><span style=display:flex><span>- name: v1beta1
</span></span><span style=display:flex><span>......
</span></span><span style=display:flex><span>storage: true
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>- name: v1
</span></span><span style=display:flex><span>......
</span></span><span style=display:flex><span>storage: false
</span></span></code></pre></div><p>After:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>versions:
</span></span><span style=display:flex><span>- name: v1beta1
</span></span><span style=display:flex><span>......
</span></span><span style=display:flex><span>storage: false
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>- name: v1
</span></span><span style=display:flex><span>......
</span></span><span style=display:flex><span>storage: true
</span></span></code></pre></div></li><li><p>Convert <code>custom-resources</code> to the newest version.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get &lt;custom-resource-name&gt; -A -ojson | k apply -f -
</span></span></code></pre></div></li><li><p>Patch the CRD to keep only the latest version under storedVersions.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl patch customresourcedefinitions &lt;crd-name&gt; --subresource=<span style=color:#a31515>&#39;status&#39;</span> --type=<span style=color:#a31515>&#39;merge&#39;</span> -p <span style=color:#a31515>&#39;{&#34;status&#34;:{&#34;storedVersions&#34;:[&#34;your-latest-cr-version&#34;]}}&#39;</span>
</span></span></code></pre></div></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-4a6ec91cf4f6e1a431b0017c63b7f938>7 - GPU Enabled Cluster</h1><div class=lead>Setting up a GPU Enabled Cluster for Deep Learning</div><h2 id=disclaimer>Disclaimer<a class=td-heading-self-link href=#disclaimer aria-label="Heading self-link"></a></h2><p>Be aware, that the following sections might be opinionated. Kubernetes, and the GPU support in particular, are rapidly evolving, which means that this guide is likely to be outdated sometime soon. For this reason, <strong>contributions are highly appreciated</strong> to update this guide.</p><h2 id=create-a-cluster>Create a Cluster<a class=td-heading-self-link href=#create-a-cluster aria-label="Heading self-link"></a></h2><p>First thing first, let’s create a Kubernetes (K8s) cluster with GPU accelerated nodes. In this example we will use an AWS <strong>p2.xlarge</strong> EC2 instance because it&rsquo;s the cheapest available option at the moment. Use such cheap instances for learning to limit your resource costs. <strong>This costs around 1€/hour per GPU</strong></p><p><img src=/__resources/howto-gpu_666dae.png alt=gpu-selection></p><h2 id=install-nvidia-driver-as-daemonset>Install NVidia Driver as Daemonset<a class=td-heading-self-link href=#install-nvidia-driver-as-daemonset aria-label="Heading self-link"></a></h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: DaemonSet
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: nvidia-driver-installer
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    k8s-app: nvidia-driver-installer
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      name: nvidia-driver-installer
</span></span><span style=display:flex><span>      k8s-app: nvidia-driver-installer
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        name: nvidia-driver-installer
</span></span><span style=display:flex><span>        k8s-app: nvidia-driver-installer
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      hostPID: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      initContainers:
</span></span><span style=display:flex><span>      - image: squat/modulus:4a1799e7aa0143bcbb70d354bab3e419b1f54972
</span></span><span style=display:flex><span>        name: modulus
</span></span><span style=display:flex><span>        args:
</span></span><span style=display:flex><span>        - compile
</span></span><span style=display:flex><span>        - nvidia
</span></span><span style=display:flex><span>        - <span style=color:#a31515>&#34;410.104&#34;</span>
</span></span><span style=display:flex><span>        securityContext:
</span></span><span style=display:flex><span>          privileged: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        env:
</span></span><span style=display:flex><span>        - name: MODULUS_CHROOT
</span></span><span style=display:flex><span>          value: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>        - name: MODULUS_INSTALL
</span></span><span style=display:flex><span>          value: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>        - name: MODULUS_INSTALL_DIR
</span></span><span style=display:flex><span>          value: /opt/drivers
</span></span><span style=display:flex><span>        - name: MODULUS_CACHE_DIR
</span></span><span style=display:flex><span>          value: /opt/modulus/cache
</span></span><span style=display:flex><span>        - name: MODULUS_LD_ROOT
</span></span><span style=display:flex><span>          value: /root
</span></span><span style=display:flex><span>        - name: IGNORE_MISSING_MODULE_SYMVERS
</span></span><span style=display:flex><span>          value: <span style=color:#a31515>&#34;1&#34;</span>          
</span></span><span style=display:flex><span>        volumeMounts:
</span></span><span style=display:flex><span>        - name: etc-coreos
</span></span><span style=display:flex><span>          mountPath: /etc/coreos
</span></span><span style=display:flex><span>          readOnly: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        - name: usr-share-coreos
</span></span><span style=display:flex><span>          mountPath: /usr/share/coreos
</span></span><span style=display:flex><span>          readOnly: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        - name: ld-root
</span></span><span style=display:flex><span>          mountPath: /root
</span></span><span style=display:flex><span>        - name: module-cache
</span></span><span style=display:flex><span>          mountPath: /opt/modulus/cache
</span></span><span style=display:flex><span>        - name: module-install-dir-base
</span></span><span style=display:flex><span>          mountPath: /opt/drivers
</span></span><span style=display:flex><span>        - name: dev
</span></span><span style=display:flex><span>          mountPath: /dev
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - image: <span style=color:#a31515>&#34;gcr.io/google-containers/pause:3.1&#34;</span>
</span></span><span style=display:flex><span>        name: pause
</span></span><span style=display:flex><span>      tolerations:
</span></span><span style=display:flex><span>      - key: <span style=color:#a31515>&#34;nvidia.com/gpu&#34;</span>
</span></span><span style=display:flex><span>        effect: <span style=color:#a31515>&#34;NoSchedule&#34;</span>
</span></span><span style=display:flex><span>        operator: <span style=color:#a31515>&#34;Exists&#34;</span>
</span></span><span style=display:flex><span>      volumes:
</span></span><span style=display:flex><span>      - name: etc-coreos
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /etc/coreos
</span></span><span style=display:flex><span>      - name: usr-share-coreos
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /usr/share/coreos
</span></span><span style=display:flex><span>      - name: ld-root
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /
</span></span><span style=display:flex><span>      - name: module-cache
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /opt/modulus/cache
</span></span><span style=display:flex><span>      - name: dev
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /dev
</span></span><span style=display:flex><span>      - name: module-install-dir-base
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /opt/drivers
</span></span></code></pre></div><h2 id=install-device-plugin>Install Device Plugin<a class=td-heading-self-link href=#install-device-plugin aria-label="Heading self-link"></a></h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: DaemonSet
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: nvidia-gpu-device-plugin
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    k8s-app: nvidia-gpu-device-plugin
</span></span><span style=display:flex><span>    <span style=color:green>#addonmanager.kubernetes.io/mode: Reconcile</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      k8s-app: nvidia-gpu-device-plugin
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        k8s-app: nvidia-gpu-device-plugin
</span></span><span style=display:flex><span>      annotations:
</span></span><span style=display:flex><span>        scheduler.alpha.kubernetes.io/critical-pod: <span style=color:#a31515>&#39;&#39;</span>
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      priorityClassName: system-node-critical
</span></span><span style=display:flex><span>      volumes:
</span></span><span style=display:flex><span>      - name: device-plugin
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /var/lib/kubelet/device-plugins
</span></span><span style=display:flex><span>      - name: dev
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /dev
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - image: <span style=color:#a31515>&#34;k8s.gcr.io/nvidia-gpu-device-plugin@sha256:08509a36233c5096bb273a492251a9a5ca28558ab36d74007ca2a9d3f0b61e1d&#34;</span>
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/usr/bin/nvidia-gpu-device-plugin&#34;</span>, <span style=color:#a31515>&#34;-logtostderr&#34;</span>, <span style=color:#a31515>&#34;-host-path=/opt/drivers/nvidia&#34;</span>]
</span></span><span style=display:flex><span>        name: nvidia-gpu-device-plugin
</span></span><span style=display:flex><span>        resources:
</span></span><span style=display:flex><span>          requests:
</span></span><span style=display:flex><span>            cpu: 50m
</span></span><span style=display:flex><span>            memory: 10Mi
</span></span><span style=display:flex><span>          limits:
</span></span><span style=display:flex><span>            cpu: 50m
</span></span><span style=display:flex><span>            memory: 10Mi
</span></span><span style=display:flex><span>        securityContext:
</span></span><span style=display:flex><span>          privileged: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        volumeMounts:
</span></span><span style=display:flex><span>        - name: device-plugin
</span></span><span style=display:flex><span>          mountPath: /device-plugin
</span></span><span style=display:flex><span>        - name: dev
</span></span><span style=display:flex><span>          mountPath: /dev
</span></span><span style=display:flex><span>  updateStrategy:
</span></span><span style=display:flex><span>    type: RollingUpdate
</span></span></code></pre></div><h2 id=test>Test<a class=td-heading-self-link href=#test aria-label="Heading self-link"></a></h2><p>To run an example training on a GPU node, first start a base image with Tensorflow with GPU support & Keras:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: deeplearning-workbench
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: deeplearning-workbench
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: deeplearning-workbench
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: deeplearning-workbench
</span></span><span style=display:flex><span>        image: afritzler/deeplearning-workbench
</span></span><span style=display:flex><span>        resources:
</span></span><span style=display:flex><span>          limits:
</span></span><span style=display:flex><span>            nvidia.com/gpu: 1
</span></span><span style=display:flex><span>      tolerations:
</span></span><span style=display:flex><span>      - key: <span style=color:#a31515>&#34;nvidia.com/gpu&#34;</span>
</span></span><span style=display:flex><span>        effect: <span style=color:#a31515>&#34;NoSchedule&#34;</span>
</span></span><span style=display:flex><span>        operator: <span style=color:#a31515>&#34;Exists&#34;</span>
</span></span></code></pre></div><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>The <code>tolerations</code> section above is not required if you deploy the <code>ExtendedResourceToleration</code> admission controller to your cluster. You can do this in the <code>kubernetes</code> section of your Gardener cluster <code>shoot.yaml</code> as follows:</p></blockquote><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeAPIServer:
</span></span><span style=display:flex><span>      admissionPlugins:
</span></span><span style=display:flex><span>      - name: ExtendedResourceToleration
</span></span></code></pre></div><p>Now exec into the container and start an example Keras training:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl exec -it deeplearning-workbench-8676458f5d-p4d2v -- /bin/bash
</span></span><span style=display:flex><span>cd /keras/example
</span></span><span style=display:flex><span>python imdb_cnn.py
</span></span></code></pre></div><h2 id=related-links>Related Links<a class=td-heading-self-link href=#related-links aria-label="Heading self-link"></a></h2><ul><li><a href=https://github.com/afritzler/kubernetes-gpu>Andreas Fritzler</a> from the Gardener Core team for the R&amp;D, who has provided this setup.</li><li><a href=https://github.com/squat/modulus>Build and install NVIDIA driver on CoreOS</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-cc899b4348fe215d93eaa9bbdc637d3e>8 - Shoot Cluster Maintenance</h1><div class=lead>Understanding and configuring Gardener&rsquo;s Day-2 operations for Shoot clusters.</div><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>Day two operations for shoot clusters are related to:</p><ul><li>The Kubernetes version of the control plane and the worker nodes</li><li>The operating system version of the worker nodes</li></ul><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>When referring to an update of the &ldquo;operating system version&rdquo; in this document, the update of the machine image of the shoot cluster&rsquo;s worker nodes is meant. For example, Amazon Machine Images (AMI) for AWS.</p></blockquote><p>The following table summarizes what options Gardener offers to maintain these versions:</p><table><thead><tr><th style=text-align:left></th><th style=text-align:left>Auto-Update</th><th style=text-align:left>Forceful Updates</th><th style=text-align:left>Manual Updates</th></tr></thead><tbody><tr><td style=text-align:left>Kubernetes version</td><td style=text-align:left>Patches only</td><td style=text-align:left>Patches and consecutive minor updates only</td><td style=text-align:left>yes</td></tr><tr><td style=text-align:left>Operating system version</td><td style=text-align:left>yes</td><td style=text-align:left>yes</td><td style=text-align:left>yes</td></tr></tbody></table><h2 id=allowed-target-versions-in-the-cloudprofile>Allowed Target Versions in the <code>CloudProfile</code><a class=td-heading-self-link href=#allowed-target-versions-in-the-cloudprofile aria-label="Heading self-link"></a></h2><p>Administrators maintain the allowed target versions that you can update to in the <code>CloudProfile</code> for each IaaS-Provider. Users with access to a Gardener project can check supported target versions with:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get cloudprofile [IAAS-SPECIFIC-PROFILE] -o yaml
</span></span></code></pre></div><table><thead><tr><th style=text-align:left>Path</th><th style=text-align:left>Description</th><th style=text-align:left>More Information</th></tr></thead><tbody><tr><td style=text-align:left><code>spec.kubernetes.versions</code></td><td style=text-align:left>The supported Kubernetes version <code>major.minor.patch</code>.</td><td style=text-align:left><a href=https://github.com/kubernetes/design-proposals-archive/blob/main/release/versioning.md#patch-releases>Patch releases</a></td></tr><tr><td style=text-align:left><code>spec.machineImages</code></td><td style=text-align:left>The supported operating system versions for worker nodes</td><td style=text-align:left></td></tr></tbody></table><p>Both the Kubernetes version and the operating system version follow semantic versioning that allows Gardener to handle updates automatically.</p><p>For more information, see <a href=http://semver.org/>Semantic Versioning</a>.</p><h3 id=impact-of-version-classifications-on-updates>Impact of Version Classifications on Updates<a class=td-heading-self-link href=#impact-of-version-classifications-on-updates aria-label="Heading self-link"></a></h3><p>Gardener allows to classify versions in the <code>CloudProfile</code> as <code>preview</code>, <code>supported</code>, <code>deprecated</code>, or <code>expired</code>. During maintenance operations, <code>preview</code> versions are excluded from updates, because they’re often recently released versions that haven’t yet undergone thorough testing and may contain bugs or security issues.</p><p>For more information, see <a href=/docs/gardener/shoot-operations/shoot_versions/#version-classifications>Version Classifications</a>.</p><h2 id=let-gardener-manage-your-updates>Let Gardener Manage Your Updates<a class=td-heading-self-link href=#let-gardener-manage-your-updates aria-label="Heading self-link"></a></h2><h3 id=the-maintenance-window>The Maintenance Window<a class=td-heading-self-link href=#the-maintenance-window aria-label="Heading self-link"></a></h3><p>Gardener can manage updates for you automatically. It offers users to specify a <em>maintenance window</em> during which updates are scheduled:</p><ul><li>The time interval of the maintenance window can’t be less than 30 minutes or more than 6 hours.</li><li>If there’s no maintenance window specified during the creation of a shoot cluster, Gardener chooses a maintenance window randomly to spread the load.</li></ul><p>You can either specify the maintenance window in the shoot cluster specification (<code>.spec.maintenance.timeWindow</code>) or the start time of the maintenance window using the Gardener dashboard (<strong>CLUSTERS</strong> > <strong>[YOUR-CLUSTER]</strong> > <strong>OVERVIEW</strong> > <strong>Lifecycle</strong> > <strong>Maintenance</strong>).</p><h3 id=auto-update-and-forceful-updates>Auto-Update and Forceful Updates<a class=td-heading-self-link href=#auto-update-and-forceful-updates aria-label="Heading self-link"></a></h3><p>To trigger updates during the maintenance window automatically, Gardener offers the following methods:</p><ul><li><p><em>Auto-update</em>:<br>Gardener starts an update during the next maintenance window whenever there’s a version available in the <code>CloudProfile</code> that is higher than the one of your shoot cluster specification, and that isn’t classified as <code>preview</code> version. For Kubernetes versions, auto-update only updates to higher patch levels.</p><p>You can either activate auto-update on the Gardener dashboard (<strong>CLUSTERS</strong> > <strong>[YOUR-CLUSTER]</strong> > <strong>OVERVIEW</strong> > <strong>Lifecycle</strong> > <strong>Maintenance</strong>) or in the shoot cluster specification:</p><ul><li><code>.spec.maintenance.autoUpdate.kubernetesVersion: true</code></li><li><code>.spec.maintenance.autoUpdate.machineImageVersion: true</code></li></ul></li><li><p><em>Forceful updates</em>:<br>In the maintenance window, Gardener compares the current version given in the shoot cluster specification with the version list in the <code>CloudProfile</code>. If the version has an expiration date and if the date is before the start of the maintenance window, Gardener starts an update to the highest version available in the <code>CloudProfile</code> that isn’t classified as <code>preview</code> version. The highest version in <code>CloudProfile</code> can’t have an expiration date. For Kubernetes versions, Gardener only updates to higher patch levels or consecutive minor versions.</p></li></ul><p>If you don’t want to wait for the next maintenance window, you can annotate the shoot cluster specification with <code>shoot.gardener.cloud/operation: maintain</code>. Gardener then checks immediately if there’s an auto-update or a forceful update needed.</p><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>Forceful version updates are executed even if the auto-update for the Kubernetes version(or the auto-update for the machine image version) is deactivated (set to <code>false</code>).</p></blockquote><p>With expiration dates, administrators can give shoot cluster owners more time for testing before the actual version update happens, which allows for smoother transitions to new versions.</p><h3 id=kubernetes-update-paths>Kubernetes Update Paths<a class=td-heading-self-link href=#kubernetes-update-paths aria-label="Heading self-link"></a></h3><p>The bigger the delta of the Kubernetes source version and the Kubernetes target version, the better it must be planned and executed by operators. Gardener only provides automatic support for updates that can be applied safely to the cluster workload:</p><table><thead><tr><th style=text-align:left>Update Type</th><th style=text-align:left>Example</th><th style=text-align:left>Update Method</th></tr></thead><tbody><tr><td style=text-align:left>Patches</td><td style=text-align:left><code>1.10.12</code> to <code>1.10.13</code></td><td style=text-align:left>auto-update or Forceful update</td></tr><tr><td style=text-align:left>Update to consecutive minor version</td><td style=text-align:left><code>1.10.12</code> to <code>1.11.10</code></td><td style=text-align:left>Forceful update</td></tr><tr><td style=text-align:left>Other</td><td style=text-align:left><code>1.10.12</code> to <code>1.12.0</code></td><td style=text-align:left>Manual update</td></tr></tbody></table><p>Gardener doesn’t support automatic updates of nonconsecutive minor versions, because Kubernetes doesn’t guarantee updateability in this case. However, multiple minor version updates are possible if not only the minor source version is expired, but also the minor target version is expired. Gardener then updates the Kubernetes version first to the expired target version, and waits for the next maintenance window to update this version to the next minor target version.</p><blockquote class="alert alert-warning"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>alert-outline</title><path d="M12 2 1 21H23M12 6l7.53 13H4.47M11 10v4h2V10m-2 6v2h2V16"/></svg><p>Warning</p></div><p>The administrator who maintains the <code>CloudProfile</code> has to ensure that the list of Kubernetes versions consists of consecutive minor versions, for example, from <code>1.10.x</code> to <code>1.11.y</code>. If the minor version increases in bigger steps, for example, from <code>1.10.x</code> to <code>1.12.y</code>, then the shoot cluster updates will fail during the maintenance window.</p></blockquote><h2 id=manual-updates>Manual Updates<a class=td-heading-self-link href=#manual-updates aria-label="Heading self-link"></a></h2><p>To update the Kubernetes version or the node operating system manually, change the <code>.spec.kubernetes.version</code> field or the <code>.spec.provider.workers.machine.image.version</code> field correspondingly.</p><p>Manual updates are required if you would like to do a minor update of the Kubernetes version. Gardener doesn’t do such updates automatically, as they can have breaking changes that could impact the cluster workload.</p><p>Manual updates are either executed immediately (default) or can be confined to the maintenance time window.<br>Choosing the latter option causes changes to the cluster (for example, node pool rolling-updates) and the subsequent reconciliation to only predictably happen during a defined time window (available since <a href=https://github.com/gardener/gardener/releases/tag/v1.4.0>Gardener version 1.4</a>).</p><p>For more information, see <a href=/docs/gardener/shoot/shoot_maintenance/#confine-specification-changesupdates-roll-out>Confine Specification Changes/Update Roll Out</a>.</p><blockquote class="alert alert-warning"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>alert-outline</title><path d="M12 2 1 21H23M12 6l7.53 13H4.47M11 10v4h2V10m-2 6v2h2V16"/></svg><p>Warning</p></div><p>Before applying such an update on minor or major releases, operators should check for all the breaking changes introduced in the target Kubernetes release changelog.</p></blockquote><h2 id=examples>Examples<a class=td-heading-self-link href=#examples aria-label="Heading self-link"></a></h2><p>In the examples for the <code>CloudProfile</code> and the shoot cluster specification, only the fields relevant for the example are shown.</p><h3 id=auto-update-of-kubernetes-version>Auto-Update of Kubernetes Version<a class=td-heading-self-link href=#auto-update-of-kubernetes-version aria-label="Heading self-link"></a></h3><p>Let&rsquo;s assume that the Kubernetes versions <code>1.10.5</code> and <code>1.11.0</code> were added in the following <code>CloudProfile</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.11.0
</span></span><span style=display:flex><span>    - version: 1.10.5
</span></span><span style=display:flex><span>    - version: 1.10.0
</span></span></code></pre></div><p>Before this change, the shoot cluster specification looked like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.10.0
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    timeWindow:
</span></span><span style=display:flex><span>      begin: 220000+0000
</span></span><span style=display:flex><span>      end: 230000+0000
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>As a consequence, the shoot cluster is updated to Kubernetes version <code>1.10.5</code> between 22:00-23:00 UTC. Your shoot cluster isn&rsquo;t updated automatically to <code>1.11.0</code>, even though it&rsquo;s the highest Kubernetes version in the <code>CloudProfile</code>, because Gardener only does automatic updates of the Kubernetes patch level.</p><h3 id=forceful-update-due-to-expired-kubernetes-version>Forceful Update Due to Expired Kubernetes Version<a class=td-heading-self-link href=#forceful-update-due-to-expired-kubernetes-version aria-label="Heading self-link"></a></h3><p>Let&rsquo;s assume the following <code>CloudProfile</code> exists on the cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.12.8
</span></span><span style=display:flex><span>    - version: 1.11.10
</span></span><span style=display:flex><span>    - version: 1.10.13
</span></span><span style=display:flex><span>    - version: 1.10.12
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2019-04-13T08:00:00Z&#34;</span>
</span></span></code></pre></div><p>Let&rsquo;s assume the shoot cluster has the following specification:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.10.12
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    timeWindow:
</span></span><span style=display:flex><span>      begin: 220000+0100
</span></span><span style=display:flex><span>      end: 230000+0100
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>The shoot cluster specification refers to a Kubernetes version that has an <code>expirationDate</code>. In the maintenance window on <code>2019-04-12</code>, the Kubernetes version stays the same as it’s still not expired. But in the maintenance window on <code>2019-04-14</code>, the Kubernetes version of the shoot cluster is updated to <code>1.10.13</code> (independently of the value of <code>.spec.maintenance.autoUpdate.kubernetesVersion</code>).</p><h3 id=forceful-update-to-new-minor-kubernetes-version>Forceful Update to New Minor Kubernetes Version<a class=td-heading-self-link href=#forceful-update-to-new-minor-kubernetes-version aria-label="Heading self-link"></a></h3><p>Let&rsquo;s assume the following <code>CloudProfile</code> exists on the cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.12.8
</span></span><span style=display:flex><span>    - version: 1.11.10
</span></span><span style=display:flex><span>    - version: 1.11.09
</span></span><span style=display:flex><span>    - version: 1.10.12
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2019-04-13T08:00:00Z&#34;</span>
</span></span></code></pre></div><p>Let&rsquo;s assume the shoot cluster has the following specification:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.10.12
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    timeWindow:
</span></span><span style=display:flex><span>      begin: 220000+0100
</span></span><span style=display:flex><span>      end: 230000+0100
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>The shoot cluster specification refers a Kubernetes version that has an <code>expirationDate</code>. In the maintenance window on <code>2019-04-14</code>, the Kubernetes version of the shoot cluster is updated to <code>1.11.10</code>, which is the highest patch version of minor target version <code>1.11</code> that follows the source version <code>1.10</code>.</p><h3 id=automatic-update-from-expired-machine-image-version>Automatic Update from Expired Machine Image Version<a class=td-heading-self-link href=#automatic-update-from-expired-machine-image-version aria-label="Heading self-link"></a></h3><p>Let&rsquo;s assume the following <code>CloudProfile</code> exists on the cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: coreos
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 2191.5.0
</span></span><span style=display:flex><span>    - version: 2191.4.1
</span></span><span style=display:flex><span>    - version: 2135.6.0
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2019-04-13T08:00:00Z&#34;</span>
</span></span></code></pre></div><p>Let&rsquo;s assume the shoot cluster has the following specification:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: name
</span></span><span style=display:flex><span>      maximum: 1
</span></span><span style=display:flex><span>      minimum: 1
</span></span><span style=display:flex><span>      maxSurge: 1
</span></span><span style=display:flex><span>      maxUnavailable: 0
</span></span><span style=display:flex><span>      image:
</span></span><span style=display:flex><span>        name: coreos
</span></span><span style=display:flex><span>        version: 2135.6.0
</span></span><span style=display:flex><span>        type: m5.large
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        type: gp2
</span></span><span style=display:flex><span>        size: 20Gi
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    timeWindow:
</span></span><span style=display:flex><span>      begin: 220000+0100
</span></span><span style=display:flex><span>      end: 230000+0100
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>The shoot cluster specification refers a machine image version that has an <code>expirationDate</code>. In the maintenance window on <code>2019-04-12</code>, the machine image version stays the same as it’s still not expired. But in the maintenance window on <code>2019-04-14</code>, the machine image version of the shoot cluster is updated to <code>2191.5.0</code> (independently of the value of <code>.spec.maintenance.autoUpdate.machineImageVersion</code>) as version <code>2135.6.0</code> is expired.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ade1d0180231461aa095f9c90b594f91>9 - Tailscale</h1><h1 id=access-the-kubernetes-apiserver-from-your-tailnet>Access the Kubernetes apiserver from your tailnet<a class=td-heading-self-link href=#access-the-kubernetes-apiserver-from-your-tailnet aria-label="Heading self-link"></a></h1><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>If you would like to strengthen the security of your Kubernetes cluster even further, this guide post explains how this can be achieved.</p><p>The most common way to secure a Kubernetes cluster which was created with Gardener is to apply the ACLs described in the <a href=https://github.com/stackitcloud/gardener-extension-acl>Gardener ACL Extension</a> repository or to use <a href=https://gardener.cloud/docs/gardener/exposureclasses/>ExposureClass</a>, which exposes the Kubernetes apiserver in a corporate network not exposed to the public internet.</p><p>However, those solutions are not without their drawbacks. Managing the ACL extension becomes fairly difficult with the growing number of participants, especially in a dynamic environment and work from home scenarios, and using ExposureClass requires you to first have a corporate network suitable for this purpose.</p><p>But there is a solution which bridges the gap between these two approaches by the use of a mesh VPN based on <a href=https://www.wireguard.com/>WireGuard</a>.</p><h2 id=tailscale>Tailscale<a class=td-heading-self-link href=#tailscale aria-label="Heading self-link"></a></h2><p>Tailscale is a mesh VPN network which uses Wireguard under the hood, but automates the key exchange procedure.
Please consult the official <a href=https://tailscale.com/kb/1151/what-is-tailscale>tailscale documentation</a> for a detailed explanation.</p><h2 id=target-architecture>Target Architecture<a class=td-heading-self-link href=#target-architecture aria-label="Heading self-link"></a></h2><p><img src=/__resources/tailscale.drawio_7f256a.svg alt=architecture></p><h3 id=installation>Installation<a class=td-heading-self-link href=#installation aria-label="Heading self-link"></a></h3><p>In order to be able to access the Kubernetes apiserver only from a tailscale VPN, you need this steps:</p><ol><li>Create a tailscale account and ensure <a href="https://tailscale.com/kb/1081/magicdns?q=magic">MagicDNS</a> is enabled.</li><li>Create an OAuth ClientID and Secret <a href=https://tailscale.com/kb/1236/kubernetes-operator#prerequisites>OAuth ClientID and Secret</a>. Don&rsquo;t forget to create the required tags.</li><li>Install the tailscale operator <a href=https://tailscale.com/kb/1236/kubernetes-operator#installation>tailscale operator</a>.</li></ol><p>If all went well after the operator installation, you should be able to see the tailscale operator by running <code>tailscale status</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># tailscale status</span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>100.83.240.121  tailscale-operator   tagged-devices linux   -
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h3 id=expose-the-kubernetes-apiserver>Expose the Kubernetes apiserver<a class=td-heading-self-link href=#expose-the-kubernetes-apiserver aria-label="Heading self-link"></a></h3><p>Now you are ready to expose the Kubernetes apiserver in the tailnet by annotating the service which was created by Gardener:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl annotate -n default kubernetes tailscale.com/expose=true tailscale.com/hostname=kubernetes
</span></span></code></pre></div><p>It is required to <code>kubernetes</code> as the hostname, because this is part of the certificate common name of the Kubernetes apiserver.</p><p>After annotating the service, it will be exposed in the tailnet and can be shown by running <code>tailscale status</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># tailscale status</span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>100.83.240.121  tailscale-operator   tagged-devices linux   -
</span></span><span style=display:flex><span>100.96.191.87   kubernetes           tagged-devices linux   idle, tx 19548 rx 71656
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h3 id=modify-the-kubeconfig>Modify the kubeconfig<a class=td-heading-self-link href=#modify-the-kubeconfig aria-label="Heading self-link"></a></h3><p>In order to access the cluster via the VPN, you must modify the kubeconfig to point to the Kubernetes service exposed in the tailnet, by changing the <code>server</code> entry to <code>https://kubernetes</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>  - cluster:
</span></span><span style=display:flex><span>      certificate-authority-data: &lt;base64 encoded secret&gt;
</span></span><span style=display:flex><span>      server: https://kubernetes
</span></span><span style=display:flex><span>    name: my-cluster
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h3 id=enable-acls-to-block-all-ips>Enable ACLs to Block All IPs<a class=td-heading-self-link href=#enable-acls-to-block-all-ips aria-label="Heading self-link"></a></h3><p>Now you are ready to use your cluster from every device which is part of your tailnet. Therefore you can now block all access to the Kubernetes apiserver with the ACL extension.</p><h2 id=caveats>Caveats<a class=td-heading-self-link href=#caveats aria-label="Heading self-link"></a></h2><h3 id=multiple-kubernetes-clusters>Multiple Kubernetes Clusters<a class=td-heading-self-link href=#multiple-kubernetes-clusters aria-label="Heading self-link"></a></h3><p>You can actually not join multiple Kubernetes Clusters to join your <code>tailnet</code> because the <code>kubernetes</code> service in every cluster would overlap.</p><h3 id=headscale>Headscale<a class=td-heading-self-link href=#headscale aria-label="Heading self-link"></a></h3><p>It is possible to host a tailscale coordination by your own if you do not want to rely on the service tailscale.com offers.
The <a href=https://github.com/juanfont/headscale>headscale project</a> is a open source implementation of this.</p><p>This works for basic tailscale VPN setups, but not for the tailscale operator described here, because <code>headscale</code> does not implement all required API endpoints for the tailscale operator.
The details can be found in this <a href=https://github.com/juanfont/headscale/issues/1202>Github Issue</a>.</p></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://gardener-cloud.slack.com/><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://x.com/GardenerProject><img src=/images/branding/x-logo-white.svg class=media-icon><div class=media-text>X</div></a></li></ul><span class=copyright>Copyright 2019-2025 Gardener project authors.
<a href=https://www.sap.com/about/legal/terms-of-use.html>Terms of Use
<i class="fa fa-external-link" aria-hidden=true></i>
</a>|
<a href=https://www.sap.com/about/legal/terms-of-use.html>Privacy Statement
<i class="fa fa-external-link" aria-hidden=true></i>
</a>|
<a href=https://www.sap.com/about/legal/terms-of-use.html>Legal Disclosure
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=/js/main.min.69e2c1ae9320465ab10236d9ef752c6a4442c54b48b883b17c497b7c7d96a796.js integrity="sha256-aeLBrpMgRlqxAjbZ73UsakRCxUtIuIOxfEl7fH2Wp5Y=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script><script src=/js/navbar.js></script><script src=/js/filtering.js></script><script src=/js/page-content.js></script></body></html>