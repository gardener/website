<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener â€“ Provider KubeVirt</title><link>https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-kubevirt/</link><description>Recent content in Provider KubeVirt on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-kubevirt/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Dev Setup</title><link>https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-kubevirt/docs/dev-setup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-kubevirt/docs/dev-setup/</guid><description>
&lt;h1 id="development-setup">Development Setup&lt;/h1>
&lt;p>This document describes the recommended development setup for the &lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt">KubeVirt provider extension&lt;/a>. Following the guidelines presented here would allow you to test the full Gardener reconciliation and deletion flows with the KubeVirt provider extension and the &lt;a href="https://github.com/gardener/machine-controller-manager-provider-kubevirt">KubeVirt MCM extension&lt;/a>.&lt;/p>
&lt;p>In this setup, only Gardener itself is running in your local development cluster. All other components, as well as KubeVirt VMs, are deployed and run on external clusters, which avoids high CPU and memory load on your local laptop.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>Follow the steps outlined in &lt;a href="https://gardener.cloud/docs/gardener/development/local_setup/">Setting up a local development environment&lt;/a> for Gardener in order to install all needed prerequisites and enable running &lt;code>gardener-apiserver&lt;/code>, &lt;code>gardener-controller-manager&lt;/code>, and &lt;code>gardenlet&lt;/code> locally. You can use either minikube, kind, or the nodeless cluster as your local development cluster.&lt;/p>
&lt;p>Before continuing, copy all files from &lt;code>docs/development/manifests&lt;/code> and &lt;code>docs/development/scripts&lt;/code> to your &lt;code>dev&lt;/code> directory and adapt them as needed. The sections that follow assume that you have already done this and all needed manifests and scripts can be found in your &lt;code>dev&lt;/code> directory.&lt;/p>
&lt;h2 id="creating-the-controllerregistrations">Creating the ControllerRegistrations&lt;/h2>
&lt;p>Before you register seeds or create shoots, you need to register all needed extensions using &lt;code>ControllerRegistration&lt;/code> resources. The easiest way to manage &lt;code>ControllerRegistrations&lt;/code> is via &lt;a href="https://github.com/gardener/gem">gem&lt;/a>.&lt;/p>
&lt;p>After installing &lt;code>gem&lt;/code>, create a &lt;code>requirements.yaml&lt;/code> file similar to &lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/requirements.yaml">requirements.yaml&lt;/a>. The example file contains only the extensions needed for the development setup described here, but you could add any other Gardener extensions you may need.&lt;/p>
&lt;p>In your &lt;code>requirements.yaml&lt;/code> file you can refer to a released extension version, or to a revision (commit) from a Gardener repo or your fork of it. This version or revision is used to find the correct &lt;code>controller-registration.yaml&lt;/code> file for the extension.&lt;/p>
&lt;p>You can generate or update the &lt;code>controller-registrations.yaml&lt;/code> file out of your &lt;code>requirements.yaml&lt;/code> file by running:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>gem ensure --requirements dev/requirements.yaml --controller-registrations dev/controller-registrations.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>After generating or updating the &lt;code>controller-registrations.yaml&lt;/code> file, review it and make sure all versions are the ones you want to use for your tests. For example, if you are working on a PR for the KubeVirt provider extension, in addition to specifying the revision in your fork in &lt;code>requirements.yaml&lt;/code>, you may need to change the version from &lt;code>0.1.0-dev&lt;/code> to something unique to you or your PR, e.g. &lt;code>0.1.0-dev-johndoe&lt;/code>. You can also add &lt;code>pullPolicy: Always&lt;/code> to ensure that if you push a new extension image with that version and delete the corresponding pod, the new image will always be pulled when the pod is recreated.&lt;/p>
&lt;p>Once you are satisfied with your controller registrations, apply the &lt;code>controller-registrations.yaml&lt;/code> to your local Gardener:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/controller-registrations.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="registering-the-seed-cluster">Registering the Seed Cluster&lt;/h2>
&lt;p>Create or choose an external cluster, different from your local development cluster, to register as seed in your local Gardener. This can be any cluster and it can be the same or different from your &lt;a href="#creating-the-provider-cluster">provider cluster&lt;/a>. It is recommended to use a different cluster to avoid confusion between the two. If you want to use your provider cluster as seed, first create it as described below and then return to this step.&lt;/p>
&lt;p>To register your cluster as a seed, create the secret containing the kubeconfig for your seed cluster, the secret containing the credentials for your cloud provider (e.g. GCP), and the seed resource itself. See the following files as examples:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/secret-gcp1-kubeconfig.yaml">secret-gcp1-kubeconfig.yaml&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/secret-seed-operator-gcp.yaml">secret-seed-operator-gcp.yaml&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/seed-gcp1.yaml">seed-gcp1.yaml&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/secret-gcp1-kubeconfig.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/secret-seed-operator-gcp.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/seed-gcp1.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="creating-the-project">Creating the Project&lt;/h2>
&lt;p>At this point, you should create a &lt;code>dev&lt;/code> project in your local Gardener.&lt;/p>
&lt;p>Create the project resource for your local &lt;code>dev&lt;/code> project, see &lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/project-dev.yaml">project-dev&lt;/a> as an example.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/project-dev.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="creating-the-dns-domain-secrets">Creating the DNS Domain Secrets&lt;/h2>
&lt;p>At this point, you should create the domain secrets used by the DNS extension.&lt;/p>
&lt;p>If you want to use an external DNS provider (e.g. route53), create default and internal domain secrets similar to &lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/secret-default-domain.yaml">secret-default-domain.yaml&lt;/a> and &lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/secret-internal-domain.yaml">secret-internal-domain.yaml&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/secret-default-domain.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/secret-internal-domain.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Alternatively, if you don&amp;rsquo;t want to use an external DNS provider and use &lt;code>nip.io&lt;/code> addresses instead, create just an internal domain secret similar to &lt;a href="https://github.com/gardener/gardener/blob/master/example/10-secret-internal-domain-unmanaged.yaml">10-secret-internal-domain-unmanaged.yaml&lt;/a>. For more information, see &lt;a href="https://gardener.cloud/docs/gardener/development/local_setup/#prepare-the-gardener">Prepare the Gardener&lt;/a>.&lt;/p>
&lt;h2 id="creating-the-provider-cluster">Creating the Provider Cluster&lt;/h2>
&lt;p>Create or choose an external cluster, different from your local development cluster, to use as a provider cluster. The only requirement to this cluster is that virtualization extensions are supported on its nodes. You can check if this is the case as described in &lt;a href="https://kubevirt.io/pages/cloud.html">Easy install using Cloud Providers&lt;/a>, by executing the command &lt;code>egrep 'svm|vmx' /proc/cpuinfo&lt;/code> and checking for non-empty output.&lt;/p>
&lt;h3 id="creating-an-os-image-with-nested-virtualization-enabled">Creating an OS Image with Nested Virtualization Enabled&lt;/h3>
&lt;p>Before you can create such a cluster, you need to ensure that nested virtualizaton is enabled for its instances by using an appropriate OS image. To create such an image in GCP, follow the steps described in &lt;a href="https://cloud.google.com/compute/docs/instances/enable-nested-virtualization-vm-instances">Enabling nested virtualization for VM instances&lt;/a>. For example, to create a custom Ubuntu image with nested virtualizaton enabled based on Ubuntu 18.04, execute the following commands:&lt;/p>
&lt;pre tabindex="0">&lt;code>gcloud compute disks create ubuntu-disk1
--image-project ubuntu-os-cloud \
--image ubuntu-1804-bionic-v20200916 \
--zone us-central1-b
gcloud compute images create ubuntu-1804-bionic-v20200916-vmx-enabled \
--source-disk ubuntu-disk1 \
--source-disk-zone us-central1-b \
--licenses &amp;#34;https://compute.googleapis.com/compute/v1/projects/vm-options/global/licenses/enable-vmx&amp;#34;
gcloud compute images list | grep ubuntu
&lt;/code>&lt;/pre>&lt;p>Once the image has been created, to create the provider cluster, you could use any Kubernetes provisioning tool, including of course Gardener itself, to create a cluster using this image.&lt;/p>
&lt;h3 id="creating-the-provider-cluster-using-gardener">Creating the Provider Cluster Using Gardener&lt;/h3>
&lt;p>To create the provider cluster using Gardener, simply create a shoot in the seed you registered previously using a custom GCP cloud profile that contains the above image, such as &lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/cloudprofile-gcp.yaml">cloudprofile-gcp.yaml&lt;/a>. To do this, follow these steps:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Create the custom GCP cloud profile, for example &lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/cloudprofile-gcp.yaml">cloudprofile-gcp.yaml&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/cloudprofile-gcp.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Create the shoot secret binding, you could bind to the &lt;code>seed-operator-gcp&lt;/code> secret you created previously for your seed, see &lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/secretbinding-shoot-operator-gcp.yaml">secretbinding-shoot-operator-gcp.yaml&lt;/a> as an example.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/secretbinding-shoot-operator-gcp.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Create the GCP shoot itself. See &lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/shoot-gcp-vmx.yaml">shoot-gcp-vmx.yaml&lt;/a> as an example. Note that this shoot should use the image with name &lt;code>ubuntu&lt;/code> and version &lt;code>18.4.20200916-vmx&lt;/code> from the custom GCP cloud profile you created previously. Also, please rename the shoot to contain an unique prefix such as your github username, e.g. &lt;code>johndoe-gcp-vmx&lt;/code>, to avoid naming conflicts in GCP.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/shoot-gcp-vmx.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>During the reconciliation by your local &lt;code>gardenlet&lt;/code>, you may want to connect to the seed to monitor the shoot namespace &lt;code>shoot--dev--&amp;lt;prefix&amp;gt;-gcp-vmx&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Once the shoot is successfully reconciled by your local &lt;code>gardenlet&lt;/code>, get its kubeconfig by executing:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl get secret &amp;lt;prefix&amp;gt;-gcp-vmx.kubeconfig -n garden-dev -o jsonpath={.data.kubeconfig} | base64 -d &amp;gt; dev/kubeconfig-gcp-vmx.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h3 id="installing-kubevirt-cdi-and-multus-in-the-provider-cluster">Installing KubeVirt, CDI, and Multus in the Provider Cluster&lt;/h3>
&lt;p>Once the provider cluster has been created (with Gardener or any other provisioning tool), you should install KubeVirt, CDI, and optionally Multus in it so that it can serve its purpose as a provider cluster.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Install KubeVirt and CDI in this cluster by executing the &lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/hack/kubevirt/install-kubevirt.sh">install-kubevirt.sh&lt;/a> script:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>export KUBECONFIG=dev/kubeconfig-gcp-vmx.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>hack/kubevirt/install-kubevirt.sh
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Optionally, to use networking features, install &lt;a href="https://github.com/k8snetworkplumbingwg/multus-cni/blob/master/docs/quickstart.md">Multus CNI&lt;/a> as described in its documentation, or by applying the provided &lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/hack/kubevirt/multus.yaml">multus.yaml&lt;/a> manifest.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>export KUBECONFIG=dev/kubeconfig-gcp-vmx.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f hack/kubevirt/multus.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Note:&lt;/strong> In order to use any additional CNI plugins, the plugin binaries must be present in the &lt;code>/opt/cni/bin&lt;/code> directory of the provider cluster nodes. For testing purposes, they can be installed manually by downloading a &lt;a href="https://github.com/containernetworking/plugins">containernetworking/plugins&lt;/a> release and copying the needed plugins to the &lt;code>/opt/cni/bin&lt;/code> directory of each provider cluster node.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="testing-the-gardener-reconciliation-flow">Testing the Gardener Reconciliation Flow&lt;/h2>
&lt;p>To test the Gardener reconciliation flow with the KubeVirt provider extensions, create the KubeVirt shoot cluster in your local &lt;code>dev&lt;/code> project, by following these steps:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Create the KubeVirt cloud profile, for example &lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/cloudprofile-kubevirt.yaml">cloudprofile-kubevirt.yaml&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/cloudprofile-kubevirt.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Note:&lt;/strong> The example cloud profile is intentionally rather simple and does not take advantage of some of the features supported by the KubeVirt provider extension. To test these features, modify the cloud profile manifest accordingly. For more information, see &lt;a href="https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-kubevirt/docs/usage-as-operator/">Using the KubeVirt provider extension with Gardener as operator&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Create the shoot secret and secret binding. You should create a secret containing the kubeconfig for your provider cluster, and a corresponding secret binding:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl create secret generic kubevirt-credentials -n garden-dev --from-file=kubeconfig=dev/kubeconfig-gcp-vmx.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/secretbinding-kubevirt-credentials.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Create the KubeVirt shoot itself. See &lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/shoot-kubevirt.yaml">shoot-kubevirt.yaml&lt;/a> as an example. Note that the nodes CIDR for this shoot must be the same range as the pods CIDR of your provider cluster.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/shoot-kubevirt.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Note:&lt;/strong> The example shoot is intentionally very simple and does not take advantage of many of the features supported by the KubeVirt provider extension. To test these features, modify the shoot manifest accordingly. For more information, see &lt;a href="https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-kubevirt/docs/usage-as-end-user/">Using the KubeVirt provider extension with Gardener as end-user&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>During the shoot reconciliation by your local &lt;code>gardenlet&lt;/code>, you may want to:&lt;/p>
&lt;ul>
&lt;li>Monitor the &lt;code>gardenlet&lt;/code> logs in your local console where &lt;code>gardenlet&lt;/code> is running.&lt;/li>
&lt;li>Connect to the seed to monitor the shoot namespace &lt;code>shoot--dev--kubevirt&lt;/code> and the logs of the KubeVirt provider extension in the &lt;code>extension-provider-kubevirt-*&lt;/code> namespace.&lt;/li>
&lt;li>Connect to the provider cluster to monitor the &lt;code>default&lt;/code> namespace where VMs and VMIs are being created.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Once the shoot has been successfully reconciled, get its kubeconfig by executing:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl get secret kubevirt.kubeconfig -n garden-dev -o jsonpath={.data.kubeconfig} | base64 -d &amp;gt; dev/kubeconfig-kubevirt.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>At this point, you may want to connect to the KubeVirt shoot and check if it&amp;rsquo;s usable.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="testing-the-gardener-deletion-flow">Testing the Gardener Deletion Flow&lt;/h2>
&lt;p>To test the Gardener deletion flow with the KubeVirt provider extensions, delete the KubeVirt shoot cluster in your local &lt;code>dev&lt;/code> project, by following these steps:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Delete the KubeVirt shoot itself using the &lt;a href="https://github.com/gardener/gardener/blob/master/hack/usage/delete">delete&lt;/a> script.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl annotate shoot kubevirt -n garden-dev confirmation.gardener.cloud/deletion=1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl delete shoot kubevirt -n garden-dev
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>During the shoot deletion by your local &lt;code>gardenlet&lt;/code>, you may want to:&lt;/p>
&lt;ul>
&lt;li>Monitor the &lt;code>gardenlet&lt;/code> logs in your local console where &lt;code>gardenlet&lt;/code> is running.&lt;/li>
&lt;li>Connect to the seed to monitor the shoot namespace &lt;code>shoot--dev--kubevirt&lt;/code> and the logs of the KubeVirt provider extension in the &lt;code>extension-provider-kubevirt-*&lt;/code> namespace.&lt;/li>
&lt;li>Connect to the provider cluster to monitor the &lt;code>default&lt;/code> namespace where VMs and VMIs are being created.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol></description></item><item><title>Docs: Local Setup Admission</title><link>https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-kubevirt/docs/local-setup-admission/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-kubevirt/docs/local-setup-admission/</guid><description>
&lt;h3 id="admission-kubevirt">admission-kubevirt&lt;/h3>
&lt;p>&lt;code>admission-kubevirt&lt;/code> is an admission webhook server which is responsible for the validation of the cloud provider (KubeVirt in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&amp;rsquo;t be able to perform similar validation.&lt;/p>
&lt;p>Follow the steps below to run the admission webhook server locally.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Start the Gardener API server.&lt;/p>
&lt;p>For details, check the Gardener &lt;a href="https://gardener.cloud/docs/gardener/development/local_setup/">local setup&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Start the webhook server&lt;/p>
&lt;p>Make sure that the &lt;code>KUBECONFIG&lt;/code> environment variable is pointing to the local garden cluster.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make start-admission
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Setup the &lt;code>ValidatingWebhookConfiguration&lt;/code>.&lt;/p>
&lt;p>&lt;code>hack/dev-setup-admission-kubevirt.sh&lt;/code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the &lt;code>ValidatingWebhookConfiguration&lt;/code> manifest.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./hack/dev-setup-admission-kubevirt.sh
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;p>You are now ready to experiment with the &lt;code>admission-kubevirt&lt;/code> webhook server locally.&lt;/p></description></item><item><title>Docs: Usage As End User</title><link>https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-kubevirt/docs/usage-as-end-user/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-kubevirt/docs/usage-as-end-user/</guid><description>
&lt;h1 id="using-the-kubevirt-provider-extension-with-gardener-as-end-user">Using the KubeVirt provider extension with Gardener as end-user&lt;/h1>
&lt;p>The &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml">&lt;code>core.gardener.cloud/v1beta1.Shoot&lt;/code> resource&lt;/a> declares a few fields that are meant to contain provider-specific configuration.&lt;/p>
&lt;p>This document describes how this configuration looks like for KubeVirt and provides an example &lt;code>Shoot&lt;/code> manifest with minimal configuration that you can use to create a KubeVirt shoot cluster (without the landscape-specific information such as cloud profile names, secret binding names, etc.).&lt;/p>
&lt;h2 id="provider-secret-data">Provider Secret Data&lt;/h2>
&lt;p>Every shoot cluster references a &lt;code>SecretBinding&lt;/code> which itself references a &lt;code>Secret&lt;/code>, and this &lt;code>Secret&lt;/code> contains the kubeconfig of your &lt;em>KubeVirt provider cluster&lt;/em>. This cluster is the cluster where KubeVirt itself is installed, and that hosts the KubeVirt virtual machines used as shoot worker nodes. This &lt;code>Secret&lt;/code> must look as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: provider-cluster-kubeconfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden-dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type: Opaque
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeconfig: base64(kubeconfig)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="permissions">Permissions&lt;/h3>
&lt;p>All KubeVirt resources (&lt;code>VirtualMachines&lt;/code>, &lt;code>DataVolumes&lt;/code>, etc.) are created in the namespace of the current context of the above kubeconfig, that is &lt;code>my-shoot&lt;/code> in the example below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>current-context: provider-cluster
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>contexts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: provider-cluster
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> context:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster: provider-cluster
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: my-shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user: provider-cluster-token
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If no namespace is specified, the &lt;code>default&lt;/code> namespace is assumed. You can use the same namespace for multiple shoots. The user specified in the &lt;code>kubeconfig&lt;/code> must have permissions to read and write KubeVirt and Kubernetes core resources in this namespace.&lt;/p>
&lt;h2 id="infrastructureconfig">&lt;code>InfrastructureConfig&lt;/code>&lt;/h2>
&lt;p>The infrastructure configuration can contain additional networks used by the shoot worker nodes. If this configuration is empty, all KubeVirt virtual machines used as shoot worker nodes use only the pod network of the provider cluster.&lt;/p>
&lt;p>An example &lt;code>InfrastructureConfig&lt;/code> for the KubeVirt extension looks as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: InfrastructureConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>networks:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sharedNetworks:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Reference to the network defined by the NetworkAttachmentDefinition default/net-conf&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: net-conf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tenantNetworks:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: network-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Configuration for the CNI plugins bridge and firewall&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config: |&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;cniVersion&amp;#34;: &amp;#34;0.4.0&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;name&amp;#34;: &amp;#34;bridge-firewall&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;plugins&amp;#34;: [
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;type&amp;#34;: &amp;#34;bridge&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;isGateway&amp;#34;: true,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;isDefaultGateway&amp;#34;: true,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;ipMasq&amp;#34;: true,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;ipam&amp;#34;: {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;type&amp;#34;: &amp;#34;host-local&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;subnet&amp;#34;: &amp;#34;10.100.0.0/16&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> },
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;type&amp;#34;: &amp;#34;firewall&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> ]
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> }&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Don&amp;#39;t attach the pod network at all, instead use this network as default&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> default: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>A non-empty infrastructure configuration can contain:&lt;/p>
&lt;ul>
&lt;li>References to pre-existing, &lt;em>shared&lt;/em> networks that can be shared between multiple shoots. These networks must exist in the provider cluster prior to shoot creation.&lt;/li>
&lt;li>CNI configurations for &lt;em>tenant&lt;/em> networks that are created, updated, and deleted together with the shoot. If one of these networks is marked as &lt;code>default: true&lt;/code>, it becomes the default network instead of the pod network of the provider cluster. This can be used to achieve higher level of network isolation, since the networks of the different shoots can be isolated from each other, and in some cases better performance.&lt;/li>
&lt;/ul>
&lt;p>Both shared and tenant networks are maintained in the provider cluster via &lt;a href="https://github.com/intel/multus-cni/blob/master/README.md">Multus CNI&lt;/a> &lt;a href="https://github.com/k8snetworkplumbingwg/multus-cni/blob/master/docs/quickstart.md">NetworkAttachmentDefinition&lt;/a> resources. For shared networks, these resources must be created in advance, while for tenant networks they are managed by the shoot reconciliation process.&lt;/p>
&lt;p>In order to use any additional CNI plugins in a tenant network configuration, such as &lt;code>bridge&lt;/code> or &lt;code>firewall&lt;/code> in the above example, the plugin binaries must be present in the &lt;code>/opt/cni/bin&lt;/code> directory of the provider cluster nodes. They can be installed manually by downloading a &lt;a href="https://github.com/containernetworking/plugins">containernetworking/plugins&lt;/a> release (not recommended except for testing a new configuration). Alternatively, they can be installed via a specially prepared daemon set that ensures the existence of the plugin binaries on each provider cluster node.&lt;/p>
&lt;p>&lt;strong>Note:&lt;/strong> Although it is possible to update the network configuration in &lt;code>InfrastructureConfig&lt;/code>, any such changes will result in recreating all KubeVirt VMs, so that the new network configuration is properly taken into account. This will be done automatically by the MCM using rolling update.&lt;/p>
&lt;h2 id="controlplaneconfig">&lt;code>ControlPlaneConfig&lt;/code>&lt;/h2>
&lt;p>The control plane configuration contains options for the KubeVirt-specific control plane components. Currently, the only component deployed by the KubeVirt extension is the &lt;a href="https://github.com/kubevirt/cloud-provider-kubevirt">KubeVirt Cloud Controller Manager (CCM)&lt;/a>.&lt;/p>
&lt;p>An example &lt;code>ControlPlaneConfig&lt;/code> for the KubeVirt extension looks as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ControlPlaneConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cloudControllerManager:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> featureGates:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> CustomResourceValidation: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>cloudControllerManager.featureGates&lt;/code> contains a map of explicitly enabled or disabled feature gates. For production usage it&amp;rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability. If you don&amp;rsquo;t want to configure anything for the CCM, simply omit the key in the YAML specification.&lt;/p>
&lt;h2 id="workerconfig">&lt;code>WorkerConfig&lt;/code>&lt;/h2>
&lt;p>The KubeVirt extension supports specifying additional data volumes per machine in the worker pool. For each data volume, you must specify a name and a type.&lt;/p>
&lt;p>Below is an example &lt;code>Shoot&lt;/code> resource snippet with root volume and data volumes:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> workers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: cpu-worker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volume:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> size: 20Gi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dataVolumes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: volume-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> size: 10Gi
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Note:&lt;/strong> The additional data volumes will be attached as blank disks to the KubeVirt VMs. These disks must be formatted and mounted manually to the VM before they can be used.&lt;/p>
&lt;p>The KubeVirt extension does not currently support encryption for volumes.&lt;/p>
&lt;p>Additionally, it is possible to specify additional KubeVirt-specific options for configuring the worker pools. They can be specified in &lt;code>.spec.provider.workers[].providerConfig&lt;/code> and are evaluated by the KubeVirt worker controller when it reconciles the shoot machines.&lt;/p>
&lt;p>An example &lt;code>WorkerConfig&lt;/code> for the KubeVirt extension looks as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: WorkerConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>devices:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># disks allow to customize disks attached to KubeVirt VM&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># check [link](https://kubevirt.io/user-guide/#/creation/disks-and-volumes?id=disks-and-volumes) for full specification and options&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> disks:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># name must match defined dataVolume name&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># to modify root volume the name must be equal to &amp;#39;root-disk&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: root-disk &lt;span style="color:#008000"># modify root-disk&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># disk type, check [link](https://kubevirt.io/user-guide/#/creation/disks-and-volumes?id=disks) for more types&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> disk:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># bus indicates the type of disk device to emulate.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> bus: virtio
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># set disk device cache&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cache: writethrough
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># dedicatedIOThread indicates this disk should have an exclusive IO Thread&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dedicatedIOThread: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: volume-1 &lt;span style="color:#008000"># modify dataVolume named volume-1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> disk: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># whether to have random number generator from host&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rng: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># whether or not to enable virtio multi-queue for block devices&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> blockMultiQueue: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># if specified, virtual network interfaces configured with a virtio bus will also enable the vhost multiqueue feature&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networkInterfaceMultiQueue: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cpu:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># number of cores inside the VMI&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cores: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># number of sockets inside the VMI&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sockets: 2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># number of threads inside the VMI&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> threads: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># models specifies the CPU model of the VMI&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># list of available models https://github.com/libvirt/libvirt/tree/master/src/cpu_map.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># and options https://libvirt.org/formatdomain.html#cpu-model-and-topology&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> model: &lt;span style="color:#a31515">&amp;#34;host-model&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># features specifies the CPU features list inside the VMI&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> features:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#a31515">&amp;#34;pcid&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># dedicatedCPUPlacement requests the scheduler to place the VirtualMachineInstance on a node&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># with dedicated pCPUs and pin the vCPUs to it.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dedicatedCpuPlacement: &lt;span style="color:#00f">false&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># isolateEmulatorThread requests one more dedicated pCPU to be allocated for the VMI to place the emulator thread on it.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> isolateEmulatorThread: &lt;span style="color:#00f">false&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># memory configuration for KubeVirt VMs, allows to set &amp;#39;hugepages&amp;#39; and &amp;#39;guest&amp;#39; settings. &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># See https://kubevirt.io/api-reference/master/definitions.html#_v1_memory&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>memory:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># hugepages requires appropriate feature gate to be enabled, take a look at the following links for more details:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># * k8s - https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># * okd - https://docs.okd.io/latest/scalability_and_performance/what-huge-pages-do-and-how-they-are-consumed-by-apps.html&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hugepages:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pageSize: &lt;span style="color:#a31515">&amp;#34;2Mi&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># guest allows to specifying the amount of memory which is visible inside the Guest OS. It must lie between requests and limits.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Defaults to the requested memory in the machineTypes.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> guest: &lt;span style="color:#a31515">&amp;#34;1Gi&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># overcommitGuestOverhead informs the scheduler to not take the guest-management overhead into account. Instead&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># put the overhead only into the container&amp;#39;s memory limit. This can lead to crashes if&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># all memory is in use on a node. Defaults to false.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># For more details take a look at https://kubevirt.io/user-guide/#/usage/overcommit?id=overcommit-the-guest-overhead&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>overcommitGuestOverhead: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># DNS policy for KubeVirt VMs. Valid values are &amp;#39;ClusterFirstWithHostNet&amp;#39;, &amp;#39;ClusterFirst&amp;#39;, &amp;#39;Default&amp;#39; or &amp;#39;None&amp;#39;.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Defaults to &amp;#39;ClusterFirst`.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># See https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dnsPolicy: ClusterFirst
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># DNS configuration for KubeVirt VMs, merged with the generated DNS configuration based on dnsPolicy.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># See https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dnsConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nameservers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - 8.8.8.8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Disable using pre-allocated data volumes. Defaults to &amp;#39;false&amp;#39;.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>disablePreAllocatedDataVolumes: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># cpu allows to set the CPU topology of the VMI&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># See https://kubevirt.io/api-reference/master/definitions.html#_v1_cpu&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Currently, these KubeVirt-specific options may include:&lt;/p>
&lt;ul>
&lt;li>The CPU topology and memory configuration of the KubVirt VMs. For more information, see &lt;a href="https://kubevirt.io/api-reference/master/definitions.html#_v1_cpu">CPU.v1&lt;/a> and &lt;a href="https://kubevirt.io/api-reference/master/definitions.html#_v1_memory">Memory.v1&lt;/a>.&lt;/li>
&lt;li>The DNS policy and DNS configuration of the KubeVirt VMs. For more information, see &lt;a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">DNS for Services and Pods&lt;/a>.&lt;/li>
&lt;li>Whether to use &lt;em>pre-allocated data volumes&lt;/em> with KubeVirt VMs. With pre-allocated data volumes (the default), a data volume is created in advance for each machine class, the OS image is imported into this volume only once, and actual KubeVirt VM data volumes are cloned from this data volume. Typically, this significantly speeds up the data volume creation process. You can disable this feature by setting the &lt;code>disablePreAllocatedDataVolumes&lt;/code> option to &lt;code>true&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="region-and-zone-support">Region and Zone Support&lt;/h2>
&lt;p>Nodes in the provider cluster may belong to provider-specific regions and zones, and Kubernetes would then use this information to spread pods across zones as described in &lt;a href="https://kubernetes.io/docs/setup/best-practices/multiple-zones/">Running in multiple zones&lt;/a>. You may want to take advantage of these capabilities in the shoot cluster as well.&lt;/p>
&lt;p>To achieve this, the KubeVirt provider extension ensures that the region and zones specified in the &lt;code>Shoot&lt;/code> resource are taken into account when creating the KubeVirt VMs used as shoot cluster nodes.&lt;/p>
&lt;p>Below is an example &lt;code>Shoot&lt;/code> resource snippet with region and zones:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> region: europe-west1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> workers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: cpu-worker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> zones:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - europe-west1-c
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - europe-west1-d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The shoot region and zones must correspond to the region and zones of the provider cluster. A KubeVirt VM designated for specific region and zone will only be scheduled on provider cluster nodes belonging to these region and zone. If there are no such nodes, or they have insufficient resources, the KubeVirt VM may remain in &lt;code>Pending&lt;/code> state for a longer period and the shoot reconciliation may fail. Therefore, always make sure that the provider cluster contains nodes for all zones specified in the shoot.&lt;/p>
&lt;p>If multiple zones are specified for a worker pool, the KubeVirt VMs will be equally distributed over these zones in the specified order.&lt;/p>
&lt;p>If your provider cluster is not region and zone aware, or if it contains nodes that don&amp;rsquo;t belong to any region or zone, you can use &lt;code>default&lt;/code> as a region or zone name in the &lt;code>Shoot&lt;/code> resource to target such nodes.&lt;/p>
&lt;p>Note that the &lt;code>region&lt;/code> and &lt;code>zones&lt;/code> are mandatory fields in the &lt;code>Shoot&lt;/code> resource, so you must specify either a concrete region / zone or &lt;code>default&lt;/code>.&lt;/p>
&lt;p>Once the KubeVirt VMs are scheduled on the correct provider cluster nodes, the KubeVirt Cloud Controller Manager (CCM) mentioned above will appropriately label the shoot worker nodes themselves with the appropriate &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/">region and zone labels&lt;/a>, by propagating the region and zone from the provider cluster nodes, so that Kubernetes multi-zone capabilities are also available in the shoot cluster.&lt;/p>
&lt;h2 id="example-shoot-manifest">Example &lt;code>Shoot&lt;/code> Manifest&lt;/h2>
&lt;p>Please find below an example &lt;code>Shoot&lt;/code> manifest for one availability zone:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: johndoe-kubevirt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden-dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cloudProfileName: kubevirt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretBindingName: provider-cluster-kubeconfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> region: europe-west1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: kubevirt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># infrastructureConfig:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># kind: InfrastructureConfig&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># networks:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># tenantNetworks:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># - name: network-1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># config: &amp;#34;{...}&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># default: true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># controlPlaneConfig:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># kind: ControlPlaneConfig&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># cloudControllerManager:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># featureGates:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># CustomResourceValidation: true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> workers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: cpu-worker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machine:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: standard-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> image:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: ubuntu
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: &lt;span style="color:#a31515">&amp;#34;18.04&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> minimum: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maximum: 2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volume:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> size: 20Gi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># dataVolumes:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># - name: volume-1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># type: default&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># size: 10Gi&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># providerConfig:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># kind: WorkerConfig&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># disablePreAllocatedDataVolumes: true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> zones:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - europe-west1-c
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networking:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: calico
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pods: 100.96.0.0/11
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Must match the IPAM subnet of the default tenant network, if present.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Otherwise, must be the same as the provider cluster pod network range.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodes: 10.225.128.0/17 &lt;span style="color:#008000"># 10.100.0.0/16&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> services: 100.64.0.0/13
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.17.8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maintenance:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> autoUpdate:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetesVersion: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineImageVersion: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> addons:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetesDashboard:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nginxIngress:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Usage As Operator</title><link>https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-kubevirt/docs/usage-as-operator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-kubevirt/docs/usage-as-operator/</guid><description>
&lt;h1 id="using-the-kubevirt-provider-extension-with-gardener-as-operator">Using the KubeVirt provider extension with Gardener as operator&lt;/h1>
&lt;p>The &lt;a href="https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml">&lt;code>core.gardener.cloud/v1beta1.CloudProfile&lt;/code> resource&lt;/a> declares a &lt;code>providerConfig&lt;/code> field that is meant to contain provider-specific configuration. The &lt;a href="https://github.com/gardener/gardener/blob/master/example/50-seed.yaml">&lt;code>core.gardener.cloud/v1beta1.Seed&lt;/code> resource&lt;/a> is structured in a similar way. Additionally, it allows configuring settings for the backups of the main etcds&amp;rsquo; data of shoot clusters control planes running in this seed cluster.&lt;/p>
&lt;p>This document explains what is necessary to configure for this provider extension.&lt;/p>
&lt;h2 id="cloudprofile-resource">&lt;code>CloudProfile&lt;/code> resource&lt;/h2>
&lt;p>In this section we are describing how the configuration for &lt;code>CloudProfile&lt;/code>s looks like for KubeVirt and provide an example &lt;code>CloudProfile&lt;/code> manifest with minimal configuration that you can use to allow creating KubeVirt shoot clusters.&lt;/p>
&lt;h3 id="cloudprofileconfig">&lt;code>CloudProfileConfig&lt;/code>&lt;/h3>
&lt;p>The cloud profile configuration contains information about the machine images source URLs. You have to map every version that you specify in &lt;code>.spec.machineImages[].versions&lt;/code> here so that the KubeVirt extension could find the source URL for every version you want to offer.&lt;/p>
&lt;p>An example &lt;code>CloudProfileConfig&lt;/code> for the KubeVirt extension looks as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: CloudProfileConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>machineImages:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: ubuntu
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> versions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: &lt;span style="color:#a31515">&amp;#34;18.04&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sourceURL: https://cloud-images.ubuntu.com/bionic/current/bionic-server-cloudimg-amd64.img
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># machineTypes extend cloud profile&amp;#39;s spec.machineType object to KubeVirt provider specific config&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>machineTypes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># name is used as a reference to the machineType object&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: standard-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># limits is equivalent to resource limits of pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-requests-and-limits-of-pod-and-container&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> limits:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cpu: &lt;span style="color:#a31515">&amp;#34;2&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memory: 8Gi
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="example-cloudprofile-manifest">Example &lt;code>CloudProfile&lt;/code> manifest&lt;/h3>
&lt;p>Please find below an example &lt;code>CloudProfile&lt;/code> manifest:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: CloudProfile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: kubevirt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: kubevirt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> providerConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: CloudProfileConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineImages:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: ubuntu
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> versions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: &lt;span style="color:#a31515">&amp;#34;18.04&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sourceURL: https://cloud-images.ubuntu.com/bionic/current/bionic-server-cloudimg-amd64.img
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> versions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.18.5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.17.8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineImages:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: ubuntu
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> versions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: &lt;span style="color:#a31515">&amp;#34;18.04&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineTypes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: standard-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cpu: &lt;span style="color:#a31515">&amp;#34;1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> gpu: &lt;span style="color:#a31515">&amp;#34;0&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memory: 4Gi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumeTypes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> class: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> regions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: europe-west1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> zones:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: europe-west1-b
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: europe-west1-c
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: europe-west1-d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="seed-resource">&lt;code>Seed&lt;/code> resource&lt;/h2>
&lt;p>This provider extension does not support any provider configuration for the &lt;code>Seed&lt;/code>&amp;rsquo;s &lt;code>.spec.provider.providerConfig&lt;/code> field.&lt;/p></description></item></channel></rss>