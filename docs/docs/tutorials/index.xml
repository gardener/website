<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener – Tutorials</title><link>https://gardener.cloud/docs/tutorials/</link><description>Recent content in Tutorials on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Tue, 01 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://gardener.cloud/docs/tutorials/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Authenticating with an Identity Provider</title><link>https://gardener.cloud/docs/tutorials/oidc-login/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/tutorials/oidc-login/</guid><description>
&lt;p>Use an identity provider to authenticate users to access shoot clusters.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>Please read the following background material on &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens">Authenticating&lt;/a>.&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Kubernetes on its own doesn’t provide any user management. In other words, users aren’t managed through Kubernetes resources. Whenever you refer to a human user it’s sufficient to use a unique ID, for example, an email address. Nevertheless, Gardener project owners can use an identity provider to authenticate user access for shoot clusters in the following way:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="#configure-an-identity-provider">Configure an Identity Provider&lt;/a> using &lt;strong>OpenID Connect&lt;/strong> (OIDC).&lt;/li>
&lt;li>&lt;a href="#configure-a-local-kubectl-oidc-login">Configure a local kubectl oidc-login&lt;/a> to enable &lt;code>oidc-login&lt;/code>.&lt;/li>
&lt;li>&lt;a href="#configure-the-shoot-cluster">Configure the shoot cluster&lt;/a> to share details of the OIDC-compliant identity provider with the Kubernetes API Server.&lt;/li>
&lt;li>&lt;a href="#authorize-an-authenticated-user">Authorize an authenticated user&lt;/a> using role-based access control (RBAC).&lt;/li>
&lt;li>&lt;a href="#verify-the-result">Verify the result&lt;/a>&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>Gardener allows administrators to modify aspects of the control plane setup. It gives administrators full control of how the control plane is parameterized. While this offers much flexibility, administrators need to ensure that they don’t configure a control plane that goes beyond the service level agreements of the responsible operators team.&lt;/p>
&lt;/blockquote>
&lt;h2 id="configure-an-identity-provider">Configure an Identity Provider&lt;/h2>
&lt;p>Create a tenant in an OIDC compatible Identity Provider. For simplicity, we use &lt;em>Auth0&lt;/em>, which has a free plan.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>In your tenant, create a client application to use authentication with &lt;code>kubectl&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Create-client-application_c47ce0.png" alt="Create client application">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Provide a &lt;em>Name&lt;/em>, choose &lt;em>Native&lt;/em> as application type, and choose &lt;em>CREATE&lt;/em>.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Choose-application-type_392e78.png" alt="Choose application type">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>On tab &lt;em>Settings&lt;/em>, copy the following parameters to a local text file:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;em>Domain&lt;/em>&lt;/p>
&lt;blockquote>
&lt;p>Corresponds to the &lt;strong>issuer&lt;/strong> in OIDC. It must be an &lt;code>https&lt;/code>-secured endpoint (Auth0 requires a trailing &lt;code>/&lt;/code> at the end). More information: &lt;a href="https://openid.net/specs/openid-connect-core-1_0.html#Terminology">Issuer Identifier&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Client ID&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Client Secret&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Basic-information_2f952f.png" alt="Basic information">&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Configure the client to have a callback url of http://localhost:8000. This callback connects to your local &lt;code>kubectl oidc-login&lt;/code> plugin:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Configure-callback_1a247f.png" alt="Configure callback">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Save your changes.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Verify that &lt;code>https://&amp;lt;Auth0 Domain&amp;gt;/.well-known/openid-configuration&lt;/code> is reachable.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Choose &lt;em>Users &amp;amp; Roles&lt;/em> &amp;gt; &lt;em>Users&lt;/em> &amp;gt; &lt;em>CREATE USERS&lt;/em> to create a user with a user and password:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Create-user_97a940.png" alt="Create user">&lt;/p>
&lt;blockquote>
&lt;p>Users must have a &lt;em>verified&lt;/em> email address.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ol>
&lt;h2 id="configure-a-local-kubectl-oidc-login">Configure a local &lt;code>kubectl&lt;/code> &lt;code>oidc-login&lt;/code>&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Install the &lt;code>kubectl&lt;/code> plugin &lt;a href="https://github.com/int128/kubelogin">oidc-login&lt;/a>. We highly recommend the &lt;a href="https://github.com/kubernetes-sigs/krew">krew&lt;/a> install tool, which also makes other plugins easily available.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl krew install oidc-login
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The response looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>Updated the local copy of plugin index.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Installing plugin: oidc-login
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>CAVEATS:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>\
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| You need to setup the OIDC provider, Kubernetes API server, role binding and kubeconfig.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| See https://github.com/int128/kubelogin for more.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Installed plugin: oidc-login
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Prepare a &lt;code>kubeconfig&lt;/code> for later use:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>cp ~/.kube/config ~/.kube/config-oidc
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Modify the configuration of &lt;code>~/.kube/config-oidc&lt;/code> as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>contexts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- context:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster: shoot--project--mycluster
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user: my-oidc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: shoot--project--mycluster
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>users:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: my-oidc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> exec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: client.authentication.k8s.io/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> command: kubectl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> args:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - oidc-login
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - get-token
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --oidc-issuer-url=https://&amp;lt;Issuer&amp;gt;/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --oidc-client-id=&amp;lt;Client ID&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --oidc-client-secret=&amp;lt;Client Secret&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --oidc-extra-scope=email,offline_access,profile
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;p>To test our OIDC-based authentication, context &lt;code>shoot--project--mycluster&lt;/code> of &lt;code>~/.kube/config-oidc&lt;/code> is used in a later step. For now, continue to use the configuration &lt;code>~/.kube/config&lt;/code> with administration rights for your cluster.&lt;/p>
&lt;h2 id="configure-the-shoot-cluster">Configure the shoot cluster&lt;/h2>
&lt;p>Modify the shoot cluster YAML as follows, using the client ID and the domain (as issuer) from the settings of the client application you created in Auth0:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: garden.sapcloud.io/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: mycluster
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden-project
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeAPIServer:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidcConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clientID: &amp;lt;Client ID&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerURL: &lt;span style="color:#a31515">&amp;#34;https://&amp;lt;Issuer&amp;gt;/&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernameClaim: email
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This change of the &lt;code>Shoot&lt;/code> manifest triggers a reconciliation. Once the reconciliation is finished, your OIDC configuration is applied. It &lt;strong>doesn&amp;rsquo;t&lt;/strong> invalidate other certificate-based authentication methods. Wait for Gardener to reconcile the change. It can take up to 5 minutes.&lt;/p>
&lt;h2 id="authorize-an-authenticated-user">Authorize an authenticated user&lt;/h2>
&lt;p>In Auth0, you created a user with a verified email address, &lt;code>test@test.com&lt;/code> in our example. For simplicity, we authorize a single user identified by this email address with cluster role &lt;code>view&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: rbac.authorization.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ClusterRoleBinding
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: viewer-test
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>roleRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: ClusterRole
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: view
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>subjects:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: User
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: test@test.com
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>As administrator, apply the cluster role binding in your shoot cluster.&lt;/p>
&lt;h2 id="verify-the-result">Verify the result&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>To step into the shoes of your user, use the prepared &lt;code>kubeconfig&lt;/code> file &lt;code>~/.kube/config-oidc&lt;/code>, and switch to the context that uses &lt;code>oidc-login&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>cd ~/.kube
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export KUBECONFIG=$(pwd)/config-oidc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl config use-context `shoot--project--mycluster`
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>&lt;code>kubectl&lt;/code> delegates the authentication to plugin &lt;code>oidc-login&lt;/code> the first time the user uses &lt;code>kubectl&lt;/code> to contact the API server, for example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl get all
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The plugin opens a browser for an interactive authentication session with Auth0, and in parallel serves a local webserver for the configured callback.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Enter your login credentials.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Login-through-identity-provider_54293b.png" alt="Login through identity provider">&lt;/p>
&lt;p>You should get a successful response from the API server:&lt;/p>
&lt;pre tabindex="0">&lt;code>Opening in existing browser session.
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
service/kubernetes ClusterIP 100.64.0.1 &amp;lt;none&amp;gt; 443/TCP 86m
&lt;/code>&lt;/pre>&lt;blockquote>
&lt;p>After a successful login, &lt;code>kubectl&lt;/code> uses a token for authentication so that you don’t have to provide user and password for every new &lt;code>kubectl&lt;/code> command. How long the token is valid can be configured. If you want to log in again earlier, reset plugin &lt;code>oidc-login&lt;/code>:&lt;/p>
&lt;ol>
&lt;li>Delete directory &lt;code>~/.kube/cache/oidc-login&lt;/code>.&lt;/li>
&lt;li>Delete the browser cache.&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>To see if your user uses cluster role &lt;code>view&lt;/code>, do some checks with &lt;code>kubectl auth can-i&lt;/code>.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The response for the following commands should be &lt;code>no&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl auth can-i create clusterrolebindings
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl auth can-i get secrets
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl auth can-i describe secrets
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>The response for the following commands should be &lt;code>yes&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl auth can-i list pods
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl auth can-i get pods
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>If the last step is successful, you’ve configured your cluster to authenticate against an identity provider using OIDC.&lt;/p>
&lt;h2 id="related-links">Related Links&lt;/h2>
&lt;p>&lt;a href="https://auth0.com/pricing/">Auth0 Pricing&lt;/a>&lt;/p></description></item><item><title>Docs: Dynamic Volume Provisioning</title><link>https://gardener.cloud/docs/tutorials/dynamic-pvc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/tutorials/dynamic-pvc/</guid><description>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>The example shows how to run a postgres database on Kubernetes and how to dynamically provision and mount the storage
volumes needed by the database&lt;/p>
&lt;h2 id="run-postgres-database">Run postgres database&lt;/h2>
&lt;p>Define the following Kubernetes resources in a yaml file&lt;/p>
&lt;ul>
&lt;li>PersistentVolumeClaim (PVC)&lt;/li>
&lt;li>Deployment&lt;/li>
&lt;/ul>
&lt;h4 id="persistentvolumeclaim">PersistentVolumeClaim&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: PersistentVolumeClaim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: postgresdb-pvc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> accessModes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - ReadWriteOnce
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requests:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> storage: 9Gi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> storageClassName: &lt;span style="color:#a31515">&amp;#39;default&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This defines a PVC using storage class &lt;code>default&lt;/code>. Storage classes abstract from the underlying storage provider as well
as other parameters, like disk-type (e.g.; solid-state vs standard disks).&lt;/p>
&lt;p>The default storage class has annotation &lt;strong>{&amp;ldquo;storageclass.kubernetes.io/is-default-class&amp;rdquo;:&amp;ldquo;true&amp;rdquo;}&lt;/strong>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl describe sc default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Name: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>IsDefaultClass: Yes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Annotations: kubectl.kubernetes.io/last-applied-configuration={&lt;span style="color:#a31515">&amp;#34;apiVersion&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;storage.k8s.io/v1beta1&amp;#34;&lt;/span>,&lt;span style="color:#a31515">&amp;#34;kind&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;StorageClass&amp;#34;&lt;/span>,&lt;span style="color:#a31515">&amp;#34;metadata&amp;#34;&lt;/span>:{&lt;span style="color:#a31515">&amp;#34;annotations&amp;#34;&lt;/span>:{&lt;span style="color:#a31515">&amp;#34;storageclass.kubernetes.io/is-default-class&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>},&lt;span style="color:#a31515">&amp;#34;labels&amp;#34;&lt;/span>:{&lt;span style="color:#a31515">&amp;#34;addonmanager.kubernetes.io/mode&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;Exists&amp;#34;&lt;/span>},&lt;span style="color:#a31515">&amp;#34;name&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;default&amp;#34;&lt;/span>,&lt;span style="color:#a31515">&amp;#34;namespace&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;&amp;#34;&lt;/span>},&lt;span style="color:#a31515">&amp;#34;parameters&amp;#34;&lt;/span>:{&lt;span style="color:#a31515">&amp;#34;type&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;gp2&amp;#34;&lt;/span>},&lt;span style="color:#a31515">&amp;#34;provisioner&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;kubernetes.io/aws-ebs&amp;#34;&lt;/span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>,storageclass.kubernetes.io/is-default-class=true
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Provisioner: kubernetes.io/aws-ebs
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Parameters: type=gp2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>AllowVolumeExpansion: &amp;lt;unset&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>MountOptions: &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ReclaimPolicy: Delete
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>VolumeBindingMode: Immediate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Events: &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>A Persistent Volume is automatically created when it is dynamically provisioned. In following example, the PVC is defined
as &amp;ldquo;postgresdb-pvc&amp;rdquo;, and a corresponding PV &amp;ldquo;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb&amp;rdquo; is created and associated with pvc automatically.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl create -f .&lt;span style="color:#a31515">\p&lt;/span>ostgres_deployment.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>persistentvolumeclaim &lt;span style="color:#a31515">&amp;#34;postgresdb-pvc&amp;#34;&lt;/span> created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl get pv
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 3s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl get pvc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>postgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 8s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Notice that the &lt;strong>RECLAIM POLICY&lt;/strong> is &lt;strong>Delete&lt;/strong> (default value), which is one of the two reclaim policies, the other
one is &lt;strong>Retain&lt;/strong>. (A third policy &lt;strong>Recycle&lt;/strong> has been deprecated). In case of &lt;strong>Delete&lt;/strong>, the PV is deleted automatically
when the PVC is removed, and the data on the PVC will also be lost.&lt;/p>
&lt;p>On the other hand, PV with &lt;strong>Retain&lt;/strong> policy will not be deleted when the PVC is removed, and moved to &lt;strong>Release&lt;/strong> status, so
that data can be recovered by Administrators later.&lt;/p>
&lt;p>You can use the &lt;code>kubectl patch&lt;/code> command to change the reclaim policy as described here &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/change-pv-reclaim-policy/">here&lt;/a>
or use &lt;code>kubectl edit pv &amp;lt;pv-name&amp;gt;&lt;/code> to edit online as below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl get pv
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 44m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># change the relcaim policy from &amp;#34;Delete&amp;#34; to &amp;#34;Retain&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl edit pv pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>persistentvolume &lt;span style="color:#a31515">&amp;#34;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb&amp;#34;&lt;/span> edited
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># check the reclaim policy afterwards&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl get pv
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 45m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="deployment">Deployment&lt;/h4>
&lt;p>Once a PVC is created, you can use it in your container via &lt;code>volumes.persistentVolumeClaim.claimName&lt;/code>. In below
example, pvc &lt;strong>postgresdb-pvc&lt;/strong> is mounted as readable and writable, and in &lt;code>volumeMounts&lt;/code> two paths in the container are mounted to subfolders in the volume.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Deployment
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: postgres
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app: postgres
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> deployment.kubernetes.io/revision: &lt;span style="color:#a31515">&amp;#34;1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> replicas: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> strategy:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: RollingUpdate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rollingUpdate:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maxUnavailable: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maxSurge: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> selector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app: postgres
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: postgres
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app: postgres
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: postgres
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> image: &lt;span style="color:#a31515">&amp;#34;cpettech.docker.repositories.sap.ondemand.com/jtrack_postgres:howto&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> env:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: POSTGRES_USER
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: postgres
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: POSTGRES_PASSWORD
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: p5FVqfuJFrM42cVX9muQXxrC3r8S9yn0zqWnFR6xCoPqxqVQ
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: POSTGRES_INITDB_XLOGDIR
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: &lt;span style="color:#a31515">&amp;#34;/var/log/postgresql/logs&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ports:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - containerPort: 5432
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumeMounts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - mountPath: /var/lib/postgresql/data
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: postgre-db
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> subPath: data &lt;span style="color:#008000"># https://github.com/kubernetes/website/pull/2292. Solve the issue of crashing initdb due to non-empty directory (i.e. lost+found)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - mountPath: /var/log/postgresql/logs
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: postgre-db
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> subPath: logs
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: postgre-db
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> persistentVolumeClaim:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> claimName: postgresdb-pvc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> readOnly: &lt;span style="color:#00f">false&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imagePullSecrets:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: cpettechregistry
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To check the mount points in the container:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl get po
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>postgres-7f485fd768-c5jf9 1/1 Running 0 32m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl exec -it postgres-7f485fd768-c5jf9 bash
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@postgres-7f485fd768-c5jf9:/# ls /var/lib/postgresql/data/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>base pg_clog pg_dynshmem pg_ident.conf pg_multixact pg_replslot pg_snapshots pg_stat_tmp pg_tblspc PG_VERSION postgresql.auto.conf postmaster.opts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>global pg_commit_ts pg_hba.conf pg_logical pg_notify pg_serial pg_stat pg_subtrans pg_twophase pg_xlog postgresql.conf postmaster.pid
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>root@postgres-7f485fd768-c5jf9:/# ls /var/log/postgresql/logs/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>000000010000000000000001 archive_status
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="deleting-a-persistentvolumeclaim">Deleting a PersistentVolumeClaim&lt;/h4>
&lt;p>In case of &amp;ldquo;Delete&amp;rdquo; policy, deleting a PVC will also delete its associated PV. If &amp;ldquo;Retain&amp;rdquo; is the reclaim policy, the
PV will change status from &lt;strong>Bound&lt;/strong> to &lt;strong>Released&lt;/strong> when PVC is deleted.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Check pvc and pv before deletion&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl get pvc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>postgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 50m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl get pv
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 50m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># delete pvc&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl delete pvc postgresdb-pvc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>persistentvolumeclaim &lt;span style="color:#a31515">&amp;#34;postgresdb-pvc&amp;#34;&lt;/span> deleted
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># pv changed to status &amp;#34;Released&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl get pv
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Released default/postgresdb-pvc default 51m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: GPU Enabled Cluster</title><link>https://gardener.cloud/docs/tutorials/gpu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/tutorials/gpu/</guid><description>
&lt;h2 id="intro">Intro&lt;/h2>
&lt;p>Be aware, that the following sections might be opinionated. Kubernetes, and the GPU support in particular,
are rapidly evolving, which means that this guide is likely to be outdated sometime soon. For this reason,
&lt;strong>contributions are highly appreciated&lt;/strong> to update this guide.&lt;/p>
&lt;h2 id="create-a-cluster">Create a Cluster&lt;/h2>
&lt;p>First thing first, let’s create a k8s cluster with GPU accelerated nodes. In this example we will use AWS
&lt;strong>p2.xlarge&lt;/strong> EC2 instance because it&amp;rsquo;s the cheapest available option at the moment. Use such cheap instances
for learning to limit your resource costs. &lt;strong>This costs around 1€/hour per GPU&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/howto-gpu_88d839.png" alt="gpu-selection">&lt;/p>
&lt;h2 id="install-nvidia-driver-as-daemonset">Install NVidia Driver as Daemonset&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: DaemonSet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: nvidia-driver-installer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: kube-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> k8s-app: nvidia-driver-installer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> selector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: nvidia-driver-installer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> k8s-app: nvidia-driver-installer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: nvidia-driver-installer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> k8s-app: nvidia-driver-installer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPID: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> initContainers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - image: squat/modulus:4a1799e7aa0143bcbb70d354bab3e419b1f54972
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: modulus
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> args:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - compile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - nvidia
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#a31515">&amp;#34;410.104&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> securityContext:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> privileged: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> env:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: MODULUS_CHROOT
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: MODULUS_INSTALL
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: MODULUS_INSTALL_DIR
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: /opt/drivers
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: MODULUS_CACHE_DIR
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: /opt/modulus/cache
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: MODULUS_LD_ROOT
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: /root
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: IGNORE_MISSING_MODULE_SYMVERS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: &lt;span style="color:#a31515">&amp;#34;1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumeMounts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: etc-coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /etc/coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> readOnly: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: usr-share-coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /usr/share/coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> readOnly: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: ld-root
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /root
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: module-cache
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /opt/modulus/cache
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: module-install-dir-base
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /opt/drivers
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - image: &lt;span style="color:#a31515">&amp;#34;gcr.io/google-containers/pause:3.1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: pause
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tolerations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: &lt;span style="color:#a31515">&amp;#34;nvidia.com/gpu&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> effect: &lt;span style="color:#a31515">&amp;#34;NoSchedule&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> operator: &lt;span style="color:#a31515">&amp;#34;Exists&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: etc-coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPath:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: /etc/coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: usr-share-coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPath:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: /usr/share/coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: ld-root
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPath:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: /
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: module-cache
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPath:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: /opt/modulus/cache
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPath:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: /dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: module-install-dir-base
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPath:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: /opt/drivers
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="install-device-plugin">Install Device Plugin&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: DaemonSet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: nvidia-gpu-device-plugin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: kube-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> k8s-app: nvidia-gpu-device-plugin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#addonmanager.kubernetes.io/mode: Reconcile&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> selector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> k8s-app: nvidia-gpu-device-plugin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> k8s-app: nvidia-gpu-device-plugin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scheduler.alpha.kubernetes.io/critical-pod: &lt;span style="color:#a31515">&amp;#39;&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> priorityClassName: system-node-critical
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: device-plugin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPath:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: /var/lib/kubelet/device-plugins
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPath:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: /dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - image: &lt;span style="color:#a31515">&amp;#34;k8s.gcr.io/nvidia-gpu-device-plugin@sha256:08509a36233c5096bb273a492251a9a5ca28558ab36d74007ca2a9d3f0b61e1d&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> command: [&lt;span style="color:#a31515">&amp;#34;/usr/bin/nvidia-gpu-device-plugin&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;-logtostderr&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;-host-path=/opt/drivers/nvidia&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: nvidia-gpu-device-plugin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requests:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cpu: 50m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memory: 10Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> limits:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cpu: 50m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memory: 10Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> securityContext:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> privileged: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumeMounts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: device-plugin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /device-plugin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> updateStrategy:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: RollingUpdate
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="test">Test&lt;/h2>
&lt;p>To run an example training on a GPU node, start first a base image with Tensorflow with GPU support &amp;amp; Keras&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Deployment
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: deeplearning-workbench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> replicas: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> selector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app: deeplearning-workbench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app: deeplearning-workbench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: deeplearning-workbench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> image: afritzler/deeplearning-workbench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> limits:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nvidia.com/gpu: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tolerations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: &lt;span style="color:#a31515">&amp;#34;nvidia.com/gpu&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> effect: &lt;span style="color:#a31515">&amp;#34;NoSchedule&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> operator: &lt;span style="color:#a31515">&amp;#34;Exists&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note: the &lt;code>tolerations&lt;/code> section above is not required if you deploy the &lt;code>ExtendedResourceToleration&lt;/code>
admission controller to your cluster. You can do this in the &lt;code>kubernetes&lt;/code> section of your Gardener
cluster &lt;code>shoot.yaml&lt;/code> as follows:&lt;/p>
&lt;pre tabindex="0">&lt;code> kubernetes:
kubeAPIServer:
admissionPlugins:
- name: ExtendedResourceToleration
&lt;/code>&lt;/pre>&lt;p>Now exec into the container and start an example Keras training&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl exec -it deeplearning-workbench-8676458f5d-p4d2v -- /bin/bash
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cd /keras/example
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>python imdb_cnn.py
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="acknowledgments--references">Acknowledgments &amp;amp; References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/afritzler/kubernetes-gpu">Andreas Fritzler&lt;/a> from the Gardener Core team for the R&amp;amp;D and providing this setup.&lt;/li>
&lt;li>&lt;a href="https://github.com/squat/modulus">Build and install NVIDIA driver on CoreOS&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/device-plugins/nvidia-gpu/daemonset.yaml">Nvidia Device Plugin&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Install Knative in Gardener Clusters</title><link>https://gardener.cloud/docs/tutorials/knative-install/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/tutorials/knative-install/</guid><description>
&lt;p>This guide walks you through the installation of the latest version of Knative
using pre-built images on a &lt;a href="https://gardener.cloud">Gardener&lt;/a> created cluster
environment. To set up your own Gardener, see the
&lt;a href="https://github.com/gardener/gardener/blob/master/docs/README.md">documentation&lt;/a>
or have a look at the
&lt;a href="https://github.com/gardener/landscape-setup-template">landscape-setup-template&lt;/a>
project. To learn more about this open source project, read the
&lt;a href="https://kubernetes.io/blog/2018/05/17/gardener/">blog on kubernetes.io&lt;/a>.&lt;/p>
&lt;h2 id="before-you-begin">Before you begin&lt;/h2>
&lt;p>Knative requires a Kubernetes cluster v1.15 or newer.&lt;/p>
&lt;h3 id="install-and-configure-kubectl">Install and configure kubectl&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>If you already have &lt;code>kubectl&lt;/code> CLI, run &lt;code>kubectl version --short&lt;/code> to check
the version. You need v1.10 or newer. If your &lt;code>kubectl&lt;/code> is older, follow the
next step to install a newer version.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl">Install the kubectl CLI&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="access-gardener">Access Gardener&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>Create a project in the Gardener dashboard. This will essentially create a
Kubernetes namespace with the name &lt;code>garden-&amp;lt;my-project&amp;gt;&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/#configure-kubectl">Configure access to your Gardener project&lt;/a>
using a kubeconfig. If you are not the Gardener Administrator already, you
can create a technical user in the Gardener dashboard: go to the &amp;ldquo;Members&amp;rdquo;
section and add a service account. You can then download the kubeconfig for
your project. You can skip this step if you create your cluster using the
user interface; it is only needed for programmatic access, make sure you set
&lt;code>export KUBECONFIG=garden-my-project.yaml&lt;/code> in your shell.
&lt;img src="https://gardener.cloud/__resources/gardener_service_account_0a4a8a.png" alt="Download kubeconfig for Gardener" title="downloading the kubeconfig using a service account">&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="creating-a-kubernetes-cluster">Creating a Kubernetes cluster&lt;/h3>
&lt;p>You can create your cluster using &lt;code>kubectl&lt;/code> cli by providing a cluster
specification yaml file. You can find an example for GCP
&lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml">here&lt;/a>.
Make sure the namespace matches that of your project. Then just apply the
prepared so-called &amp;ldquo;shoot&amp;rdquo; cluster crd with kubectl:&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl apply --filename my-cluster.yaml
&lt;/code>&lt;/pre>&lt;p>The easier alternative is to create the cluster following the cluster creation
wizard in the Gardener dashboard:
&lt;img src="https://gardener.cloud/__resources/gardener_shoot_creation_49a4ca.png" alt="shoot creation" title="shoot creation via the dashboard">&lt;/p>
&lt;h3 id="configure-kubectl-for-your-cluster">Configure kubectl for your cluster&lt;/h3>
&lt;p>You can now download the kubeconfig for your freshly created cluster in the
Gardener dashboard or via cli as follows:&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl --namespace shoot--my-project--my-cluster get secret kubecfg --output jsonpath={.data.kubeconfig} | base64 --decode &amp;gt; my-cluster.yaml
&lt;/code>&lt;/pre>&lt;p>This kubeconfig file has full administrators access to you cluster. For the rest
of this guide be sure you have &lt;code>export KUBECONFIG=my-cluster.yaml&lt;/code> set.&lt;/p>
&lt;h2 id="installing-istio">Installing Istio&lt;/h2>
&lt;p>Knative depends on Istio. If your cloud platform offers a managed Istio
installation, we recommend installing Istio that way, unless you need the
ability to customize your installation.&lt;/p>
&lt;p>Otherwise, see the &lt;a href="https://knative.dev/docs/install/installing-istio/">Installing Istio for Knative guide&lt;/a>
to install Istio.&lt;/p>
&lt;p>You must install Istio on your Kubernetes cluster before continuing with these
instructions to install Knative.&lt;/p>
&lt;h2 id="installing-cluster-local-gateway-for-serving-cluster-internal-traffic">Installing &lt;code>cluster-local-gateway&lt;/code> for serving cluster-internal traffic&lt;/h2>
&lt;p>If you installed Istio, you can install a &lt;code>cluster-local-gateway&lt;/code> within your Knative cluster so that you can serve cluster-internal traffic. If you want to configure your revisions to use routes that are visible only within your cluster, &lt;a href="https://knative.dev/docs/admin/install/knative-offerings/">install and use the &lt;code>cluster-local-gateway&lt;/code>&lt;/a>.&lt;/p>
&lt;h2 id="installing-knative">Installing Knative&lt;/h2>
&lt;p>The following commands install all available Knative components as well as the
standard set of observability plugins. Knative&amp;rsquo;s installation guide - &lt;a href="https://knative.dev/docs/admin/install/">Installing Knative&lt;/a>.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>If you are upgrading from Knative 0.3.x: Update your domain and static IP
address to be associated with the LoadBalancer &lt;code>istio-ingressgateway&lt;/code> instead
of &lt;code>knative-ingressgateway&lt;/code>. Then run the following to clean up leftover
resources:&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl delete svc knative-ingressgateway -n istio-system
kubectl delete deploy knative-ingressgateway -n istio-system
&lt;/code>&lt;/pre>&lt;p>If you have the Knative Eventing Sources component installed, you will also
need to delete the following resource before upgrading:&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl delete statefulset/controller-manager -n knative-sources
&lt;/code>&lt;/pre>&lt;p>While the deletion of this resource during the upgrade process will not
prevent modifications to Eventing Source resources, those changes will not be
completed until the upgrade process finishes.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>To install Knative, first install the CRDs by running the &lt;code>kubectl apply&lt;/code>
command once with the &lt;code>-l knative.dev/crd-install=true&lt;/code> flag. This prevents
race conditions during the install, which cause intermittent errors:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply --selector knative.dev/crd-install=true &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span>--filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span>--filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span>--filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>To complete the install of Knative and its dependencies, run the
&lt;code>kubectl apply&lt;/code> command again, this time without the &lt;code>--selector&lt;/code> flag, to
complete the install of Knative and its dependencies:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply --filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span>--filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span>--filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Monitor the Knative components until all of the components show a &lt;code>STATUS&lt;/code> of
&lt;code>Running&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl get pods --namespace knative-serving
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl get pods --namespace knative-eventing
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl get pods --namespace knative-monitoring
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h2 id="set-your-custom-domain">Set your custom domain&lt;/h2>
&lt;ol>
&lt;li>Fetch the external IP or CNAME of the knative-ingressgateway&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>kubectl --namespace istio-system get service knative-ingressgateway
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
knative-ingressgateway LoadBalancer 100.70.219.81 35.233.41.212 80:32380/TCP,443:32390/TCP,32400:32400/TCP 4d
&lt;/code>&lt;/pre>&lt;ol start="2">
&lt;li>Create a wildcard DNS entry in your custom domain to point to above IP or
CNAME&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>*.knative.&amp;lt;my domain&amp;gt; == A 35.233.41.212
# or CNAME if you are on AWS
*.knative.&amp;lt;my domain&amp;gt; == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com
&lt;/code>&lt;/pre>&lt;ol start="3">
&lt;li>Adapt your knative config-domain (set your domain in the data field)&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>kubectl --namespace knative-serving get configmaps config-domain --output yaml
apiVersion: v1
data:
knative.&amp;lt;my domain&amp;gt;: &amp;#34;&amp;#34;
kind: ConfigMap
name: config-domain
namespace: knative-serving
&lt;/code>&lt;/pre>&lt;h2 id="whats-next">What&amp;rsquo;s next&lt;/h2>
&lt;p>Now that your cluster has Knative installed, you can see what Knative has to
offer.&lt;/p>
&lt;p>To deploy your first app with the
&lt;a href="https://knative.dev/docs/serving/getting-started-knative-app/">Getting Started with Knative App Deployment&lt;/a>
guide.&lt;/p>
&lt;p>Get started with Knative Eventing by walking through one of the
&lt;a href="https://knative.dev/docs/eventing/samples/">Eventing Samples&lt;/a>.&lt;/p>
&lt;p>&lt;a href="https://knative.dev/docs/serving/installing-cert-manager/">Install Cert-Manager&lt;/a> if you want to use the
&lt;a href="https://knative.dev/docs/serving/using-auto-tls/">automatic TLS cert provisioning feature&lt;/a>.&lt;/p>
&lt;h2 id="cleaning-up">Cleaning up&lt;/h2>
&lt;p>Use the Gardener dashboard to delete your cluster, or execute the following with
kubectl pointing to your &lt;code>garden-my-project.yaml&lt;/code> kubeconfig:&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project annotate shoot my-cluster confirmation.gardener.cloud/deletion=true
kubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project delete shoot my-cluster
&lt;/code>&lt;/pre></description></item></channel></rss>