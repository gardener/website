<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.95.0"><link rel=canonical type=text/html href=https://gardener.cloud/docs/other-components/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/other-components/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Other Components | Gardener</title><meta name=description content="Other components included in the Gardener project"><meta property="og:title" content="Other Components"><meta property="og:description" content="Other components included in the Gardener project"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/other-components/"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="Other Components"><meta itemprop=description content="Other components included in the Gardener project"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="Other Components"><meta name=twitter:description content="Other components included in the Gardener project"><link rel=preload href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css as=style><link href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7N3XF5XLGV"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7N3XF5XLGV",{anonymize_ip:!1})}</script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.2b2e15b755b15e589d1293b11d87c8c8.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/other-components/>Return to the regular view of this page</a>.</p></div><h1 class=title>Other Components</h1><div class=lead>Other components included in the Gardener project</div><div class=content></div></div><div class=td-content><h1 id=pg-4c5520f0940e52e6166ce8932f4dd864>1 - Machine Controller Manager</h1><div class=lead>Declarative way of managing machines for Kubernetes cluster</div><h1 id=machine-controller-manager>machine-controller-manager</h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/machine-controller-manager-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/machine-controller-manager-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/machine-controller-manager><img src=https://goreportcard.com/badge/github.com/gardener/machine-controller-manager alt="Go Report Card"></a></p><p><strong>Note</strong>
One can add support for a new cloud provider by following <a href=/docs/other-components/machine-controller-manager/development/cp_support_new/>Adding support for new provider</a>.</p><h1 id=overview>Overview</h1><p>Machine Controller Manager aka MCM is a group of cooperative controllers that manage the lifecycle of the worker machines. It is inspired by the design of Kube Controller Manager in which various sub controllers manage their respective Kubernetes Clients. MCM gives you the following benefits:</p><ul><li>seamlessly manage machines/nodes with a declarative API (of course, across different cloud providers)</li><li>integrate generically with the cluster autoscaler</li><li>plugin with tools such as the node-problem-detector</li><li>transport the immutability design principle to machine/nodes</li><li>implement e.g. rolling upgrades of machines/nodes</li></ul><p>MCM supports following providers. These provider code is maintained externally (out-of-tree), and the links for the same are linked below:</p><ul><li><a href=https://github.com/gardener/machine-controller-manager-provider-alicloud>Alicloud</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-aws>AWS</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-azure>Azure</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-equinix-metal>Equinix Metal</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-gcp>GCP</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-kubevirt>KubeVirt</a></li><li><a href=https://github.com/metal-stack/machine-controller-manager-provider-metal>Metal Stack</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-openstack>Openstack</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-vsphere>V Sphere</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-yandex>Yandex</a></li></ul><p>It can easily be extended to support other cloud providers as well.</p><p>Example of managing machine:</p><pre tabindex=0><code>kubectl create/get/delete machine vm1
</code></pre><h2 id=key-terminologies>Key terminologies</h2><p>Nodes/Machines/VMs are different terminologies used to represent similar things. We use these terms in the following way</p><ol><li>VM: A virtual machine running on any cloud provider. It could also refer to a physical machine (PM) in case of a bare metal setup.</li><li>Node: Native kubernetes node objects. The objects you get to see when you do a <em>&ldquo;kubectl get nodes&rdquo;</em>. Although nodes can be either physical/virtual machines, for the purposes of our discussions it refers to a VM.</li><li>Machine: A VM that is provisioned/managed by the Machine Controller Manager.</li></ol><h1 id=design-of-machine-controller-manager>Design of Machine Controller Manager</h1><p>The design of the Machine Controller Manager is influenced by the Kube Controller Manager, where-in multiple sub-controllers are used to manage the Kubernetes clients.</p><h2 id=design-principles>Design Principles</h2><p>It&rsquo;s designed to run in the master plane of a Kubernetes cluster. It follows the best principles and practices of writing controllers, including, but not limited to:</p><ul><li>Reusing code from kube-controller-manager</li><li>leader election to allow HA deployments of the controller</li><li><code>workqueues</code> and multiple thread-workers</li><li><code>SharedInformers</code> that limit to minimum network calls, de-serialization and provide helpful create/update/delete events for resources</li><li>rate-limiting to allow back-off in case of network outages and general instability of other cluster components</li><li>sending events to respected resources for easy debugging and overview</li><li>Prometheus metrics, health and (optional) profiling endpoints</li></ul><h2 id=objects-of-machine-controller-manager>Objects of Machine Controller Manager</h2><p>Machine Controller Manager reconciles a set of Custom Resources namely <code>MachineDeployment</code>, <code>MachineSet</code> and <code>Machines</code> which are managed & monitored by their controllers MachineDeployment Controller, MachineSet Controller, Machine Controller respectively along with another cooperative controller called the Safety Controller.</p><p>Machine Controller Manager makes use of 4 CRD objects and 1 Kubernetes secret object to manage machines. They are as follows:</p><table><thead><tr><th>Custom ResourceObject</th><th>Description</th></tr></thead><tbody><tr><td><code>MachineClass</code></td><td>A <code>MachineClass</code> represents a template that contains cloud provider specific details used to create machines.</td></tr><tr><td><code>Machine</code></td><td>A <code>Machine</code> represents a VM which is backed by the cloud provider.</td></tr><tr><td><code>MachineSet</code></td><td>A <code>MachineSet</code> ensures that the specified number of <code>Machine</code> replicas are running at a given point of time.</td></tr><tr><td><code>MachineDeployment</code></td><td>A <code>MachineDeployment</code> provides a declarative update for <code>MachineSet</code> and <code>Machines</code>.</td></tr><tr><td><code>Secret</code></td><td>A <code>Secret</code> here is a Kubernetes secret that stores cloudconfig (initialization scripts used to create VMs) and cloud specific credentials.</td></tr></tbody></table><p>See <a href=/docs/other-components/machine-controller-manager/documents/apis/>here</a> for CRD API Documentation</p><h2 id=components-of-machine-controller-manager>Components of Machine Controller Manager</h2><table><thead><tr><th>Controller</th><th>Description</th></tr></thead><tbody><tr><td>MachineDeployment controller</td><td>Machine Deployment controller reconciles the <code>MachineDeployment</code> objects and manages the lifecycle of <code>MachineSet</code> objects. <code>MachineDeployment</code> consumes provider specific <code>MachineClass</code> in its <code>spec.template.spec</code> which is the template of the VM spec that would be spawned on the cloud by MCM.</td></tr><tr><td>MachineSet controller</td><td>MachineSet controller reconciles the <code>MachineSet</code> objects and manages the lifecycle of <code>Machine</code> objects.</td></tr><tr><td>Safety controller</td><td>There is a Safety Controller responsible for handling the unidentified or unknown behaviours from the cloud providers. Safety Controller:<ul><li>freezes the MachineDeployment controller and MachineSet controller if the number of <code>Machine</code> objects goes beyond a certain threshold on top of <code>Spec.replicas</code>. It can be configured by the flag <code>--safety-up</code> or <code>--safety-down</code> and also <code>--machine-safety-overshooting-period`</code>.</li><li>freezes the functionality of the MCM if either of the <code>target-apiserver</code> or the <code>control-apiserver</code> is not reachable.</li><li>unfreezes the MCM automatically once situation is resolved to normal. A <code>freeze</code> label is applied on <code>MachineDeployment</code>/<code>MachineSet</code> to enforce the freeze condition.</li></ul></td></tr></tbody></table><p>Along with the above Custom Controllers and Resources, MCM requires the <code>MachineClass</code> to use K8s <code>Secret</code> that stores cloudconfig (initialization scripts used to create VMs) and cloud specific credentials. All these controllers work in an co-operative manner. They form a parent-child relationship with <code>MachineDeployment</code> Controller being the grandparent, <code>MachineSet</code> Controller being the parent, and <code>Machine</code> Controller being the child.</p><h2 id=development>Development</h2><p>To start using or developing the Machine Controller Manager, see the documentation in the <code>/docs</code> repository.</p><h2 id=faq>FAQ</h2><p>An FAQ is available <a href=/docs/other-components/machine-controller-manager/faq/>here</a>.</p><h2 id=cluster-api-implementation>cluster-api Implementation</h2><ul><li><code>cluster-api</code> branch of machine-controller-manager implements the machine-api aspect of the <a href=https://github.com/kubernetes-sigs/cluster-api>cluster-api project</a>.</li><li>Link: <a href=https://github.com/gardener/machine-controller-manager/tree/cluster-api>https://github.com/gardener/machine-controller-manager/tree/cluster-api</a></li><li>Once cluster-api project gets stable, we may make <code>master</code> branch of MCM as well cluster-api compliant, with well-defined migration notes.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d702430c37918dfe3b62f47159b326de>1.1 - Development</h1></div><div class=td-content><h1 id=pg-8dcc33c171ac72087a0db9386e545c28>1.1.1 - Adding Support for a Cloud Provider</h1><h1 id=adding-support-for-a-new-provider>Adding support for a new provider</h1><p>Steps to be followed while implementing a new (hyperscale) provider are mentioned below. This is the easiest way to add new provider support using a blueprint code.</p><p>However, you may also develop your machine controller from scratch, which would provide you with more flexibility. First, however, make sure that your custom machine controller adheres to the <code>Machine.Status</code> struct defined in the <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/types.go>MachineAPIs</a>. This will make sure the MCM can act with higher-level controllers like MachineSet and MachineDeployment controller. The key is the <code>Machine.Status.CurrentStatus.Phase</code> key that indicates the status of the machine object.</p><p>Our strong recommendation would be to follow the steps below. This provides the most flexibility required to support machine management for adding new providers. And if you feel to extend the functionality, feel free to update our <a href=https://github.com/gardener/machine-controller-manager/tree/master/pkg/util/provider>machine controller libraries</a>.</p><h2 id=setting-up-your-repository>Setting up your repository</h2><ol><li>Create a new empty repository named <code>machine-controller-manager-provider-{provider-name}</code> on GitHub username/project. Do not initialize this repository with a README.</li><li>Copy the remote repository <code>URL</code> (HTTPS/SSH) to this repository displayed once you create this repository.</li><li>Now, on your local system, create directories as required. {your-github-username} given below could also be {github-project} depending on where you have created the new repository.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir -p $GOPATH/src/github.com/{your-github-username}
</span></span></code></pre></div></li><li>Navigate to this created directory.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd $GOPATH/src/github.com/{your-github-username}
</span></span></code></pre></div></li><li>Clone <a href=https://github.com/gardener/machine-controller-manager-provider-sampleprovider>this repository</a> on your local machine.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone git@github.com:gardener/machine-controller-manager-provider-sampleprovider.git
</span></span></code></pre></div></li><li>Rename the directory from <code>machine-controller-manager-provider-sampleprovider</code> to <code>machine-controller-manager-provider-{provider-name}</code>.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mv machine-controller-manager-provider-sampleprovider machine-controller-manager-provider-{provider-name}
</span></span></code></pre></div></li><li>Navigate into the newly-created directory.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd machine-controller-manager-provider-{provider-name}
</span></span></code></pre></div></li><li>Update the remote <code>origin</code> URL to the newly created repository&rsquo;s URL you had copied above.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git remote set-url origin git@github.com:{your-github-username}/machine-controller-manager-provider-{provider-name}.git
</span></span></code></pre></div></li><li>Rename GitHub project from <code>gardener</code> to <code>{github-org/your-github-username}</code> wherever you have cloned the repository above. Also, edit all occurrences of the word <code>sampleprovider</code> to <code>{provider-name}</code> in the code. Then, use the hack script given below to do the same.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make rename-project PROJECT_NAME={github-org/your-github-username} PROVIDER_NAME={provider-name}
</span></span><span style=display:flex><span>eg:
</span></span><span style=display:flex><span>    make rename-project PROJECT_NAME=gardener PROVIDER_NAME=AmazonWebServices (or)
</span></span><span style=display:flex><span>    make rename-project PROJECT_NAME=githubusername PROVIDER_NAME=AWS
</span></span></code></pre></div></li><li>Now, commit your changes and push them upstream.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git add -A
</span></span><span style=display:flex><span>git commit -m <span style=color:#a31515>&#34;Renamed SampleProvide to {provider-name}&#34;</span>
</span></span><span style=display:flex><span>git push origin master
</span></span></code></pre></div></li></ol><h2 id=code-changes-required>Code changes required</h2><p>The contract between the Machine Controller Manager (MCM) and the Machine Controller (MC) AKA driver has been <a href=/docs/other-components/machine-controller-manager/development/machine_error_codes/>documented here</a> and the <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/util/provider/machinecodes/codes/codes.go>machine error codes can be found here</a>. You may refer to them for any queries.</p><p>⚠️</p><ul><li>Keep in mind that <strong>there should be a unique way to map between machine objects and VMs</strong>. This can be done by mapping machine object names with VM-Name/ tags/ other metadata.</li><li>Optionally, there should also be a unique way to map a VM to its machine class object. This can be done by tagging VM objects with tags/resource groups associated with the machine class.</li></ul><h4 id=steps-to-integrate>Steps to integrate</h4><ol><li>Update the <code>pkg/provider/apis/provider_spec.go</code> specification file to reflect the structure of the <code>ProviderSpec</code> blob. It typically contains the machine template details in the <code>MachineClass</code> object. Follow the sample spec provided already in the file. A sample provider specification can be found <a href=https://github.com/gardener/machine-controller-manager-provider-aws/blob/master/pkg/aws/apis/aws_provider_spec.go>here</a>.</li><li>Fill in the methods described at <code>pkg/provider/core.go</code> to manage VMs on your cloud provider. Comments are provided above each method to help you fill them up with desired <code>REQUEST</code> and <code>RESPONSE</code> parameters.<ul><li>A sample provider implementation for these methods can be found <a href=https://github.com/gardener/machine-controller-manager-provider-aws/blob/master/pkg/aws/core.go>here</a>.</li><li>Fill in the required methods <code>CreateMachine()</code>, and <code>DeleteMachine()</code> methods.</li><li>Optionally fill in methods like <code>GetMachineStatus()</code>, <code>ListMachines()</code>, and <code>GetVolumeIDs()</code>. You may choose to fill these once the working of the required methods seems to be working.</li><li><code>GetVolumeIDs()</code> expects VolumeIDs to be decoded from the volumeSpec based on the cloud provider.</li><li>There is also an OPTIONAL method <code>GenerateMachineClassForMigration()</code> that helps in migration of <code>{ProviderSpecific}MachineClass</code> to <code>MachineClass</code> CR (custom resource). This only makes sense if you have an existing implementation (in-tree) acting on different CRD types. You would like to migrate this. If not, you MUST return an error (machine error UNIMPLEMENTED) to avoid processing this step.</li></ul></li><li>Perform validation of APIs that you have described and make it a part of your methods as required at each request.</li><li>Write unit tests to make it work with your implementation by running <code>make test</code>.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make test
</span></span></code></pre></div></li><li>Re-generate the vendors to update any new vendors imported.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make revendor
</span></span></code></pre></div></li><li>Update the sample YAML files on the <code>kubernetes/</code> directory to provide sample files through which the working of the machine controller can be tested.</li><li>Update <code>README.md</code> to reflect any additional changes</li></ol><h2 id=testing-your-code-changes>Testing your code changes</h2><p>Make sure <code>$TARGET_KUBECONFIG</code> points to the cluster where you wish to manage machines. Likewise, <code>$CONTROL_NAMESPACE</code> represents the namespaces where MCM is looking for machine CR objects, and <code>$CONTROL_KUBECONFIG</code> points to the cluster that holds these machine CRs.</p><ol><li>On the first terminal running at <code>$GOPATH/src/github.com/{github-org/your-github-username}/machine-controller-manager-provider-{provider-name}</code>,<ul><li>Run the machine controller (driver) using the command below.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start
</span></span></code></pre></div></li></ul></li><li>On the second terminal pointing to <code>$GOPATH/src/github.com/gardener</code>,<ul><li>Clone the <a href=https://github.com/gardener/machine-controller-manager>latest MCM code</a><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone git@github.com:gardener/machine-controller-manager.git
</span></span></code></pre></div></li><li>Navigate to the newly-created directory.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd machine-controller-manager
</span></span></code></pre></div></li><li>Deploy the required CRDs from the machine-controller-manager repo,<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/crds
</span></span></code></pre></div></li><li>Run the machine-controller-manager in the <code>master</code> branch<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start
</span></span></code></pre></div></li></ul></li><li>On the third terminal pointing to <code>$GOPATH/src/github.com/{github-org/your-github-username}/machine-controller-manager-provider-{provider-name}</code><ul><li>Fill in the object files given below and deploy them as described below.</li><li>Deploy the <code>machine-class</code><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/machine-class.yaml
</span></span></code></pre></div></li><li>Deploy the <code>kubernetes secret</code> if required.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/secret.yaml
</span></span></code></pre></div></li><li>Deploy the <code>machine</code> object and make sure it joins the cluster successfully.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/machine.yaml
</span></span></code></pre></div></li><li>Once the machine joins, you can test by deploying a machine-deployment.</li><li>Deploy the <code>machine-deployment</code> object and make sure it joins the cluster successfully.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/machine-deployment.yaml
</span></span></code></pre></div></li><li>Make sure to delete both the <code>machine</code> and <code>machine-deployment</code> objects after use.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete -f kubernetes/machine.yaml
</span></span><span style=display:flex><span>kubectl delete -f kubernetes/machine-deployment.yaml
</span></span></code></pre></div></li></ul></li></ol><h2 id=releasing-your-docker-image>Releasing your docker image</h2><ol><li>Make sure you have logged into gcloud/docker using the CLI.</li><li>To release your docker image, run the following.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>    make release IMAGE_REPOSITORY=&lt;link-to-image-repo&gt;
</span></span></code></pre></div><ol start=3><li>A sample kubernetes deploy file can be found at <code>kubernetes/deployment.yaml</code>. Update the same (with your desired MCM and MC images) to deploy your MCM pod.</li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-433c0dbbab7c15b3055612b41544e493>1.1.2 - Integration Tests</h1><h1 id=integration-tests>Integration tests</h1><h2 id=usage>Usage</h2><h2 id=general-setup--configurations>General setup & configurations</h2><p>Integration tests for <code>machine-controller-manager-provider-{provider-name}</code> can be executed manually by following below steps.</p><ol><li>Clone the repository <code>machine-controller-manager-provider-{provider-name}</code> on the local system.</li><li>Navigate to <code>machine-controller-manager-provider-{provider-name}</code> directory and create a <code>dev</code> sub-directory in it.</li><li>Copy the kubeconfig of Control Cluster from into <code>dev/control-kubeconfig.yaml</code>.</li><li>(optional) Copy the kubeconfig of Target Cluster into <code>dev/target-kubeconfig.yaml</code> and update the <code>Makefile</code> variable <code>TARGET_KUBECONFIG</code> to point to <code>dev/target-kubeconfig.yaml</code>.</li><li>If the tags on instances & associated resources on the provider are of <code>String</code> type (for example, GCP tags on its instances are of type <code>String</code> and not key-value pair) then add <code>TAGS_ARE_STRINGS := true</code> in the <code>Makefile</code> and export it.</li><li>Atleast, one of the two controllers&rsquo; container images must be set in the <code>Makefile</code> variables <code>MCM_IMAGE_TAG</code> and <code>MC_IMAGE_TAG</code> for the controllers to run in the Control Cluster . These images will be used along with <code>kubernetes/deployment.yaml</code> to deploy/update controllers in the Control Cluster . If the intention is to run the controllers locally then unset the variables <code>MCM_IMAGE_TAG</code> and <code>MC_IMAGE_TAG</code> and set variable <code>MACHINE_CONTROLLER_MANAGER_DEPLOYMENT_NAME := machine-controller-manager</code> in the <code>Makefile</code>.</li><li>In order to apply the CRDs when the Control Cluster is a Gardener Shoot or if none of the controller images are specified, <code>machine-controller-manager</code> repository will be cloned automatically. Incase, this repository already exists in local system, then create a softlink as below which helps to test changes in <code>machine-controller-manager</code> quickly.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ln -sf &lt;path-for-machine-controller-manager-repo&gt; dev/mcm
</span></span></code></pre></div></li></ol><h2 id=scenario-based-additional-configurations>Scenario based additional configurations</h2><h3 id=gardener-shoot-as-the-control-cluster>Gardener Shoot as the Control Cluster</h3><p>If the Control Cluster is a Gardener Shoot cluster then,</p><ol><li>Deploy a <code>Secret</code> named <code>test-mc-secret</code> (that contains the provider secret and cloud-config) in the <code>default</code> namespace of the Control Cluster.</li><li>Create a <code>dev/machineclassv1.yaml</code> file in the cloned repository. The name of the <code>MachineClass</code> itself should be <code>test-mc-v1</code>. The value of <code>providerSpec.secretRef.name</code> should be <code>test-mc-secret</code>.</li><li>(Optional) Create an additional <code>dev/machineclassv2.yaml</code> file similar to above but with a bigger machine type and update the <code>Makefile</code> variable <code>MACHINECLASS_V2</code> to point to <code>dev/machineclassv2.yaml</code>.</li></ol><h3 id=gardener-seed-as-the-control-cluster>Gardener Seed as the Control Cluster</h3><p>If the Control Cluster is a Gardener SEED cluster then, the suite ideally employs the already existing <code>MachineClass</code> and Secrets. However,</p><ol><li>(Optional) User can employ a custom <code>MachineClass</code> for the tests using below steps:<ol><li>Deploy a <code>Secret</code> named <code>test-mc-secret</code> (that contains the provider secret and cloud-config) in the shoot namespace of the Control Cluster. That is, the value of <code>metadata.namespace</code> should be <code>technicalID</code> of the Shoot and it will be of the pattern <code>shoot--&lt;project>--&lt;shoot-name></code>.</li><li>Create a <code>dev/machineclassv1.yaml</code> file.<ol><li><code>providerSpec.secretRef.name</code> should refer the secret created in the previous step.</li><li><code>metadata.namespace</code> and <code>providerSpec.secretRef.namespace</code> should be <code>technicalID</code> (<code>shoot--&lt;project>--&lt;shoot-name></code>) of the shoot.</li><li>The name of the <code>MachineClass</code> itself should be <code>test-mc-v1</code>.</li></ol></li></ol></li></ol><h2 id=running-the-tests>Running the tests</h2><ol><li>There is a rule <code>test-integration</code> in the <code>Makefile</code>, which can be used to start the integration test:<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make test-integration 
</span></span><span style=display:flex><span>Starting integration tests...
</span></span><span style=display:flex><span>Running Suite: Controller Suite
</span></span><span style=display:flex><span>===============================
</span></span></code></pre></div></li><li>The controllers log files (<code>mcm_process.log</code> and <code>mc_process.log</code>) are stored in <code>.ci/controllers-test/logs</code> repo and can be used later.</li></ol><h2 id=adding-integration-tests-for-new-providers>Adding Integration Tests for new providers</h2><p>For a new provider, Running Integration tests works with no changes. But for the orphan resource test cases to work correctly, the provider-specific API calls and the Resource Tracker Interface (RTI) should be implemented. Please check <a href=https://github.com/gardener/machine-controller-manager-provider-aws/blob/master/test/integration/provider/><code>machine-controller-manager-provider-aws</code></a> for reference.</p><h2 id=extending-integration-tests>Extending integration tests</h2><ul><li>Update <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/test/integration/common/framework.go#L481>ControllerTests</a> to be extend the testcases for all providers. Common testcases for machine|machineDeployment creation|deletion|scaling are packaged into <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/test/integration/common/framework.go#L481>ControllerTests</a>.</li><li>To extend the provider specfic test cases, the changes should be done in the <code>machine-controller-manager-provider-{provider-name}</code> repository. For example, to extended the testcases for <code>machine-controller-manager-provider-aws</code>, make changes to <code>test/integration/controller/controller_test.go</code> inside the <code>machine-controller-manager-provider-aws</code> repository. <code>commons</code> contains the <code>Cluster</code> and <code>Clientset</code> objects that makes it easy to extend the tests.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-36957ddd64714687c5ba570bff3effe0>1.1.3 - Local Setup</h1><h1 id=preparing-the-local-development-setup-mac-os-x>Preparing the Local Development Setup (Mac OS X)</h1><ul><li><a href=#preparing-the-local-development-setup-mac-os-x>Preparing the Local Development Setup (Mac OS X)</a><ul><li><a href=#installing-golang-environment>Installing Golang environment</a></li><li><a href=#installing-docker-optional>Installing <code>Docker</code> (Optional)</a></li><li><a href=#setup-docker-hub-account-optional>Setup <code>Docker Hub</code> account (Optional)</a></li><li><a href=#local-development>Local development</a><ul><li><a href=#installing-the-machine-controller-manager-locally>Installing the Machine Controller Manager locally</a></li></ul></li><li><a href=#prepare-the-cluster>Prepare the cluster</a></li><li><a href=#getting-started>Getting started</a></li><li><a href=#testing-machine-classes>Testing Machine Classes</a></li><li><a href=#usage>Usage</a></li></ul></li></ul><p>Conceptionally, the Machine Controller Manager is designed to run in a container within a Pod inside a Kubernetes cluster. For development purposes, you can run the Machine Controller Manager as a Go process on your local machine. This process connects to your remote cluster to manage VMs for that cluster. That means that the Machine Controller Manager runs outside a Kubernetes cluster which requires providing a <a href=https://kubernetes.io/docs/tasks/access-application-cluster/authenticate-across-clusters-kubeconfig/>Kubeconfig</a> in your local filesystem and point the Machine Controller Manager to it when running it (see below).</p><p>Although the following installation instructions are for Mac OS X, similar alternate commands could be found for any Linux distribution.</p><h2 id=installing-golang-environment>Installing Golang environment</h2><p>Install the latest version of Golang (at least <code>v1.8.3</code> is required) by using <a href=https://brew.sh/>Homebrew</a>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ brew install golang
</span></span></code></pre></div><p>In order to perform linting on the Go source code, install <a href=https://github.com/golang/lint>Golint</a>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ go get -u golang.org/x/lint/golint
</span></span></code></pre></div><h2 id=installing-docker-optional>Installing <code>Docker</code> (Optional)</h2><p>In case you want to build Docker images for the Machine Controller Manager you have to install Docker itself. We recommend using <a href=https://docs.docker.com/docker-for-mac/>Docker for Mac OS X</a> which can be downloaded from <a href=https://download.docker.com/mac/stable/Docker.dmg>here</a>.</p><h2 id=setup-docker-hub-account-optional>Setup <code>Docker Hub</code> account (Optional)</h2><p>Create a Docker hub account at <a href=https://hub.docker.com/>Docker Hub</a> if you don&rsquo;t already have one.</p><h2 id=local-development>Local development</h2><p>⚠️ Before you start developing, please ensure to comply with the following requirements:</p><ol><li>You have understood the <a href=https://kubernetes.io/docs/concepts/>principles of Kubernetes</a>, and its <a href=https://kubernetes.io/docs/concepts/overview/components/>components</a>, what their purpose is and how they interact with each other.</li><li>You have understood the <a href=/docs/other-components/machine-controller-manager/#design-of-machine-controller-manager>architecture of the Machine Controller Manager</a></li></ol><p>The development of the Machine Controller Manager could happen by targetting any cluster. You basically need a Kubernetes cluster running on a set of machines. You just need the <a href=https://kubernetes.io/docs/tasks/access-application-cluster/authenticate-across-clusters-kubeconfig/>Kubeconfig</a> file with the required access permissions attached to it.</p><h3 id=installing-the-machine-controller-manager-locally>Installing the Machine Controller Manager locally</h3><p>Clone the repository from GitHub.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ git clone git@github.com:gardener/machine-controller-manager.git
</span></span><span style=display:flex><span>$ cd machine-controller-manager
</span></span></code></pre></div><h2 id=prepare-the-cluster>Prepare the cluster</h2><ul><li>Connect to the remote kubernetes cluster where you plan to deploy the Machine Controller Manager using kubectl. Set the environment variable KUBECONFIG to the path of the yaml file containing your cluster info</li><li>Now, create the required CRDs on the remote cluster using the following command,</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/crds.yaml
</span></span></code></pre></div><h2 id=getting-started>Getting started</h2><ul><li>Create a <code>dev</code> directory.</li><li>Copy the kubeconfig of kubernetes cluster where you wish to deploy the machines into <code>dev/target-kubeconfig.yaml</code>.</li><li>(optional) Copy the kubeconfig of kubernetes cluster from where you wish to manage the machines into <code>dev/control-kubeconfig.yaml</code>. If you do this, also update the <code>Makefile</code> variable CONTROL_KUBECONFIG to point to <code>dev/control-kubeconfig.yaml</code> and CONTROL_NAMESPACE to the namespace in which your controller watches over.</li><li>There is a rule dev in the <code>Makefile</code> which will automatically start the Machine Controller Manager with development settings:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make start
</span></span><span style=display:flex><span>I1227 11:08:19.963638   55523 controllermanager.go:204] Starting shared informers
</span></span><span style=display:flex><span>I1227 11:08:20.766085   55523 controller.go:247] Starting machine-controller-manager
</span></span></code></pre></div><p>⚠️ The file <code>dev/target-kubeconfig.yaml</code> points to the cluster whose nodes you want to manage. <code>dev/control-kubeconfig.yaml</code> points to the cluster from where you want to manage the nodes from. However, <code>dev/control-kubeconfig.yaml</code> is optional.</p><p>The Machine Controller Manager should now be ready to manage the VMs in your kubernetes cluster.</p><p>⚠️ This is assuming that your MCM is built to manage machines for any in-tree supported providers. There is a new way to deploy and manage out of tree (external) support for providers whose development can be <a href=/docs/other-components/machine-controller-manager/development/cp_support_new/>found here</a></p><h2 id=testing-machine-classes>Testing Machine Classes</h2><p>To test the creation/deletion of a single instance for one particular machine class you can use the <code>managevm</code> cli. The corresponding <code>INFRASTRUCTURE-machine-class.yaml</code> and the <code>INFRASTRUCTURE-secret.yaml</code> need to be defined upfront. To build and run it</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>GO111MODULE=on go build -mod=vendor -o managevm cmd/machine-controller-manager-cli/main.go
</span></span><span style=display:flex><span><span style=color:green># create machine</span>
</span></span><span style=display:flex><span>./managevm --secret PATH_TO/INFRASTRUCTURE-secret.yaml --machineclass PATH_TO/INFRASTRUCTURE-machine-class.yaml --classkind INFRASTRUCTURE --machinename test
</span></span><span style=display:flex><span><span style=color:green># delete machine</span>
</span></span><span style=display:flex><span>./managevm --secret PATH_TO/INFRASTRUCTURE-secret.yaml --machineclass PATH_TO/INFRASTRUCTURE-machine-class.yaml --classkind INFRASTRUCTURE --machinename test --machineid INFRASTRUCTURE:///REGION/INSTANCE_ID
</span></span></code></pre></div><h2 id=usage>Usage</h2><p>To start using Machine Controller Manager, follow the links given at <a href=/docs/other-components/machine-controller-manager/>usage here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-22a88520e1411c37c1ad5d69c3a7df2d>1.1.4 - Machine Error Codes</h1><h1 id=machine-error-code-handling>Machine Error code handling</h1><h2 id=notational-conventions>Notational Conventions</h2><p>The keywords &ldquo;MUST&rdquo;, &ldquo;MUST NOT&rdquo;, &ldquo;REQUIRED&rdquo;, &ldquo;SHALL&rdquo;, &ldquo;SHALL NOT&rdquo;, &ldquo;SHOULD&rdquo;, &ldquo;SHOULD NOT&rdquo;, &ldquo;RECOMMENDED&rdquo;, &ldquo;NOT RECOMMENDED&rdquo;, &ldquo;MAY&rdquo;, and &ldquo;OPTIONAL&rdquo; are to be interpreted as described in <a href=https://datatracker.ietf.org/doc/html/rfc2119>RFC 2119</a> (Bradner, S., &ldquo;Key words for use in RFCs to Indicate Requirement Levels&rdquo;, BCP 14, RFC 2119, March 1997).</p><p>The key words &ldquo;unspecified&rdquo;, &ldquo;undefined&rdquo;, and &ldquo;implementation-defined&rdquo; are to be interpreted as described in the <a href="https://www.open-std.org/jtc1/sc22/wg14/www/C99RationaleV5.10.pdf#page=18">rationale for the C99 standard</a>.</p><p>An implementation is not compliant if it fails to satisfy one or more of the MUST, REQUIRED, or SHALL requirements for the protocols it implements.
An implementation is compliant if it satisfies all the MUST, REQUIRED, and SHALL requirements for the protocols it implements.</p><h2 id=terminology>Terminology</h2><table><thead><tr><th>Term</th><th>Definition</th></tr></thead><tbody><tr><td>CR</td><td>Custom Resource (CR) is defined by a cluster admin using the Kubernetes Custom Resource Definition primitive.</td></tr><tr><td>VM</td><td>A Virtual Machine (VM) provisioned and managed by a provider. It could also refer to a physical machine in case of a bare metal provider.</td></tr><tr><td>Machine</td><td>Machine refers to a VM that is provisioned/managed by MCM. It typically describes the metadata used to store/represent a Virtual Machine</td></tr><tr><td>Node</td><td>Native kubernetes <code>Node</code> object. The objects you get to see when you do a &ldquo;kubectl get nodes&rdquo;. Although nodes can be either physical/virtual machines, for the purposes of our discussions it refers to a VM.</td></tr><tr><td>MCM</td><td><a href=https://github.com/gardener/machine-controller-manager>Machine Controller Manager (MCM)</a> is the controller used to manage higher level Machine Custom Resource (CR) such as machine-set and machine-deployment CRs.</td></tr><tr><td>Provider/Driver/MC</td><td><code>Provider</code> (or) <code>Driver</code> (or) <code>Machine Controller (MC)</code> is the driver responsible for managing machine objects present in the cluster from whom it manages these machines. A simple example could be creation/deletion of VM on the provider.</td></tr></tbody></table><h2 id=pre-requisite>Pre-requisite</h2><h3 id=machineclass-resources>MachineClass Resources</h3><p>MCM introduces the CRD <code>MachineClass</code>. This is a blueprint for creating machines that join a certain cluster as nodes in a certain role. The provider only works with <code>MachineClass</code> resources that have the structure described here.</p><h4 id=providerspec>ProviderSpec</h4><p>The <code>MachineClass</code> resource contains a <code>providerSpec</code> field that is passed in the <code>ProviderSpec</code> request field to CMI methods such as <a href=#createmachine>CreateMachine</a>. The <code>ProviderSpec</code> can be thought of as a machine template from which the VM specification must be adopted. It can contain key-value pairs of these specs. An example for these key-value pairs are given below.</p><table><thead><tr><th>Parameter</th><th>Mandatory</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>vmPool</code></td><td>Yes</td><td><code>string</code></td><td>VM pool name, e.g. <code>TEST-WOKER-POOL</code></td></tr><tr><td><code>size</code></td><td>Yes</td><td><code>string</code></td><td>VM size, e.g. <code>xsmall</code>, <code>small</code>, etc. Each size maps to a number of CPUs and memory size.</td></tr><tr><td><code>rootFsSize</code></td><td>No</td><td><code>int</code></td><td>Root (<code>/</code>) filesystem size in GB</td></tr><tr><td><code>tags</code></td><td>Yes</td><td><code>map</code></td><td>Tags to be put on the created VM</td></tr></tbody></table><p>Most of the <code>ProviderSpec</code> fields are not mandatory. If not specified, the provider passes an empty value in the respective <code>Create VM</code> parameter.</p><p>The <code>tags</code> can be used to map a VM to its corresponding machine object&rsquo;s Name</p><p>The <code>ProviderSpec</code> is validated by methods that receive it as a request field for presence of all mandatory parameters and tags, and for validity of all parameters.</p><h4 id=secrets>Secrets</h4><p>The <code>MachineClass</code> resource also contains a <code>secretRef</code> field that contains a reference to a secret. The keys of this secret are passed in the <code>Secrets</code> request field to CMI methods.</p><p>The secret can contain sensitive data such as</p><ul><li><code>cloud-credentials</code> secret data used to authenticate at the provider</li><li><code>cloud-init</code> scripts used to initialize a new VM. The cloud-init script is expected to contain scripts to initialize the Kubelet and make it join the cluster.</li></ul><h4 id=identifying-cluster-machines>Identifying Cluster Machines</h4><p>To implement certain methods, the provider should be able to identify all machines associated with a particular Kubernetes cluster. This can be achieved using one/more of the below mentioned ways:</p><ul><li>Names of VMs created by the provider are prefixed by the cluster ID specified in the ProviderSpec.</li><li>VMs created by the provider are tagged with the special tags like <code>kubernetes.io/cluster</code> (for the cluster ID) and <code>kubernetes.io/role</code> (for the role), specified in the ProviderSpec.</li><li>Mapping <code>Resource Groups</code> to individual cluster.</li></ul><h3 id=error-scheme>Error Scheme</h3><p>All provider API calls defined in this spec MUST return a <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/util/provider/machinecodes/codes/codes.go>machine error status</a>, which is very similar to <a href=https://github.com/grpc/grpc/blob/master/src/proto/grpc/status/status.proto>standard machine status</a>.</p><h3 id=machine-provider-interface>Machine Provider Interface</h3><ul><li>The provider MUST have a unique way to map a <code>machine object</code> to a <code>VM</code> which triggers the deletion for the corresponding VM backing the machine object.</li><li>The provider SHOULD have a unique way to map the <code>ProviderSpec</code> of a machine-class to a unique <code>Cluster</code>. This avoids deletion of other machines, not backed by the MCM.</li></ul><h4 id=createmachine><code>CreateMachine</code></h4><p>A Provider is REQUIRED to implement this interface method.
This interface method will be called by the MCM to provision a new VM on behalf of the requesting machine object.</p><ul><li><p>This call requests the provider to create a VM backing the machine-object.</p></li><li><p>If VM backing the <code>Machine.Name</code> already exists, and is compatible with the specified <code>Machine</code> object in the <code>CreateMachineRequest</code>, the Provider MUST reply <code>0 OK</code> with the corresponding <code>CreateMachineResponse</code>.</p></li><li><p>The provider can OPTIONALLY make use of the MachineClass supplied in the <code>MachineClass</code> in the <code>CreateMachineRequest</code> to communicate with the provider.</p></li><li><p>The provider can OPTIONALLY make use of the secrets supplied in the <code>Secret</code> in the <code>CreateMachineRequest</code> to communicate with the provider.</p></li><li><p>The provider can OPTIONALLY make use of the <code>Status.LastKnownState</code> in the <code>Machine</code> object to decode the state of the VM operation based on the last known state of the VM. This can be useful to restart/continue an operations which are mean&rsquo;t to be atomic.</p></li><li><p>The provider MUST have a unique way to map a <code>machine object</code> to a <code>VM</code>. This could be implicitly provided by the provider by letting you set VM-names (or) could be explicitly specified by the provider using appropriate tags to map the same.</p></li><li><p>This operation SHOULD be idempotent.</p></li><li><p>The <code>CreateMachineResponse</code> returned by this method is expected to return</p><ul><li><code>ProviderID</code> that uniquely identifys the VM at the provider. This is expected to match with the node.Spec.ProviderID on the node object.</li><li><code>NodeName</code> that is the expected name of the machine when it joins the cluster. It must match with the node name.</li><li><code>LastKnownState</code> is an OPTIONAL field that can store details of the last known state of the VM. It can be used by future operation calls to determine current infrastucture state. This state is saved on the machine object.</li></ul></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// CreateMachine call is responsible for VM creation on the provider
</span></span></span><span style=display:flex><span><span style=color:green></span>CreateMachine(context.Context, <span>*</span>CreateMachineRequest) (<span>*</span>CreateMachineResponse, error)<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// CreateMachineRequest is the create request for VM creation
</span></span></span><span style=display:flex><span><span style=color:green></span>type CreateMachineRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// Machine object from whom VM is to be created
</span></span></span><span style=display:flex><span><span style=color:green></span>	Machine <span>*</span>v1alpha1.Machine<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineClass backing the machine object
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineClass <span>*</span>v1alpha1.MachineClass<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>//  Secret backing the machineClass object
</span></span></span><span style=display:flex><span><span style=color:green></span>	Secret <span>*</span>corev1.Secret<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// CreateMachineResponse is the create response for VM creation
</span></span></span><span style=display:flex><span><span style=color:green></span>type CreateMachineResponse struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// ProviderID is the unique identification of the VM at the cloud provider.
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// ProviderID typically matches with the node.Spec.ProviderID on the node object.
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// Eg: gce://project-name/region/vm-ID
</span></span></span><span style=display:flex><span><span style=color:green></span>	ProviderID <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// NodeName is the name of the node-object registered to kubernetes.
</span></span></span><span style=display:flex><span><span style=color:green></span>	NodeName <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// LastKnownState represents the last state of the VM during an creation/deletion error
</span></span></span><span style=display:flex><span><span style=color:green></span>	LastKnownState <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span></code></pre></div><h5 id=createmachine-errors>CreateMachine Errors</h5><p>If the provider is unable to complete the CreateMachine call successfully, it MUST return a non-ok ginterface method code in the machine status.
If the conditions defined below are encountered, the provider MUST return the specified machine error code.
The MCM MUST implement the specified error recovery behavior when it encounters the machine error code.</p><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>The call was successful in creating/adopting a VM that matches supplied creation request. The <code>CreateMachineResponse</code> is returned with desired values</td><td></td><td>N</td></tr><tr><td>1 CANCELED</td><td>Cancelled</td><td>Call was cancelled. Perform any pending clean-up tasks and return the call</td><td></td><td>N</td></tr><tr><td>2 UNKNOWN</td><td>Something went wrong</td><td>Not enough information on what went wrong</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>3 INVALID_ARGUMENT</td><td>Re-check supplied parameters</td><td>Re-check the supplied <code>Machine.Name</code> and <code>ProviderSpec</code>. Make sure all parameters are in permitted range of values. Exact issue to be given in <code>.message</code></td><td>Update providerSpec to fix issues.</td><td>N</td></tr><tr><td>4 DEADLINE_EXCEEDED</td><td>Timeout</td><td>The call processing exceeded supplied deadline</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>6 ALREADY_EXISTS</td><td>Already exists but desired parameters doesn&rsquo;t match</td><td>Parameters of the existing VM don&rsquo;t match the ProviderSpec</td><td>Create machine with a different name</td><td>N</td></tr><tr><td>7 PERMISSION_DENIED</td><td>Insufficent permissions</td><td>The requestor doesn&rsquo;t have enough permissions to create an VM and it&rsquo;s required dependencies</td><td>Update requestor permissions to grant the same</td><td>N</td></tr><tr><td>8 RESOURCE_EXHAUSTED</td><td>Resource limits have been reached</td><td>The requestor doesn&rsquo;t have enough resource limits to process this creation request</td><td>Enhance resource limits associated with the user/account to process this</td><td>N</td></tr><tr><td>9 PRECONDITION_FAILED</td><td>VM is in inconsistent state</td><td>The VM is in a state that is invalid for this operation</td><td>Manual intervention might be needed to fix the state of the VM</td><td>N</td></tr><tr><td>10 ABORTED</td><td>Operation is pending</td><td>Indicates that there is already an operation pending for the specified machine</td><td>Wait until previous pending operation is processed</td><td>Y</td></tr><tr><td>11 OUT_OF_RANGE</td><td>Resources were out of range</td><td>The requested number of CPUs, memory size, of FS size in ProviderSpec falls outside of the corresponding valid range</td><td>Update request paramaters to request valid resource requests</td><td>N</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this service.</td><td>Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Needs manual intervension to fix this</td><td>N</td></tr><tr><td>14 UNAVAILABLE</td><td>Not Available</td><td>Unavailable indicates the service is currently unavailable.</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>16 UNAUTHENTICATED</td><td>Missing provider credentials</td><td>Request does not have valid authentication credentials for the operation</td><td>Fix the provider credentials</td><td>N</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h4 id=deletemachine><code>DeleteMachine</code></h4><p>A Provider is REQUIRED to implement this driver call.
This driver call will be called by the MCM to deprovision/delete/terminate a VM backed by the requesting machine object.</p><ul><li><p>If a VM corresponding to the specified machine-object&rsquo;s name does not exist or the artifacts associated with the VM do not exist anymore (after deletion), the Provider MUST reply <code>0 OK</code>.</p></li><li><p>The provider SHALL only act on machines belonging to the cluster-id/cluster-name obtained from the <code>ProviderSpec</code>.</p></li><li><p>The provider can OPTIONALY make use of the secrets supplied in the <code>Secrets</code> map in the <code>DeleteMachineRequest</code> to communicate with the provider.</p></li><li><p>The provider can OPTIONALY make use of the <code>Spec.ProviderID</code> map in the <code>Machine</code> object.</p></li><li><p>The provider can OPTIONALLY make use of the <code>Status.LastKnownState</code> in the <code>Machine</code> object to decode the state of the VM operation based on the last known state of the VM. This can be useful to restart/continue an operations which are mean&rsquo;t to be atomic.</p></li><li><p>This operation SHOULD be idempotent.</p></li><li><p>The provider must have a unique way to map a <code>machine object</code> to a <code>VM</code> which triggers the deletion for the corresponding VM backing the machine object.</p></li><li><p>The <code>DeleteMachineResponse</code> returned by this method is expected to return</p><ul><li><code>LastKnownState</code> is an OPTIONAL field that can store details of the last known state of the VM. It can be used by future operation calls to determine current infrastucture state. This state is saved on the machine object.</li></ul></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// DeleteMachine call is responsible for VM deletion/termination on the provider
</span></span></span><span style=display:flex><span><span style=color:green></span>DeleteMachine(context.Context, <span>*</span>DeleteMachineRequest) (<span>*</span>DeleteMachineResponse, error)<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// DeleteMachineRequest is the delete request for VM deletion
</span></span></span><span style=display:flex><span><span style=color:green></span>type DeleteMachineRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// Machine object from whom VM is to be deleted
</span></span></span><span style=display:flex><span><span style=color:green></span>	Machine <span>*</span>v1alpha1.Machine<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineClass backing the machine object
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineClass <span>*</span>v1alpha1.MachineClass<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// Secret backing the machineClass object
</span></span></span><span style=display:flex><span><span style=color:green></span>	Secret <span>*</span>corev1.Secret<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// DeleteMachineResponse is the delete response for VM deletion
</span></span></span><span style=display:flex><span><span style=color:green></span>type DeleteMachineResponse struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// LastKnownState represents the last state of the VM during an creation/deletion error
</span></span></span><span style=display:flex><span><span style=color:green></span>	LastKnownState <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span></code></pre></div><h5 id=deletemachine-errors>DeleteMachine Errors</h5><p>If the provider is unable to complete the DeleteMachine call successfully, it MUST return a non-ok machine code in the machine status.
If the conditions defined below are encountered, the provider MUST return the specified machine error code.</p><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>The call was successful in deleting a VM that matches supplied deletion request.</td><td></td><td>N</td></tr><tr><td>1 CANCELED</td><td>Cancelled</td><td>Call was cancelled. Perform any pending clean-up tasks and return the call</td><td></td><td>N</td></tr><tr><td>2 UNKNOWN</td><td>Something went wrong</td><td>Not enough information on what went wrong</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>3 INVALID_ARGUMENT</td><td>Re-check supplied parameters</td><td>Re-check the supplied <code>Machine.Name</code> and make sure that it is in the desired format and not a blank value. Exact issue to be given in <code>.message</code></td><td>Update <code>Machine.Name</code> to fix issues.</td><td>N</td></tr><tr><td>4 DEADLINE_EXCEEDED</td><td>Timeout</td><td>The call processing exceeded supplied deadline</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>7 PERMISSION_DENIED</td><td>Insufficent permissions</td><td>The requestor doesn&rsquo;t have enough permissions to delete an VM and it&rsquo;s required dependencies</td><td>Update requestor permissions to grant the same</td><td>N</td></tr><tr><td>9 PRECONDITION_FAILED</td><td>VM is in inconsistent state</td><td>The VM is in a state that is invalid for this operation</td><td>Manual intervention might be needed to fix the state of the VM</td><td>N</td></tr><tr><td>10 ABORTED</td><td>Operation is pending</td><td>Indicates that there is already an operation pending for the specified machine</td><td>Wait until previous pending operation is processed</td><td>Y</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this service.</td><td>Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Needs manual intervension to fix this</td><td>N</td></tr><tr><td>14 UNAVAILABLE</td><td>Not Available</td><td>Unavailable indicates the service is currently unavailable.</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>16 UNAUTHENTICATED</td><td>Missing provider credentials</td><td>Request does not have valid authentication credentials for the operation</td><td>Fix the provider credentials</td><td>N</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h4 id=getmachinestatus><code>GetMachineStatus</code></h4><p>A Provider can OPTIONALLY implement this driver call. Else should return a <code>UNIMPLEMENTED</code> status in error.
This call will be invoked by the MC to get the status of a machine.
This optional driver call helps in optimizing the working of the provider by avoiding unwanted calls to <code>CreateMachine()</code> and <code>DeleteMachine()</code>.</p><ul><li>If a VM corresponding to the specified machine object&rsquo;s <code>Machine.Name</code> exists on provider the <code>GetMachineStatusResponse</code> fields are to be filled similar to the <code>CreateMachineResponse</code>.</li><li>The provider SHALL only act on machines belonging to the cluster-id/cluster-name obtained from the <code>ProviderSpec</code>.</li><li>The provider can OPTIONALY make use of the secrets supplied in the <code>Secrets</code> map in the <code>GetMachineStatusRequest</code> to communicate with the provider.</li><li>The provider can OPTIONALY make use of the VM unique ID (returned by the provider on machine creation) passed in the <code>ProviderID</code> map in the <code>GetMachineStatusRequest</code>.</li><li>This operation MUST be idempotent.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// GetMachineStatus call get&#39;s the status of the VM backing the machine object on the provider
</span></span></span><span style=display:flex><span><span style=color:green></span>GetMachineStatus(context.Context, <span>*</span>GetMachineStatusRequest) (<span>*</span>GetMachineStatusResponse, error)<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// GetMachineStatusRequest is the get request for VM info
</span></span></span><span style=display:flex><span><span style=color:green></span>type GetMachineStatusRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// Machine object from whom VM status is to be fetched
</span></span></span><span style=display:flex><span><span style=color:green></span>	Machine <span>*</span>v1alpha1.Machine<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineClass backing the machine object
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineClass <span>*</span>v1alpha1.MachineClass<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>//  Secret backing the machineClass object
</span></span></span><span style=display:flex><span><span style=color:green></span>	Secret <span>*</span>corev1.Secret<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// GetMachineStatusResponse is the get response for VM info
</span></span></span><span style=display:flex><span><span style=color:green></span>type GetMachineStatusResponse struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// ProviderID is the unique identification of the VM at the cloud provider.
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// ProviderID typically matches with the node.Spec.ProviderID on the node object.
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// Eg: gce://project-name/region/vm-ID
</span></span></span><span style=display:flex><span><span style=color:green></span>	ProviderID <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// NodeName is the name of the node-object registered to kubernetes.
</span></span></span><span style=display:flex><span><span style=color:green></span>	NodeName <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span></code></pre></div><h5 id=getmachinestatus-errors>GetMachineStatus Errors</h5><p>If the provider is unable to complete the GetMachineStatus call successfully, it MUST return a non-ok machine code in the machine status.
If the conditions defined below are encountered, the provider MUST return the specified machine error code.</p><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>The call was successful in getting machine details for given machine <code>Machine.Name</code></td><td></td><td>N</td></tr><tr><td>1 CANCELED</td><td>Cancelled</td><td>Call was cancelled. Perform any pending clean-up tasks and return the call</td><td></td><td>N</td></tr><tr><td>2 UNKNOWN</td><td>Something went wrong</td><td>Not enough information on what went wrong</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>3 INVALID_ARGUMENT</td><td>Re-check supplied parameters</td><td>Re-check the supplied <code>Machine.Name</code> and make sure that it is in the desired format and not a blank value. Exact issue to be given in <code>.message</code></td><td>Update <code>Machine.Name</code> to fix issues.</td><td>N</td></tr><tr><td>4 DEADLINE_EXCEEDED</td><td>Timeout</td><td>The call processing exceeded supplied deadline</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>5 NOT_FOUND</td><td>Machine isn&rsquo;t found at provider</td><td>The machine could not be found at provider</td><td>Not required</td><td>N</td></tr><tr><td>7 PERMISSION_DENIED</td><td>Insufficent permissions</td><td>The requestor doesn&rsquo;t have enough permissions to get details for the VM and it&rsquo;s required dependencies</td><td>Update requestor permissions to grant the same</td><td>N</td></tr><tr><td>9 PRECONDITION_FAILED</td><td>VM is in inconsistent state</td><td>The VM is in a state that is invalid for this operation</td><td>Manual intervention might be needed to fix the state of the VM</td><td>N</td></tr><tr><td>11 OUT_OF_RANGE</td><td>Multiple VMs found</td><td>Multiple VMs found with matching machine object names</td><td>Orphan VM handler to cleanup orphan VMs / Manual intervention maybe required if orphan VM handler isn&rsquo;t enabled.</td><td>Y</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this service.</td><td>Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Needs manual intervension to fix this</td><td>N</td></tr><tr><td>14 UNAVAILABLE</td><td>Not Available</td><td>Unavailable indicates the service is currently unavailable.</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>16 UNAUTHENTICATED</td><td>Missing provider credentials</td><td>Request does not have valid authentication credentials for the operation</td><td>Fix the provider credentials</td><td>N</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h4 id=listmachines><code>ListMachines</code></h4><p>A Provider can OPTIONALLY implement this driver call. Else should return a <code>UNIMPLEMENTED</code> status in error.
The Provider SHALL return the information about all the machines associated with the <code>MachineClass</code>.
Make sure to use appropriate filters to achieve the same to avoid data transfer overheads.
This optional driver call helps in cleaning up orphan VMs present in the cluster. If not implemented, any orphan VM that might have been created incorrectly by the MCM/Provider (due to bugs in code/infra) might require manual clean up.</p><ul><li>If the Provider succeeded in returning a list of <code>Machine.Name</code> with their corresponding <code>ProviderID</code>, then return <code>0 OK</code>.</li><li>The <code>ListMachineResponse</code> contains a map of <code>MachineList</code> whose<ul><li>Key is expected to contain the <code>ProviderID</code> &</li><li>Value is expected to contain the <code>Machine.Name</code> corresponding to it&rsquo;s kubernetes machine CR object</li></ul></li><li>The provider can OPTIONALY make use of the secrets supplied in the <code>Secrets</code> map in the <code>ListMachinesRequest</code> to communicate with the provider.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// ListMachines lists all the machines that might have been created by the supplied machineClass
</span></span></span><span style=display:flex><span><span style=color:green></span>ListMachines(context.Context, <span>*</span>ListMachinesRequest) (<span>*</span>ListMachinesResponse, error)<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// ListMachinesRequest is the request object to get a list of VMs belonging to a machineClass
</span></span></span><span style=display:flex><span><span style=color:green></span>type ListMachinesRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineClass object
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineClass <span>*</span>v1alpha1.MachineClass<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// Secret backing the machineClass object
</span></span></span><span style=display:flex><span><span style=color:green></span>	Secret <span>*</span>corev1.Secret<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// ListMachinesResponse is the response object of the list of VMs belonging to a machineClass
</span></span></span><span style=display:flex><span><span style=color:green></span>type ListMachinesResponse struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineList is the map of list of machines. Format for the map should be &lt;ProviderID, MachineName&gt;.
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineList map[<span style=color:#2b91af>string</span>]<span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span></code></pre></div><h5 id=listmachines-errors>ListMachines Errors</h5><p>If the provider is unable to complete the ListMachines call successfully, it MUST return a non-ok machine code in the machine status.
If the conditions defined below are encountered, the provider MUST return the specified machine error code.
The MCM MUST implement the specified error recovery behavior when it encounters the machine error code.</p><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>The call for listing all VMs associated with <code>ProviderSpec</code> was successful.</td><td></td><td>N</td></tr><tr><td>1 CANCELED</td><td>Cancelled</td><td>Call was cancelled. Perform any pending clean-up tasks and return the call</td><td></td><td>N</td></tr><tr><td>2 UNKNOWN</td><td>Something went wrong</td><td>Not enough information on what went wrong</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>3 INVALID_ARGUMENT</td><td>Re-check supplied parameters</td><td>Re-check the supplied <code>ProviderSpec</code> and make sure that all required fields are present in their desired value format. Exact issue to be given in <code>.message</code></td><td>Update <code>ProviderSpec</code> to fix issues.</td><td>N</td></tr><tr><td>4 DEADLINE_EXCEEDED</td><td>Timeout</td><td>The call processing exceeded supplied deadline</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>7 PERMISSION_DENIED</td><td>Insufficent permissions</td><td>The requestor doesn&rsquo;t have enough permissions to list VMs and it&rsquo;s required dependencies</td><td>Update requestor permissions to grant the same</td><td>N</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this service.</td><td>Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Needs manual intervension to fix this</td><td>N</td></tr><tr><td>14 UNAVAILABLE</td><td>Not Available</td><td>Unavailable indicates the service is currently unavailable.</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>16 UNAUTHENTICATED</td><td>Missing provider credentials</td><td>Request does not have valid authentication credentials for the operation</td><td>Fix the provider credentials</td><td>N</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h4 id=getvolumeids><code>GetVolumeIDs</code></h4><p>A Provider can OPTIONALLY implement this driver call. Else should return a <code>UNIMPLEMENTED</code> status in error.
This driver call will be called by the MCM to get the <code>VolumeIDs</code> for the list of <code>PersistentVolumes (PVs)</code> supplied.
This OPTIONAL (but recommended) driver call helps in serailzied eviction of pods with PVs while draining of machines. This implies applications backed by PVs would be evicted one by one, leading to shorter application downtimes.</p><ul><li>On succesful returnal of a list of <code>Volume-IDs</code> for all supplied <code>PVSpecs</code>, the Provider MUST reply <code>0 OK</code>.</li><li>The <code>GetVolumeIDsResponse</code> is expected to return a repeated list of <code>strings</code> consisting of the <code>VolumeIDs</code> for <code>PVSpec</code> that could be extracted.</li><li>If for any <code>PV</code> the Provider wasn&rsquo;t able to identify the <code>Volume-ID</code>, the provider MAY chose to ignore it and return the <code>Volume-IDs</code> for the rest of the <code>PVs</code> for whom the <code>Volume-ID</code> was found.</li><li>Getting the <code>VolumeID</code> from the <code>PVSpec</code> depends on the Cloud-provider. You can extract this information by parsing the <code>PVSpec</code> based on the <code>ProviderType</code><ul><li><a href=https://github.com/kubernetes/api/blob/release-1.15/core/v1/types.go#L297-L339>https://github.com/kubernetes/api/blob/release-1.15/core/v1/types.go#L297-L339</a></li><li><a href=https://github.com/kubernetes/api/blob/release-1.15//core/v1/types.go#L175-L257>https://github.com/kubernetes/api/blob/release-1.15//core/v1/types.go#L175-L257</a></li></ul></li><li>This operation MUST be idempotent.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// GetVolumeIDsRequest is the request object to get a list of VolumeIDs for a PVSpec
</span></span></span><span style=display:flex><span><span style=color:green></span>type GetVolumeIDsRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// PVSpecsList is a list of PV specs for whom volume-IDs are required
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// Plugin should parse this raw data into pre-defined list of PVSpecs
</span></span></span><span style=display:flex><span><span style=color:green></span>	PVSpecs []<span>*</span>corev1.PersistentVolumeSpec<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// GetVolumeIDsResponse is the response object of the list of VolumeIDs for a PVSpec
</span></span></span><span style=display:flex><span><span style=color:green></span>type GetVolumeIDsResponse struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// VolumeIDs is a list of VolumeIDs.
</span></span></span><span style=display:flex><span><span style=color:green></span>	VolumeIDs []<span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span></code></pre></div><h5 id=getvolumeids-errors>GetVolumeIDs Errors</h5><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>The call getting list of <code>VolumeIDs</code> for the list of <code>PersistentVolumes</code> was successful.</td><td></td><td>N</td></tr><tr><td>1 CANCELED</td><td>Cancelled</td><td>Call was cancelled. Perform any pending clean-up tasks and return the call</td><td></td><td>N</td></tr><tr><td>2 UNKNOWN</td><td>Something went wrong</td><td>Not enough information on what went wrong</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>3 INVALID_ARGUMENT</td><td>Re-check supplied parameters</td><td>Re-check the supplied <code>PVSpecList</code> and make sure that it is in the desired format. Exact issue to be given in <code>.message</code></td><td>Update <code>PVSpecList</code> to fix issues.</td><td>N</td></tr><tr><td>4 DEADLINE_EXCEEDED</td><td>Timeout</td><td>The call processing exceeded supplied deadline</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this service.</td><td>Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Needs manual intervension to fix this</td><td>N</td></tr><tr><td>14 UNAVAILABLE</td><td>Not Available</td><td>Unavailable indicates the service is currently unavailable.</td><td>Retry operation after sometime</td><td>Y</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h4 id=generatemachineclassformigration><code>GenerateMachineClassForMigration</code></h4><p>A Provider SHOULD implement this driver call, else it MUST return a <code>UNIMPLEMENTED</code> status in error.
This driver call will be called by the Machine Controller to try to perform a machineClass migration for an unknown machineClass Kind. This helps in migration of one kind of machineClass to another kind. For instance an machineClass custom resource of <code>AWSMachineClass</code> to <code>MachineClass</code>.</p><ul><li>On successful generation of machine class the Provider MUST reply <code>0 OK</code> (or) <code>nil</code> error.</li><li><code>GenerateMachineClassForMigrationRequest</code> expects the provider-specific machine class (eg. AWSMachineClass)
to be supplied as the <code>ProviderSpecificMachineClass</code>. The provider is responsible for unmarshalling the golang struct. It also passes a reference to an existing <code>MachineClass</code> object.</li><li>The provider is expected to fill in this<code>MachineClass</code> object based on the conversions.</li><li>An optional <code>ClassSpec</code> containing the <code>type ClassSpec struct</code> is also provided to decode the provider info.</li><li><code>GenerateMachineClassForMigration</code> is only responsible for filling up the passed <code>MachineClass</code> object.</li><li>The task of creating the new <code>CR</code> of the new kind (MachineClass) with the same name as the previous one and also annotating the old machineClass CR with a migrated annotation and migrating existing references is done by the calling library implicitly.</li><li>This operation MUST be idempotent.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// GenerateMachineClassForMigrationRequest is the request for generating the generic machineClass
</span></span></span><span style=display:flex><span><span style=color:green>// for the provider specific machine class
</span></span></span><span style=display:flex><span><span style=color:green></span>type GenerateMachineClassForMigrationRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// ProviderSpecificMachineClass is provider specfic machine class object.
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// E.g. AWSMachineClass
</span></span></span><span style=display:flex><span><span style=color:green></span>	ProviderSpecificMachineClass interface{}<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineClass is the machine class object generated that is to be filled up
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineClass <span>*</span>v1alpha1.MachineClass<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// ClassSpec contains the class spec object to determine the machineClass kind
</span></span></span><span style=display:flex><span><span style=color:green></span>	ClassSpec <span>*</span>v1alpha1.ClassSpec<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// GenerateMachineClassForMigrationResponse is the response for generating the generic machineClass
</span></span></span><span style=display:flex><span><span style=color:green>// for the provider specific machine class
</span></span></span><span style=display:flex><span><span style=color:green></span>type GenerateMachineClassForMigrationResponse struct{}<span>
</span></span></span></code></pre></div><h5 id=migratemachineclass-errors>MigrateMachineClass Errors</h5><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>Migration of provider specific machine class was successful</td><td>Machine reconcilation is retried once the new class has been created</td><td>Y</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this provider.</td><td>None</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Might need manual intervension to fix this</td><td>Y</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h2 id=configuration-and-operation>Configuration and Operation</h2><h3 id=supervised-lifecycle-management>Supervised Lifecycle Management</h3><ul><li>For Providers packaged in software form:<ul><li>Provider Packages SHOULD use a well-documented container image format (e.g., Docker, OCI).</li><li>The chosen package image format MAY expose configurable Provider properties as environment variables, unless otherwise indicated in the section below.
Variables so exposed SHOULD be assigned default values in the image manifest.</li><li>A Provider Supervisor MAY programmatically evaluate or otherwise scan a Provider Package’s image manifest in order to discover configurable environment variables.</li><li>A Provider SHALL NOT assume that an operator or Provider Supervisor will scan an image manifest for environment variables.</li></ul></li></ul><h4 id=environment-variables>Environment Variables</h4><ul><li>Variables defined by this specification SHALL be identifiable by their <code>MC_</code> name prefix.</li><li>Configuration properties not defined by the MC specification SHALL NOT use the same <code>MC_</code> name prefix; this prefix is reserved for common configuration properties defined by the MC specification.</li><li>The Provider Supervisor SHOULD supply all RECOMMENDED MC environment variables to a Provider.</li><li>The Provider Supervisor SHALL supply all REQUIRED MC environment variables to a Provider.</li></ul><h5 id=logging>Logging</h5><ul><li>Providers SHOULD generate log messages to ONLY standard output and/or standard error.<ul><li>In this case the Provider Supervisor SHALL assume responsibility for all log lifecycle management.</li></ul></li><li>Provider implementations that deviate from the above recommendation SHALL clearly and unambiguously document the following:<ul><li>Logging configuration flags and/or variables, including working sample configurations.</li><li>Default log destination(s) (where do the logs go if no configuration is specified?)</li><li>Log lifecycle management ownership and related guidance (size limits, rate limits, rolling, archiving, expunging, etc.) applicable to the logging mechanism embedded within the Provider.</li></ul></li><li>Providers SHOULD NOT write potentially sensitive data to logs (e.g. secrets).</li></ul><h5 id=available-services>Available Services</h5><ul><li>Provider Packages MAY support all or a subset of CMI services; service combinations MAY be configurable at runtime by the Provider Supervisor.<ul><li>This specification does not dictate the mechanism by which mode of operation MUST be discovered, and instead places that burden upon the VM Provider.</li></ul></li><li>Misconfigured provider software SHOULD fail-fast with an OS-appropriate error code.</li></ul><h5 id=linux-capabilities>Linux Capabilities</h5><ul><li>Providers SHOULD clearly document any additionally required capabilities and/or security context.</li></ul><h5 id=cgroup-isolation>Cgroup Isolation</h5><ul><li>A Provider MAY be constrained by cgroups.</li></ul><h5 id=resource-requirements>Resource Requirements</h5><ul><li>VM Providers SHOULD unambiguously document all of a Provider’s resource requirements.</li></ul><h3 id=deploying>Deploying</h3><ul><li><strong>Recommended:</strong> The MCM and Provider are typically expected to run as two containers inside a common <code>Pod</code>.</li><li>However, for the security reasons they could execute on seperate Pods provided they have a secure way to exchange data between them.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d3b47d54c811f89c20bb333a16422ab3>1.1.5 - Testing And Dependencies</h1><h2 id=dependency-management>Dependency management</h2><p>We use golang modules to manage golang dependencies. In order to add a new package dependency to the project, you can perform <code>go get &lt;PACKAGE>@&lt;VERSION></code> or edit the <code>go.mod</code> file and append the package along with the version you want to use.</p><h3 id=updating-dependencies>Updating dependencies</h3><p>The <code>Makefile</code> contains a rule called <code>revendor</code> which performs <code>go mod vendor</code> and <code>go mod tidy</code>.</p><p><code>go mod vendor</code> resets the main module&rsquo;s vendor directory to include all packages needed to build and test all the main module&rsquo;s packages. It does not include test code for vendored packages.</p><p><code>go mod tidy</code> makes sure go.mod matches the source code in the module. It adds any missing modules necessary to build the current module&rsquo;s packages and dependencies, and it removes unused modules that don&rsquo;t provide any relevant packages.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make revendor
</span></span></code></pre></div><p>The dependencies are installed into the <code>vendor</code> folder which <strong>should be added</strong> to the VCS.</p><p>⚠️ Make sure you test the code after you have updated the dependencies!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9143519cb73c14a3f4ae463f9b24457d>1.2 - Documents</h1></div><div class=td-content><h1 id=pg-e2f257b15e56d71d58253c3abb85f408>1.2.1 - Apis</h1><h2 id=specification>Specification</h2><h3 id=providerspec-schema>ProviderSpec Schema</h3><br><h3 id=Machine><b>Machine</b></h3><p><p>Machine is the representation of a physical or virtual machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></td><td>string</td><td><code>machine.sapcloud.io.v1alpha1</code></td></tr><tr><td><code>kind</code></td><td>string</td><td><code>Machine</code></td></tr><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><p>ObjectMeta for machine object</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=#MachineSpec>MachineSpec</a></em></td><td><p>Spec contains the specification of the machine</p><br><br><table><tr><td><code>class</code></td><td><em><a href=#ClassSpec>ClassSpec</a></em></td><td><em>(Optional)</em><p>Class contains the machineclass attributes of a machine</p></td></tr><tr><td><code>providerID</code></td><td><em>string</em></td><td><em>(Optional)</em><p>ProviderID represents the provider&rsquo;s unique ID given to a machine</p></td></tr><tr><td><code>nodeTemplate</code></td><td><em><a href=#NodeTemplateSpec>NodeTemplateSpec</a></em></td><td><em>(Optional)</em><p>NodeTemplateSpec describes the data a node should have when created from a template</p></td></tr><tr><td><code>MachineConfiguration</code></td><td><em><a href=#MachineConfiguration>MachineConfiguration</a></em></td><td><p>(Members of <code>MachineConfiguration</code> are embedded into this type.)</p><em>(Optional)</em><p>Configuration for the machine-controller.</p></td></tr></table></td></tr><tr><td><code>status</code></td><td><em><a href=#MachineStatus>MachineStatus</a></em></td><td><p>Status contains fields depicting the status</p></td></tr></tbody></table><br><h3 id=MachineClass><b>MachineClass</b></h3><p><p>MachineClass can be used to templatize and re-use provider configuration
across multiple Machines / MachineSets / MachineDeployments.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></td><td>string</td><td><code>machine.sapcloud.io.v1alpha1</code></td></tr><tr><td><code>kind</code></td><td>string</td><td><code>MachineClass</code></td></tr><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>nodeTemplate</code></td><td><em><a href=#NodeTemplate>NodeTemplate</a></em></td><td><em>(Optional)</em><p>NodeTemplate contains subfields to track all node resources and other node info required to scale nodegroup from zero</p></td></tr><tr><td><code>credentialsSecretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>CredentialsSecretRef can optionally store the credentials (in this case the SecretRef does not need to store them).
This might be useful if multiple machine classes with the same credentials but different user-datas are used.</p></td></tr><tr><td><code>providerSpec</code></td><td><em><a href=#https%3a%2f%2fgodoc.org%2fk8s.io%2fapimachinery%2fpkg%2fruntime%23RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><p>Provider-specific configuration to use during node creation.</p></td></tr><tr><td><code>provider</code></td><td><em>string</em></td><td><p>Provider is the combination of name and location of cloud-specific drivers.</p></td></tr><tr><td><code>secretRef</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef stores the necessary secrets such as credentials or userdata.</p></td></tr></tbody></table><br><h3 id=MachineDeployment><b>MachineDeployment</b></h3><p><p>MachineDeployment enables declarative updates for machines and MachineSets.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></td><td>string</td><td><code>machine.sapcloud.io.v1alpha1</code></td></tr><tr><td><code>kind</code></td><td>string</td><td><code>MachineDeployment</code></td></tr><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=#MachineDeploymentSpec>MachineDeploymentSpec</a></em></td><td><em>(Optional)</em><p>Specification of the desired behavior of the MachineDeployment.</p><br><br><table><tr><td><code>replicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Number of desired machines. This is a pointer to distinguish between explicit
zero and not specified. Defaults to 0.</p></td></tr><tr><td><code>selector</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em><p>Label selector for machines. Existing MachineSets whose machines are
selected by this will be the ones affected by this MachineDeployment.</p></td></tr><tr><td><code>template</code></td><td><em><a href=#MachineTemplateSpec>MachineTemplateSpec</a></em></td><td><p>Template describes the machines that will be created.</p></td></tr><tr><td><code>strategy</code></td><td><em><a href=#MachineDeploymentStrategy>MachineDeploymentStrategy</a></em></td><td><em>(Optional)</em><p>The MachineDeployment strategy to use to replace existing machines with new ones.</p></td></tr><tr><td><code>minReadySeconds</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Minimum number of seconds for which a newly created machine should be ready
without any of its container crashing, for it to be considered available.
Defaults to 0 (machine will be considered available as soon as it is ready)</p></td></tr><tr><td><code>revisionHistoryLimit</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>The number of old MachineSets to retain to allow rollback.
This is a pointer to distinguish between explicit zero and not specified.</p></td></tr><tr><td><code>paused</code></td><td><em>bool</em></td><td><em>(Optional)</em><p>Indicates that the MachineDeployment is paused and will not be processed by the
MachineDeployment controller.</p></td></tr><tr><td><code>rollbackTo</code></td><td><em><a href=#RollbackConfig>RollbackConfig</a></em></td><td><em>(Optional)</em><p>DEPRECATED.
The config this MachineDeployment is rolling back to. Will be cleared after rollback is done.</p></td></tr><tr><td><code>progressDeadlineSeconds</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>The maximum time in seconds for a MachineDeployment to make progress before it
is considered to be failed. The MachineDeployment controller will continue to
process failed MachineDeployments and a condition with a ProgressDeadlineExceeded
reason will be surfaced in the MachineDeployment status. Note that progress will
not be estimated during the time a MachineDeployment is paused. This is not set
by default, which is treated as infinite deadline.</p></td></tr></table></td></tr><tr><td><code>status</code></td><td><em><a href=#MachineDeploymentStatus>MachineDeploymentStatus</a></em></td><td><em>(Optional)</em><p>Most recently observed status of the MachineDeployment.</p></td></tr></tbody></table><br><h3 id=MachineSet><b>MachineSet</b></h3><p><p>MachineSet TODO</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></td><td>string</td><td><code>machine.sapcloud.io.v1alpha1</code></td></tr><tr><td><code>kind</code></td><td>string</td><td><code>MachineSet</code></td></tr><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=#MachineSetSpec>MachineSetSpec</a></em></td><td><em>(Optional)</em><br><br><table><tr><td><code>replicas</code></td><td><em>int32</em></td><td><em>(Optional)</em></td></tr><tr><td><code>selector</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>machineClass</code></td><td><em><a href=#ClassSpec>ClassSpec</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>template</code></td><td><em><a href=#MachineTemplateSpec>MachineTemplateSpec</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>minReadySeconds</code></td><td><em>int32</em></td><td><em>(Optional)</em></td></tr></table></td></tr><tr><td><code>status</code></td><td><em><a href=#MachineSetStatus>MachineSetStatus</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><br><h3 id=ClassSpec><b>ClassSpec</b></h3><p>(<em>Appears on:</em>
<a href=#MachineSetSpec>MachineSetSpec</a>,
<a href=#MachineSpec>MachineSpec</a>)</p><p><p>ClassSpec is the class specification of machine</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiGroup</code></td><td><em>string</em></td><td><p>API group to which it belongs</p></td></tr><tr><td><code>kind</code></td><td><em>string</em></td><td><p>Kind for machine class</p></td></tr><tr><td><code>name</code></td><td><em>string</em></td><td><p>Name of machine class</p></td></tr></tbody></table><br><h3 id=ConditionStatus><b>ConditionStatus</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#MachineDeploymentCondition>MachineDeploymentCondition</a>,
<a href=#MachineSetCondition>MachineSetCondition</a>)</p><p></p><br><h3 id=CurrentStatus><b>CurrentStatus</b></h3><p>(<em>Appears on:</em>
<a href=#MachineStatus>MachineStatus</a>)</p><p><p>CurrentStatus contains information about the current status of Machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>phase</code></td><td><em><a href=#MachinePhase>MachinePhase</a></em></td><td></td></tr><tr><td><code>timeoutActive</code></td><td><em>bool</em></td><td></td></tr><tr><td><code>lastUpdateTime</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last update time of current status</p></td></tr></tbody></table><br><h3 id=LastOperation><b>LastOperation</b></h3><p>(<em>Appears on:</em>
<a href=#MachineSetStatus>MachineSetStatus</a>,
<a href=#MachineStatus>MachineStatus</a>,
<a href=#MachineSummary>MachineSummary</a>)</p><p><p>LastOperation suggests the last operation performed on the object</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>description</code></td><td><em>string</em></td><td><p>Description of the current operation</p></td></tr><tr><td><code>lastUpdateTime</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last update time of current operation</p></td></tr><tr><td><code>state</code></td><td><em><a href=#MachineState>MachineState</a></em></td><td><p>State of operation</p></td></tr><tr><td><code>type</code></td><td><em><a href=#MachineOperationType>MachineOperationType</a></em></td><td><p>Type of operation</p></td></tr></tbody></table><br><h3 id=MachineConfiguration><b>MachineConfiguration</b></h3><p>(<em>Appears on:</em>
<a href=#MachineSpec>MachineSpec</a>)</p><p><p>MachineConfiguration describes the configurations useful for the machine-controller.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>drainTimeout</code></td><td><em><a href=#https%3a%2f%2fgodoc.org%2fk8s.io%2fapimachinery%2fpkg%2fapis%2fmeta%2fv1%23Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>MachineDraintimeout is the timeout after which machine is forcefully deleted.</p></td></tr><tr><td><code>healthTimeout</code></td><td><em><a href=#https%3a%2f%2fgodoc.org%2fk8s.io%2fapimachinery%2fpkg%2fapis%2fmeta%2fv1%23Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>MachineHealthTimeout is the timeout after which machine is declared unhealhty/failed.</p></td></tr><tr><td><code>creationTimeout</code></td><td><em><a href=#https%3a%2f%2fgodoc.org%2fk8s.io%2fapimachinery%2fpkg%2fapis%2fmeta%2fv1%23Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>MachineCreationTimeout is the timeout after which machinie creation is declared failed.</p></td></tr><tr><td><code>maxEvictRetries</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>MaxEvictRetries is the number of retries that will be attempted while draining the node.</p></td></tr><tr><td><code>nodeConditions</code></td><td><em>*string</em></td><td><em>(Optional)</em><p>NodeConditions are the set of conditions if set to true for MachineHealthTimeOut, machine will be declared failed.</p></td></tr></tbody></table><br><h3 id=MachineDeploymentCondition><b>MachineDeploymentCondition</b></h3><p>(<em>Appears on:</em>
<a href=#MachineDeploymentStatus>MachineDeploymentStatus</a>)</p><p><p>MachineDeploymentCondition describes the state of a MachineDeployment at a certain point.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></td><td><em><a href=#MachineDeploymentConditionType>MachineDeploymentConditionType</a></em></td><td><p>Type of MachineDeployment condition.</p></td></tr><tr><td><code>status</code></td><td><em><a href=#ConditionStatus>ConditionStatus</a></em></td><td><p>Status of the condition, one of True, False, Unknown.</p></td></tr><tr><td><code>lastUpdateTime</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>The last time this condition was updated.</p></td></tr><tr><td><code>lastTransitionTime</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last time the condition transitioned from one status to another.</p></td></tr><tr><td><code>reason</code></td><td><em>string</em></td><td><p>The reason for the condition&rsquo;s last transition.</p></td></tr><tr><td><code>message</code></td><td><em>string</em></td><td><p>A human readable message indicating details about the transition.</p></td></tr></tbody></table><br><h3 id=MachineDeploymentConditionType><b>MachineDeploymentConditionType</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#MachineDeploymentCondition>MachineDeploymentCondition</a>)</p><p></p><br><h3 id=MachineDeploymentSpec><b>MachineDeploymentSpec</b></h3><p>(<em>Appears on:</em>
<a href=#MachineDeployment>MachineDeployment</a>)</p><p><p>MachineDeploymentSpec is the specification of the desired behavior of the MachineDeployment.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>replicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Number of desired machines. This is a pointer to distinguish between explicit
zero and not specified. Defaults to 0.</p></td></tr><tr><td><code>selector</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em><p>Label selector for machines. Existing MachineSets whose machines are
selected by this will be the ones affected by this MachineDeployment.</p></td></tr><tr><td><code>template</code></td><td><em><a href=#MachineTemplateSpec>MachineTemplateSpec</a></em></td><td><p>Template describes the machines that will be created.</p></td></tr><tr><td><code>strategy</code></td><td><em><a href=#MachineDeploymentStrategy>MachineDeploymentStrategy</a></em></td><td><em>(Optional)</em><p>The MachineDeployment strategy to use to replace existing machines with new ones.</p></td></tr><tr><td><code>minReadySeconds</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Minimum number of seconds for which a newly created machine should be ready
without any of its container crashing, for it to be considered available.
Defaults to 0 (machine will be considered available as soon as it is ready)</p></td></tr><tr><td><code>revisionHistoryLimit</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>The number of old MachineSets to retain to allow rollback.
This is a pointer to distinguish between explicit zero and not specified.</p></td></tr><tr><td><code>paused</code></td><td><em>bool</em></td><td><em>(Optional)</em><p>Indicates that the MachineDeployment is paused and will not be processed by the
MachineDeployment controller.</p></td></tr><tr><td><code>rollbackTo</code></td><td><em><a href=#RollbackConfig>RollbackConfig</a></em></td><td><em>(Optional)</em><p>DEPRECATED.
The config this MachineDeployment is rolling back to. Will be cleared after rollback is done.</p></td></tr><tr><td><code>progressDeadlineSeconds</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>The maximum time in seconds for a MachineDeployment to make progress before it
is considered to be failed. The MachineDeployment controller will continue to
process failed MachineDeployments and a condition with a ProgressDeadlineExceeded
reason will be surfaced in the MachineDeployment status. Note that progress will
not be estimated during the time a MachineDeployment is paused. This is not set
by default, which is treated as infinite deadline.</p></td></tr></tbody></table><br><h3 id=MachineDeploymentStatus><b>MachineDeploymentStatus</b></h3><p>(<em>Appears on:</em>
<a href=#MachineDeployment>MachineDeployment</a>)</p><p><p>MachineDeploymentStatus is the most recently observed status of the MachineDeployment.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>observedGeneration</code></td><td><em>int64</em></td><td><em>(Optional)</em><p>The generation observed by the MachineDeployment controller.</p></td></tr><tr><td><code>replicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Total number of non-terminated machines targeted by this MachineDeployment (their labels match the selector).</p></td></tr><tr><td><code>updatedReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Total number of non-terminated machines targeted by this MachineDeployment that have the desired template spec.</p></td></tr><tr><td><code>readyReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Total number of ready machines targeted by this MachineDeployment.</p></td></tr><tr><td><code>availableReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Total number of available machines (ready for at least minReadySeconds) targeted by this MachineDeployment.</p></td></tr><tr><td><code>unavailableReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Total number of unavailable machines targeted by this MachineDeployment. This is the total number of
machines that are still required for the MachineDeployment to have 100% available capacity. They may
either be machines that are running but not yet available or machines that still have not been created.</p></td></tr><tr><td><code>conditions</code></td><td><em><a href=#MachineDeploymentCondition>[]MachineDeploymentCondition</a></em></td><td><p>Represents the latest available observations of a MachineDeployment&rsquo;s current state.</p></td></tr><tr><td><code>collisionCount</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>Count of hash collisions for the MachineDeployment. The MachineDeployment controller uses this
field as a collision avoidance mechanism when it needs to create the name for the
newest MachineSet.</p></td></tr><tr><td><code>failedMachines</code></td><td><em><a href=#%2agithub.com%2fgardener%2fmachine-controller-manager%2fpkg%2fapis%2fmachine%2fv1alpha1.MachineSummary>[]*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.MachineSummary</a></em></td><td><em>(Optional)</em><p>FailedMachines has summary of machines on which lastOperation Failed</p></td></tr></tbody></table><br><h3 id=MachineDeploymentStrategy><b>MachineDeploymentStrategy</b></h3><p>(<em>Appears on:</em>
<a href=#MachineDeploymentSpec>MachineDeploymentSpec</a>)</p><p><p>MachineDeploymentStrategy describes how to replace existing machines with new ones.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></td><td><em><a href=#MachineDeploymentStrategyType>MachineDeploymentStrategyType</a></em></td><td><em>(Optional)</em><p>Type of MachineDeployment. Can be &ldquo;Recreate&rdquo; or &ldquo;RollingUpdate&rdquo;. Default is RollingUpdate.</p></td></tr><tr><td><code>rollingUpdate</code></td><td><em><a href=#RollingUpdateMachineDeployment>RollingUpdateMachineDeployment</a></em></td><td><em>(Optional)</em><p>Rolling update config params. Present only if MachineDeploymentStrategyType =</p><h2>RollingUpdate.</h2><p>TODO: Update this to follow our convention for oneOf, whatever we decide it
to be.</p></td></tr></tbody></table><br><h3 id=MachineDeploymentStrategyType><b>MachineDeploymentStrategyType</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#MachineDeploymentStrategy>MachineDeploymentStrategy</a>)</p><p></p><br><h3 id=MachineOperationType><b>MachineOperationType</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#LastOperation>LastOperation</a>)</p><p><p>MachineOperationType is a label for the operation performed on a machine object.</p></p><br><h3 id=MachinePhase><b>MachinePhase</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#CurrentStatus>CurrentStatus</a>)</p><p><p>MachinePhase is a label for the condition of a machines at the current time.</p></p><br><h3 id=MachineSetCondition><b>MachineSetCondition</b></h3><p>(<em>Appears on:</em>
<a href=#MachineSetStatus>MachineSetStatus</a>)</p><p><p>MachineSetCondition describes the state of a machine set at a certain point.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></td><td><em><a href=#MachineSetConditionType>MachineSetConditionType</a></em></td><td><p>Type of machine set condition.</p></td></tr><tr><td><code>status</code></td><td><em><a href=#ConditionStatus>ConditionStatus</a></em></td><td><p>Status of the condition, one of True, False, Unknown.</p></td></tr><tr><td><code>lastTransitionTime</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>The last time the condition transitioned from one status to another.</p></td></tr><tr><td><code>reason</code></td><td><em>string</em></td><td><em>(Optional)</em><p>The reason for the condition&rsquo;s last transition.</p></td></tr><tr><td><code>message</code></td><td><em>string</em></td><td><em>(Optional)</em><p>A human readable message indicating details about the transition.</p></td></tr></tbody></table><br><h3 id=MachineSetConditionType><b>MachineSetConditionType</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#MachineSetCondition>MachineSetCondition</a>)</p><p><p>MachineSetConditionType is the condition on machineset object</p></p><br><h3 id=MachineSetSpec><b>MachineSetSpec</b></h3><p>(<em>Appears on:</em>
<a href=#MachineSet>MachineSet</a>)</p><p><p>MachineSetSpec is the specification of a MachineSet.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>replicas</code></td><td><em>int32</em></td><td><em>(Optional)</em></td></tr><tr><td><code>selector</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>machineClass</code></td><td><em><a href=#ClassSpec>ClassSpec</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>template</code></td><td><em><a href=#MachineTemplateSpec>MachineTemplateSpec</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>minReadySeconds</code></td><td><em>int32</em></td><td><em>(Optional)</em></td></tr></tbody></table><br><h3 id=MachineSetStatus><b>MachineSetStatus</b></h3><p>(<em>Appears on:</em>
<a href=#MachineSet>MachineSet</a>)</p><p><p>MachineSetStatus holds the most recently observed status of MachineSet.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>replicas</code></td><td><em>int32</em></td><td><p>Replicas is the number of actual replicas.</p></td></tr><tr><td><code>fullyLabeledReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>The number of pods that have labels matching the labels of the pod template of the replicaset.</p></td></tr><tr><td><code>readyReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>The number of ready replicas for this replica set.</p></td></tr><tr><td><code>availableReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>The number of available replicas (ready for at least minReadySeconds) for this replica set.</p></td></tr><tr><td><code>observedGeneration</code></td><td><em>int64</em></td><td><em>(Optional)</em><p>ObservedGeneration is the most recent generation observed by the controller.</p></td></tr><tr><td><code>machineSetCondition</code></td><td><em><a href=#MachineSetCondition>[]MachineSetCondition</a></em></td><td><em>(Optional)</em><p>Represents the latest available observations of a replica set&rsquo;s current state.</p></td></tr><tr><td><code>lastOperation</code></td><td><em><a href=#LastOperation>LastOperation</a></em></td><td><p>LastOperation performed</p></td></tr><tr><td><code>failedMachines</code></td><td><em><a href=#%5b%5dgithub.com%2fgardener%2fmachine-controller-manager%2fpkg%2fapis%2fmachine%2fv1alpha1.MachineSummary>[]github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.MachineSummary</a></em></td><td><em>(Optional)</em><p>FailedMachines has summary of machines on which lastOperation Failed</p></td></tr></tbody></table><br><h3 id=MachineSpec><b>MachineSpec</b></h3><p>(<em>Appears on:</em>
<a href=#Machine>Machine</a>,
<a href=#MachineTemplateSpec>MachineTemplateSpec</a>)</p><p><p>MachineSpec is the specification of a Machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>class</code></td><td><em><a href=#ClassSpec>ClassSpec</a></em></td><td><em>(Optional)</em><p>Class contains the machineclass attributes of a machine</p></td></tr><tr><td><code>providerID</code></td><td><em>string</em></td><td><em>(Optional)</em><p>ProviderID represents the provider&rsquo;s unique ID given to a machine</p></td></tr><tr><td><code>nodeTemplate</code></td><td><em><a href=#NodeTemplateSpec>NodeTemplateSpec</a></em></td><td><em>(Optional)</em><p>NodeTemplateSpec describes the data a node should have when created from a template</p></td></tr><tr><td><code>MachineConfiguration</code></td><td><em><a href=#MachineConfiguration>MachineConfiguration</a></em></td><td><p>(Members of <code>MachineConfiguration</code> are embedded into this type.)</p><em>(Optional)</em><p>Configuration for the machine-controller.</p></td></tr></tbody></table><br><h3 id=MachineState><b>MachineState</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#LastOperation>LastOperation</a>)</p><p><p>MachineState is a current state of the machine.</p></p><br><h3 id=MachineStatus><b>MachineStatus</b></h3><p>(<em>Appears on:</em>
<a href=#Machine>Machine</a>)</p><p><p>MachineStatus holds the most recently observed status of Machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>conditions</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23nodecondition-v1-core>[]Kubernetes core/v1.NodeCondition</a></em></td><td><p>Conditions of this machine, same as node</p></td></tr><tr><td><code>lastOperation</code></td><td><em><a href=#LastOperation>LastOperation</a></em></td><td><p>Last operation refers to the status of the last operation performed</p></td></tr><tr><td><code>currentStatus</code></td><td><em><a href=#CurrentStatus>CurrentStatus</a></em></td><td><p>Current status of the machine object</p></td></tr><tr><td><code>lastKnownState</code></td><td><em>string</em></td><td><em>(Optional)</em><p>LastKnownState can store details of the last known state of the VM by the plugins.
It can be used by future operation calls to determine current infrastucture state</p></td></tr></tbody></table><br><h3 id=MachineSummary><b>MachineSummary</b></h3><p><p>MachineSummary store the summary of machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></td><td><em>string</em></td><td><p>Name of the machine object</p></td></tr><tr><td><code>providerID</code></td><td><em>string</em></td><td><p>ProviderID represents the provider&rsquo;s unique ID given to a machine</p></td></tr><tr><td><code>lastOperation</code></td><td><em><a href=#LastOperation>LastOperation</a></em></td><td><p>Last operation refers to the status of the last operation performed</p></td></tr><tr><td><code>ownerRef</code></td><td><em>string</em></td><td><p>OwnerRef</p></td></tr></tbody></table><br><h3 id=MachineTemplateSpec><b>MachineTemplateSpec</b></h3><p>(<em>Appears on:</em>
<a href=#MachineDeploymentSpec>MachineDeploymentSpec</a>,
<a href=#MachineSetSpec>MachineSetSpec</a>)</p><p><p>MachineTemplateSpec describes the data a machine should have when created from a template</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object&rsquo;s metadata.
More info: <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata>https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata</a></p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=#MachineSpec>MachineSpec</a></em></td><td><em>(Optional)</em><p>Specification of the desired behavior of the machine.
More info: <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status>https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status</a></p><br><br><table><tr><td><code>class</code></td><td><em><a href=#ClassSpec>ClassSpec</a></em></td><td><em>(Optional)</em><p>Class contains the machineclass attributes of a machine</p></td></tr><tr><td><code>providerID</code></td><td><em>string</em></td><td><em>(Optional)</em><p>ProviderID represents the provider&rsquo;s unique ID given to a machine</p></td></tr><tr><td><code>nodeTemplate</code></td><td><em><a href=#NodeTemplateSpec>NodeTemplateSpec</a></em></td><td><em>(Optional)</em><p>NodeTemplateSpec describes the data a node should have when created from a template</p></td></tr><tr><td><code>MachineConfiguration</code></td><td><em><a href=#MachineConfiguration>MachineConfiguration</a></em></td><td><p>(Members of <code>MachineConfiguration</code> are embedded into this type.)</p><em>(Optional)</em><p>Configuration for the machine-controller.</p></td></tr></table></td></tr></tbody></table><br><h3 id=NodeTemplate><b>NodeTemplate</b></h3><p>(<em>Appears on:</em>
<a href=#MachineClass>MachineClass</a>)</p><p><p>NodeTemplate contains subfields to track all node resources and other node info required to scale nodegroup from zero</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>capacity</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23resourcelist-v1-core>Kubernetes core/v1.ResourceList</a></em></td><td><p>Capacity contains subfields to track all node resources required to scale nodegroup from zero</p></td></tr><tr><td><code>instanceType</code></td><td><em>string</em></td><td><p>Instance type of the node belonging to nodeGroup</p></td></tr><tr><td><code>region</code></td><td><em>string</em></td><td><p>Region of the expected node belonging to nodeGroup</p></td></tr><tr><td><code>zone</code></td><td><em>string</em></td><td><p>Zone of the expected node belonging to nodeGroup</p></td></tr></tbody></table><br><h3 id=NodeTemplateSpec><b>NodeTemplateSpec</b></h3><p>(<em>Appears on:</em>
<a href=#MachineSpec>MachineSpec</a>)</p><p><p>NodeTemplateSpec describes the data a node should have when created from a template</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23nodespec-v1-core>Kubernetes core/v1.NodeSpec</a></em></td><td><em>(Optional)</em><p>NodeSpec describes the attributes that a node is created with.</p><br><br><table><tr><td><code>podCIDR</code></td><td><em>string</em></td><td><em>(Optional)</em><p>PodCIDR represents the pod IP range assigned to the node.</p></td></tr><tr><td><code>podCIDRs</code></td><td><em>[]string</em></td><td><em>(Optional)</em><p>podCIDRs represents the IP ranges assigned to the node for usage by Pods on that node. If this
field is specified, the 0th entry must match the podCIDR field. It may contain at most 1 value for
each of IPv4 and IPv6.</p></td></tr><tr><td><code>providerID</code></td><td><em>string</em></td><td><em>(Optional)</em><p>ID of the node assigned by the cloud provider in the format: <providername>://<providerspecificnodeid></p></td></tr><tr><td><code>unschedulable</code></td><td><em>bool</em></td><td><em>(Optional)</em><p>Unschedulable controls node schedulability of new pods. By default, node is schedulable.
More info: <a href=https://kubernetes.io/docs/concepts/nodes/node/#manual-node-administration>https://kubernetes.io/docs/concepts/nodes/node/#manual-node-administration</a></p></td></tr><tr><td><code>taints</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23taint-v1-core>[]Kubernetes core/v1.Taint</a></em></td><td><em>(Optional)</em><p>If specified, the node&rsquo;s taints.</p></td></tr><tr><td><code>configSource</code></td><td><em><a href=#https%3a%2f%2fkubernetes.io%2fdocs%2freference%2fgenerated%2fkubernetes-api%2fv1.19%2f%23nodeconfigsource-v1-core>Kubernetes core/v1.NodeConfigSource</a></em></td><td><em>(Optional)</em><p>Deprecated: Previously used to specify the source of the node&rsquo;s configuration for the DynamicKubeletConfig feature. This feature is removed.</p></td></tr><tr><td><code>externalID</code></td><td><em>string</em></td><td><em>(Optional)</em><p>Deprecated. Not all kubelets will set this field. Remove field after 1.13.
see: <a href=https://issues.k8s.io/61966>https://issues.k8s.io/61966</a></p></td></tr></table></td></tr></tbody></table><br><h3 id=RollbackConfig><b>RollbackConfig</b></h3><p>(<em>Appears on:</em>
<a href=#MachineDeploymentSpec>MachineDeploymentSpec</a>)</p><p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>revision</code></td><td><em>int64</em></td><td><em>(Optional)</em><p>The revision to rollback to. If set to 0, rollback to the last revision.</p></td></tr></tbody></table><br><h3 id=RollingUpdateMachineDeployment><b>RollingUpdateMachineDeployment</b></h3><p>(<em>Appears on:</em>
<a href=#MachineDeploymentStrategy>MachineDeploymentStrategy</a>)</p><p><p>Spec to control the desired behavior of rolling update.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>maxUnavailable</code></td><td><em><a href=#https%3a%2f%2fgodoc.org%2fk8s.io%2fapimachinery%2fpkg%2futil%2fintstr%23IntOrString>k8s.io/apimachinery/pkg/util/intstr.IntOrString</a></em></td><td><em>(Optional)</em><p>The maximum number of machines that can be unavailable during the update.
Value can be an absolute number (ex: 5) or a percentage of desired machines (ex: 10%).
Absolute number is calculated from percentage by rounding down.
This can not be 0 if MaxSurge is 0.
By default, a fixed value of 1 is used.
Example: when this is set to 30%, the old MC can be scaled down to 70% of desired machines
immediately when the rolling update starts. Once new machines are ready, old MC
can be scaled down further, followed by scaling up the new MC, ensuring
that the total number of machines available at all times during the update is at
least 70% of desired machines.</p></td></tr><tr><td><code>maxSurge</code></td><td><em><a href=#https%3a%2f%2fgodoc.org%2fk8s.io%2fapimachinery%2fpkg%2futil%2fintstr%23IntOrString>k8s.io/apimachinery/pkg/util/intstr.IntOrString</a></em></td><td><em>(Optional)</em><p>The maximum number of machines that can be scheduled above the desired number of
machines.
Value can be an absolute number (ex: 5) or a percentage of desired machines (ex: 10%).
This can not be 0 if MaxUnavailable is 0.
Absolute number is calculated from percentage by rounding up.
By default, a value of 1 is used.
Example: when this is set to 30%, the new MC can be scaled up immediately when
the rolling update starts, such that the total number of old and new machines do not exceed
130% of desired machines. Once old machines have been killed,
new MC can be scaled up further, ensuring that total number of machines running
at any time during the update is atmost 130% of desired machines.</p></td></tr></tbody></table><hr><p><em>Generated with <a href=https://github.com/ahmetb/gen-crd-api-reference-docs>gen-crd-api-reference-docs</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-79be72cd6d9b4d9cc8a5fd3bcf4d97b7>1.3 - Operations</h1></div><div class=td-content><h1 id=pg-227cca7f320594f2f0d872cb0b5c89cb>1.3.1 - Deployment</h1><h1 id=deploying-the-machine-controller-manager-into-a-kubernetes-cluster>Deploying the Machine Controller Manager into a Kubernetes cluster</h1><ul><li><a href=#deploying-the-machine-controller-manager-into-a-kubernetes-cluster>Deploying the Machine Controller Manager into a Kubernetes cluster</a><ul><li><a href=#prepare-the-cluster>Prepare the cluster</a></li><li><a href=#build-the-docker-image>Build the Docker image</a></li><li><a href=#configuring-optional-parameters-while-deploying>Configuring optional parameters while deploying</a></li><li><a href=#usage>Usage</a></li></ul></li></ul><p>As already mentioned, the Machine Controller Manager is designed to run as controller in a Kubernetes cluster. The existing source code can be compiled and tested on a local machine as described in <a href=/docs/other-components/machine-controller-manager/development/local_setup/>Setting up a local development environment</a>. You can deploy the Machine Controller Manager using the steps described below.</p><h2 id=prepare-the-cluster>Prepare the cluster</h2><ul><li>Connect to the remote kubernetes cluster where you plan to deploy the Machine Controller Manager using the kubectl. Set the environment variable KUBECONFIG to the path of the yaml file containing the cluster info.</li><li>Now, create the required CRDs on the remote cluster using the following command,</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/crds
</span></span></code></pre></div><h2 id=build-the-docker-image>Build the Docker image</h2><blockquote><p>⚠️ Modify the <code>Makefile</code> to refer to your own registry.</p></blockquote><ul><li>Run the build which generates the binary to <code>bin/machine-controller-manager</code></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make build
</span></span></code></pre></div><ul><li>Build docker image from latest compiled binary</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make docker-image
</span></span></code></pre></div><ul><li>Push the last created docker image onto the online docker registry.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make push
</span></span></code></pre></div><ul><li>Now you can deploy this docker image to your cluster. A <a href=https://github.com/gardener/machine-controller-manager/blob/master/kubernetes/deployment/out-of-tree/deployment.yaml>sample development file</a> is provided. By default, the deployment manages the cluster it is running in. Optionally, the kubeconfig could also be passed as a flag as described in <code>/kubernetes/deployment/out-of-tree/deployment.yaml</code>. This is done when you want your controller running outside the cluster to be managed from.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/deployment/out-of-tree/deployment.yaml
</span></span></code></pre></div><ul><li>Also deploy the required clusterRole and clusterRoleBindings</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/deployment/out-of-tree/clusterrole.yaml
</span></span><span style=display:flex><span>$ kubectl apply -f kubernetes/deployment/out-of-tree/clusterrolebinding.yaml
</span></span></code></pre></div><h2 id=configuring-optional-parameters-while-deploying>Configuring optional parameters while deploying</h2><p>Machine-controller-manager supports several configurable parameters while deploying. Refer to <a href=https://github.com/gardener/machine-controller-manager/blob/master/kubernetes/deployment/out-of-tree/deployment.yaml#L21-L30>the following lines</a>, to know how each parameter can be configured, and what it&rsquo;s purpose is for.</p><h2 id=usage>Usage</h2><p>To start using Machine Controller Manager, follow the links given at <a href=/docs/other-components/machine-controller-manager/>usage here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-6e8ec8f797bdf2a763e1eb51463d5a08>1.3.2 - Machine</h1><h1 id=creatingdeleting-machines-vm>Creating/Deleting machines (VM)</h1><ul><li><a href=#creatingdeleting-machines-vm>Creating/Deleting machines (VM)</a><ul><li><a href=#setting-up-your-usage-environment>Setting up your usage environment</a></li><li><a href=#important>Important :</a></li><li><a href=#creating-machine>Creating machine</a></li><li><a href=#inspect-status-of-machine>Inspect status of machine</a></li><li><a href=#delete-machine>Delete machine</a></li></ul></li></ul><h2 id=setting-up-your-usage-environment>Setting up your usage environment</h2><ul><li>Follow the <a href=/docs/other-components/machine-controller-manager/operations/prerequisite/>steps described here</a></li></ul><h2 id=important->Important :</h2><blockquote><p>Make sure that the <code>kubernetes/machine_objects/machine.yaml</code> points to the same class name as the <code>kubernetes/machine_classes/aws-machine-class.yaml</code>.</p></blockquote><blockquote><p>Similarly <code>kubernetes/machine_objects/aws-machine-class.yaml</code> secret name and namespace should be same as that mentioned in <code>kubernetes/secrets/aws-secret.yaml</code></p></blockquote><h2 id=creating-machine>Creating machine</h2><ul><li>Modify <code>kubernetes/machine_objects/machine.yaml</code> as per your requirement and create the VM as shown below:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/machine_objects/machine.yaml
</span></span></code></pre></div><p>You should notice that the Machine Controller Manager has immediately picked up your manifest and started to create a new machine by talking to the cloud provider.</p><ul><li>Check Machine Controller Manager machines in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machine
</span></span><span style=display:flex><span>NAME           STATUS    AGE
</span></span><span style=display:flex><span>test-machine   Running   5m
</span></span></code></pre></div><p>A new machine is created with the name provided in the <code>kubernetes/machine_objects/machine.yaml</code> file.</p><ul><li>After a few minutes (~3 minutes for AWS), you should notice a new node joining the cluster. You can verify this by running:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get nodes
</span></span><span style=display:flex><span>NAME                                         STATUS     AGE     VERSION
</span></span><span style=display:flex><span>ip-10-250-14-52.eu-east-1.compute.internal.  Ready      1m      v1.8.0
</span></span></code></pre></div><p>This shows that a new node has successfully joined the cluster.</p><h2 id=inspect-status-of-machine>Inspect status of machine</h2><p>To inspect the status of any created machine, run the command given below.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machine test-machine -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: machine.sapcloud.io/v1alpha1
</span></span><span style=display:flex><span>kind: Machine
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    kubectl.kubernetes.io/last-applied-configuration: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      </span>      {<span style=color:#a31515>&#34;apiVersion&#34;</span>:<span style=color:#a31515>&#34;machine.sapcloud.io/v1alpha1&#34;</span>,<span style=color:#a31515>&#34;kind&#34;</span>:<span style=color:#a31515>&#34;Machine&#34;</span>,<span style=color:#a31515>&#34;metadata&#34;</span>:{<span style=color:#a31515>&#34;annotations&#34;</span>:{},<span style=color:#a31515>&#34;labels&#34;</span>:{<span style=color:#a31515>&#34;test-label&#34;</span>:<span style=color:#a31515>&#34;test-label&#34;</span>},<span style=color:#a31515>&#34;name&#34;</span>:<span style=color:#a31515>&#34;test-machine&#34;</span>,<span style=color:#a31515>&#34;namespace&#34;</span>:<span style=color:#a31515>&#34;&#34;</span>},<span style=color:#a31515>&#34;spec&#34;</span>:{<span style=color:#a31515>&#34;class&#34;</span>:{<span style=color:#a31515>&#34;kind&#34;</span>:<span style=color:#a31515>&#34;AWSMachineClass&#34;</span>,<span style=color:#a31515>&#34;name&#34;</span>:<span style=color:#a31515>&#34;test-aws&#34;</span>}}}
</span></span><span style=display:flex><span>  clusterName: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  creationTimestamp: 2017-12-27T06:58:21Z
</span></span><span style=display:flex><span>  finalizers:
</span></span><span style=display:flex><span>  - machine.sapcloud.io/operator
</span></span><span style=display:flex><span>  generation: 0
</span></span><span style=display:flex><span>  initializers: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    node: ip-10-250-14-52.eu-east-1.compute.internal
</span></span><span style=display:flex><span>    test-label: test-label
</span></span><span style=display:flex><span>  name: test-machine
</span></span><span style=display:flex><span>  namespace: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  resourceVersion: <span style=color:#a31515>&#34;12616948&#34;</span>
</span></span><span style=display:flex><span>  selfLink: /apis/machine.sapcloud.io/v1alpha1/test-machine
</span></span><span style=display:flex><span>  uid: 535e596c-ead3-11e7-a6c0-828f843e4186
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  class:
</span></span><span style=display:flex><span>    kind: AWSMachineClass
</span></span><span style=display:flex><span>    name: test-aws
</span></span><span style=display:flex><span>  providerID: aws:///eu-east-1/i-00bef3f2618ffef23
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - lastHeartbeatTime: 2017-12-27T07:00:46Z
</span></span><span style=display:flex><span>    lastTransitionTime: 2017-12-27T06:59:16Z
</span></span><span style=display:flex><span>    message: kubelet has sufficient disk space available
</span></span><span style=display:flex><span>    reason: KubeletHasSufficientDisk
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;False&#34;</span>
</span></span><span style=display:flex><span>    type: OutOfDisk
</span></span><span style=display:flex><span>  - lastHeartbeatTime: 2017-12-27T07:00:46Z
</span></span><span style=display:flex><span>    lastTransitionTime: 2017-12-27T06:59:16Z
</span></span><span style=display:flex><span>    message: kubelet has sufficient memory available
</span></span><span style=display:flex><span>    reason: KubeletHasSufficientMemory
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;False&#34;</span>
</span></span><span style=display:flex><span>    type: MemoryPressure
</span></span><span style=display:flex><span>  - lastHeartbeatTime: 2017-12-27T07:00:46Z
</span></span><span style=display:flex><span>    lastTransitionTime: 2017-12-27T06:59:16Z
</span></span><span style=display:flex><span>    message: kubelet has no disk pressure
</span></span><span style=display:flex><span>    reason: KubeletHasNoDiskPressure
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;False&#34;</span>
</span></span><span style=display:flex><span>    type: DiskPressure
</span></span><span style=display:flex><span>  - lastHeartbeatTime: 2017-12-27T07:00:46Z
</span></span><span style=display:flex><span>    lastTransitionTime: 2017-12-27T07:00:06Z
</span></span><span style=display:flex><span>    message: kubelet is posting ready status
</span></span><span style=display:flex><span>    reason: KubeletReady
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>    type: Ready
</span></span><span style=display:flex><span>  currentStatus:
</span></span><span style=display:flex><span>    lastUpdateTime: 2017-12-27T07:00:06Z
</span></span><span style=display:flex><span>    phase: Running
</span></span><span style=display:flex><span>  lastOperation:
</span></span><span style=display:flex><span>    description: Machine is now ready
</span></span><span style=display:flex><span>    lastUpdateTime: 2017-12-27T07:00:06Z
</span></span><span style=display:flex><span>    state: Successful
</span></span><span style=display:flex><span>    type: Create
</span></span><span style=display:flex><span>  node: ip-10-250-14-52.eu-west-1.compute.internal
</span></span></code></pre></div><h2 id=delete-machine>Delete machine</h2><p>To delete the VM using the <code>kubernetes/machine_objects/machine.yaml</code> as shown below</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl delete -f kubernetes/machine_objects/machine.yaml
</span></span></code></pre></div><p>Now the Machine Controller Manager picks up the manifest immediately and starts to delete the existing VM by talking to the cloud provider. The node should be detached from the cluster in a few minutes (~1min for AWS).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-da88ac1c86384ffd93401f00ca638fde>1.3.3 - Machine Deployment</h1><h1 id=maintaining-machine-replicas-using-machines-deployments>Maintaining machine replicas using machines-deployments</h1><ul><li><a href=#maintaining-machine-replicas-using-machines-deployments>Maintaining machine replicas using machines-deployments</a><ul><li><a href=#setting-up-your-usage-environment>Setting up your usage environment</a><ul><li><a href=#important-warning>Important ⚠️</a></li></ul></li><li><a href=#creating-machine-deployment>Creating machine-deployment</a></li><li><a href=#inspect-status-of-machine-deployment>Inspect status of machine-deployment</a></li><li><a href=#health-monitoring>Health monitoring</a></li><li><a href=#update-your-machines>Update your machines</a><ul><li><a href=#inspect-existing-cluster-configuration>Inspect existing cluster configuration</a></li><li><a href=#perform-a-rolling-update>Perform a rolling update</a></li><li><a href=#re-check-cluster-configuration>Re-check cluster configuration</a></li><li><a href=#more-variants-of-updates>More variants of updates</a></li></ul></li><li><a href=#undo-an-update>Undo an update</a></li><li><a href=#pause-an-update>Pause an update</a></li><li><a href=#delete-machine-deployment>Delete machine-deployment</a></li></ul></li></ul><h2 id=setting-up-your-usage-environment>Setting up your usage environment</h2><p>Follow the <a href=/docs/other-components/machine-controller-manager/operations/prerequisite/>steps described here</a></p><h3 id=important->Important ⚠️</h3><blockquote><p>Make sure that the <code>kubernetes/machine_objects/machine-deployment.yaml</code> points to the same class name as the <code>kubernetes/machine_classes/aws-machine-class.yaml</code>.</p></blockquote><blockquote><p>Similarly <code>kubernetes/machine_classes/aws-machine-class.yaml</code> secret name and namespace should be same as that mentioned in <code>kubernetes/secrets/aws-secret.yaml</code></p></blockquote><h2 id=creating-machine-deployment>Creating machine-deployment</h2><ul><li>Modify <code>kubernetes/machine_objects/machine-deployment.yaml</code> as per your requirement. Modify the number of replicas to the desired number of machines. Then, create an machine-deployment.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/machine_objects/machine-deployment.yaml
</span></span></code></pre></div><p>Now the Machine Controller Manager picks up the manifest immediately and starts to create a new machines based on the number of replicas you have provided in the manifest.</p><ul><li>Check Machine Controller Manager machine-deployments in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machinedeployment
</span></span><span style=display:flex><span>NAME                      READY   DESIRED   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style=display:flex><span>test-machine-deployment   3       3         3            0           10m
</span></span></code></pre></div><p>You will notice a new machine-deployment with your given name</p><ul><li>Check Machine Controller Manager machine-sets in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset
</span></span><span style=display:flex><span>NAME                                 DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f   3         3         0       10m
</span></span></code></pre></div><p>You will notice a new machine-set backing your machine-deployment</p><ul><li>Check Machine Controller Manager machines in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machine
</span></span><span style=display:flex><span>NAME                                       STATUS    AGE
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f-5d24b   Pending   5m
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f-6mpn4   Pending   5m
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f-dpt2q   Pending   5m
</span></span></code></pre></div><p>Now you will notice N (number of replicas specified in the manifest) new machines whose name are prefixed with the machine-deployment object name that you created.</p><ul><li>After a few minutes (~3 minutes for AWS), you would see that new nodes have joined the cluster. You can see this using</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$  kubectl get nodes
</span></span><span style=display:flex><span>NAME                                          STATUS    AGE       VERSION
</span></span><span style=display:flex><span>ip-10-250-20-19.eu-west-1.compute.internal    Ready     1m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-27-123.eu-west-1.compute.internal   Ready     1m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-31-80.eu-west-1.compute.internal    Ready     1m        v1.8.0
</span></span></code></pre></div><p>This shows how new nodes have joined your cluster</p><h2 id=inspect-status-of-machine-deployment>Inspect status of machine-deployment</h2><p>To inspect the status of any created machine-deployment run the command below,</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machinedeployment test-machine-deployment -o yaml
</span></span></code></pre></div><p>You should get the following output.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: machine.sapcloud.io/v1alpha1
</span></span><span style=display:flex><span>kind: MachineDeployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    deployment.kubernetes.io/revision: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>    kubectl.kubernetes.io/last-applied-configuration: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      </span>      {<span style=color:#a31515>&#34;apiVersion&#34;</span>:<span style=color:#a31515>&#34;machine.sapcloud.io/v1alpha1&#34;</span>,<span style=color:#a31515>&#34;kind&#34;</span>:<span style=color:#a31515>&#34;MachineDeployment&#34;</span>,<span style=color:#a31515>&#34;metadata&#34;</span>:{<span style=color:#a31515>&#34;annotations&#34;</span>:{},<span style=color:#a31515>&#34;name&#34;</span>:<span style=color:#a31515>&#34;test-machine-deployment&#34;</span>,<span style=color:#a31515>&#34;namespace&#34;</span>:<span style=color:#a31515>&#34;&#34;</span>},<span style=color:#a31515>&#34;spec&#34;</span>:{<span style=color:#a31515>&#34;minReadySeconds&#34;</span>:200,<span style=color:#a31515>&#34;replicas&#34;</span>:3,<span style=color:#a31515>&#34;selector&#34;</span>:{<span style=color:#a31515>&#34;matchLabels&#34;</span>:{<span style=color:#a31515>&#34;test-label&#34;</span>:<span style=color:#a31515>&#34;test-label&#34;</span>}},<span style=color:#a31515>&#34;strategy&#34;</span>:{<span style=color:#a31515>&#34;rollingUpdate&#34;</span>:{<span style=color:#a31515>&#34;maxSurge&#34;</span>:1,<span style=color:#a31515>&#34;maxUnavailable&#34;</span>:1},<span style=color:#a31515>&#34;type&#34;</span>:<span style=color:#a31515>&#34;RollingUpdate&#34;</span>},<span style=color:#a31515>&#34;template&#34;</span>:{<span style=color:#a31515>&#34;metadata&#34;</span>:{<span style=color:#a31515>&#34;labels&#34;</span>:{<span style=color:#a31515>&#34;test-label&#34;</span>:<span style=color:#a31515>&#34;test-label&#34;</span>}},<span style=color:#a31515>&#34;spec&#34;</span>:{<span style=color:#a31515>&#34;class&#34;</span>:{<span style=color:#a31515>&#34;kind&#34;</span>:<span style=color:#a31515>&#34;AWSMachineClass&#34;</span>,<span style=color:#a31515>&#34;name&#34;</span>:<span style=color:#a31515>&#34;test-aws&#34;</span>}}}}}
</span></span><span style=display:flex><span>  clusterName: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  creationTimestamp: 2017-12-27T08:55:56Z
</span></span><span style=display:flex><span>  generation: 0
</span></span><span style=display:flex><span>  initializers: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>  name: test-machine-deployment
</span></span><span style=display:flex><span>  namespace: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  resourceVersion: <span style=color:#a31515>&#34;12634168&#34;</span>
</span></span><span style=display:flex><span>  selfLink: /apis/machine.sapcloud.io/v1alpha1/test-machine-deployment
</span></span><span style=display:flex><span>  uid: c0b488f7-eae3-11e7-a6c0-828f843e4186
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  minReadySeconds: 200
</span></span><span style=display:flex><span>  replicas: 3
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      test-label: test-label
</span></span><span style=display:flex><span>  strategy:
</span></span><span style=display:flex><span>    rollingUpdate:
</span></span><span style=display:flex><span>      maxSurge: 1
</span></span><span style=display:flex><span>      maxUnavailable: 1
</span></span><span style=display:flex><span>    type: RollingUpdate
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      creationTimestamp: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        test-label: test-label
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      class:
</span></span><span style=display:flex><span>        kind: AWSMachineClass
</span></span><span style=display:flex><span>        name: test-aws
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  availableReplicas: 3
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - lastTransitionTime: 2017-12-27T08:57:22Z
</span></span><span style=display:flex><span>    lastUpdateTime: 2017-12-27T08:57:22Z
</span></span><span style=display:flex><span>    message: Deployment has minimum availability.
</span></span><span style=display:flex><span>    reason: MinimumReplicasAvailable
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>    type: Available
</span></span><span style=display:flex><span>  readyReplicas: 3
</span></span><span style=display:flex><span>  replicas: 3
</span></span><span style=display:flex><span>  updatedReplicas: 3
</span></span></code></pre></div><h2 id=health-monitoring>Health monitoring</h2><p>Health monitor is also applied similar to how it&rsquo;s described for <a href=/docs/other-components/machine-controller-manager/operations/machine_set/>machine-sets</a></p><h2 id=update-your-machines>Update your machines</h2><p>Let us consider the scenario where you wish to update all nodes of your cluster from t2.xlarge machines to m5.xlarge machines. Assume that your current <em>test-aws</em> has its <strong>spec.machineType: t2.xlarge</strong> and your deployment <em>test-machine-deployment</em> points to this AWSMachineClass.</p><h4 id=inspect-existing-cluster-configuration>Inspect existing cluster configuration</h4><ul><li>Check Nodes present in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get nodes
</span></span><span style=display:flex><span>NAME                                          STATUS    AGE       VERSION
</span></span><span style=display:flex><span>ip-10-250-20-19.eu-west-1.compute.internal    Ready     1m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-27-123.eu-west-1.compute.internal   Ready     1m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-31-80.eu-west-1.compute.internal    Ready     1m        v1.8.0
</span></span></code></pre></div><ul><li>Check Machine Controller Manager machine-sets in the cluster. You will notice one machine-set backing your machine-deployment</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset
</span></span><span style=display:flex><span>NAME                                 DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f   3         3         3       10m
</span></span></code></pre></div><ul><li>Login to your cloud provider (AWS). In the VM management console, you will find N VMs created of type t2.xlarge.</li></ul><h4 id=perform-a-rolling-update>Perform a rolling update</h4><p>To update this machine-deployment VMs to <code>m5.xlarge</code>, we would do the following:</p><ul><li>Copy your existing aws-machine-class.yaml</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cp kubernetes/machine_classes/aws-machine-class.yaml kubernetes/machine_classes/aws-machine-class-new.yaml
</span></span></code></pre></div><ul><li>Modify aws-machine-class-new.yaml, and update its <em>metadata.name: test-aws2</em> and <em>spec.machineType: m5.xlarge</em></li><li>Now create this modified MachineClass</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/machine_classes/aws-machine-class-new.yaml
</span></span></code></pre></div><ul><li>Edit your existing machine-deployment</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl edit machinedeployment test-machine-deployment
</span></span></code></pre></div><ul><li>Update from <em>spec.template.spec.class.name: test-aws</em> to <em>spec.template.spec.class.name: test-aws2</em></li></ul><h4 id=re-check-cluster-configuration>Re-check cluster configuration</h4><p>After a few minutes (~3mins)</p><ul><li>Check nodes present in cluster now. They are different nodes.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get nodes
</span></span><span style=display:flex><span>NAME                                          STATUS    AGE       VERSION
</span></span><span style=display:flex><span>ip-10-250-11-171.eu-west-1.compute.internal   Ready     4m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-17-213.eu-west-1.compute.internal   Ready     5m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-31-81.eu-west-1.compute.internal    Ready     5m        v1.8.0
</span></span></code></pre></div><ul><li>Check Machine Controller Manager machine-sets in the cluster. You will notice two machine-sets backing your machine-deployment</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset
</span></span><span style=display:flex><span>NAME                                 DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f   0         0         0       1h
</span></span><span style=display:flex><span>test-machine-deployment-86ff45cc5    3         3         3       20m
</span></span></code></pre></div><ul><li>Login to your cloud provider (AWS). In the VM management console, you will find N VMs created of type t2.xlarge in terminated state, and N new VMs of type m5.xlarge in running state.</li></ul><p>This shows how a rolling update of a cluster from nodes with t2.xlarge to m5.xlarge went through.</p><h4 id=more-variants-of-updates>More variants of updates</h4><ul><li>The above demonstration was a simple use case. This could be more complex like - updating the system disk image versions/ kubelet versions/ security patches etc.</li><li>You can also play around with the maxSurge and maxUnavailable fields in machine-deployment.yaml</li><li>You can also change the update strategy from rollingupdate to recreate</li></ul><h2 id=undo-an-update>Undo an update</h2><ul><li>Edit the existing machine-deployment</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl edit machinedeployment test-machine-deployment
</span></span></code></pre></div><ul><li>Edit the deployment to have this new field of <em>spec.rollbackTo.revision: 0</em> as shown as comments in <code>kubernetes/machine_objects/machine-deployment.yaml</code></li><li>This will undo your update to the previous version.</li></ul><h2 id=pause-an-update>Pause an update</h2><ul><li>You can also pause the update while update is going on by editing the existing machine-deployment</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl edit machinedeployment test-machine-deployment
</span></span></code></pre></div><ul><li><p>Edit the deployment to have this new field of <em>spec.paused: true</em> as shown as comments in <code>kubernetes/machine_objects/machine-deployment.yaml</code></p></li><li><p>This will pause the rollingUpdate if it&rsquo;s in process</p></li><li><p>To resume the update, edit the deployment as mentioned above and remove the field <em>spec.paused: true</em> updated earlier</p></li></ul><h2 id=delete-machine-deployment>Delete machine-deployment</h2><ul><li>To delete the VM using the <code>kubernetes/machine_objects/machine-deployment.yaml</code></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl delete -f kubernetes/machine_objects/machine-deployment.yaml
</span></span></code></pre></div><p>The Machine Controller Manager picks up the manifest and starts to delete the existing VMs by talking to the cloud provider. The nodes should be detached from the cluster in a few minutes (~1min for AWS).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-42199c1438c8f192eebbcdc0c8efa41c>1.3.4 - Machine Set</h1><h1 id=maintaining-machine-replicas-using-machines-sets>Maintaining machine replicas using machines-sets</h1><ul><li><a href=#maintaining-machine-replicas-using-machines-sets>Maintaining machine replicas using machines-sets</a><ul><li><a href=#setting-up-your-usage-environment>Setting up your usage environment</a></li><li><a href=#important-warning>Important ⚠️</a></li><li><a href=#creating-machine-set>Creating machine-set</a></li><li><a href=#inspect-status-of-machine-set>Inspect status of machine-set</a></li><li><a href=#health-monitoring>Health monitoring</a></li><li><a href=#delete-machine-set>Delete machine-set</a></li></ul></li></ul><h2 id=setting-up-your-usage-environment>Setting up your usage environment</h2><ul><li>Follow the <a href=/docs/other-components/machine-controller-manager/operations/prerequisite/>steps described here</a></li></ul><h2 id=important->Important ⚠️</h2><blockquote><p>Make sure that the <code>kubernetes/machines_objects/machine-set.yaml</code> points to the same class name as the <code>kubernetes/machine_classes/aws-machine-class.yaml</code>.</p></blockquote><blockquote><p>Similarly <code>kubernetes/machine_classes/aws-machine-class.yaml</code> secret name and namespace should be same as that mentioned in <code>kubernetes/secrets/aws-secret.yaml</code></p></blockquote><h2 id=creating-machine-set>Creating machine-set</h2><ul><li>Modify <code>kubernetes/machine_objects/machine-set.yaml</code> as per your requirement. You can modify the number of replicas to the desired number of machines. Then, create an machine-set:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/machine_objects/machine-set.yaml
</span></span></code></pre></div><p>You should notice that the Machine Controller Manager has immediately picked up your manifest and started to create a new machines based on the number of replicas you have provided in the manifest.</p><ul><li>Check Machine Controller Manager machine-sets in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset
</span></span><span style=display:flex><span>NAME               DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>test-machine-set   3         3         0       1m
</span></span></code></pre></div><p>You will see a new machine-set with your given name</p><ul><li>Check Machine Controller Manager machines in the cluster:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machine
</span></span><span style=display:flex><span>NAME                     STATUS    AGE
</span></span><span style=display:flex><span>test-machine-set-b57zs   Pending   5m
</span></span><span style=display:flex><span>test-machine-set-c4bg8   Pending   5m
</span></span><span style=display:flex><span>test-machine-set-kvskg   Pending   5m
</span></span></code></pre></div><p>Now you will see N (number of replicas specified in the manifest) new machines whose names are prefixed with the machine-set object name that you created.</p><ul><li>After a few minutes (~3 minutes for AWS), you should notice new nodes joining the cluster. You can verify this by running:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get nodes
</span></span><span style=display:flex><span>NAME                                         STATUS    AGE       VERSION
</span></span><span style=display:flex><span>ip-10-250-0-234.eu-west-1.compute.internal   Ready     3m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-15-98.eu-west-1.compute.internal   Ready     3m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-6-21.eu-west-1.compute.internal    Ready     2m        v1.8.0
</span></span></code></pre></div><p>This shows how new nodes have joined your cluster</p><h2 id=inspect-status-of-machine-set>Inspect status of machine-set</h2><ul><li>To inspect the status of any created machine-set run the following command:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset test-machine-set -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: machine.sapcloud.io/v1alpha1
</span></span><span style=display:flex><span>kind: MachineSet
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    kubectl.kubernetes.io/last-applied-configuration: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      </span>      {<span style=color:#a31515>&#34;apiVersion&#34;</span>:<span style=color:#a31515>&#34;machine.sapcloud.io/v1alpha1&#34;</span>,<span style=color:#a31515>&#34;kind&#34;</span>:<span style=color:#a31515>&#34;MachineSet&#34;</span>,<span style=color:#a31515>&#34;metadata&#34;</span>:{<span style=color:#a31515>&#34;annotations&#34;</span>:{},<span style=color:#a31515>&#34;name&#34;</span>:<span style=color:#a31515>&#34;test-machine-set&#34;</span>,<span style=color:#a31515>&#34;namespace&#34;</span>:<span style=color:#a31515>&#34;&#34;</span>,<span style=color:#a31515>&#34;test-label&#34;</span>:<span style=color:#a31515>&#34;test-label&#34;</span>},<span style=color:#a31515>&#34;spec&#34;</span>:{<span style=color:#a31515>&#34;minReadySeconds&#34;</span>:200,<span style=color:#a31515>&#34;replicas&#34;</span>:3,<span style=color:#a31515>&#34;selector&#34;</span>:{<span style=color:#a31515>&#34;matchLabels&#34;</span>:{<span style=color:#a31515>&#34;test-label&#34;</span>:<span style=color:#a31515>&#34;test-label&#34;</span>}},<span style=color:#a31515>&#34;template&#34;</span>:{<span style=color:#a31515>&#34;metadata&#34;</span>:{<span style=color:#a31515>&#34;labels&#34;</span>:{<span style=color:#a31515>&#34;test-label&#34;</span>:<span style=color:#a31515>&#34;test-label&#34;</span>}},<span style=color:#a31515>&#34;spec&#34;</span>:{<span style=color:#a31515>&#34;class&#34;</span>:{<span style=color:#a31515>&#34;kind&#34;</span>:<span style=color:#a31515>&#34;AWSMachineClass&#34;</span>,<span style=color:#a31515>&#34;name&#34;</span>:<span style=color:#a31515>&#34;test-aws&#34;</span>}}}}}
</span></span><span style=display:flex><span>  clusterName: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  creationTimestamp: 2017-12-27T08:37:42Z
</span></span><span style=display:flex><span>  finalizers:
</span></span><span style=display:flex><span>  - machine.sapcloud.io/operator
</span></span><span style=display:flex><span>  generation: 0
</span></span><span style=display:flex><span>  initializers: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>  name: test-machine-set
</span></span><span style=display:flex><span>  namespace: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  resourceVersion: <span style=color:#a31515>&#34;12630893&#34;</span>
</span></span><span style=display:flex><span>  selfLink: /apis/machine.sapcloud.io/v1alpha1/test-machine-set
</span></span><span style=display:flex><span>  uid: 3469faaa-eae1-11e7-a6c0-828f843e4186
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  machineClass: {}
</span></span><span style=display:flex><span>  minReadySeconds: 200
</span></span><span style=display:flex><span>  replicas: 3
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      test-label: test-label
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      creationTimestamp: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        test-label: test-label
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      class:
</span></span><span style=display:flex><span>        kind: AWSMachineClass
</span></span><span style=display:flex><span>        name: test-aws
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  availableReplicas: 3
</span></span><span style=display:flex><span>  fullyLabeledReplicas: 3
</span></span><span style=display:flex><span>  machineSetCondition: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>  lastOperation:
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>  observedGeneration: 0
</span></span><span style=display:flex><span>  readyReplicas: 3
</span></span><span style=display:flex><span>  replicas: 3
</span></span></code></pre></div><h2 id=health-monitoring>Health monitoring</h2><ul><li>If you try to delete/terminate any of the machines backing the machine-set by either talking to the Machine Controller Manager or from the cloud provider, the Machine Controller Manager recreates a matching healthy machine to replace the deleted machine.</li><li>Similarly, if any of your machines are unreachable or in an unhealthy state (kubelet not ready / disk pressure) for longer than the configured timeout (~ 5mins), the Machine Controller Manager recreates the nodes to replace the unhealthy nodes.</li></ul><h2 id=delete-machine-set>Delete machine-set</h2><ul><li>To delete the VM using the <code>kubernetes/machine_objects/machine-set.yaml</code>:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl delete -f kubernetes/machine-set.yaml
</span></span></code></pre></div><p>Now the Machine Controller Manager has immediately picked up your manifest and started to delete the existing VMs by talking to the cloud provider. Your nodes should be detached from the cluster in a few minutes (~1min for AWS).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-332d14c64a273da7528fad02b8806e65>1.3.5 - Prerequisite</h1><h1 id=setting-up-the-usage-environment>Setting up the usage environment</h1><ul><li><a href=#setting-up-the-usage-environment>Setting up the usage environment</a><ul><li><a href=#important-warning>Important ⚠️</a></li><li><a href=#set-kubeconfig>Set KUBECONFIG</a></li><li><a href=#replace-provider-credentials-and-desired-vm-configurations>Replace provider credentials and desired VM configurations</a></li><li><a href=#deploy-required-crds-and-objects>Deploy required CRDs and Objects</a></li><li><a href=#check-current-cluster-state>Check current cluster state</a></li></ul></li></ul><h2 id=important->Important ⚠️</h2><blockquote><p>All paths are relative to the root location of this project repository.</p></blockquote><blockquote><p>Run the Machine Controller Manager either as described in <a href=/docs/other-components/machine-controller-manager/development/local_setup/>Setting up a local development environment</a> or <a href=/docs/other-components/machine-controller-manager/operations/deployment/>Deploying the Machine Controller Manager into a Kubernetes cluster</a>.</p></blockquote><blockquote><p>Make sure that the following steps are run before managing machines/ machine-sets/ machine-deploys.</p></blockquote><h2 id=set-kubeconfig>Set KUBECONFIG</h2><p>Using the existing <a href=https://kubernetes.io/docs/tasks/access-application-cluster/authenticate-across-clusters-kubeconfig/>Kubeconfig</a>, open another Terminal panel/window with the <code>KUBECONFIG</code> environment variable pointing to this Kubeconfig file as shown below,</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ export KUBECONFIG=&lt;PATH_TO_REPO&gt;/dev/kubeconfig.yaml
</span></span></code></pre></div><h2 id=replace-provider-credentials-and-desired-vm-configurations>Replace provider credentials and desired VM configurations</h2><p>Open <code>kubernetes/machine_classes/aws-machine-class.yaml</code> and replace required values there with the desired VM configurations.</p><p>Similarily open <code>kubernetes/secrets/aws-secret.yaml</code> and replace - <em>userData, providerAccessKeyId, providerSecretAccessKey</em> with base64 encoded values of cloudconfig file, AWS access key id, and AWS secret access key respectively. Use the following command to get the base64 encoded value of your details</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ echo <span style=color:#a31515>&#34;sample-cloud-config&#34;</span> | base64
</span></span><span style=display:flex><span>base64-encoded-cloud-config
</span></span></code></pre></div><p>Do the same for your access key id and secret access key.</p><h2 id=deploy-required-crds-and-objects>Deploy required CRDs and Objects</h2><p>Create all the required CRDs in the cluster using <code>kubernetes/crds.yaml</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/crds.yaml
</span></span></code></pre></div><p>Create the class template that will be used as an machine template to create VMs using <code>kubernetes/machine_classes/aws-machine-class.yaml</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/machine_classes/aws-machine-class.yaml
</span></span></code></pre></div><p>Create the secret used for the cloud credentials and cloudconfig using <code>kubernetes/secrets/aws-secret.yaml</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/secrets/aws-secret.yaml
</span></span></code></pre></div><h2 id=check-current-cluster-state>Check current cluster state</h2><p>Get to know the current cluster state using the following commands,</p><ul><li>Checking aws-machine-class in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get awsmachineclass
</span></span><span style=display:flex><span>NAME       MACHINE TYPE   AMI          AGE
</span></span><span style=display:flex><span>test-aws   t2.large       ami-123456   5m
</span></span></code></pre></div><ul><li>Checking kubernetes secrets in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get secret
</span></span><span style=display:flex><span>NAME                  TYPE                                  DATA      AGE
</span></span><span style=display:flex><span>test-secret           Opaque                                3         21h
</span></span></code></pre></div><ul><li>Checking kubernetes nodes in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get nodes
</span></span></code></pre></div><p>Lists the default set of nodes attached to your cluster</p><ul><li>Checking Machine Controller Manager machines in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machine
</span></span><span style=display:flex><span>No resources found.
</span></span></code></pre></div><ul><li>Checking Machine Controller Manager machine-sets in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset
</span></span><span style=display:flex><span>No resources found.
</span></span></code></pre></div><ul><li>Checking Machine Controller Manager machine-deploys in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machinedeployment
</span></span><span style=display:flex><span>No resources found.
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-9c506ae4839d6c04412db7f05a0e4d2a>1.4 - Proposals</h1></div><div class=td-content><h1 id=pg-62e1d100b3bccabcb90bdb26a65462a9>1.4.1 - Excess Reserve Capacity</h1><h1 id=excess-reserve-capacity>Excess Reserve Capacity</h1><ul><li><a href=#excess-reserve-capacity>Excess Reserve Capacity</a><ul><li><a href=#goal>Goal</a></li><li><a href=#note>Note</a></li><li><a href=#possible-approaches>Possible Approaches</a><ul><li><a href=#approach-1-enhance-machine-controller-manager-to-also-entertain-the-excess-machines>Approach 1: Enhance Machine-controller-manager to also entertain the excess machines</a></li><li><a href=#approach-2-enhance-cluster-autoscaler-by-simulating-fake-pods-in-it>Approach 2: Enhance Cluster-autoscaler by simulating fake pods in it</a></li><li><a href=#approach-3-enhance-cluster-autoscaler-to-support-pluggable-scaling-events>Approach 3: Enhance cluster-autoscaler to support pluggable scaling-events</a></li><li><a href=#approach-4-make-intelligent-use-of-low-priority-pods>Approach 4: Make intelligent use of Low-priority pods</a></li></ul></li></ul></li></ul><h2 id=goal>Goal</h2><p>Currently, autoscaler optimizes the number of machines for a given application-workload. Along with effective resource utilization, this feature brings concern where, many times, when new application instances are created - they don&rsquo;t find space in existing cluster. This leads the cluster-autoscaler to create new machines via MachineDeployment, which can take from 3-4 minutes to ~10 minutes, for the machine to really come-up and join the cluster. In turn, application-instances have to wait till new machines join the cluster.</p><p>One of the promising solutions to this issue is Excess Reserve Capacity. Idea is to keep a certain number of machines or percent of resources[cpu/memory] always available, so that new workload, in general, can be scheduled immediately unless huge spike in the workload. Also, the user should be given enough flexibility to choose how many resources or how many machines should be kept alive and non-utilized as this affects the Cost directly.</p><h2 id=note>Note</h2><ul><li>We decided to go with Approach-4 which is based on low priority pods. Please find more details here: <a href=https://github.com/gardener/gardener/issues/254>https://github.com/gardener/gardener/issues/254</a></li><li>Approach-3 looks more promising in long term, we may decide to adopt that in future based on developments/contributions in autoscaler-community.</li></ul><h2 id=possible-approaches>Possible Approaches</h2><p>Following are the possible approaches, we could think of so far.</p><h3 id=approach-1-enhance-machine-controller-manager-to-also-entertain-the-excess-machines>Approach 1: Enhance Machine-controller-manager to also entertain the excess machines</h3><ul><li><p>Machine-controller-manager currently takes care of the machines in the shoot cluster starting from creation-deletion-health check to efficient rolling-update of the machines. From the architecture point of view, MachineSet makes sure that X number of machines are always <strong>running and healthy</strong>. MachineDeployment controller smartly uses this facility to perform rolling-updates.</p></li><li><p>We can expand the scope of MachineDeployment controller to maintain excess number of machines by introducing new parallel independent controller named <em>MachineTaint</em> controller. This will result in MCM to include Machine, MachineSet, MachineDeployment, MachineSafety, MachineTaint controllers. MachineTaint controller does not need to introduce any new CRD - analogy fits where taint-controller also resides into kube-controller-manager.</p></li><li><p>Only Job of MachineTaint controller will be:</p><ul><li>List all the Machines under each MachineDeployment.</li><li>Maintain taints of <a href=https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/><em>noSchedule</em> and <em>noExecute</em></a> on <code>X</code> latest MachineObjects.</li><li>There should be an event-based informer mechanism where MachineTaintController gets to know about any Update/Delete/Create event of MachineObjects - in turn, maintains the <em>noSchedule</em> and <em>noExecute</em> taints on all the <em>latest</em> machines.
- Why latest machines?
- Whenever autoscaler decides to add new machines - essentially ScaleUp event - taints from the older machines are removed and newer machines get the taints. This way X number of Machines immediately becomes free for new pods to be scheduled.
- While ScaleDown event, autoscaler specifically mentions which machines should be deleted, and that should not bring any concerns. Though we will have to put proper label/annotation defined by autoscaler on taintedMachines, so that autoscaler does not consider the taintedMachines for deletion while scale-down.
* Annotation on tainted node: <code>"cluster-autoscaler.kubernetes.io/scale-down-disabled": "true"</code></li></ul></li><li><p>Implementation Details:</p><ul><li>Expect new <strong>optional field</strong> <em>ExcessReplicas</em> in <code>MachineDeployment.Spec</code>. MachineDeployment controller now adds both <code>Spec.Replicas</code> and <code>Spec.ExcessReplicas</code>[if provided], and considers that as a standard desiredReplicas.
- Current working of MCM will not be affected if ExcessReplicas field is kept nil.</li><li>MachineController currently reads the <em>NodeObject</em> and sets the MachineConditions in MachineObject. Machine-controller will now also read the taints/labels from the MachineObject - and maintains it on the <em>NodeObject</em>.</li></ul></li><li><p>We expect cluster-autoscaler to intelligently make use of the provided feature from MCM.</p><ul><li>CA gets the input of <em>min:max:excess</em> from Gardener. CA continues to set the <code>MachineDeployment.Spec.Replicas</code> as usual based on the application-workload.</li><li>In addition, CA also sets the <code>MachieDeployment.Spec.ExcessReplicas</code> .</li><li>Corner-case:
* CA should decrement the excessReplicas field accordingly when <em>desiredReplicas+excessReplicas</em> on MachineDeployment goes beyond <em>max</em>.</li></ul></li></ul><h3 id=approach-2-enhance-cluster-autoscaler-by-simulating-fake-pods-in-it>Approach 2: Enhance Cluster-autoscaler by simulating fake pods in it</h3><ul><li>There was already an attempt by community to support this feature.<ul><li>Refer for details to: <a href=https://github.com/kubernetes/autoscaler/pull/77/files>https://github.com/kubernetes/autoscaler/pull/77/files</a></li></ul></li></ul><h3 id=approach-3-enhance-cluster-autoscaler-to-support-pluggable-scaling-events>Approach 3: Enhance cluster-autoscaler to support pluggable scaling-events</h3><ul><li>Forked version of cluster-autoscaler could be improved to plug-in the algorithm for excess-reserve capacity.</li><li>Needs further discussion around upstream support.</li><li>Create golang channel to separate the algorithms to trigger scaling (hard-coded in cluster-autoscaler, currently) from the algorithms about how to to achieve the scaling (already pluggable in cluster-autoscaler). This kind of separation can help us introduce/plug-in new algorithms (such as based node resource utilisation) without affecting existing code-base too much while almost completely re-using the code-base for the actual scaling.</li><li>Also this approach is not specific to our fork of cluster-autoscaler. It can be made upstream eventually as well.</li></ul><h3 id=approach-4-make-intelligent-use-of-low-priority-pods>Approach 4: Make intelligent use of Low-priority pods</h3><ul><li>Refer to: <a href=https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/>pod-priority-preemption</a></li><li>TL; DR:<ul><li>High priority pods can preempt the low-priority pods which are already scheduled.</li><li>Pre-create bunch[equivivalent of X shoot-control-planes] of low-priority pods with priority of zero, then start creating the workload pods with better priority which will reschedule the low-priority pods or otherwise keep them in pending state if the limit for max-machines has reached.</li><li>This is still alpha feature.</li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-099b6b715491def6df5afea44f6779f6>1.4.2 - GRPC Based Implementation of Cloud Providers</h1><h1 id=grpc-based-implementation-of-cloud-providers---wip>GRPC based implementation of Cloud Providers - WIP</h1><h2 id=goal>Goal:</h2><p>Currently the Cloud Providers&rsquo; (CP) functionalities ( Create(), Delete(), List() ) are part of the Machine Controller Manager&rsquo;s (MCM)repository. Because of this, adding support for new CPs into MCM requires merging code into MCM which may not be required for core functionalities of MCM itself. Also, for various reasons it may not be feasible for all CPs to merge their code with MCM which is an Open Source project.</p><p>Because of these reasons, it was decided that the CP&rsquo;s code will be moved out in separate repositories so that they can be maintained separately by the respective teams. Idea is to make MCM act as a GRPC server, and CPs as GRPC clients. The CP can register themselves with the MCM using a GRPC service exposed by the MCM. Details of this approach is discussed below.</p><h2 id=how-it-works>How it works:</h2><p>MCM acts as GRPC server and listens on a pre-defined port 5000. It implements below GRPC services. Details of each of these services are mentioned in next section.</p><ul><li><code>Register()</code></li><li><code>GetMachineClass()</code></li><li><code>GetSecret()</code></li></ul><h2 id=grpc-services-exposed-by-mcm>GRPC services exposed by MCM:</h2><h3 id=register>Register()</h3><p><code>rpc Register(stream DriverSide) returns (stream MCMside) {}</code></p><p>The CP GRPC client calls this service to register itself with the MCM. The CP passes the <code>kind</code> and the <code>APIVersion</code> which it implements, and MCM maintains an internal map for all the registered clients. A GRPC stream is returned in response which is kept open througout the life of both the processes. MCM uses this stream to communicate with the client for machine operations: <code>Create()</code>, <code>Delete()</code> or <code>List()</code>.
The CP client is responsible for reading the incoming messages continuously, and based on the <code>operationType</code> parameter embedded in the message, it is supposed to take the required action. This part is already handled in the package <code>grpc/infraclient</code>.
To add a new CP client, import the package, and implement the <code>ExternalDriverProvider</code> interface:</p><pre tabindex=0><code>type ExternalDriverProvider interface {
	Create(machineclass *MachineClassMeta, credentials, machineID, machineName string) (string, string, error)
	Delete(machineclass *MachineClassMeta, credentials, machineID string) error
	List(machineclass *MachineClassMeta, credentials, machineID string) (map[string]string, error)
}
</code></pre><h3 id=getmachineclass>GetMachineClass()</h3><p><code>rpc GetMachineClass(MachineClassMeta) returns (MachineClass) {}</code></p><p>As part of the message from MCM for various machine operations, the name of the machine class is sent instead of the full machine class spec. The CP client is expected to use this GRPC service to get the full spec of the machine class. This optionally enables the client to cache the machine class spec, and make the call only if the machine calass spec is not already cached.</p><h3 id=getsecret>GetSecret()</h3><p><code>rpc GetSecret(SecretMeta) returns (Secret) {}</code></p><p>As part of the message from MCM for various machine operations, the Cloud Config (CC) and CP credentials are not sent. The CP client is expected to use this GRPC service to get the secret which has CC and CP&rsquo;s credentials from MCM. This enables the client to cache the CC and credentials, and to make the call only if the data is not already cached.</p><h2 id=how-to-add-a-new-cloud-providers-support>How to add a new Cloud Provider&rsquo;s support</h2><p>Import the package <code>grpc/infraclient</code> and <code>grpc/infrapb</code> from MCM (currently in MCM&rsquo;s &ldquo;grpc-driver&rdquo; branch)</p><ul><li>Implement the interface <code>ExternalDriverProvider</code><ul><li><code>Create()</code>: Creates a new machine</li><li><code>Delete()</code>: Deletes a machine</li><li><code>List()</code>: Lists machines</li></ul></li><li>Use the interface <code>MachineClassDataProvider</code><ul><li><code>GetMachineClass()</code>: Makes the call to MCM to get machine class spec</li><li><code>GetSecret()</code>: Makes the call to MCM to get secret containing Cloud Config and CP&rsquo;s credentials</li></ul></li></ul><h3 id=example-implementation>Example implementation:</h3><p>Refer GRPC based implementation for AWS client:
<a href=https://github.com/ggaurav10/aws-driver-grpc>https://github.com/ggaurav10/aws-driver-grpc</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-9146991d7ac0a6e2b783c8a7932c0c52>1.4.3 - Hotupdate Instances</h1><h1 id=hot-update-virtualmachine-tags-without-triggering-a-rolling-update>Hot-Update VirtualMachine tags without triggering a rolling-update</h1><ul><li><a href=#hot-update-virtualmachine-tags-without-triggering-a-rolling-update>Hot-Update VirtualMachine tags without triggering a rolling-update</a><ul><li><a href=#motivation>Motivation</a></li><li><a href=#boundary-condition>Boundary Condition</a></li><li><a href=#what-is-available-today>What is available today?</a></li><li><a href=#what-are-the-problems-with-the-current-approach>What are the problems with the current approach?</a><ul><li><a href=#machineclass-update-and-its-impact>MachineClass Update and its impact</a></li></ul></li><li><a href=#proposal>Proposal</a><ul><li><a href=#shoot-yaml-changes>Shoot YAML changes</a></li><li><a href=#provider-specific-workerconfig-api-changes>Provider specific WorkerConfig API changes</a></li><li><a href=#gardener-provider-extension-changes>Gardener provider extension changes</a></li><li><a href=#driver-interface-changes>Driver interface changes</a></li><li><a href=#machine-class-reconciliation>Machine Class reconciliation</a><ul><li><a href=#reconciliation-changes>Reconciliation Changes</a></li></ul></li></ul></li></ul></li></ul><h2 id=motivation>Motivation</h2><ul><li><p><a href=https://github.com/gardener/machine-controller-manager/issues/750>MCM Issue#750</a> There is a requirement to provide a way for consumers to add tags which can be hot-updated onto VMs. This requirement can be generalized to also offer a convenient way to specify tags which can be applied to VMs, NICs, Devices etc.</p></li><li><p><a href=https://github.com/gardener/machine-controller-manager/issues/635>MCM Issue#635</a> which in turn points to <a href=https://github.com/gardener/machine-controller-manager-provider-aws/issues/36#issuecomment-677530395>MCM-Provider-AWS Issue#36</a> - The issue hints at other fields like enable/disable <a href=https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html#EIP_Disable_SrcDestCheck>source/destination checks for NAT instances</a> which needs to be hot-updated on network interfaces.</p></li><li><p>In GCP provider - <code>instance.ServiceAccounts</code> can be updated without the need to roll-over the instance. <a href=https://cloud.google.com/compute/docs/access/service-accounts>See</a></p></li></ul><h2 id=boundary-condition>Boundary Condition</h2><p>All tags that are added via means other than MachineClass.ProviderSpec should be preserved as-is. Only updates done to tags in <code>MachineClass.ProviderSpec</code> should be applied to the infra resources (VM/NIC/Disk).</p><h2 id=what-is-available-today>What is available today?</h2><p>WorkerPool configuration inside <a href=https://github.com/gardener/gardener/blob/fb29d38e6615ed17d409a8271a285254d9dd00ad/example/90-shoot.yaml#L61-L62>shootYaml</a> provides a way to set labels. As per the <a href=https://gardener.cloud/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.Worker>definition</a> these labels will be applied on <code>Node</code> resources. Currently these labels are also passed to the VMs as tags. There is no distinction made between <code>Node</code> labels and <code>VM</code> tags.</p><p><code>MachineClass</code> has a field which holds <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/v1alpha1/machineclass_types.go#L54>provider specific configuration</a> and one such configuration is <code>tags</code>. Gardener provider extensions updates the tags in <code>MachineClass</code>.</p><ul><li>AWS provider extension directly passes the labels to the <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/0a740eeca301320275d77d1c48d3c32d4ebcd7dd/pkg/controller/worker/machines.go#L158-L164>tag section</a> of machineClass.</li><li>Azure provider extension <a href=https://github.com/gardener/gardener-extension-provider-azure/blob/b6424f0122e174863e783555aa0ad68700edd87b/pkg/controller/worker/machines.go#L371-L373>sanitizes</a> the woker pool labels and adds them as <a href=https://github.com/gardener/gardener-extension-provider-azure/blob/b6424f0122e174863e783555aa0ad68700edd87b/pkg/controller/worker/machines.go#L187>tags in MachineClass</a>.</li><li>GCP provider extension <a href=https://github.com/gardener/gardener-extension-provider-gcp/blob/eb851f716e45336b486f3aaf46268859de2adecb/pkg/controller/worker/machines.go#L312-L315>sanitizes</a> them, and then sets them as <a href=https://github.com/gardener/gardener-extension-provider-gcp/blob/eb851f716e45336b486f3aaf46268859de2adecb/pkg/controller/worker/machines.go#L169>labels in the MachineClass</a>. In GCP tags only have keys and are currently <a href=https://github.com/gardener/gardener-extension-provider-gcp/blob/eb851f716e45336b486f3aaf46268859de2adecb/pkg/controller/worker/machines.go#L204-L207>hard coded</a>.</li></ul><p>Let us look at an example of <code>MachineClass.ProviderSpec</code> in AWS:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>providerSpec:
</span></span><span style=display:flex><span>  ami: ami-02fe00c0afb75bbd3
</span></span><span style=display:flex><span>  tags:
</span></span><span style=display:flex><span>    <span style=color:green>#[section-1] pool lables added by gardener extension</span>
</span></span><span style=display:flex><span>    <span style=color:green>#########################################################</span>
</span></span><span style=display:flex><span>    kubernetes.io/arch: amd64
</span></span><span style=display:flex><span>    networking.gardener.cloud/node-local-dns-enabled: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>    node.kubernetes.io/role: node
</span></span><span style=display:flex><span>    worker.garden.sapcloud.io/group: worker-ser234
</span></span><span style=display:flex><span>    worker.gardener.cloud/cri-name: containerd
</span></span><span style=display:flex><span>    worker.gardener.cloud/pool: worker-ser234
</span></span><span style=display:flex><span>    worker.gardener.cloud/system-components: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:green>#[section-2] Tags defined in the gardener-extension-provider-aws</span>
</span></span><span style=display:flex><span>    <span style=color:green>###########################################################</span>
</span></span><span style=display:flex><span>    kubernetes.io/cluster/cluster-full-name: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>    kubernetes.io/role/node: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:green>#[section-3]</span>
</span></span><span style=display:flex><span>    <span style=color:green>###########################################################</span>
</span></span><span style=display:flex><span>    user-defined-key1: user-defined-val1
</span></span><span style=display:flex><span>    user-defined-key2: user-defined-val2
</span></span></code></pre></div><blockquote><p>Refer <a href=https://github.com/gardener/gardener/blob/c11c86ae07d8ea784f5c41362cd41800f06bb3ed/pkg/operation/botanist/component/extensions/worker/worker.go#L171-L197>src</a> for tags defined in <code>section-1</code>.
Refer <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/0a740eeca301320275d77d1c48d3c32d4ebcd7dd/pkg/controller/worker/machines.go#L158-L164>src</a> for tags defined in <code>section-2</code>.
Tags in <code>section-3</code> are defined by the user.</p></blockquote><p>Out of the above three tag categories, MCM depends <code>section-2</code> tags (<code>mandatory-tags</code>) for its <code>orphan collection</code> and Driver&rsquo;s <code>DeleteMachine</code>and <code>GetMachineStatus</code> to work.</p><p><code>ProviderSpec.Tags</code> are transported to the provider specific resources as follows:</p><table><thead><tr><th>Provider</th><th>Resources Tags are set on</th><th>Code Reference</th><th>Comment</th></tr></thead><tbody><tr><td>AWS</td><td>Instance(VM), Volume, Network-Interface</td><td><a href=https://github.com/gardener/machine-controller-manager-provider-aws/blob/v0.17.0/pkg/aws/core.go#L116-L129>aws-VM-Vol-NIC</a></td><td>No distinction is made between tags set on VM, NIC or Volume</td></tr><tr><td>Azure</td><td>Instance(VM), Network-Interface</td><td><a href=https://github.com/gardener/machine-controller-manager-provider-azure/blob/v0.10.0/pkg/azure/utils.go#L234>azure-VM-parameters</a> & <a href=https://github.com/gardener/machine-controller-manager-provider-azure/blob/v0.10.0/pkg/azure/utils.go#L116>azureNIC-Parameters</a></td><td></td></tr><tr><td>GCP</td><td>Instance(VM), 1 tag: <code>name</code> (denoting the name of the worker) is added to Disk</td><td><a href=https://github.com/gardener/machine-controller-manager-provider-gcp/blob/v0.14.0/pkg/gcp/machine_controller_util.go#L78-L80>gcp-VM</a> & <a href=https://github.com/gardener/gardener-extension-provider-gcp/blob/v1.28.1/pkg/controller/worker/machines.go#L291-L293>gcp-Disk</a></td><td>In GCP key-value pairs are called <code>labels</code> while <code>network tags</code> have only keys</td></tr><tr><td>AliCloud</td><td>Instance(VM)</td><td><a href=https://github.com/gardener/machine-controller-manager-provider-alicloud/blob/master/pkg/spi/spi.go#L125-L129>aliCloud-VM</a></td><td></td></tr></tbody></table><h2 id=what-are-the-problems-with-the-current-approach>What are the problems with the current approach?</h2><p>There are a few shortcomings in the way tags/labels are handled:</p><ul><li>Tags can only be set at the time a machine is created.</li><li>There is no distinction made amongst tags/labels that are added to VM&rsquo;s, disks or network interfaces. As stated above for AWS same set of tags are added to all. There is a limit defined on the number of tags/labels that can be associated to the devices (disks, VMs, NICs etc). Example: In AWS a max of <a href=https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html>50 user created tags are allowed</a>. Similar restrictions are applied on different resources across providers. Therefore adding all tags to all devices even if the subset of tags are not meant for that resource exhausts the total allowed tags/labels for that resource.</li><li>The only placeholder in shoot yaml as mentioned above is meant to only hold labels that should be applied on primarily on the <a href=https://github.com/gardener/gardener/blob/v1.66.1/pkg/apis/core/v1beta1/types_shoot.go#L1315-L1317>Node</a> objects. So while you could use the node labels for <a href=https://github.com/gardener/machine-controller-manager/issues/727>extended resources</a>, using it also for tags is not clean.</li><li>There is no provision in the shoot YAML today to add tags only to a subset of resources.</li></ul><h3 id=machineclass-update-and-its-impact>MachineClass Update and its impact</h3><p>When <a href=https://github.com/gardener/gardener/blob/v1.66.1/pkg/apis/core/types_shoot.go#L1042-L1043>Worker.ProviderConfig</a> is changed then a <a href=https://github.com/gardener/gardener/blob/v1.66.1/extensions/pkg/controller/worker/machines.go#L146-L148>worker-hash</a> is computed which includes the raw <code>ProviderConfig</code>. This hash value is then used as a suffix when constructing the name for a <code>MachineClass</code>. See <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/master/pkg/controller/worker/machines.go#L190>aws-extension-provider</a> as an example. A change in the name of the <code>MachineClass</code> will then in-turn trigger a rolling update of machines. Since <code>tags</code> are provider specific and therefore will be part of <code>ProviderConfig</code>, any update to them will result in a rolling-update of machines.</p><h2 id=proposal>Proposal</h2><h3 id=shoot-yaml-changes>Shoot YAML changes</h3><p>Provider specific configuration is set via <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L57-L58>providerConfig</a> section for each worker pool.</p><p>Example worker provider config (current):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>providerConfig:
</span></span><span style=display:flex><span>   apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>   kind: WorkerConfig
</span></span><span style=display:flex><span>   volume:
</span></span><span style=display:flex><span>     iops: 10000
</span></span><span style=display:flex><span>   dataVolumes:
</span></span><span style=display:flex><span>   - name: kubelet-dir
</span></span><span style=display:flex><span>     snapshotID: snap-13234
</span></span><span style=display:flex><span>   iamInstanceProfile: <span style=color:green># (specify either ARN or name)</span>
</span></span><span style=display:flex><span>     name: my-profile
</span></span><span style=display:flex><span>     arn: my-instance-profile-arn
</span></span></code></pre></div><p>It is proposed that an additional field be added for <code>tags</code> under <code>providerConfig</code>. Proposed changed YAML:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>providerConfig:
</span></span><span style=display:flex><span>   apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>   kind: WorkerConfig
</span></span><span style=display:flex><span>   volume:
</span></span><span style=display:flex><span>     iops: 10000
</span></span><span style=display:flex><span>   dataVolumes:
</span></span><span style=display:flex><span>   - name: kubelet-dir
</span></span><span style=display:flex><span>     snapshotID: snap-13234
</span></span><span style=display:flex><span>   iamInstanceProfile: <span style=color:green># (specify either ARN or name)</span>
</span></span><span style=display:flex><span>     name: my-profile
</span></span><span style=display:flex><span>     arn: my-instance-profile-arn
</span></span><span style=display:flex><span>   tags:
</span></span><span style=display:flex><span>     vm:
</span></span><span style=display:flex><span>       key1: val1
</span></span><span style=display:flex><span>       key2: val2
</span></span><span style=display:flex><span>       ..
</span></span><span style=display:flex><span>     <span style=color:green># for GCP network tags are just keys (there is no value associated to them). </span>
</span></span><span style=display:flex><span>     <span style=color:green># What is shown below will work for AWS provider.</span>
</span></span><span style=display:flex><span>     network:
</span></span><span style=display:flex><span>       key3: val3
</span></span><span style=display:flex><span>       key4: val4
</span></span></code></pre></div><p>Under <code>tags</code> clear distinction is made between tags for VMs, Disks, network interface etc. Each provider has a different allowed-set of characters that it accepts as key names, has different limits on the tags that can be set on a resource (disk, NIC, VM etc.) and also has a different format (GCP network tags are only keys).</p><blockquote><p>TODO:</p><ul><li><p>Check if worker.labels are getting added as tags on infra resources. We should continue to support it and double check that these should only be added to VMs and not to other resources.</p></li><li><p>Should we support users adding VM tags as node labels?</p></li></ul></blockquote><h3 id=provider-specific-workerconfig-api-changes>Provider specific WorkerConfig API changes</h3><blockquote><p>Taking <code>AWS</code> provider extension as an example to show the changes.</p></blockquote><p><a href=https://github.com/gardener/gardener-extension-provider-aws/blob/master/pkg/apis/aws/types_worker.go#L27-L38>WorkerConfig</a> will now have the following changes:</p><ol><li>A new field for tags will be introduced.</li><li>Additional metadata for struct fields will now be added via <code>struct tags</code>.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>type</span> WorkerConfig <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>    metav1.TypeMeta
</span></span><span style=display:flex><span>    Volume *Volume
</span></span><span style=display:flex><span>    <span style=color:green>// .. all fields are not mentioned here.
</span></span></span><span style=display:flex><span><span style=color:green></span>    <span style=color:green>// Tags are a collection of tags to be set on provider resources (e.g. VMs, Disks, Network Interfaces etc.)
</span></span></span><span style=display:flex><span><span style=color:green></span>    Tags *Tags <span style=color:#a31515>`hotupdatable:true`</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// Tags is a placeholder for all tags that can be set/updated on VMs, Disks and Network Interfaces.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> Tags <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>    <span style=color:green>// VM tags set on the VM instances.
</span></span></span><span style=display:flex><span><span style=color:green></span>    VM <span style=color:#00f>map</span>[<span style=color:#2b91af>string</span>]<span style=color:#2b91af>string</span>
</span></span><span style=display:flex><span>    <span style=color:green>// Network tags set on the network interfaces.
</span></span></span><span style=display:flex><span><span style=color:green></span>    Network <span style=color:#00f>map</span>[<span style=color:#2b91af>string</span>]<span style=color:#2b91af>string</span>
</span></span><span style=display:flex><span>    <span style=color:green>// Disk tags set on the volumes/disks.
</span></span></span><span style=display:flex><span><span style=color:green></span>    Disk <span style=color:#00f>map</span>[<span style=color:#2b91af>string</span>]<span style=color:#2b91af>string</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>There is a need to distinguish fields within <code>ProviderSpec</code> (which is then mapped to the above <code>WorkerConfig</code>) which can be updated without the need to change the hash suffix for <code>MachineClass</code> and thus trigger a rolling update on machines.</p><p>To achieve that we propose to use <strong>struct tag</strong> <code>hotupdatable</code> whose value indicates if the field can be updated without the need to do a rolling update. To ensure backward compatibility, all fields which do not have this tag or have <code>hotupdatable</code> set to <code>false</code> will be considered as immutable and will require a rolling update to take affect.</p><h3 id=gardener-provider-extension-changes>Gardener provider extension changes</h3><blockquote><p>Taking AWS provider extension as an example. Following changes should be made to all gardener provider extensions</p></blockquote><p>AWS Gardener Extension <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/v1.42.1/pkg/controller/worker/machines.go#L104-L107>generates machine config</a> using worker pool configuration. As part of that it also computes the <code>workerPoolHash</code> which is then used to create the <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/master/pkg/controller/worker/machines.go#L193>name of the MachineClass</a>.</p><p>Currently <code>WorkerPoolHash</code> function uses the <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/47d6bb34a538f3dfeedcf99361696de72d1eeae2/vendor/github.com/gardener/gardener/extensions/pkg/controller/worker/machines.go#L146-L148>entire providerConfig</a> to compute the hash. Proposal is to do the following:</p><ol><li>Remove the <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/47d6bb34a538f3dfeedcf99361696de72d1eeae2/vendor/github.com/gardener/gardener/extensions/pkg/controller/worker/machines.go#L146-L148>code</a> from function <code>WorkerPoolHash</code>.</li><li>Add another function to compute hash using all immutable fields in the provider config struct and then pass that to <code>worker.WorkerPoolHash</code> as <code>additionalData</code>.</li></ol><p>The above will ensure that tags and any other field in <code>WorkerConfig</code> which is marked with <code>updatable:true</code> is not considered for hash computation and will therefore not contribute to changing the name of <code>MachineClass</code> object thus preventing a rolling update.</p><p><code>WorkerConfig</code> and therefore the contained tags will be set as <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/v1alpha1/machineclass_types.go#L54>ProviderSpec</a> in <code>MachineClass</code>.</p><p>If only fields which have <code>updatable:true</code> are changed then it should result in update/patch of <code>MachineClass</code> and not creation.</p><h3 id=driver-interface-changes>Driver interface changes</h3><p><a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/util/provider/driver/driver.go#L28>Driver</a> interface which is a facade to provider specific API implementations will have one additional method.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-golang data-lang=golang><span style=display:flex><span><span style=color:#00f>type</span> Driver <span style=color:#00f>interface</span> {
</span></span><span style=display:flex><span>    <span style=color:green>// .. existing methods are not mentioned here for brevity.
</span></span></span><span style=display:flex><span><span style=color:green></span>    UpdateMachine(context.Context, *UpdateMachineRequest) <span style=color:#2b91af>error</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// UpdateMachineRequest is the request to update machine tags. 
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> UpdateMachineRequest <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>    ProviderID <span style=color:#2b91af>string</span>
</span></span><span style=display:flex><span>    LastAppliedProviderSpec raw.Extension
</span></span><span style=display:flex><span>    MachineClass *v1alpha1.MachineClass
</span></span><span style=display:flex><span>    Secret *corev1.Secret
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><blockquote><p>If any <code>machine-controller-manager-provider-&lt;providername></code> has not implemented <code>UpdateMachine</code> then updates of tags on Instances/NICs/Disks will not be done. An error message will be logged instead.</p></blockquote><blockquote></blockquote><h3 id=machine-class-reconciliation>Machine Class reconciliation</h3><p>Current <a href=https://github.com/gardener/machine-controller-manager/blob/v0.48.1/pkg/util/provider/machinecontroller/machineclass.go#L140-L194>MachineClass reconciliation</a> does not reconcile <code>MachineClass</code> resource updates but it only enqueues associated machines. The reason is that it is assumed that anything that is changed in a MachineClass will result in a creation of a new MachineClass with a different name. This will result in a rolling update of all machines using the MachineClass as a template.</p><p>However, it is possible that there is data that all machines in a <code>MachineSet</code> share which do not require a rolling update (e.g. tags), therefore there is a need to reconcile the MachineClass as well.</p><h4 id=reconciliation-changes>Reconciliation Changes</h4><p>In order to ensure that machines get updated eventually with changes to the <code>hot-updatable</code> fields defined in the <code>MachineClass.ProviderConfig</code> as <code>raw.Extension</code>.</p><p>We should only fix <a href=https://github.com/gardener/machine-controller-manager/issues/751>MCM Issue#751</a> in the MachineClass reconciliation and let it enqueue the machines as it does today. We additionally propose the following two things:</p><ol><li><p>Introduce a new annotation <code>last-applied-providerspec</code> on every machine resource. This will capture the last successfully applied <code>MachineClass.ProviderSpec</code> on this instance.</p></li><li><p>Enhance the machine reconciliation to include code to hot-update machine.</p></li></ol><p>In <a href=https://github.com/gardener/machine-controller-manager/blob/v0.48.1/pkg/util/provider/machinecontroller/machine.go#L114>machine-reconciliation</a> there are currently two flows <code>triggerDeletionFlow</code> and <code>triggerCreationFlow</code>. When a machine gets enqueued due to changes in MachineClass then in this method following changes needs to be introduced:</p><p>Check if the machine has <code>last-applied-providerspec</code> annotation.</p><p><em>Case 1.1</em></p><p>If the annotation is not present then there can be just 2 possibilities:</p><ul><li><p>It is a fresh/new machine and no backing resources (VM/NIC/Disk) exist yet. The current flow checks if the providerID is empty and <code>Status.CurrenStatus.Phase</code> is empty then it enters into the <code>triggerCreationFlow</code>.</p></li><li><p>It is an existing machine which does not yet have this annotation. In this case call <code>Driver.UpdateMachine</code>. If the driver returns no error then add <code>last-applied-providerspec</code> annotation with the value of <code>MachineClass.ProviderSpec</code> to this machine.</p></li></ul><p><em>Case 1.2</em></p><p>If the annotation is present then compare the last applied provider-spec with the current provider-spec. If there are changes (check their hash values) then call <code>Driver.UpdateMachine</code>. If the driver returns no error then add <code>last-applied-providerspec</code> annotation with the value of <code>MachineClass.ProviderSpec</code> to this machine.</p><blockquote><p>NOTE: It is assumed that if there are changes to the fields which are not marked as <code>hotupdatable</code> then it will result in the change of name for MachineClass resulting in a rolling update of machines. If the name has not changed + machine is enqueued + there is a change in machine-class then it will be change to a hotupdatable fields in the spec.</p></blockquote><p>Trigger update flow can be done after <code>reconcileMachineHealth</code> and <code>syncMachineNodeTemplates</code> in <a href=https://github.com/gardener/machine-controller-manager/blob/v0.48.1/pkg/util/provider/machinecontroller/machine.go#L164-L175>machine-reconciliation</a>.</p><p>There are 2 edge cases that needs attention and special handling:</p><blockquote><p>Premise: It is identified that there is an update done to one or more hotupdatable fields in the MachineClass.ProviderSpec.</p></blockquote><p><em>Edge-Case-1</em></p><p>In the machine reconciliation, an update-machine-flow is triggered which in-turn calls <code>Driver.UpdateMachine</code>. Consider the case where the hot update needs to be done to all VM, NIC and Disk resources. The driver returns an error which indicates a <code>partial-failure</code>. As we have mentioned above only when <code>Driver.UpdateMachine</code> returns no error will <code>last-applied-providerspec</code> be updated. In case of partial failure the annotation will not be updated. This event will be re-queued for a re-attempt. However consider a case where before the item is re-queued, another update is done to MachineClass reverting back the changes to the original spec.</p><table><thead><tr><th>At T1</th><th>At T2 (T2 > T1)</th><th>At T3 (T3> T2)</th></tr></thead><tbody><tr><td>last-applied-providerspec=S1<br>MachineClass.ProviderSpec = S1</td><td>last-applied-providerspec=S1<br>MachineClass.ProviderSpec = S2<br> Another update to MachineClass.ProviderConfig = S3 is enqueue (S3 == S1)</td><td>last-applied-providerspec=S1<br>Driver.UpdateMachine for S1-S2 update - returns partial failure<br>Machine-Key is requeued</td></tr></tbody></table><p>At T4 (T4> T3) when a machine is reconciled then it checks that <code>last-applied-providerspec</code> is S1 and current MachineClass.ProviderSpec = S3 and since S3 is same as S1, no update is done. At T2 Driver.UpdateMachine was called to update the machine with <code>S2</code> but it partially failed. So now you will have resources which are partially updated with S2 and no further updates will be attempted.</p><p><em>Edge-Case-2</em></p><p>The above situation can also happen when <code>Driver.UpdateMachine</code> is in the process of updating resources. It has hot-updated lets say 1 resource. But now MCM crashes. By the time it comes up another update to MachineClass.ProviderSpec is done essentially reverting back the previous change (same case as above). In this case reconciliation loop never got a chance to get any response from the driver.</p><p>To handle the above edge cases there are 2 options:</p><p><em>Option #1</em></p><p>Introduce a new annotation <code>inflight-providerspec-hash</code> . The value of this annotation will be the hash value of the <code>MachineClass.ProviderSpec</code> that is in the process of getting applied on this machine. The machine will be updated with this annotation just before calling <code>Driver.UpdateMachine</code> (in the trigger-update-machine-flow). If the driver returns no error then (in a single update):</p><ol><li><p><code>last-applied-providerspec</code> will be updated</p></li><li><p><code>inflight-providerspec-hash</code> annotation will be removed.</p></li></ol><p><em>Option #2</em> - Preferred</p><p>Leverage <code>Machine.Status.LastOperation</code> with <code>Type</code> set to <code>MachineOperationUpdate</code> and <code>State</code> set to <code>MachineStateProcessing</code> This status will be updated just before calling <code>Driver.UpdateMachine</code>.</p><p>Semantically <code>LastOperation</code> captures the details of the operation post-operation and not pre-operation. So this solution would be a divergence from the norm.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4c81f3bece299b069a468ca5e8928e05>1.5 - ToDo</h1></div><div class=td-content><h1 id=pg-1ccd85a54ff93a5c31b2f99d1f444eb1>1.5.1 - Outline</h1><h1 id=machine-controller-manager>Machine Controller Manager</h1><p>CORE &ndash; ./machine-controller-manager(provider independent)
Out of tree : Machine controller (provider specific)
MCM is a set controllers:</p><ul><li><p>Machine Deployment Controller</p></li><li><p>Machine Set Controller</p></li><li><p>Machine Controller</p></li><li><p>Machine Safety Controller</p></li></ul><h2 id=questions-and-refactoring-suggestions>Questions and refactoring Suggestions</h2><h3 id=refactoring>Refactoring</h3><table><thead><tr><th>Statement</th><th>FilePath</th><th>Status</th></tr></thead><tbody><tr><td>ConcurrentNodeSyncs” bad name - nothing to do with node syncs actually.<br>If its value is ’10’ then it will start 10 goroutines (workers) per resource type (machine, machinist, machinedeployment, provider-specific-class, node - study the different resource types.</td><td>cmd/machine-controller-manager/app/options/options.go</td><td>pending</td></tr><tr><td>LeaderElectionConfiguration is very similar to the one present in “client-go/tools/leaderelection/leaderelection.go” - can we simply used the one in client-go instead of defining again?</td><td>pkg/options/types.go - MachineControllerManagerConfiguration</td><td>pending</td></tr><tr><td>Have all userAgents as constant. Right now there is just one.</td><td>cmd/app/controllermanager.go</td><td>pending</td></tr><tr><td>Shouldn’t run function be defined on MCMServer struct itself?</td><td>cmd/app/controllermanager.go</td><td>pending</td></tr><tr><td>clientcmd.BuildConfigFromFlags fallsback to inClusterConfig which will surely not work as that is not the target. Should it not check and exit early?</td><td>cmd/app/controllermanager.go - run Function</td><td>pending</td></tr><tr><td>A more direct way to create an in cluster config is using <code>k8s.io/client-go/rest</code> -> rest.InClusterConfig instead of using clientcmd.BuildConfigFromFlags passing empty arguments and depending upon the implementation to fallback to creating a inClusterConfig. If they change the implementation that you get affected.</td><td>cmd/app/controllermanager.go - run Function</td><td>pending</td></tr><tr><td>Introduce a method on MCMServer which gets a target KubeConfig and controlKubeConfig or alternatively which creates respective clients.</td><td>cmd/app/controllermanager.go - run Function</td><td>pending</td></tr><tr><td>Why can’t we use Kubernetes.NewConfigOrDie also for kubeClientControl?</td><td>cmd/app/controllermanager.go - run Function</td><td>pending</td></tr><tr><td>I do not see any benefit of client builders actually. All you need to do is pass in a config and then directly use client-go functions to create a client.</td><td>cmd/app/controllermanager.go - run Function</td><td>pending</td></tr><tr><td>Function: getAvailableResources - rename this to getApiServerResources</td><td>cmd/app/controllermanager.go</td><td>pending</td></tr><tr><td>Move the method which waits for API server to up and ready to a separate method which returns a discoveryClient when the API server is ready.</td><td>cmd/app/controllermanager.go - getAvailableResources function</td><td>pending</td></tr><tr><td>Many methods in client-go used are now deprecated. Switch to the ones that are now recommended to be used instead.</td><td>cmd/app/controllermanager.go - startControllers</td><td>pending</td></tr><tr><td>This method needs a general overhaul</td><td>cmd/app/controllermanager.go - startControllers</td><td>pending</td></tr><tr><td>If the design is influenced/copied from KCM then its very different. There are different controller structs defined for deployment, replicaset etc which makes the code much more clearer. You can see “kubernetes/cmd/kube-controller-manager/apps.go” and then follow the trail from there. - agreed needs to be changed in future (if time permits)</td><td>pkg/controller/controller.go</td><td>pending</td></tr><tr><td>I am not sure why “MachineSetControlInterface”, “RevisionControlInterface”, “MachineControlInterface”, “FakeMachineControl” are defined in this file?</td><td>pkg/controller/controller_util.go</td><td>pending</td></tr><tr><td><code>IsMachineActive</code> - combine the first 2 conditions into one with OR.</td><td>pkg/controller/controller_util.go</td><td>pending</td></tr><tr><td>Minor change - correct the comment, first word should always be the method name. Currently none of the comments have correct names.</td><td>pkg/controller/controller_util.go</td><td>pending</td></tr><tr><td>There are too many deep copies made. What is the need to make another deep copy in this method? You are not really changing anything here.</td><td>pkg/controller/deployment.go - updateMachineDeploymentFinalizers</td><td>pending</td></tr><tr><td>Why can&rsquo;t these validations be done as part of a validating webhook?</td><td>pkg/controller/machineset.go - reconcileClusterMachineSet</td><td>pending</td></tr><tr><td>Small change to the following <code>if</code> condition. <code>else if</code> is not required a simple <code>else</code> is sufficient. <a href=#1.1-code1>Code1</a></td><td></td><td></td></tr><tr><td>pkg/controller/machineset.go - reconcileClusterMachineSet</td><td>pending</td><td></td></tr><tr><td>Why call these <code>inactiveMachines</code>, these are live and running and therefore active.</td><td>pkg/controller/machineset.go - terminateMachines</td><td>pending</td></tr></tbody></table><h3 id=clarification>Clarification</h3><table><thead><tr><th>Statement</th><th>FilePath</th><th>Status</th></tr></thead><tbody><tr><td>Why are there 2 versions - internal and external versions?</td><td>General</td><td>pending</td></tr><tr><td>Safety controller freezes MCM controllers in the following cases:<br>* Num replicas go beyond a threshold (above the defined replicas)<br>* Target API service is not reachable<br>There seems to be an overlap between DWD and MCM Safety controller. In the meltdown scenario why is MCM being added to DWD, you could have used Safety controller for that.</td><td>General</td><td>pending</td></tr><tr><td>All machine resources are v1alpha1 - should we not promote it to beta. V1alpha1 has a different semantic and does not give any confidence to the consumers.</td><td>cmd/app/controllermanager.go</td><td>pending</td></tr><tr><td>Shouldn’t controller manager use context.Context instead of creating a stop channel? - Check if signals (<code>os.Interrupt</code> and <code>SIGTERM</code> are handled properly. Do not see code where this is handled currently.)</td><td>cmd/app/controllermanager.go</td><td>pending</td></tr><tr><td>What is the rationale behind a timeout of 10s? If the API server is not up, should this not just block as it can anyways not do anything. Also, if there is an error returned then you exit the MCM which does not make much sense actually as it will be started again and you will again do the poll for the API server to come back up. Forcing an exit of MCM will not have any impact on the reachability of the API server in anyway so why exit?</td><td>cmd/app/controllermanager.go - getAvailableResources</td><td>pending</td></tr><tr><td>There is a very weird check - <code>availableResources[machineGVR] || availableResources[machineSetGVR] || availableResources[machineDeploymentGVR]</code><br>Shouldn’t this be conjunction instead of disjunction?<br>* What happens if you do not find one or all of these resources?<br>Currently an error log is printed and nothing else is done. MCM can be used outside gardener context where consumers can directly create MachineClass and Machine and not create MachineSet / Maching Deployment. There is no distinction made between context (gardener or outside-gardener).<br></td><td>cmd/app/controllermanager.go - StartControllers</td><td>pending</td></tr><tr><td>Instead of having an empty select {} to block forever, isn’t it better to wait on the stop channel?</td><td>cmd/app/controllermanager.go - StartControllers</td><td>pending</td></tr><tr><td>Do we need provider specific queues and syncs and listers</td><td>pkg/controller/controller.go</td><td>pending</td></tr><tr><td>Why are resource types prefixed with “Cluster”? - not sure , check PR</td><td>pkg/controller/controller.go</td><td>pending</td></tr><tr><td>When will forgetAfterSuccess be false and why? - as per the current code this is never the case. - Himanshu will check</td><td>cmd/app/controllermanager.go - createWorker</td><td>pending</td></tr><tr><td>What is the use of “ExpectationsInterface” and “UIDTrackingContExpectations”?<br>* All expectations related code should be in its own file “expectations.go” and not in this file.</td><td>pkg/controller/controller_util.go</td><td>pending</td></tr><tr><td>Why do we not use lister but directly use the controlMachingClient to get the deployment? Is it because you want to avoid any potential delays caused by update of the local cache held by the informer and accessed by the lister? What is the load on API server due to this?</td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>Why is this conversion needed? <a href=#1.2-code2>code2</a></td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>A deep copy of <code>machineDeployment</code> is already passed and within the function another deepCopy is made. Any reason for it?</td><td>pkg/controller/deployment.go - addMachineDeploymentFinalizers</td><td>pending</td></tr><tr><td>What is an <code>Status.ObservedGeneration</code>?<br>*<em>Read more about generations and observedGeneration at:<br><a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata>https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata</a><br></em><a href=https://alenkacz.medium.com/kubernetes-operator-best-practices-implementing-observedgeneration-250728868792>https://alenkacz.medium.com/kubernetes-operator-best-practices-implementing-observedgeneration-250728868792</a><br>Ideally the update to the <code>ObservedGeneration</code> should only be made after successful reconciliation and not before. I see that this is just copied from <code>deployment_controller.go</code> as is</td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>Why and when will a <code>MachineDeployment</code> be marked as frozen and when will it be un-frozen?</td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>Shoudn&rsquo;t the validation of the machine deployment be done during the creation via a validating webhook instead of allowing it to be stored in etcd and then failing the validation during sync? I saw the checks and these can be done via validation webhook.</td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>RollbackTo has been marked as deprecated. What is the replacement? <a href=#1.3-code3>code3</a></td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>What is the max machineSet deletions that you could process in a single run? The reason for asking this question is that for every machineSetDeletion a new goroutine spawned.<br>* Is the <code>Delete</code> call a synchrounous call? Which means it blocks till the machineset deletion is triggered which then also deletes the machines (due to cascade-delete and blockOwnerDeletion= true)?</td><td>pkg/controller/deployment.go - terminateMachineSets</td><td>pending</td></tr><tr><td>If there are validation errors or error when creating label selector then a nil is returned. In the worker reconcile loop if the return value is nil then it will remove it from the queue (forget + done). What is the way to see any errors? Typically when we describe a resource the errors are displayed. Will these be displayed when we discribe a <code>MachineDeployment</code>?</td><td>pkg/controller/deployment.go - reconcileClusterMachineSet</td><td>pending</td></tr><tr><td>If an error is returned by <code>updateMachineSetStatus</code> and it is <code>IsNotFound</code> error then returning an error will again queue the <code>MachineSet</code>. Is this desired as <code>IsNotFound</code> indicates the <code>MachineSet</code> has been deleted and is no longer there?</td><td>pkg/controller/deployment.go - reconcileClusterMachineSet</td><td>pending</td></tr><tr><td>is <code>machineControl.DeleteMachine</code> a synchronous operation which will wait till the machine has been deleted? Also where is the <code>DeletionTimestamp</code> set on the <code>Machine</code>? Will it be automatically done by the API server?</td><td>pkg/controller/deployment.go - prepareMachineForDeletion</td><td>pending</td></tr></tbody></table><h3 id=bugsenhancements>Bugs/Enhancements</h3><table><thead><tr><th>Statement + TODO</th><th>FilePath</th><th>Status</th></tr></thead><tbody><tr><td>This defines QPS and Burst for its requests to the KAPI. Check if it would make sense to explicitly define a FlowSchema and PriorityLevelConfiguration to ensure that the requests from this controller are given a well-defined preference. What is the rational behind deciding these values?</td><td>pkg/options/types.go - MachineControllerManagerConfiguration</td><td>pending</td></tr><tr><td>In function “validateMachineSpec” fldPath func parameter is never used.</td><td>pkg/apis/machine/validation/machine.go</td><td>pending</td></tr><tr><td>If there is an update failure then this method recursively calls itself without any sort of delays which could lead to a LOT of load on the API server. (opened: <a href=https://github.com/gardener/machine-controller-manager/issues/686>https://github.com/gardener/machine-controller-manager/issues/686</a>)</td><td>pkg/controller/deployment.go - updateMachineDeploymentFinalizers</td><td>pending</td></tr><tr><td>We are updating <code>filteredMachines</code> by invoking <code>syncMachinesNodeTemplates</code>, <code>syncMachinesConfig</code> and <code>syncMachinesClassKind</code> but we do not create any deepCopy here. Everywhere else the general principle is when you mutate always make a deepCopy and then mutate the copy instead of the original as a lister is used and that changes the cached copy.<br><code>Fix</code>: <code>SatisfiedExpectations</code> check has been commented and there is a TODO there to fix it. Is there a PR for this?</td><td>pkg/controller/machineset.go - reconcileClusterMachineSet</td><td>pending</td></tr></tbody></table><p>Code references</p><h1 id=11-code1>1.1 code1</h1><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>       <span style=color:#00f>if</span> machineSet.DeletionTimestamp == <span style=color:#00f>nil</span> {
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        		<span style=color:green>// manageReplicas is the core machineSet method where scale up/down occurs
</span></span></span><span style=display:flex><span><span style=color:green></span>        
</span></span><span style=display:flex><span>        		<span style=color:green>// It is not called when deletion timestamp is set
</span></span></span><span style=display:flex><span><span style=color:green></span>        
</span></span><span style=display:flex><span>        		manageReplicasErr = c.manageReplicas(ctx, filteredMachines, machineSet)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span>​</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        	} <span style=color:#00f>else</span> <span style=color:#00f>if</span> machineSet.DeletionTimestamp != <span style=color:#00f>nil</span> { 
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>            <span style=color:green>//FIX: change this to simple else without the if
</span></span></span></code></pre></div><h1 id=12-code2>1.2 code2</h1><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>    <span style=color:#00f>defer</span> dc.enqueueMachineDeploymentAfter(deployment, 10*time.Minute)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    *  <span style=color:#a31515>`Clarification`</span>:  Why  is  this  conversion  needed<span>?</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    err = v1alpha1.Convert_v1alpha1_MachineDeployment_To_machine_MachineDeployment(deployment, internalMachineDeployment, <span style=color:#00f>nil</span>)
</span></span></code></pre></div><h1 id=13-code3>1.3 code3</h1><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// rollback is not re-entrant in case the underlying machine sets are updated with a new
</span></span></span><span style=display:flex><span><span style=color:green></span>
</span></span><span style=display:flex><span>	<span style=color:green>// revision so we should ensure that we won&#39;t proceed to update machine sets until we
</span></span></span><span style=display:flex><span><span style=color:green></span>
</span></span><span style=display:flex><span>	<span style=color:green>// make sure that the deployment has cleaned up its rollback spec in subsequent enqueues.
</span></span></span><span style=display:flex><span><span style=color:green></span>
</span></span><span style=display:flex><span>	<span style=color:#00f>if</span> d.Spec.RollbackTo != <span style=color:#00f>nil</span> {
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#00f>return</span> dc.rollback(ctx, d, machineSets, machineMap)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	}
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-a021045e0a5544e5e8c6993bca77c7bf>1.6 - FAQ</h1><div class=lead>Frequently Asked Questions</div><h1 id=frequently-asked-questions>Frequently Asked Questions</h1><p>The answers in this FAQ apply to the newest (HEAD) version of Machine Controller Manager. If
you&rsquo;re using an older version of MCM please refer to corresponding version of
this document. Few of the answers assume that the MCM being used is in conjuction with <a href=https://github.com/gardener/autoscaler>cluster-autoscaler</a>:</p><h1 id=table-of-contents>Table of Contents:</h1><ul><li><p><a href=#basics>Basics</a></p><ul><li><a href=#what-is-machine-controller-manager>What is Machine Controller Manager?</a></li><li><a href=#Why-is-my-machine-deleted>Why is my machine deleted?</a></li><li><a href=#What-are-the-different-sub-controllers-in-MCM>What are the different sub-controllers in MCM?</a></li><li><a href=#What-is-safety-controller-in-MCM>What is Safety Controller in MCM?</a></li></ul></li><li><p><a href=#how-to>How to?</a></p><ul><li><a href=#How-to-install-MCM-in-a-kubernetes-cluster>How to install MCM in a Kubernetes cluster?</a></li><li><a href=#How-to-better-control-the-rollout-process-of-the-worker-nodes>How to better control the rollout process of the worker nodes?</a></li><li><a href=#How-to-scale-down-machinedeployment-by-selective-deletion-of-machines>How to scale down MachineDeployment by selective deletion of machines?</a></li><li><a href=#How-to-force-delete-a-machine>How to force delete a machine?</a></li><li><a href=#How-to-pause-the-ongoing-rolling-update-of-the-machinedeployment>How to pause the ongoing rolling-update of the machinedeployment?</a></li><li><a href=#how-to-delete-machine-object-immedietly-if-i-dont-have-access-to-it>How to delete machine object immedietly if I don&rsquo;t have access to it?</a></li><li><a href=#How-to-avoid-garbage-collection-of-your-node>How to avoid garbage collection of your node?</a></li></ul></li><li><p><a href=#internals>Internals</a></p><ul><li><a href=#What-is-the-high-level-design-of-MCM>What is the high level design of MCM?</a></li><li><a href=#What-are-the-different-configuration-options-in-MCM>What are the different configuration options in MCM?</a></li><li><a href=#What-are-the-different-timeoutsconfigurations-in-a-machines-lifecycle>What are the different timeouts/configurations in a machine&rsquo;s lifecycle?</a></li><li><a href=#How-is-the-drain-of-a-machine-implemented>How is the drain of a machine implemented?</a></li><li><a href=#How-are-the-stateful-applications-drained-during-machine-deletion>How are the stateful applications drained during machine deletion?</a></li><li><a href=#How-does-maxEvictRetries-configuration-work-with-drainTimeout-configuration>How does maxEvictRetries configuration work with drainTimeout configuration?</a></li><li><a href=#What-are-the-different-phases-of-a-machine>What are the different phases of a machine?</a></li><li><a href=#what-health-checks-are-performed-on-a-machine>What health checks are performed on a machine?</a></li><li><a href=#how-does-rate-limiting-replacement-of-machine-work-in-mcm-how-is-it-related-to-meltdown-protection>How does rate limiting replacement of machine work in MCM ? How is it related to meltdown protection?</a></li><li><a href=#how-mcm-responds-when-scale-outscale-in-is-done-during-rolling-update-of-a-machinedeployment>How MCM responds when scale-out/scale-in is done during rolling update of a machinedeployment?</a></li></ul></li><li><p><a href=#troubleshooting>Troubleshooting</a></p><ul><li><a href=#My-machine-is-stuck-in-deletion-for-1-hr-why>My machine is stuck in deletion for 1 hr, why?</a></li><li><a href=#My-machine-is-not-joining-the-cluster-why>My machine is not joining the cluster, why?</a></li></ul></li><li><p><a href=#developer>Developer</a></p><ul><li><a href=#How-should-I-test-my-code-before-submitting-a-PR>How should I test my code before submitting a PR?</a></li><li><a href=#I-need-to-change-the-APIs-what-are-the-recommended-steps>I need to change the APIs, what are the recommended steps?</a></li><li><a href=#How-can-I-update-the-dependencies-of-MCM>How can I update the dependencies of MCM?</a></li></ul></li><li><p><a href=#in-the-context-of-gardener>In the context of Gardener</a></p><ul><li><a href=#How-can-I-configure-MCM-using-Shoot-resource>How can I configure MCM using Shoot resource?</a></li><li><a href=#How-is-my-worker-pool-spread-across-zones>How is my worker-pool spread across zones?</a></li></ul></li></ul><h1 id=basics>Basics</h1><h3 id=what-is-machine-controller-manager>What is Machine Controller Manager?</h3><p>Machine Controller Manager aka MCM is a bunch of controllers used for the lifecycle management of the worker machines. It reconciles a set of CRDs such as <code>Machine</code>, <code>MachineSet</code>, <code>MachineDeployment</code> which depicts the functionality of <code>Pod</code>, <code>Replicaset</code>, <code>Deployment</code> of the core Kubernetes respectively. Read more about it at <a href=https://github.com/gardener/machine-controller-manager/tree/master/docs>README</a>.</p><ul><li>Gardener uses MCM to manage its Kubernetes nodes of the shoot cluster. However, by design, MCM can be used independent of Gardener.</li></ul><h3 id=why-is-my-machine-deleted>Why is my machine deleted?</h3><p>A machine is deleted by MCM generally for 2 reasons-</p><ul><li><p>Machine is unhealthy for at least <code>MachineHealthTimeout</code> period. The default <code>MachineHealthTimeout</code> is 10 minutes.</p><ul><li>By default, a machine is considered unhealthy if any of the following node conditions - <code>DiskPressure</code>, <code>KernelDeadlock</code>, <code>FileSystem</code>, <code>Readonly</code> is set to <code>true</code>, or <code>KubeletReady</code> is set to <code>false</code>. However, this is something that is configurable using the following <a href=https://github.com/gardener/machine-controller-manager/blob/master/kubernetes/deployment/out-of-tree/deployment.yaml#L30>flag</a>.</li></ul></li><li><p>Machine is scaled down by the <code>MachineDeployment</code> resource.</p><ul><li>This is very usual when an external controller cluster-autoscaler (aka CA) is used with MCM. CA deletes the under-utilized machines by scaling down the <code>MachineDeployment</code>. Read more about cluster-autoscaler&rsquo;s scale down behavior <a href=https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#how-does-scale-down-work>here</a>.</li></ul></li></ul><h3 id=what-are-the-different-sub-controllers-in-mcm>What are the different sub-controllers in MCM?</h3><p>MCM mainly contains the following sub-controllers:</p><ul><li><code>MachineDeployment Controller</code>: Responsible for reconciling the <code>MachineDeployment</code> objects. It manages the lifecycle of the <code>MachineSet</code> objects.</li><li><code>MachineSet Controller</code>: Responsible for reconciling the <code>MachineSet</code> objects. It manages the lifecycle of the <code>Machine</code> objects.</li><li><code>Machine Controller</code>: responsible for reconciling the <code>Machine</code> objects. It manages the lifecycle of the actual VMs/machines created in cloud/on-prem. This controller has been moved out of tree. Please refer an AWS machine controller for more info - <a href=https://github.com/gardener/machine-controller-manager-provider-gcp>link</a>.</li><li>Safety-controller: Responsible for handling the unidentified/unknown behaviors from the cloud providers. Please read more about its functionality <a href=#what-is-safety-controller>below</a>.</li></ul><h3 id=what-is-safety-controller-in-mcm>What is Safety Controller in MCM?</h3><p><code>Safety Controller</code> contains following functions:</p><ul><li>Orphan VM handler:<ul><li>It lists all the VMs in the cloud matching the <code>tag</code> of given cluster name and maps the VMs with the <code>machine</code> objects using the <code>ProviderID</code> field. VMs without any backing <code>machine</code> objects are logged and deleted after confirmation.</li><li>This handler runs every 30 minutes and is configurable via <a href=https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go#L112>machine-safety-orphan-vms-period</a> flag.</li></ul></li><li>Freeze mechanism:<ul><li><code>Safety Controller</code> freezes the <code>MachineDeployment</code> and <code>MachineSet</code> controller if the number of <code>machine</code> objects goes beyond a certain threshold on top of <code>Spec.Replicas</code>. It can be configured by the flag <a href=https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go#L102-L103>&ndash;safety-up or &ndash;safety-down</a> and also <a href=https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go#L113>machine-safety-overshooting-period</a>.</li><li><code>Safety Controller</code> freezes the functionality of the MCM if either of the <code>target-apiserver</code> or the <code>control-apiserver</code> is not reachable.</li><li><code>Safety Controller</code> unfreezes the MCM automatically once situation is resolved to normal. A <code>freeze</code> label is applied on <code>MachineDeployment</code>/<code>MachineSet</code> to enforce the freeze condition.</li></ul></li></ul><h1 id=how-to>How to?</h1><h3 id=how-to-install-mcm-in-a-kubernetes-cluster>How to install MCM in a Kubernetes cluster?</h3><p>MCM can be installed in a cluster with following steps:</p><ul><li><p>Apply all the CRDs from <a href=https://github.com/gardener/machine-controller-manager/tree/master/kubernetes/crds>here</a></p></li><li><p>Apply all the deployment, role-related objects from <a href=https://github.com/gardener/machine-controller-manager/tree/master/kubernetes/deployment/out-of-tree>here</a>.</p><ul><li>Control cluster is the one where the <code>machine-*</code> objects are stored. Target cluster is where all the node objects are registered.</li></ul></li></ul><h3 id=how-to-better-control-the-rollout-process-of-the-worker-nodes>How to better control the rollout process of the worker nodes?</h3><p>MCM allows configuring the rollout of the worker machines using <code>maxSurge</code> and <code>maxUnavailable</code> fields. These fields are applicable only during the rollout process and means nothing in general scale up/down scenarios.
The overall process is very similar to how the <code>Deployment Controller</code> manages pods during <code>RollingUpdate</code>.</p><ul><li><code>maxSurge</code> refers to the number of additional machines that can be added on top of the <code>Spec.Replicas</code> of MachineDeployment <em>during rollout process</em>.</li><li><code>maxUnavailable</code> refers to the number of machines that can be deleted from <code>Spec.Replicas</code> field of the MachineDeployment <em>during rollout process</em>.</li></ul><h3 id=how-to-scale-down-machinedeployment-by-selective-deletion-of-machines>How to scale down MachineDeployment by selective deletion of machines?</h3><p>During scale down, triggered via <code>MachineDeployment</code>/<code>MachineSet</code>, MCM prefers to delete the <code>machine/s</code> which have the least priority set.
Each <code>machine</code> object has an annotation <code>machinepriority.machine.sapcloud.io</code> set to <code>3</code> by default. Admin can reduce the priority of the given machines by changing the annotation value to <code>1</code>. The next scale down by <code>MachineDeployment</code> shall delete the machines with the least priority first.</p><h3 id=how-to-force-delete-a-machine>How to force delete a machine?</h3><p>A machine can be force deleted by adding the label <code>force-deletion: "True"</code> on the <code>machine</code> object before executing the actual delete command. During force deletion, MCM skips the drain function and simply triggers the deletion of the machine. This label should be used with caution as it can violate the PDBs for pods running on the machine.</p><h3 id=how-to-pause-the-ongoing-rolling-update-of-the-machinedeployment>How to pause the ongoing rolling-update of the machinedeployment?</h3><p>An ongoing rolling-update of the machine-deployment can be paused by using <code>spec.paused</code> field. See the example below:</p><pre tabindex=0><code>apiVersion: machine.sapcloud.io/v1alpha1
kind: MachineDeployment
metadata:
  name: test-machine-deployment
spec:
  paused: true
</code></pre><p>It can be unpaused again by removing the <code>Paused</code> field from the machine-deployment.</p><h3 id=how-to-delete-machine-object-immedietly-if-i-dont-have-access-to-it>How to delete machine object immedietly if I don&rsquo;t have access to it?</h3><p>If the user doesn&rsquo;t have access to the machine objects (like in case of Gardener clusters) and they would like to replace a node immedietly then they can place the annotation <code>node.machine.sapcloud.io/trigger-deletion-by-mcm: "true"</code> on their node. This will start the replacement of the machine with a new node.</p><p>On the other hand if the user deletes the node object immedietly then replacement will start only after <code>MachineHealthTimeout</code>.</p><p>This annotation can also be used if the user wants to expedite the <a href=#how-does-rate-limiting-replacement-of-machine-work-in-mcm-how-is-it-related-to-meltdown-protection>replacement of unhealthy nodes</a></p><p><code>NOTE</code>:</p><ul><li><code>node.machine.sapcloud.io/trigger-deletion-by-mcm: "false"</code> annotation is NOT acted upon by MCM , neither does it mean that MCM will not replace this machine.</li><li>this annotation would delete the desired machine but another machine would be created to maintain <code>desired replicas</code> specified for the machineDeployment/machineSet. Currently if the user doesn&rsquo;t have access to machineDeployment/machineSet then they cannot remove a machine without replacement.</li></ul><h3 id=how-to-avoid-garbage-collection-of-your-node>How to avoid garbage collection of your node?</h3><p>MCM provides an in-built safety mechanism to garbage collect VMs which have no corresponding machine object. This is done to save costs and is one of the key features of MCM.
However, sometimes users might like to add nodes directly to the cluster without the help of MCM and would prefer MCM to not garbage collect such VMs.
To do so they should remove/not-use tags on their VMs containing the following strings:</p><ol><li><code>kubernetes.io/cluster/</code></li><li><code>kubernetes.io/role/</code></li><li><code>kubernetes-io-cluster-</code></li><li><code>kubernetes-io-role-</code></li></ol><h1 id=internals>Internals</h1><h3 id=what-is-the-high-level-design-of-mcm>What is the high level design of MCM?</h3><p>Please refer the following <a href=/docs/other-components/machine-controller-manager/#design-of-machine-controller-manager>document</a>.</p><h3 id=what-are-the-different-configuration-options-in-mcm>What are the different configuration options in MCM?</h3><p>MCM allows configuring many knobs to fine-tune its behavior according to the user&rsquo;s need.
Please refer to the <a href=https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go>link</a> to check the exact configuration options.</p><h3 id=what-are-the-different-timeoutsconfigurations-in-a-machines-lifecycle>What are the different timeouts/configurations in a machine&rsquo;s lifecycle?</h3><p>A machine&rsquo;s lifecycle is governed by mainly following timeouts, which can be configured <a href=https://github.com/gardener/machine-controller-manager/blob/master/kubernetes/machine_objects/machine-deployment.yaml#L30-L34>here</a>.</p><ul><li><code>MachineDrainTimeout</code>: Amount of time after which drain times out and the machine is force deleted. Default ~2 hours.</li><li><code>MachineHealthTimeout</code>: Amount of time after which an unhealthy machine is declared <code>Failed</code> and the machine is replaced by <code>MachineSet</code> controller.</li><li><code>MachineCreationTimeout</code>: Amount of time after which a machine creation is declared <code>Failed</code> and the machine is replaced by the <code>MachineSet</code> controller.</li><li><code>NodeConditions</code>: List of node conditions which if set to true for <code>MachineHealthTimeout</code> period, the machine is declared <code>Failed</code> and replaced by <code>MachineSet</code> controller.</li><li><code>MaxEvictRetries</code>: An integer number depicting the number of times a failed <em>eviction</em> should be retried on a pod during drain process. A pod is <em>deleted</em> after <code>max-retries</code>.</li></ul><h3 id=how-is-the-drain-of-a-machine-implemented>How is the drain of a machine implemented?</h3><p>MCM imports the functionality from the upstream Kubernetes-drain library. Although, few parts have been modified to make it work best in the context of MCM. Drain is executed before machine deletion for graceful migration of the applications.
Drain internally uses the <code>EvictionAPI</code> to evict the pods and triggers the <code>Deletion</code> of pods after <code>MachineDrainTimeout</code>. Please note:</p><ul><li>Stateless pods are evicted in parallel.</li><li>Stateful applications (with PVCs) are serially evicted. Please find more info in this <a href=#how-are-the-stateful-applications-drained-during-machine-deletion>answer below</a>.</li></ul><h3 id=how-are-the-stateful-applications-drained-during-machine-deletion>How are the stateful applications drained during machine deletion?</h3><p>Drain function serially evicts the stateful-pods. It is observed that serial eviction of stateful pods yields better overall availability of pods as the underlying cloud in most cases detaches and reattaches disks serially anyways.
It is implemented in the following manner:</p><ul><li>Drain lists all the pods with attached volumes. It evicts very first stateful-pod and waits for its related entry in Node object&rsquo;s <code>.status.volumesAttached</code> to be removed by KCM. It does the same for all the stateful-pods.</li><li>It waits for <code>PvDetachTimeout</code> (default 2 minutes) for a given pod&rsquo;s PVC to be removed, else moves forward.</li></ul><h3 id=how-does-maxevictretries-configuration-work-with-draintimeout-configuration>How does <code>maxEvictRetries</code> configuration work with <code>drainTimeout</code> configuration?</h3><p>It is recommended to only set <code>MachineDrainTimeout</code>. It satisfies the related requirements. <code>MaxEvictRetries</code> is auto-calculated based on <code>MachineDrainTimeout</code>, if <code>maxEvictRetries</code> is not provided. Following will be the overall behavior of both configurations together:</p><ul><li>If <code>maxEvictRetries</code> isn&rsquo;t set and only <code>maxDrainTimeout</code> is set:<ul><li>MCM auto calculates the <code>maxEvictRetries</code> based on the <code>drainTimeout</code>.</li></ul></li><li>If <code>drainTimeout</code> isn&rsquo;t set and only <code>maxEvictRetries</code> is set:<ul><li>Default <code>drainTimeout</code> and user provided <code>maxEvictRetries</code> for each pod is considered.</li></ul></li><li>If both <code>maxEvictRetries</code> and <code>drainTimoeut</code> are set:<ul><li>Then both will be respected.</li></ul></li><li>If none are set:<ul><li>Defaults are respected.</li></ul></li></ul><h3 id=what-are-the-different-phases-of-a-machine>What are the different phases of a machine?</h3><p>A phase of a <code>machine</code> can be identified with <code>Machine.Status.CurrentStatus.Phase</code>. Following are the possible phases of a <code>machine</code> object:</p><ul><li><p><code>Pending</code>: Machine creation call has succeeded. MCM is waiting for machine to join the cluster.</p></li><li><p><code>CrashLoopBackOff</code>: Machine creation call has failed. MCM will retry the operation after a minor delay.</p></li><li><p><code>Running</code>: Machine creation call has succeeded. Machine has joined the cluster successfully and corresponding node doesn&rsquo;t have <code>node.gardener.cloud/critical-components-not-ready</code> taint.</p></li><li><p><code>Unknown</code>: Machine <a href=#what-health-checks-are-performed-on-a-machine>health checks</a> are failing, eg <code>kubelet</code> has stopped posting the status.</p></li><li><p><code>Failed</code>: Machine health checks have failed for a prolonged time. Hence it is declared failed by <code>Machine</code> controller in a <a href=#how-does-rate-limiting-replacement-of-machine-work-in-mcm-how-is-it-related-to-meltdown-protection>rate limited fashion</a>. <code>Failed</code> machines get replaced immediately.</p></li><li><p><code>Terminating</code>: Machine is being terminated. Terminating state is set immediately when the deletion is triggered for the <code>machine</code> object. It also includes time when it&rsquo;s being drained.</p></li></ul><p><code>NOTE</code>: No phase means the machine is being created on the cloud-provider.</p><h3 id=what-health-checks-are-performed-on-a-machine>What health checks are performed on a machine?</h3><p>Health check performed on a machine are:</p><ul><li>Existense of corresponding node obj</li><li>Status of certain user-configurable node conditions.<ul><li>These conditions can be specified using the flag <code>--node-conditions</code> for OOT MCM provider or can be specified per machine object.</li><li>The default user configurable node conditions can be found <a href=https://github.com/gardener/machine-controller-manager/blob/91eec24516b8339767db5a40e82698f9fe0daacd/pkg/util/provider/app/options/options.go#L60>here</a></li></ul></li><li><code>True</code> status of <code>NodeReady</code> condition . This condition shows kubelet&rsquo;s status</li></ul><p>If any of the above checks fails , the machine turns to <code>Unknown</code> phase.</p><h3 id=how-does-rate-limiting-replacement-of-machine-work-in-mcm-how-is-it-related-to-meltdown-protection>How does rate limiting replacement of machine work in MCM? How is it related to meltdown protection?</h3><p>Currently MCM replaces only <code>1</code> <code>Unkown</code> machine at a time per machinedeployment. This means until the particular <code>Unknown</code> machine get terminated and its replacement joins, no other <code>Unknown</code> machine would be removed.</p><p>The above is achieved by enabling <code>Machine</code> controller to turn machine from <code>Unknown</code> -> <code>Failed</code> only if the above condition is met. <code>MachineSet</code> controller on the other hand marks <code>Failed</code> machine as <code>Terminating</code> immediately.</p><p>One reason for this rate limited replacement was to ensure that in case of network failures , where node&rsquo;s kubelet can&rsquo;t reach out to kube-apiserver , all nodes are not removed together i.e. <code>meltdown protection</code>.
In gardener context however, <a href=https://github.com/gardener/dependency-watchdog/blob/master/docs/concepts/prober.md#origin>DWD</a> is deployed to deal with this scenario, but to stay protected from corner cases , this mechanism has been introduced in MCM.</p><p><code>NOTE</code>: Rate limiting replacement is not yet configurable</p><h3 id=how-mcm-responds-when-scale-outscale-in-is-done-during-rolling-update-of-a-machinedeployment>How MCM responds when scale-out/scale-in is done during rolling update of a machinedeployment?</h3><p><code>Machinedeployment</code> controller executes the logic of <code>scaling</code> BEFORE logic of <code>rollout</code>. It identifies <code>scaling</code> by comparing the <code>deployment.kubernetes.io/desired-replicas</code> of each machineset under the machinedeployment with machinedeployment&rsquo;s <code>.spec.replicas</code>. If the difference is found for any machineSet, a scaling event is detected.</p><p>Case <code>scale-out</code> -> ONLY New machineSet is scaled out<br>Case <code>scale-in</code> -> ALL machineSets(new or old) are scaled in , in proportion to their replica count , any leftover is adjusted in the largest machineSet.</p><p>During update for scaling event, a machineSet is updated if any of the below is true for it:</p><ul><li><code>.spec.Replicas</code> needs update</li><li><code>deployment.kubernetes.io/desired-replicas</code> needs update</li></ul><p>Once scaling is achieved, rollout continues.</p><h1 id=troubleshooting>Troubleshooting</h1><h3 id=my-machine-is-stuck-in-deletion-for-1-hr-why>My machine is stuck in deletion for 1 hr, why?</h3><p>In most cases, the <code>Machine.Status.LastOperation</code> provides information around why a machine can&rsquo;t be deleted.
Though following could be the reasons but not limited to:</p><ul><li>Pod/s with misconfigured PDBs block the drain operation. PDBs with <code>maxUnavailable</code> set to 0, doesn&rsquo;t allow the eviction of the pods. Hence, drain/eviction is retried till <code>MachineDrainTimeout</code>. Default <code>MachineDrainTimeout</code> could be as large as ~2hours. Hence, blocking the machine deletion.<ul><li>Short term: User can manually delete the pod in the question, <em>with caution</em>.</li><li>Long term: Please set more appropriate PDBs which allow disruption of at least one pod.</li></ul></li><li>Expired cloud credentials can block the deletion of the machine from infrastructure.</li><li>Cloud provider can&rsquo;t delete the machine due to internal errors. Such situations are best debugged by using cloud provider specific CLI or cloud console.</li></ul><h3 id=my-machine-is-not-joining-the-cluster-why>My machine is not joining the cluster, why?</h3><p>In most cases, the <code>Machine.Status.LastOperation</code> provides information around why a machine can&rsquo;t be created.
It could possibly be debugged with following steps:</p><ul><li>Firstly make sure all the relevant controllers like <code>kube-controller-manager</code> , <code>cloud-controller-manager</code> are running.</li><li>Verify if the machine is actually created in the cloud. User can use the <code>Machine.Spec.ProviderId</code> to query the machine in cloud.</li><li>A Kubernetes node is generally bootstrapped with the cloud-config. Please verify, if <code>MachineDeployment</code> is pointing the correct <code>MachineClass</code>, and <code>MachineClass</code> is pointing to the correct <code>Secret</code>. The secret object contains the actual cloud-config in <code>base64</code> format which will be used to boot the machine.</li><li>User must also check the logs of the MCM pod to understand any broken logical flow of reconciliation.</li></ul><h3 id=my-rolling-update-is-stuck--why>My rolling update is stuck , why?</h3><p>The following can be the reason:</p><ul><li>Insufficient capacity for the new instance type the machineClass mentions.</li><li><a href=#my-machine-is-stuck-in-deletion-for-1-hr-why>Old machines are stuck in deletion</a></li><li>If you are using Gardener for setting up kubernetes cluster, then machine object won&rsquo;t turn to <code>Running</code> state until <code>node-critical-components</code> are ready. Refer <a href=/docs/gardener/usage/node-readiness/>this</a> for more details.</li></ul><h1 id=developer>Developer</h1><h3 id=how-should-i-test-my-code-before-submitting-a-pr>How should I test my code before submitting a PR?</h3><ul><li>Developer can locally setup the MCM using following <a href=/docs/other-components/machine-controller-manager/development/local_setup/>guide</a></li><li>Developer must also enhance the unit tests related to the incoming changes.</li><li>Developer can locally run the unit test by executing:</li></ul><pre tabindex=0><code>make test-unit
</code></pre><ul><li>Developer can locally run <a href=/docs/other-components/machine-controller-manager/development/integration_tests/>integration tests</a> to ensure basic functionality of MCM is not altered.</li></ul><h3 id=i-need-to-change-the-apis-what-are-the-recommended-steps>I need to change the APIs, what are the recommended steps?</h3><p>Developer should add/update the API fields at both of the following places:</p><ul><li><a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/types.go>https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/types.go</a></li><li><a href=https://github.com/gardener/machine-controller-manager/tree/master/pkg/apis/machine/v1alpha1>https://github.com/gardener/machine-controller-manager/tree/master/pkg/apis/machine/v1alpha1</a></li></ul><p>Once API changes are done, auto-generate the code using following command:</p><pre tabindex=0><code>make generate
</code></pre><p>Please ignore the API-violation errors for now.</p><h3 id=how-can-i-update-the-dependencies-of-mcm>How can I update the dependencies of MCM?</h3><p>MCM uses <code>gomod</code> for depedency management.
Developer should add/udpate depedency in the go.mod file. Please run following command to automatically revendor the dependencies.</p><pre tabindex=0><code>make revendor
</code></pre><h1 id=in-the-context-of-gardener>In the context of Gardener</h1><h3 id=how-can-i-configure-mcm-using-shoot-resource>How can I configure MCM using Shoot resource?</h3><p>All of the knobs of MCM can be configured by the <code>workers</code> <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L29-L126>section</a> of the shoot resource.</p><ul><li>Gardener creates a <code>MachineDeployment</code> per zone for each worker-pool under <code>workers</code> section.</li><li><code>workers.dataVolumes</code> allows to attach multiple disks to a machine during creation. Refer the <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L29-L126>link</a>.</li><li><code>workers.machineControllerManager</code> allows configuration of multiple knobs of the <code>MachineDeployment</code> from the shoot resource.</li></ul><h3 id=how-is-my-worker-pool-spread-across-zones>How is my worker-pool spread across zones?</h3><p>Shoot resource allows the worker-pool to spread across multiple zones using the field <code>workers.zones</code>. Refer <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L115>link</a>.</p><ul><li>Gardener creates one <code>MachineDeployment</code> per zone. Each <code>MachineDeployment</code> is initiated with the following replica:</li></ul><pre tabindex=0><code>MachineDeployment.Spec.Replicas = (Workers.Minimum)/(Number of availibility zones)
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-f95024613e3fc900eec5f206fcaf61c5>2 - etcd Druid</h1><div class=lead>An operator for etcd management in Gardener</div><h1 id=etcd-druid>ETCD Druid</h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/etcd-druid-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/etcd-druid-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/etcd-druid><img src=https://goreportcard.com/badge/github.com/gardener/gardener alt="Go Report Card"></a></p><h2 id=background>Background</h2><p><a href=https://github.com/etcd-io/etcd>Etcd</a> in the control plane of Kubernetes clusters which are managed by Gardener is deployed as a StatefulSet. The statefulset has replica of a pod containing two containers namely, etcd and <a href=https://github.com/gardener/etcd-backup-restore>backup-restore</a>. The etcd container calls components in etcd-backup-restore via REST api to perform data validation before etcd is started. If this validation fails etcd data is restored from the latest snapshot stored in the cloud-provider&rsquo;s object store. Once etcd has started, the etcd-backup-restore periodically creates full and delta snapshots. It also performs defragmentation of etcd data periodically.</p><p>The etcd-backup-restore needs as input the cloud-provider information comprising of security credentials to access the object store, the object store bucket name and prefix for the directory used to store snapshots. Currently, for operations like migration and validation, the bash script has to be updated to initiate the operation.</p><h2 id=goals>Goals</h2><ul><li>Deploy etcd and etcd-backup-restore using an etcd CRD.</li><li>Support more than one etcd replica.</li><li>Perform scheduled snapshots.</li><li>Support operations such as restores, defragmentation and scaling with zero-downtime.</li><li>Handle cloud-provider specific operation logic.</li><li>Trigger a full backup on request before volume deletion.</li><li>Offline compaction of full and delta snapshots stored in object store.</li></ul><h2 id=proposal>Proposal</h2><p>The existing method of deploying etcd and backup-sidecar as a StatefulSet alleviates the pain of ensuring the pods are live and ready after node crashes. However, deploying etcd as a Statefulset introduces a plethora of challenges. The etcd controller should be smart enough to handle etcd statefulsets taking into account limitations imposed by statefulsets. The controller shall update the status regarding how to target the K8s objects it has created. This field in the status can be leveraged by <code>HVPA</code> to scale etcd resources eventually.</p><h2 id=crd-specification>CRD specification</h2><p>The etcd CRD should contain the information required to create the etcd and backup-restore sidecar in a pod/statefulset.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>apiVersion: druid.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Etcd
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  finalizers:
</span></span><span style=display:flex><span>  - druid.gardener.cloud/etcd
</span></span><span style=display:flex><span>  name: test
</span></span><span style=display:flex><span>  namespace: demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    app: etcd-statefulset
</span></span><span style=display:flex><span>    gardener.cloud/role: controlplane
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-dns: allowed
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-private-networks: allowed
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-public-networks: allowed
</span></span><span style=display:flex><span>    role: test
</span></span><span style=display:flex><span>  backup:
</span></span><span style=display:flex><span>    deltaSnapshotMemoryLimit: 1Gi
</span></span><span style=display:flex><span>    deltaSnapshotPeriod: 300s
</span></span><span style=display:flex><span>    fullSnapshotSchedule: 0 <span style=color:#00f>*/24</span> * * *
</span></span><span style=display:flex><span>    garbageCollectionPeriod: 43200s
</span></span><span style=display:flex><span>    garbageCollectionPolicy: Exponential
</span></span><span style=display:flex><span>    imageRepository: eu.gcr.io/gardener-project/gardener/etcdbrctl
</span></span><span style=display:flex><span>    imageVersion: v0.25.0
</span></span><span style=display:flex><span>    port: 8080
</span></span><span style=display:flex><span>    resources:
</span></span><span style=display:flex><span>      limits:
</span></span><span style=display:flex><span>        cpu: 500m
</span></span><span style=display:flex><span>        memory: 2Gi
</span></span><span style=display:flex><span>      requests:
</span></span><span style=display:flex><span>        cpu: 23m
</span></span><span style=display:flex><span>        memory: 128Mi
</span></span><span style=display:flex><span>    snapstoreTempDir: /var/etcd/data/temp
</span></span><span style=display:flex><span>  etcd:
</span></span><span style=display:flex><span>    Quota: 8Gi
</span></span><span style=display:flex><span>    clientPort: 2379
</span></span><span style=display:flex><span>    defragmentationSchedule: 0 <span style=color:#00f>*/24</span> * * *
</span></span><span style=display:flex><span>    enableTLS: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>    imageRepository: eu.gcr.io/gardener-project/gardener/etcd-wrapper
</span></span><span style=display:flex><span>    imageVersion: v0.1.0
</span></span><span style=display:flex><span>    initialClusterState: new
</span></span><span style=display:flex><span>    initialClusterToken: new
</span></span><span style=display:flex><span>    metrics: basic
</span></span><span style=display:flex><span>    pullPolicy: IfNotPresent
</span></span><span style=display:flex><span>    resources:
</span></span><span style=display:flex><span>      limits:
</span></span><span style=display:flex><span>        cpu: 2500m
</span></span><span style=display:flex><span>        memory: 4Gi
</span></span><span style=display:flex><span>      requests:
</span></span><span style=display:flex><span>        cpu: 500m
</span></span><span style=display:flex><span>        memory: 1000Mi
</span></span><span style=display:flex><span>    serverPort: 2380
</span></span><span style=display:flex><span>    storageCapacity: 80Gi
</span></span><span style=display:flex><span>    storageClass: gardener.cloud-fast
</span></span><span style=display:flex><span>  sharedConfig:
</span></span><span style=display:flex><span>    autoCompactionMode: periodic
</span></span><span style=display:flex><span>    autoCompactionRetention: 30m
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: etcd-statefulset
</span></span><span style=display:flex><span>    gardener.cloud/role: controlplane
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-dns: allowed
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-private-networks: allowed
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-public-networks: allowed
</span></span><span style=display:flex><span>    role: test
</span></span><span style=display:flex><span>  pvcRetentionPolicy: DeleteAll
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  storageCapacity: 80Gi
</span></span><span style=display:flex><span>  storageClass: gardener.cloud-fast
</span></span><span style=display:flex><span>  store:
</span></span><span style=display:flex><span>    storageContainer: test
</span></span><span style=display:flex><span>    storageProvider: S3
</span></span><span style=display:flex><span>    storePrefix: etcd-test
</span></span><span style=display:flex><span>    storeSecret: etcd-backup
</span></span><span style=display:flex><span>  tlsClientSecret: etcd-client-tls
</span></span><span style=display:flex><span>  tlsServerSecret: etcd-server-tls
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  etcd:
</span></span><span style=display:flex><span>    apiVersion: apps/v1
</span></span><span style=display:flex><span>    kind: StatefulSet
</span></span><span style=display:flex><span>    name: etcd-test
</span></span></code></pre></div><h2 id=implementation-agenda>Implementation Agenda</h2><p>As first step implement defragmentation during maintenance windows. Subsequently, we will add zero-downtime upgrades and defragmentation.</p><h2 id=workflow>Workflow</h2><h3 id=deployment-workflow>Deployment workflow</h3><p><img src=/__resources/controller_989379.png alt=controller-diagram></p><h3 id=defragmentation-workflow>Defragmentation workflow</h3><p><img src=/__resources/defrag_ab05c8.png alt=defrag-diagram></p></div><div class=td-content style=page-break-before:always><h1 id=pg-0fd23e3f8def1f60476fbcc7a230dc6f>2.1 - API Reference</h1><p>Packages:</p><ul><li><a href=#druid.gardener.cloud%2fv1alpha1>druid.gardener.cloud/v1alpha1</a></li></ul><h2 id=druid.gardener.cloud/v1alpha1>druid.gardener.cloud/v1alpha1</h2><p><p>Package v1alpha1 is the v1alpha1 version of the etcd-druid API.</p></p>Resource Types:<ul></ul><h3 id=druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</a>)</p><p><p>BackupSpec defines parameters associated with the full and delta snapshots of etcd.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>port</code></br><em>int32</em></td><td><em>(Optional)</em><p>Port define the port on which etcd-backup-restore server will be exposed.</p></td></tr><tr><td><code>tls</code></br><em><a href=#druid.gardener.cloud/v1alpha1.TLSConfig>TLSConfig</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>image</code></br><em>string</em></td><td><em>(Optional)</em><p>Image defines the etcd container image and tag</p></td></tr><tr><td><code>store</code></br><em><a href=#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a></em></td><td><em>(Optional)</em><p>Store defines the specification of object store provider for storing backups.</p></td></tr><tr><td><code>resources</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcerequirements-v1-core>Kubernetes core/v1.ResourceRequirements</a></em></td><td><em>(Optional)</em><p>Resources defines compute Resources required by backup-restore container.
More info: <a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/</a></p></td></tr><tr><td><code>compactionResources</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcerequirements-v1-core>Kubernetes core/v1.ResourceRequirements</a></em></td><td><em>(Optional)</em><p>CompactionResources defines compute Resources required by compaction job.
More info: <a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/</a></p></td></tr><tr><td><code>fullSnapshotSchedule</code></br><em>string</em></td><td><em>(Optional)</em><p>FullSnapshotSchedule defines the cron standard schedule for full snapshots.</p></td></tr><tr><td><code>garbageCollectionPolicy</code></br><em><a href=#druid.gardener.cloud/v1alpha1.GarbageCollectionPolicy>GarbageCollectionPolicy</a></em></td><td><em>(Optional)</em><p>GarbageCollectionPolicy defines the policy for garbage collecting old backups</p></td></tr><tr><td><code>garbageCollectionPeriod</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>GarbageCollectionPeriod defines the period for garbage collecting old backups</p></td></tr><tr><td><code>deltaSnapshotPeriod</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>DeltaSnapshotPeriod defines the period after which delta snapshots will be taken</p></td></tr><tr><td><code>deltaSnapshotMemoryLimit</code></br><em>k8s.io/apimachinery/pkg/api/resource.Quantity</em></td><td><em>(Optional)</em><p>DeltaSnapshotMemoryLimit defines the memory limit after which delta snapshots will be taken</p></td></tr><tr><td><code>compression</code></br><em><a href=#druid.gardener.cloud/v1alpha1.CompressionSpec>CompressionSpec</a></em></td><td><em>(Optional)</em><p>SnapshotCompression defines the specification for compression of Snapshots.</p></td></tr><tr><td><code>enableProfiling</code></br><em>bool</em></td><td><em>(Optional)</em><p>EnableProfiling defines if profiling should be enabled for the etcd-backup-restore-sidecar</p></td></tr><tr><td><code>etcdSnapshotTimeout</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>EtcdSnapshotTimeout defines the timeout duration for etcd FullSnapshot operation</p></td></tr><tr><td><code>leaderElection</code></br><em><a href=#druid.gardener.cloud/v1alpha1.LeaderElectionSpec>LeaderElectionSpec</a></em></td><td><em>(Optional)</em><p>LeaderElection defines parameters related to the LeaderElection configuration.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.ClientService>ClientService</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</a>)</p><p><p>ClientService defines the parameters of the client service that a user can specify</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>annotations</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Annotations specify the annotations that should be added to the client service</p></td></tr><tr><td><code>labels</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Labels specify the labels that should be added to the client service</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.CompactionMode>CompactionMode
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.SharedConfig>SharedConfig</a>)</p><p><p>CompactionMode defines the auto-compaction-mode: &lsquo;periodic&rsquo; or &lsquo;revision&rsquo;.
&lsquo;periodic&rsquo; for duration based retention and &lsquo;revision&rsquo; for revision number based retention.</p></p><h3 id=druid.gardener.cloud/v1alpha1.CompressionPolicy>CompressionPolicy
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.CompressionSpec>CompressionSpec</a>)</p><p><p>CompressionPolicy defines the type of policy for compression of snapshots.</p></p><h3 id=druid.gardener.cloud/v1alpha1.CompressionSpec>CompressionSpec</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a>)</p><p><p>CompressionSpec defines parameters related to compression of Snapshots(full as well as delta).</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>enabled</code></br><em>bool</em></td><td><em>(Optional)</em></td></tr><tr><td><code>policy</code></br><em><a href=#druid.gardener.cloud/v1alpha1.CompressionPolicy>CompressionPolicy</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.Condition>Condition</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskStatus>EtcdCopyBackupsTaskStatus</a>,
<a href=#druid.gardener.cloud/v1alpha1.EtcdStatus>EtcdStatus</a>)</p><p><p>Condition holds the information about the state of a resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></br><em><a href=#druid.gardener.cloud/v1alpha1.ConditionType>ConditionType</a></em></td><td><p>Type of the Etcd condition.</p></td></tr><tr><td><code>status</code></br><em><a href=#druid.gardener.cloud/v1alpha1.ConditionStatus>ConditionStatus</a></em></td><td><p>Status of the condition, one of True, False, Unknown.</p></td></tr><tr><td><code>lastTransitionTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last time the condition transitioned from one status to another.</p></td></tr><tr><td><code>lastUpdateTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last time the condition was updated.</p></td></tr><tr><td><code>reason</code></br><em>string</em></td><td><p>The reason for the condition&rsquo;s last transition.</p></td></tr><tr><td><code>message</code></br><em>string</em></td><td><p>A human-readable message indicating details about the transition.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.ConditionStatus>ConditionStatus
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.Condition>Condition</a>)</p><p><p>ConditionStatus is the status of a condition.</p></p><h3 id=druid.gardener.cloud/v1alpha1.ConditionType>ConditionType
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.Condition>Condition</a>)</p><p><p>ConditionType is the type of condition.</p></p><h3 id=druid.gardener.cloud/v1alpha1.CrossVersionObjectReference>CrossVersionObjectReference</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.EtcdStatus>EtcdStatus</a>)</p><p><p>CrossVersionObjectReference contains enough information to let you identify the referred resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>kind</code></br><em>string</em></td><td><p>Kind of the referent</p></td></tr><tr><td><code>name</code></br><em>string</em></td><td><p>Name of the referent</p></td></tr><tr><td><code>apiVersion</code></br><em>string</em></td><td><em>(Optional)</em><p>API version of the referent</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.Etcd>Etcd</h3><p><p>Etcd is the Schema for the etcds API</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</a></em></td><td><br><br><table><tr><td><code>selector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><p>selector is a label query over pods that should match the replica count.
It must match the pod template&rsquo;s labels.
More info: <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors>https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors</a></p></td></tr><tr><td><code>labels</code></br><em>map[string]string</em></td><td></td></tr><tr><td><code>annotations</code></br><em>map[string]string</em></td><td><em>(Optional)</em></td></tr><tr><td><code>etcd</code></br><em><a href=#druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</a></em></td><td></td></tr><tr><td><code>backup</code></br><em><a href=#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a></em></td><td></td></tr><tr><td><code>sharedConfig</code></br><em><a href=#druid.gardener.cloud/v1alpha1.SharedConfig>SharedConfig</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>schedulingConstraints</code></br><em><a href=#druid.gardener.cloud/v1alpha1.SchedulingConstraints>SchedulingConstraints</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>replicas</code></br><em>int32</em></td><td></td></tr><tr><td><code>priorityClassName</code></br><em>string</em></td><td><em>(Optional)</em><p>PriorityClassName is the name of a priority class that shall be used for the etcd pods.</p></td></tr><tr><td><code>storageClass</code></br><em>string</em></td><td><em>(Optional)</em><p>StorageClass defines the name of the StorageClass required by the claim.
More info: <a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1>https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1</a></p></td></tr><tr><td><code>storageCapacity</code></br><em>k8s.io/apimachinery/pkg/api/resource.Quantity</em></td><td><em>(Optional)</em><p>StorageCapacity defines the size of persistent volume.</p></td></tr><tr><td><code>volumeClaimTemplate</code></br><em>string</em></td><td><em>(Optional)</em><p>VolumeClaimTemplate defines the volume claim template to be created</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#druid.gardener.cloud/v1alpha1.EtcdStatus>EtcdStatus</a></em></td><td></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</a>)</p><p><p>EtcdConfig defines parameters associated etcd deployed</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>quota</code></br><em>k8s.io/apimachinery/pkg/api/resource.Quantity</em></td><td><em>(Optional)</em><p>Quota defines the etcd DB quota.</p></td></tr><tr><td><code>defragmentationSchedule</code></br><em>string</em></td><td><em>(Optional)</em><p>DefragmentationSchedule defines the cron standard schedule for defragmentation of etcd.</p></td></tr><tr><td><code>serverPort</code></br><em>int32</em></td><td><em>(Optional)</em></td></tr><tr><td><code>clientPort</code></br><em>int32</em></td><td><em>(Optional)</em></td></tr><tr><td><code>image</code></br><em>string</em></td><td><em>(Optional)</em><p>Image defines the etcd container image and tag</p></td></tr><tr><td><code>authSecretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>metrics</code></br><em><a href=#druid.gardener.cloud/v1alpha1.MetricsLevel>MetricsLevel</a></em></td><td><em>(Optional)</em><p>Metrics defines the level of detail for exported metrics of etcd, specify &lsquo;extensive&rsquo; to include histogram metrics.</p></td></tr><tr><td><code>resources</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcerequirements-v1-core>Kubernetes core/v1.ResourceRequirements</a></em></td><td><em>(Optional)</em><p>Resources defines the compute Resources required by etcd container.
More info: <a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/</a></p></td></tr><tr><td><code>clientUrlTls</code></br><em><a href=#druid.gardener.cloud/v1alpha1.TLSConfig>TLSConfig</a></em></td><td><em>(Optional)</em><p>ClientUrlTLS contains the ca, server TLS and client TLS secrets for client communication to ETCD cluster</p></td></tr><tr><td><code>peerUrlTls</code></br><em><a href=#druid.gardener.cloud/v1alpha1.TLSConfig>TLSConfig</a></em></td><td><em>(Optional)</em><p>PeerUrlTLS contains the ca and server TLS secrets for peer communication within ETCD cluster
Currently, PeerUrlTLS does not require client TLS secrets for gardener implementation of ETCD cluster.</p></td></tr><tr><td><code>etcdDefragTimeout</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>EtcdDefragTimeout defines the timeout duration for etcd defrag call</p></td></tr><tr><td><code>heartbeatDuration</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>HeartbeatDuration defines the duration for members to send heartbeats. The default value is 10s.</p></td></tr><tr><td><code>clientService</code></br><em><a href=#druid.gardener.cloud/v1alpha1.ClientService>ClientService</a></em></td><td><em>(Optional)</em><p>ClientService defines the parameters of the client service that a user can specify</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTask>EtcdCopyBackupsTask</h3><p><p>EtcdCopyBackupsTask is a task for copying etcd backups from a source to a target store.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec>EtcdCopyBackupsTaskSpec</a></em></td><td><br><br><table><tr><td><code>sourceStore</code></br><em><a href=#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a></em></td><td><p>SourceStore defines the specification of the source object store provider for storing backups.</p></td></tr><tr><td><code>targetStore</code></br><em><a href=#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a></em></td><td><p>TargetStore defines the specification of the target object store provider for storing backups.</p></td></tr><tr><td><code>maxBackupAge</code></br><em>uint32</em></td><td><em>(Optional)</em><p>MaxBackupAge is the maximum age in days that a backup must have in order to be copied.
By default all backups will be copied.</p></td></tr><tr><td><code>maxBackups</code></br><em>uint32</em></td><td><em>(Optional)</em><p>MaxBackups is the maximum number of backups that will be copied starting with the most recent ones.</p></td></tr><tr><td><code>waitForFinalSnapshot</code></br><em><a href=#druid.gardener.cloud/v1alpha1.WaitForFinalSnapshotSpec>WaitForFinalSnapshotSpec</a></em></td><td><em>(Optional)</em><p>WaitForFinalSnapshot defines the parameters for waiting for a final full snapshot before copying backups.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskStatus>EtcdCopyBackupsTaskStatus</a></em></td><td></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec>EtcdCopyBackupsTaskSpec</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTask>EtcdCopyBackupsTask</a>)</p><p><p>EtcdCopyBackupsTaskSpec defines the parameters for the copy backups task.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>sourceStore</code></br><em><a href=#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a></em></td><td><p>SourceStore defines the specification of the source object store provider for storing backups.</p></td></tr><tr><td><code>targetStore</code></br><em><a href=#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a></em></td><td><p>TargetStore defines the specification of the target object store provider for storing backups.</p></td></tr><tr><td><code>maxBackupAge</code></br><em>uint32</em></td><td><em>(Optional)</em><p>MaxBackupAge is the maximum age in days that a backup must have in order to be copied.
By default all backups will be copied.</p></td></tr><tr><td><code>maxBackups</code></br><em>uint32</em></td><td><em>(Optional)</em><p>MaxBackups is the maximum number of backups that will be copied starting with the most recent ones.</p></td></tr><tr><td><code>waitForFinalSnapshot</code></br><em><a href=#druid.gardener.cloud/v1alpha1.WaitForFinalSnapshotSpec>WaitForFinalSnapshotSpec</a></em></td><td><em>(Optional)</em><p>WaitForFinalSnapshot defines the parameters for waiting for a final full snapshot before copying backups.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskStatus>EtcdCopyBackupsTaskStatus</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTask>EtcdCopyBackupsTask</a>)</p><p><p>EtcdCopyBackupsTaskStatus defines the observed state of the copy backups task.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>conditions</code></br><em><a href=#druid.gardener.cloud/v1alpha1.Condition>[]Condition</a></em></td><td><em>(Optional)</em><p>Conditions represents the latest available observations of an object&rsquo;s current state.</p></td></tr><tr><td><code>observedGeneration</code></br><em>int64</em></td><td><em>(Optional)</em><p>ObservedGeneration is the most recent generation observed for this resource.</p></td></tr><tr><td><code>lastError</code></br><em>string</em></td><td><em>(Optional)</em><p>LastError represents the last occurred error.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdMemberConditionStatus>EtcdMemberConditionStatus
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.EtcdMemberStatus>EtcdMemberStatus</a>)</p><p><p>EtcdMemberConditionStatus is the status of an etcd cluster member.</p></p><h3 id=druid.gardener.cloud/v1alpha1.EtcdMemberStatus>EtcdMemberStatus</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.EtcdStatus>EtcdStatus</a>)</p><p><p>EtcdMemberStatus holds information about a etcd cluster membership.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of the etcd member. It is the name of the backing <code>Pod</code>.</p></td></tr><tr><td><code>id</code></br><em>string</em></td><td><em>(Optional)</em><p>ID is the ID of the etcd member.</p></td></tr><tr><td><code>role</code></br><em><a href=#druid.gardener.cloud/v1alpha1.EtcdRole>EtcdRole</a></em></td><td><em>(Optional)</em><p>Role is the role in the etcd cluster, either <code>Leader</code> or <code>Member</code>.</p></td></tr><tr><td><code>status</code></br><em><a href=#druid.gardener.cloud/v1alpha1.EtcdMemberConditionStatus>EtcdMemberConditionStatus</a></em></td><td><p>Status of the condition, one of True, False, Unknown.</p></td></tr><tr><td><code>reason</code></br><em>string</em></td><td><p>The reason for the condition&rsquo;s last transition.</p></td></tr><tr><td><code>lastTransitionTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>LastTransitionTime is the last time the condition&rsquo;s status changed.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdRole>EtcdRole
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.EtcdMemberStatus>EtcdMemberStatus</a>)</p><p><p>EtcdRole is the role of an etcd cluster member.</p></p><h3 id=druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.Etcd>Etcd</a>)</p><p><p>EtcdSpec defines the desired state of Etcd</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>selector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><p>selector is a label query over pods that should match the replica count.
It must match the pod template&rsquo;s labels.
More info: <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors>https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors</a></p></td></tr><tr><td><code>labels</code></br><em>map[string]string</em></td><td></td></tr><tr><td><code>annotations</code></br><em>map[string]string</em></td><td><em>(Optional)</em></td></tr><tr><td><code>etcd</code></br><em><a href=#druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</a></em></td><td></td></tr><tr><td><code>backup</code></br><em><a href=#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a></em></td><td></td></tr><tr><td><code>sharedConfig</code></br><em><a href=#druid.gardener.cloud/v1alpha1.SharedConfig>SharedConfig</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>schedulingConstraints</code></br><em><a href=#druid.gardener.cloud/v1alpha1.SchedulingConstraints>SchedulingConstraints</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>replicas</code></br><em>int32</em></td><td></td></tr><tr><td><code>priorityClassName</code></br><em>string</em></td><td><em>(Optional)</em><p>PriorityClassName is the name of a priority class that shall be used for the etcd pods.</p></td></tr><tr><td><code>storageClass</code></br><em>string</em></td><td><em>(Optional)</em><p>StorageClass defines the name of the StorageClass required by the claim.
More info: <a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1>https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1</a></p></td></tr><tr><td><code>storageCapacity</code></br><em>k8s.io/apimachinery/pkg/api/resource.Quantity</em></td><td><em>(Optional)</em><p>StorageCapacity defines the size of persistent volume.</p></td></tr><tr><td><code>volumeClaimTemplate</code></br><em>string</em></td><td><em>(Optional)</em><p>VolumeClaimTemplate defines the volume claim template to be created</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdStatus>EtcdStatus</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.Etcd>Etcd</a>)</p><p><p>EtcdStatus defines the observed state of Etcd.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>observedGeneration</code></br><em>int64</em></td><td><em>(Optional)</em><p>ObservedGeneration is the most recent generation observed for this resource.</p></td></tr><tr><td><code>etcd</code></br><em><a href=#druid.gardener.cloud/v1alpha1.CrossVersionObjectReference>CrossVersionObjectReference</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>conditions</code></br><em><a href=#druid.gardener.cloud/v1alpha1.Condition>[]Condition</a></em></td><td><em>(Optional)</em><p>Conditions represents the latest available observations of an etcd&rsquo;s current state.</p></td></tr><tr><td><code>serviceName</code></br><em>string</em></td><td><em>(Optional)</em><p>ServiceName is the name of the etcd service.</p></td></tr><tr><td><code>lastError</code></br><em>string</em></td><td><em>(Optional)</em><p>LastError represents the last occurred error.</p></td></tr><tr><td><code>clusterSize</code></br><em>int32</em></td><td><em>(Optional)</em><p>Cluster size is the size of the etcd cluster.</p></td></tr><tr><td><code>currentReplicas</code></br><em>int32</em></td><td><em>(Optional)</em><p>CurrentReplicas is the current replica count for the etcd cluster.</p></td></tr><tr><td><code>replicas</code></br><em>int32</em></td><td><em>(Optional)</em><p>Replicas is the replica count of the etcd resource.</p></td></tr><tr><td><code>readyReplicas</code></br><em>int32</em></td><td><em>(Optional)</em><p>ReadyReplicas is the count of replicas being ready in the etcd cluster.</p></td></tr><tr><td><code>ready</code></br><em>bool</em></td><td><em>(Optional)</em><p>Ready is <code>true</code> if all etcd replicas are ready.</p></td></tr><tr><td><code>updatedReplicas</code></br><em>int32</em></td><td><em>(Optional)</em><p>UpdatedReplicas is the count of updated replicas in the etcd cluster.</p></td></tr><tr><td><code>labelSelector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em><p>LabelSelector is a label query over pods that should match the replica count.
It must match the pod template&rsquo;s labels.</p></td></tr><tr><td><code>members</code></br><em><a href=#druid.gardener.cloud/v1alpha1.EtcdMemberStatus>[]EtcdMemberStatus</a></em></td><td><em>(Optional)</em><p>Members represents the members of the etcd cluster</p></td></tr><tr><td><code>peerUrlTLSEnabled</code></br><em>bool</em></td><td><em>(Optional)</em><p>PeerUrlTLSEnabled captures the state of peer url TLS being enabled for the etcd member(s)</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.GarbageCollectionPolicy>GarbageCollectionPolicy
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a>)</p><p><p>GarbageCollectionPolicy defines the type of policy for snapshot garbage collection.</p></p><h3 id=druid.gardener.cloud/v1alpha1.LeaderElectionSpec>LeaderElectionSpec</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a>)</p><p><p>LeaderElectionSpec defines parameters related to the LeaderElection configuration.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>reelectionPeriod</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>ReelectionPeriod defines the Period after which leadership status of corresponding etcd is checked.</p></td></tr><tr><td><code>etcdConnectionTimeout</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>EtcdConnectionTimeout defines the timeout duration for etcd client connection during leader election.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.MetricsLevel>MetricsLevel
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</a>)</p><p><p>MetricsLevel defines the level &lsquo;basic&rsquo; or &lsquo;extensive&rsquo;.</p></p><h3 id=druid.gardener.cloud/v1alpha1.SchedulingConstraints>SchedulingConstraints</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</a>)</p><p><p>SchedulingConstraints defines the different scheduling constraints that must be applied to the
pod spec in the etcd statefulset.
Currently supported constraints are Affinity and TopologySpreadConstraints.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>affinity</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#affinity-v1-core>Kubernetes core/v1.Affinity</a></em></td><td><em>(Optional)</em><p>Affinity defines the various affinity and anti-affinity rules for a pod
that are honoured by the kube-scheduler.</p></td></tr><tr><td><code>topologySpreadConstraints</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#topologyspreadconstraint-v1-core>[]Kubernetes core/v1.TopologySpreadConstraint</a></em></td><td><em>(Optional)</em><p>TopologySpreadConstraints describes how a group of pods ought to spread across topology domains,
that are honoured by the kube-scheduler.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.SecretReference>SecretReference</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.TLSConfig>TLSConfig</a>)</p><p><p>SecretReference defines a reference to a secret.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>SecretReference</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>(Members of <code>SecretReference</code> are embedded into this type.)</p></td></tr><tr><td><code>dataKey</code></br><em>string</em></td><td><em>(Optional)</em><p>DataKey is the name of the key in the data map containing the credentials.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.SharedConfig>SharedConfig</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</a>)</p><p><p>SharedConfig defines parameters shared and used by Etcd as well as backup-restore sidecar.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>autoCompactionMode</code></br><em><a href=#druid.gardener.cloud/v1alpha1.CompactionMode>CompactionMode</a></em></td><td><em>(Optional)</em><p>AutoCompactionMode defines the auto-compaction-mode:&lsquo;periodic&rsquo; mode or &lsquo;revision&rsquo; mode for etcd and embedded-Etcd of backup-restore sidecar.</p></td></tr><tr><td><code>autoCompactionRetention</code></br><em>string</em></td><td><em>(Optional)</em><p>AutoCompactionRetention defines the auto-compaction-retention length for etcd as well as for embedded-Etcd of backup-restore sidecar.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.StorageProvider>StorageProvider
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a>)</p><p><p>StorageProvider defines the type of object store provider for storing backups.</p></p><h3 id=druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a>,
<a href=#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec>EtcdCopyBackupsTaskSpec</a>)</p><p><p>StoreSpec defines parameters related to ObjectStore persisting backups</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>container</code></br><em>string</em></td><td><em>(Optional)</em><p>Container is the name of the container the backup is stored at.</p></td></tr><tr><td><code>prefix</code></br><em>string</em></td><td><p>Prefix is the prefix used for the store.</p></td></tr><tr><td><code>provider</code></br><em><a href=#druid.gardener.cloud/v1alpha1.StorageProvider>StorageProvider</a></em></td><td><em>(Optional)</em><p>Provider is the name of the backup provider.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><em>(Optional)</em><p>SecretRef is the reference to the secret which used to connect to the backup store.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.TLSConfig>TLSConfig</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a>,
<a href=#druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</a>)</p><p><p>TLSConfig hold the TLS configuration details.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>tlsCASecretRef</code></br><em><a href=#druid.gardener.cloud/v1alpha1.SecretReference>SecretReference</a></em></td><td></td></tr><tr><td><code>serverTLSSecretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>clientTLSSecretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.WaitForFinalSnapshotSpec>WaitForFinalSnapshotSpec</h3><p>(<em>Appears on:</em>
<a href=#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec>EtcdCopyBackupsTaskSpec</a>)</p><p><p>WaitForFinalSnapshotSpec defines the parameters for waiting for a final full snapshot before copying backups.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>enabled</code></br><em>bool</em></td><td><p>Enabled specifies whether to wait for a final full snapshot before copying backups.</p></td></tr><tr><td><code>timeout</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>Timeout is the timeout for waiting for a final full snapshot. When this timeout expires, the copying of backups
will be performed anyway. No timeout or 0 means wait forever.</p></td></tr></tbody></table><hr><p><em>Generated with <a href=https://github.com/ahmetb/gen-crd-api-reference-docs>gen-crd-api-reference-docs</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-3103ec847060071588637bc1f2a8d3a0>2.2 - Development</h1></div><div class=td-content><h1 id=pg-0c90938490d04d322f741df0488294f7>2.2.1 - etcd Network Latency</h1><h1 id=network-latency-analysis-sn-etcd-sz-vs--mn-etcd-sz-vs-mn-etcd-mz>Network Latency analysis: <code>sn-etcd-sz</code> vs <code>mn-etcd-sz</code> vs <code>mn-etcd-mz</code></h1><p>This page captures the ETCD cluster latency analysis for below scenarios using the benchmark tool (build from <a href=https://github.com/seshachalam-yv/etcd>ETCD benchmark tool</a>).</p><p><code>sn-etcd-sz</code> -> single-node ETCD single zone (Only single replica of etcd will be running)</p><p><code>mn-etcd-sz</code> -> multi-node ETCD single zone (Multiple replicas of etcd pods will be running across nodes in a single zone)</p><p><code>mn-etcd-mz</code> -> multi-node ETCD multi zone (Multiple replicas of etcd pods will be running across nodes in multiple zones)</p><h2 id=put-analysis>PUT Analysis</h2><h3 id=summary>Summary</h3><ul><li><code>sn-etcd-sz</code> latency is <strong>~20% less than</strong> <code>mn-etcd-sz</code> when benchmark tool with single client.</li><li><code>mn-etcd-sz</code> latency is less than <code>mn-etcd-mz</code> but the difference is <code>~+/-5%</code>.</li><li>Compared to <code>mn-etcd-sz</code>, <code>sn-etcd-sz</code> latency is higher and gradually grows with more clients and larger value size.</li><li>Compared to <code>mn-etcd-mz</code>, <code>mn-etcd-sz</code> latency is higher and gradually grows with more clients and larger value size.</li><li><em>Compared to <code>follower</code>, <code>leader</code> latency is less</em>, when benchmark tool with single client for all cases.</li><li><em>Compared to <code>follower</code>, <code>leader</code> latency is high</em>, when benchmark tool with multiple clients for all cases.</li></ul><p>Sample commands:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># write to leader</span>
</span></span><span style=display:flex><span>benchmark put --target-leader --conns=1 --clients=1 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0 --val-size=256 --total=10000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_HOST 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># write to follower</span>
</span></span><span style=display:flex><span>benchmark put  --conns=1 --clients=1 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0 --val-size=256 --total=10000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_FOLLOWER_HOST
</span></span></code></pre></div><h3 id=latency-analysis-during-put-requests-to-etcd>Latency analysis during PUT requests to ETCD</h3><ul><li><details><summary>In this case benchmark tool tries to put key with random 256 bytes value.</summary><ul><li><p>Benchmark tool loads key/value to <code>leader</code> with single client .</p><ul><li><code>sn-etcd-sz</code> latency (~0.815ms) is <strong>~50% lesser than</strong> <code>mn-etcd-sz</code> (~1.74ms ).</li><li><ul><li><code>mn-etcd-sz</code> latency (~1.74ms ) is slightly lesser than <code>mn-etcd-mz</code> (~1.8ms) but the difference is negligible (within same ms).</li></ul></li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>1220.0520</td><td style=text-align:center>0.815ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>586.545</td><td style=text-align:center>1.74ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>554.0155654442634</td><td style=text-align:center>1.8ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value to <code>follower</code> with single client.</p><ul><li><code>mn-etcd-sz</code> latency(<code>~2.2ms</code>) is <strong>20% to 30% lesser than</strong> <code>mn-etcd-mz</code>(<code>~2.7ms</code>).</li><li><em>Compare to <code>follower</code>, <code>leader</code> has lower latency.</em></li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-1</td><td style=text-align:center>445.743</td><td style=text-align:center>2.23ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-1</td><td style=text-align:center>378.9366747610789</td><td style=text-align:center>2.63ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>457.967</td><td style=text-align:center>2.17ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>345.6586129825796</td><td style=text-align:center>2.89ms</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value to <code>leader</code> with multiple clients.</p><ul><li><code>sn-etcd-sz</code> latency(<code>~78.3ms</code>) is <strong>~10% greater than</strong> <code>mn-etcd-sz</code>(<code>~71.81ms</code>).</li><li><code>mn-etcd-sz</code> latency(<code>~71.81ms</code>) is less than <code>mn-etcd-mz</code>(<code>~72.5ms</code>) but the difference is negligible.</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>leader</td><td style=text-align:center>12638.905</td><td style=text-align:center>78.32ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>leader</td><td style=text-align:center>13789.248</td><td style=text-align:center>71.81ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>leader</td><td style=text-align:center>13728.446436395223</td><td style=text-align:center>72.5ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value to <code>follower</code> with multiple clients.</p><ul><li><code>mn-etcd-sz</code> latency(<code>~69.8ms</code>) is <strong>~5% greater than</strong> <code>mn-etcd-mz</code>(<code>~72.6ms</code>).</li><li><em>Compare to <code>leader</code>, <code>follower</code> has lower latency</em>.</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>follower-1</td><td style=text-align:center>14271.983</td><td style=text-align:center>69.80ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>follower-1</td><td style=text-align:center>13695.98</td><td style=text-align:center>72.62ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>follower-2</td><td style=text-align:center>14325.436</td><td style=text-align:center>69.47ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>follower-2</td><td style=text-align:center>15750.409490407475</td><td style=text-align:center>63.3ms</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li></ul></details></li><li><details><summary>In this case benchmark tool tries to put key with random 1 MB value.</summary><ul><li><p>Benchmark tool loads key/value to <code>leader</code> with single client.</p><ul><li><code>sn-etcd-sz</code> latency(<code>~16.35ms</code>) is <strong>~20% lesser than</strong> <code>mn-etcd-sz</code>(<code>~20.64ms</code>).</li><li><code>mn-etcd-sz</code> latency(<code>~20.64ms</code>) is less than <code>mn-etcd-mz</code>(<code>~21.08ms</code>) but the difference is negligible..</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>61.117</td><td style=text-align:center>16.35ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>48.416</td><td style=text-align:center>20.64ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>45.7517341664802</td><td style=text-align:center>21.08ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value withto <code>follower</code> single client.</p><ul><li><code>mn-etcd-sz</code> latency(<code>~23.10ms</code>) is <strong>~10% greater than</strong> <code>mn-etcd-mz</code>(<code>~21.8ms</code>).</li><li><em>Compare to <code>follower</code>, <code>leader</code> has lower latency</em>.</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-1</td><td style=text-align:center>43.261</td><td style=text-align:center>23.10ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-1</td><td style=text-align:center>45.7517341664802</td><td style=text-align:center>21.8ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-1</td><td style=text-align:center>45.33</td><td style=text-align:center>22.05ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>40.0518</td><td style=text-align:center>24.95ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>43.28573155709838</td><td style=text-align:center>23.09ms</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>45.92</td><td style=text-align:center>21.76ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>35.5705</td><td style=text-align:center>28.1ms</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value to <code>leader</code> with multiple clients.</p><ul><li><code>sn-etcd-sz</code> latency(<code>~6.0375secs</code>) is <strong>~30% greater than</strong> <code>mn-etcd-sz``~4.000secs</code>).</li><li><code>mn-etcd-sz</code> latency(<code>~4.000secs</code>) is less than <code>mn-etcd-mz</code>(<code>~ 4.09secs</code>) but the difference is negligible.</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>leader</td><td style=text-align:center>55.373</td><td style=text-align:center>6.0375secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>leader</td><td style=text-align:center>67.319</td><td style=text-align:center>4.000secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>leader</td><td style=text-align:center>65.91914167957594</td><td style=text-align:center>4.09secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value to <code>follower</code> with multiple clients.</p><ul><li><em><code>mn-etcd-sz</code> latency(<code>~4.04secs</code>) is <strong>~5% greater than</strong> <code>mn-etcd-mz</code>(<code>~ 3.90secs</code>).</em></li><li><em>Compare to <code>leader</code>, <code>follower</code> has lower latency</em>.</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-1</td><td style=text-align:center>66.528</td><td style=text-align:center>4.0417secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-1</td><td style=text-align:center>70.6493461856332</td><td style=text-align:center>3.90secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-1</td><td style=text-align:center>71.95</td><td style=text-align:center>3.84secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-2</td><td style=text-align:center>66.447</td><td style=text-align:center>4.0164secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-2</td><td style=text-align:center>67.53038086369484</td><td style=text-align:center>3.87secs</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-2</td><td style=text-align:center>68.46</td><td style=text-align:center>3.92secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li></ul></details></li></ul><hr><br><h2 id=range-analysis>Range Analysis</h2><p>Sample commands are:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># Single connection read request with sequential keys</span>
</span></span><span style=display:flex><span>benchmark range 0 --target-leader --conns=1 --clients=1 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0  --total=10000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --consistency=l <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_HOST 
</span></span><span style=display:flex><span><span style=color:green># --consistency=s [Serializable]</span>
</span></span><span style=display:flex><span>benchmark range 0 --target-leader --conns=1 --clients=1 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0  --total=10000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --consistency=s <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_HOST 
</span></span><span style=display:flex><span><span style=color:green># Each read request with range query matches key 0 9999 and repeats for total number of requests.  </span>
</span></span><span style=display:flex><span>benchmark range 0 9999 --target-leader --conns=1 --clients=1 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --total=10 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --consistency=s <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=https://etcd-main-client:2379
</span></span><span style=display:flex><span><span style=color:green># Read requests with multiple connections</span>
</span></span><span style=display:flex><span>benchmark range 0 --target-leader --conns=100 --clients=1000 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0  --total=100000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --consistency=l <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_HOST 
</span></span><span style=display:flex><span>benchmark range 0 --target-leader --conns=100 --clients=1000 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0  --total=100000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --consistency=s <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_HOST 
</span></span></code></pre></div><h3 id=latency-analysis-during-range-requests-to-etcd>Latency analysis during Range requests to ETCD</h3><ul><li><details><summary>In this case benchmark tool tries to get specific key with random 256 bytes value.</summary><ul><li><p>Benchmark tool range requests to <code>leader</code> with single client.</p><ul><li><p><code>sn-etcd-sz</code> latency(<code>~1.24ms</code>) is <strong>~40% greater than</strong> <code>mn-etcd-sz</code>(<code>~0.67ms</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~0.67ms</code>) is <strong>~20% lesser than</strong> <code>mn-etcd-mz</code>(<code>~0.85ms</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>800.272</td><td style=text-align:center>1.24ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>1173.9081</td><td style=text-align:center>0.67ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>999.3020189178693</td><td style=text-align:center>0.85ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~40% less</strong> for all cases</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>1411.229</td><td style=text-align:center>0.70ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>2033.131</td><td style=text-align:center>0.35ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>2100.2426362012025</td><td style=text-align:center>0.47ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>follower</code> with single client .</p><ul><li><code>mn-etcd-sz</code> latency(<code>~1.3ms</code>) is <strong>~20% lesser than</strong> <code>mn-etcd-mz</code>(<code>~1.6ms</code>).</li><li><em>Compare to <code>follower</code>, <code>leader</code> read request latency is <strong>~50% less</strong> for both <code>mn-etcd-sz</code>, <code>mn-etcd-mz</code></em></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>765.325</td><td style=text-align:center>1.3ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>596.1</td><td style=text-align:center>1.6ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~50% less</strong> for all cases</li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>1823.631</td><td style=text-align:center>0.54ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>1442.6</td><td style=text-align:center>0.69ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>1416.39</td><td style=text-align:center>0.70ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>2077.449</td><td style=text-align:center>0.47ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>leader</code> with multiple client.</p><ul><li><p><code>sn-etcd-sz</code> latency(<code>~84.66ms</code>) is <strong>~20% greater than</strong> <code>mn-etcd-sz</code>(<code>~73.95ms</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~73.95ms</code>) is <strong>more or less equal to</strong> <code>mn-etcd-mz</code>(<code>~ 73.8ms</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>11775.721</td><td style=text-align:center>84.66ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>13446.9598</td><td style=text-align:center>73.95ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>13527.19810605353</td><td style=text-align:center>73.8ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~20% lesser</strong> for all cases</p></li><li><p><code>sn-etcd-sz</code> latency(<code>~69.37ms</code>) is <strong>more or less equal to</strong> <code>mn-etcd-sz</code>(<code>~69.89ms</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~69.89ms</code>) is <strong>slightly higher than</strong> <code>mn-etcd-mz</code>(<code>~67.63ms</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>14334.9027</td><td style=text-align:center>69.37ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>14270.008</td><td style=text-align:center>69.89ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>14715.287354023869</td><td style=text-align:center>67.63ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>follower</code> with multiple client.</p><ul><li><p><code>mn-etcd-sz</code> latency(<code>~60.69ms</code>) is <strong>~20% lesser than</strong> <code>mn-etcd-mz</code>(<code>~70.76ms</code>).</p></li><li><p>Compare to <code>leader</code>, <code>follower</code> has lower read request latency.</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>11586.032</td><td style=text-align:center>60.69ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>14050.5</td><td style=text-align:center>70.76ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p><code>mn-etcd-sz</code> latency(<code>~86.09ms</code>) is <strong>~20 higher than</strong> <code>mn-etcd-mz</code>(<code>~64.6ms</code>).</p></li><li><ul><li>Compare to <code>mn-etcd-sz</code> consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~20% higher</strong>.*</li></ul></li><li><p>Compare to <code>mn-etcd-mz</code> consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~slightly less</strong>.</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>11582.438</td><td style=text-align:center>86.09ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>15422.2</td><td style=text-align:center>64.6ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>leader</code> all keys.</p><ul><li><p><code>sn-etcd-sz</code> latency(<code>~678.77ms</code>) is <strong>~5% slightly lesser than</strong> <code>mn-etcd-sz</code>(<code>~697.29ms</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~697.29ms</code>) is less than <code>mn-etcd-mz</code>(<code>~701ms</code>) but the difference is negligible.</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>6.8875</td><td style=text-align:center>678.77ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>6.720</td><td style=text-align:center>697.29ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>6.7</td><td style=text-align:center>701ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><ul><li>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~5% slightly higher</strong> for all cases</li></ul></li><li><p><code>sn-etcd-sz</code> latency(<code>~687.36ms</code>) is less than <code>mn-etcd-sz</code>(<code>~692.68ms</code>) but the difference is negligible.</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~692.68ms</code>) is <strong>~5% slightly lesser than</strong> <code>mn-etcd-mz</code>(<code>~735.7ms</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>6.76</td><td style=text-align:center>687.36ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>6.635</td><td style=text-align:center>692.68ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>6.3</td><td style=text-align:center>735.7ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>follower</code> all keys</p><ul><li><p><code>mn-etcd-sz</code>(<code>~737.68ms</code>) latency is <strong>~5% slightly higher than</strong> <code>mn-etcd-mz</code>(<code>~713.7ms</code>).</p></li><li><p>Compare to <code>leader</code> consistency <code>Linearizable</code>read request, <code>follower</code> is <em>~5% slightly higher</em>.</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>6.163</td><td style=text-align:center>737.68ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>6.52</td><td style=text-align:center>713.7ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p><code>mn-etcd-sz</code> latency(<code>~757.73ms</code>) is <strong>~10% higher than</strong> <code>mn-etcd-mz</code>(<code>~690.4ms</code>).</p></li><li><p>Compare to <code>follower</code> consistency <code>Linearizable</code>read request, <code>follower</code> consistency <code>Serializable</code> is <em>~3% slightly higher</em> for <code>mn-etcd-sz</code>.</p></li><li><p><em>Compare to <code>follower</code> consistency <code>Linearizable</code>read request, <code>follower</code> consistency <code>Serializable</code> is <em>~5% less</em> for <code>mn-etcd-mz</code>.</em></p></li><li><p>*Compare to <code>leader</code> consistency <code>Serializable</code>read request, <code>follower</code> consistency <code>Serializable</code> is <em>~5% less</em> for <code>mn-etcd-mz</code>. *</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>6.0295</td><td style=text-align:center>757.73ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>6.87</td><td style=text-align:center>690.4ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li></ul><hr><br></details></li><li><details><summary>In this case benchmark tool tries to get specific key with random `1MB` value.</summary><ul><li><p>Benchmark tool range requests to <code>leader</code> with single client.</p><ul><li><p><code>sn-etcd-sz</code> latency(<code>~5.96ms</code>) is <strong>~5% lesser than</strong> <code>mn-etcd-sz</code>(<code>~6.28ms</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~6.28ms</code>) is <strong>~10% higher than</strong> <code>mn-etcd-mz</code>(<code>~5.3ms</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>167.381</td><td style=text-align:center>5.96ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>158.822</td><td style=text-align:center>6.28ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>187.94</td><td style=text-align:center>5.3ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~15% less</strong> for <code>sn-etcd-sz</code>, <code>mn-etcd-sz</code>, <code>mn-etcd-mz</code></p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>184.95</td><td style=text-align:center>5.398ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>176.901</td><td style=text-align:center>5.64ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>209.99</td><td style=text-align:center>4.7ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>follower</code> with single client.</p><ul><li><p><code>mn-etcd-sz</code> latency(<code>~6.66ms</code>) is <strong>~10% higher than</strong> <code>mn-etcd-mz</code>(<code>~6.16ms</code>).</p></li><li><p><em>Compare to <code>leader</code>, <code>follower</code> read request latency is <strong>~10% high</strong> for <code>mn-etcd-sz</code></em></p></li><li><p><em>Compare to <code>leader</code>, <code>follower</code> read request latency is <strong>~20% high</strong> for <code>mn-etcd-mz</code></em></p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>150.680</td><td style=text-align:center>6.66ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>162.072</td><td style=text-align:center>6.16ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~15% less</strong> for <code>mn-etcd-sz</code>(<code>~5.84ms</code>), <code>mn-etcd-mz</code>(<code>~5.01ms</code>).</p></li><li><p><em>Compare to <code>leader</code>, <code>follower</code> read request latency is <strong>~5% slightly high</strong> for <code>mn-etcd-sz</code>, <code>mn-etcd-mz</code></em></p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>170.918</td><td style=text-align:center>5.84ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>199.01</td><td style=text-align:center>5.01ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>leader</code> with multiple clients.</p><ul><li><code>sn-etcd-sz</code> latency(<code>~1.593secs</code>) is <strong>~20% lesser than</strong> <code>mn-etcd-sz</code>(<code>~1.974secs</code>).</li><li><code>mn-etcd-sz</code> latency(<code>~1.974secs</code>) is <strong>~5% greater than</strong> <code>mn-etcd-mz</code>(<code>~1.81secs</code>).</li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>252.149</td><td style=text-align:center>1.593secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>205.589</td><td style=text-align:center>1.974secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>230.42</td><td style=text-align:center>1.81secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p><em>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>more or less same</strong> for <code>sn-etcd-sz</code>(<code>~1.57961secs</code>), <code>mn-etcd-mz</code>(<code>~1.8secs</code>) not a big difference</em></p></li><li><p>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~10% high</strong> for <code>mn-etcd-sz</code>(<code>~ 2.277secs</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>252.406</td><td style=text-align:center>1.57961secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>181.905</td><td style=text-align:center>2.277secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>227.64</td><td style=text-align:center>1.8secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Benchmark tool range requests to <code>follower</code> with multiple client.</p><ul><li><code>mn-etcd-sz</code> latency is <strong>~20% less than</strong> <code>mn-etcd-mz</code>.</li><li>Compare to <code>leader</code> consistency <code>Linearizable</code>, <code>follower</code> read request latency is ~15 less for <code>mn-etcd-sz</code>(<code>~1.694secs</code>).</li><li>Compare to <code>leader</code> consistency <code>Linearizable</code>, <code>follower</code> read request latency is ~10% higher for <code>mn-etcd-sz</code>(<code>~1.977secs</code>).</li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>248.489</td><td style=text-align:center>1.694secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>210.22</td><td style=text-align:center>1.977secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-2</td><td style=text-align:center>205.765</td><td style=text-align:center>1.967secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-2</td><td style=text-align:center>195.2</td><td style=text-align:center>2.159secs</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table><ul><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>231.458</td><td style=text-align:center>1.7413secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>214.80</td><td style=text-align:center>1.907secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-2</td><td style=text-align:center>183.320</td><td style=text-align:center>2.2810secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-2</td><td style=text-align:center>195.40</td><td style=text-align:center>2.164secs</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Benchmark tool range requests to <code>leader</code> all keys.</p><ul><li><p><code>sn-etcd-sz</code> latency(<code>~8.993secs</code>) is <strong>~3% slightly lower than</strong> <code>mn-etcd-sz</code>(<code>~9.236secs</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~9.236secs</code>) is <strong>~2% slightly lower than</strong> <code>mn-etcd-mz</code>(<code>~9.100secs</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>0.5139</td><td style=text-align:center>8.993secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>0.506</td><td style=text-align:center>9.236secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>0.508</td><td style=text-align:center>9.100secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>read request, <code>follower</code> for <code>sn-etcd-sz</code>(<code>~9.secs</code>) is <strong>a slight difference <code>10ms</code></strong>.</p></li><li><p>Compare to consistency <code>Linearizable</code>read request, <code>follower</code> for <code>mn-etcd-sz</code>(<code>~9.113secs</code>) is <strong>~1% less</strong>, not a big difference.</p></li><li><p>Compare to consistency <code>Linearizable</code>read request, <code>follower</code> for <code>mn-etcd-mz</code>(<code>~8.799secs</code>) is <strong>~3% less</strong>, not a big difference.</p></li><li><p><code>sn-etcd-sz</code> latency(<code>~9.secs</code>) is <strong>~1% slightly less than</strong> <code>mn-etcd-sz</code>(<code>~9.113secs</code>).</p></li><li><p><em><code>mn-etcd-sz</code> latency(<code>~9.113secs</code>) is <strong>~3% slightly higher than</strong> <code>mn-etcd-mz</code>(<code>~8.799secs</code>)</em>.</p><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>0.51125</td><td style=text-align:center>9.0003secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>0.4993</td><td style=text-align:center>9.113secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>0.522</td><td style=text-align:center>8.799secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>follower</code> all keys</p><ul><li><p><code>mn-etcd-sz</code> latency(<code>~9.065secs</code>) is <strong>~1% slightly higher than</strong> <code>mn-etcd-mz</code>(<code>~9.007secs</code>).</p></li><li><p>Compare to <code>leader</code> consistency <code>Linearizable</code>read request, <code>follower</code> is <em>~1% slightly higher</em> for both cases <code>mn-etcd-sz</code>, <code>mn-etcd-mz</code> .</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>0.512</td><td style=text-align:center>9.065secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>0.533</td><td style=text-align:center>9.007secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>read request, <code>follower</code> for <code>mn-etcd-sz</code>(<code>~9.553secs</code>) is <strong>~5% high</strong>.</p></li><li><p><em>Compare to consistency <code>Linearizable</code>read request, <code>follower</code> for <code>mn-etcd-mz</code>(<code>~7.7433secs</code>) is <strong>~15% less</strong></em>.</p></li><li><p><em><code>mn-etcd-sz</code>(<code>~9.553secs</code>) latency is <strong>~20% higher than</strong> <code>mn-etcd-mz</code>(<code>~7.7433secs</code>)</em>.</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>0.4743</td><td style=text-align:center>9.553secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>0.5500</td><td style=text-align:center>7.7433secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li></ul><hr><br></details></li></ul><hr><br><br><blockquote><p>NOTE: This Network latency analysis is inspired by <a href=https://etcd.io/docs/v3.5/op-guide/performance/>ETCD performance</a>.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-48230a8cc284565a1dacea23838ecffd>2.2.2 - Local e2e Tests</h1><h1 id=e2e-test-suite>e2e Test Suite</h1><p>Developers can run extended e2e tests, in addition to unit tests, for Etcd-Druid in or from
their local environments. This is recommended to verify the desired behavior of several features
and to avoid regressions in future releases.</p><p>The very same tests typically run as part of the component&rsquo;s release job as well as on demand, e.g.,
when triggered by Etcd-Druid maintainers for open pull requests.</p><p>Testing Etcd-Druid automatically involves a certain test coverage for <a href=https://github.com/gardener/etcd-backup-restore/>gardener/etcd-backup-restore</a>
which is deployed as a side-car to the actual <code>etcd</code> container.</p><h2 id=prerequisites>Prerequisites</h2><p>The e2e test lifecycle is managed with the help of <a href=https://skaffold.dev/>skaffold</a>. Every involved step like <code>setup</code>,
<code>deploy</code>, <code>undeploy</code> or <code>cleanup</code> is executed against a <strong>Kubernetes</strong> cluster which makes it a mandatory prerequisite at the same time.
Only <a href=https://skaffold.dev/>skaffold</a> itself with involved <code>docker</code>, <code>helm</code> and <code>kubectl</code> executions as well as
the e2e-tests are executed locally. Required binaries are automatically downloaded if you use the corresponding <code>make</code> target,
as described in this document.</p><p>It&rsquo;s expected that especially the <code>deploy</code> step is run against a Kubernetes cluster which doesn&rsquo;t contain an Druid deployment or any left-overs like <code>druid.gardener.cloud</code> CRDs.
The <code>deploy</code> step will likely fail in such scenarios.</p><blockquote><p>Tip: Create a fresh <a href=https://kind.sigs.k8s.io/>KinD</a> cluster or a similar one with a small footprint before executing the tests.</p></blockquote><h2 id=providers>Providers</h2><p>The following providers are supported for e2e tests:</p><ul><li>AWS</li><li>Azure</li><li>GCP</li></ul><blockquote><p>Valid credentials need to be provided when tests happen with mentioned cloud providers.</p></blockquote><h2 id=flow>Flow</h2><p>An e2e test execution involves the following steps:</p><table><thead><tr><th>Step</th><th>Description</th></tr></thead><tbody><tr><td><code>setup</code></td><td>Create a storage bucket which is used for etcd backups (only with cloud providers).</td></tr><tr><td><code>deploy</code></td><td>Build Docker image, upload it to registry (if remote cluster - see <a href=https://skaffold.dev/docs/pipeline-stages/builders/docker/>Docker build</a>), deploy Helm chart (<code>charts/druid</code>) to Kubernetes cluster.</td></tr><tr><td><code>test</code></td><td>Execute e2e tests as defined in <code>test/e2e</code>.</td></tr><tr><td><code>undeploy</code></td><td>Remove the deployed artifacts from Kubernetes cluster.</td></tr><tr><td><code>cleanup</code></td><td>Delete storage bucket and Druid deployment from test cluster.</td></tr></tbody></table><h3 id=make-target>Make target</h3><p>Executing e2e-tests is as easy as executing the following command <strong>with defined Env-Vars as desribed in the following
section and as needed for your test scenario</strong>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make test-e2e
</span></span></code></pre></div><h3 id=common-env-variables>Common Env Variables</h3><p>The following environment variables influence how the flow described above is executed:</p><ul><li><code>PROVIDERS</code>: Providers used for testing (<code>all</code>, <code>aws</code>, <code>azure</code>, <code>gcp</code>). Multiple entries must be comma separated.<blockquote><p><strong>Note</strong>: Some tests will use very first entry from env <code>PROVIDERS</code> for e2e testing (ex: multi-node tests). So for multi-node tests to use specific provider, specify that provider as first entry in env <code>PROVIDERS</code>.</p></blockquote></li><li><code>KUBECONFIG</code>: Kubeconfig pointing to cluster where Etcd-Druid will be deployed (preferably <a href=https://kind.sigs.k8s.io>KinD</a>).</li><li><code>TEST_ID</code>: Some ID which is used to create assets for and during testing.</li><li><code>STEPS</code>: Steps executed by <code>make</code> target (<code>setup</code>, <code>deploy</code>, <code>test</code>, <code>undeploy</code>, <code>cleanup</code> - default: all steps).</li></ul><h3 id=aws-env-variables>AWS Env Variables</h3><ul><li><code>AWS_ACCESS_KEY_ID</code>: Key ID of the user.</li><li><code>AWS_SECRET_ACCESS_KEY</code>: Access key of the user.</li><li><code>AWS_REGION</code>: Region in which the test bucket is created.</li></ul><p>Example:</p><pre tabindex=0><code>make \
  AWS_ACCESS_KEY_ID=&#34;abc&#34; \
  AWS_SECRET_ACCESS_KEY=&#34;xyz&#34; \
  AWS_REGION=&#34;eu-central-1&#34; \
  KUBECONFIG=&#34;$HOME/.kube/config&#34; \
  PROVIDERS=&#34;aws&#34; \
  TEST_ID=&#34;some-test-id&#34; \
  STEPS=&#34;setup,deploy,test,undeploy,cleanup&#34; \
test-e2e
</code></pre><h3 id=azure-env-variables>Azure Env Variables</h3><ul><li><code>STORAGE_ACCOUNT</code>: Storage account used for managing the storage container.</li><li><code>STORAGE_KEY</code>: Key of storage account.</li></ul><p>Example:</p><pre tabindex=0><code>make \
  STORAGE_ACCOUNT=&#34;abc&#34; \
  STORAGE_KEY=&#34;eHl6Cg==&#34; \
  KUBECONFIG=&#34;$HOME/.kube/config&#34; \
  PROVIDERS=&#34;azure&#34; \
  TEST_ID=&#34;some-test-id&#34; \
  STEPS=&#34;setup,deploy,test,undeploy,cleanup&#34; \
test-e2e
</code></pre><h3 id=gcp-env-variables>GCP Env Variables</h3><ul><li><code>GCP_SERVICEACCOUNT_JSON_PATH</code>: Path to the service account json file used for this test.</li><li><code>GCP_PROJECT_ID</code>: ID of the GCP project.</li></ul><p>Example:</p><pre tabindex=0><code>make \
  GCP_SERVICEACCOUNT_JSON_PATH=&#34;/var/lib/secrets/serviceaccount.json&#34; \
  GCP_PROJECT_ID=&#34;xyz-project&#34; \
  KUBECONFIG=&#34;$HOME/.kube/config&#34; \
  PROVIDERS=&#34;gcp&#34; \
  TEST_ID=&#34;some-test-id&#34; \
  STEPS=&#34;setup,deploy,test,undeploy,cleanup&#34; \
test-e2e
</code></pre><h2 id=e2e-test-with-localstack>e2e test with localstack</h2><p>Above mentioned e2e tests need actual storage from cloud provider to be setup. But there is a tool named <a href=https://docs.localstack.cloud/user-guide/aws/s3/>localstack</a> that enables to run e2e test with mock AWS storage. We can also provision KIND cluster for e2e tests. So, together with localstack and KIND cluster, we don&rsquo;t need to depend on any actual cloud provider infrastructure to be setup to run e2e tests.</p><h3 id=how-are-the-kind-cluster-and-localstack-set-up>How are the KIND cluster and localstack set up</h3><p>KIND or Kubernetes-In-Docker is a kubernetes cluster that is set up inside a docker container. This cluster is with limited capability as it does not have much compute power. But this cluster can easily be setup inside a container and can be tear down easily just by removing a container. That&rsquo;s why KIND cluster is very easy to use for e2e tests. <code>Makefile</code> command helps to spin up a KIND cluster and use the cluster to run e2e tests.</p><p>There is a docker image for localstack. The image is deployed as pod inside the KIND cluster through <code>hack/e2e-test/infrastructure/localstack/localstack.yaml</code>. <code>Makefile</code> takes care of deploying the yaml file in a KIND cluster.</p><p>The developer needs to run <code>make ci-e2e-kind</code> command. This command in turn runs <code>hack/ci-e2e-kind.sh</code> which spin up the KIND cluster and deploy localstack in it and then run the e2e tests using localstack as mock AWS storage provider. e2e tests are actually run on host machine but deploy the druid controller inside KIND cluster. Druid controller spawns multinode ETCD clusters inside KIND cluster. e2e tests verify whether the druid controller performs its jobs correctly or not. Mock localstack storage is cleaned up after every e2e tests. That&rsquo;s why the e2e tests need to access the localstack pod running inside KIND cluster. The network traffic between host machine and localstack pod is resolved via mapping localstack pod port to host port while setting up the KIND cluster via <code>hack/e2e-test/infrastructure/kind/cluster.yaml</code></p><h3 id=how-to-execute-e2e-tests-with-localstack-and-kind-cluster>How to execute e2e tests with localstack and KIND cluster</h3><p>The developer just needs to install KIND following <a href=https://kind.sigs.k8s.io/docs/user/quick-start/>https://kind.sigs.k8s.io/docs/user/quick-start/</a> and start docker daemon. Additionaly, KIND cluster can be enabled via docker desktop.</p><p>Check if KIND is working on your machine by running the following command:
<code>kind create cluster --name kind-2</code></p><p>If successful, delete the cluster by:
<code>kind delete cluster --name kind-2</code></p><p>Run the following <code>make</code> command to perform e2e tests that will automatically take care of spinning up KIND clsuter and deploying localstack pod:
<code>make ci-e2e-kind</code></p></div><div class=td-content style=page-break-before:always><h1 id=pg-6834bee73ee14d55dd782ee77aeff7d3>2.3 - Operations</h1></div><div class=td-content><h1 id=pg-53b93ad882f2db95bf04ffdb5d95f79a>2.3.1 - Compaction Metrics</h1><h1 id=monitoring>Monitoring</h1><p>etcd-druid uses <a href=http://prometheus.io/>Prometheus</a> for metrics reporting. The metrics can be used for real-time monitoring and debugging of compaction jobs.</p><p>The simplest way to see the available metrics is to cURL the metrics endpoint <code>/metrics</code>. The format is described <a href=http://prometheus.io/docs/instrumenting/exposition_formats/>here</a>.</p><p>Follow the <a href=http://prometheus.io/docs/introduction/getting_started/>Prometheus getting started doc</a> to spin up a Prometheus server to collect etcd metrics.</p><p>The naming of metrics follows the suggested <a href=http://prometheus.io/docs/practices/naming/>Prometheus best practices</a>. All compaction related metrics are put under namespace <code>etcddruid</code> and subsystem <code>compaction</code>.</p><h3 id=compaction>Compaction</h3><p>These metrics give an idea about the compaction jobs that run after some interval in shoot control planes. Studying the metrices, we can deduce how many compaction job ran successfully, how many failed, how many delta events compacted etc.</p><table><thead><tr><th>Name</th><th>Description</th><th>Type</th></tr></thead><tbody><tr><td>etcddruid_compaction_jobs_total</td><td>Total number of compaction jobs initiated by compaction controller.</td><td>Counter</td></tr><tr><td>etcddruid_compaction_jobs_current</td><td>Number of currently running compaction job.</td><td>Gauge</td></tr><tr><td>etcddruid_compaction_job_duration_seconds</td><td>Total time taken in seconds to finish a running compaction job.</td><td>Histogram</td></tr><tr><td>etcddruid_compaction_num_delta_events</td><td>Total number of etcd events to be compacted by a compaction job.</td><td>Gauge</td></tr></tbody></table><p>There are two labels for <code>etcddruid_compaction_jobs_total</code> metrics. The label <code>succeeded</code> shows how many of the compaction jobs are succeeded and label <code>failed</code> shows how many of compaction jobs are failed.</p><p>There are two labels for <code>etcddruid_compaction_job_duration_seconds</code> metrics. The label <code>succeeded</code> shows how much time taken by a successful job to complete and label <code>failed</code> shows how much time taken by a failed compaction job.</p><p><code>etcddruid_compaction_jobs_current</code> metric comes with label <code>etcd_namespace</code> that indicates the namespace of the ETCD running in the control plane of a shoot cluster..</p><h2 id=prometheus-supplied-metrics>Prometheus supplied metrics</h2><p>The Prometheus client library provides a number of metrics under the <code>go</code> and <code>process</code> namespaces.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-eebd77dd02590370d9a5ab31aec95697>2.3.2 - Multinode Metrics</h1><h2 id=metrics-multi-node-etcd>Metrics: Multi-node etcd</h2><h4 id=etcd>Etcd</h4><table><thead><tr><th>No.</th><th>Metrics Name</th><th>Description</th><th>Comments</th></tr></thead><tbody><tr><td>1</td><td>etcd_disk_wal_fsync_duration_seconds</td><td>latency distributions of fsync called by WAL.</td><td>High disk operation latencies indicate disk issues.</td></tr><tr><td>2</td><td>etcd_disk_backend_commit_duration_seconds</td><td>latency distributions of commit called by backend.</td><td>High disk operation latencies indicate disk issues.</td></tr><tr><td>3</td><td>etcd_server_has_leader</td><td>whether or not a leader exists. 1: leader exists, 0: leader not exists.</td><td>To capture quorum loss or to check the availability of etcd cluster.</td></tr><tr><td>4</td><td>etcd_server_is_leader</td><td>whether or not this member is a leader. 1 if it is, 0 otherwise.</td><td></td></tr><tr><td>5</td><td>etcd_server_leader_changes_seen_total</td><td>number of leader changes seen.</td><td>Helpful in fine tuning the zonal cluster like etcd-heartbeat time etc, it can also indicates the etcd load and network issues.</td></tr><tr><td>6</td><td>etcd_server_is_learner</td><td>whether or not this member is a learner. 1 if it is, 0 otherwise.</td><td></td></tr><tr><td>7</td><td>etcd_server_learner_promote_successes</td><td>total number of successful learner promotions while this member is leader.</td><td>Might be helpful in checking the success of API calls called by backup-restore.</td></tr><tr><td>8</td><td>etcd_network_client_grpc_received_bytes_total</td><td>total number of bytes received from grpc clients.</td><td>Client Traffic In.</td></tr><tr><td>9</td><td>etcd_network_client_grpc_sent_bytes_total</td><td>total number of bytes sent to grpc clients.</td><td>Client Traffic Out.</td></tr><tr><td>10</td><td>etcd_network_peer_sent_bytes_total</td><td>total number of bytes sent to peers.</td><td>Useful for network usage.</td></tr><tr><td>11</td><td>etcd_network_peer_received_bytes_total</td><td>total number of bytes received from peers.</td><td>Useful for network usage.</td></tr><tr><td>12</td><td>etcd_network_active_peers</td><td>current number of active peer connections.</td><td>Might be useful in detecting issues like network partition.</td></tr><tr><td>13</td><td>etcd_server_proposals_committed_total</td><td>total number of consensus proposals committed.</td><td>A consistently large lag between a single member and its leader indicates that member is slow or unhealthy.</td></tr><tr><td>14</td><td>etcd_server_proposals_pending</td><td>current number of pending proposals to commit.</td><td>Pending proposals suggests there is a high client load or the member cannot commit proposals.</td></tr><tr><td>15</td><td>etcd_server_proposals_failed_total</td><td>total number of failed proposals seen.</td><td>Might indicates downtime caused by a loss of quorum.</td></tr><tr><td>16</td><td>etcd_server_proposals_applied_total</td><td>total number of consensus proposals applied.</td><td>Difference between etcd_server_proposals_committed_total and etcd_server_proposals_applied_total should usually be small.</td></tr><tr><td>17</td><td>etcd_mvcc_db_total_size_in_bytes</td><td>total size of the underlying database physically allocated in bytes.</td><td></td></tr><tr><td>18</td><td>etcd_server_heartbeat_send_failures_total</td><td>total number of leader heartbeat send failures.</td><td>Might be helpful in fine-tuning the cluster or detecting slow disk or any network issues.</td></tr><tr><td>19</td><td>etcd_network_peer_round_trip_time_seconds</td><td>round-trip-time histogram between peers.</td><td>Might be helpful in fine-tuning network usage specially for zonal etcd cluster.</td></tr><tr><td>20</td><td>etcd_server_slow_apply_total</td><td>total number of slow apply requests.</td><td>Might indicate overloaded from slow disk.</td></tr><tr><td>21</td><td>etcd_server_slow_read_indexes_total</td><td>total number of pending read indexes not in sync with leader&rsquo;s or timed out read index requests.</td><td></td></tr></tbody></table><h4 id=backup-restore>Backup-restore</h4><table><thead><tr><th>No.</th><th>Metrics Name</th><th>Description</th></tr></thead><tbody><tr><td>1.</td><td>etcdbr_cluster_size</td><td>to capture the scale-up/scale-down scenarios.</td></tr><tr><td>2.</td><td>etcdbr_is_learner</td><td>whether or not this member is a learner. 1 if it is, 0 otherwise.</td></tr><tr><td>3.</td><td>etcdbr_is_learner_count_total</td><td>total number times member added as the learner.</td></tr><tr><td>4.</td><td>etcdbr_restoration_duration_seconds</td><td>total latency distribution required to restore the etcd member.</td></tr><tr><td>5.</td><td>etcdbr_add_learner_duration_seconds</td><td>total latency distribution of adding the etcd member as a learner to the cluster.</td></tr><tr><td>6.</td><td>etcdbr_member_remove_duration_seconds</td><td>total latency distribution removing the etcd member from the cluster.</td></tr><tr><td>7.</td><td>etcdbr_member_promote_duration_seconds</td><td>total latency distribution of promoting the learner to the voting member.</td></tr><tr><td>8.</td><td>etcdbr_defragmentation_duration_seconds</td><td>total latency distribution of defragmentation of each etcd cluster member.</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-4b504efd0610aff9bef256d441fe5fa7>2.3.3 - Recover from etcd Permanent Quorum Loss</h1><h2 id=quorum-loss-in-etcd-cluster>Quorum loss in ETCD Cluster</h2><p><a href=https://etcd.io/docs/v3.4/op-guide/recovery/>Quorum loss</a> means when majority of ETCD pods(greater than or equal to n/2 + 1) are down simultaneously for some reason.</p><p>There are two types of quorum loss that can happen to <a href=/docs/other-components/etcd-druid/proposals/multi-node/>ETCD multinode cluster</a> :</p><ol><li><p><strong>Transient quorum loss</strong> - A quorum loss is called transient when majority of ETCD pods are down simultaneously for some time. The pods may be down due to network unavailability, high resource usages etc. When the pods come back after some time, they can re-join to the cluster and the quorum is recovered automatically without any manual intervention. There should not be a permanent failure for majority of etcd pods due to hardware failure or disk corruption.</p></li><li><p><strong>Permanent quorum loss</strong> - A quorum loss is called permanent when majority of ETCD cluster members experience permanent failure, whether due to hardware failure or disk corruption etc. then the etcd cluster is not going to recover automatically from the quorum loss. A human operator will now need to intervene and execute the following steps to recover the multi-node ETCD cluster.</p></li></ol><p>If permanent quorum loss occurs to a multinode ETCD cluster, the operator needs to note down the PVCs, configmaps, statefulsets, CRs etc related to that ETCD cluster and work on those resources only. Following steps guide a human operator to recover from permanent quorum loss of a ETCD cluster. We assume the name of the ETCD CR for the ETCD cluster is <code>etcd-main</code>.</p><p><strong>ETCD cluster in shoot control plane of gardener deployment:</strong>
There are two <a href=/docs/other-components/etcd-druid/proposals/multi-node/>ETCD clusters</a> running in shoot control plane. One is named as <code>etcd-events</code> and another is named <code>etcd-main</code>. The operator needs to take care of permanent quorum loss to a specific cluster. If permanent quorum loss occurs to <code>etcd-events</code> cluster, the operator needs to note down the PVCs, configmaps, statefulsets, CRs etc related to <code>etcd-events</code> cluster and work on those resources only.</p><p>⚠️ <strong>Note:</strong> Please note that manually restoring etcd can result in data loss. This guide is the last resort to bring an ETCD cluster up and running again.</p><p>If etcd-druid and etcd-backup-restore is being used with gardener, then</p><p>Target the control plane of affected shoot cluster via <code>kubectl</code>. Alternatively, you can use <a href=https://github.com/gardener/gardenctl-v2>gardenctl</a> to target the control plane of the affected shoot cluster. You can get the details to target the control plane from the Access tile in the shoot cluster details page on the Gardener dashboard. Ensure that you are targeting the correct namespace.</p><ol><li>Add the following annotation to the <code>Etcd</code> resource <code>kubectl annotate etcd etcd-main druid.gardener.cloud/ignore-reconciliation="true"</code></li><li>Note down the configmap name that is attached to the <code>etcd-main</code> statefulset. If you describe the statefulset with <code>kubectl describe sts etcd-main</code>, look for the lines similar to following lines to identify attached configmap name. It will be needed at later stages:</li></ol><pre tabindex=0><code>  Volumes:
   etcd-config-file:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      etcd-bootstrap-4785b0
    Optional:  false
</code></pre><pre tabindex=0><code>  Alternatively, the related configmap name can be obtained by executing following command as well:
</code></pre><p><code>kubectl get sts etcd-main -o jsonpath='{.spec.template.spec.volumes[?(@.name=="etcd-config-file")].configMap.name}'</code></p><ol start=3><li><p>Scale down the <code>etcd-main</code> statefulset replicas to <code>0</code></p><p><code>kubectl scale sts etcd-main --replicas=0</code></p></li><li><p>The PVCs will look like the following on listing them with the command <code>kubectl get pvc</code> :</p><pre tabindex=0><code>main-etcd-etcd-main-0        Bound    pv-shoot--garden--aws-ha-dcb51848-49fa-4501-b2f2-f8d8f1fad111   80Gi       RWO            gardener.cloud-fast   13d
main-etcd-etcd-main-1        Bound    pv-shoot--garden--aws-ha-b4751b28-c06e-41b7-b08c-6486e03090dd   80Gi       RWO            gardener.cloud-fast   13d
main-etcd-etcd-main-2        Bound    pv-shoot--garden--aws-ha-ff17323b-d62e-4d5e-a742-9de823621490   80Gi       RWO            gardener.cloud-fast   13d
</code></pre><p>Delete all PVCs that are attached to <code>etcd-main</code> cluster.</p><p><code>kubectl delete pvc -l instance=etcd-main</code></p></li><li><p>Edit the <code>etcd-main</code> cluster&rsquo;s configmap (ex: <code>etcd-bootstrap-4785b0</code>) as follows:</p><p>Find the <code>initial-cluster</code> field in the configmap. It will look like the following:</p><pre tabindex=0><code># Initial cluster
  initial-cluster: etcd-main-0=https://etcd-main-0.etcd-main-peer.default.svc:2380,etcd-main-1=https://etcd-main-1.etcd-main-peer.default.svc:2380,etcd-main-2=https://etcd-main-2.etcd-main-peer.default.svc:2380
</code></pre><p>Change the <code>initial-cluster</code> field to have only one member (<code>etcd-main-0</code>) in the string. It should now look like this:</p><pre tabindex=0><code># Initial cluster
  initial-cluster: etcd-main-0=https://etcd-main-0.etcd-main-peer.default.svc:2380
</code></pre></li><li><p>Scale up the <code>etcd-main</code> statefulset replicas to <code>1</code></p><p><code>kubectl scale sts etcd-main --replicas=1</code></p></li><li><p>Wait for the single-member etcd cluster to be completely ready.</p><p><code>kubectl get pods etcd-main-0</code> will give the following output when ready:</p><pre tabindex=0><code>NAME          READY   STATUS    RESTARTS   AGE
etcd-main-0   2/2     Running   0          1m
</code></pre></li><li><p>Remove the following annotation from the <code>Etcd</code> resource <code>etcd-main</code>: <code>kubectl annotate etcd etcd-main druid.gardener.cloud/ignore-reconciliation-</code></p></li><li><p>Finally add the following annotation to the <code>Etcd</code> resource <code>etcd-main</code>: <code>kubectl annotate etcd etcd-main gardener.cloud/operation="reconcile"</code></p></li><li><p>Verify that the etcd cluster is formed correctly.</p><p>All the <code>etcd-main</code> pods will have outputs similar to following:</p><pre tabindex=0><code>NAME          READY   STATUS    RESTARTS   AGE
etcd-main-0   2/2     Running   0          5m
etcd-main-1   2/2     Running   0          1m
etcd-main-2   2/2     Running   0          1m
</code></pre><p>Additionally, check if the ETCD CR is ready with <code>kubectl get etcd etcd-main</code> :</p><pre tabindex=0><code class=language-✹ data-lang=✹>NAME        READY   AGE
etcd-main   true    13d
</code></pre><p>Additionally, check the leases for 30 seconds at least. There should be leases starting with <code>etcd-main</code> as many as <code>etcd-main</code> replicas. One of those leases will have holder identity as <code>&lt;etcd-member-id>:Leader</code> and rest of those leases have holder identities as <code>&lt;etcd-member-id>:Member</code>. The <code>AGE</code> of those leases can also be inspected to identify if those leases were updated in conjunction with the restart of the ETCD cluster: Example:</p><pre tabindex=0><code>NAME        HOLDER                  AGE
etcd-main-0 4c37667312a3912b:Member 1m
etcd-main-1 75a9b74cfd3077cc:Member 1m
etcd-main-2 c62ee6af755e890d:Leader 1m
</code></pre></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-bc897a30fbe9d38f76f9636c098a74bc>2.3.4 - Scale Up</h1><h2 id=scaling-up-a-single-node-to-multi-node-etcd-cluster-deployed-by-etcd-druid>Scaling-up a single-node to multi-node etcd cluster deployed by etcd-druid</h2><p>To mark a cluster for scale-up from single node to multi-node etcd, just patch the etcd custom resource&rsquo;s <code>.spec.replicas</code> from <code>1</code> to <code>3</code> (for example).</p><h3 id=challenges-for-scale-up>Challenges for scale-up</h3><ol><li>Etcd cluster with single replica don&rsquo;t have any peers, so no peer communication is required hence peer URL may or may not be TLS enabled. However, while scaling up from single node etcd to multi-node etcd, there will be a requirement to have peer communication between members of the etcd cluster. Peer communication is required for various reasons, for instance for members to sync up cluster state, data, and to perform leader election or any cluster wide operation like removal or addition of a member etc. Hence in a multi-node etcd cluster we need to have TLS enable peer URL for peer communication.</li><li>Providing the correct configuration to start new etcd members as it is different from boostrapping a cluster since these new etcd members will join an existing cluster.</li></ol><h3 id=approach>Approach</h3><p>We first went through the etcd doc of <a href=https://etcd.io/docs/v3.4/op-guide/runtime-configuration/#update-advertise-peer-urls>update-advertise-peer-urls</a> to find out information regarding peer URL updation. Interestingly, etcd doc has mentioned the following:</p><pre tabindex=0><code>To update the advertise peer URLs of a member, first update it explicitly via member command and then restart the member.
</code></pre><p>But we can&rsquo;t assume peer URL is not TLS enabled for single-node cluster as it depends on end-user. A user may or may not enable the TLS for peer URL for a single node etcd cluster. So, How do we detect whether peer URL was enabled or not when cluster is marked for scale-up?</p><h3 id=detecting-if-peerurl-tls-is-enabled-or-not>Detecting if peerURL TLS is enabled or not</h3><p>For this we use an annotation in member lease object <code>member.etcd.gardener.cloud/tls-enabled</code> set by backup-restore sidecar of etcd. As etcd configuration is provided by backup-restore, so it can find out whether TLS is enabled or not and accordingly set this annotation <code>member.etcd.gardener.cloud/tls-enabled</code> to either <code>true</code> or <code>false</code> in member lease object.
And with the help of this annotation and config-map values etcd-druid is able to detect whether there is a change in a peer URL or not.</p><h3 id=etcd-druid-helps-in-scaling-up-etcd-cluster>Etcd-Druid helps in scaling up etcd cluster</h3><p>Now, it is detected whether peer URL was TLS enabled or not for single node etcd cluster. Etcd-druid can now use this information to take action:</p><ul><li>If peer URL was already TLS enabled then no action is required from etcd-druid side. Etcd-druid can proceed with scaling up the cluster.</li><li>If peer URL was not TLS enabled then etcd-druid has to intervene and make sure peer URL should be TLS enabled first for the single node before marking the cluster for scale-up.</li></ul><h3 id=action-taken-by-etcd-druid-to-enable-the-peerurl-tls>Action taken by etcd-druid to enable the peerURL TLS</h3><ol><li>Etcd-druid will update the <code>etcd-bootstrap</code> config-map with new config like initial-cluster,initial-advertise-peer-urls etc. Backup-restore will detect this change and update the member lease annotation to <code>member.etcd.gardener.cloud/tls-enabled: "true"</code>.</li><li>In case the peer URL TLS has been changed to <code>enabled</code>: Etcd-druid will add tasks to the deployment flow.<ul><li>To ensure that the TLS enablement of peer URL is properly reflected in etcd, the existing etcd StatefulSet pods should be restarted twice.</li><li>The first restart pushes a new configuration which contains Peer URL TLS configuration. Backup-restore will update the member peer url. This will result in the change of the peer url in the etcd&rsquo;s database, but it may not reflect in the already running etcd container. Ideally a restart of an etcd container would have been sufficient but currently k8s doesn&rsquo;t expose an API to force restart a single container within a pod. Therefore, we need to restart the StatefulSet pod(s) once again. When the pod(s) is restarted the second time it will now start etcd with the correct peer url which will be TLS enabled.</li><li>To achieve 2 restarts following is done:<ul><li>An update is made to the spec mounting the peer URL TLS secrets. This will cause a rolling update of the existing pod.</li><li>Once the update is successfully completed, then we delete StatefulSet pods, causing a restart by the StatefulSet controller.</li></ul></li></ul></li></ol><h3 id=after-peerurl-is-tls-enabled>After PeerURL is TLS enabled</h3><p>After peer URL TLS enablement for single node etcd cluster, now etcd-druid adds a scale-up annotation: <code>gardener.cloud/scaled-to-multi-node</code> to the etcd statefulset and etcd-druid will patch the statefulsets <code>.spec.replicas</code> to <code>3</code>(for example). The statefulset controller will then bring up new pods(etcd with backup-restore as a sidecar). Now etcd&rsquo;s sidecar i.e backup-restore will check whether this member is already a part of a cluster or not and incase it is unable to check (may be due to some network issues) then backup-restore checks presence of this annotation: <code>gardener.cloud/scaled-to-multi-node</code> in etcd statefulset to detect scale-up. If it finds out it is the scale-up case then backup-restore adds new etcd member as a <a href=https://etcd.io/docs/v3.3/learning/learner/>learner</a> first and then starts the etcd learner by providing the correct configuration. Once learner gets in sync with the etcd cluster leader, it will get promoted to a voting member.</p><h3 id=providing-the-correct-etcd-config>Providing the correct etcd config</h3><p>As backup-restore detects that it&rsquo;s a scale-up scenario, backup-restore sets <code>initial-cluster-state</code> to <code>existing</code> as this member will join an existing cluster and it calculates the rest of the config from the updated config-map provided by etcd-druid.</p><p><img src=/__resources/scale-up-sequenceDiagram_046066.png alt="Sequence diagram"></p><h4 id=future-improvements>Future improvements:</h4><p>The need of restarting etcd pods twice will change in the future. please refer: <a href=https://github.com/gardener/etcd-backup-restore/issues/538>https://github.com/gardener/etcd-backup-restore/issues/538</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-51f1df6f38c0f5cfdeea4a5bb8ea84e2>2.3.5 - Single Member Restoration</h1><h1 id=restoration-of-a-single-member-in-multi-node-etcd-deployed-by-etcd-druid>Restoration of a single member in multi-node etcd deployed by etcd-druid.</h1><p><strong>Note</strong>:</p><ul><li>For a cluster with n members, we are proposing the solution to only single member restoration within a etcd cluster not the quorum loss scenario (when majority of members within a cluster fail).</li><li>In this proposal we are not targetting the recovery of single member which got separated from cluster due to <a href=https://etcd.io/docs/v3.3/op-guide/failures/#network-partition>network partition</a>.</li></ul><h2 id=motivation>Motivation</h2><p>If a single etcd member within a multi-node etcd cluster goes down due to DB corruption/PVC corruption/Invalid data-dir then it needs to be brought back. Unlike in the single-node case, a minority member of a multi-node cluster can&rsquo;t be restored from the snapshots present in storage container as you can&rsquo;t restore from the old snapshots as it contains the metadata information of cluster which leads to <strong>memberID mismatch</strong> that prevents the new member from coming up as new member is getting its metadata information from db which got restore from old snapshots.</p><h2 id=solution>Solution</h2><ul><li>If a corresponding backup-restore sidecar detects that its corresponding etcd is down due to <a href=https://github.com/gardener/etcd-backup-restore/blob/7d27a47f5793b0949492d225ada5fd8344b6b6a2/pkg/initializer/validator/datavalidator.go#L177>data-dir corruption</a> or <a href=https://github.com/gardener/etcd-backup-restore/blob/7d27a47f5793b0949492d225ada5fd8344b6b6a2/pkg/initializer/validator/datavalidator.go#L204>Invalid data-dir</a></li><li>Then backup-restore will first remove the failing etcd member from the cluster using the <a href=https://github.com/etcd-io/etcd/blob/ae9734ed278b7a1a7dfc82e800471ebbf9fce56f/clientv3/cluster.go#L45-L46>MemberRemove API</a> call and clean the data-dir of failed etcd member.</li><li>It won&rsquo;t affect the etcd cluster as quorum is still maintained.</li><li>After successfully removing failed etcd member from the cluster, backup-restore sidecar will try to add a new etcd member to a cluster to get the same cluster size as before.</li><li>Backup-restore firstly adds new member as a <a href=https://etcd.io/docs/v3.3/learning/learner/>Learner</a> using the <a href=https://github.com/etcd-io/etcd/blob/ae9734ed278b7a1a7dfc82e800471ebbf9fce56f/clientv3/cluster.go#L42-L43>MemberAddAsLearner API</a> call, once learner is added to the cluster and it&rsquo;s get in sync with leader and becomes up-to-date then promote the learner(non-voting member) to a voting member using <a href=https://github.com/etcd-io/etcd/blob/ae9734ed278b7a1a7dfc82e800471ebbf9fce56f/clientv3/cluster.go#L51-L52>MemberPromote API</a> call.</li><li>So, the failed member first needs to be removed from the cluster and then added as a new member.</li></ul><h3 id=example>Example:</h3><ol><li>If a <code>3</code> member etcd cluster has 1 downed member(due to invalid data-dir), the cluster can still make forward progress because the quorum is <code>2</code>.</li><li>Etcd downed member get restarted and it&rsquo;s corresponding backup-restore sidecar receives an <a href=https://github.com/gardener/etcd-backup-restore/blob/master/doc/proposals/design.md#workflow>initialization</a> request.</li><li>Then, backup-restore sidecar checks for data corruption/invalid data-dir.</li><li>Backup-restore sidecar detects that data-dir is invalid and its a multi-node etcd cluster.</li><li>Then, backup-restore sidecar removed the downed etcd member from cluster.</li><li>The number of members in a cluster becomes <code>2</code> and the quorum remains at <code>2</code>, so it won&rsquo;t affect the etcd cluster.</li><li>Clean the data-dir and add a member as a learner(non-voting member).</li><li>As soon as learner gets in sync with leader, promote the learner to a voting member, hence increasing number of members in a cluster back to <code>3</code>.</li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-feb8be4a3199203fdee73330193347ff>2.4 - Feature Gates</h1><h1 id=feature-gates-in-etcd-druid>Feature Gates in Etcd-Druid</h1><p>This page contains an overview of the various feature gates an administrator can specify on etcd-druid.</p><h2 id=overview>Overview</h2><p>Feature gates are a set of key=value pairs that describe etcd-druid features. You can turn these features on or off by passing them to the <code>--feature-gates</code> CLI flag in the etcd-druid command.</p><p>The following tables are a summary of the feature gates that you can set on etcd-druid.</p><ul><li>The “Since” column contains the etcd-druid release when a feature is introduced or its release stage is changed.</li><li>The “Until” column, if not empty, contains the last etcd-druid release in which you can still use a feature gate.</li><li>If a feature is in the <em>Alpha</em> or <em>Beta</em> state, you can find the feature listed in the Alpha/Beta feature gate table.</li><li>If a feature is stable you can find all stages for that feature listed in the Graduated/Deprecated feature gate table.</li><li>The Graduated/Deprecated feature gate table also lists deprecated and withdrawn features.</li></ul><h2 id=feature-gates-for-alpha-or-beta-features>Feature Gates for Alpha or Beta Features</h2><table><thead><tr><th>Feature</th><th>Default</th><th>Stage</th><th>Since</th><th>Until</th></tr></thead><tbody><tr><td><code>UseEtcdWrapper</code></td><td><code>false</code></td><td><code>Alpha</code></td><td><code>0.19</code></td><td></td></tr></tbody></table><h2 id=feature-gates-for-graduated-or-deprecated-features>Feature Gates for Graduated or Deprecated Features</h2><table><thead><tr><th>Feature</th><th>Default</th><th>Stage</th><th>Since</th><th>Until</th></tr></thead></table><h2 id=using-a-feature>Using a Feature</h2><p>A feature can be in <em>Alpha</em>, <em>Beta</em> or <em>GA</em> stage.
An <em>Alpha</em> feature means:</p><ul><li>Disabled by default.</li><li>Might be buggy. Enabling the feature may expose bugs.</li><li>Support for feature may be dropped at any time without notice.</li><li>The API may change in incompatible ways in a later software release without notice.</li><li>Recommended for use only in short-lived testing clusters, due to increased
risk of bugs and lack of long-term support.</li></ul><p>A <em>Beta</em> feature means:</p><ul><li>Enabled by default.</li><li>The feature is well tested. Enabling the feature is considered safe.</li><li>Support for the overall feature will not be dropped, though details may change.</li><li>The schema and/or semantics of objects may change in incompatible ways in a
subsequent beta or stable release. When this happens, we will provide instructions
for migrating to the next version. This may require deleting, editing, and
re-creating API objects. The editing process may require some thought.
This may require downtime for applications that rely on the feature.</li><li>Recommended for only non-critical uses because of potential for
incompatible changes in subsequent releases.</li></ul><blockquote><p>Please do try <em>Beta</em> features and give feedback on them!
After they exit beta, it may not be practical for us to make more changes.</p></blockquote><p>A <em>General Availability</em> (GA) feature is also referred to as a <em>stable</em> feature. It means:</p><ul><li>The feature is always enabled; you cannot disable it.</li><li>The corresponding feature gate is no longer needed.</li><li>Stable versions of features will appear in released software for many subsequent versions.</li></ul><h2 id=list-of-feature-gates>List of Feature Gates</h2><table><thead><tr><th>Feature</th><th>Description</th></tr></thead><tbody><tr><td><code>UseEtcdWrapper</code></td><td>Enables the use of etcd-wrapper image and a compatible version of etcd-backup-restore, along with component-specific configuration changes necessary for the usage of the etcd-wrapper image.</td></tr></tbody></table></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2023 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid@8.13.4/dist/mermaid.min.js integrity="sha512-JERecFUBbsm75UpkVheAuDOE8NdHjQBrPACfEQYPwvPG+fjgCpHAz1Jw2ci9EXmd3DdfiWth3O3CQvcfEg8gsA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.7b24c0fb082ffb2de6cb14d6c95e9f8053053709ffcf8c761ef8e9ad2f8021e4.js integrity="sha256-eyTA+wgv+y3myxTWyV6fgFMFNwn/z4x2HvjprS+AIeQ=" crossorigin=anonymous></script></body></html>