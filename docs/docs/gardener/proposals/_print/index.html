<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.95.0"><link rel=canonical type=text/html href=https://gardener.cloud/docs/gardener/proposals/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/gardener/proposals/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Proposals | Gardener</title><meta name=description content="Gardener Enhancement Proposal (GEP)
Changes to the Gardener code base are often incorporated directly via pull requests which either themselves …"><meta property="og:title" content="Proposals"><meta property="og:description" content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/gardener/proposals/"><meta itemprop=name content="Proposals"><meta itemprop=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta name=twitter:card content="summary"><meta name=twitter:title content="Proposals"><meta name=twitter:description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><link rel=preload href=/scss/main.min.bdb6db391c5e5922ee3c9dea909f094d46df773b84500b1c939e30ee964cfd30.css as=style><link href=/scss/main.min.bdb6db391c5e5922ee3c9dea909f094d46df773b84500b1c939e30ee964cfd30.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7N3XF5XLGV"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7N3XF5XLGV",{anonymize_ip:!1})}</script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.c6245617e60d7f0d09e4778133c9bc29.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/gardener/proposals/>Return to the regular view of this page</a>.</p></div><h1 class=title>Proposals</h1><div class=content><h1 id=gardener-enhancement-proposal-gep>Gardener Enhancement Proposal (GEP)</h1><p>Changes to the Gardener code base are often incorporated directly via pull requests which either themselves contain a description about the motivation and scope of a change or a linked GitHub issue does.</p><p>If a perspective feature has a bigger extent, requires the involvement of several parties or more discussion is needed before the actual implementation can be started, you may consider filing a pull request with a Gardener Enhancement Proposal (GEP) first.</p><p>GEPs are a measure to propose a change or to add a feature to Gardener, help you to describe the change(s) conceptionally, and to list the steps that are necessary to reach this goal. It helps the Gardener maintainers as well as the community to understand the motivation and scope around your proposed change(s) and encourages their contribution to discussions and future pull requests. If you are familiar with the Kubernetes community, GEPs are analogue to Kubernetes Enhancement Proposals (<a href=https://github.com/kubernetes/enhancements/tree/master/keps>KEPs</a>).</p><h2 id=reasons-for-a-gep>Reasons for a GEP</h2><p>You may consider filing a GEP for the following reasons:</p><ul><li>A Gardener architectural change is intended / necessary</li><li>Major changes to the Gardener code base</li><li>A phased implementation approach is expected because of the widespread scope of the change</li><li>Your proposed changes may be controversial</li></ul><p>We encourage you to take a look at already merged <a href=https://github.com/gardener/gardener/tree/master/docs/proposals>GEPs</a> since they give you a sense of what a typical GEP comprises.</p><h2 id=before-creating-a-gep>Before creating a GEP</h2><p>Before starting your work and creating a GEP, please take some time to familiarize yourself with our
general <a href=https://gardener.cloud/docs/contribute/>Gardener Contribution Guidelines</a>.</p><p>It is recommended to discuss and outline the motivation of your prospective GEP as a draft with the community before you take the investment of creating the actual GEP. This early briefing supports the understanding for the broad community and leads to a fast feedback for your proposal from the respective experts in the community.
An appropriate format for this may be the regular <a href=https://gardener.cloud/docs/contribute/#bi-weekly-meetings>Gardener community meetings</a>.</p><h2 id=how-to-file-a-gep>How to file a GEP</h2><p>GEPs should be created as Markdown <code>.md</code> files and are submitted through a GitHub pull request to their current home in <a href=https://github.com/gardener/gardener/tree/master/docs/proposals>docs/proposals</a>. Please use the provided <a href=/docs/gardener/proposals/00-template/>template</a> or follow the structure of existing <a href=https://github.com/gardener/gardener/tree/master/docs/proposals>GEPs</a> which makes reviewing easier and faster. Additionally, please link the new GEP in our documentation <a href=https://github.com/gardener/gardener/blob/master/docs/README.md#Proposals>index</a>.</p><p>If not already done, please present your GEP in the <a href=https://gardener.cloud/docs/contribute/#bi-weekly-meetings>regular community meetings</a> to brief the community about your proposal (we strive for personal communication :) ). Also consider that this may be an important step to raise awareness and understanding for everyone involved.</p><p>The GEP template contains a small set of metadata, which is helpful for keeping track of the enhancement
in general and especially of who is responsible for implementing and reviewing PRs that are part of
the enhancement.</p><h3 id=main-reviewers>Main Reviewers</h3><p>Apart from general metadata, the GEP should name at least one &ldquo;main reviewer&rdquo;.
You can find a main reviewer for your GEP either when discussing the proposal in the community meeting, by asking in our
<a href=https://gardener.cloud/docs/contribute/#slack-channel>Slack Channel</a> or at latest during the GEP PR review.
New GEPs should only be accepted once at least one main reviewer is nominated/assigned.</p><p>The main reviewers are charged with the following tasks:</p><ul><li>familiarizing themselves with the details of the proposal</li><li>reviewing the GEP PR itself and any further updates to the document</li><li>discussing design details and clarifying implementation questions with the author before and after
the proposal was accepted</li><li>reviewing PRs related to the GEP in-depth</li></ul><p>Other community members are of course also welcome to help the GEP author, review his work and raise
general concerns with the enhancement. Nevertheless, the main reviewers are supposed to focus on more
in-depth reviews and accompaning the whole GEP process end-to-end, which helps with getting more
high-quality reviews and faster feedback cycles instead of having more people looking at the process
with lower priority and less focus.</p><h2 id=gep-process>GEP Process</h2><ol><li>Pre-discussions about GEP (if necessary)</li><li>Find a main reviewer for your enhancement</li><li>GEP is filed through GitHub PR</li><li>Presentation in Gardener community meeting (if possible)</li><li>Review of GEP from maintainers/community</li><li>GEP is merged if accepted</li><li>Implementation of GEP</li><li>Consider keeping GEP up-to-date in case implementation differs essentially</li></ol></div></div><div class=td-content style=page-break-before:always><h1 id=pg-a871ba89d724217f0cabd24297c97c72>1 - 01 Extensibility</h1><h1 id=gardener-extensibility-and-extraction-of-cloud-specificos-specific-knowledge-308httpsgithubcomgardenergardenerissues308-262httpsgithubcomgardenergardenerissues262>Gardener extensibility and extraction of cloud-specific/OS-specific knowledge (<a href=https://github.com/gardener/gardener/issues/308>#308</a>, <a href=https://github.com/gardener/gardener/issues/262>#262</a>)</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#summary>Summary</a></li><li><a href=#motivation>Motivation</a><ul><li><a href=#goals>Goals</a></li><li><a href=#non-goals>Non-Goals</a></li></ul></li><li><a href=#proposal>Proposal</a><ul><li><a href=#modification-of-existing-cloudprofile-and-shoot-resources>Modification of existing <code>CloudProfile</code> and <code>Shoot</code> resources</a><ul><li><a href=#cloudprofiles>CloudProfiles</a></li><li><a href=#shoots>Shoots</a></li></ul></li><li><a href=#crd-definitions-and-workflow-adaptation>CRD definitions and workflow adaptation</a><ul><li><a href=#custom-resource-definitions>Custom resource definitions</a><ul><li><a href=#dns-records>DNS records</a></li><li><a href=#infrastructure-provisioning>Infrastructure provisioning</a></li><li><a href=#backup-infrastructure-provisioning>Backup infrastructure provisioning</a></li><li><a href=#cloud-config-user-data-for-bootstrapping-machines>Cloud config (user-data) for bootstrapping machines</a></li><li><a href=#worker-pools-definition>Worker pools definition</a></li><li><a href=#generic-resources>Generic resources</a></li></ul></li><li><a href=#shoot-state>Shoot state</a></li><li><a href=#shoot-health-checksconditions>Shoot health checks/conditions</a></li><li><a href=#reconciliation-flow>Reconciliation flow</a></li><li><a href=#deletion-flow>Deletion flow</a></li></ul></li><li><a href=#gardenlet>Gardenlet</a></li><li><a href=#shoot-control-plane-movementmigration>Shoot control plane movement/migration</a></li></ul></li><li><a href=#registration-of-external-controllers-at-gardener>Registration of external controllers at Gardener</a></li><li><a href=#other-cloud-specific-parts>Other cloud-specific parts</a><ul><li><a href=#defaulting-and-validation-admission-plugins>Defaulting and validation admission plugins</a></li><li><a href=#dns-hosted-zone-admission-plugin>DNS Hosted Zone admission plugin</a></li><li><a href=#shoot-quota-admission-plugin>Shoot Quota admission plugin</a></li><li><a href=#shoot-maintenance-controller>Shoot maintenance controller</a></li></ul></li><li><a href=#alternatives>Alternatives</a></li></ul><h2 id=summary>Summary</h2><p>Gardener has evolved to a large compound of packages containing lots of highly specific knowledge which makes it very hard to extend (supporting a new cloud provider, new OS, &mldr;, or behave differently depending on the underlying infrastructure).</p><p>This proposal aims to move out the cloud-specific implementations (called &ldquo;(cloud) botanists&rdquo;) and the OS-specifics into dedicated controllers, and simultaneously to allow deviation from the standard Gardener deployment.</p><h2 id=motivation>Motivation</h2><p>Currently, it is too hard to support additional cloud providers or operation systems/distributions as everything must be done in-tree which might affect the implementation of other cloud providers as well.
The various conditions and branches make the code hard to maintain and hard to test.
Every change must be done centrally, requires to completely rebuild Gardener, and cannot be deployed individually. Similar to the motivation for Kubernetes to extract their cloud-specifics into dedicated cloud-controller-managers or to extract the container/storage/network/&mldr; specifics into CRI/CSI/CNI/&mldr;, we aim to do the same right now.</p><h3 id=goals>Goals</h3><ul><li>Gardener does not contain any cloud-specific knowledge anymore but defines a clear contract allowing external controllers (botanists) to support different environments (AWS, Azure, GCP, &mldr;).</li><li>Gardener does not contain any operation system-specific knowledge anymore but defines a clear contract allowing external controllers to support different operation systems/distributions (CoreOS, SLES, Ubuntu, &mldr;).</li><li>It shall become much easier to move control planes of Shoot clusters between Seed clusters (<a href=https://github.com/gardener/gardener/issues/232>#232</a>) which is a necessary requirement of an automated setup for the Gardener Ring (<a href=https://github.com/gardener/gardener/issues/233>#233</a>).</li></ul><h3 id=non-goals>Non-Goals</h3><ul><li>We want to also factor out the specific knowledge of the addon deployments (nginx-ingress, kubernetes-dashboard, &mldr;), but we already have dedicated projects/issues for that: <a href=https://github.com/gardener/bouquet>https://github.com/gardener/bouquet</a> and <a href=https://github.com/gardener/gardener/issues/246>#246</a>. We will keep the addons in-tree as part of this proposal and tackle their extraction separately.</li><li>We do not want to make the Gardener a plain workflow engine that just executes a given template (which indeed would allow to be generic, open, and extensible in their highest forms but which would end-up in building a &ldquo;programming/scripting language&rdquo; inside a serialization format (YAML/JSON/&mldr;)). Rather, we want to have well-defined contracts and APIs, keeping Gardener responsible for the clusters management.</li></ul><h2 id=proposal>Proposal</h2><p>Gardener heavily relies on and implements Kubernetes principles, and its ultimate strategy is to use Kubernetes wherever applicable.
The extension concept in Kubernetes is based on (next to others) <code>CustomResourceDefinition</code>s, <code>ValidatingWebhookConfiguration</code>s and <code>MutatingWebhookConfiguration</code>s, and <code>InitializerConfiguration</code>s.
Consequently, Gardener&rsquo;s extensibility concept relies on these mechanisms.</p><p>Instead of implementing all aspects directly in Gardener it will deploy some CRDs to the Seed cluster which will be watched by dedicated controllers (also running in the Seed clusters), each one implementing one aspect of cluster management. This way one complex strongly coupled Gardener implementation covering all infrastructures is decomposed into a set of loosely coupled controllers implementing aspects of APIs defined by Gardener.
Gardener will just wait until the controllers report that they are done (or have faced an error) in the CRD&rsquo;s <code>.status</code> field instead of doing the respective tasks itself.
We will have one specific CRD for every specific operation (e.g., DNS, infrastructure provisioning, machine cloud config generation, &mldr;).
However, there are also parts inside Gardener which can be handled generically (not by cloud botanists) because they are the same or very similar for all the environments.
One example of those is the deployment of a <code>Namespace</code> in the Seed which will run the Shoot&rsquo;s control plane
Another one is the deployment of a <code>Service</code> for the Shoot&rsquo;s kube-apiserver.
In case a cloud botanist needs to cooperate and react on those operations it should register a <code>ValidatingWebhookConfiguration</code>, a <code>MutatingWebhookConfiguration</code>, or a <code>InitializerConfiguration</code>.
With this approach it can validate, modify, or react on any resource created by Gardener to make it cloud infrastructure specific.</p><p>The web hooks should be registered with <code>failurePolicy=Fail</code> to ensure that a request made by Gardener fails if the respective web hook is not available.</p><h3 id=modification-of-existing-cloudprofile-and-shoot-resources>Modification of existing <code>CloudProfile</code> and <code>Shoot</code> resources</h3><p>We will introduce the new API group <code>gardener.cloud</code>:</p><h4 id=cloudprofiles>CloudProfiles</h4><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: aws
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: aws
</span></span><span style=display:flex><span><span style=color:green># caBundle: |</span>
</span></span><span style=display:flex><span><span style=color:green>#   -----BEGIN CERTIFICATE-----</span>
</span></span><span style=display:flex><span><span style=color:green>#   ...</span>
</span></span><span style=display:flex><span><span style=color:green>#   -----END CERTIFICATE-----</span>
</span></span><span style=display:flex><span>  dnsProviders:
</span></span><span style=display:flex><span>  - type: aws-route53
</span></span><span style=display:flex><span>  - type: unmanaged
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - 1.12.1
</span></span><span style=display:flex><span>    - 1.11.0
</span></span><span style=display:flex><span>    - 1.10.5
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: m4.large
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 8Gi
</span></span><span style=display:flex><span>  <span style=color:green># storage: 20Gi   # optional (not needed in every environment, may only be specified if no volumeTypes have been specified)</span>
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  volumeTypes:      <span style=color:green># optional (not needed in every environment, may only be specified if no machineType has a `storage` field)</span>
</span></span><span style=display:flex><span>  - name: gp2
</span></span><span style=display:flex><span>    class: standard
</span></span><span style=display:flex><span>  - name: io1
</span></span><span style=display:flex><span>    class: premium
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: aws.cloud.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: CloudProfileConfig
</span></span><span style=display:flex><span>    constraints:
</span></span><span style=display:flex><span>      minimumVolumeSize: 20Gi
</span></span><span style=display:flex><span>      machineImages:
</span></span><span style=display:flex><span>      - name: coreos
</span></span><span style=display:flex><span>        regions:
</span></span><span style=display:flex><span>        - name: eu-west-1
</span></span><span style=display:flex><span>          ami: ami-32d1474b
</span></span><span style=display:flex><span>        - name: us-east-1
</span></span><span style=display:flex><span>          ami: ami-e582d29f
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - region: eu-west-1
</span></span><span style=display:flex><span>        zones:
</span></span><span style=display:flex><span>        - name: eu-west-1a
</span></span><span style=display:flex><span>          unavailableMachineTypes: <span style=color:green># list of machine types defined above that are not available in this zone</span>
</span></span><span style=display:flex><span>          - name: m4.large
</span></span><span style=display:flex><span>          unavailableVolumeTypes:  <span style=color:green># list of volume types defined above that are not available in this zone</span>
</span></span><span style=display:flex><span>          - name: gp2
</span></span><span style=display:flex><span>        - name: eu-west-1b
</span></span><span style=display:flex><span>        - name: eu-west-1c
</span></span></code></pre></div><h4 id=shoots>Shoots</h4><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-aws
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: aws
</span></span><span style=display:flex><span>  secretBindingName: core-aws
</span></span><span style=display:flex><span>  cloud:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    region: eu-west-1
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.cloud.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc: <span style=color:green># specify either &#39;id&#39; or &#39;cidr&#39;</span>
</span></span><span style=display:flex><span>        <span style=color:green># id: vpc-123456</span>
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        internal:
</span></span><span style=display:flex><span>        - 10.250.112.0/22
</span></span><span style=display:flex><span>        public:
</span></span><span style=display:flex><span>        - 10.250.96.0/22
</span></span><span style=display:flex><span>        workers:
</span></span><span style=display:flex><span>        - 10.250.0.0/19
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - eu-west-1a
</span></span><span style=display:flex><span>    workerPools:
</span></span><span style=display:flex><span>    - name: pool-01
</span></span><span style=display:flex><span>    <span style=color:green># Taints, labels, and annotations are not yet implemented. This requires interaction with the machine-controller-manager, see</span>
</span></span><span style=display:flex><span>    <span style=color:green># https://github.com/gardener/machine-controller-manager/issues/174. It is only mentioned here as future proposal.</span>
</span></span><span style=display:flex><span>    <span style=color:green># taints:</span>
</span></span><span style=display:flex><span>    <span style=color:green># - key: foo</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   value: bar</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   effect: PreferNoSchedule</span>
</span></span><span style=display:flex><span>    <span style=color:green># labels:</span>
</span></span><span style=display:flex><span>    <span style=color:green># - key: bar</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   value: baz</span>
</span></span><span style=display:flex><span>    <span style=color:green># annotations:</span>
</span></span><span style=display:flex><span>    <span style=color:green># - key: foo</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   value: hugo</span>
</span></span><span style=display:flex><span>      machineType: m4.large
</span></span><span style=display:flex><span>      volume: <span style=color:green># optional, not needed in every environment, may only be specified if the referenced CloudProfile contains the volumeTypes field</span>
</span></span><span style=display:flex><span>        type: gp2
</span></span><span style=display:flex><span>        size: 20Gi
</span></span><span style=display:flex><span>      providerConfig:
</span></span><span style=display:flex><span>        apiVersion: aws.cloud.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>        kind: WorkerPoolConfig
</span></span><span style=display:flex><span>        machineImage:
</span></span><span style=display:flex><span>          name: coreos
</span></span><span style=display:flex><span>          ami: ami-d0dcef3
</span></span><span style=display:flex><span>        zones:
</span></span><span style=display:flex><span>        - eu-west-1a
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      maxSurge: 1
</span></span><span style=display:flex><span>      maxUnavailable: 0
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.11.0
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>  dns:
</span></span><span style=display:flex><span>    provider: aws-route53
</span></span><span style=display:flex><span>    domain: johndoe-aws.garden-dev.example.com
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    timeWindow:
</span></span><span style=display:flex><span>      begin: 220000+0100
</span></span><span style=display:flex><span>      end: 230000+0100
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  backup:
</span></span><span style=display:flex><span>    schedule: <span style=color:#a31515>&#34;*/5 * * * *&#34;</span>
</span></span><span style=display:flex><span>    maximum: 7
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kube2iam:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>    kubernetes-dashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    cluster-autoscaler:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginx-ingress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      loadBalancerSourceRanges: []
</span></span><span style=display:flex><span>    kube-lego:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      email: john.doe@example.com
</span></span></code></pre></div><p>ℹ The specifications for the other cloud providers Gardener already has an implementation for looks similar.</p><h3 id=crd-definitions-and-workflow-adaptation>CRD definitions and workflow adaptation</h3><p>In the following we are outlining the CRD definitions which define the API between Gardener and the dedicated controllers.
After that we will take a look at the current <a href=https://github.com/gardener/gardener/blob/master/pkg/gardenlet/controller/shoot/shoot_control_reconcile.go>reconciliation</a>/<a href=https://github.com/gardener/gardener/blob/master/pkg/gardenlet/controller/shoot/shoot_control_delete.go>deletion</a> flow and describe how it would look like in case we would implement this proposal.</p><h4 id=custom-resource-definitions>Custom resource definitions</h4><p>Every CRD has a <code>.spec.type</code> field containing the respective instance of the dimension the CRD represents, e.g. the cloud provider, the DNS provider or the operation system name.
Moreover, the <code>.status</code> field must contain</p><ul><li><code>observedGeneration</code> (<code>int64</code>), a field indicating on which generation the controller last worked on.</li><li><code>state</code> (<code>*runtime.RawExtension</code>), a field which is not interpreted by Gardener but persisted; it should be treated opaque and only be used by the respective CRD-specific controller (it can store anything it needs to re-construct its own state).</li><li><code>lastError</code> (<code>object</code>), a field which is optional and only present if the last operation ended with an error state.</li><li><code>lastOperation</code> (<code>object</code>), a field which always exists and which indicates what the last operation of the controller was.</li><li><code>conditions</code> (<code>list</code>), a field allowing the controller to report health checks for its area of responsibility.</li></ul><p>Some CRDs might have a <code>.spec.providerConfig</code> or a <code>.status.providerStatus</code> field containing controller-specific information that is treated opaque by Gardener and will only be copied to dependent or depending CRDs.</p><h5 id=dns-records>DNS records</h5><p>Every Shoot needs two DNS records (or three, depending on whether nginx-ingress addon is enabled), one so-called &ldquo;internal&rdquo; record that Gardener uses in the kubeconfigs of the Shoot cluster&rsquo;s system components, and one so-called &ldquo;external&rdquo; record which is used in the kubeconfig provided to the user.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: dns.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: DNSProvider
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: alicloud
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: alicloud-dns
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: alicloud-credentials
</span></span><span style=display:flex><span>  domains:
</span></span><span style=display:flex><span>    include:
</span></span><span style=display:flex><span>    - my.own.domain.com
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: dns.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: DNSEntry
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: dns
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  dnsName: dns.my.own.domain.com
</span></span><span style=display:flex><span>  ttl: 600
</span></span><span style=display:flex><span>  targets:
</span></span><span style=display:flex><span>  - 8.8.8.8
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  observedGeneration: 4
</span></span><span style=display:flex><span>  state: some-state
</span></span><span style=display:flex><span>  lastError:
</span></span><span style=display:flex><span>    lastUpdateTime: 2018-04-04T07:08:51Z
</span></span><span style=display:flex><span>    description: some-error message
</span></span><span style=display:flex><span>    codes:
</span></span><span style=display:flex><span>    - ERR_UNAUTHORIZED
</span></span><span style=display:flex><span>  lastOperation:
</span></span><span style=display:flex><span>    lastUpdateTime: 2018-04-04T07:24:51Z
</span></span><span style=display:flex><span>    progress: 70
</span></span><span style=display:flex><span>    type: Reconcile
</span></span><span style=display:flex><span>    state: Processing
</span></span><span style=display:flex><span>    description: Currently provisioning ...
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - lastTransitionTime: 2018-07-11T10:18:25Z
</span></span><span style=display:flex><span>    message: DNS record has been created and is available.
</span></span><span style=display:flex><span>    reason: RecordResolvable
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>    type: Available
</span></span><span style=display:flex><span>    propagate: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>  providerStatus:
</span></span><span style=display:flex><span>    apiVersion: aws.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: DNSStatus
</span></span><span style=display:flex><span>    ...
</span></span></code></pre></div><h5 id=infrastructure-provisioning>Infrastructure provisioning</h5><p>The <code>Infrastructure</code> CRD contains the information about VPC, networks, security groups, availability zones, &mldr;, basically, everything that needs to be prepared before an actual VMs/load balancers/&mldr; can be provisioned.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Infrastructure
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: infrastructure
</span></span><span style=display:flex><span>  namespace: shoot--core--aws-01
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: aws
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: aws.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: InfrastructureConfig
</span></span><span style=display:flex><span>    networks:
</span></span><span style=display:flex><span>      vpc:
</span></span><span style=display:flex><span>        cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>      internal:
</span></span><span style=display:flex><span>      - 10.250.112.0/22
</span></span><span style=display:flex><span>      public:
</span></span><span style=display:flex><span>      - 10.250.96.0/22
</span></span><span style=display:flex><span>      workers:
</span></span><span style=display:flex><span>      - 10.250.0.0/19
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - eu-west-1a
</span></span><span style=display:flex><span>  dns:
</span></span><span style=display:flex><span>    apiserver: api.aws-01.core.example.com
</span></span><span style=display:flex><span>  region: eu-west-1
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: my-aws-credentials
</span></span><span style=display:flex><span>  sshPublicKey: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    </span>    base64(key)
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  observedGeneration: ...
</span></span><span style=display:flex><span>  state: ...
</span></span><span style=display:flex><span>  lastError: ..
</span></span><span style=display:flex><span>  lastOperation: ...
</span></span><span style=display:flex><span>  providerStatus:
</span></span><span style=display:flex><span>    apiVersion: aws.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: InfrastructureStatus
</span></span><span style=display:flex><span>    vpc:
</span></span><span style=display:flex><span>      id: vpc-1234
</span></span><span style=display:flex><span>      subnets:
</span></span><span style=display:flex><span>      - id: subnet-acbd1234
</span></span><span style=display:flex><span>        name: workers
</span></span><span style=display:flex><span>        zone: eu-west-1
</span></span><span style=display:flex><span>      securityGroups:
</span></span><span style=display:flex><span>      - id: sg-xyz12345
</span></span><span style=display:flex><span>        name: workers
</span></span><span style=display:flex><span>    iam:
</span></span><span style=display:flex><span>      nodesRoleARN: &lt;some-arn&gt;
</span></span><span style=display:flex><span>      instanceProfileName: foo
</span></span><span style=display:flex><span>    ec2:
</span></span><span style=display:flex><span>      keyName: bar
</span></span></code></pre></div><h5 id=backup-infrastructure-provisioning>Backup infrastructure provisioning</h5><p>The <code>BackupInfrastructure</code> CRD in the Seeds tells the cloud-specific controller to prepare a blob store bucket/container which can later be used to store etcd backups.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: BackupInfrastructure
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: etcd-backup
</span></span><span style=display:flex><span>  namespace: shoot--core--aws-01
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: aws
</span></span><span style=display:flex><span>  region: eu-west-1
</span></span><span style=display:flex><span>  storageContainerName: asdasjndasd-1293912378a-2213
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: my-aws-credentials
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  observedGeneration: ...
</span></span><span style=display:flex><span>  state: ...
</span></span><span style=display:flex><span>  lastError: ..
</span></span><span style=display:flex><span>  lastOperation: ...
</span></span></code></pre></div><h5 id=cloud-config-user-data-for-bootstrapping-machines>Cloud config (user-data) for bootstrapping machines</h5><p>Gardener will continue to keep knowledge about the content of the cloud config scripts, but it will hand over it to the respective OS-specific controller which will generate the specific valid representation.
Gardener creates two <code>MachineCloudConfig</code> CRDs, one for the cloud-config-downloader (which will later flow into the <code>WorkerPool</code> CRD) and one for the real cloud-config (which will be stored as a <code>Secret</code> in the Shoot&rsquo;s <code>kube-system</code> namespace, and downloaded and executed from the cloud-config-downloader on the machines).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: MachineCloudConfig
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: pool-01-downloader
</span></span><span style=display:flex><span>  namespace: shoot--core--aws-01
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: CoreOS
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>  - name: cloud-config-downloader.service
</span></span><span style=display:flex><span>    command: start
</span></span><span style=display:flex><span>    enable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    content: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      [Unit]
</span></span></span><span style=display:flex><span><span style=color:#a31515>      Description=Downloads the original cloud-config from Shoot API Server and executes it
</span></span></span><span style=display:flex><span><span style=color:#a31515>      After=docker.service docker.socket
</span></span></span><span style=display:flex><span><span style=color:#a31515>      Wants=docker.socket
</span></span></span><span style=display:flex><span><span style=color:#a31515>      [Service]
</span></span></span><span style=display:flex><span><span style=color:#a31515>      Restart=always
</span></span></span><span style=display:flex><span><span style=color:#a31515>      RestartSec=30
</span></span></span><span style=display:flex><span><span style=color:#a31515>      EnvironmentFile=/etc/environment
</span></span></span><span style=display:flex><span><span style=color:#a31515>      ExecStart=/bin/sh /var/lib/cloud-config-downloader/download-cloud-config.sh</span>      
</span></span><span style=display:flex><span>  files:
</span></span><span style=display:flex><span>  - path: /var/lib/cloud-config-downloader/credentials/kubeconfig
</span></span><span style=display:flex><span>    permissions: 0644
</span></span><span style=display:flex><span>    content:
</span></span><span style=display:flex><span>      secretRef:
</span></span><span style=display:flex><span>        name: cloud-config-downloader
</span></span><span style=display:flex><span>        dataKey: kubeconfig
</span></span><span style=display:flex><span>  - path: /var/lib/cloud-config-downloader/download-cloud-config.sh
</span></span><span style=display:flex><span>    permissions: 0644
</span></span><span style=display:flex><span>    content:
</span></span><span style=display:flex><span>      inline:
</span></span><span style=display:flex><span>        encoding: b64
</span></span><span style=display:flex><span>        data: IyEvYmluL2Jhc2ggL...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  observedGeneration: ...
</span></span><span style=display:flex><span>  state: ...
</span></span><span style=display:flex><span>  lastError: ..
</span></span><span style=display:flex><span>  lastOperation: ...
</span></span><span style=display:flex><span>  cloudConfig: | <span style=color:green># base64-encoded</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cloud-config</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    coreos:
</span></span><span style=display:flex><span>      update:
</span></span><span style=display:flex><span>        reboot-strategy: <span style=color:#00f>off</span>
</span></span><span style=display:flex><span>      units:
</span></span><span style=display:flex><span>      - name: cloud-config-downloader.service
</span></span><span style=display:flex><span>        command: start
</span></span><span style=display:flex><span>        enable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        content: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>          [Unit]
</span></span></span><span style=display:flex><span><span style=color:#a31515>          Description=Downloads the original cloud-config from Shoot API Server and execute it
</span></span></span><span style=display:flex><span><span style=color:#a31515>          After=docker.service docker.socket
</span></span></span><span style=display:flex><span><span style=color:#a31515>          Wants=docker.socket
</span></span></span><span style=display:flex><span><span style=color:#a31515>          [Service]
</span></span></span><span style=display:flex><span><span style=color:#a31515>          Restart=always
</span></span></span><span style=display:flex><span><span style=color:#a31515>          RestartSec=30
</span></span></span><span style=display:flex><span><span style=color:#a31515>          ...</span>          
</span></span></code></pre></div><p>ℹ The cloud-config-downloader script does not only download the cloud-config initially but at regular intervals, e.g., every <code>30s</code>.
If it sees an updated cloud-config then it applies it again by reloading and restarting all systemd units in order to reflect the changes.
The way how this reloading of the cloud-config happens is OS-specific as well and not known to Gardener anymore, however, it must be part of the script already.
On CoreOS, you have to execute <code>/usr/bin/coreos-cloudinit --from-file=&lt;path></code> whereas on SLES you execute <code>cloud-init --file &lt;path> single -n write_files --frequency=once</code>.
As Gardener doesn&rsquo;t know these commands it will write a placeholder expression instead (e.g., <code>{RELOAD-CLOUD-CONFIG-WITH-PATH:&lt;path>}</code>) and the OS-specific controller is asked to replace it with the proper expression.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: MachineCloudConfig
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: pool-01-original <span style=color:green># stored as secret and downloaded later</span>
</span></span><span style=display:flex><span>  namespace: shoot--core--aws-01
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: CoreOS
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>  - name: docker.service
</span></span><span style=display:flex><span>    drop-ins:
</span></span><span style=display:flex><span>    - name: 10-docker-opts.conf
</span></span><span style=display:flex><span>      content: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>        [Service]
</span></span></span><span style=display:flex><span><span style=color:#a31515>        Environment=&#34;DOCKER_OPTS=--log-opt max-size=60m --log-opt max-file=3&#34;</span>        
</span></span><span style=display:flex><span>  - name: docker-monitor.service
</span></span><span style=display:flex><span>    command: start
</span></span><span style=display:flex><span>    enable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    content: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      [Unit]
</span></span></span><span style=display:flex><span><span style=color:#a31515>      Description=Docker-monitor daemon
</span></span></span><span style=display:flex><span><span style=color:#a31515>      After=kubelet.service
</span></span></span><span style=display:flex><span><span style=color:#a31515>      [Service]
</span></span></span><span style=display:flex><span><span style=color:#a31515>      Restart=always
</span></span></span><span style=display:flex><span><span style=color:#a31515>      EnvironmentFile=/etc/environment
</span></span></span><span style=display:flex><span><span style=color:#a31515>      ExecStart=/opt/bin/health-monitor docker</span>      
</span></span><span style=display:flex><span>  - name: kubelet.service
</span></span><span style=display:flex><span>    command: start
</span></span><span style=display:flex><span>    enable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    content: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      [Unit]
</span></span></span><span style=display:flex><span><span style=color:#a31515>      Description=kubelet daemon
</span></span></span><span style=display:flex><span><span style=color:#a31515>      Documentation=https://kubernetes.io/docs/admin/kubelet
</span></span></span><span style=display:flex><span><span style=color:#a31515>      After=docker.service
</span></span></span><span style=display:flex><span><span style=color:#a31515>      Wants=docker.socket rpc-statd.service
</span></span></span><span style=display:flex><span><span style=color:#a31515>      [Service]
</span></span></span><span style=display:flex><span><span style=color:#a31515>      Restart=always
</span></span></span><span style=display:flex><span><span style=color:#a31515>      RestartSec=10
</span></span></span><span style=display:flex><span><span style=color:#a31515>      EnvironmentFile=/etc/environment
</span></span></span><span style=display:flex><span><span style=color:#a31515>      ExecStartPre=/bin/docker run --rm -v /opt/bin:/opt/bin:rw k8s.gcr.io/hyperkube:v1.11.2 cp /hyperkube /opt/bin/
</span></span></span><span style=display:flex><span><span style=color:#a31515>      ExecStartPre=/bin/sh -c &#39;hostnamectl set-hostname $(cat /etc/hostname | cut -d &#39;.&#39; -f 1)&#39;
</span></span></span><span style=display:flex><span><span style=color:#a31515>      ExecStart=/opt/bin/hyperkube kubelet \
</span></span></span><span style=display:flex><span><span style=color:#a31515>          --allow-privileged=true \
</span></span></span><span style=display:flex><span><span style=color:#a31515>          --bootstrap-kubeconfig=/var/lib/kubelet/kubeconfig-bootstrap \
</span></span></span><span style=display:flex><span><span style=color:#a31515>          ...</span>      
</span></span><span style=display:flex><span>  files:
</span></span><span style=display:flex><span>  - path: /var/lib/kubelet/ca.crt
</span></span><span style=display:flex><span>    permissions: 0644
</span></span><span style=display:flex><span>    content:
</span></span><span style=display:flex><span>      secretRef:
</span></span><span style=display:flex><span>        name: ca-kubelet
</span></span><span style=display:flex><span>        dataKey: ca.crt
</span></span><span style=display:flex><span>  - path: /var/lib/cloud-config-downloader/download-cloud-config.sh
</span></span><span style=display:flex><span>    permissions: 0644
</span></span><span style=display:flex><span>    content:
</span></span><span style=display:flex><span>      inline:
</span></span><span style=display:flex><span>        encoding: b64
</span></span><span style=display:flex><span>        data: IyEvYmluL2Jhc2ggL...
</span></span><span style=display:flex><span>  - path: /etc/sysctl.d/99-k8s-general.conf
</span></span><span style=display:flex><span>    permissions: 0644
</span></span><span style=display:flex><span>    content:
</span></span><span style=display:flex><span>      inline:
</span></span><span style=display:flex><span>        data: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>          vm.max_map_count = 135217728
</span></span></span><span style=display:flex><span><span style=color:#a31515>          kernel.softlockup_panic = 1
</span></span></span><span style=display:flex><span><span style=color:#a31515>          kernel.softlockup_all_cpu_backtrace = 1
</span></span></span><span style=display:flex><span><span style=color:#a31515>          ...</span>          
</span></span><span style=display:flex><span>  - path: /opt/bin/health-monitor
</span></span><span style=display:flex><span>    permissions: 0755
</span></span><span style=display:flex><span>    content:
</span></span><span style=display:flex><span>      inline:
</span></span><span style=display:flex><span>        data: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>          #!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#a31515>          set -o nounset
</span></span></span><span style=display:flex><span><span style=color:#a31515>          set -o pipefail
</span></span></span><span style=display:flex><span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>          function docker_monitoring {
</span></span></span><span style=display:flex><span><span style=color:#a31515>          ...</span>          
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  observedGeneration: ...
</span></span><span style=display:flex><span>  state: ...
</span></span><span style=display:flex><span>  lastError: ..
</span></span><span style=display:flex><span>  lastOperation: ...
</span></span><span style=display:flex><span>  cloudConfig: ...
</span></span></code></pre></div><p>Cloud-specific controllers which might need to add another kernel option or another flag to the kubelet, maybe even another file to the disk, can register a <code>MutatingWebhookConfiguration</code> to that resource and modify it upon creation/update.
The task of the <code>MachineCloudConfig</code> controller is to only generate the OS-specific cloud-config based on the <code>.spec</code> field, but not to add or change any logic related to Shoots.</p><h5 id=worker-pools-definition>Worker pools definition</h5><p>For every worker pool defined in the <code>Shoot</code> Gardener will create a <code>WorkerPool</code> CRD which shall be picked up by a cloud-specific controller and be translated to <code>MachineClass</code>es and <code>MachineDeployment</code>s.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: WorkerPool
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: pool-01
</span></span><span style=display:flex><span>  namespace: shoot--core--aws-01
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudConfig: base64(downloader-cloud-config)
</span></span><span style=display:flex><span>  infrastructureProviderStatus:
</span></span><span style=display:flex><span>    apiVersion: aws.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: InfrastructureStatus
</span></span><span style=display:flex><span>    vpc:
</span></span><span style=display:flex><span>      id: vpc-1234
</span></span><span style=display:flex><span>      subnets:
</span></span><span style=display:flex><span>      - id: subnet-acbd1234
</span></span><span style=display:flex><span>        name: workers
</span></span><span style=display:flex><span>        zone: eu-west-1
</span></span><span style=display:flex><span>      securityGroups:
</span></span><span style=display:flex><span>      - id: sg-xyz12345
</span></span><span style=display:flex><span>        name: workers
</span></span><span style=display:flex><span>    iam:
</span></span><span style=display:flex><span>      nodesRoleARN: &lt;some-arn&gt;
</span></span><span style=display:flex><span>      instanceProfileName: foo
</span></span><span style=display:flex><span>    ec2:
</span></span><span style=display:flex><span>      keyName: bar
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: aws.cloud.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: WorkerPoolConfig
</span></span><span style=display:flex><span>    machineImage:
</span></span><span style=display:flex><span>      name: CoreOS
</span></span><span style=display:flex><span>      ami: ami-d0dcef3b
</span></span><span style=display:flex><span>    machineType: m4.large
</span></span><span style=display:flex><span>    volumeType: gp2
</span></span><span style=display:flex><span>    volumeSize: 20Gi
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - eu-west-1a
</span></span><span style=display:flex><span>  region: eu-west-1
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: my-aws-credentials
</span></span><span style=display:flex><span>  minimum: 2
</span></span><span style=display:flex><span>  maximum: 2
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  observedGeneration: ...
</span></span><span style=display:flex><span>  state: ...
</span></span><span style=display:flex><span>  lastError: ..
</span></span><span style=display:flex><span>  lastOperation: ...
</span></span></code></pre></div><h5 id=generic-resources>Generic resources</h5><p>Some components are cloud-specific and must be deployed by the cloud-specific botanists.
Others might need to deploy another pod next to the shoot&rsquo;s control plane or must do anything else.
Some of these might be important for a functional cluster (e.g., the cloud-controller-manager, or a CSI plugin in the future), and controllers should be able to report errors back to the user.
Consequently, in order to trigger the controllers to deploy these components Gardener would write a <code>Generic</code> CRD to the Seed to trigger the deployment.
No operation is depending on the status of these resources, however, the entire reconciliation flow is.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Generic
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cloud-components
</span></span><span style=display:flex><span>  namespace: shoot--core--aws-01
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: cloud-components
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: my-aws-credentials
</span></span><span style=display:flex><span>  shootSpec:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  observedGeneration: ...
</span></span><span style=display:flex><span>  state: ...
</span></span><span style=display:flex><span>  lastError: ..
</span></span><span style=display:flex><span>  lastOperation: ...
</span></span></code></pre></div><h4 id=shoot-state>Shoot state</h4><p>In order to enable moving the control plane of a Shoot between Seed clusters (e.g., if a Seed cluster is not available anymore or entirely broken) Gardener must store some non-reconstructable state, potentially also the state written by the controllers.
Gardener watches these extension CRDs and copies the <code>.status.state</code> in a <code>ShootState</code> resource into the Garden cluster.
Any observed status change of the respective CRD-controllers must be immediately reflected in the <code>ShootState</code> resource.
The contract between Gardener and those controllers is: <strong>Every controller must be capable of reconstructing its own environment based on both the state it has written before and on the real world&rsquo;s conditions/state.</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ShootState
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: shoot--core--aws-01
</span></span><span style=display:flex><span>shootRef:
</span></span><span style=display:flex><span>  name: aws-01
</span></span><span style=display:flex><span>  project: core
</span></span><span style=display:flex><span>state:
</span></span><span style=display:flex><span>  secrets:
</span></span><span style=display:flex><span>  - name: ca
</span></span><span style=display:flex><span>    data: ...
</span></span><span style=display:flex><span>  - name: kube-apiserver-cert
</span></span><span style=display:flex><span>    data: ...
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>  - kind: DNS
</span></span><span style=display:flex><span>    name: record-1
</span></span><span style=display:flex><span>    state: &lt;copied-state-of-dns-crd&gt;
</span></span><span style=display:flex><span>  - kind: Infrastructure
</span></span><span style=display:flex><span>    name: networks
</span></span><span style=display:flex><span>    state: &lt;copied-state-of-infrastructure-crd&gt;
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  &lt;other fields required to keep track of&gt;
</span></span></code></pre></div><p>We cannot assume that Gardener is always online to observe the most recent states the controllers have written to their resources.
Consequently, the information stored here must not be used as &ldquo;single point of truth&rdquo;, but the controllers must potentially check the real world&rsquo;s status to reconstruct themselves.
However, this must anyway be part of their normal reconciliation logic and is a general best practice for Kubernetes controllers.</p><h4 id=shoot-health-checksconditions>Shoot health checks/conditions</h4><p>Some of the existing conditions already contain specific code which shall be simplified as well.
All of the CRDs described above have a <code>.status.conditions</code> field to which the controllers may write relevant health information of their function area.
Gardener will pick them up and copy them over to the Shoots <code>.status.conditions</code> (only those conditions setting <code>propagate=true</code>).</p><h4 id=reconciliation-flow>Reconciliation flow</h4><p>We are now examining the current Shoot creation/reconciliation flow and describe how it could look like when applying this proposal:</p><table><thead><tr><th>Operation</th><th>Description</th></tr></thead><tbody><tr><td>botanist.DeployNamespace</td><td>Gardener creates the namespace for the Shoot in the Seed cluster.</td></tr><tr><td>botanist.DeployKubeAPIServerService</td><td>Gardener creates a Service of type <code>LoadBalancer</code> in the Seed.<br>AWS Botanist registers a Mutating Webhook and adds its AWS-specific annotation.</td></tr><tr><td>botanist.WaitUntilKubeAPIServerServiceIsReady</td><td>Gardener checks the <code>.status</code> object of the just created <code>Service</code> in the Seed. The contract is that also clouds not supporting load balancers must react on the <code>Service</code> object and modify the <code>.status</code> to correctly reflect the kube-apiserver&rsquo;s ingress IP.</td></tr><tr><td>botanist.DeploySecrets</td><td>Gardener creates the secrets/certificates it needs like it does today, but it provides utility functions that can be adopted by Botanists/other controllers if they need additional certificates/secrets created on their own. (We should also add labels to all secrets)</td></tr><tr><td>botanist.Shoot.Components.DNS.Internal{Provider/Entry}.Deploy</td><td>Gardener creates a DNS-specific CRD in the Seed, and the responsible DNS-controller picks it up and creates a corresponding DNS record (see CRD specification above).</td></tr><tr><td>botanist.Shoot.Components.DNS.External{Provider/Entry}.Deploy</td><td>Gardener creates a DNS-specific CRD in the Seed, and the responsible DNS-controller picks it up and creates a corresponding DNS record: (see CRD specification above).</td></tr><tr><td>shootCloudBotanist.DeployInfrastructure</td><td>Gardener creates a Infrastructure-specific CRD in the Seed, and the responsible Botanist picks it up and does its job: (see CRD above).</td></tr><tr><td>botanist.DeployBackupInfrastructure</td><td>Gardener creates a <code>BackupInfrastructure</code> resource in the Garden cluster.<br>(The BackupInfrastructure controller creates a BackupInfrastructure-specific CRD in the Seed, and the responsible Botanist picks it up and does its job: (see CRD above).)</td></tr><tr><td>botanist.WaitUntilBackupInfrastructureReconciled</td><td>Gardener checks the <code>.status</code> object of the just created <code>BackupInfrastructure</code> resource.</td></tr><tr><td>hybridBotanist.DeployETCD</td><td>Gardener does only deploy the etcd <code>StatefulSet</code> without backup-restore sidecar at all.<br>The cloud-specific Botanist registers a Mutating Webhook and adds the backup-restore sidecar, and it also creates the <code>Secret</code> needed by the backup-restore sidecar.</td></tr><tr><td>botanist.WaitUntilEtcdReady</td><td>Gardener checks the <code>.status</code> object of the etcd <code>Statefulset</code> and waits until readiness is indicated.</td></tr><tr><td>hybridBotanist.DeployCloudProviderConfig</td><td>Gardener does not execute this anymore because it doesn&rsquo;t know anything about cloud-specific configuration.</td></tr><tr><td>hybridBotanist.DeployKubeAPIServer</td><td>Gardener does only deploy the kube-apiserver <code>Deployment</code> without any cloud-specific flags/configuration.<br>The cloud-specific Botanist registers a Mutating Webhook and adds whatever is needed for the kube-apiserver to run in its cloud environment.</td></tr><tr><td>hybridBotanist.DeployKubeControllerManager</td><td>Gardener does only deploy the kube-controller-manager <code>Deployment</code> without any cloud-specific flags/configuration.<br>The cloud-specific Botanist registers a Mutating Webhook and adds whatever is needed for the kube-controller-manager to run in its cloud environment (e.g., the cloud-config).</td></tr><tr><td>hybridBotanist.DeployKubeScheduler</td><td>Gardener does only deploy the kube-scheduler <code>Deployment</code> without any cloud-specific flags/configuration.<br>The cloud-specific Botanist registers a Mutating Webhook and adds whatever is needed for the kube-scheduler to run in its cloud environment.</td></tr><tr><td>hybridBotanist.DeployCloudControllerManager</td><td>Gardener does not execute this anymore because it doesn&rsquo;t know anything about cloud-specific configuration. The Botanists would be responsible to deploy their own cloud-controller-manager now.<br>They would watch for the kube-apiserver Deployment to exist, and as soon as it does, they deploy the CCM.<br>(Side note: The Botanist would also be responsible to deploy further controllers needed for this cloud environment, e.g. F5-controllers or CSI plugins).</td></tr><tr><td>botanist.WaitUntilKubeAPIServerReady</td><td>Gardener checks the <code>.status</code> object of the kube-apiserver <code>Deployment</code> and waits until readiness is indicated.</td></tr><tr><td>botanist.InitializeShootClients</td><td>Unchanged; Gardener creates a Kubernetes client for the Shoot cluster.</td></tr><tr><td>botanist.DeployMachineControllerManager</td><td>Deleted, Gardener does no longer deploy MCM itself. See below.</td></tr><tr><td>hybridBotanist.ReconcileMachines</td><td>Gardener creates a <code>Worker</code> CRD in the Seed, and the responsible <code>Worker</code> controller picks it up and does its job (see CRD above). It also deploys the machine-controller-manager.<br>Gardener waits until the status indicates that the controller is done.</td></tr><tr><td>hybridBotanist.DeployKubeAddonManager</td><td>This function also computes the CoreOS cloud-config (because the secret storing it is managed by the kube-addon-manager).<br>Gardener would deploy the CloudConfig-specific CRD in the Seed, and the responsible OS controller picks it up and does its job (see CRD above).<br>The Botanists which would have to modify something would register a Webhook for this CloudConfig-specific resource and apply their changes.<br>The rest is mostly unchanged, Gardener generates the manifests for the addons and deploys the kube-addon-manager into the Seed.<br>AWS Botanist registers a Webhook for nginx-ingress.<br>Azure Botanist registers a Webhook for calico.<br>Gardener will no longer deploy the <code>StorageClass</code>es. Instead, the Botanists wait until the kube-apiserver is available and deploy them.<br><br>In the long term we want to get rid of optional addons inside the Gardener core and implement a sophisticated addon concept (see <a href=https://github.com/gardener/gardener/issues/246>#246</a>).</td></tr><tr><td>shootCloudBotanist.DeployKube2IAMResources</td><td>This function would be removed (currently Gardener would execute a Terraform job creating the IAM roles specified in the Shoot manifest). We cannot keep this behavior, the user would be responsible to create the needed IAM roles on its own.</td></tr><tr><td>botanist.Shoot.Components.Nginx.DNSEtnry</td><td>Gardener creates a DNS-specific CRD in the Seed, and the responsible DNS-controller picks it up and creates a corresponding DNS record (see CRD specification above).</td></tr><tr><td>botanist.WaitUntilVPNConnectionExists</td><td>Unchanged, Gardener checks that it is possible to port-forward to a Shoot pod.</td></tr><tr><td>seedCloudBotanist.ApplyCreateHook</td><td>This function would be removed (actually, only the AWS Botanist implements it).<br>AWS Botanist deploys the aws-lb-readvertiser once the API Server is deployed and updates the ELB health check protocol one the load balancer pointing to the API server is created.</td></tr><tr><td>botanist.DeploySeedMonitoring</td><td>Unchanged, Gardener deploys the monitoring stack into the Seed.</td></tr><tr><td>botanist.DeployClusterAutoscaler</td><td>Unchanged, Gardener deploys the cluster-autoscaler into the Seed.</td></tr></tbody></table><p>ℹ We can easily lift the contract later and allow dynamic network plugins or not using the VPN solution at all.
We could also introduce a dedicated <code>ControlPlane</code> CRD and leave the complete responsibility of deploying kube-apiserver, kube-controller-manager, etc. to other controllers (if we need it at some point in time).</p><h4 id=deletion-flow>Deletion flow</h4><p>We are now examining the current Shoot deletion flow and describe shortly how it could look like when applying this proposal:</p><table><thead><tr><th>Operation</th><th>Description</th></tr></thead><tbody><tr><td>botanist.DeploySecrets</td><td>This is just refreshing the cloud provider secret in the Shoot namespace in the Seed (in case the user has changed it before triggering the deletion). This function would stay as it is.</td></tr><tr><td>hybridBotanist.RefreshMachineClassSecrets</td><td>This function would disappear.<br>Worker Pool controller needs to watch the referenced secret and update the generated MachineClassSecrets immediately.</td></tr><tr><td>hybridBotanist.RefreshCloudProviderConfig</td><td>This function would disappear. Botanist needs to watch the referenced secret and update the generated cloud-provider-config immediately.</td></tr><tr><td>botanist.RefreshCloudControllerManagerChecksums</td><td>See &ldquo;hybridBotanist.RefreshCloudProviderConfig&rdquo;.</td></tr><tr><td>botanist.RefreshKubeControllerManagerChecksums</td><td>See &ldquo;hybridBotanist.RefreshCloudProviderConfig&rdquo;.</td></tr><tr><td>botanist.InitializeShootClients</td><td>Unchanged; Gardener creates a Kubernetes client for the Shoot cluster.</td></tr><tr><td>botanist.DeleteSeedMonitoring</td><td>Unchanged; Gardener deletes the monitoring stack.</td></tr><tr><td>botanist.DeleteKubeAddonManager</td><td>Unchanged; Gardener deletes the kube-addon-manager.</td></tr><tr><td>botanist.DeleteClusterAutoscaler</td><td>Unchanged; Gardener deletes the cluster-autoscaler.</td></tr><tr><td>botanist.WaitUntilKubeAddonManagerDeleted</td><td>Unchanged; Gardener waits until the kube-addon-manager is deleted.</td></tr><tr><td>botanist.CleanCustomResourceDefinitions</td><td>Unchanged, Gardener cleans the CRDs in the Shoot.</td></tr><tr><td>botanist.CleanKubernetesResources</td><td>Unchanged, Gardener cleans all remaining Kubernetes resources in the Shoot.</td></tr><tr><td>hybridBotanist.DestroyMachines</td><td>Gardener deletes the WorkerPool-specific CRD in the Seed, and the responsible WorkerPool-controller picks it up and does its job.<br>Gardener waits until the CRD is deleted.</td></tr><tr><td>shootCloudBotanist.DestroyKube2IAMResources</td><td>This function would disappear (currently Gardener would execute a Terraform job deleting the IAM roles specified in the <code>Shoot</code> manifest). We cannot keep this behavior, the user would be responsible to delete the needed IAM roles on its own.</td></tr><tr><td>shootCloudBotanist.DestroyInfrastructure</td><td>Gardener deletes the Infrastructure-specific CRD in the Seed, and the responsible Botanist picks it up and does its job.<br>Gardener waits until the CRD is deleted.</td></tr><tr><td>botanist.Shoot.Components.DNS.External{Provider/Entry}.Destroy</td><td>Gardener deletes the DNS-specific CRD in the Seed, and the responsible DNS-controller picks it up and does its job.<br>Gardener waits until the CRD is deleted.</td></tr><tr><td>botanist.DeleteKubeAPIServer</td><td>Unchanged; Gardener deletes the kube-apiserver.</td></tr><tr><td>botanist.DeleteBackupInfrastructure</td><td>Unchanged; Gardener deletes the <code>BackupInfrastructure</code> object in the Garden cluster.<br>(The BackupInfrastructure controller deletes the BackupInfrastructure-specific CRD in the Seed, and the responsible Botanist picks it up and does its job.<br>The BackupInfrastructure controller waits until the CRD is deleted.)</td></tr><tr><td>botanist.Shoot.Components.DNS.Internal{Provider/Entry}.Destroy</td><td>Gardener deletes the DNS-specific CRD in the Seed, and the responsible DNS-controller picks it up and does its job.<br>Gardener waits until the CRD is deleted.</td></tr><tr><td>botanist.DeleteNamespace</td><td>Unchanged; Gardener deletes the Shoot namespace in the Seed cluster.</td></tr><tr><td>botanist.WaitUntilSeedNamespaceDeleted</td><td>Unchanged; Gardener waits until the Shoot namespace in the Seed has been deleted.</td></tr><tr><td>botanist.DeleteGardenSecrets</td><td>Unchanged; Gardener deletes the kubeconfig/ssh-keypair <code>Secret</code> in the project namespace in the Garden.</td></tr></tbody></table><h3 id=gardenlet>Gardenlet</h3><p>One part of the whole extensibility work will also to further split Gardener itself.
Inspired from Kubernetes itself we plan to move the <code>Shoot</code> reconciliation/deletion controller loops as well as the <code>BackupInfrastructure</code> reconciliation/deletion controller loops into a dedicated &ldquo;gardenlet&rdquo; component that will run in the Seed cluster.
With that, it can talk locally to the responsible kube-apiserver and we do no longer need to perform every operation out of the Garden cluster.
This approach will also help us with scalability, performance, maintainability, testability in general.</p><p>This architectural change implies that the Kubernetes API server of the Garden cluster must be exposed publicly (or at least be reachable by the registered Seeds). The Gardener controller-manager will remain and will keep its <code>CloudProfile</code>, <code>SecretBinding</code>, <code>Quota</code>, <code>Project</code>, and <code>Seed</code> controller loops. One part of the seed controller could be to deploy the &ldquo;gardenlet&rdquo; into the Seeds, however, this would require network connectivity to the Seed cluster.</p><h3 id=shoot-control-plane-movementmigration>Shoot control plane movement/migration</h3><p>Automatically moving control planes is difficult with the current implementation as some resources created in the old Seed must be moved to the new one. However, some of them are not under Gardener&rsquo;s control (e.g., <code>Machine</code> resources). Moreover, the old control plane must be deactivated somehow to ensure that not two controllers work on the same things (e.g., virtual machines) from different environments.</p><p>Gardener does not only deploy a DNS controller into the Seeds but also into its own Garden cluster.
For every Shoot cluster, Gardener commissions it to create a DNS <code>TXT</code> record containing the name of the Seed responsible for the Shoot (holding the control plane), e.g.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>dig -t txt aws-01.core.garden.example.com
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>;; ANSWER SECTION:
</span></span><span style=display:flex><span>aws-01.core.garden.example.com. 120 IN	TXT <span style=color:#a31515>&#34;Seed=seed-01&#34;</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>Gardener always keeps the DNS record up-to-date based on which Seed is responsible.</p><p>In the above CRD examples one object in the <code>.spec</code> section was omitted as it is needed to get Shoot control plane movement/migration working (the field is only explained now in this section and not before; it was omitted on purpose to support focusing on the relevant specifications first).
Every CRD also has the following section in its <code>.spec</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>leadership:
</span></span><span style=display:flex><span>  record: aws-01.core.garden.example.com
</span></span><span style=display:flex><span>  value: seed-01
</span></span><span style=display:flex><span>  leaseSeconds: 60
</span></span></code></pre></div><p>Before every operation the CRD-controllers check this DNS record (based on the <code>.spec.leadership.leaseSeconds</code> configuration) and verify that its result is equal to the <code>.spec.leadership.value</code> field.
If both match they know that they should act on the resource, otherwise they stop doing anything.</p><p>ℹ We will provide an easy-to-use framework for the controllers containing all of these features out-of-the-box in order to allow the developers to focus on writing the actual controller logic.</p><p>When a Seed control plane move is triggered, the <code>.spec.cloud.seed</code> field of the respective <code>Shoot</code> is changed.
Gardener will change the respective DNS record&rsquo;s value (<code>aws-01.core.garden.example.com</code>) to contain the new Seed name.
After that it will wait <code>2*60s</code> to be sure that all controllers have observed the change.
Then it starts reconciling and applying the CRDs together with a preset <code>.status.state</code> into the new Seed (based on its last observations which were stored in the respective <code>ShootState</code> object stored in the Garden cluster).
The controllers are - as per contract - asked to reconstruct their own environment based on the <code>.status.state</code> they have written before and the real world&rsquo;s status.
Apart from that, the normal reconciliation flow gets executed.</p><p>Gardener stores the list of Seeds that were responsible for hosting a Shoots control plane at some time in the Shoots <code>.status.seeds</code> list so that it knows which Seeds must be cleaned up (i.e., where the control plane must be deleted because it has been moved).
Once cleaned up, the Seed&rsquo;s name will be removed from that list.</p><h3 id=backupinfrastructure-migration>BackupInfrastructure migration</h3><p>One part of the reconciliation flow above is the provisioning of the infrastructure for the Shoot&rsquo;s etcd backups (usually, this is a blob store bucket/container).
Gardener already uses a separate <code>BackupInfrastructure</code> resource that is written into the Garden cluster and picked up by a dedicated <code>BackupInfrastructure</code> controller (bundled into the Gardener controller manager).
This dedicated resource exists mainly for the reason to allow keeping backups for a certain &ldquo;grace period&rdquo; even after the Shoot deletion itself:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: BackupInfrastructure
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: aws-01-bucket
</span></span><span style=display:flex><span>  namespace: garden-core
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  seed: seed-01
</span></span><span style=display:flex><span>  shootUID: uuid-of-shoot
</span></span></code></pre></div><p>The actual provisioning is executed in a corresponding Seed cluster as Gardener can only assume network connectivity to the underlying cloud environment in the Seed.
We would like to keep the created artifacts in the Seed (e.g., Terraform state) near to the control plane.
Consequently, when Gardener moves a control plane, it will update the <code>.spec.seed</code> field of the <code>BackupInfrastructure</code> resource as well.
With the exact same logic described above the <code>BackupInfrastructure</code> controller inside the Gardener will move to the new Seed.</p><h2 id=registration-of-external-controllers-at-gardener>Registration of external controllers at Gardener</h2><p>We want to have a dynamic registration process, i.e. we don&rsquo;t want to hard-code any information about which controllers shall be deployed.
The ideal solution would be to not even requiring a restart of Gardener when a new controller registers.</p><p>Every controller is registered by a <code>ControllerRegistration</code> resource that introduces every controller together with its supported resources (dimension (<code>kind</code>) and shape (<code>type</code>) combination) to Gardener:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControllerRegistration
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: dns-aws-route53
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>  - kind: DNS
</span></span><span style=display:flex><span>    type: aws-route53
</span></span><span style=display:flex><span><span style=color:green># deployment:</span>
</span></span><span style=display:flex><span><span style=color:green>#   type: helm</span>
</span></span><span style=display:flex><span><span style=color:green>#   providerConfig:</span>
</span></span><span style=display:flex><span><span style=color:green>#     chart.tgz: base64(helm-chart)</span>
</span></span><span style=display:flex><span><span style=color:green>#     values.yaml: |</span>
</span></span><span style=display:flex><span><span style=color:green>#       foo: bar</span>
</span></span></code></pre></div><p>Every <code>.kind</code>/<code>.type</code> combination may only exist once in the system.</p><p>When a <code>Shoot</code> shall be reconciled Gardener can identify based on the referenced <code>Seed</code> and the content of the <code>Shoot</code> specification which controllers are needed in the respective Seed cluster.
It will demand the operators in the Garden cluster to deploy the controllers they are responsible for to a specific Seed.
This kind of communication happens via CRDs as well:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControllerInstallation
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: dns-aws-route53
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  registrationRef:
</span></span><span style=display:flex><span>    name: dns-aws-route53
</span></span><span style=display:flex><span>  seedRef:
</span></span><span style=display:flex><span>    name: seed-01
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - lastTransitionTime: 2018-08-07T15:09:23Z
</span></span><span style=display:flex><span>    message: The controller has been successfully deployed to the seed.
</span></span><span style=display:flex><span>    reason: ControllerDeployed
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>    type: Available
</span></span></code></pre></div><p>The default scenario is that every controller is gets deployed by a dedicated operator that knows how to handle its lifecycle operations like deployment, update, upgrade, deletion.
This operator watches <code>ControllerInstallation</code> resources and reacts on those it is responsible for (that it has created earlier).
Gardener is responsible for writing the <code>.spec</code> field, the operator is responsible for providing information in the <code>.status</code> indicating whether the controller was successfully deployed and is ready to be used.
Gardener will be also able to ask for deletion of controllers from Seeds when they are not needed there anymore by deleting the corresponding <code>ControllerInstallation</code> object.</p><p>ℹ The provided easy-to-use framework for the controllers will also contain these needed features to implement corresponding operators.</p><p>For most cases the controller deployment is very simple (just deploying it into the seed with some static configuration).
In these cases it would produce unnecessary effort to ask for providing another component (the operator) that deploys the controller.
To simplify this situation Gardener will be able to react on <code>ControllerInstallation</code>s specifying <code>.spec.registration.deployment.type=helm</code>.
The controller would be registered with the <code>ControllerRegistration</code> resources that would contain a Helm chart with all resources needed to deploy this controller into a seed (plus some static values).
Gardener would render the Helm chart and deploy the resources into the seed.
It will not react if <code>.spec.registration.deployment.type!=helm</code> which allows to also use any other deployment mechanism. Controllers that are getting deployed by operators would not specify the <code>.spec.deployment</code> section in the <code>ControllerRegistration</code> at all.</p><p>ℹ Any controller requiring dynamic configuration values (e.g., based on the cloud provider or the region of the seed) must be installed with the operator approach.</p><h2 id=other-cloud-specific-parts>Other cloud-specific parts</h2><p>The Gardener API server has a few admission controllers that contain cloud-specific code as well. We have to replace these parts as well.</p><h3 id=defaulting-and-validation-admission-plugins>Defaulting and validation admission plugins</h3><p>Right now, the admission controllers inside the Gardener API server do perform a lot of validation and defaulting of fields in the Shoot specification.
The cloud-specific parts of these admission controllers will be replaced by mutating admission webhooks that will get called instead.
As we will have a dedicated operator running in the Garden cluster anyway it will also get the responsibility to register this webhook if it needs to validate/default parts of the Shoot specification.</p><p>Example: The <code>.spec.cloud.workerPools[*].providerConfig.machineImage</code> field in the new Shoot manifest mentioned above could be omitted by the user and would get defaulted by the cloud-specific operator.</p><h3 id=dns-hosted-zone-admission-plugin>DNS Hosted Zone admission plugin</h3><p>For the same reasons the existing DNS Hosted Zone admission plugin will be removed from the Gardener core and moved into the responsibility of the respective DNS-specific operators running in the Garden cluster.</p><h3 id=shoot-quota-admission-plugin>Shoot Quota admission plugin</h3><p>The Shoot quota admission plugin validates create or update requests on Shoots and checks that the specified machine/storage configuration is defined as per referenced <code>Quota</code> objects.
The cloud-specifics in this controller are no longer needed as the <code>CloudProfile</code> and the <code>Shoot</code> resource have been adapted:
The machine/storage configuration is no longer in cloud-specific sections but hard-wired fields in the general <code>Shoot</code> specification (see example resources above).
The quota admission plugin will be simplified and remains in the Gardener core.</p><h3 id=shoot-maintenance-controller>Shoot maintenance controller</h3><p>Every Shoot cluster can define a maintenance time window in which Gardener will update the Kubernetes patch version (if enabled) and the used machine image version in the Shoot resource.
While the Kubernetes version is not part of the <code>providerConfig</code> section in the <code>CloudProfile</code> resource, the <code>machineImage</code> field is, and thus Gardener can&rsquo;t understand it any longer.
In the future Gardener has to rely on the cloud-specific operator (probably the same doing the defaulting/validation mentioned before) to update this field.
In the maintenance time window the maintenance controller will update the Kubernetes patch version (if enabled) and add a <code>trigger.gardener.cloud=maintenance</code> annotation in the Shoot resource.
The already registered mutating web hook will call the operator who has to remove this annotation and update the <code>machineImage</code> in the <code>.spec.cloud.workerPools[*].providerConfig</code> sections.</p><h2 id=alternatives>Alternatives</h2><ul><li>Alternative to DNS approach for Shoot control plane movement/migration: We have thought about rotating the credentials when a move is triggered which would make all controllers ineffective immediately. However, one problem with this is that we require IAM privileges for the users infrastructure account which might be not desired. Another, more complicated problem is that we cannot assume API access in order to create technical users for all cloud environments that might be supported.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b5f9b425f9c9a3fa5345982414974ca5>2 - 02 Backupinfra</h1><h1 id=backup-infrastructure-crd-and-controller-redesign>Backup Infrastructure CRD and Controller Redesign</h1><h2 id=goal>Goal</h2><ul><li>As an operator, I would like to efficiently use the backup bucket for multiple clusters, thereby limiting the total number of buckets required.</li><li>As an operator, I would like to use different cloud provider for backup bucket provisioning other than cloud provider used for seed infrastructure.</li><li>Have seed independent backups, so that we can easily migrate a shoot from one seed to another.</li><li>Execute the backup operations (including bucket creation and deletion) from a seed, because network connectivity may only be ensured from the seeds (not necessarily from the garden cluster).</li><li>Preserve the garden cluster as source of truth (no information is missing in the garden cluster to reconstruct the state of the backups even if seed and shoots are lost completely).</li><li>Do not violate the infrastructure limits in regards to blob store limits/quotas.</li></ul><h2 id=motivation>Motivation</h2><p>Currently, every shoot cluster has its own etcd backup bucket with a centrally configured retention period. With the growing number of clusters, we are soon running out of the <a href=https://gist.github.com/swapnilgm/5c4d5506811e63c32ab3d73c4171d30f>quota limits of buckets on the cloud provider</a>. Moreover, even if the clusters are deleted, the backup buckets do exist, for a configured period of retention. Hence, there is need of minimizing the total count of buckets.</p><p>In addition, currently we use seed infrastructure credentials to provision the bucket for etcd backups. This results in binding backup bucket provider to seed infrastructure provider.</p><h2 id=terminology>Terminology</h2><ul><li><strong>Bucket</strong> : It is equivalent to s3 bucket, abs container, gcs bucket, swift container, alicloud bucket</li><li><strong>Object</strong> : It is equivalent s3 object, abs blob, gcs object, swift object, alicloud object, snapshot/backup of etcd on object store.</li><li><strong>Directory</strong> : As such there is no concept of directory in object store but usually the use directory as <code>/</code> separate common prefix for set of objects. Alternatively they use term folder for same.</li><li><strong>deletionGracePeriod</strong>: This means grace period or retention period for which backups will be persisted post deletion of shoot.</li></ul><h2 id=current-spec>Current Spec:</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-YAML data-lang=YAML><span style=display:flex><span><span style=color:green>#BackupInfra spec</span>
</span></span><span style=display:flex><span>Kind: BackupInfrastructure
</span></span><span style=display:flex><span>Spec:
</span></span><span style=display:flex><span>    seed: seedName
</span></span><span style=display:flex><span>    shootUID : shoot.status.uid
</span></span></code></pre></div><h2 id=current-naming-conventions>Current naming conventions</h2><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>SeedNamespace :</td><td>Shoot&ndash;projectname&ndash;shootname</td></tr><tr><td>seed:</td><td>seedname</td></tr><tr><td>ShootUID :</td><td>shoot.status.UID</td></tr><tr><td>BackupInfraname:</td><td>seednamespce+sha(uid)[:5]</td></tr><tr><td>Backup-bucket-name:</td><td>BackupInfraName</td></tr><tr><td>BackupNamespace:</td><td>backup&ndash;BackupInfraName</td></tr></tbody></table><h2 id=proposal>Proposal</h2><p>Considering <a href=/docs/gardener/proposals/01-extensibility/#backup-infrastructure-provisioning>Gardener extension proposal</a> in mind, the backup infrastructure controller can be divided in two parts. There will be basically four backup infrastructure related CRD&rsquo;s. Two on the garden apiserver. And two on the seed cluster. Before going into to workflow, let&rsquo;s just first have look at the CRD.</p><h3 id=crd-on-garden-cluster>CRD on Garden cluster</h3><p>Just to give brief before going into the details, we will be sticking to the fact that Garden apiserver is always source of truth. Since backupInfra will be maintained post deletion of shoot, the info regarding this should always come from garden apiserver, we will continue to have BackupInfra resource on garden apiserver with some modifications.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: garden.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: BackupBucket
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: packet-region1-uid[:5]
</span></span><span style=display:flex><span>  <span style=color:green># No namespace needed. This will be cluster scope resource.</span>
</span></span><span style=display:flex><span>  ownerReferences:
</span></span><span style=display:flex><span>  - kind: CloudProfile
</span></span><span style=display:flex><span>    name: packet
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider: aws
</span></span><span style=display:flex><span>  region: eu-west-1
</span></span><span style=display:flex><span>  secretRef: <span style=color:green># Required for root</span>
</span></span><span style=display:flex><span>    name: backup-operator-aws
</span></span><span style=display:flex><span>    namespace: garden
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  lastOperation: ...
</span></span><span style=display:flex><span>  observedGeneration: ...
</span></span><span style=display:flex><span>  seed: ...
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: garden.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: BackupEntry
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: shoot--dev--example--3ef42 <span style=color:green># Naming convention explained before</span>
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>  ownerReferences:
</span></span><span style=display:flex><span>  - apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>    blockOwnerDeletion: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>    controller: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    kind: Shoot
</span></span><span style=display:flex><span>    name: example
</span></span><span style=display:flex><span>    uid: 19a9538b-5058-11e9-b5a6-5e696cab3bc8
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  shootUID: 19a9538b-5058-11e9-b5a6-5e696cab3bc8 <span style=color:green># Just for reference to find back associated shoot.</span>
</span></span><span style=display:flex><span>  <span style=color:green># Following section comes from cloudProfile or seed yaml based on granularity decision.</span>
</span></span><span style=display:flex><span>  bucketName: packet-region1-uid[:5]
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  lastOperation: ...
</span></span><span style=display:flex><span>  observedGeneration: ...
</span></span><span style=display:flex><span>  seed: ...
</span></span></code></pre></div><h3 id=crd-on-seed-cluster>CRD on Seed cluster</h3><p>Considering the extension proposal, we want individual component to be handled by controller inside seed cluster. We will have Backup related resource in registered seed cluster as well.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: BackupBucket
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: packet-random[:5]
</span></span><span style=display:flex><span>  <span style=color:green># No namespace need. This will be cluster scope resource</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: aws
</span></span><span style=display:flex><span>  region: eu-west-1
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: backup-operator-aws
</span></span><span style=display:flex><span>    namespace: backup-garden
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  observedGeneration: ...
</span></span><span style=display:flex><span>  state: ...
</span></span><span style=display:flex><span>  lastError: ..
</span></span><span style=display:flex><span>  lastOperation: ...
</span></span></code></pre></div><p>There are two points for introducing BackupEntry resource.</p><ol><li>Cloud provider specific code goes completely in seed cluster.</li><li>Network issue is also handled by moving deletion part to backup-extension-controller in seed cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: BackupEntry
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: shoot--dev--example--3ef42 <span style=color:green># Naming convention explained later</span>
</span></span><span style=display:flex><span>  <span style=color:green># No namespace need. This will be cluster scope resource</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: aws
</span></span><span style=display:flex><span>  region: eu-west-1
</span></span><span style=display:flex><span>  secretRef: <span style=color:green># Required for root</span>
</span></span><span style=display:flex><span>    name: backup-operator-aws
</span></span><span style=display:flex><span>    namespace: backup-garden
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  observedGeneration: ...
</span></span><span style=display:flex><span>  state: ...
</span></span><span style=display:flex><span>  lastError: ..
</span></span><span style=display:flex><span>  lastOperation: ...
</span></span></code></pre></div><h3 id=workflow>Workflow</h3><ul><li>Gardener administrator will configure the cloudProfile with backup infra credentials and provider config as follows.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green># CloudProfile.yaml:</span>
</span></span><span style=display:flex><span>Spec:
</span></span><span style=display:flex><span>    backup:
</span></span><span style=display:flex><span>        provider: aws
</span></span><span style=display:flex><span>        region: eu-west-1
</span></span><span style=display:flex><span>        secretRef:
</span></span><span style=display:flex><span>            name: backup-operator-aws
</span></span><span style=display:flex><span>            namespace: garden
</span></span></code></pre></div><p>Here CloudProfileController will interpret this spec as follows:</p><ul><li>If <code>spec.backup</code> is nil<ul><li>No backup for any shoot.</li></ul></li><li>If <code>spec.backup.region</code> is not nil,<ul><li>Then respect it, i.e. use the provider and unique region field mentioned there for BackupBucket.</li><li>Here Preferably, <code>spec.backup.region</code> field will be unique, since for cross provider, it doesn’t make much sense. Since region name will be different for different providers.</li></ul></li><li>Otherwise, spec.backup.region is nil then,<ul><li>If same provider case i.e. spec.backup.provider = spec.(type-of-provider) or nil,<ul><li>Then, for each region from <code>spec.(type-of-provider).constraints.regions</code> create a <code>BackupBucket</code> instance. This can be done lazily i.e. create <code>BackupBucket</code> instance for region only if some seed actually spawned in the region has been registered. This will avoid creating IaaS bucket even if no seed is registered in that region, but region is listed in <code>cloudprofile</code>.</li><li>Shoot controller will choose backup container as per the seed region. (With shoot control plane migration also, seed’s availability zone might change but the region will be remaining same as per current scope.)</li></ul></li><li>Otherwise cross provider case i.e. spec.backup.provider != spec.(type-of-provider)<ul><li>Report validation error: Since, for example, we can’t expect <code>spec.backup.provider</code> = <code>aws</code> to support region in, <code>spec.packet.constraint.region</code>. Where type-of-provider is <code>packet</code></li></ul></li></ul></li></ul><p>Following diagram represent overall flow in details:</p><p><img src=/__resources/02-backupinfra-provisioning-sequence-diagram_c08a3e.svg alt=sequence-diagram></p><h4 id=reconciliation>Reconciliation</h4><p>Reconciliation on backup entry in seed cluster mostly comes in picture at the time of deletion. But we can add initialization steps like creation of <a href=#terminology>directory</a> specific to shoot in backup bucket. We can simply create BackupEntry at the time of shoot deletion as well.</p><h4 id=deletion>Deletion</h4><ul><li>On shoot deletion, the BackupEntry instance i.e. shoot specific instance will get deletion timestamp because of ownerReference.</li><li>If <code>deletionGracePeriod</code> configured in GCM component configuration is expired, BackupInfrastructure Controller will delete the backup folder associated with it from backup object store.</li><li>Finally, it will remove the <code>finalizer</code> from backupEntry instance.</li></ul><h3 id=alternative>Alternative</h3><p><img src=/__resources/02-backupinfra-provisioning-with-deletion-job_e51f05.svg alt=sequence-diagram></p><h2 id=discussion-points--variations>Discussion points / variations</h2><h3 id=manual-vs-dynamic-bucket-creation>Manual vs dynamic bucket creation</h3><ul><li><p>As per limit observed on different cloud providers, we can have single bucket for backups on one cloud providers. So, we could avoid the little complexity introduced in above approach by pre-provisioning buckets as a part of landscape setup. But there won&rsquo;t be anybody to detect bucket existence and its reconciliation. Ideally this should be avoided.</p></li><li><p>Another thing we can have is, we can let administrator register the pool of root backup infra resource and let the controller schedule backup on one of this.</p></li><li><p>One more variation here could be to create bucket dynamically per hash of shoot UID.</p></li></ul><h3 id=sdk-vs-terraform>SDK vs Terraform</h3><p>Initial reason for going for terraform script is its stability and the provided parallelism/concurrency in resource creation. For backup infrastructure, Terraform scripts are very minimal right now. Its simply have bucket creation script. With shared bucket logic, if possible we might want to isolate access at <a href=#terminology>directory</a> level but again its additional one call. So, we will prefer switching to SDK for all object store operations.</p><h3 id=limiting-the-number-of-shoots-per-bucket>Limiting the number of shoots per bucket</h3><p>Again as per limit observed on different cloud providers, we can have single bucket for backups on one cloud providers. But if we want to limit the number of shoots associated with bucket, we can have central map of configuration in <code>gardener-controller-component-configuration.yaml</code>.
Where we will mark supported count of shoots per cloud provider. Most probable space could be,
<code>controller.backupInfrastructures.quota</code>. If limit is reached we can create new <code>BucketBucket</code> instance.</p><p>e.g.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: controllermanager.config.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControllerManagerConfiguration
</span></span><span style=display:flex><span>controllers:
</span></span><span style=display:flex><span>  backupInfrastructure:
</span></span><span style=display:flex><span>    quota:
</span></span><span style=display:flex><span>      - provider: aws
</span></span><span style=display:flex><span>        limit: 100 <span style=color:green># Number mentioned here are random, just for example purpose.</span>
</span></span><span style=display:flex><span>      - provider: azure
</span></span><span style=display:flex><span>        limit: 80
</span></span><span style=display:flex><span>      - provider: openstack
</span></span><span style=display:flex><span>        limit: 100
</span></span><span style=display:flex><span>      ...
</span></span></code></pre></div><h2 id=backward-compatibility>Backward compatibility</h2><h3 id=migration>Migration</h3><ul><li>Create shoot specific folder.</li><li>Transfer old objects.</li><li>Create manifest of objects on new bucket<ul><li>Each entry will have status: None,Copied, NotFound.</li><li>Copy objects one by one.</li></ul></li><li>Scale down etcd-main with old config. ⚠️ Cluster down time</li><li>Copy remaining objects</li><li>Scale up etcd-main with new config.</li><li>Destroy Old bucket and old backup namespace. It can be immediate or preferably <strong>lazy</strong> deletion.</li></ul><p><img src=/__resources/02-backupinfra-migration_a671d5.svg alt=backup-migration-sequence-diagram></p><h3 id=legacy-mode-alternative>Legacy Mode alternative</h3><ul><li>If Backup namespace present in seed cluster, then follow the legacy approach.</li><li>i.e. reconcile creation/existence of shoot specific bucket and backup namespace.</li><li>If backup namespace is not created, use shared bucket.</li><li><strong>Limitation</strong> Never know when the existing cluster will be deleted, and hence, it might be little difficult to maintain with next release of gardener. This might look simple and straight-forward for now but may become pain point in future, if in worst case, because of some new use cases or refactoring, we have to change the design again. Also, even after multiple garden release we won&rsquo;t be able to remove deprecated existing BackupInfrastructure CRD</li></ul><h3 id=references>References</h3><ul><li><a href=/docs/gardener/proposals/01-extensibility/#backup-infrastructure-provisioning>Gardener extension proposal</a></li><li><a href=https://gist.github.com/swapnilgm/5c4d5506811e63c32ab3d73c4171d30f>Cloud providers object store limit comparison</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-7f676322b31081c2c8e84da2b175e0a4>3 - 03 Networking Extensibility</h1><h1 id=network-extensibility>Network Extensibility</h1><p>Currently Gardener follows a mono network-plugin support model (i.e., Calico). Although this can seem to be the more stable approach, it does not completely reflect the real use-case. This proposal brings forth an effort to add an extra level of customizability to Gardener networking.</p><h2 id=motivation>Motivation</h2><p>Gardener is an open-source project that provides a nested user model. Basically, there are two types of services provided by Gardener to its users:</p><ul><li><strong>Managed</strong>: users only request a Kubernetes cluster (Clusters-as-a-Service)</li><li><strong>Hosted</strong>: users utilize Gardener to provide their own managed version of Kubernetes (Cluster-Provisioner-as-a-service)</li></ul><p>For the first set of users, the choice of network plugin might not be so important, however, for the second class of users (i.e., Hosted) it is important to be able to customize networking based on their needs.</p><p>Furthermore, Gardener provisions clusters on different cloud-providers with different networking requirements. For example, Azure does not support Calico Networking [1], this leads to the introduction of manual exceptions in static add-on charts which is error prune and can lead to failures during upgrades.</p><p>Finally, every provider is different, and thus the network always needs to adapt to the infrastructure needs to provider better performance. Consistency does not necessarily lie in the implementation but in the interface.</p><h2 id=gardener-network-extension>Gardener Network Extension</h2><p>The goal of the Gardener Network Extensions is to support different network plugin, therefore, the specification for the network resource won&rsquo;t be fixed and will be customized based on the underlying network plugin. To do so, a <code>NetworkConfig</code> field in the spec will be provided where each plugin will define. Below is an example for deploy Calico as the cluster network plugin.</p><h3 id=long-term-spec>Long Term Spec</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Network
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: calico-network
</span></span><span style=display:flex><span>  namespace: shoot--core--test-01
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: calico
</span></span><span style=display:flex><span>  clusterCIDR: 192.168.0.0/24
</span></span><span style=display:flex><span>  serviceCIDR:  10.96.0.0/24
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: calico.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: NetworkConfig
</span></span><span style=display:flex><span>    ipam:
</span></span><span style=display:flex><span>      type: host-local
</span></span><span style=display:flex><span>      cidr: usePodCIDR
</span></span><span style=display:flex><span>    backend: bird
</span></span><span style=display:flex><span>    typha:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  observedGeneration: ...
</span></span><span style=display:flex><span>  state: ...
</span></span><span style=display:flex><span>  lastError: ..
</span></span><span style=display:flex><span>  lastOperation: ...
</span></span><span style=display:flex><span>  providerStatus:
</span></span><span style=display:flex><span>    apiVersion: calico.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: NetworkStatus
</span></span><span style=display:flex><span>    components:
</span></span><span style=display:flex><span>      kubeControllers: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      calicoNodes: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    connectivityTests:
</span></span><span style=display:flex><span>      pods: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      services: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    networkModules:
</span></span><span style=display:flex><span>      arp_proxy: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    config:
</span></span><span style=display:flex><span>      clusterCIDR: 192.168.0.0/24
</span></span><span style=display:flex><span>      serviceCIDR:  10.96.0.0/24
</span></span><span style=display:flex><span>      ipam:
</span></span><span style=display:flex><span>        type: host-local
</span></span><span style=display:flex><span>        cidr: usePodCIDR
</span></span></code></pre></div><h3 id=first-implementation-short-term>First Implementation (Short Term)</h3><p>As an initial implementation the network plugin type will be specified by the user e.g., Calico (without further configuration in the provider spec). This will then be used to generate
the <code>Network</code> resource in the seed. The Network operator will pick it up, and apply the configuration based on the <code>spec.cloudProvider</code> specified directly to the shoot or via the
Gardener resource manager (still in the works).</p><p>The <code>cloudProvider</code> field in the spec is just an initial catalyst but not meant to be stay long-term. In the future, the network provider configuration will be customized to match the best
needs of the infrastructure.</p><p>Here is how the simplified initial spec would look like:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Network
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: calico-network
</span></span><span style=display:flex><span>  namespace: shoot--core--test-01
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: calico
</span></span><span style=display:flex><span>  cloudProvider: {aws,azure,...}
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  observedGeneration: 2
</span></span><span style=display:flex><span>  lastOperation: ...
</span></span><span style=display:flex><span>  lastError: ...
</span></span></code></pre></div><h2 id=functionality>Functionality</h2><p>The network resource need to be created early-on during cluster provisioning. Once created, the Network operator residing in every seed will create all the necessary networking resources and apply them to the shoot cluster.</p><p>The status of the Network resource should reflect the health of the networking components as well as additional tests if required.</p><h2 id=references>References</h2><p>[1] <a href=https://docs.projectcalico.org/v3.0/reference/public-cloud/azure>Azure support for Calico Networking</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-d76ec792a311963802e5b8c02ff3c5d5>4 - 05 Versioning Policy</h1><h1 id=gardener-versioning-policy>Gardener Versioning Policy</h1><p>Please refer to <a href=/docs/gardener/usage/shoot_versions/>this document</a> for the documentation of the implementation of this GEP.</p><h2 id=goal>Goal</h2><ul><li>As a Garden operator I would like to define a clear Kubernetes version policy, which informs my users about deprecated or expired Kubernetes versions.</li><li>As an user of Gardener, I would like to get information which Kubernetes version is supported for how long. I want to be able to get this information via API (cloudprofile) and also in the Dashboard.</li></ul><h2 id=motivation>Motivation</h2><p>The Kubernetes community releases <strong>minor</strong> versions roughly every three months and usually maintains <strong>three minor</strong> versions (the actual and the last two) with bug fixes and security updates. Patch releases are done more frequently. Operators of Gardener should be able to define their own Kubernetes version policy. This GEP suggests the possibility for operators to classify Kubernetes versions, while they are going through their &ldquo;maintenance life-cycle&rdquo;.</p><h2 id=kubernetes-version-classifications>Kubernetes Version Classifications</h2><p>An operator should be able to classify Kubernetes versions differently while they go through their &ldquo;maintenance life-cycle&rdquo;, starting with <strong>preview</strong>, <strong>supported</strong>, <strong>deprecated</strong>, and finally <strong>expired</strong>. This information should be programmatically available in the <code>cloudprofiles</code> of the Garden cluster as well as in the Dashboard. Please also note, that Gardener keeps the control plane and the workers on the same Kubernetes version.</p><p>For further explanation of the possible classifications, we assume that an operator wants to support four minor versions e.g. v1.16, v1.15, v1.14 and v1.13.</p><ul><li><p><strong>preview:</strong> After a fresh release of a new Kubernetes <strong>minor</strong> version (e.g. v1.17.0) the operator could tag it as <em>preview</em> until he has gained sufficient experience. It will not become the default in the Gardener Dashboard until he promotes that minor version to <em>supported</em>, which could happen a few weeks later with the first patch version.</p></li><li><p><strong>supported:</strong> The operator would tag the latest Kubernetes patch versions of the actual (if not still in <em>preview</em>) and the last three minor Kubernetes versions as <em>supported</em> (e.g. v1.16.1, v1.15.4, v1.14.9 and v1.13.12). The latest of these becomes the default in the Gardener Dashboard (e.g. v1.16.1).</p></li><li><p><strong>deprecated:</strong> The operator could decide, that he generally wants to classify every version that is not the latest patch version as <em>deprecated</em> and flag this versions accordingly (e.g. v1.16.0 and older, v1.15.3 and older, 1.14.8 and older as well as v1.13.11 and older). He could also tag all versions (latest or not) of every Kubernetes minor release that is neither the actual nor one of the last three minor Kubernetes versions as <em>deprecated</em>, too (e.g. v1.12.x and older). Deprecated versions will eventually expire (i.e., removed).</p></li><li><p><strong>expired:</strong> This state is a <em>logical</em> state only. It doesn&rsquo;t have to be maintained in the <code>cloudprofile</code>. All cluster versions whose <code>expirationDate</code> as defined in the <code>cloudprofile</code> is expired, are automatically in this <em>logical</em> state. After that date has passed, users cannot create new clusters with that version anymore and any cluster that is on that version will be forcefully migrated in its next maintenance time window, even if the owner has opted out of automatic cluster updates! The forceful update will pick the latest patch version of the current minor Kubernetes version. If the cluster was already on that latest patch version and the latest patch version is also expired, it will continue with latest patch version of the <strong>next minor Kubernetes version</strong>, so <strong>it will result in an update of a minor Kubernetes version, which is potentially harmful to your workload, so you should avoid that/plan ahead!</strong> If that&rsquo;s expired as well, the update process repeats until a non-expired Kubernetes version is reached, so <strong>depending on the circumstances described above, it can happen that the cluster receives multiple consecutive minor Kubernetes version updates!</strong></p></li></ul><p>To fulfill his specific versioning policy, the Garden operator should be able to classify his versions as well set the expiration date in the <code>cloudprofiles</code>. The user should see this classifiers as well as the expiration date in the dashboard.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-65e79b644bcfac2ee0ea485728885ad7>5 - 06 Etcd Druid</h1><h1 id=integrating-etcd-druid-with-gardener>Integrating etcd-druid with Gardener</h1><p>Etcd is currently deployed by garden-controller-manager as a Statefulset. The sidecar container spec contains details pertaining to cloud-provider object-store which is injected into the statefulset via a mutable webhook running as part of the gardener extension <a href=/docs/gardener/extensions/controlplane-webhooks/#what-needs-to-be-implemented-to-support-a-new-cloud-provider>story</a>. This approach restricts the operations on etcd such as scale-up and upgrade. Etcd-druid will eliminate the need to hijack statefulset creation to add cloudprovider details. It has been designed to provide an intricate control over the procedure of deploying and maintaining etcd. The roadmap for etcd-druid can be found <a href=https://github.com/gardener/etcd-druid/issues/2>here</a>.</p><p>This document explains how Gardener deploys etcd and what resources it creates for etcd-druid to deploy an etcd cluster.</p><h2 id=resources-required-by-etcd-druid-created-by-gardener>Resources required by etcd-druid (created by Gardener)</h2><ul><li>Secret containing credentials to access backup bucket in Cloud provider object store.</li><li>TLS server and client secrets for etcd and backup-sidecar</li><li>Etcd CRD resource that contains parameters pertaining to etcd, backup-sidecar and cloud-provider object store.</li></ul><p>When an etcd resource is created in the cluster, the druid acts on it by creating an etcd statefulset, a service and a configmap containing etcd bootstrap script. The secrets containing the infrastructure credentials and the TLS certificates are mounted as volumes. If no secret/information regarding backups is stated then etcd data backups are not taken. Only data corruption checks are performed prior to starting etcd.</p><p>Garden-controller-manager, being cloud agnostic, deploys the etcd resource. This will not contain any cloud-specific information other than the cloud-provider. The extension controller that contains the cloud specific implementation to create the backup bucket will create it if needed and create a secret containing the credentials to access the bucket. The etcd backup secret name should be exposed in the BackupEntry status. Then, Gardener can read it and write it into the ETCD resource. The secret will have to be made available in the namespace the etcd statefulset will be deployed. If etcd and backup-sidecar communicates over TLS then the CA certificates, server and client certificates, and keys will also have to be made available in the namespace as well. The etcd resource will have reference to these aforementioned secrets. etcd-druid will deploy the statefulset only if the secrets are available.</p><h2 id=workflow>Workflow</h2><ul><li>etcd-druid will be deployed and etcd CRD will be created as part of the seed bootstrap.</li><li>Garden-controller-manager creates backupBucket extension resource. Extension controller creates the backup bucket associated with the seed.</li><li>Garden-controller-manager creates backupentry associated with each shoot in the seed namespace.</li><li>Garden-controller-manager creates etcd resource with secretRefs and etcd information populated appropriately.</li><li>etcd-druid acts on the etcd resource; druid creates the statefulset, the service and the configmap.</li></ul><p><img src=/__resources/druid_integration_539d05.png alt=etcd-druid></p></div><div class=td-content style=page-break-before:always><h1 id=pg-8da6d960cda1d76486946f507800f02b>6 - 07 Shoot Control Plane Migration</h1><h1 id=shoot-control-plane-migration>Shoot Control Plane Migration</h1><h2 id=motivation>Motivation</h2><p>Currently moving the control plane of a shoot cluster can only be done manually and requires deep knowledge of how exactly to transfer the resources and state from one seed to another. This can make it slow and prone to errors.</p><p>Automatic migration can be very useful in a couple of scenarios:</p><ul><li>Seed goes down and can&rsquo;t be repaired (fast enough or at all) and it&rsquo;s control planes need to be brought to another seed</li><li>Seed needs to be changed, but this operation requires the recreation of the seed (e.g. turn a single-AZ seed into a multi-AZ seed)</li><li>Seeds need to be rebalanced</li><li>New seeds become available in a region closer to/in the region of the workers and the control plane should be moved there to improve latency</li><li>Gardener ring, which is a self-supporting setup/underlay for a highly available (usually cross-region) Gardener deployment</li></ul><h2 id=goals>Goals</h2><ul><li>Provide a mechanism to migrate the control plane of a shoot cluster from one seed to another</li><li>The mechanism should support migration from a seed which is no longer reachable (Disaster Recovery)</li><li>The shoot cluster nodes are preserved and continue to run the workload, but will talk to the new control plane after the migration completes</li><li>Extension controllers implement a mechanism which allows them to store their state or to be restored from an already existing state on a different seed cluster.</li><li>The already existing shoot reconciliation flow is reused for migration with minimum changes</li></ul><h2 id=terminology>Terminology</h2><p><strong>Source Seed</strong> is the seed which currently hosts the control plane of a Shoot Cluster</p><p><strong>Destination Seed</strong> is the seed to which the control plane is being migrated</p><h2 id=resources-and-controller-state-which-have-to-be-migrated-between-two-seeds>Resources and controller state which have to be migrated between two seeds:</h2><p><strong>Note:</strong> The following lists are just FYI and are meant to show the current resources which need to be moved to the <strong>Destination Seed</strong></p><h3 id=secrets>Secrets</h3><p>Gardener has preconfigured lists of needed secrets which are generated when a shoot is created and deployed in the seed. Following is a minimum set of secrets which must be migrated to the <strong>Destination Seed</strong>. Other secrets can be regenerated from them.</p><ul><li>ca</li><li>ca-front-proxy</li><li>static-token</li><li>ca-kubelet</li><li>ca-metrics-server</li><li>etcd-encryption-secret</li><li>kube-aggregator</li><li>kube-apiserver-basic-auth</li><li>kube-apiserver</li><li>service-account-key</li><li>ssh-keypair</li></ul><h3 id=custom-resources-and-state-of-extension-controllers>Custom Resources and state of extension controllers</h3><p>Gardenlet deploys custom resources in the <strong>Source Seed</strong> cluster during shoot reconciliation which are reconciled by extension controllers. The state of these controllers and any additional resources they create is independent of the gardenlet and must also be migrated to the <strong>Destination Seed</strong>. Following is a list of custom resources, and the state which is generated by them that has to be migrated.</p><ul><li><strong>BackupBucket</strong>: nothing relevant for migration</li><li><strong>BackupEntry</strong>: nothing relevant for migration</li><li><strong>ControlPlane</strong>: nothing relevant for migration</li><li><strong>DNSProvider</strong>/DNSEntry: nothing relevant for migration</li><li><strong>Extensions</strong>: migration of state needs to be handled individually</li><li><strong>Infrastructure</strong>: terraform state</li><li><strong>Network</strong>: nothing relevant for migration</li><li><strong>OperatingSystemConfig</strong>: nothing relevant for migration</li><li><strong>Worker</strong>: Machine-Controller-Manager related objects: machineclasses, machinedeployments, machinesets, machines</li></ul><p>This list depends on the currently installed extensions and can change in the future</p><h2 id=proposal>Proposal</h2><h3 id=custom-resource-on-the-garden-cluster>Custom Resource on the garden cluster</h3><p>The Garden cluster has a new Custom Resource which is stored in the project namespace of the Shoot called <code>ShootState</code>. It contains all the required data described above so that the control plane can be recreated on the <strong>Destination Seed</strong>.</p><p>This data is separated into two sections. The first is generated by the gardenlet and then either used to generate new resources (e.g secrets) or is directly deployed to the Shoot&rsquo;s control plane on the <strong>Destination Seed</strong>.</p><p>The second is generated by the extension controllers in the seed.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ShootState
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-shoot
</span></span><span style=display:flex><span>  namespace: garden-core
</span></span><span style=display:flex><span>  ownerReference:
</span></span><span style=display:flex><span>    apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>    blockOwnerDeletion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    controller: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    kind: Shoot
</span></span><span style=display:flex><span>    name: my-shoot
</span></span><span style=display:flex><span>    uid: ...
</span></span><span style=display:flex><span>  finalizers:
</span></span><span style=display:flex><span>  - gardener
</span></span><span style=display:flex><span>gardenlet:
</span></span><span style=display:flex><span>  secrets:
</span></span><span style=display:flex><span>  - name: ca
</span></span><span style=display:flex><span>    data:
</span></span><span style=display:flex><span>      ca.crt: ...
</span></span><span style=display:flex><span>      ca.key: ...
</span></span><span style=display:flex><span>  - name: ssh-keypair
</span></span><span style=display:flex><span>    data:
</span></span><span style=display:flex><span>      id_rsa: ...
</span></span><span style=display:flex><span>  - name:
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>extensions:
</span></span><span style=display:flex><span>- kind: Infrastructure
</span></span><span style=display:flex><span>  state: ... (Terraform state)
</span></span><span style=display:flex><span>- kind: ControlPlane
</span></span><span style=display:flex><span>  purpose: normal
</span></span><span style=display:flex><span>  state: ... (Certificates generated by the extension)
</span></span><span style=display:flex><span>- kind: Worker
</span></span><span style=display:flex><span>  state: ... (Machine objects)
</span></span></code></pre></div><p>The state data is saved as a <code>runtime.RawExtension</code> type, which can be encoded/decoded by the corresponding extension controller.</p><p>There can be sensitive data in the <code>ShootState</code> which has to be hidden from the end-users. Hence, it will be recommended to provide an etcd encryption configuration to the Gardener API server in order to encrypt the <code>ShootState</code> resource.</p><h4 id=size-limitations>Size limitations</h4><p>There are limits on the size of the request bodies sent to the kubernetes API server when creating or updating resources: by default ETCD can only accept request bodies which do not exceed 1.5 MiB (this can be configured with the <code>--max-request-bytes</code> flag); the kubernetes API Server has a request body limit of 3 MiB which cannot be set from the outside (with a command line flag); the gRPC configuration used by the API server to talk to ETCD has a limit of 2 MiB per request body which cannot be configured from the outside; and <code>watch</code> requests have a 16 MiB limit on the buffer used to stream resources.</p><p>This means that if <code>ShootState</code> is bigger than 1.5 MiB, the ETCD max request bytes will have to be increased. However, there is still an upper limit of 2 MiB imposed by the gRPC configuration.</p><p>If <code>ShootState</code> exceeds this size limitation it must make use of configmap/secret references to store the state of extension controllers. This is an implementation detail of Gardener and can be done at a later time if necessary as extensions will not be affected.</p><p>Splitting the <code>ShootState</code> into multiple resources could have a positive benefit on performance as the Gardener API Server and Gardener Controller Manager would handle multiple small resources instead of one big resource.</p><h3 id=gardener-extensions-changes>Gardener extensions changes</h3><p>All extension controllers which require state migration must save their state in a new <code>status.state</code> field and act on an annotation <code>gardener.cloud/operation=restore</code> in the respective Custom Resources which should trigger a restoration operation instead of reconciliation. A restoration operation means that the extension has to restore its state in the Shoot&rsquo;s namespace on the <strong>Destination Seed</strong> from the <code>status.state</code> field.</p><p>As an example: the <code>Infrastructure</code> resource must save the terraform state.</p><pre tabindex=0><code>apiVersion: extensions.gardener.cloud/v1alpha1
kind: Infrastructure
metadata:
  name: infrastructure
  namespace: shoot--foo--bar
spec:
  type: azure
  region: eu-west-1
  secretRef:
    name: cloudprovider
    namespace: shoot--foo--bar
  providerConfig:
    apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
    kind: InfrastructureConfig
    resourceGroup:
      name: mygroup
    networks:
      vnet: # specify either &#39;name&#39; or &#39;cidr&#39;
      # name: my-vnet
        cidr: 10.250.0.0/16
      workers: 10.250.0.0/19
status:
  state: |
      {
          &#34;version&#34;: 3,
          &#34;terraform_version&#34;: &#34;0.11.14&#34;,
          &#34;serial&#34;: 2,
          &#34;lineage&#34;: &#34;3a1e2faa-e7b6-f5f0-5043-368dd8ea6c10&#34;,
          &#34;modules&#34;: [
              {
              }
          ]
          ...
      }
</code></pre><p>Extensions which do not require state migration should set <code>status.state=nil</code> in their Custom Resources and trigger a normal reconciliation operation if the CR contains the <code>core.gardener.cloud/operation=restore</code> annotation.</p><p>Similar to the contract for the <a href=/docs/gardener/extensions/reconcile-trigger/>reconcile operation</a>, the extension controller has to remove the <code>restore</code> annotation after the restoration operation has finished.</p><p>An additional annotation <code>gardener.cloud/operation=migrate</code> is added to the Custom Resources. It is used to tell the extension controllers in the <strong>Source Seed</strong> that they must stop reconciling resources (in case they are requeued due to errors) and should perform cleanup activities in the Shoot&rsquo;s control plane. These cleanup activities involve removing the finalizers on Custom Resources and deleting them without actually deleting any infrastructure resources.</p><p><strong>Note:</strong> The same size limitations from the previous section are relevant here as well.</p><h3 id=shoot-reconciliation-flow-changes>Shoot reconciliation flow changes</h3><p>The only data which must be stored in the <code>ShootState</code> by the gardenlet is secrets (e.g ca for the API server). Therefore the <code>botanist.DeploySecrets</code> step is changed. It is split into two functions which take a list of secrets that have to be generated.</p><ul><li><code>botanist.GenerateSecretState</code> Generates certificate authorities and other secrets which have to be persisted in the ShootState and must not be regenerated on the <strong>Destination Seed</strong>.</li><li><code>botanist.DeploySecrets</code> Takes secret data from the <code>ShootState</code>, generates new ones (e.g. client tls certificates from the saved certificate authorities) and deploys everything in the Shoot&rsquo;s control plane on the <strong>Destination Seed</strong></li></ul><h3 id=shootstate-synchronization-controller>ShootState synchronization controller</h3><p>The ShootState synchronization controller will become part of the gardenlet. It syncs the state of extension custom resources from the shoot namespace to the garden cluster and updates the corresponding <code>spec.extension.state</code> field in the <code>ShootState</code> resource. The controller can <code>watch</code> Custom Resources used by the extensions and update the <code>ShootState</code> only when changes occur.</p><h3 id=migration-workflow>Migration workflow</h3><ol><li>Starting migration<ul><li>Migration can only be started after a Shoot cluster has been successfully created so that the <code>status.seed</code> field in the <code>Shoot</code> resource has been set</li><li>The <code>Shoot</code> resource&rsquo;s field <code>spec.seedName="new-seed"</code> is edited to hold the name of the <strong>Destination Seed</strong> and reconciliation is automatically triggered</li><li>The Garden Controller Manager checks if the equality between <code>spec.seedName</code> and <code>status.seed</code>, detects that they are different and triggers migration.</li></ul></li><li>The Garden Controller Manager waits for the <strong>Destination Seed</strong> to be ready</li><li>Shoot&rsquo;s API server is stopped</li><li>Backup the Shoot&rsquo;s ETCD.</li><li>Extension resources in the <strong>Source Seed</strong> are annotated with <code>gardener.cloud/operation=migrate</code></li><li>Scale Down the Shoot&rsquo;s control plane in the <strong>Source Seed</strong>.</li><li>The gardenlet in the <strong>Destination Seed</strong> fetches the state of extension resources from the <code>ShootState</code> resource in the garden cluster.</li><li>Normal reconciliation flow is resumed in the <strong>Destination Seed</strong>. Extension resources are annotated with <code>gardener.cloud/operation=restore</code> to instruct the extension controllers to reconstruct their state.</li><li>The Shoot&rsquo;s namespace in <strong>Source Seed</strong> is deleted.</li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-7581ec6f70a5cf65db6f716b423048e5>7 - 09 Test Framework</h1><h1 id=gardener-integration-test-framework>Gardener integration test framework</h1><h2 id=motivation>Motivation</h2><p>As we want to improve our code coverage in the next months we will need a simple and easy to use test framework.
The current testframework already contains a lot of general test functions that ease the work for writing new tests.
However there are multiple disadvantages with the current structure of the tests and the testframework:</p><ol><li>Every new test is an own testsuite and therefore needs its own <code>TestDef</code> (<a href=https://github.com/gardener/gardener/tree/master/.test-defs>https://github.com/gardener/gardener/tree/master/.test-defs</a>). With this approach there will be hundreds of test definitions, growing with every new test (or at least new test suite).
But in most cases new tests do not need their own special <code>TestDef</code>: it&rsquo;s just the wrong scope for the testmachinery and will result in unnecessary complex testruns and configurations. In addition it would result in additional maintenance for a huge number of <code>TestDefs</code>.</li><li>The testsuites currently have their own specific interface/configuration that they need in order to be executed correctly (see <a href=https://github.com/gardener/gardener/blob/master/.test-defs/ShootKubernetesUpdateTest.yaml#L14>K8s Update test</a>).
Consequently the configuration has to be defined in the testruns which result in one step per test with their very own configuration which means that the testmachinery cannot simply select testdefinitions by label.
As the testmachinery cannot make use of its ability to run labeled tests (e.g. run all tests labeled <code>default</code>), the testflow size increases with every new tests and the testruns have to be manually adjusted with every new test.</li><li>The current gardener test framework contains multiple test operations where some are just used for specific tests (e.g. <code>plant_operations</code>) and some are more general (<code>garden_operation</code>). Also the functions offered by the operations vary in their specialization as some are really specific to just one test e.g. shoot test operation with <code>WaitUntilGuestbookAppIsAvailable</code> whereas others are more general like <code>WaitUntilPodIsRunning</code>.<br>This structure makes it hard for developers to find commonly used functions and also hard to integrate as the common framework grows with specialized functions.</li></ol><h2 id=goals>Goals</h2><p>In order to clean the testframework, make it easier for new developers to write tests and easier to add and maintain test execution within the testmachinery, the following goals are defined:</p><ul><li>Have a small number of test suites (gardener, shoots see <a href=#test_flavors>test flavors</a>) to only maintain a fixed number of testdefinitions.</li><li>Use ginkgo test labels (inspired by the k8s e2e tests) to differentiate test behavior, test execution and test importance.</li><li>Use standardized configuration for all tests (differ depending on the test suite) but provide better tooling to dynamically read additional configuration from configuration files like the <code>cloudprofile</code>.</li><li>Clean the testframework to only contain general functionality and keep specific functions inside the tests</li></ul><h2 id=proposal>Proposal</h2><p>The proposed new test framework consists of the following changes to tackle the above described goals.
​</p><h4 id=test-flavors>Test Flavors</h4><p>Reducing the number of test definitions is done by ​combining the current specified test suites into the following 3 general ones:</p><ul><li><em>System test suite</em><ul><li>e.g. create-shoot, delete-shoot, hibernate</li><li>need their own testdef because they have a special meaning in the context of the testmachinery</li></ul></li><li><em>Gardener test suite</em><ul><li>e.g. RBAC, scheduler</li><li>All tests that only need a gardener installation but no shoot cluster</li><li>Possible functions/environment:<ul><li>New project for test suite (copy secret binding, cleanup)?</li></ul></li></ul></li><li><em>Shoot test suite</em><ul><li>e.g. shoot app, network</li><li>Test that require a running shoot</li><li>Possible functions:<ul><li>Namespace per test</li><li>cleanup of ns</li></ul></li></ul></li></ul><p>As inspired by the k8s e2e tests, test labels are used to differentiate the tests by their behavior, their execution and their importance.
Test labels means that tests are described using predefined labels in the test&rsquo;s text (e.g <code>ginkgo.It("[BETA] this is a test")</code>).
With this labeling strategy, it is also possible to see the test properties directly in the code and promoting a test can be done via a pullrequest and will then be automatically recognized by the testmachinery with the next release.</p><p>Using ginkgo focus to only run desired tests and combined testsuites, an example test definition will look like the following.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: TestDefinition
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gardener-beta-suite
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  description: Test suite that runs all gardener tests that are labeled as beta
</span></span><span style=display:flex><span>  activeDeadlineSeconds: 7200
</span></span><span style=display:flex><span>  labels: [<span style=color:#a31515>&#34;gardener&#34;</span>, <span style=color:#a31515>&#34;beta&#34;</span>]
</span></span><span style=display:flex><span>​
</span></span><span style=display:flex><span>  command: [bash, -c]
</span></span><span style=display:flex><span>  args:
</span></span><span style=display:flex><span>  - &gt;-<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    go test -timeout=0 -mod=vendor ./test/integration/suite
</span></span></span><span style=display:flex><span><span style=color:#a31515>    --v -ginkgo.v -ginkgo.progress -ginkgo.no-color
</span></span></span><span style=display:flex><span><span style=color:#a31515>    -ginkgo.focus=&#34;[GARDENER] [BETA]&#34;</span>    
</span></span></code></pre></div><p>Using this approach, the overall number of testsuites is then reduced to a fixed number (excluding the system steps) of <code>test suites * labelCombinations</code>.</p><h4 id=framework>Framework</h4><p>The new framework will consist of a common framework, a gardener framework (integrating the commom framework) and a shoot framework (integrating the gardener framework).</p><p>All of these frameworks will have their own configuration that is exposed via commandline flags so that for example the shoot test framework can be executed by <code>go test -timeout=0 -mod=vendor ./test/integration/suite --v -ginkgo.v -ginkgo.focus="[SHOOT]" --kubecfg=/path/to/config --shoot-name=xx</code>.</p><p>The available test labels should be declared in the code with predefined values and in a predefined order so that everyone is aware about possible labels and the tests are labeled similarly across all integration tests. This approach is somehow similar to what kubernetes is doing in their e2e test suite but with some more restrictions (compare <a href=https://github.com/kubernetes/kubernetes/blob/master/test/e2e/apps/deployment.go#L84>example k8s e2e test</a>).<br>A possible solution to have consistent labeling would be to define them with every new <code>ginkgo.It</code> definition: <code>f.Beta().Flaky().It("my test")</code> which internally orders them and would produce a ginkgo test with the text : <code>[BETA] [FLAKY] my test</code>.</p><p><strong>General Functions</strong>
The test framework should include some general functions that can and will be reused by every test.
These general functions may include:
​</p><ul><li>Logging</li><li>State Dump</li><li>Detailed test output (status, duration, etc..)</li><li>Cleanup handling per test (<code>It</code>)</li><li>General easy to use functions like <code>WaitUntilDeploymentCompleted</code>, <code>GetLogs</code>, <code>ExecCommand</code>, <code>AvailableCloudprofiles</code>, etc..
​</li></ul><h4 id=example>Example</h4><p>A possible test with the new test framework would look like:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>var</span> _ = ginkgo.Describe(<span style=color:#a31515>&#34;Shoot network testing&#34;</span>, <span style=color:#00f>func</span>() {
</span></span><span style=display:flex><span>  <span style=color:green>// the testframework registers some cleanup handling for a state dump on failure and maybe cleanup of created namespaces
</span></span></span><span style=display:flex><span><span style=color:green></span>  f := framework.NewShootFramework()
</span></span><span style=display:flex><span>  f.CAfterEach(<span style=color:#00f>func</span>(ctx context.Context) {
</span></span><span style=display:flex><span>    ginkgo.By(<span style=color:#a31515>&#34;cleanup network test daemonset&#34;</span>)
</span></span><span style=display:flex><span>    err := f.ShootClient.Client().Delete(ctx, &amp;appsv1.DaemonSet{ObjectMeta: metav1.ObjectMeta{Name: name, Namespace: namespace}})
</span></span><span style=display:flex><span>    <span style=color:#00f>if</span> err != <span style=color:#00f>nil</span> {
</span></span><span style=display:flex><span>      <span style=color:#00f>if</span> !apierrors.IsNotFound(err) {
</span></span><span style=display:flex><span>        Expect(err).To(HaveOccurred())
</span></span><span style=display:flex><span>      }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }, FinalizationTimeout)
</span></span><span style=display:flex><span>  f.Release().Default().CIt(<span style=color:#a31515>&#34;should reach all webservers on all nodes&#34;</span>, <span style=color:#00f>func</span>(ctx context.Context) {
</span></span><span style=display:flex><span>    ginkgo.By(<span style=color:#a31515>&#34;Deploy the net test daemon set&#34;</span>)
</span></span><span style=display:flex><span>    templateFilepath := filepath.Join(f.ResourcesDir, <span style=color:#a31515>&#34;templates&#34;</span>, nginxTemplateName)
</span></span><span style=display:flex><span>    err := f.RenderAndDeployTemplate(f.Namespace(), tempalteFilepath)
</span></span><span style=display:flex><span>    Expect(err).ToNot(HaveOccurred())
</span></span><span style=display:flex><span>    err = f.WaitUntilDaemonSetIsRunning(ctx, f.ShootClient.Client(), name, namespace)
</span></span><span style=display:flex><span>    Expect(err).NotTo(HaveOccurred())
</span></span><span style=display:flex><span>    pods := &amp;corev1.PodList{}
</span></span><span style=display:flex><span>    err = f.ShootClient.Client().List(ctx, pods, client.MatchingLabels{<span style=color:#a31515>&#34;app&#34;</span>: <span style=color:#a31515>&#34;net-nginx&#34;</span>})
</span></span><span style=display:flex><span>    Expect(err).NotTo(HaveOccurred())
</span></span><span style=display:flex><span>    <span style=color:green>// check if all webservers can be reached from all nodes
</span></span></span><span style=display:flex><span><span style=color:green></span>    ginkgo.By(<span style=color:#a31515>&#34;test connectivity to webservers&#34;</span>)
</span></span><span style=display:flex><span>    shootRESTConfig := f.ShootClient.RESTConfig()
</span></span><span style=display:flex><span>    <span style=color:#00f>var</span> res <span style=color:#2b91af>error</span>
</span></span><span style=display:flex><span>    <span style=color:#00f>for</span> _, from := <span style=color:#00f>range</span> pods.Items {
</span></span><span style=display:flex><span>      <span style=color:#00f>for</span> _, to := <span style=color:#00f>range</span> pods.Items {
</span></span><span style=display:flex><span>        <span style=color:green>// test pods
</span></span></span><span style=display:flex><span><span style=color:green></span>        f.Logger.Infof(<span style=color:#a31515>&#34;%s to %s: %s&#34;</span>, from.GetName(), to.GetName(), data)
</span></span><span style=display:flex><span>      }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    Expect(res).ToNot(HaveOccurred())
</span></span><span style=display:flex><span>  }, NetworkTestTimeout)
</span></span><span style=display:flex><span>})
</span></span></code></pre></div><h2 id=future-plans>Future Plans</h2><h4 id=ownership>Ownership</h4><p>When the test coverage is increased and there will be more tests, we will need to track ownership for tests.
At the beginning the ownership will be shared across all maintainers of the residing repository but this is not suitable anymore as tests will grow and get more complex.</p><p>Therefore the test ownership should be tracked via subgroups (in kubernetes this would be a SIG (comp. <a href=https://github.com/kubernetes/kubernetes/blob/master/test/e2e/apps/framework.go#L22>sig apps e2e test</a>)). These subgroup will then be tracked via labels and the members of these groups will then be notified if tests fail.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3769be973f9bb9047a57a60178b9367c>8 - 10 Shoot Additional Container Runtimes</h1><h1 id=gardener-extensibility-to-support-shoot-additional-container-runtimes>Gardener extensibility to support shoot additional container runtimes</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#gardener-extensibility-to-support-shoot-additional-container-runtimes>Gardener extensibility to support shoot additional container runtimes</a><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#summary>Summary</a></li><li><a href=#motivation>Motivation</a><ul><li><a href=#goals>Goals</a></li></ul></li><li><a href=#proposal>Proposal</a></li><li><a href=#design-details>Design Details</a></li></ul></li></ul><h2 id=summary>Summary</h2><p>Gardener-managed Kubernetes clusters are sometimes used to run sensitive workloads, which sometimes are comprised of OCI images originating from untrusted sources. Additional use-cases want to leverage economy-of-scale to run workloads for multiple tenants on the same cluster. In some cases, Gardener users want to use operating systems which do not easily support the Docker engine.</p><p>This proposal aims to allow Gardener Shoot clusters to use CRI instead of the legacy Docker API, and to provide extension type for adding CRI shims (like <a href=https://gvisor.dev/>GVisor</a> and <a href=https://katacontainers.io/>Kata Containers</a>) which can be used to add support in Gardener Shoot clusters for these runtimes.</p><h2 id=motivation>Motivation</h2><p>While pods and containers are intended to create isolated areas for concurrently running workloads on nodes, this isolation is not as robust as could be expected. Containers leverage the core Linux CGroup and Namespace features to isolate workloads, and many kernel vulnerabilities have the potential to allow processes to escape from their isolation. Once a process has escaped from its container, any other process running on the same node is compromised. Several projects try to mitigate this problem; for example Kata Containers allow isolating a Kubernetes Pod in a micro-vm, gVisor reduces the kernel attack surface by adding another level of indirection between the actual payload and the real kernel.</p><p>Kubernetes supports running pods using these alternate runtimes via the <a href=https://kubernetes.io/docs/concepts/containers/runtime-class/>RuntimeClass</a> concept, which was promoted to Beta in Kubernetes 1.14. Once Kubernetes is configured to use the Container Runtime Interface to control pods, it becomes possible to leverage CRI and run specific pods using different Runtime Classes. Additionally, configuring Kubernetes to use CRI instead of the legacy Dockershim is <a href=https://events19.linuxfoundation.org/wp-content/uploads/2017/11/How-Container-Runtime-Matters-in-Kubernetes_-OSS-Kunal-Kushwaha.pdf>faster</a>.</p><p>The motivation behind this proposal is to make all of this functionality accessible to Shoot clusters managed by Gardener.</p><h3 id=goals>Goals</h3><ul><li>Gardener must allow to configue its managed clusters with the CRI interface instead of the legacy Dockershim.</li><li>Low-level runtimes like gVisor or Kata Containers are provided as gardener extensions which are (optionally) installed into a landscape by the Gardener operator. There must be no runtime-specific knowledge in the core Gardener code.</li><li>It shall be possible to configure multiple low-level runtimes in Shoot clusters, on the Worker Group level.</li></ul><h2 id=proposal>Proposal</h2><p>Gardener today assumes that all supported operating systems have Docker pre-installed in the base image. Starting with Docker Engine 1.11, Docker itself was refactored and cleaned-up to be based on the <a href=https://containerd.io/>containerd</a> library. The first phase would be to allow the change of the Kubelet configuration as described <a href=https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd>here</a> so that Kubernetes would use containerd instead of the default Dockershim. This will be implemented for CoreOS, Ubuntu, and SuSE-CHost.</p><p>We will implement two Gardener extensions, providing gVisor and Kata Containers as options for Gardener landscapes.
The <code>WorkerGroup</code> specification will be extended to allow specifying the CRI name and a list of additional required Runtimes for nodes in that group. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>workers:
</span></span><span style=display:flex><span>- name: worker-b8jg5
</span></span><span style=display:flex><span>  machineType: m5.large
</span></span><span style=display:flex><span>  volumeType: gp2
</span></span><span style=display:flex><span>  volumeSize: 50Gi
</span></span><span style=display:flex><span>  autoScalerMin: 1
</span></span><span style=display:flex><span>  autoScalerMax: 2
</span></span><span style=display:flex><span>  maxSurge: 1
</span></span><span style=display:flex><span>  cri:
</span></span><span style=display:flex><span>    name: containerd
</span></span><span style=display:flex><span>    containerRuntimes:
</span></span><span style=display:flex><span>    - type: gvisor
</span></span><span style=display:flex><span>    - type: kata-containers
</span></span><span style=display:flex><span>  machineImage:
</span></span><span style=display:flex><span>    name: coreos
</span></span><span style=display:flex><span>    version: 2135.6.0
</span></span></code></pre></div><p>Each extension will need to address the following concern:</p><ol><li>Add the low-level runtime binaries to the worker nodes. Each extension should get the runtime binaries from a container.</li><li>Hook the runtime binary into the containerd configuration file, so that the runtime becomes available to containerd.</li><li>Apply a label to each node that allows identifying nodes where the runtime is available.</li><li>Apply the relevant <code>RuntimeClass</code> to the Shoot cluster, to expose the functionality to users.</li><li>Provide a separate binary with a <code>ValidatingWebhook</code> (deployable to the garden cluster) to catch invalid configurations. For example, Kata Containers on AWS requires a <code>machineType</code> of <code>i3.metal</code>, so any <code>Shoot</code> requests with a Kata Containers runtime and a different machine type on AWS should be rejected.</li></ol><h2 id=design-details>Design Details</h2><ol><li><p>Change the nodes container runtime to work with CRI and ContainerD (Only if specified in the Shoot spec):</p><ol><li><p>In order to configure each worker machine in the cluster to work with CRI, the following configurations should be done:</p><ol><li>Add kubelet execution flags:<ol><li>&ndash;container-runtime=remote</li><li>&ndash;container-runtime-endpoint=unix:///run/containerd/containerd.sock</li></ol></li><li>Make sure that default containerd configuration file exist in path /etc/containerd/config.toml.</li></ol></li><li><p>ContainerD and Docker configurations are different for each OS. To make sure the default configurations above works well in each worker machine, each OS extension would be responsible to configure them during the reconciliation of the
OperatingSystemConfig:</p><ol><li>os-ubuntu -<ol><li>Create ContainerD unit Drop-In to execute ContainerD with the default configurations file in path /etc/containerd/config.toml.</li><li>Create the container runtime metadata file with a OS path for binaries installations: /usr/bin.</li></ol></li><li>os-coreos -<ol><li>Create ContainerD unit Drop-In to execute ContainerD with the default configurations file in path /etc/containerd/config.toml.</li><li>Create Docker Drop-In unit to execute Docker with the correct socket path of ContainerD.</li><li>Create the container runtime metadata file with a OS path for binaries installations: /var/bin.</li></ol></li><li>os-suse-chost -<ol><li>Create ContainerD service unit and execute ContainerD with the default configurations file in path /etc/containerd/config.toml.</li><li>Download and install ctr-cli which is not shipped with the current SuSe image.</li><li>Create the container runtime metadata file with a OS path for binaries installations /usr/sbin.</li></ol></li></ol></li><li><p>To rotate the ContainerD (CRI) logs we will activate the kubelet feature flag: CRIContainerLogRotation=true.</p></li><li><p>Docker monitor service will be replaced with equivalent ContainerD monitor service.</p></li></ol></li><li><p>Validate workers additional runtime configurations:</p><ol><li>Disallow additional runtimes with shoots &lt; 1.14</li><li>kata-container validation: Machine type support nested virtualization.</li></ol></li><li><p>Add support for each additional container runtime in the cluster.</p><ol><li><p>In order to install each additional available runtime in the cluster we should:</p><ol><li>Install the runtime binaries in each Worker&rsquo;s pool nodes that specified the runtime support.</li><li>Apply the relevant RuntimeClass to the cluster.</li></ol></li><li><p>The installation above should be done by a new kind of extension: ContainerRuntime resource. For each container runtime type (Kata-container/gvisor) a dedicate extension controller will be created.</p><ol><li><p>A label for each container runtime support will be added to every node that belongs to the worker pool. This should be done similar
to the way labels created today for each node, through kubelet execution parameters (_kubelet.flags: &ndash;node-labels). When creating the OperatingSystemConfig (original) for the worker each container runtime support should be mapped to a label on the node.
For Example:
label: container.runtime.kata-containers=true (shoot.spec.cloud.<iaas>.worker.containerRuntimes.kata-container)
label: container.runtime.gvisor=true (shoot.spec.cloud.<iaas>.worker.containerRuntimes.gvisor)</p></li><li><p>During the Shoot reconciliation (Similar steps to the Extensions today) Gardener will create new ContainerRuntime resource if a container runtime exist in at least one worker spec:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ContainerRuntime
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: kata-containers-runtime-extension
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: kata-containers
</span></span></code></pre></div><p>Gardener will wait that all ContainerRuntimes extensions will be reconciled by the appropriate extensions controllers.</p></li><li><p>Each runtime extension controller will be responsible to reconcile it&rsquo;s RuntimeContainer resource type.
rc-kata-containers extension controller will reconcile RuntimeContainer resource from type kata-container and rc-gvisor will reconcile RuntimeContainer resource from gvisor.
Reconciliation process by container runtime extension controllers:</p><ol><li>Runtime extension controller from specific type should apply a chart which responsible for the installation of the runtime container in the cluster:<ol><li>DaemonSet which will run a privileged pod on each node with the label: container.runtime.<type of the resource>:true The pod will be responsible for:<ol><li>Copy the runtime container binaries (From extension package ) to the relevant path in the host OS.</li><li>Add the relevant container runtime plugin section to the containerd configuration file (/etc/containerd/config.toml).</li><li>Restart containerd in the node.</li></ol></li><li>RuntimeClasses in the cluster to support the runtime class. for example:<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: node.k8s.io/v1beta1
</span></span><span style=display:flex><span>kind: RuntimeClass
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gvisor
</span></span><span style=display:flex><span>handler: runsc
</span></span></code></pre></div></li></ol></li><li>Update the status of the relevant RuntimeContainer resource to succeeded.</li></ol></li></ol></li></ol></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-f79a3d9aaa070750a3943e02881b340a>9 - 12 Oidc Webhook Authenticator</h1><h1 id=oidc-webhook-authenticator>OIDC Webhook Authenticator</h1><h2 id=problem>Problem</h2><p>In Kubernetes you can authenticate via several authentication strategies:</p><ul><li>x509 Client Certificates</li><li>Static Token Files</li><li>Bootstrap Tokens</li><li>Static Password File (Basic authentication - deprecated and removed in 1.19)</li><li>Service Account Tokens</li><li>OpenID Connect Tokens</li><li>Webhook Token Authentication</li><li>Authenticating Proxy</li></ul><p>End-users should use <a href=https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens>OpenID Connect (OIDC) Tokens</a> created by OIDC-compatible Identity Provider (IDP) and present <a href=https://openid.net/specs/openid-connect-core-1_0.html#IDToken>id_token</a> to the <code>kube-apiserver</code>. If the <code>kube-apiserver</code> is configured to trust the IDP and the token is valid, then the user is authenticated and the <a href=https://github.com/kubernetes/kubernetes/blob/99019502bd6ed038dbd1c444974d5e8c6a8dda19/staging/src/k8s.io/api/authentication/v1/types.go#L100-L117>UserInfo</a> is send to the authorization stack.</p><p>Ideally, operators of the Gardener cluster should be able to authenticate to end-user Shoot clusters with <code>id_token</code> generated by OIDC IDP, but in many cases, end-users might have already configured OIDC for their cluster and more than one OIDC configurations are not allowed.</p><p>Another interesting application of multiple OIDC providers would be per <code>Project</code> OIDC provider where end-users of Gardener can add their own OIDC-compatible IDPs.</p><p>To workaround the one OIDC per <code>kube-apiserver</code> limitation, a new <code>OIDC Webhook Authenticator</code> (OWA) could be implemented.</p><h2 id=goals>Goals</h2><ul><li>Dynamic registrations of OpenID Connect configurations.</li><li>Close as possible to the Kubernetes build-in OIDC Authenticator.</li><li>Build as an optional extension and not required for functional Shoot or Gardener cluster.</li></ul><h2 id=non-goals>Non-goals</h2><ul><li><a href=https://kubernetes.io/docs/reference/access-authn-authz/webhook/>Dynamic Authorization</a> is out of scope.</li></ul><h2 id=proposal>Proposal</h2><p>The <code>kube-apiserver</code> can use <a href=https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication>Webhook Token Authentication</a> to send a <a href=https://tools.ietf.org/html/rfc6750#section-2.1>Bearer Tokens (id_token)</a> to external webhook for validation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;apiVersion&#34;: <span style=color:#a31515>&#34;authentication.k8s.io/v1beta1&#34;</span>,
</span></span><span style=display:flex><span>  &#34;kind&#34;: <span style=color:#a31515>&#34;TokenReview&#34;</span>,
</span></span><span style=display:flex><span>  &#34;spec&#34;: {
</span></span><span style=display:flex><span>    &#34;token&#34;: <span style=color:#a31515>&#34;(BEARERTOKEN)&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Where upon verification, the remote webhook returns the identity of the user (if authentication succeeds):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;apiVersion&#34;: <span style=color:#a31515>&#34;authentication.k8s.io/v1beta1&#34;</span>,
</span></span><span style=display:flex><span>  &#34;kind&#34;: <span style=color:#a31515>&#34;TokenReview&#34;</span>,
</span></span><span style=display:flex><span>  &#34;status&#34;: {
</span></span><span style=display:flex><span>    &#34;authenticated&#34;: <span style=color:#00f>true</span>,
</span></span><span style=display:flex><span>    &#34;user&#34;: {
</span></span><span style=display:flex><span>      &#34;username&#34;: <span style=color:#a31515>&#34;janedoe@example.com&#34;</span>,
</span></span><span style=display:flex><span>      &#34;uid&#34;: <span style=color:#a31515>&#34;42&#34;</span>,
</span></span><span style=display:flex><span>      &#34;groups&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;developers&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;qa&#34;</span>
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      &#34;extra&#34;: {
</span></span><span style=display:flex><span>        &#34;extrafield1&#34;: [
</span></span><span style=display:flex><span>          <span style=color:#a31515>&#34;extravalue1&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:#a31515>&#34;extravalue2&#34;</span>
</span></span><span style=display:flex><span>        ]
</span></span><span style=display:flex><span>      }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id=registration-of-new-openidconnect>Registration of new OpenIDConnect</h3><p>This new OWA can be configured with multiple OIDC providers and the entire flow can look like this:</p><ol><li><p>Admin adds a new <code>OpenIDConnect</code> resource (via CRD) to the cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: authentication.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OpenIDConnect
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: foo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  issuerURL: https://foo.bar
</span></span><span style=display:flex><span>  clientID: some-client-id
</span></span><span style=display:flex><span>  usernameClaim: email
</span></span><span style=display:flex><span>  usernamePrefix: <span style=color:#a31515>&#34;test-&#34;</span>
</span></span><span style=display:flex><span>  groupsClaim: groups
</span></span><span style=display:flex><span>  groupsPrefix: <span style=color:#a31515>&#34;baz-&#34;</span>
</span></span><span style=display:flex><span>  supportedSigningAlgs:
</span></span><span style=display:flex><span>  - RS256
</span></span><span style=display:flex><span>  requiredClaims:
</span></span><span style=display:flex><span>    baz: bar
</span></span><span style=display:flex><span>  caBundle: LS0tLS1CRUdJTiBDRVJU...base64-encoded CA certs for issuerURL.
</span></span></code></pre></div></li><li><p>OWA watches for changes on this resource and does <a href=https://openid.net/specs/openid-connect-discovery-1_0.html>OIDC discovery</a>. The <a href=https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfigurationResponse>OIDC provider&rsquo;s configuration</a> has to be accessible under the <code>spec.issuerURL</code> with a <a href=https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig>well-known path (.well-known/openid-configuration)</a>.</p></li><li><p>OWA uses the <code>jwks_uri</code> obtained from the OIDC providers configuration, to fetch the OIDC provider&rsquo;s public keys from that endpoint.</p></li><li><p>OWA uses those keys, issuer, client_id and other settings to add an OIDC authenticator to an in-memory list of <a href="https://pkg.go.dev/k8s.io/apiserver/pkg/authentication/authenticator?tab=doc#Token">Token Authenticators</a>.</p></li></ol><p><img src=/__resources/registration_4bbe69.svg alt="alt text" title="Authentication with OIDC webhook"></p><h3 id=end-user-authentication-via-new-openidconnect-idp>End-user authentication via new OpenIDConnect IDP</h3><p>When a user presents an <code>id_token</code> obtained from a OpenID Connect the flow looks like this:</p><ol><li><p>The user authenticates against a Custom IDP.</p></li><li><p><code>id_token</code> is obtained from the Custom IDP.</p></li><li><p>The user uses <code>id_token</code> to perform an API call to <code>kube-apiserver</code>.</p></li><li><p>As the <code>id_token</code> is not matched by any build-in or configured authenticators in the <code>kube-apiserver</code>, it is send to OWA for validation.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;TokenReview&#34;: {
</span></span><span style=display:flex><span>    &#34;kind&#34;: <span style=color:#a31515>&#34;TokenReview&#34;</span>,
</span></span><span style=display:flex><span>    &#34;apiVersion&#34;: <span style=color:#a31515>&#34;authentication.k8s.io/v1beta1&#34;</span>,
</span></span><span style=display:flex><span>    &#34;spec&#34;: {
</span></span><span style=display:flex><span>      &#34;token&#34;: <span style=color:#a31515>&#34;ddeewfwef...&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></li><li><p>OWA uses <code>TokenReview</code> to authenticate the calling API server (the <code>kube-apiserver</code> for delegation of authentication and authorization may be different from the calling <code>kube-apiserver</code>).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;TokenReview&#34;: {
</span></span><span style=display:flex><span>    &#34;kind&#34;: <span style=color:#a31515>&#34;TokenReview&#34;</span>,
</span></span><span style=display:flex><span>    &#34;apiVersion&#34;: <span style=color:#a31515>&#34;authentication.k8s.io/v1beta1&#34;</span>,
</span></span><span style=display:flex><span>    &#34;spec&#34;: {
</span></span><span style=display:flex><span>      &#34;token&#34;: <span style=color:#a31515>&#34;api-server-token...&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></li><li><p>After the Authentication API server returns the identity of the calling API server:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    &#34;apiVersion&#34;: <span style=color:#a31515>&#34;authentication.k8s.io/v1&#34;</span>,
</span></span><span style=display:flex><span>    &#34;kind&#34;: <span style=color:#a31515>&#34;TokenReview&#34;</span>,
</span></span><span style=display:flex><span>    &#34;metadata&#34;: {
</span></span><span style=display:flex><span>        &#34;creationTimestamp&#34;: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    &#34;spec&#34;: {
</span></span><span style=display:flex><span>        &#34;token&#34;: <span style=color:#a31515>&#34;eyJhbGciOiJSUzI1NiIsImtpZCI6InJocEdLTXZlYjV1OE5heD...&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    &#34;status&#34;: {
</span></span><span style=display:flex><span>        &#34;authenticated&#34;: <span style=color:#00f>true</span>,
</span></span><span style=display:flex><span>        &#34;user&#34;: {
</span></span><span style=display:flex><span>            &#34;groups&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;system:serviceaccounts&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;system:serviceaccounts:shoot--abcd&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;system:authenticated&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            &#34;uid&#34;: <span style=color:#a31515>&#34;14db103e-88bb-4fb3-8efd-ca9bec91c7bf&#34;</span>,
</span></span><span style=display:flex><span>            &#34;username&#34;: <span style=color:#a31515>&#34;system:serviceaccount:shoot--abcd:kube-apiserver&#34;</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>OWA makes a <code>SubjectAccessReview</code> call to the Authorization API server to ensure that calling API server is allowed to validate tokens:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;apiVersion&#34;: <span style=color:#a31515>&#34;authorization.k8s.io/v1&#34;</span>,
</span></span><span style=display:flex><span>  &#34;kind&#34;: <span style=color:#a31515>&#34;SubjectAccessReview&#34;</span>,
</span></span><span style=display:flex><span>  &#34;spec&#34;: {
</span></span><span style=display:flex><span>    &#34;groups&#34;: [
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;system:serviceaccounts&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;system:serviceaccounts:shoot--abcd&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;system:authenticated&#34;</span>
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    &#34;nonResourceAttributes&#34;: {
</span></span><span style=display:flex><span>      &#34;path&#34;: <span style=color:#a31515>&#34;/validate-token&#34;</span>,
</span></span><span style=display:flex><span>      &#34;verb&#34;: <span style=color:#a31515>&#34;post&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    &#34;user&#34;: <span style=color:#a31515>&#34;system:serviceaccount:shoot--abcd:kube-apiserver&#34;</span>
</span></span><span style=display:flex><span>  },
</span></span><span style=display:flex><span>  &#34;status&#34;: {
</span></span><span style=display:flex><span>    &#34;allowed&#34;: <span style=color:#00f>true</span>,
</span></span><span style=display:flex><span>    &#34;reason&#34;: <span style=color:#a31515>&#34;RBAC: allowed by RoleBinding \&#34;kube-apiserver\&#34; of ClusterRole \&#34;kube-apiserver\&#34; to ServiceAccount \&#34;system:serviceaccount:shoot--abcd:kube-apiserver\&#34;&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></li><li><p>OWA then iterates over all registered <code>OpenIDConnect</code> Token authenticators and tries to validate the token.</p></li><li><p>Upon a successful validation it returns the <code>TokeReview</code> with user, groups and extra parameters:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;TokenReview&#34;: {
</span></span><span style=display:flex><span>    &#34;kind&#34;: <span style=color:#a31515>&#34;TokenReview&#34;</span>,
</span></span><span style=display:flex><span>    &#34;apiVersion&#34;: <span style=color:#a31515>&#34;authentication.k8s.io/v1beta1&#34;</span>,
</span></span><span style=display:flex><span>    &#34;spec&#34;: {
</span></span><span style=display:flex><span>      &#34;token&#34;: <span style=color:#a31515>&#34;ddeewfwef...&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    &#34;status&#34;: {
</span></span><span style=display:flex><span>      &#34;authenticated&#34;: <span style=color:#00f>true</span>,
</span></span><span style=display:flex><span>      &#34;user&#34;: {
</span></span><span style=display:flex><span>        &#34;username&#34;: <span style=color:#a31515>&#34;test-foo@bar.com&#34;</span>,
</span></span><span style=display:flex><span>        &#34;groups&#34;: [
</span></span><span style=display:flex><span>          <span style=color:#a31515>&#34;baz-employee&#34;</span>
</span></span><span style=display:flex><span>        ],
</span></span><span style=display:flex><span>        &#34;extra&#34;: {
</span></span><span style=display:flex><span>          &#34;gardener.cloud/authenticator/name&#34;: [
</span></span><span style=display:flex><span>            <span style=color:#a31515>&#34;foo&#34;</span>
</span></span><span style=display:flex><span>          ],
</span></span><span style=display:flex><span>          &#34;gardener.cloud/authenticator/uid&#34;: [
</span></span><span style=display:flex><span>            <span style=color:#a31515>&#34;e5062528-e5a4-4b97-ad83-614d015b0979&#34;</span>
</span></span><span style=display:flex><span>          ]
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>      }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>It also adds some extra information which can be used by custom authorizers later on:</p><ol><li><code>gardener.cloud/authenticator/name</code> contains the name of the <code>OpenIDConnect</code> authenticator which was used.</li><li><code>gardener.cloud/authenticator/uid</code> contains the <code>metadata.uid</code> of the <code>OpenIDConnect</code> authenticator which was used.</li></ol></li><li><p>The <code>kube-apiserver</code> proceeds with authorization checks and returns response.</p></li></ol><p>An overview of the flow:</p><p><img src=/__resources/authentication_eac090.svg alt="alt text" title="Authentication with OIDC webhook"></p><h2 id=deployment-for-shoot-clusters>Deployment for Shoot clusters</h2><p>OWA can be deployed per Shoot cluster via the <a href=https://github.com/gardener/gardener-extension-shoot-oidc-service>Shoot OIDC Service Extension</a>. The shoot&rsquo;s <code>kube-apiserver</code> is mutated so that it has the following flag configured.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>--authentication-token-webhook-config-file=/etc/webhook/kubeconfig
</span></span></code></pre></div><p>OWA on the other hand uses the shoot&rsquo;s <code>kube-apiserver</code> and delegates auth capabilities to it. This means that the needed RBAC is managed in the shoot cluster. By default only the shoot&rsquo;s <code>kube-apiserver</code> has permissions to validate tokens against OWA.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3a2b9eef5225b9d23e95c0373526624f>10 - 13 Automated Seed Management</h1><h1 id=automated-seed-management>Automated Seed Management</h1><p>Automated seed management involves automating certain aspects of managing seeds in Garden clusters, such as:</p><ul><li><a href=#ensuring-seeds-capacity-for-shoots-is-not-exceeded>Ensuring that the seeds capacity for shoots is not exceeded</a></li><li><a href=#managedseeds>Creating, deleting, and updating seeds declaratively as &ldquo;managed seeds&rdquo;</a></li><li><a href=#managedseedsets>Declaratively managing sets of similar &ldquo;managed seeds&rdquo; as &ldquo;managed seed sets&rdquo; which can be scaled up/down</a></li><li><a href=#auto-scaling-seeds>Auto-scaling seeds upon reaching capacity thresholds</a></li></ul><p>Implementing the above features would involve changes to various existing Gardener components, as well as perhaps introducing new ones. This document describes these features in more detail and proposes a design approach for some of them.</p><p>In Gardener, scheduling shoots onto seeds is quite similar to scheduling pods onto nodes in Kubernetes. Therefore, a guiding principle behind the proposed design approaches is taking advantage of best practices and existing components already used in Kubernetes.</p><h2 id=ensuring-seeds-capacity-for-shoots-is-not-exceeded>Ensuring Seeds Capacity for Shoots Is Not Exceeded</h2><p>Seeds have a practical limit of how many shoots they can accommodate. Exceeding this limit is undesirable as the system performance will be noticeably impacted. Therefore, it is important to ensure that a seed&rsquo;s capacity for shoots is not exceeded by introducing a maximum number of shoots that can be scheduled onto a seed and making sure that it is taken into account by the scheduler.</p><p>An initial discussion of this topic is available in <a href=https://github.com/gardener/gardener/issues/2938>Issue #2938</a>. The proposed solution is based on the following flow:</p><ul><li>The <code>gardenlet</code> is configured with certain <em>resources</em> and their total <em>capacity</em> (and, for certain resources, the amount reserved for Gardener).</li><li>The <code>gardenlet</code> seed controller updates the Seed status with the capacity of each resource and how much of it is actually available to be consumed by shoots, using <code>capacity</code> and <code>allocatable</code> fields that are very similar to the corresponding fields in <a href=https://github.com/kubernetes/api/blob/2c3c141c931c0ab1ce1396c3152c72852b3d37ee/core/v1/types.go#L4582-L4593>the Node status</a>.</li><li>When scheduling shoots, <code>gardener-scheduler</code> is influenced by the remaining capacity of the seed. In the simplest possible implementation, it never schedules shoots onto a seed that has already reached its capacity for a resource needed by the shoot.</li></ul><p>Initially, the only resource considered would be the maximum number of shoots that can be scheduled onto a seed. Later, more resources could be added to make more precise scheduling calculations.</p><p><strong>Note:</strong> Resources could also be requested by shoots, similarly to how pods can request node resources, and the scheduler could then ensure that such requests are taken into account when scheduling shoots onto seeds. However, the user is rarely, if at all, concerned with what resources does a shoot consume from a seed, and this should also be regarded as an implementation detail that could change in the future. Therefore, such resource requests are not included in this GEP.</p><p>In addition, an extensibility plugin framework could be introduced in the future in order to advertise custom resources, including provider-specific resources, so that <code>gardenlet</code> would be able to update the seed status with their capacity and allocatable values, for example load balancers on Azure. Such a concept is not described here in further details as it is sufficiently complex to require a separate GEP.</p><p>Example Seed status with <code>capacity</code> and <code>allocatable</code> fields:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  capacity:
</span></span><span style=display:flex><span>    shoots: <span style=color:#a31515>&#34;100&#34;</span>
</span></span><span style=display:flex><span>    persistent-volumes: <span style=color:#a31515>&#34;200&#34;</span> <span style=color:green># Built-in resource</span>
</span></span><span style=display:flex><span>    azure.provider.extensions.gardener.cloud/load-balancers: <span style=color:#a31515>&#34;30&#34;</span> <span style=color:green># Custom resource advertised by an Azure-specific plugin</span>
</span></span><span style=display:flex><span>  allocatable:
</span></span><span style=display:flex><span>    shoots: <span style=color:#a31515>&#34;100&#34;</span>
</span></span><span style=display:flex><span>    persistent-volumes: <span style=color:#a31515>&#34;197&#34;</span> <span style=color:green># 3 persistent volumes are reserved for Gardener</span>
</span></span><span style=display:flex><span>    azure.provider.extensions.gardener.cloud/load-balancers: <span style=color:#a31515>&#34;300&#34;</span>
</span></span></code></pre></div><h3 id=gardenlet-configuration>Gardenlet Configuration</h3><p>As mentioned above, the total resource capacity for built-in resources such as the number of shoots is specified as part of the <code>gardenlet</code> configuration, not in the Seed spec. The <code>gardenlet</code> configuration itself could be specified in the spec of the newly introduced <a href=#managedseeds>ManagedSeed</a> resource. Here it is assumed that in the future this could become the recommended and most widely used way to manage seeds. If the same <code>gardenlet</code> is responsible for multiple seeds, they would all share the same capacity settings.</p><p>To specify the total resource capacity for built-in resources, as well as the amount of such resources reserved for Gardener, the 2 new fields <code>resources.capacity</code> and <code>resources.reserved</code> are introduced in the <code>GardenletConfiguration</code> resource. The <code>gardenlet</code> seed controller would then initialize the <code>capacity</code> and <code>allocatable</code> fields in the seed status as follows:</p><ul><li>The <code>capacity</code> value is set to the configured <code>resources.capacity</code>.</li><li>The <code>allocatable</code> value is set to the configured <code>resources.capacity</code> minus <code>resources.reserved</code>.</li></ul><p>Example <code>GardenletConfiguration</code> with <code>resources.capacity</code> and <code>resources.reserved</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>resources:
</span></span><span style=display:flex><span>  capacity:
</span></span><span style=display:flex><span>    shoots: 100
</span></span><span style=display:flex><span>    persistent-volumes: 200
</span></span><span style=display:flex><span>  reserved:
</span></span><span style=display:flex><span>    persistent-volumes: 3
</span></span></code></pre></div><h3 id=scheduling-algorithm>Scheduling Algorithm</h3><p>Currently <code>gardener-scheduler</code> uses a simple non-extensible algorithm in order to schedule shoots onto seeds. It goes through the following stages:</p><ul><li>Filter out seeds that don&rsquo;t meet scheduling requirements such as being ready, matching cloud profile and shoot label selectors, matching the shoot provider, and not having taints that are not tolerated by the shoot.</li><li>From the remaining seeds, determine candidates that are considered best based on their region, by using a strategy that can be either &ldquo;same region&rdquo; or &ldquo;minimal distance&rdquo;.</li><li>Among these candidates, choose the one with the least number of shoots.</li></ul><p>This scheduling algorithm should be adapted in order to properly take into account resources capacity and requests. As a first step, during the filtering stage, any seeds that would exceed their capacity for shoots, or their capacity for any resources requested by the shoot, should simply be filtered out and not considered during the next stages.</p><p>Later, the scheduling algorithm could be further enhanced by replacing the step in which the region strategy is applied by a scoring step similar to the one in <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/>Kubernetes Scheduler</a>. In this scoring step, the scheduler would rank the remaining seeds to choose the most suitable shoot placement. It would assign a score to each seed that survived filtering based on a list of scoring rules. These rules might include for example <code>MinimalDistance</code> and <code>SeedResourcesLeastAllocated</code>, among others. Each rule would produce its own score for the seed, and the overall seed score would be calculated as a weighted sum of all such scores. Finally, the scheduler would assign the shoot to the seed with the highest ranking.</p><h2 id=managedseeds>ManagedSeeds</h2><p>When all or most of the existing seeds are near capacity, new seeds should be created in order to accommodate more shoots. Conversely, sometimes there could be too many seeds for the number of shoots, and so some of the seeds could be deleted to save resources. Currently, the process of creating a new seed involves a number of manual steps, such as creating a new shoot that meets certain criteria, and then registering it as a seed in Gardener. This could be automated to some extent by annotating a shoot with the <code>use-as-seed</code> annotation, in order to create a &ldquo;shooted seed&rdquo;. However, adding more than one similar seeds still requires manually creating all needed shoots, annotating them appropriately, and making sure that they are successfully reconciled and registered.</p><p>To create, delete, and update seeds effectively in a declarative way and allow auto-scaling, a &ldquo;creatable seed&rdquo; resource along with a &ldquo;set&rdquo; (and in the future, perhaps also a &ldquo;deployment&rdquo;) of such creatable seeds should be introduced, similar to Kubernetes <code>Pod</code>, <code>ReplicaSet</code>, and <code>Deployment</code> (or to MCM <code>Machine</code>, <code>MachineSet</code>, and <code>MachineDeployment</code>) resources. With such resources (and their respective controllers), creating a new seed based on a template would become as simple as increasing the <code>replicas</code> field in the &ldquo;set&rdquo; resource.</p><p>In <a href=https://github.com/gardener/gardener/issues/2181>Issue #2181</a> it is already proposed that the <code>use-as-seed</code> annotation is replaced by a dedicated <code>ShootedSeed</code> resource. The solution proposed here further elaborates on this idea.</p><h3 id=managedseed-resource>ManagedSeed Resource</h3><p>The <code>ManagedSeed</code> resource is a dedicated custom resource that represents an evolution of the &ldquo;shooted seed&rdquo; and properly replaces the <code>use-as-seed</code> annotation. This resource contains:</p><ul><li>The name of the Shoot that should be registered as a Seed.</li><li>An optional <code>seedTemplate</code> section that contains the Seed spec and parts of the metadata, such as labels and annotations.</li><li>An optional <code>gardenlet</code> section that contains:<ul><li><code>gardenlet</code> deployment parameters, such as the number of replicas, the image, etc.</li><li>The <code>GardenletConfiguration</code> resource that contains controllers configuration, feature gates, and a <code>seedConfig</code> section that contains the <code>Seed</code> spec and parts of its metadata.</li><li>Additional configuration parameters, such as the garden connection bootstrap mechanism (see <a href=/docs/gardener/concepts/gardenlet/#tls-bootstrapping>TLS Bootstrapping</a>), and whether to merge the provided configuration with the configuration of the parent <code>gardenlet</code>.</li></ul></li></ul><p>Either the <code>seedTemplate</code> or the <code>gardenlet</code> section must be specified, but not both:</p><ul><li>If the <code>seedTemplate</code> section is specified, <code>gardenlet</code> is not deployed to the shoot, and a new <code>Seed</code> resource is created based on the template.</li><li>If the <code>gardenlet</code> section is specified, <code>gardenlet</code> is deployed to the shoot, and it registers a new seed upon startup based on the <code>seedConfig</code> section of the <code>GardenletConfiguration</code> resource.</li></ul><p>A ManagedSeed allows fine-tuning the seed and the <code>gardenlet</code> configuration of shooted seeds in order to deviate from the global defaults, e.g. lower the concurrent sync for some of the seed&rsquo;s controllers or enable a feature gate only on certain seeds. Also, it simplifies the deletion protection of such seeds.</p><p>Also, the <code>ManagedSeed</code> resource is a more powerful alternative to the <code>use-as-seed</code> annotation. The implementation of the <code>use-as-seed</code> annotation itself could be refactored to use a <code>ManagedSeed</code> resource extracted from the annotation by a controller.</p><p>Although in this proposal a ManagedSeed is always a &ldquo;shooted seed&rdquo;, that is a Shoot that is registered as a Seed, this idea could be further extended in the future by adding a <code>type</code> field that could be either <code>Shoot</code> (implied in this proposal), or something different. Such an extension would allow to register and manage as Seed a cluster that is not a Shoot, e.g. a GKE cluster.</p><p>Last but not least, ManagedSeeds could be used as the basis for creating and deleting seeds automatically via the <code>ManagedSeedSet</code> resource that is described in <a href=#managedseedsets>ManagedSeedSets</a>.</p><p>Unlike the <code>Seed</code> resource, the <code>ManagedSeed</code> resource is namespaced. If created in the <code>garden</code> namespace, the resulting seed is globally available. If created in a project namespace, the resulting seed can be used as a &ldquo;private seed&rdquo; by shoots in the project, either by being decorated with project-specific taints and labels, or by being of the special <code>PrivateSeed</code> kind that is also namespaced. The concept of private seeds / cloudprofiles is described in <a href=https://github.com/gardener/gardener/issues/2874>Issue #2874</a>. Until this concept is implemented, <code>ManagedSeed</code> resources might need to be restricted to the <code>garden</code> namespace, similarly to how shoots with the <code>use-as-seed</code> annotation currently are.</p><p>Example <code>ManagedSeed</code> resource with a <code>seedTemplate</code> section:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: seedmanagement.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ManagedSeed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: crazy-botany
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  shoot:
</span></span><span style=display:flex><span>    name: crazy-botany <span style=color:green># Shoot that should be registered as a Seed</span>
</span></span><span style=display:flex><span>  seedTemplate: <span style=color:green># Seed template, including spec and parts of the metadata</span>
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        foo: bar
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      provider:
</span></span><span style=display:flex><span>        type: gcp
</span></span><span style=display:flex><span>        region: europe-west1
</span></span><span style=display:flex><span>      taints:
</span></span><span style=display:flex><span>      - key: seed.gardener.cloud/protected
</span></span><span style=display:flex><span>      ...
</span></span></code></pre></div><p>Example <code>ManagedSeed</code> resource with a <code>gardenlet</code> section:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: seedmanagement.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ManagedSeed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: crazy-botany
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  shoot:
</span></span><span style=display:flex><span>    name: crazy-botany <span style=color:green># Shoot that should be registered as a Seed</span>
</span></span><span style=display:flex><span>  gardenlet: 
</span></span><span style=display:flex><span>    deployment: <span style=color:green># Gardenlet deployment configuration</span>
</span></span><span style=display:flex><span>      replicaCount: 1
</span></span><span style=display:flex><span>      revisionHistoryLimit: 10
</span></span><span style=display:flex><span>      serviceAccountName: gardenlet
</span></span><span style=display:flex><span>      image:
</span></span><span style=display:flex><span>        repository: eu.gcr.io/gardener-project/gardener/gardenlet
</span></span><span style=display:flex><span>        tag: latest
</span></span><span style=display:flex><span>        pullPolicy: IfNotPresent
</span></span><span style=display:flex><span>      resources:
</span></span><span style=display:flex><span>        ...
</span></span><span style=display:flex><span>      podLabels:
</span></span><span style=display:flex><span>        ...
</span></span><span style=display:flex><span>      podAnnotations: 
</span></span><span style=display:flex><span>        ...
</span></span><span style=display:flex><span>      additionalVolumes:
</span></span><span style=display:flex><span>        ...
</span></span><span style=display:flex><span>      additionalVolumeMounts:
</span></span><span style=display:flex><span>        ...
</span></span><span style=display:flex><span>      env:
</span></span><span style=display:flex><span>        ...
</span></span><span style=display:flex><span>      vpa: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>    config: <span style=color:green># GardenletConfiguration resource</span>
</span></span><span style=display:flex><span>      apiVersion: gardenlet.config.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: GardenletConfiguration
</span></span><span style=display:flex><span>      seedConfig: <span style=color:green># Seed template, including spec and parts of the metadata</span>
</span></span><span style=display:flex><span>        metadata:
</span></span><span style=display:flex><span>          labels:
</span></span><span style=display:flex><span>            foo: bar
</span></span><span style=display:flex><span>        spec:
</span></span><span style=display:flex><span>          provider:
</span></span><span style=display:flex><span>            type: gcp
</span></span><span style=display:flex><span>            region: europe-west1
</span></span><span style=display:flex><span>          taints:
</span></span><span style=display:flex><span>          - key: seed.gardener.cloud/protected
</span></span><span style=display:flex><span>          ...
</span></span><span style=display:flex><span>      controllers:
</span></span><span style=display:flex><span>        shoot:
</span></span><span style=display:flex><span>          concurrentSyncs: 20
</span></span><span style=display:flex><span>      featureGates:
</span></span><span style=display:flex><span>        ...
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>    bootstrap: BootstrapToken
</span></span><span style=display:flex><span>    mergeWithParent: <span style=color:#00f>true</span>
</span></span></code></pre></div><h3 id=managedseed-controller>ManagedSeed Controller</h3><p>ManagedSeeds are reconciled by a new <em>managed seed controller</em> in <code>gardenlet</code>. Its implementation is very similar to the current <a href=https://github.com/gardener/gardener/blob/master/pkg/gardenlet/controller/shoot/seed_registration_control.go>seed registration controller</a>, and in fact could be regarded as a refactoring of the latter, with the difference that it uses the <code>ManagedSeed</code> resource rather than the <code>use-as-seed</code> annotation on a Shoot. The <code>gardenlet</code> only reconciles ManagedSeeds that refer to Shoots scheduled on Seeds the <code>gardenlet</code> is responsible for.</p><p>Once this controller is considered sufficiently stable, the current <code>use-as-seed</code> annotation and the controller mentioned above should be marked as deprecated and eventually removed.</p><p>A <code>ManagedSeed</code> that is in use by shoots cannot be deleted, unless the shoots are either deleted or moved to other seeds first. The managed seed controller ensures that this is the case by only allowing a ManagedSeed to be deleted if its Seed has been already deleted.</p><h3 id=managedseed-admission-plugins>ManagedSeed Admission Plugins</h3><p>In addition to the managed seed controller mentioned above, new <code>gardener-apiserver</code> admission plugins should be introduced to properly validate the creation and update of ManagedSeeds, as well as the deletion of shoots registered as seeds. These plugins should ensure that:</p><ul><li>A <code>Shoot</code> that is being referred to by a <code>ManagedSeed</code> cannot be deleted.</li><li>Certain <code>Seed</code> spec fields, for example the provider type and region, networking CIDRs for pods, services, and nodes, etc., are the same as (or compatible with) the corresponding <code>Shoot</code> spec fields of the shoot that is being registered as seed.</li><li>If such <code>Seed</code> spec fields are omitted or empty, the plugins should supply proper defaults based on the values in the <code>Shoot</code> resource.</li></ul><h3 id=provider-specific-seed-bootstrapping-actions>Provider-specific Seed Bootstrapping Actions</h3><p>Bootstrapping a new seed might require additional provider-specific actions to the ones performed automatically by the managed seed controller. For example, on Azure this might include getting a new subscription, extending quotas, etc. This could eventually be automated by introducing an extension mechanism for the Gardener seed bootstrapping flow, to be handled by a new type of controller in the provider extensions. However, such an extension mechanism is not in the scope of this proposal and might require a separate GEP.</p><p>One idea that could be further explored is the use <em>shoot readiness gates</em>, similar to Kubernetes <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-readiness-gate>pod readiness gates</a>, in order to control whether a Shoot is considered <code>Ready</code> before it could be registered as a Seed. A provider-specific extension could set the special condition that is specified as a readiness gate to <code>True</code> only after it has successfully performed the provider-specific actions needed.</p><h3 id=changes-to-existing-controllers>Changes to Existing Controllers</h3><p>Since the Shoot registration as a Seed is decoupled from the Shoot reconciliation, existing <code>gardenlet</code> controllers would not have to be changed in order to properly support ManagedSeeds. The main change to <code>gardenlet</code> that would be needed is introducing the new <em>managed seed controller</em> mentioned above, and possibly retiring the old one at some point. In addition, the Shoot controller would need to be adapted as it currently performs certain actions differently if the shoot has a &ldquo;shooted seed&rdquo;.</p><p>The introduction of the <code>ManagedSeed</code> resource would also require no changes to existing <code>gardener-controller-manager</code> controllers that operate on Shoots (for example, shoot hibernation and maintenance controllers).</p><h2 id=managedseedsets>ManagedSeedSets</h2><p>Similarly to a <a href=https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/>ReplicaSet</a>, the purpose of a ManagedSeedSet is to maintain a stable set of replica <a href=#managedseeds>ManagedSeeds</a> available at any given time. As such, it is used to guarantee the availability of a specified number of identical ManagedSeeds, on an equal number of identical Shoots.</p><h3 id=managedseedset-resource>ManagedSeedSet Resource</h3><p>The <code>ManagedSeedSet</code> resource has a <code>selector</code> field that specifies how to identify ManagedSeeds it can acquire, a number of <code>replicas</code> indicating how many ManagedSeeds (and their corresponding Shoots) it should be maintaining, and a two templates:</p><ul><li>A ManagedSeed template (<code>template</code>) specifying the data of new ManagedSeeds it should create to meet the number of replicas criteria.</li><li>A Shoot template (<code>shootTemplate</code>) specifying the data of new Shoots it should create to host the ManagedSeeds.</li></ul><p>A ManagedSeedSet then fulfills its purpose by creating and deleting ManagedSeeds (and their corresponding Shoots) as needed to reach the desired number.</p><p>A ManagedSeedSet is linked to its ManagedSeeds and Shoots via the <code>metadata.ownerReferences</code> field, which specifies what resource the current object is owned by. All ManagedSeeds and Shoots acquired by a ManagedSeedSet have their owning ManagedSeedSet&rsquo;s identifying information within their <code>ownerReferences</code> field.</p><p>Example <code>ManagedSeedSet</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: seedmanagement.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ManagedSeedSet
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: crazy-botany
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 3
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      foo: bar
</span></span><span style=display:flex><span>  updateStrategy:
</span></span><span style=display:flex><span>    type: RollingUpdate <span style=color:green># Update strategy, must be `RollingUpdate`</span>
</span></span><span style=display:flex><span>    rollingUpdate:
</span></span><span style=display:flex><span>      partition: 2 <span style=color:green># Only update the last replica (#2), assuming there are no gaps (&#34;rolling out a canary&#34;)</span>
</span></span><span style=display:flex><span>  template: <span style=color:green># ManagedSeed template, including spec and parts of the metadata</span>
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        foo: bar
</span></span><span style=display:flex><span>    spec: 
</span></span><span style=display:flex><span>      <span style=color:green># shoot.name is not specified since it&#39;s filled automatically by the controller</span>
</span></span><span style=display:flex><span>      seedTemplate: <span style=color:green># Either a seed or a gardenlet section must be specified, see above</span>
</span></span><span style=display:flex><span>        metadata:
</span></span><span style=display:flex><span>          labels:
</span></span><span style=display:flex><span>            foo: bar
</span></span><span style=display:flex><span>        provider:
</span></span><span style=display:flex><span>          type: gcp
</span></span><span style=display:flex><span>          region: europe-west1
</span></span><span style=display:flex><span>        taints:
</span></span><span style=display:flex><span>        - key: seed.gardener.cloud/protected
</span></span><span style=display:flex><span>        ...
</span></span><span style=display:flex><span>  shootTemplate: <span style=color:green># Shoot template, including spec and parts of the metadata</span>
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        foo: bar
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      cloudProfileName: gcp
</span></span><span style=display:flex><span>      secretBindingName: shoot-operator-gcp
</span></span><span style=display:flex><span>      region: europe-west1
</span></span><span style=display:flex><span>      provider:
</span></span><span style=display:flex><span>        type: gcp
</span></span><span style=display:flex><span>      ...
</span></span></code></pre></div><h3 id=managedseedset-controller>ManagedSeedSet Controller</h3><p>ManagedSeedSets are reconciled by a new <em>managed seed set controller</em> in <code>gardener-controller-manager</code>. During the reconciliation this controller creates and deletes ManagedSeeds and Shoots in response to changes to the <code>replicas</code> and <code>selector</code> fields.</p><p><strong>Note:</strong> The introduction of the <code>ManagedSeedSet</code> resource would not require any changes to <code>gardenlet</code> or to existing <code>gardener-controller-manager</code> controllers.</p><h3 id=managing-managedseed-updates>Managing ManagedSeed Updates</h3><p>To manage ManagedSeed updates, we considered two possible approaches:</p><ul><li>A ManagedSeedSet, similarly to a ReplicaSet, does not manage updates to its replicas in any way. In the future, we might introduce ManagedSeedDeployments, a higher-level concept that manages ManagedSeedSets and provides declarative updates to ManagedSeeds along with other useful features, similarly to a <a href=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/>Deployment</a>. Such a mechanism would involve creating new ManagedSeedSets, and therefore new seeds, behind the scenes, and moving existing shoots to them.</li><li>A ManagedSeedSet does manage updates to its replicas, similarly to a <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/>StatefulSet</a>. Updates are performed &ldquo;in-place&rdquo;, without creating new seeds and moving existing shoots to them. Such a mechanism could also take advantage of other StatefulSet features, such as ordered rolling updates and phased rollouts.</li></ul><p>There is an important difference between seeds and pods or nodes in that seeds are more &ldquo;heavyweight&rdquo; and therefore updating a set of seeds by introducing new seeds and moving shoots to them tends to be much more complex, time-consuming, and prone to failures compared to updating the seeds &ldquo;in place&rdquo;. Furthermore, updating seeds in this way depends on a mature implementation of <a href=/docs/gardener/proposals/07-shoot-control-plane-migration/>GEP-7: Shoot Control Plane Migration</a>, which is not available right now. Due to these considerations, we favor the second approach over the first one.</p><h4 id=managedseed-identity-and-order>ManagedSeed Identity and Order</h4><p>A StatefulSet manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods. It maintains a <em>stable identity</em> (including network identity) for each of their Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.</p><p>A StatefulSet achieves the above by associating each replica with an <em>ordinal number</em>. With n replicas, these ordinal numbers range from 0 to n-1. When scaling out, newly added replicas always have ordinal numbers larger than those of previously existing replicas. When scaling in, it is the replicas with the largest original numbers that are removed.</p><p>Besides stable identity and persistent storage, these ordinal numbers are also used to implement the following StatefulSet features:</p><ul><li>Ordered, graceful deployment and scaling.</li><li>Ordered, automated rolling updates. Such rolling updates can be <em>partitioned</em> (limited to replicas with ordinal numbers greater than or equal to the &ldquo;partition&rdquo;) to achieve <em>phased rollouts</em>.</li></ul><p>A ManagedSeedSet, unlike a StatefulSet, does not need to maintain a stable identity for its ManagedSeeds. Furthermore, it would not be practical to always remove the replicas with the largest ordinal numbers when scaling in, since the corresponding seeds may have shoots scheduled onto them, while other seeds, with lower ordinals, may have fewer shoots (or none), and therefore be much better candidates for being removed.</p><p>On the other hand, it would be beneficial if a ManagedSeedSet, like a StatefulSet, provides ordered deployment and scaling, ordered rolling updates, and phased rollouts. The main advantage of these features is that a deployment or update failure would affect fewer replicas (ideally just one), containing any potential damage and making the situation easier to handle, thus achieving some of the goals stated in <a href=https://github.com/gardener/gardener/issues/87>Issue #87</a>. They could also help to contain seed rolling updates outside business hours.</p><p>Based on the above considerations, we propose the following mechanism for handling ManagedSeed identity and order:</p><ul><li>A ManagedSeedSet uses <em>ordinal numbers generated by an increasing sequence</em> to identify ManagedSeeds and Shoots it creates and manages. These numbers always start from 0 and are incremented by 1 for each newly added replica.</li><li>Replicas (both ManagedSeeds and Shoots) are named after the ManagedSeedSet with the ordinal number appended. For example, for a ManagedSeedSet named <code>test</code> its replicas are named <code>test-0</code>, <code>test-1</code>, etc.</li><li>Gaps in the sequence created by removing replicas with ordinal numbers in the middle of the range are never filled in. A newly added replica always receives a number that is not only free, but also unique to itself. For example, if there are 2 replicas named <code>test-0</code> and <code>test-1</code> and any one of them is removed, a newly added replica will still be named <code>test-2</code>.</li></ul><p>Although such ordinal numbers can also provide some form of stable identity, in this case it is much more important that they can provide a predictable ordering for deployments and updates, and can also be used to partition rolling updates similarly to StatefulSet ordinal numbers.</p><h4 id=update-strategies>Update Strategies</h4><p>The ManagedSeedSet&rsquo;s <code>.spec.updateStrategy</code> field allows configuring automated rolling updates for the ManagedSeeds and Shoots in a ManagedSeedSet.</p><p><strong>Rolling Updates</strong></p><p>The <code>RollingUpdate</code> update strategy implements automated, rolling update for the ManagedSeeds and Shoots in a ManagedSeedSet. With this strategy, the ManagedSeedSet controller will update each ManagedSeed and Shoot in the ManagedSeedSet. It will proceed from the largest number to the smallest, updating each ManagedSeed and its corresponding Shoot one at a time. It will wait until both the Shoot and the Seed of an updated ManagedSeed are Ready prior to updating its predecessor.</p><p>As a further improvement upon the above, the controller could check not only the ManagedSeeds and their corresponding Shoots for readiness, but also the Shoots scheduled onto these ManagedSeeds. The rollout would then only continue if no more than X percent of these Shoots are not reconciled and Ready. Since checking all these additional conditions might require some complex logic, it should be performed by an independent <em>managed seed care controller</em> that updates the ManagedSeed resource with the readiness of its Seed and all Shoots scheduled onto the Seed.</p><p>Note that unlike a StatefulSet, an <code>OnDelete</code> update strategy is not supported.</p><p><strong>Partitions</strong></p><p>The <code>RollingUpdate</code> update strategy can be partitioned, by specifying a <code>.spec.updateStrategy.rollingUpdate.partition</code>. If a partition is specified, only ManagedSeeds and Shoots with ordinals greater than or equal to the partition will be updated when any of the ManagedSeedSet&rsquo;s templates is updated. All remaining ManagedSeeds and Shoots will not be updated. If a ManagedSeedSet&rsquo;s <code>.spec.updateStrategy.rollingUpdate.partition</code> is greater than the largest ordinal number in use by a replica, updates to its templates will not be propagated to its replicas (but newly added replicas may still use the updated templates depending on the partition value).</p><h4 id=keeping-track-of-revision-history-and-performing-rollbacks>Keeping Track of Revision History and Performing Rollbacks</h4><p>Similarly to a StatefulSet, the ManagedSeedSet controller uses <a href=https://pkg.go.dev/k8s.io/api/apps/v1#ControllerRevision>ControllerRevisions</a> to keep track of the revision history, and <code>controller-revision-hash</code> labels to maintain an association between a ManagedSeed or a Shoot and the concrete template revisions based on which they were created or last updated. These are used for the following purposes:</p><ul><li>During an update, determine which replicas are still not on the latest revision and therefore should be updated.</li><li>Display the revision history of a ManagedSeedSet via <code>kubectl rollout history</code>.</li><li>Roll back all ManagedSeedSet replicas to a specific revision via <code>kubectl rollout undo</code></li></ul><p><strong>Note:</strong> The above <code>kubectl rollout</code> commands will not work with custom resources such as ManagedSeedSets out of the box (the <a href=https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#rollout>documentation</a> says explicitly that valid resource types are only deployments, daemonsets, and statefulsets), but it should be possible to eventually support such commands for ManagedSeedSets via a <a href=https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>kubectl plugin</a>.</p><h3 id=scaling-in-managedseedsets>Scaling-in ManagedSeedSets</h3><p>Deleting ManagedSeeds in response to decreasing the replicas of a ManagedSeedSet deserves special attention for two reasons:</p><ul><li>A seed that is already in use by shoots cannot be deleted, unless the shoots are either deleted or moved to other seeds first.</li><li>When there are more empty seeds than requested for deletion, determining which seeds to delete might not be as straightforward as with pods or nodes.</li></ul><p>The above challenges could be addressed as follows:</p><ul><li>In order to scale in a ManagedSeedSet successfully, there should be at least as many empty ManagedSeeds as the difference between the old and the new replicas. In some cases, the user might need to ensure that this is the case by draining some seeds manually before decreasing the replicas field.</li><li>It should be possible to protect ManagedSeeds from deletion even if they are empty, perhaps via an annotation such as <code>seedmanagement.gardener.cloud/protect-from-deletion</code>. Such seeds are not taken into account when determining whether the scale in operation can succeed.</li><li>The decision which seeds to delete among the ManagedSeeds that are empty and not protected should be based on hints, perhaps again in the form of annotations, that could be added manually by the user, as well as other factors, see <a href=#prioritizing-managedseed-deletion>Prioritizing ManagedSeed Deletion</a>.</li></ul><h4 id=prioritizing-managedseed-deletion>Prioritizing ManagedSeed Deletion</h4><p>To help the controller decide which empty ManagedSeeds are to be deleted first, the user could manually annotate ManagedSeeds with a <em>seed priority annotation</em> such as <code>seedmanagement.gardener.cloud/priority</code>. ManagedSeeds with lower priority are more likely to be deleted first. If not specified, a certain default value is assumed, for example 3.</p><p>Besides this annotation, the controller should take into account also other factors, such as the current seed conditions (<code>NotReady</code> should be preferred for deletion over <code>Ready</code>), as well as its age (older should be preferred for deletion over newer).</p><h2 id=auto-scaling-seeds>Auto-scaling Seeds</h2><p>The most interesting and advanced automated seed management feature is making sure that a Garden cluster has enough seeds registered to schedule new shoots (and, in the future, reschedule shoots from drained seeds) without exceeding the seeds capacity for shoots, but not more than actually needed at any given moment. This would involve introducing an auto-scaling mechanism for seeds in Garden clusters.</p><p>The proposed solution builds upon the ideas introduced earlier. The <a href=#managedseeds><code>ManagedSeedSet</code></a> resource (and in the future, also the <code>ManagedSeedDeployment</code> resource) could have a <code>scale</code> subresource that changes the <code>replicas</code> field. This would allow a new &ldquo;seed autoscaler&rdquo; controller to scale these resources via a special &ldquo;autoscaler&rdquo; resource (for example <code>SeedAutoscaler</code>), similarly to how the Kubernetes <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/>Horizontal Pod Autoscaler</a> controller scales pods, as described in <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/>Horizontal Pod Autoscaler Walkthrough</a>.</p><p>The primary metric used for scaling should be the number of shoots already scheduled onto that seed either as a direct value or as a percentage of the seed&rsquo;s capacity for shoots introduced in <a href=#ensuring-seeds-capacity-for-shoots-is-not-exceeded>Ensuring Seeds Capacity for Shoots Is Not Exceeded</a> (<em>utilization</em>). Later, custom metrics based on other resources, including provider-specific resources, could be considered as well.</p><p><strong>Note:</strong> Even if the controller is called <em>Horizontal Pod Autoscaler</em>, it is capable of scaling any resource with a <code>scale</code> subresource, using any custom metric. Therefore, initially it was proposed to use this controller directly. However, a number of important drawbacks were identified with this approach, and so it is no longer proposed here.</p><h3 id=seedautoscaler-resource>SeedAutoscaler Resource</h3><p>The SeedAutoscaler automatically scales the number of <a href=#managedseeds>ManagedSeeds</a> in a <a href=#managedseedsets>ManagedSeedSet</a> based on observed resource utilization. The resource could be any resource that is tracked via the <code>capacity</code> and <code>allocatable</code> fields in the Seed status, including in particular the number of shoots already scheduled onto the seed.</p><p>The SeedAutoscaler is implemented as a custom resource and a new controller. The resource determines the behavior of the controller. The <code>SeedAutoscaler</code> resource has a <code>scaleTargetRef</code> that specifies the target resource to be scaled, the minimum and maximum number of replicas, as well as a list of metrics. The only supported metric type initially is <code>Resource</code> for resources that are tracked via the <code>capacity</code> and <code>allocatable</code> fields in the Seed status. The resource target can be of type <code>Utilization</code> or <code>AverageValue</code>.</p><p>Example <code>SeedAutoscaler</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: seedmanagement.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: SeedAutoscaler
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: crazy-botany
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  scaleTargetRef:
</span></span><span style=display:flex><span>    apiVersion: seedmanagement.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: ManagedSeedSet
</span></span><span style=display:flex><span>    name: crazy-botany
</span></span><span style=display:flex><span>  minReplicas: 1
</span></span><span style=display:flex><span>  maxReplicas: 10
</span></span><span style=display:flex><span>  metrics:
</span></span><span style=display:flex><span>  - type: Resource <span style=color:green># Only Resource is supported</span>
</span></span><span style=display:flex><span>    resource:
</span></span><span style=display:flex><span>      name: shoots
</span></span><span style=display:flex><span>      target:
</span></span><span style=display:flex><span>        type: Utilization <span style=color:green># Utilization or AverageValue</span>
</span></span><span style=display:flex><span>        averageUtilization: 50
</span></span></code></pre></div><h3 id=seedautoscaler-controller>SeedAutoscaler Controller</h3><p><code>SeedAutoscaler</code> resources are reconciled by a new <em>seed autoscaler controller</em>, either in <code>gardener-controller-manager</code> or out-of-tree, similarly to <a href=https://github.com/gardener/autoscaler>cluster-autoscaler</a>. The controller periodically adjusts the number of replicas in a ManagedSeedSet to match the observed average resource utilization to the target specified by user.</p><p><strong>Note:</strong> The SeedAutoscaler controller should perhaps not be limited to evaluating only metrics, it could also take into account also taints, label selectors, etc. This is not yet reflected in the example <code>SeedAutoscaler</code> resource above. Such details are intentionally not specified in this GEP, they should be further explored in the issues created to track the actual implementation.</p><h4 id=evaluating-metrics-for-autoscaling>Evaluating Metrics for Autoscaling</h4><p>The metrics used by the controller, for example the <code>shoots</code> metric above, could be evaluated in one of the following ways:</p><ul><li>Directly, by looking at the <code>capacity</code> and <code>allocatable</code> fields in the Seed status and comparing to the actual resource consumption calculated by simply counting all shoots that meet a certain criteria (e.g. shoots that are scheduled onto the seed), then taking an average over all seeds in the set.</li><li>By sampling existing metrics exported for example by <a href=https://github.com/gardener/gardener-metrics-exporter><code>gardener-metrics-exporter</code></a>.</li></ul><p>The second approach decouples the seed autoscaler controller from the actual metrics evaluation, and therefore allows plugging in new metrics more easily. It also has the advantage that the exported metrics could also be used for other purposes, e.g. for triggering Prometheus alerts or building Grafana dashboards. It has the disadvantage that the seed autoscaler controller would depend on the metrics exporter to do its job properly.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-37ede2ec02b1dbd34b37081524495a25>11 - 17 Shoot Control Plane Migration Bad Case</h1><h1 id=shoot-control-plane-migration-bad-case-scenario>Shoot Control Plane Migration &ldquo;Bad Case&rdquo; Scenario</h1><p>The <a href=/docs/gardener/proposals/07-shoot-control-plane-migration/#migration-workflow>migration flow</a> described as part of <a href=/docs/gardener/proposals/07-shoot-control-plane-migration/>GEP-7</a> can only be executed if both the Garden cluster and source seed cluster are healthy, and <code>gardenlet</code> in the source seed cluster can connect to the Garden cluster. In this case, <code>gardenlet</code> can directly scale down the shoot&rsquo;s control plane in the source seed, after checking the <code>spec.seedName</code> field.</p><p>However, there might be situations in which <code>gardenlet</code> in the source seed cluster can&rsquo;t connect to the Garden cluster and determine that <code>spec.seedName</code> has changed. Similarly, the connection to the seed <code>kube-apiserver</code> could also be broken. This might be caused by issues with the seed cluster itself. In other situations, the migration flow steps in the source seed might have started but might not be able to finish successfully. In all such cases, it should still be possible to migrate a shoot&rsquo;s control plane to a different seed, even though executing the migration flow steps in the source seed might not be possible. The potential &ldquo;split brain&rdquo; situation caused by having the shoot&rsquo;s control plane components attempting to reconcile the shoot resources in two different seeds must still be avoided, by ensuring that the shoot&rsquo;s control plane in the source seed is deactivated before it is activated in the destination seed.</p><p>The mechanisms and adaptations described below have been tested as part of a PoC prior to describing them here.</p><h2 id=owner-election--copying-snapshots>Owner Election / Copying Snapshots</h2><p>To achieve the goals outlined above, an &ldquo;owner election&rdquo; (or rather, &ldquo;ownership passing&rdquo;) mechanism is introduced to ensure that the source and destination seeds are able to successfully negotiate a single &ldquo;owner&rdquo; during the migration. This mechanism is based on special <em>owner DNS records</em> that uniquely identify the seed that currently hosts the shoot&rsquo;s control plane (&ldquo;owns&rdquo; the shoot).</p><p>For example, for a shoot named <code>i500152-gcp</code> in project <code>dev</code> that uses an internal domain suffix <code>internal.dev.k8s.ondemand.com</code> and is scheduled on a seed with an identity <code>shoot--i500152--gcp2-0841c87f-8db9-4d04-a603-35570da6341f-sap-landscape-dev</code>, the owner DNS record is a TXT record with a domain name <code>owner.i500152-gcp.dev.internal.dev.k8s.ondemand.com</code> and a single value <code>shoot--i500152--gcp2-0841c87f-8db9-4d04-a603-35570da6341f-sap-landscape-dev</code>. The owner DNS record is created and maintained by reconciling an <code>owner</code> DNSRecord resource.</p><p>Unlike other extension resources, the <code>owner</code> DNSRecord resource is not reconciled every time the shoot is reconciled, but only when the resource is created. Therefore, the owner DNS record value (the owner ID) is updated only when the shoot is migrated to a different seed. For more information, see <a href=https://github.com/gardener/gardener/pull/4307>Add handling of owner DNSRecord resources</a>.</p><p>The owner DNS record domain name and owner ID are passed to components that need to perform ownership checks, such as the <code>backup-restore</code> container of the <code>etcd-main</code> StatefulSet, and all extension controllers. These components then check regularly whether the actual owner ID (the value of the record) matches the passed ID. If they don&rsquo;t, the ownership check is considered failed, which causes the special behavior described below.</p><p><strong>Note:</strong> A previous revision of this document proposed using &ldquo;sync objects&rdquo; written to and read from the backup container of the source seed as JSON files by the <code>etcd-backup-restore</code> processes in both seeds. With the introduction of owner DNS records such sync objects are no longer needed.</p><p>For the destination seed to actually become the owner, it needs to acquire the shoot&rsquo;s etcd data by copying the final full snapshot (and potentially also older snapshots) from the backup container of the source seed.</p><p>The mechanism to copy the snapshots and pass the ownership from the source to the destination seed consists of the following steps:</p><ol><li><p>The reconciliation flow (&ldquo;restore&rdquo; phase) is triggered in the destination seed without first executing the migration flow in the source seed (or perhaps it was executed, but it failed, and its state is currently unknown).</p></li><li><p>The <code>owner</code> DNSRecord resource is created in the destination seed. As a result, the actual owner DNS record is updated with the destination seed ID. From this point, ownership checks by the <code>etcd-backup-restore</code> process and <a href=#extension-controller-watchdogs>extension controller watchdogs</a> in the source seed will fail, which will cause the special behavior described below.</p></li><li><p>An additional &ldquo;source&rdquo; backup entry referencing the source seed backup bucket is deployed to the Garden cluster and the destination seed and reconciled by the backup entry controller. As a result, a secret with the appropriate credentials for accessing the source seed backup container named <code>source-etcd-backup</code> is created in the destination seed. The normal backup entry (referencing the destination seed backup container) is also deployed and reconciled, as usual, resulting in the usual <code>etcd-backup</code> secret being created.</p></li><li><p>A special &ldquo;copy&rdquo; version of the <code>etcd-main</code> Etcd resource is deployed to the destination seed. In its <code>backup</code> section, this resource contains a <code>sourceStore</code> in addition to the usual <code>store</code>, which contains the parameters needed to use the source seed backup container, such as its name and the secret created in the previous step.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  backup:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>    store:
</span></span><span style=display:flex><span>      container: 408740b8-6491-415e-98e6-76e92e5956ac
</span></span><span style=display:flex><span>      secretRef:
</span></span><span style=display:flex><span>        name: etcd-backup
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>    sourceStore:
</span></span><span style=display:flex><span>      container: d1435fea-cd5e-4d5b-a198-81f4025454ff
</span></span><span style=display:flex><span>      secretRef:
</span></span><span style=display:flex><span>        name: source-etcd-backup
</span></span><span style=display:flex><span>      ...
</span></span></code></pre></div></li><li><p>The <code>etcd-druid</code> in the destination seed reconciles the above resource by deploying a <code>etcd-copy</code> Job that contains a single <code>backup-restore</code> container. It executes the newly introduced <code>copy</code> command of <code>etcd-backup-restore</code> that copies the snapshots from the source to the destination backup container.</p></li><li><p>Before starting the copy itself, the <code>etcd-backup-restore</code> process in the destination seed checks if a final full snapshot (a full snapshot marked as <code>final=true</code>) exists in the backup container. If such a snapshot is not found, it waits for it to appear in order to proceed. This waiting is up to a certain timeout that should be sufficient for a full snapshot to be taken; after this timeout has elapsed, it proceeds anyway, and the reconciliation flow continues from step 9. As described in <a href=#handling-inability-to-access-the-backup-container>Handling Inability to Access the Backup Container</a> below, this is safe to do.</p></li><li><p>The <code>etcd-backup-restore</code> process in the source seed detects that the owner ID in the owner DNS record is different from the expected owner ID (because it was updated in step 2) and switches to a special &ldquo;final snapshot&rdquo; mode. In this mode the regular snapshotter is stopped, the readiness probe of the main <code>etcd</code> container starts returning 503, and one final full snapshot is taken. This snapshot is marked as <code>final=true</code> in order to ensure that it&rsquo;s only taken once, and in order to enable the <code>etcd-backup-restore</code> process in the destination seed to find it (see step 6).</p><p><strong>Note:</strong> While testing our PoC, we noticed that simply making the readiness probe of the main <code>etcd</code> container fail doesn&rsquo;t terminate the existing open connections from <code>kube-apiserver</code> to <code>etcd</code>. For this to happen, either the <code>kube-apiserver</code> or the <code>etcd</code> process has to be restarted at least once. Therefore, when the snapshotter is stopped because an ownership change has been detected, the main <code>etcd</code> process is killed (using <code>SIGTERM</code> to allow graceful termination) to ensure that any open connections from <code>kube-apiserver</code> are terminated. For this to work, the 2 containers must <a href=https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/>share the process namespace</a>.</p></li><li><p>Since the <code>kube-apiserver</code> process in the source seed is no longer able to connect to <code>etcd</code>, all shoot control plane controllers (<code>kube-controller-manager</code>, <code>kube-scheduler</code>, <code>machine-controller-manager</code>, etc.) and extension controllers reconciling shoot resources in the source seed that require a connection to the shoot in order to work start failing. All remaining extension controllers are prevented from reconciling shoot resources via the <a href=#extension-controller-watchdogs>watchdogs</a> mechanism. At this point, the source seed has effectively lost its ownership of the shoot, and it is safe for the destination seed to assume the ownership.</p></li><li><p>After the <code>etcd-backup-restore</code> process in the destination seed detects that a final full snapshot exists, it copies all snapshots (or a subset of all snapshots) from the source to the destination backup container. When this is done, the Job finishes successfully which signals to the reconciliation flow that the snapshots have been copied.</p><p><strong>Note:</strong> To save time, only the final full snapshot taken in step 6, or a subset defined by some criteria, could be copied, instead of all snapshots.</p></li><li><p>The special &ldquo;copy&rdquo; version of the <code>etcd-main</code> Etcd resource is deleted from the source seed, and as a result the <code>etcd-copy</code> Job is also deleted by <code>etcd-druid</code>.</p></li><li><p>The additional &ldquo;source&rdquo; backup entry referencing the source seed backup container is deleted from the Garden cluster and the destination seed. As a result, its corresponding <code>source-etcd-backup</code> secret is also deleted from the destination seed.</p></li><li><p>From this point, the reconciliation flow proceeds as already described in <a href=/docs/gardener/proposals/07-shoot-control-plane-migration/>GEP-7</a>. This is safe, since the source seed cluster is no longer able to interfere with the shoot.</p></li></ol><h2 id=handling-inability-to-access-the-backup-container>Handling Inability to Access the Backup Container</h2><p>The mechanism described above assumes that the <code>etcd-backup-restore</code> process in the source seed is able to access its backup container in order to take snapshots. If this is not the case, but an ownership change was detected, the <code>etcd-backup-restore</code> process still sets the readiness probe status of the main <code>etcd</code> container to 503, and kills the main <code>etcd</code> process as described above to ensure that any open connections from <code>kube-apiserver</code> are terminated. This effectively deactivates the source seed control plane to ensure that the ownership of the shoot can be passed to a different seed.</p><p>Because of this, <code>etcd-backup-restore</code> process in the destination seed responsible for copying the snapshots can avoid waiting forever for a final full snapshot to appear. Instead, after a certain timeout has elapsed, it can proceed with the copying. In this situation, whatever latest snapshot is found in the source backup container will be restored in the destination seed. The shoot is still migrated to a healthy seed at the cost of losing the etcd data that accumulated between the point in time when the connection to the source backup container was lost, and the point in time when the source seed cluster was deactivated.</p><p>When the connection to the backup container is restored in the source seed, a final full snapshot will be eventually taken. Depending on the stage of the restoration flow in the destination seed, this snapshot may be copied to the destination seed and restored, or it may simply be ignored since the snapshots have already been copied.</p><h2 id=handling-inability-to-resolve-the-owner-dns-record>Handling Inability to Resolve the Owner DNS Record</h2><p>The situation when the owner DNS record cannot be resolved is treated similarly to a failed ownership check: the <code>etcd-backup-restore</code> process sets the readiness probe status of the main <code>etcd</code> container to 503, and kills the main <code>etcd</code> process as described above to ensure that any open connections from <code>kube-apiserver</code> are terminated, effectively deactivating the source seed control plane. The final full snapshot is not taken in this case to ensure that the control plane can be re-activated if needed.</p><p>When the owner DNS record can be resolved again, the following 2 situations are possible:</p><ul><li>If the source seed is still the owner of the shoot, the <code>etcd-backup-restore</code> process will set the readiness probe status of the main <code>etcd</code> container to 200, so <code>kube-apiserver</code> will be able to connect to <code>etcd</code> and the source seed control plane will be activated again.</li><li>If the source seed is no longer the owner of the shoot, the etcd readiness probe will continue to fail, and the source seed control plane will remain inactive. In addition, the final full snapshot will be taken at this time, for the same reason as described in <a href=#handling-inability-to-access-the-backup-container>Handling Inability to Access the Backup Container</a>.</li></ul><p><strong>Note:</strong> We expect that actual DNS outages are extremely unlikely. A more likely reason for an inability to resolve a DNS record could be network issues with the underlying infrastructure. In such cases, the shoot would usually not be usable / reachable anyway, so deactivating its control plane would not cause a worse outage.</p><h2 id=migration-flow-adaptations>Migration Flow Adaptations</h2><p>Certain changes to the migration flow are needed in order to ensure that it is compatible with the <a href=#owner-election--copying-snapshots>owner election</a> mechanism described above. Instead of taking a full snapshot of the source seed etcd, the flow deletes the owner DNS record by deleting the <code>owner</code> DNSRecord resource. This causes the ownership check by <code>etcd-backup-restore</code> to fail, and the final full snapshot to be eventually taken, so the migration flow waits for a final full snapshot to appear as the last step before deleting the shoot namespace in the source seed. This ensures that the reconciliation flow described above will find a final full snapshot waiting to be copied at step 6.</p><p>Checking for the final full snapshot is performed by calling the already existing <code>etcd-backup-restore</code> endpoint <code>snapshot/latest</code>. This is possible, since the <code>backup-restore</code> container is always running at this point.</p><p>After the final full snapshot has been taken, the readiness probe of the main <code>etcd</code> container starts failing, which means that if the migration flow is retried due to an error it must skip the step that waits for <code>etcd-main</code> to become ready. To determine if this is the case, a check whether the final full snapshot has been taken or not is performed by calling the same <code>etcd-backup-restore</code> endpoint, e.g. <code>snapshot/latest</code>. This is possible if the <code>etcd-main</code> Etcd resource exists with non-zero replicas. Otherwise:</p><ul><li>If the resource doesn&rsquo;t exist, it must have been already deleted, so the final full snapshot n must have been already taken.</li><li>If it exists with zero replicas, the shoot must be hibernated, and the migration flow must have never been executed (since it scales up etcd as one of its first steps), so the final full snapshot must not have been taken yet.</li></ul><h2 id=extension-controller-watchdogs>Extension Controller Watchdogs</h2><p>Some extension controllers will stop reconciling shoot resources after the connection to the shoot&rsquo;s <code>kube-apiserver</code> is lost. Others, most notably the infrastructure controller, will not be affected. Even though new shoot reconciliations won&rsquo;t be performed by <code>gardenlet</code>, such extension controllers might be stuck in a retry loop triggered by a previous reconciliation, which may cause them to reconcile their resources after <code>gardenlet</code> has already stopped reconciling the shoot. In addition, a reconciliation started when the seed still owned the shoot might take some time and therefore might still be running after the ownership has changed. To ensure that the source seed is completely deactivated, an additional safety mechanism is needed.</p><p>This mechanism should handle the following interesting cases:</p><ul><li><code>gardenlet</code> cannot connect to the Garden <code>kube-apiserver</code>. In this case it cannot fetch shoots and therefore does not know if control plane migration has been triggered. Even though <code>gardenlet</code> will not trigger new reconciliations, extension controllers could still attempt to reconcile their resources if they are stuck a retry loop from a previous reconciliation, and already running reconciliations will not be stopped.</li><li><code>gardenlet</code> cannot connect to the seed&rsquo;s <code>kube-apiserver</code>. In this case <code>gardenlet</code> knows if migration has been triggered, but it will not start shoot migration or reconciliation as it will first check the seed conditions and try to update the <code>Cluster</code> resource, both of which will fail. Extension controllers could still be able to connect to the seed&rsquo;s <code>kube-apiserver</code> (if they are not running where <code>gardenlet</code> is running), and similarly to the previous case, they could still attempt to reconcile their resources.</li><li>The seed components (<code>etcd-druid</code>, extension controllers, etc) cannot connect to the seed&rsquo;s <code>kube-apiserver</code>. In this case extension controllers would not be able to reconcile their resources as they cannot fetch them from the seed&rsquo;s <code>kube-apiserver</code>. When the connection to the <code>kube-apiserver</code> comes back, the controllers might be stuck in a retry loop from a previous reconciliation, or the resources could still be annotated with <code>gardener.cloud/operation=reconcile</code>. This could lead to a race condition depending on who manages to <code>update</code> or <code>get</code> the resources first. If <code>gardenlet</code> manages to update the resources before they are read by the extension controllers, they would be properly updated with <code>gardener.cloud/operation=migrate</code>. Otherwise, they would be reconciled as usual.</li></ul><p><strong>Note:</strong> A previous revision of this document proposed using &ldquo;cluster leases&rdquo; as such an additional safety mechanism. With the introduction of owner DNS records cluster leases are no longer needed.</p><p>The safety mechanism is based on <em>extension controller watchdogs</em>. These are simply additional goroutines that are started when a reconciliation is started by an extension controller. These goroutines perform an ownership check on a regular basis using the owner DNS record, similar to the check performed by the <code>etcd-backup-restore</code> process described above. If the check fails, the watchdog cancels the reconciliation context, which immediately aborts the reconciliation.</p><p><strong>Note:</strong> The <code>dns-external</code> extension controller is the only extension controller that neither needs the shoot&rsquo;s <code>kube-apiserver</code>, nor uses the watchdog mechanism described here. Therefore, this controller will continue reconciling <code>DNSEntry</code> resources even after the source seed has lost the ownership of the shoot. With the PoC, we manually delete the <code>DNSOwner</code> resources from the source seed cluster to prevent this from happening. Eventually, the <code>dns-external</code> controller should be adapted to use the owner DNS records to ensure that it disables itself after the seed has lost the ownership of the shoot. Changes in this direction have already been agreed and relevant PRs proposed.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-790545861fe7f3b616f4d087f63febbb>12 - Bastion Management and SSH Key Pair Rotation</h1><h1 id=gep-15-bastion-management-and-ssh-key-pair-rotation>GEP-15: Bastion Management and SSH Key Pair Rotation</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#motivation>Motivation</a><ul><li><a href=#goals>Goals</a></li><li><a href=#non-goals>Non-Goals</a></li></ul></li><li><a href=#proposal>Proposal</a><ul><li><a href=#involved-components>Involved Components</a></li><li><a href=#ssh-flow>SSH Flow</a></li><li><a href=#resource-example>Resource Example</a></li></ul></li><li><a href=#ssh-key-pair-rotation>SSH Key Pair Rotation</a><ul><li><a href=#rotation-proposal>Rotation Proposal</a></li></ul></li></ul><h2 id=motivation>Motivation</h2><p><code>gardenctl</code> (v1) has the functionality to setup <code>ssh</code> sessions to the targeted shoot cluster (nodes). To this end, infrastructure resources like VMs, public IPs, firewall rules, etc. have to be created. <code>gardenctl</code> will clean up the resources after termination of the <code>ssh</code> session (or rather when the operator is done with her work). However, there were issues in the past where these infrastructure resources were not properly cleaned up afterwards, e.g. due to some error (no retries either). Hence, the proposal is to have a dedicated controller (for each infrastructure) that manages the infrastructure resources and their cleanup. The current <code>gardenctl</code> also re-used the <code>ssh</code> node credentials for the bastion host. While that&rsquo;s possible, it would be safer to rather use personal or generated <code>ssh</code> key pairs to access the bastion host.
The static shoot-specific <code>ssh</code> key pair should be rotated regularly, e.g. once in the maintenance time window. This also means that we cannot create the node VMs anymore with infrastructure public keys as these cannot be revoked or rotated (e.g. in AWS) without terminating the VM itself.</p><p>Changes to the <code>Bastion</code> resource should only be allowed for controllers on seeds that are responsible for it. This cannot be restricted when using custom resources.
The proposal, as outlined below, suggests to implement the necessary changes in the gardener core components and to adapt the <a href=https://github.com/gardener/gardener/issues/1723>SeedAuthorizer</a> to consider <code>Bastion</code> resources that the Gardener API Server serves.</p><h3 id=goals>Goals</h3><ul><li>Operators can request and will be granted time-limited <code>ssh</code> access to shoot cluster nodes via bastion hosts.</li><li>To that end, requestors must present their public <code>ssh</code> key and only this will be installed into <code>sshd</code> on the bastion hosts.</li><li>The bastion hosts will be firewalled and ingress traffic will be permitted only from the client IP of the requestor. Except for traffic on port 22 to the cluster worker nodes, no egress from the bastion is allowed.</li><li>The actual node <code>ssh</code> private key (resp. key pair) will be rotated by Gardener and access to the nodes is only possible with this constantly rotated key pair and not with the personal one that is used only for the bastion host.</li><li>Bastion host and access is granted only for the extent of this operator request (of course multiple <code>ssh</code> sessions are possible, in parallel or repeatedly, but after &ldquo;the time is up&rdquo;, access is no longer possible).</li><li>By these means (personal public key and allow-listed client IP) nobody else can use (a.k.a. impersonate) the requestor (not even other operators).</li><li>Necessary infrastructure resources for <code>ssh</code> access (such as VMs, public IPs, firewall rules, etc.) are automatically created and also terminated after usage, but at the latest after the above mentioned time span is up.</li></ul><h3 id=non-goals>Non-Goals</h3><ul><li>Node-specific access</li><li>Auditability on operating system level (not only auditing the <code>ssh</code> login, but everything that is done on a node and other respective resources, e.g. by using dedicated operating system users)</li><li>Reuse of temporarily created necessary infrastructure resources by different users</li></ul><h2 id=proposal>Proposal</h2><h3 id=involved-components>Involved Components</h3><p>The following is a list of involved components, that either need to be newly introduced or extended if already existing</p><ul><li>Gardener API Server (<code>GAPI</code>)<ul><li>New <code>operations.gardener.cloud</code> API Group</li><li>New resource type <code>Bastion</code>, see <a href=#resource-example>resource example</a> below</li><li>New Admission Webhooks for <code>Bastion</code> resource</li><li><code>SeedAuthorizer</code>: The <code>SeedAuthorizer</code> and dependency graph needs to be extended to consider the <code>Bastion</code> resource <a href=https://github.com/gardener/gardener/tree/master/pkg/admissioncontroller/webhooks/auth/seed/graph>https://github.com/gardener/gardener/tree/master/pkg/admissioncontroller/webhooks/auth/seed/graph</a></li><li>Is configured with <code>timeToLive</code>, the time to add to the current time on each heartbeat</li></ul></li><li><code>gardenlet</code><ul><li>Deploys <code>Bastion</code> CRD under the <code>extensions.gardener.cloud</code> API Group to the Seed, see <a href=#resource-example>resource example</a> below</li><li>Similar to <code>BackupBucket</code>s or <code>BackupEntry</code>, the <code>gardenlet</code> watches the <code>Bastion</code> resource in the garden cluster and creates a seed-local <code>Bastion</code> resource, on which the provider specific bastion controller acts upon</li></ul></li><li><code>gardenctlv2</code> (or any other client)<ul><li>Creates <code>Bastion</code> resource in the garden cluster</li><li>Establishes an <code>ssh</code> connection to a shoot node, using a bastion host as proxy</li><li>Heartbeats / keeps alive the <code>Bastion</code> resource during <code>ssh</code> connection</li></ul></li><li>Gardener extension provider <infra><ul><li>Provider specific bastion controller</li><li>Should be added to gardener-extension-provider-<infra> repos, e.g. <a href=https://github.com/gardener/gardener-extension-provider-aws/tree/master/pkg/controller>https://github.com/gardener/gardener-extension-provider-aws/tree/master/pkg/controller</a></li><li>Has the permission to update the <code>Bastion/status</code> subresource on the seed cluster</li><li>Runs on seed (of course)</li></ul></li><li>Gardener Controller Manager (<code>GCM</code>)<ul><li><code>Bastion</code> heartbeat controller<ul><li>Cleans up <code>Bastion</code> resource on missing heartbeat.</li><li>Is configured with a <code>maxLifetime</code> for the <code>Bastion</code> resource</li></ul></li></ul></li><li>Gardener (RBAC)<ul><li>The project <code>admin</code> role should be extended to allow CRUD operations on the <code>Bastion</code> resource. The <code>gardener.cloud:system:project-member-aggregation</code> <code>ClusterRole</code> needs to be updated accordingly (<a href=https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/charts/application/templates/rbac-user.yaml>https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/charts/application/templates/rbac-user.yaml</a>)</li></ul></li></ul><h3 id=ssh-flow>SSH Flow</h3><ol start=0><li>Users should only get the RBAC permission to <code>create</code> / <code>update</code> <code>Bastion</code> resources for a namespace, if they should be allowed to <code>ssh</code> onto the shoot nodes in this namespace. A project member with <code>admin</code> role will have these permissions.</li><li>User/<code>gardenctlv2</code> creates <code>Bastion</code> resource in garden cluster (see <a href=#resource-example>resource example</a> below)<ul><li>First, gardenctl would figure out the own public IP of the user&rsquo;s machine. Either by calling an external service (gardenctl (v1) uses <a href=https://github.com/gardener/gardenctl/blob/master/pkg/cmd/miscellaneous.go#L226>https://github.com/gardener/gardenctl/blob/master/pkg/cmd/miscellaneous.go#L226</a>) or by calling a binary that prints the public IP(s) to stdout. The binary should be configurable. The result is set under <code>spec.ingress[].ipBlock.cidr</code></li><li>Creates new <code>ssh</code> key pair. The newly created key pair is used only once for each bastion host, so it has a 1:1 relationship to it. It is cleaned up after it is not used anymore, e.g. if the <code>Bastion</code> resource was deleted.</li><li>The public <code>ssh</code> key is set under <code>spec.sshPublicKey</code></li><li>The targeted shoot is set under <code>spec.shootRef</code></li></ul></li><li>GAPI Admission Plugin for the <code>Bastion</code> resource in the garden cluster<ul><li>on creation, sets <code>metadata.annotations["gardener.cloud/created-by"]</code> according to the user that created the resource</li><li>when <code>gardener.cloud/operation: keepalive</code> is set it will be removed by GAPI from the annotations and <code>status.lastHeartbeatTimestamp</code> will be set with the current timestamp. The <code>status.expirationTimestamp</code> will be calculated by taking the last heartbeat timestamp and adding <code>x</code> minutes (configurable, default <code>60</code> Minutes).</li><li>validates that only the creator of the bastion (see <code>gardener.cloud/created-by</code> annotation) can update <code>spec.ingress</code></li><li>validates that a Bastion can only be created for a Shoot if that Shoot is already assigned to a Seed</li><li>sets <code>spec.seedName</code> and <code>spec.providerType</code> based on the <code>spec.shootRef</code></li></ul></li><li><code>gardenlet</code><ul><li>Watches <code>Bastion</code> resource for own seed under api group <code>operations.gardener.cloud</code> in the garden cluster</li><li>Creates <code>Bastion</code> custom resource under api group <code>extensions.gardener.cloud/v1alpha1</code> in the seed cluster<ul><li>Populates bastion user data under field under <code>spec.userData</code> similar to <a href=https://github.com/gardener/gardenctl/blob/1e3e5fa1d5603e2161f45046ba7c6b5b4107369e/pkg/cmd/ssh.go#L160-L171>https://github.com/gardener/gardenctl/blob/1e3e5fa1d5603e2161f45046ba7c6b5b4107369e/pkg/cmd/ssh.go#L160-L171</a>. By this means the <code>spec.sshPublicKey</code> from the <code>Bastion</code> resource in the garden cluster will end up in the <code>authorized_keys</code> file on the bastion host.</li></ul></li></ul></li><li>Gardener extension provider <infra>/ Bastion Controller on Seed:<ul><li>With own <code>Bastion</code> Custom Resource Definition in the seed under the api group <code>extensions.gardener.cloud/v1alpha1</code></li><li>Watches <code>Bastion</code> custom resources that are created by the <code>gardenlet</code> in the seed</li><li>Controller reads <code>cloudprovider</code> credentials from seed-shoot namespace</li><li>Deploy infrastructure resources<ul><li>Bastion VM. Uses user data from <code>spec.userData</code></li><li>attaches public IP, creates security group, firewall rules, etc.</li></ul></li><li>Updates status of <code>Bastion</code> resource:<ul><li>With bastion IP under <code>status.ingress.ip</code> or hostname under <code>status.ingress.hostname</code></li><li>Updates the <code>status.lastOperation</code> with the status of the last reconcile operation</li></ul></li></ul></li><li><code>gardenlet</code><ul><li>Syncs back the <code>status.ingress</code> and <code>status.conditions</code> of the <code>Bastion</code> resource in the seed to the garden cluster in case it changed</li></ul></li><li><code>gardenctl</code><ul><li>initiates <code>ssh</code> session once <code>status.conditions['BastionReady']</code> is true of the <code>Bastion</code> resource in the garden cluster<ul><li>locates private <code>ssh</code> key matching <code>spec["sshPublicKey"]</code> which was configured beforehand by the user</li><li>reads bastion IP (<code>status.ingress.ip</code>) or hostname (<code>status.ingress.hostname</code>)</li><li>reads the private key from the <code>ssh</code> key pair for the shoot node</li><li>opens <code>ssh</code> connection to the bastion and from there to the respective shoot node</li></ul></li><li>runs heartbeat in parallel as long as the <code>ssh</code> session is open by annotating the <code>Bastion</code> resource with <code>gardener.cloud/operation: keepalive</code></li></ul></li><li><code>GCM</code>:<ul><li>Once <code>status.expirationTimestamp</code> is reached, the <code>Bastion</code> will be marked for deletion</li></ul></li><li><code>gardenlet</code>:<ul><li>Once the <code>Bastion</code> resource in the garden cluster is marked for deletion, it marks the <code>Bastion</code> resource in the seed for deletion</li></ul></li><li>Gardener extension provider <infra>/ Bastion Controller on Seed:<ul><li>all created resources will be cleaned up</li><li>On succes, removes finalizer on <code>Bastion</code> resource in seed</li></ul></li><li><code>gardenlet</code>:<ul><li>removes finalizer on <code>Bastion</code> resource in garden cluster</li></ul></li></ol><h3 id=resource-example>Resource Example</h3><p><code>Bastion</code> resource in the garden cluster</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: operations.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Bastion
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  generateName: cli-
</span></span><span style=display:flex><span>  name: cli-abcdef
</span></span><span style=display:flex><span>  namespace: garden-myproject
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    gardener.cloud/created-by: foo <span style=color:green># immutable, set by the GAPI Admission Plugin</span>
</span></span><span style=display:flex><span>    <span style=color:green># gardener.cloud/operation: keepalive # this annotation is removed by the GAPI and the status.lastHeartbeatTimestamp and status.expirationTimestamp will be updated accordingly</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  shootRef: <span style=color:green># namespace cannot be set / it&#39;s the same as .metadata.namespace</span>
</span></span><span style=display:flex><span>    name: my-cluster <span style=color:green># immutable</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># the following fields are set by the GAPI</span>
</span></span><span style=display:flex><span>  seedName: aws-eu2
</span></span><span style=display:flex><span>  providerType: aws
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  sshPublicKey: c3NoLXJzYSAuLi4K <span style=color:green># immutable, public `ssh` key of the user</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  ingress: <span style=color:green># can only be updated by the creator of the bastion</span>
</span></span><span style=display:flex><span>  - ipBlock:
</span></span><span style=display:flex><span>      cidr: 1.2.3.4/32 <span style=color:green># public IP of the user. CIDR is a string representing the IP Block. Valid examples are &#34;192.168.1.1/24&#34; or &#34;2001:db9::/64&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  observedGeneration: 1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># the following fields are managed by the controller in the seed and synced by gardenlet</span>
</span></span><span style=display:flex><span>  ingress: <span style=color:green># IP or hostname of the bastion</span>
</span></span><span style=display:flex><span>    ip: 1.2.3.5
</span></span><span style=display:flex><span>    <span style=color:green># hostname: foo.bar</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - type: BastionReady <span style=color:green># when the `status` is true of condition type `BastionReady`, the client can initiate the `ssh` connection</span>
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#39;True&#39;</span>
</span></span><span style=display:flex><span>    lastTransitionTime: <span style=color:#a31515>&#34;2021-03-19T11:59:00Z&#34;</span>
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#34;2021-03-19T11:59:00Z&#34;</span>
</span></span><span style=display:flex><span>    reason: BastionReady
</span></span><span style=display:flex><span>    message: Bastion for the cluster is ready.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># the following fields are only set by the GAPI</span>
</span></span><span style=display:flex><span>  lastHeartbeatTimestamp: <span style=color:#a31515>&#34;2021-03-19T11:58:00Z&#34;</span> <span style=color:green># will be set when setting the annotation gardener.cloud/operation: keepalive</span>
</span></span><span style=display:flex><span>  expirationTimestamp: <span style=color:#a31515>&#34;2021-03-19T12:58:00Z&#34;</span> <span style=color:green># extended on each keepalive</span>
</span></span></code></pre></div><p><code>Bastion</code> custom resource in the seed cluster</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Bastion
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cli-abcdef
</span></span><span style=display:flex><span>  namespace: shoot--myproject--mycluster
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  userData: |- <span style=color:green># this is normally base64-encoded, but decoded for the example. Contains spec.sshPublicKey from Bastion resource in garden cluster</span>
</span></span><span style=display:flex><span>    <span style=color:green>#!/bin/bash</span>
</span></span><span style=display:flex><span>    <span style=color:green># create user</span>
</span></span><span style=display:flex><span>    <span style=color:green># add ssh public key to authorized_keys</span>
</span></span><span style=display:flex><span>    <span style=color:green># ...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  ingress:
</span></span><span style=display:flex><span>  - ipBlock:
</span></span><span style=display:flex><span>      cidr: 1.2.3.4/32
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  type: aws <span style=color:green># from extensionsv1alpha1.DefaultSpec</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  observedGeneration: 1
</span></span><span style=display:flex><span>  ingress:
</span></span><span style=display:flex><span>    ip: 1.2.3.5
</span></span><span style=display:flex><span>    <span style=color:green># hostname: foo.bar</span>
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - type: BastionReady
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#39;True&#39;</span>
</span></span><span style=display:flex><span>    lastTransitionTime: <span style=color:#a31515>&#34;2021-03-19T11:59:00Z&#34;</span>
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#34;2021-03-19T11:59:00Z&#34;</span>
</span></span><span style=display:flex><span>    reason: BastionReady
</span></span><span style=display:flex><span>    message: Bastion for the cluster is ready.
</span></span></code></pre></div><h2 id=ssh-key-pair-rotation>SSH Key Pair Rotation</h2><p>Currently, the <code>ssh</code> key pair for the shoot nodes are created once during shoot cluster creation. These key pairs should be rotated on a regular basis.</p><h3 id=rotation-proposal>Rotation Proposal</h3><ul><li><code>gardeneruser</code> original user data <a href=https://github.com/gardener/gardener/tree/master/pkg/operation/botanist/component/extensions/operatingsystemconfig/original/components/gardeneruser>component</a>:<ul><li>The <code>gardeneruser</code> create script should be changed into a reconcile script, and renamed accordingly. It needs to be adapted so that the <code>authorized_keys</code> file will be updated / overwritten with the current and old <code>ssh</code> public key from the cloud-config user data.</li></ul></li><li>Rotation trigger:<ul><li>Once in the maintenance time window</li><li>On demand, by annotating the shoot with <code>gardener.cloud/operation: rotate-ssh-keypair</code></li></ul></li><li>On rotation trigger:<ul><li><code>gardenlet</code><ul><li>Prerequisite of <code>ssh</code> key pair rotation: all nodes of all the worker pools have successfully applied the desired version of their cloud-config user data</li><li>Creates or updates the secret <code>ssh-keypair.old</code> with the content of <code>ssh-keypair</code> in the seed-shoot namespace. The old private key can be used by clients as fallback, in case the new <code>ssh</code> public key is not yet applied on the node</li><li>Generates new <code>ssh-keypair</code> secret</li><li>The <code>OperatingSystemConfig</code> needs to be re-generated and deployed with the new and old <code>ssh</code> public key</li></ul></li><li>As usual (for more details, see <a href=/docs/gardener/extensions/operatingsystemconfig/>here</a>):<ul><li>Once the <code>cloud-config-&lt;X></code> secret in the <code>kube-system</code> namespace of the shoot cluster is updated, it will be picked up by the <a href=https://github.com/gardener/gardener/blob/master/pkg/operation/botanist/component/extensions/operatingsystemconfig/downloader/templates/scripts/download-cloud-config.tpl.sh><code>downloader</code> script</a> (checks every 30s for updates)</li><li>The <code>downloader</code> runs the <a href=https://github.com/gardener/gardener/blob/master/pkg/operation/botanist/component/extensions/operatingsystemconfig/executor/templates/scripts/execute-cloud-config.tpl.sh>&ldquo;execution&rdquo; script</a> from the <code>cloud-config-&lt;X></code> secret</li><li>The &ldquo;execution&rdquo; script includes also the original user data script, which it writes to <code>PATH_CLOUDCONFIG</code>, compares it against the previous cloud config and runs the script in case it has changed</li><li>Running the <a href=https://github.com/gardener/gardener/tree/master/pkg/operation/botanist/component/extensions/operatingsystemconfig/original>original user data</a> script will also run the <code>gardeneruser</code> component, where the <code>authorized_keys</code> file will be updated</li><li>After the most recent cloud-config user data was applied, the &ldquo;execution&rdquo; script annotates the node with <code>checksum/cloud-config-data: &lt;cloud-config-checksum></code> to indicate the success</li></ul></li></ul></li></ul><h3 id=limitations>Limitations</h3><p>Each operating system has its own default user (e.g. <code>core</code>, <code>admin</code>, <code>ec2-user</code> etc). These users get their SSH keys during VM creation (however there is a different handling on Google Cloud Platform as stated below). These keys currently do not get rotated respectively are not removed from the <code>authorized_keys</code> file. This means that the initial <code>ssh</code> key will still be valid for the default operating system user.</p><p>On Google Cloud Platform, the VMs do not have any static users (i.e. no <code>gardener</code> user) and there is an agent on the nodes that syncs the users with their SSH keypairs from the GCP IAM service.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-fb3867149339478f45ee6b887f9089ff>13 - Dynamic kubeconfig generation for Shoot clusters</h1><h1 id=gep-16-dynamic-kubeconfig-generation-for-shoot-clusters>GEP-16: Dynamic kubeconfig generation for Shoot clusters</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#gep-16-dynamic-kubeconfig-generation-for-shoot-clusters>GEP-16: Dynamic kubeconfig generation for Shoot clusters</a><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#summary>Summary</a></li><li><a href=#motivation>Motivation</a><ul><li><a href=#goals>Goals</a></li><li><a href=#non-goals>Non-Goals</a></li></ul></li><li><a href=#proposal>Proposal</a></li><li><a href=#alternatives>Alternatives</a></li></ul></li></ul><h2 id=summary>Summary</h2><p>This <code>GEP</code> introduces new <code>Shoot</code> subresource called <code>AdminKubeconfigRequest</code> allowing for users to dynamically generate a short-lived <code>kubeconfig</code> that can be used to access the <code>Shoot</code> cluster as <code>cluster-admin</code>.</p><h2 id=motivation>Motivation</h2><p>Today, when access to the created <code>Shoot</code> clusters is needed, a <code>kubeconfig</code> with static token credentials is used. This static token is in the <code>system:masters</code> group, granting it <code>cluster-admin</code> privileges. The <code>kubeconfig</code> is generated when the cluster is reconciled, stored in <code>ShootState</code> and replicated in the <code>Project</code>&rsquo;s namespace in a <code>Secret</code>. End-users can fetch the secret and use the <code>kubeconfig</code> inside it.</p><p>There are several problems with this approach:</p><ul><li>The token in the <code>kubeconfig</code> does not have any expiration, so end-users have to request a <code>kubeconfig</code> credential rotation if they want revoke the token.</li><li>There is no user identity in the token. e.g. if user <code>Joe</code> gets the <code>kubeconfig</code> from the <code>Secret</code>, user in that token would be <code>system:cluster-admin</code> and not <code>Joe</code> when accessing the <code>Shoot</code> cluster with it. This makes auditing events in the cluster almost impossible.</li></ul><h3 id=goals>Goals</h3><ul><li><p>Add a <code>Shoot</code> subresource called <code>adminkubeconfig</code> that would produce a <code>kubeconfig</code> used to access that <code>Shoot</code> cluster.</p></li><li><p>The <code>kubeconfig</code> is not stored in the API Server, but generated for each request.</p></li><li><p>In the <code>AdminKubeconfigRequest</code> send to that subresource, end-users can specify the expiration time of the credential.</p></li><li><p>The identity (user) in the Gardener cluster would be part of the identity (x509 client certificate). E.g if <code>Joe</code> authenticates against the Gardener API server, the generated certificate for <code>Shoot</code> authentication would have the following subject:</p><ul><li>Common Name: <code>Joe</code></li><li>Organisation: <code>system:masters</code></li></ul></li><li><p>The maximum validity of the certificate can be enforced by setting a flag on the <code>gardener-apiserver</code>.</p></li><li><p>Deprecate and remove the old <code>{shoot-name}.kubeconfig</code> secrets in each <code>Project</code> namespace.</p></li></ul><h3 id=non-goals>Non-Goals</h3><ul><li>Generate <code>OpenID Connect</code> kubeconfigs</li></ul><h2 id=proposal>Proposal</h2><p>The <code>gardener-apiserver</code> would serve a new <code>shoots/adminkubeconfig</code> resource. It can only accept <code>CREATE</code> calls and accept <code>AdminKubeconfigRequest</code>. A <code>AdminKubeconfigRequest</code> would have the following structure:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: authentication.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: AdminKubeconfigRequest
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  expirationSeconds: 3600
</span></span></code></pre></div><p>Where <code>expirationSeconds</code> is the validity of the certificate in seconds. In this case it would be <code>1 hour</code>. The maximum validity of a <code>AdminKubeconfigRequest</code> is configured by <code>--shoot-admin-kubeconfig-max-expiration</code> flag in the <code>gardener-apiserver</code>.</p><p>When such request is received, the API server would find the <code>ShootState</code> associated with that cluster and generate a <code>kubeconfig</code>. The x509 client certificate would be signed by the <code>Shoot</code> cluster&rsquo;s CA and the user used in the subject&rsquo;s common name would be from the <code>User.Info</code> used to make the request.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: authentication.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: AdminKubeconfigRequest
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  expirationSeconds: 3600
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  expirationTimestamp: <span style=color:#a31515>&#34;2021-02-22T09:06:51Z&#34;</span>
</span></span><span style=display:flex><span>  kubeConfig: <span style=color:green># this is normally base64-encoded, but decoded for the example</span>
</span></span><span style=display:flex><span>    apiVersion: v1
</span></span><span style=display:flex><span>    clusters:
</span></span><span style=display:flex><span>    - cluster:
</span></span><span style=display:flex><span>        certificate-authority-data: LS0tLS1....
</span></span><span style=display:flex><span>        server: https://api.shoot-cluster
</span></span><span style=display:flex><span>      name: shoot-cluster-a
</span></span><span style=display:flex><span>    contexts:
</span></span><span style=display:flex><span>    - context:
</span></span><span style=display:flex><span>        cluster: shoot-cluster-a
</span></span><span style=display:flex><span>        user: shoot-cluster-a
</span></span><span style=display:flex><span>      name: shoot-cluster-a
</span></span><span style=display:flex><span>    current-context: shoot-cluster-a
</span></span><span style=display:flex><span>    kind: Config
</span></span><span style=display:flex><span>    preferences: {}
</span></span><span style=display:flex><span>    users:
</span></span><span style=display:flex><span>    - name: shoot-cluster-a
</span></span><span style=display:flex><span>      user:
</span></span><span style=display:flex><span>        client-certificate-data: LS0tLS1CRUd...
</span></span><span style=display:flex><span>        client-key-data: LS0tLS1CRUd...
</span></span></code></pre></div><p>New feature gate called <code>AdminKubeconfigRequest</code> enables the above mentioned API in the <code>gardener-apiserver</code>. The old <code>{shoot-name}.kubeconfig</code> is kept, but deprecated and will be removed in the future.</p><p>In order to get the server&rsquo;s address used in the <code>kubeconfig</code>, the Shoot&rsquo;s <code>status</code> should be updated with new entries:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: crazy-botany
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec: {}
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  advertisedAddresses:
</span></span><span style=display:flex><span>  - name: external
</span></span><span style=display:flex><span>    url: https://api.shoot-cluster.external.foo
</span></span><span style=display:flex><span>  - name: internal
</span></span><span style=display:flex><span>    url: https://api.shoot-cluster.internal.foo
</span></span><span style=display:flex><span>  - name: ip
</span></span><span style=display:flex><span>    url: https://1.2.3.4
</span></span></code></pre></div><p>This is needed, because the Gardener API server might not know on which IP address the API server is advertised on (e.g. DNS is disabled).</p><p>If there are multiple entries, each would be added in a separate <code>cluster</code> in the <code>kubeconfig</code> and a <code>context</code> with the same name would be added as well. The current context would be selected as the first entry in the <code>advertisedAddresses</code> list (<code>.status.advertisedAddresses[0]</code>).</p><h2 id=alternatives>Alternatives</h2><ul><li><a href=https://github.com/gardener/oidc-webhook-authenticator>Dynamic OpenID Connect Webhook Authenticator</a> can be used instead. Ideally cluster admins can enable either or both.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-bd5352a75eaa367044f38d5f9e2080ac>14 - GEP Title</h1><h1 id=gep-nnnn-your-short-descriptive-title>GEP-NNNN: Your short, descriptive title</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#summary>Summary</a></li><li><a href=#motivation>Motivation</a><ul><li><a href=#goals>Goals</a></li><li><a href=#non-goals>Non-Goals</a></li></ul></li><li><a href=#proposal>Proposal</a></li><li><a href=#alternatives>Alternatives</a></li></ul><h2 id=summary>Summary</h2><h2 id=motivation>Motivation</h2><h3 id=goals>Goals</h3><h3 id=non-goals>Non-Goals</h3><h2 id=proposal>Proposal</h2><h2 id=alternatives>Alternatives</h2></div><div class=td-content style=page-break-before:always><h1 id=pg-36a5ec41ed11a1426dadb44d43ca1cab>15 - Highly Available Shoot Control Planes</h1><h1 id=gep20---highly-available-shoot-control-planes>GEP20 - Highly Available Shoot Control Planes</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#gep20---highly-available-shoot-control-planes>GEP20 - Highly Available Shoot Control Planes</a><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#summary>Summary</a></li><li><a href=#motivation>Motivation</a></li><li><a href=#goals>Goals</a></li><li><a href=#non-goals>Non-Goals</a></li><li><a href=#high-availablity>High Availablity</a><ul><li><a href=#topologies>Topologies</a></li><li><a href=#recommended-number-of-nodes-and-zones>Recommended number of nodes and zones</a></li><li><a href=#recommended-number-of-replicas>Recommended number of replicas</a></li></ul></li><li><a href=#gardener-shoot-api>Gardener Shoot API</a><ul><li><a href=#proposed-changes>Proposed changes</a></li></ul></li><li><a href=#gardener-scheduler>Gardener Scheduler</a><ul><li><a href=#case-1-ha-shoot-with-no-seed-assigned>Case #1: HA shoot with no seed assigned</a></li><li><a href=#case-2-ha-shoot-with-assigned-seed-and-updated-failure-tolerance>Case #2: HA shoot with assigned seed and updated failure tolerance</a></li></ul></li><li><a href=#setting-up-a-seed-for-ha>Setting up a Seed for HA</a><ul><li><a href=#hosting-a-ha-shoot-control-plane-with-node-failure-tolerance>Hosting a HA shoot control plane with <code>node</code> failure tolerance</a></li><li><a href=#hosting-a-ha-shoot-control-plane-with-zone-failure-tolerance>Hosting a HA shoot control plane with <code>zone</code> failure tolerance</a></li><li><a href=#compute-seed-usage>Compute Seed Usage</a></li></ul></li><li><a href=#scheduling-control-plane-components>Scheduling control plane components</a><ul><li><a href=#zone-pinning>Zone pinning</a></li><li><a href=#single-zone>Single-Zone</a></li><li><a href=#multi-zone-replicas--zones>Multi-Zone (#replicas &lt;= #zones)</a></li><li><a href=#multi-zone-replicas--zones-1>Multi-Zone (#replicas > #zones)</a></li></ul></li><li><a href=#disruptions-and-zero-downtime-maintenance>Disruptions and zero downtime maintenance</a></li><li><a href=#seed-system-components>Seed System Components</a></li><li><a href=#shoot-control-plane-components>Shoot Control Plane Components</a><ul><li><a href=#kube-apiserver>Kube Apiserver</a></li><li><a href=#gardener-resource-manager>Gardener Resource Manager</a></li><li><a href=#etcd>Etcd</a><ul><li><a href=#gardener-etcd-component-changes>Gardener <code>etcd</code> component changes</a></li></ul></li><li><a href=#other-critical-components-having-single-replica>Other critical components having single replica</a></li></ul></li><li><a href=#handling-outages>Handling Outages</a><ul><li><a href=#node-failures>Node failures</a><ul><li><a href=#impact-of-node-failure>Impact of Node failure</a></li></ul></li><li><a href=#what-is-zone-outage>What is Zone outage?</a><ul><li><a href=#impact-of-a-zone-outage>Impact of a Zone Outage</a></li></ul></li><li><a href=#identify-a-zone-outage>Identify a Zone outage</a></li><li><a href=#identify-zone-recovery>Identify zone recovery</a></li><li><a href=#recovery>Recovery</a><ul><li><a href=#current-recovery-mechanisms>Current Recovery Mechanisms</a></li><li><a href=#recovery-from-node-failure>Recovery from Node failure</a></li><li><a href=#recovery-from-zone-failure>Recovery from Zone failure</a></li></ul></li><li><a href=#option-1-leverage-existing-recovery-options---preferred>Option #1: Leverage existing recovery options - <code>Preferred</code></a></li><li><a href=#option-2-redundencies-for-all-critical-control-plane-components>Option #2: Redundencies for all critical control plane components</a></li><li><a href=#option-3-auto-rebalance-pods-in-the-event-of-az-failure>Option #3: Auto-rebalance pods in the event of AZ failure</a></li></ul></li><li><a href=#cost-implications-on-hosting-ha-control-plane>Cost Implications on hosting HA control plane</a><ul><li><a href=#compute--storage>Compute & Storage</a></li><li><a href=#network-latency>Network latency</a></li><li><a href=#cross-zonal-traffic>Cross-Zonal traffic</a><ul><li><a href=#ingressegress-traffic-analysis>Ingress/Egress traffic analysis</a></li><li><a href=#optimizing-cost-topology-aware-hint>Optimizing Cost: Topology Aware Hint</a></li></ul></li></ul></li><li><a href=#references>References</a></li><li><a href=#appendix>Appendix</a><ul><li><a href=#etcd-active-passive-options>ETCD Active-Passive Options</a></li><li><a href=#topology-spread-constraints-evaluation-and-findings>Topology Spread Constraints evaluation and findings</a></li><li><a href=#availability-zone-outage-simulation>Availability Zone Outage simulation</a></li><li><a href=#ingressegress-traffic-analysis-details>Ingress/Egress Traffic Analysis Details</a></li></ul></li></ul></li></ul><h2 id=summary>Summary</h2><p>Gardener today only offers highly available control planes for some of its components (like Kubernetes API Server and Gardener Resource Manager) which are deployed with multiple replicas and allow a distribution across nodes. Many of the other critical control plane components including <code>etcd</code> are only offered with a single replica, making them susceptible to both node failure as well as zone failure causing downtimes.</p><p>This GEP extends the failure domain tolerance for shoot control plane components as well as seed components to survive extensive node or availability zone (AZ) outages.</p><h2 id=motivation>Motivation</h2><p>High availability (HA) of Kubernetes control planes is desired to ensure continued operation, even in the case of partial failures of nodes or availability zones. Tolerance to common failure domains ranges from hardware (e.g. utility power sources and backup power sources, network switches, disk/data, racks, cooling systems etc.) to software.</p><p>Each consumer therefore needs to decide on the degree of failure isolation that is desired for the control plane of their respective shoot clusters.</p><h2 id=goals>Goals</h2><ul><li>Provision shoot clusters with highly available control planes (HA shoots) and a failure tolerance on node or AZ level. Consumers may enable/disable high-availability and choose failure tolerance between multiple nodes within a single zone or multiple nodes spread across multiple zones.</li><li>Migrating non-HA to HA shoots. For failure tolerance on <code>zone</code> level only if shoot is already scheduled to a <code>multi-zonal</code> seed.</li><li>Scheduling HA shoots to adequate seeds.</li></ul><h2 id=non-goals>Non-Goals</h2><ul><li>Setting up a high available Gardener service.</li><li>Upgrading from a single-zone shoot control plane to a multi-zonal shoot control plane.</li><li>Failure domains on region level, i.e. multi-region control-planes.</li><li>Downgrading HA shoots to non-HA shoots.</li><li>In the current scope, three control plane components - <code>Kube Apiserver</code>, <code>etcd</code> and <code>Gardener Resource Manager</code> will be highly available. In the future, other components could be set up in HA mode.</li><li>To achieve HA we consider components to have at least three replicas. Greater failure tolerance is not targeted by this GEP.</li></ul><h2 id=high-availablity>High Availablity</h2><h3 id=topologies>Topologies</h3><p>Many shoot control plane (<code>etcd</code>, <code>kube-apiserver</code>, <code>gardener-resource-manager</code>, &mldr;) and seed system components (<code>gardenlet</code>, <code>istio</code>, <code>etcd-druid</code>, &mldr;) provide means to achieve high availability. Commonly these either run in an <strong>Active-Active</strong> or in an <strong>Active-Passive</strong> mode.
Active-Active means that each component replica serves incoming requests (primarily intended for load balancing) whereas Active-Passive means that only one replica is active while others remain on stand-by.</p><h3 id=recommended-number-of-nodes-and-zones>Recommended number of nodes and zones</h3><p>It is recommended that for high-availability setup an odd number of nodes (node tolerance) or zones (zone tolerance) must be used. This also follows the <a href=https://etcd.io/docs/v3.4/faq/#why-an-odd-number-of-cluster-members>recommendations</a> on the etcd cluster size. The recommendations for number of zones will be largely influenced by quorum-based etcd cluster setup recommendations as other shoot control plane components are either stateless or non-quorum-based stateful components.</p><p><strong>Let&rsquo;s take the following example to explain this recommendation further:</strong></p><ul><li>Seed clusters&rsquo; worker nodes are spread across <strong>two zones</strong></li><li>Gardener would distribute a <strong>three member</strong> etcd cluster - AZ-1: 2 replicas, AZ-2: 1 replica</li><li>If AZ-1 goes down then, quorum is lost and the only remaining etcd member enters into a read-only state.</li><li>If AZ-2 goes down then:<ul><li>If the leader is in AZ-2, then it will force a re-election and the quorum will be restored with 2 etcd members in AZ-1.</li><li>If the leader is not in AZ-2, then etcd cluster will still be operational without any downtime as the quorum is not lost.</li></ul></li></ul><p><strong>Result:</strong></p><ul><li>There seems to be no clear benefit to spreading an etcd cluster across 2 zones as there is an additional cost of cross-zonal traffic that will be incurred due to communication amongst the etcd members and also due to API server communication with an etcd member across zones.</li><li>There is no significant gain in availability as compared to an etcd cluster provisioned within a single zone. Therefore it is a recommendation that for regions having 2 availability zones, etcd cluster should only be spread across nodes in a single AZ.</li></ul><p><strong>Validation</strong></p><p>Enforcing that a highly available <code>ManagedSeed</code> is setup with odd number of zones, additional checks needs to be introduced in <a href=https://github.com/gardener/gardener/blob/master/plugin/pkg/managedseed/validator/admission.go>admission plugin</a>.</p><h3 id=recommended-number-of-replicas>Recommended number of replicas</h3><p>The minimum number of replicas required to achieve HA depends on the <a href=#topologies>topology</a> and the requirement of each component that run in an <code>active-active</code> mode.</p><p><strong>Active-Active</strong></p><ul><li>If application needs a quorum to operate (e.g. <code>etcd</code>), at least <strong>three replicas</strong> are required (<a href=https://etcd.io/docs/v3.3/faq/#why-an-odd-number-of-cluster-members>ref</a>).</li><li>Non-quorum based components are also supposed to run with a minimum count of <strong>three replicas</strong> to survive node/zone outages and support load balancing.</li></ul><p><strong>Active-Passive</strong></p><ul><li>Components running in a <code>active-passive</code> mode are expected to have at least <strong>two</strong> replicas, so that there is always one replica on stand-by.</li></ul><h2 id=gardener-shoot-api>Gardener Shoot API</h2><h3 id=proposed-changes>Proposed changes</h3><p>The following changes to the shoot API are suggested to enable the HA feature for a shoot cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  controlPlane:
</span></span><span style=display:flex><span>    highAvailability:
</span></span><span style=display:flex><span>      failureTolerance:
</span></span><span style=display:flex><span>        type: &lt;node | zone&gt;
</span></span></code></pre></div><p>The consumer can optionally specify <code>highAvailability</code> in the shoot spec. Failure tolerance of <code>node</code> (aka <code>single-zone</code> shoot clusters) signifies that the HA shoot control plane can tolerate a single node failure, whereas <code>zone</code> (aka <code>multi-zone</code> shoots clusters) signifies that the HA shoot control plane can withstand an outage of a single zone.</p><h2 id=gardener-scheduler>Gardener Scheduler</h2><p>A scheduling request could be for a HA shoot with failure tolerance of <code>node</code> or <code>zone</code>. It is therefore required to appropriately select a seed.</p><h3 id=case-1-ha-shoot-with-no-seed-assigned>Case #1: HA shoot with no seed assigned</h3><p><em>Proposed Changes</em></p><p><strong>Fitering candidate seeds</strong></p><p>A new <code>filter step</code> needs to be introduced in the <a href=https://github.com/gardener/gardener/blob/4dbba56e24b5999fd728cb84b09a4d6d54f64479/pkg/scheduler/controller/shoot/reconciler.go#L110>reconciler</a> which selects candidate seeds. It ensures that shoots with <code>zone</code> tolerance are only scheduled to seeds which have worker nodes across multiple availability tones (aka <code>multi-zonal seeds</code>).</p><p><strong>Scoring of candidate seeds</strong></p><p>Today, after <strong>Gardener Scheduler</strong> filtered candidates and applied the configured strategy, it chooses the seed with least scheduled shoots <a href=https://github.com/gardener/gardener/blob/b7e6d8691ccc9421a4cee73286497a1f004d5612/pkg/scheduler/controller/shoot/reconciler.go#L160>ref</a>.</p><p>This GEP intends to enhance this very last step by also taking the requested failure tolerance into consideration: If there are potential single- and multi-zonal candidates remaining, a single-zonal seed is always preferred for a shoot requesting no or <code>node</code> tolerance, independent from the utilization of the seed (also see this <a href=https://github.com/gardener/gardener/pull/6102>draft PR</a>). A multi-zonal seed is only chosen if no single-zonal one is suitable <strong>after</strong> filtering was done and the strategy has been applied.</p><p><em>Motivation:</em> It is expected that operators will prepare their landscapes for HA control-planes by changing worker nodes of existing seeds but also by adding completely new multi-zonal seed clusters. For the latter, multi-zonal seeds should primarily be reserved for multi-zonal shoots.</p><h3 id=case-2-ha-shoot-with-assigned-seed-and-updated-failure-tolerance>Case #2: HA shoot with assigned seed and updated failure tolerance</h3><p>A shoot has a pre-defined non-HA seed. A change has been made to the shoot spec, setting control HA to <code>zone</code>.</p><p><em>Proposed Change</em></p><ul><li>If the shoot is not already scheduled on a multi-zonal seed, then the <a href=https://github.com/gardener/gardener/tree/master/plugin/pkg/shoot/validator>shoot admission plugin</a> must deny the request.</li><li>Either shoot owner creates shoot from scratch or needs to align with Gardener operator who has to move the shoot to a proper seed first via control plane migration (editing the <code>shoots/binding</code> resource).</li><li>An automated control plane migration is deliberately not performed as it involves a considerable downtime and the feature itself is not stable by the time this GEP was written.</li></ul><h2 id=setting-up-a-seed-for-ha>Setting up a Seed for HA</h2><p>As mentioned in <a href=#high-availablity>High-Availablity</a>, certain aspects need to be considered for a seed cluster to host HA shoots. The following sections explain the requirements for a seed cluster to host a single or multi zonal HA shoot cluster.</p><h3 id=hosting-a-ha-shoot-control-plane-with-node-failure-tolerance>Hosting a HA shoot control plane with <code>node</code> failure tolerance</h3><p>To host an HA shoot control plane within a single zone, it should be ensured that each worker pool that potentially runs seed system or shoot control plane components should at least have <strong>three nodes</strong>. This is also the minium size that is required by an HA <code>etcd</code> cluster with a failure tolerance of a single node. Furthermore, the nodes must run in a single zone only (see <a href=#recommended-number-of-nodes-and-zones>Recommended number of nodes and zones</a>).</p><h3 id=hosting-a-ha-shoot-control-plane-with-zone-failure-tolerance>Hosting a HA shoot control plane with <code>zone</code> failure tolerance</h3><p>To host an HA shoot control plane across availability zones, worker pools should have a minimum of <strong>three nodes spread across an odd number of availability zones (min. 3)</strong>.</p><p>An additional label <code>seed.gardener.cloud/multi-zonal: true</code> should be added to the seed indicating that this seed is capable of hosting multi-zonal HA shoot control planes, which in turn will help gardener scheduler to short-list the seeds as candidates.</p><p>In case of a <code>ManagedSeed</code> Gardener can add this label automatically to seed clusters if at least one worker pool fulfills the requirements mentioned above and doesn&rsquo;t enforce <code>Taints</code> on its nodes. Gardener may in addition validate if the <code>ManagedSeed</code> is properly set up for the <code>seed.gardener.cloud/multi-zonal: true</code> label when it is added manually.</p><h3 id=compute-seed-usage>Compute Seed Usage</h3><p>At present seed usage is computed by counting the number of shoot control planes that are hosted in a seed. Every seed has a number of shoots it can host <code>status.allocatable.shoots</code> (configurable via <a href=https://github.com/gardener/gardener/blob/4e2d28f2af7093eb94819652a6f4709b5fbfaf06/pkg/gardenlet/apis/config/v1alpha1/types.go#L449>ResourceConfiguration</a>. Operators need to rethink this value for multi-zonal seed clusters.</p><p>Which parameters could be considered?</p><ul><li>Number of available machines of a type as requested as part of the shoot spec. Sufficient capacity should be available to also allow rolling updates which will also be governed by <code>maxSurge</code> configuration at the worker pool level.</li><li>Node CIDR range must grant enough space to schedule additional replicas that the HA feature requires. (For instance, for etcd the requirement for nodes will be 3 times as compared to the current single node).</li><li>If additional zones are added to an existing non-multi-zonal seed cluster to make it multi-zonal then care should be taken that zone specific CIDRs are appropriately checked and changed if required.</li><li>Number of volumes that will be required to host a multi-node/multi-zone etcd cluster will increase by <code>(n-1)</code> where <code>n</code> is the total number of members in the etcd cluster.</li></ul><p>The above list is not an exhaustive list and is just indicative that the currently set limit of 250 will have to be revisited.</p><h2 id=scheduling-control-plane-components>Scheduling control plane components</h2><h3 id=zone-pinning>Zone pinning</h3><p>HA shoot clusters with failure tolerance of <code>node</code> as well as non-HA shoot clusters can be scheduled on <code>single-zonal</code> and <code>multi-zonal</code> seeds alike. On a <code>multi-zonal</code> seed it&rsquo;s desireable to place components of the same control plane in <strong>one zone only</strong> to reduce cost and latency effects due to cross network traffic.
Thus, it&rsquo;s essential to add <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity>Pod affinity</a> rules to control plane component with multiple replicas:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  affinity:
</span></span><span style=display:flex><span>    podAffinity:
</span></span><span style=display:flex><span>      requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span style=display:flex><span>      - labelSelector:
</span></span><span style=display:flex><span>          matchLabels:
</span></span><span style=display:flex><span>            gardener.cloud/shoot: &lt;technical-id&gt;
</span></span><span style=display:flex><span>            &lt;labels&gt;
</span></span><span style=display:flex><span>        topologyKey: <span style=color:#a31515>&#34;topology.kubernetes.io/zone&#34;</span>
</span></span></code></pre></div><p>A special challenge is to select the entire set of control plane pods belonging to a single control plane. Today, Gardener and extensions don&rsquo;t put a common label to the affected pods. We propose to introduce a new label <code>gardener.cloud/shoot: &lt;technical-id></code> where <code>technical-id</code> is the shoot namespace. A mutating webhoook in the <strong>Gardener Resource Manager</strong> should apply this label and the affinity rules to every pod in the control plane. This label and the pod affinity rule will ensure that all the pods in the control plane are pinned to a specific zone for HA shoot cluster having failure tolerance of <code>node</code>.</p><h3 id=single-zone>Single-Zone</h3><p>There are control plane components (like etcd) which requires one etcd member pod per node. Following anti-affinity rule guarantees that each pod of etcd is scheduled onto a different node.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  affinity:
</span></span><span style=display:flex><span>    podAntiAffinity:
</span></span><span style=display:flex><span>      requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span style=display:flex><span>      - labelSelector:
</span></span><span style=display:flex><span>          matchLabels:
</span></span><span style=display:flex><span>            &lt;labels&gt;
</span></span><span style=display:flex><span>        topologyKey: <span style=color:#a31515>&#34;kubernetes.io/hostname&#34;</span>
</span></span></code></pre></div><p>For other control plane components which do not have a stricter requirements to have one replica per node, a more optimal scheduling strategy should be used. Following topology spread constraint provides better utilization of node resources, allowing cluster autoscaler to downsize node groups if certain nodes are under-utilized.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  topologySpreadConstraints:
</span></span><span style=display:flex><span>  - maxSkew: 1
</span></span><span style=display:flex><span>     topologyKey: kubernetes.io/hostname
</span></span><span style=display:flex><span>     whenUnsatisfiable: DoNotSchedule
</span></span><span style=display:flex><span>     labelSelector:
</span></span><span style=display:flex><span>      matchLabels:
</span></span><span style=display:flex><span>        &lt;labels&gt;
</span></span></code></pre></div><p>Using topology spread constraints (as described above) would still ensure that if there are more than one replica defined for a control plane component then it will be distributed across more than one node ensuring failure tolerance of at least one node.</p><h3 id=multi-zone-replicas--zones>Multi-Zone (#replicas &lt;= #zones)</h3><p>If the replica count is equal to the number of available zones, then we can enforce the zone spread during scheduling.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  affinity:
</span></span><span style=display:flex><span>    podAntiAffinity:
</span></span><span style=display:flex><span>      requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span style=display:flex><span>      - labelSelector:
</span></span><span style=display:flex><span>          matchLabels:
</span></span><span style=display:flex><span>            &lt;labels&gt;
</span></span><span style=display:flex><span>      topologyKey: <span style=color:#a31515>&#34;topology.kubernetes.io/zone&#34;</span>
</span></span></code></pre></div><h3 id=multi-zone-replicas--zones-1>Multi-Zone (#replicas > #zones)</h3><p>Enforcing a zone spread for components with a replica count higher than the total amount of zones is not possible. In this case we plan to rather use the following <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/>Pod Topology Spread Constraints</a> which allows a distribution over zones and nodes. The <code>maxSkew</code> value determines how big a imbalance of the pod distribution can be and thus it allows to schedule replicas with a count beyond the number of availability zones (e.g. <a href=#kube-apiserver>Kube-Apiserver</a>).</p><blockquote><p><strong>NOTE:</strong></p><ul><li>During testing we found a few inconsistencies and some quirks (more <a href=#topology-spread-constraints-evaluation-and-findings>information</a>) which is why we rely on Topology Spread Constraints (TSC) only for this case.</li><li>In addition to circumvent issue <a href=https://github.com/kubernetes/kubernetes/issues/98215>kubernetes/kubernetes#98215</a>, Gardener is supposed to add a <code>gardener.cloud/rolloutVersion</code> label and incrementing the version every time the <code>.spec</code> of the component is changed (see <a href=https://github.com/kubernetes/kubernetes/issues/98215#issuecomment-766146323>workaround</a>).</li></ul><p><code>Update:</code> <a href=https://github.com/kubernetes/kubernetes/issues/98215>kubernetes/kubernetes#98215</a> has been very recently closed. A new <a href=https://github.com/kubernetes/kubernetes/blob/master/pkg/features/kube_features.go#L540-L551>feature-gate</a> has been created which is only available from kubernetes 1.5 onwards.</p></blockquote><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  topologySpreadConstraints:
</span></span><span style=display:flex><span>  - maxSkew: 2
</span></span><span style=display:flex><span>    topologyKey: topology.kubernetes.io/zone
</span></span><span style=display:flex><span>    whenUnsatisfiable: DoNotSchedule
</span></span><span style=display:flex><span>    labelSelector:
</span></span><span style=display:flex><span>      matchLabels:
</span></span><span style=display:flex><span>        &lt;labels&gt;
</span></span><span style=display:flex><span>        gardener.cloud/rolloutVersion: &lt;version&gt;
</span></span><span style=display:flex><span>  - maxSkew: 1
</span></span><span style=display:flex><span>    topologyKey: kubernetes.io/hostname
</span></span><span style=display:flex><span>    whenUnsatisfiable: DoNotSchedule
</span></span><span style=display:flex><span>    labelSelector:
</span></span><span style=display:flex><span>      matchLabels:
</span></span><span style=display:flex><span>        &lt;labels&gt;
</span></span><span style=display:flex><span>        gardener.cloud/rolloutVersion: &lt;version&gt;
</span></span></code></pre></div><h2 id=disruptions-and-zero-downtime-maintenance>Disruptions and zero downtime maintenance</h2><p>A secondary effect of provisioning affected seed system and shoot control plane components in an HA fashion is the support for zero downtime maintenance, i.e. a certain amount of replicas always serves requests while an update is being rolled out.
Therefore, proper <a href=https://kubernetes.io/docs/concepts/workloads/pods/disruptions/>PodDisruptionBudgets</a> are required. With this GEP it&rsquo;s planned to set <code>spec.maxUnavailable: 1</code> for every involved and further mentioned component.</p><h2 id=seed-system-components>Seed System Components</h2><p>The following seed system components already run or are planned[*] to be configured with a minimum of two replicas:</p><ul><li>Etcd-Druid* (active-passive)</li><li>Gardenlet* (active-passive)</li><li>Istio Ingress Gateway* (active-active)</li><li>Istio Control Plane* (active-passive)</li><li>Gardener Resource Manager (controllers: active-passive, webhooks: active-active)</li><li>Gardener Seed Admission Controller (active-active)</li><li>Nginx Ingress Controller (active-active)</li><li>Reversed VPN Auth Server (active-active)</li></ul><p>The reason to run controller in <code>active-passive</code> mode is that in case of an outage a stand-by instance can quickly take over the leadership which reduces the overall downtime of that component in comparison to a single replica instance that would need to be evicted and re-scheduled first (see <a href=#current-recovery-mechanisms>Current-Recovery-Mechanisms</a>).</p><p>In addition, the pods of above mentioned components will be configured with the discussed anti-affinity rules (see <a href=#scheduling-control-plane-components>Scheduling control plane components</a>). The <code>Single-Zone</code> case will be the default while <code>Multi-Zone</code> anti-affinity rules apply to seed system components, if the seed is labelled with <code>seed.gardener.cloud/multi-zonal: true</code> (see <a href=#hosting-a-multi-zonal-ha-shoot-control-plane>Hosting a multi-zonal HA shoot control plane</a>).</p><h2 id=shoot-control-plane-components>Shoot Control Plane Components</h2><p>Similar to the <a href=#seed-system-components>Seed System Components</a> the following shoot control plane components are considered critical so that Gardener ought to avoid any downtime. Thus, <a href=#current-recovery-mechanisms>current recovery mechanisms</a> are considered insufficient if only one replica is involved.</p><h3 id=kube-apiserver>Kube Apiserver</h3><p>The Kube Apiserver&rsquo;s <code>HVPA</code> resource needs to be adjusted in case of an HA shoot request:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  hpa:
</span></span><span style=display:flex><span>    template:
</span></span><span style=display:flex><span>      spec:
</span></span><span style=display:flex><span>        minReplicas: 3
</span></span></code></pre></div><p>The discussed <code>TSC</code> in <a href=#scheduling-control-plane-components>Scheduling control plane components</a> applies as well.</p><h3 id=gardener-resource-manager>Gardener Resource Manager</h3><p>The Gardener Resource Manager is already set up with <code>spec.replicas: 3</code> today. Only the <a href=#scheduling-control-plane-components>Affinity and anti-affinity</a> rules must be configured on top.</p><h3 id=etcd>Etcd</h3><p>In contrast to other components, it&rsquo;s not trivial to run multiple replicas for <code>etcd</code> because different rules and considerations apply to form a quorum-based cluster <a href=https://etcd.io/docs/v3.4/op-guide/clustering/>ref</a>.
Most of the complexity (e.g. cluster bootstrap, scale-up) is already outsourced to <a href=https://github.com/gardener/etcd-druid>Etcd-Druid</a> and efforts have been made to support may use-cases already (see <a href=https://github.com/gardener/etcd-druid/issues/107>gardener/etcd-druid#107</a> and <a href=https://github.com/gardener/etcd-druid/blob/master/docs/proposals/multi-node/README.md>Multi-Node etcd GEP</a>). Please note, that especially for <code>etcd</code> an <code>active-passive</code> alternative was evaluated <a href=#etcd-active-passive-options>here</a>. Due to the complexity and implementation effort it was decided to proceed with the <code>active-active</code> built-in support, but to keep this as a reference in case we&rsquo;ll see blockers in the future.</p><h4 id=gardener-etcd-component-changes>Gardener <code>etcd</code> component changes</h4><p>With most of the complexity being handled by <a href=https://github.com/gardener/etcd-druid>Etcd-Druid</a>, Gardener still needs to implement the following requirements if HA is enabled.:</p><ul><li>Set <code>etcd.spec.replicas: 3</code>.</li><li>Set <code>etcd.spec.etcd.schedulingConstraints</code> to the matching <a href=#scheduling-control-plane-components>anti-affinity rule</a>.</li><li>Deploy <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/>NetworkPolicies</a> that a allow a <code>peer-to-peer</code> communication between the etcd pods.</li><li>Create and pass a peer CA and server/client certificate to <code>etcd.spec.etcd.peerUrlTls</code></li></ul><p>The groundwork for this was already done by <a href=https://github.com/gardener/gardener/pull/5741>gardener/gardener#5741</a>.</p><h3 id=other-critical-components-having-single-replica>Other critical components having single replica</h3><p>Following shoot control plane components are currently setup with a single replica and are planned to run with a minimum of 2 replicas:</p><ul><li>Cluster Autoscaler (if enabled)</li><li>Cloud Controller Manager (CCM)</li><li>Kube Controller Manager (KCM)</li><li>Kube Scheduler</li><li>Machine Controller Manager (MCM)</li><li>CSI driver controller</li></ul><blockquote><p>NOTE: MCM, CCM and CSI driver controller are components deployed by provider extensions. HA specific configuration should be configured there.</p></blockquote><p>Additionally <a href=#scheduling-control-plane-components>Affinity and anti-affinity</a> rules must be configured.</p><h2 id=handling-outages>Handling Outages</h2><h3 id=node-failures>Node failures</h3><p>It is possible that node(s) hosting the control plane component are no longer available/reachable. Some of the reasons could be crashing of a node, kubelet running on the node is unable to renew its lease, network partition etc. The topology of control plane components and recovery mechanisms will determine the duration of the downtime that will result when a node is no longer reachable/available.</p><h4 id=impact-of-node-failure>Impact of Node failure</h4><p><strong>Case #1</strong></p><p>HA Failure Tolerance: <code>node</code></p><p>For control plane components having multiple replicas, each replica will be provisioned on a different node (one per node) as per the scheduling constraints.</p><p>Since there are lesser than the desired replicas for shoot control plane pods, <code>kube-scheduler</code> will attempt to look for another node while respecting pod scheduling constraints. If a node satisfying scheduling constraints is found then it will be chosen to schedule control plane pods. If there are no nodes that satisfy the scheduling constraints, then it must wait for <code>Cluster-Autoscaler</code> to scale the node group and then for the <code>Machine-Controller-Manager</code> to provision a new node in the scaled node group. In the event <code>Machine-Controller-Manager</code> is unable to create a new machine, then the replica that was evicted from the failed node, will be stuck in <code>Pending</code> state.</p><p><em>Impact on etcd</em></p><p>Assuming that default etcd cluster size of 3 members, unavailability of one node is tolerated. If more than 1 node hosting the control plane components goes down or is unavailable, then etcd cluster will lose quorum till new nodes are provisioned.</p><p><strong>Case #2</strong></p><p>HA Failure Tolerance: <code>zone</code></p><p>For control plane components having multiple replicas, each replica will be spread across zones as per the scheduling constraints.</p><p><em>etcd</em></p><p><code>kube-scheduler</code> will attempt to look for another node in the same zone since the pod scheduling constraints will prevent it from scheduling the pod onto another zone. If a node is found where there are no control plane components deployed, then it will choose that node to schedule the control plane pods. If there are no nodes that satisfy the scheduling constraints, then it must wait for <code>Machine-Controller-Manager</code> to provision a new node. Reference to <code>PVC</code> will also prevent an <code>etcd</code> member from getting scheduled in another zone since persistent volumes are not shared across availability zones.</p><p><em>Kube Apiserver and Gardener Resource Manager</em></p><p>These control plane components will use <a href=#scheduling-control-plane-components>these</a> rules which allows the replica deployed on the now deleted machine to be brought up on another node within the same zone. However, if there are no nodes available in that zone, then for <em>Gardener Resource Manager</em> which uses <code>requiredDuringSchedulingIgnoredDuringExecution</code> no replacement replica will be scheduled in another zone. <em>Kube ApiServer</em> on the other hand uses topology spread constrains with <code>maxSkew=2</code> on the <code>topologyKey: topology.kubernetes.io/zone</code> which allows it to schedule a replacement pod in another zone.</p><p><em>Other control plane components having single replica</em></p><p>Currently there are no pod scheduling constraints on such control plane components. <a href=#current-recovery-mechanisms>Current recovery mechanisms</a> as described above will come into play and recover these pods.</p><h3 id=what-is-zone-outage>What is Zone outage?</h3><p>No clear definition of a zone outage emerges. However, we can look at different reasons for a zone outage across providers that have been listed in the past and derive a definition out of it.</p><p>Some of the most common failures for zone outages have been due to:</p><ul><li>Network congestion, failure of network devices etc., resulting in loss of connectivity to the nodes within a zone.</li><li>Infrastructure power down due to cooling systems failure/general temperature threshold breach.</li><li>Loss of power due to extreme weather conditions and failure of primary and backup generators resulting in partial or complete shutdown of infrastructure.</li><li>Operator mistakes leading to cascading DNS issues, over-provisioning of servers resulting in massive increase in system memory.</li><li>Stuck volumes or volumes with severely degraded performance which are unable to service read and write requests which can potentially have cascading effects on other critical services like load balancers, database services etc.</li></ul><p>The above list is not comprehensive but a general pattern emerges. The outages range from:</p><ol><li>Shutdown of machines in a specific availability zone due to infrastructure failure which in turn could be due to many reasons listed above.</li><li>Network connectivity to the machines running in an availability zone is either severely impacted or broken.</li><li>Subset of essential services (e.g. EBS volumes in case of AWS provider) are unhealthy which might also have a cascading effect on other services.</li><li>Elevated API request failure rates when creating or updating infrastructure resources like machines, load balancers, etc.</li></ol><p>One can further derive that either there is a complete breakdown of an entire availability zone (1 and 2 above) or there is a degradation or unavailability of a subset of essential services.</p><p>In the first version of this document we define an AZ outage only when either of (1) or (2) occurs as defined above.</p><h4 id=impact-of-a-zone-outage>Impact of a Zone Outage</h4><p>As part of the <a href=#current-recovery-mechanisms>current recovery mechanisms</a>, if <code>Machine-Controller-Manager</code> is able to delete the machines, then per <code>MachineDeployment</code> it will delete one machine at a time and wait for a new machine to transition from <code>Pending</code> to <code>Running</code> state. In case of a network outage, it will be able to delete a machine and subsequently launch a new machine but the newly launched machine will be stuck in <code>Pending</code> state as the <code>Kubelet</code> running on the machine will not be able to create its lease. There will also not be any corresponding <code>Node</code> object for the newly launched machine. Rest of the machines in this <code>MachineDeployment</code> will be stuck in <code>Unknown</code> state.</p><p><strong>Kube Apiserver, Gardener Resource Manager & seed system components</strong></p><p>These pods are stateless, losing one pod can be tolerated since there will be two other replicas that will continue to run in other two zones which are available (considering that there are 3 zones in a region).</p><p><strong>etcd</strong></p><p>A minimum and default size of HA etcd cluster setup is 3. This allows tolerance of one AZ failure. If more than one AZ fails or is unreachable then etcd cluster will lose its quorum. Pod Anti-Affinity policies that are initially set will not allow automatic rescheduling of etcd pod onto another zone (unless of course affinity rules are dynamically changed). Reference to <code>PVC</code> will also prevent an <code>etcd</code> member from getting scheduled in another zone, since persistent volumes are not shared across availability zones.</p><p><strong>Other Shoot Control Plane Components</strong></p><p>All the other shoot control plane components have:</p><ul><li>Single replicas</li><li>No affinity rules influencing their scheduling</li></ul><p>See the <a href=#current-recovery-mechanisms>current recovery mechanisms</a> described above.</p><h3 id=identify-a-zone-outage>Identify a Zone outage</h3><blockquote><p>NOTE: This section should be read in context of the currently limited definition of a zone outage as described above.</p></blockquote><p>In case the <code>Machine-Controller-Manager</code> is unable to delete <code>Failed</code> machines, then following will be observed:</p><ul><li>All nodes in that zone will be stuck in <code>NotReady</code> or <code>Unknown</code> state and there will be an additional taint <code>key: node.kubernetes.io/unreachable, effect: NoSchedule</code> on the node resources.</li><li>Across all <code>MachineDeployment</code> in the affected zone, one machine will be in <code>Terminating</code> state and other existing machines will be in <code>Unknown</code> state. There might one additional machine in <code>CrashLoopBackOff</code>.</li></ul><p>In case the <code>Machine-Controller-Manager</code> is able to delete the <code>Failed</code> machines then following will be observed:</p><ul><li>For every <code>MachineDeployment</code> in the affected zone, there will be one machine stuck in <code>Pending</code> state and all other machines will be in <code>Unknown</code> state.</li><li>For the machine in <code>Pending</code> state there will not be any corresponding node object.</li><li>For all the machines in <code>Unknown</code> state, corresponding node resource will be in <code>NotReady/Unknown</code> state and there will be an additional taint <code>key: node.kubernetes.io/unreachable, effect: NoSchedule</code> on each of the node.</li></ul><p>If the above state is observed for an extended period of time (beyond a threshold that could be defined), then it can be deduced that there is a zone outage.</p><h3 id=identify-zone-recovery>Identify zone recovery</h3><blockquote><p>NOTE: This section should be read in context of the current limited definition of a zone outage as described above.</p></blockquote><p>The machines which were previously stuck in either <code>Pending</code> or <code>CrashLoopBackOff</code> state are now in <code>Running</code> state and if there are corresponding <code>Node</code> resources created for machines, then the zonal recovery has started.</p><h3 id=recovery>Recovery</h3><h4 id=current-recovery-mechanisms>Current Recovery Mechanisms</h4><p>Gardener and upstream Kubernetes already provide recovery mechanism for node and pod recovery in case of a failure of a node. Those have been tested in the scope of a <a href=#availability-zone-outage-simulation>availability zone outage simulation</a>.</p><p><em>Machine recovery</em></p><p>In the seed control plane, <code>kube-controller-manager</code> will detect that a node has not renewed its lease and after a timeout (configurable via <code>--node-monitor-grace-period</code> flag; <code>2m0s</code> by default for Shoot clusters) it will transition the <code>Node</code> to <code>Unknown</code> state. <code>machine-controller-manager</code> will detect that an existing <code>Node</code> has transitioned to <code>Unknown</code> state and will do the following:</p><ul><li>It will transition the corresponding <code>Machine</code> to <code>Failed</code> state after waiting for a duration (currently 10 mins, configured via the <a href=https://github.com/gardener/gardener-extension-provider-gcp/blob/dff3d2417ff732fcce69ba10bbe5d04e13781539/charts/internal/machine-controller-manager/seed/templates/deployment.yaml#L49>&ndash;machine-health-timeout</a> flag).</li><li>Thereafter a <code>deletion timestamp</code> will be put on this machine indicating that the machine is now going to be terminated, transitioning the machine to <code>Terminating</code> state.</li><li>It attempts to drain the node first and if it is unable to drain the node, then currently after a period of 2 hours (configurable via <a href=https://github.com/gardener/gardener-extension-provider-gcp/blob/dff3d2417ff732fcce69ba10bbe5d04e13781539/charts/internal/machine-controller-manager/seed/templates/deployment.yaml#L48>&ndash;machine-drain-timeout</a>), it will attempt to force-delete the <code>Machine</code> and create a new machine. Draining a node will be skipped if certain conditions are met e.g. node in <code>NotReady</code> state, node condition reported by <code>node-problem-detector</code> as <code>ReadonlyFilesystem</code> etc.</li></ul><p>In case <code>machine-controller-manager</code> is unable to delete a machine, then that machine will be stuck in <code>Terminating</code> state. It will attempt to launch a new machine and if that also fails, then the new machine will transition to <code>CrashLoopBackoff</code> and will be stuck in this state.</p><p><em>Pod recovery</em></p><p>Once <code>kube-controller-manager</code> transitions the node to <code>Unknown</code>/<code>NotReady</code>, it also puts the following taints on the node:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>taints:
</span></span><span style=display:flex><span>- effect: NoSchedule
</span></span><span style=display:flex><span>  key: node.kubernetes.io/unreachable
</span></span><span style=display:flex><span>- effect: NoExecute
</span></span><span style=display:flex><span>  key: node.kubernetes.io/unreachable
</span></span></code></pre></div><p>The taints have the following effect:</p><ul><li>New pods will not be scheduled unless they have a toleration added which is all permissive or matches the effect and/or key.</li></ul><p>For Deployments, once a Pod managed by a Deployment transitions to <code>Terminating</code> state the <code>kube-controller-manager</code> creates a new Pod (replica) right away to fullfil the desired replica count of the Deployment. Hence, in case of a Seed Node/zone outage for the Deployments <code>kube-controller-manager</code> creates new Pods in place of the Pods that are evicted due to the outage. The newly created Pods are scheduled on healthy Nodes and start successfully after a short period of time.
For StatefulSets, once a Pod managed by a StatefulSet transitions to <code>Terminating</code> state the <code>kube-controller-manager</code> waits until the Pod is removed from store and only after that it creates the replacement Pod. In case of a Seed node/zone outage the StatefulSet Pods in the Shoot control plane stay in <code>Terminating</code> state for 5min. After 5min the <code>shoot-care-controller</code> of <code>gardenlet</code> deletes forcefully (garbage collects) the <code>Terminating</code> Pods from the Shoot control plane. Alternatively, <code>machine-controller-manager</code> deletes the unhealthy Nodes after <code>--machine-health-timeout</code> (10min by default) and the <code>Terminating</code> Pods are removed from store shortly after the Node removal. <code>kube-controller-manager</code> creates new StatefulSet Pods. Depending on the outage type (<code>node</code> or <code>zone</code>) the new StatefulSet Pods respectively succeed or fail to recover. For <code>node</code> the Pods will likely recover, whereas for <code>zone</code> it&rsquo;s usual that Pods remain in <code>Pending</code> state as long as the outage lasts, since depending volumes cannot be moved across availability zones.</p><h4 id=recovery-from-node-failure>Recovery from Node failure</h4><p>If there is a single node failure in any availability zone irrespective of whether it is a <code>single-zone</code> or <code>multi-zone</code> setup, then the recovery is automatic (see <a href=#current-recovery-mechanisms>current recovery mechanisms</a>). In the mean time, if there are other available nodes (as per affinity rules) in the same availability zone, then the scheduler will deploy the affected shoot control plane components on these nodes.</p><h4 id=recovery-from-zone-failure>Recovery from Zone failure</h4><p>In the following section, options are presented to recover from an availability zone failure in a multi-zone shoot control plane setup.</p><h3 id=option-1-leverage-existing-recovery-options---preferred>Option #1: Leverage existing recovery options - <code>Preferred</code></h3><p>In this option existing recovery mechanisms as described above are used. There is no change to the current replicas for all shoot control plane components and there is no dynamic re-balancing of quorum based pods considered.</p><p><em>Pros:</em></p><ul><li>Less complex to implement since no dynamic re-balancing of pods is required and there is no need to determine if there is an AZ outage.</li><li>Additional cost to host an HA shoot control plane is kept to the bare minimum.</li><li>Existing recovery mechanisms are leveraged:<ul><li>For the affected Deployment Pods, the <code>kube-controller-manager</code> will create new replicas after the affected replicas are terminating. <code>kube-controller-manager</code> starts terminating the affected replicas after <code>7min</code> by default - <code>--node-monitor-grace-period</code> (<code>2min</code> by default) + <code>--default-not-ready-toleration-seconds</code>/<code>default-unreachable-toleration-seconds</code> (<code>300s</code> by default). The newly created Pods will be scheduled on healthy Nodes and will start successfully.</li></ul></li></ul><p><em>Cons:</em></p><ul><li>Existing recovery mechanisms are leveraged:<ul><li>For the affected StatefulSet Pods, the replacement Pods will fail to be scheduled due to their affinity rules (<code>etcd</code> Pods) or volume requirements (<code>prometheus</code> and <code>loki</code> Pods) to run in the outage zone and will not recover. However, the <code>etcd</code> will be still operational because of its quorum. Downtime of the monitoring and logging components for the time of an ongoing availability zone outage is acceptable for now.</li></ul></li><li>etcd cluster will run with one less member resulting in no tolerance to any further failure. If it takes a long time to recover a zone then etcd cluster is now susceptible to a quorum loss, if any further failure happens.</li><li>Any zero downtime maintenance is disabled during this time.</li><li>If the recovery of the zone takes a long time, then it is possible that difference revision between the leader and the follower (which was in the zone that is not available) becomes large. When the AZ is restored and the etcd pod is deployed again, then there will be an additional load on the etcd leader to synchronize this etcd member.</li></ul><h3 id=option-2-redundencies-for-all-critical-control-plane-components>Option #2: Redundencies for all critical control plane components</h3><p>In this option :</p><ul><li>Kube Apiserver, Gardener Resource Manager and etcd will be setup with a minimum of 3 replicas as it is done today.</li><li>All other critical control plane components are setup with more than one replicas. Based on the criticality of the functionality different replica count (>1) could be decided.</li><li>As in <code>Option #1</code> no additional recovery mechanism other than what currently exists are provided.</li></ul><p><em>Pros:</em></p><ul><li>Toleration to at least a single AZ is now provided for all critical control plane components.</li><li>There is no need for dynamic re-balancing of pods in the event of an AZ failure and there is also no need to determine if there is an AZ outage reducing the complexity.</li></ul><p><em>Cons</em>:</p><ul><li>Provisioning redundancies entails additional hosting cost. With all critical components now set up with more than one replica, the overall requirement for compute resources will increase.</li><li>Increase in the overall resource requirements will result in lesser number of shoot control planes that can be hosted in a seed, thereby requiring more seeds to be provisioned, which also increases the cost of hosting seeds.</li><li>If the recovery of the zone takes a long time, then it is possible that difference revision between the leader and the follower (which was in the zone that is not available) becomes large. When the AZ is restored and the etcd pod is deployed again, then there will be an additional load on the etcd leader to synchronize this etcd member.</li></ul><blockquote><p><strong>NOTE:</strong></p><hr><p>Before increasing the replicas for control plane components that currently have a single replica following needs to be checked:</p><ol><li>Is the control plane component stateless? If it is stateless, then it is easier to increase the replicas.</li><li>If the control plane component is not stateless, then check if leader election is required to ensure that at any time there is only one leader and the rest of the replicas will only be followers. This will require additional changes to be implemented if they are not already there.</li></ol></blockquote><h3 id=option-3-auto-rebalance-pods-in-the-event-of-az-failure>Option #3: Auto-rebalance pods in the event of AZ failure</h3><blockquote><p>NOTE: Prerequisite for this option is to have the ability to detect an outage and recover from it.</p></blockquote><p>Kube Apiserver, Gardener Resource Manager, etcd and <a href=#seed-system-components>seed system compontents</a> will be setup with multiple replicas spread across zones. Rest of the control plane components will continue to have a single replica. However, in this option etcd cluster members will be rebalanced to ensure that the desired replicas are available at all times.</p><p><em>Recovering etcd cluster to its full strength</em></p><p>Affinity rules set on etcd statefulset enforces that there will be at most one etcd member per zone. Two approaches could be taken:</p><p><strong>Variant-#1</strong></p><p>Change the affinity rules and always use <code>preferredDuringSchedulingIgnoredDuringExecution</code> for <code>topologyKey: topology.kubernetes.io/zone</code>. If all zones are available then it will prefer to distribute the etcd members across zones, each zone having just one replica. In case of zonal failure, kube scheduler will be able to re-schedule this pod in another zone while ensuring that it chooses a node within that zone that does not already have an etcd member running.</p><p><em>Pros</em></p><ul><li>Simpler to implement as it does not require any change in the affinity rules upon identification of a zonal failure.</li><li>Etcd cluster runs with full strength as long as there is a single zone where etcd pods can be rescheduled.</li></ul><p><em>Cons</em></p><ul><li>It is possible that even when there is no zonal failure, more than one etcd member can be provisioned in a single zone. The chances of that happening are slim as typically there is a dedicate worker pool for hosting etcd pods.</li></ul><p><strong>Variant-#2</strong></p><p>Use <code>requiredDuringSchedulingIgnoredDuringExecution</code> for <code>topologyKey: topology.kubernetes.io/zone</code> during the initial setup to strictly enforce one etcd member per zone.</p><p>If and when a zonal failure is detected then <code>etcd-druid</code> should do the following:</p><ul><li>Remove the PV and PVC for the etcd member in a zone having an outage</li><li>Change the affinity rules for etcd pods to now use <code>preferredDuringSchedulingIgnoredDuringExecution</code> for <code>topologyKey: topology.kubernetes.io/zone</code> during the downtime duration of a zone.</li><li>Delete the etcd pod in the zone which has an outage</li></ul><p>This will force the kube-scheduler to schedule the new pod in another zone.</p><p>When it is detected that the zone has now recovered then it should re-balance the etcd members. To achieve that the following <code>etcd-druid</code> should do the following:</p><ul><li>Change the affinity rule to again have <code>requiredDuringSchedulingIgnoredDuringExecution</code> for <code>topologyKey: topology.kubernetes.io/zone</code></li><li>Delete an etcd pod from a zone which has 2 pods running. Subsequently also delete the associated PV and PVC.</li></ul><p>The kube-scheduler will now schedule this pod in the just recovered zone.</p><p>Consequence of doing this is that <code>etcd-druid</code>, which today runs with a single replica, now needs to have a HA setup across zones.</p><p><em>Pros</em></p><ul><li>When all the zones are healthy and available, it ensures that there is at most one pod per zone, thereby providing the desired QoS w.r.t failure tolerance. Only in the case of a zone failure will it relax the rule for spreading etcd members to allowing more than one member in a zone to be provisioned. However this would ideally be temporary.</li><li>Etcd cluster runs with full strength as long as there is a single zone where etcd pods can be rescheduled.</li></ul><p><em>Cons</em></p><ul><li>It is complex to implement.</li><li>Requires <code>etcd-druid</code> to be highly available as it now plays a key role in ensuring that the affinity rules are changed and PV/PVC&rsquo;s are deleted.</li></ul><h2 id=cost-implications-on-hosting-ha-control-plane>Cost Implications on hosting HA control plane</h2><h3 id=compute--storage>Compute & Storage</h3><p>Cost differential as compared to current setup will be due to:</p><blockquote><p>Consider a 3 zone cluster (in case of multi-zonal shoot control plane) and 3 node cluster (in case of single-zonal shoot control plane)</p></blockquote><ul><li><strong>Machines</strong>: Depending on the number of zones, a minimum of one additional machine per zone will be provisioned.</li><li><strong>Persistent Volume</strong>: 4 additional PVs need to be provisioned for <code>etcd-main</code> and <code>etcd-events</code> pods.</li><li><strong>Backup-Bucket</strong>: <code>etcd</code> backup-restore container uses provider object store as a backup-bucket to store full and delta snapshots. In a multi-node etcd setup, there will only be a leading backup-bucket sidecar (in etcd leader pod) that will only be taking snapshots and uploading it to the object store. Therefore there is no additional cost incurred as compared to the current non-HA shoot control plane setup.</li></ul><h3 id=network-latency>Network latency</h3><p>Network latency measurements were done focusing on <code>etcd</code>. Three different etcd topologies were considered for comparison - single node etcd (cluster-size = 1), multi-node etcd (within a single zone, cluster-size = 3) and multi-node etcd (across 3 zones, cluster-size = 3).</p><p><strong>Test Details</strong></p><ul><li>etcd benchmark tool is used to generate load and reports generated from the benchmark tool is used.</li><li>A subset of etcd requests (namely <code>PUT</code>, <code>RANGE</code>) are considered for network analysis.</li><li>Following are the parameters that have been considered for each test run across all etcd topologies:<ul><li>Number of connections</li><li>Number of clients connecting to etcd concurrently</li><li>Size and the number of key-value pairs that are and queried</li><li>Consistency which can either be <code>serializable</code> or <code>linearizable</code></li><li>For each <code>PUT</code> or <code>GET</code> (range) request leader and followers are targetted</li></ul></li><li>Zones have other workloads running and therefore measurements will have outliers which are ignored.</li></ul><p><strong>Acronyms used</strong></p><ul><li><code>sn-sz</code> - single node, single zone etcd cluster</li><li><code>mn-sz</code> - multi node, single zone etcd cluster</li><li><code>mn-mz</code> - multi node, multi zone etcd cluster</li></ul><p><strong>Test findings for PUT requests</strong></p><ul><li>When number of clients and connections are kept at 1 then it is observed that <code>sn-sz</code> latency is lesser (range of 20%-50%) as compared to <code>mn-sz</code>. The variance is due to changes in payload size.</li><li>For the following observations it was ensured that the only a leader is targetted for both multi-node etcd cluster topologies and that the leader is in the same zone as that of <code>sn-sz</code> to have a fair comparison.<ul><li>When the number of clients, connections and payload size is increased then it has been observed that <code>sn-sz</code> latency is more (in the range of 8% to 30%) as compared to <code>mn-sz</code> and <code>mn-mz</code>.</li></ul></li><li>When comparing <code>mn-sz</code> and <code>mn-mz</code> following observations were made:<ul><li>When number of clients and connections are kept at 1 then irrespective of the payload size, <code>mn-sz</code> latency is lesser (~3%) as compared to <code>mn-mz</code>. However the difference is in the usually is within 1ms.</li><li><code>mn-sz</code> latency is lesser (range of 5%-20%) as compared to <code>mn-mz</code> when the request is directly serviced by the leader. However the difference is in a range of a micro-seconds to a few milli-seconds.</li><li><code>mn-sz</code> latency is lesser (range of 20-30%) as compared to <code>mn-mz</code> when the request is serviced by a follower. However the difference is usually within the same millisecond.</li><li>When the number of clients and connections are kept at 1 then irrespective of the payload size it is observed that latency of a leader is lesser than any follower. This is on the expected lines. However if the number of clients and connections are increased then leader seems to have a higher latency as compared to a follower which could not be explained.</li></ul></li></ul><p><strong>Test findings for GET (Range) requests</strong></p><p>Using etcd benchmark tool range requests were generated.</p><p><em>Range Request to fetch one key per request</em></p><p>We fixed the number of connections and clients to 1 and varied the payload size. Range requests were directed to leader and follower etcd members and network latencies were measured. Following are the findings:</p><ul><li><code>sn-sz</code> latency is ~40% greater as compared to <code>mn-sz</code> and around 30% greater as compared to <code>mn-mz</code> for smaller payload sizes. However for larger payload sizes (~1MB) the trend reverses and we see that <code>sn-sz</code> latency is around (15-20%) lesser as compared to <code>mn-sz</code> and <code>mn-mz</code>.</li><li><code>mn-sz</code> latency is ~20% lesser than <code>mn-mz</code></li><li>With consistency set to serializable, latency was lesser (in the range of 15-40%) as compared to when consistency was set to linearizable.</li><li>When requesting a single key at time (keeping number of connections and clients to 1):<ul><li><code>sn-sz</code> latency is ~40% greater as compared to <code>mn-sz</code> and around 30% greater as compared to <code>mn-mz</code>.</li><li><code>mn-sz</code> latency is ~20% lesser than <code>mn-mz</code></li><li>With consistency serializable latency was ~40% lesser as compared to when consistency was set to linearizable.</li><li>For both <code>mn-sz</code> and <code>mn-mz</code> leader latency is in general lesser (in the range of 20% to 50%) than that of the follower. However the difference is still in milliseconds range when consistency is set to linearizable and in micro seconds range when it is set to serializable.</li></ul></li></ul><p>When connections and clients were increased keeping the payload size fixed then following were the findings:</p><ul><li><code>sn-sz</code> latency is ~30% greater as compared to <code>mn-sz</code> and <code>mn-mz</code> with consistency set to linearizable. This is consistent with the above finding as well. However when consistency is set to serializable then across all topologies latencies are comparable (within ~1 millisecond).</li><li>With increased connections and clients the latencies of <code>mn-sz</code> and <code>mn-mz</code> are almost similar.</li><li>With consistency set to serializable, latency was ~20% lesser as compared to when consistency was set to linearizable. This is also consistent with the above findings.</li><li>When range requests are served by the follower then <code>mn-sz</code> latency is ~20% lesser than <code>mn-mz</code> when consistency is set to linearizable. However it is quite the opposite when consistency is set to serializable.</li></ul><p><em>Range requests to fetch all keys per request</em></p><p>For these tests - for payload size = 1MB, total number of key-value&rsquo;s retrieved per request are 1000 and for payload-size = 256 bytes, total number of key-value pairs retrived per request are 100000.</p><ul><li><code>sn-sz</code> latency is around 5% lesser than both <code>mn-sz</code> and <code>mn-mz</code>. This is a deviation for smaller payloads (see above), but for larger payloads this finding is consistent.</li><li>There is hardly any difference in latency between <code>mn-sz</code> and <code>mn-mz</code>.</li><li>There seems to be no significant different between serializable and linearizable consistency setting. However, when follower etcd instances serviced the request, there were mixed results and nothing could be concluded.</li></ul><p><strong>Summary</strong></p><ul><li>For range requests consistency of <code>Serializable</code> has a lesser network latency as compared to <code>Linearizable</code> which is on the expected lines as linearlizable requests must go through the raft consensus process.</li><li>For PUT requests:<ul><li><code>sn-sz</code> has a lower network latency when number of clients and connections are less. However it starts to deteriorate once that is increased along with increase in payload size makine multi-node etcd clusters out-perform single-node etcd in terms of network latency.</li><li>In general <code>mn-sz</code> has lesser network latency as compared to <code>mn-mz</code> but it is still within milliseconds and therefore is not of concern.</li><li>Requests that go directly to the leader have lesser overall network latency as compared to when the request goes to the follower. This is also expected as the follower will have to forward all PUT requests to the leader as an additional hop.</li></ul></li><li>For GET requests:<ul><li>For lower payload sizes <code>sn-sz</code> latency is greater as compared to <code>mn-sz</code> and <code>mn-mz</code> but with larger payload sizes this trend reverses.</li><li>With lower number of connections and clients <code>mn-sz</code> has lower latencies as compared to <code>mn-mz</code> however this difference diminishes as number of connections/clients/payload size is increased.</li><li>In general when consistency is set to serializable it has lower overall latency as compared to linearizable. There were some outliers w.r.t etcd followers but currently we do not give too much weightage to it.</li></ul></li></ul><p>In a nutshell we do not see any major concerns w.r.t latencies in a multi-zonal setup as compared to single-zone HA setup or single node etcd.</p><blockquote><p>NOTE: Detailed network latency analysis can be viewed <a href=https://github.com/gardener/etcd-druid/blob/master/docs/etcd-network-latency.md>here</a>.</p></blockquote><h3 id=cross-zonal-traffic>Cross-Zonal traffic</h3><p>Providers typically do not charge ingress and egress traffic which is contained within an availability zone. However, they do charge traffic that is across zones.</p><p>Cross zonal traffic rates for some of the providers are:</p><ul><li>AWS: <a href=https://aws.amazon.com/vpc/pricing/>https://aws.amazon.com/vpc/pricing/</a> and <a href=https://aws.amazon.com/ec2/pricing/on-demand/>https://aws.amazon.com/ec2/pricing/on-demand/</a></li><li>Azure: <a href=https://azure.microsoft.com/en-in/pricing/details/bandwidth/>https://azure.microsoft.com/en-in/pricing/details/bandwidth/</a></li><li>GCP: <a href=https://cloud.google.com/vpc/network-pricing>https://cloud.google.com/vpc/network-pricing</a></li></ul><p>Setting up shoot control plane with failure tolerance <code>zone</code> will therefore have a higher running cost due to ingress/egress cost as compared to a HA shoot with failure tolerance of <code>node</code> or to a non-HA control plane.</p><h4 id=ingressegress-traffic-analysis>Ingress/Egress traffic analysis</h4><p>Majority of the cross zonal traffic is generated via the following communication lines:</p><ul><li>Between Kube Apiserver and etcd members (ingress/egress)</li><li>Amongst etcd members (ingress/egress)</li></ul><p>Since both of these components are spread across zones, their contribution to the cross-zonal network cost is the largest. In this section the focus is only on these components and the cross-zonal traffic that gets generated.</p><p>Details of the network traffic is described in <a href=#Ingress/Egress-Traffic-Analysis-Details>Ingress/Egress traffic analysis</a> section.</p><p><strong>Observation Summary</strong></p><p><em>Terminology</em></p><ul><li><code>Idle state</code>: In an <code>idle</code> state of a shoot control plane, there is no user driven activity which results in a call to the API server. All the controllers have started and initial listing of watched resources has been completed (in other words informer caches are now in sync).</li></ul><p><em>Findings</em></p><ul><li>etcd inherently uses <code>raft</code> consensus protocol to provide consistency and linearizability guarantees. All <code>PUT</code> or <code>DELETE</code> requests are always and only serviced by the <code>leader etcd pod</code>. Kube Apiserver can either connect to a <code>leader</code> or a <code>follower</code> etcd.<ul><li>If Kube Apiserver connects to the leader then for every <code>PUT</code>, the leader will additionally distribute the request payload to all the followers and only if the majority of followers responded with a successful update to their local <code>boltDB</code> database, will the leader commit the message and subsequently respond back to the client. For <code>Delete</code>, a similar flow is executed but instead of passing around the entire k8s resource, only keys that need to be deleted are passed, making this operation significantly lighter from the network bandwidth consumption perspective.</li><li>If the Kube Apiserver connects to a follower then for every <code>PUT</code>, the follower will first forward the <code>PUT</code> request along with the request payload to the leader, who in turn will attempt to get consensus from majority of the followers by again sending the entire request payload to all the followers. Rest of the flow is the same as above. There is an additional network traffic from follower to leader and is equal to the weight of the request payload. For <code>Delete</code> a similar flow is executed where the follower will forward the keys that need to be deleted to the leader as an additional step. Rest of the flow is the same as the <code>PUT</code> request flow. Since the keys are quite small in size, the network bandwidth consumed is very small.</li></ul></li><li><code>GET</code> calls made to the Kube Apiserver with <code>labels + selector</code> get translated to <code>range</code> requests to etcd. etcd&rsquo;s database does not understand labels and selectors and is therefore not optimized for k8s query patterns. This call can either be serviced by the <code>leader</code> or <code>follower</code> etcd member. <code>follower</code> etcd will not forward the call to the leader.</li><li>From within controllers, periodic informer resync which generates reconcile events does not make calls to the Kube Apiserver (under the condition that no change is made to the resources for which a watch is created).</li><li>If a <code>follower</code> etcd is not in sync (w.r.t revisions) with the <code>leader</code> etcd, then it will reject the call. The client (in this case Kube Apiserver) retries. Needs to be checked, if it retries by connecting to another etcd member. This will result in additional cross zonal traffic. This is currently not a concern as members are generally kept in sync and will only go out of sync in case of a crash of a member or addition of a new member (as a learner) or during rolling updates. However, the time it takes to complete the sync is generally quick.</li><li>etcd cluster members which are spread across availability zones generated a total cross zonal traffic of ~84 Kib/s in an ideal multi-zonal shoot control plane. Across several runs we have seen this number go up to ~100Kib/s.</li><li>etcd follower to another etcd follower remains consistent at ~2Kib/s in all the cases that have been tested (see Appendix).</li><li>Kube Apiserver making a PUT call to a etcd follower is more expensive than directly making the call to the etcd leader. A PUT call also carries the entire payload of the k8s resource that is being created. <code>Topology aware hints</code> should be evaluated to potentially reduce the network cost to some extent.</li><li>In case of a large difference (w.r.t revision) between a follower and a leader, significant network traffic is observed between the leader and the follower. This is usually an edge case, but occurrence of these cases should be monitored.</li></ul><h4 id=optimizing-cost-topology-aware-hint>Optimizing Cost: Topology Aware Hint</h4><p>In a multi-zonal shoot control plane setup there will be multiple replicas of Kube Apiserver and etcd spread across different availability zones. Network cost and latency is much lower when the communication is within a zone and increases once zonal boundary is crossed. Network traffic amongst etcd members cannot be optimized as these are strictly spread across different zones. However, what could be optimized is the network traffic between Kube Apiserver and etcd member (leader or follower) deployed within a single zone. Kubernetes provides <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints/>topology aware hints</a> to influence how clients should consume endpoints. Additional metadata is added to <code>EndpointSlice</code> to influence routing of traffic to the endpoints closer to the caller. <code>Kube-Proxy</code> utilizes the hints (added as metadata) to favor routing to topologically closer endpoints.</p><p>Disclaimer: <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints/>Topology Aware Hints</a> won&rsquo;t improve network traffic if the seed has worker nodes in more than three zones and the Kube Apiserver is scaled beyond three replicas at the same time. In this case, Kube Apiserver replicas run in zones which don&rsquo;t have an etcd and thus cross zone traffic is inevitable.</p><p><em>During evaluation of this feature some caveats were discovered:</em></p><p>For each cluster, gardener provides a capability to create one or more <code>Worker Pool/Group</code>. Each worker pool can span across one or more availability zones. For a combination of each worker pool and zone there will be a corresponding <code>MachineDeployment</code> which will also map 1:1 to a <code>Node Group</code> which is understood by cluster-autoscaler.</p><p>Consider the following cluster setup:
<img src=/__resources/multi-zone-node-groups_bd3aab.png></p><p><code>EndpointSliceController</code> does the following:</p><ul><li>Computes the overall allocatable CPU across all zones - call it <code>TotalCPU</code></li><li>Computes the allocatable CPU for all nodes per zone - call it <code>ZoneTotalCPU</code></li><li>For each zone it computes the CPU ratio via <code>ZoneTotalCPU/TotalCPU</code>. If the ratio between any two zones is approaching 2x, then it will remove all topology hints.</li></ul><p>Given that the cluster-autoscaler can scale the individual node groups based on unscheduled pods or lower than threshold usage, it is possible that topological hints are added and removed dynamically. This results in non-determinism w.r.t request routing across zones, resulting in difficult to estimate cross-zonal network cost and network latencies.</p><p><a href=https://github.com/kubernetes/kubernetes/issues/110714>K8S#110714</a> has been raised.</p><h2 id=references>References</h2><ol><li><a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/>https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/</a></li><li><a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/>https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/</a></li></ol><h2 id=appendix>Appendix</h2><h3 id=etcd-active-passive-options>ETCD Active-Passive Options</h3><p>In this topology there will be just one <code>active/primary etcd</code> instance and all other <code>etcd</code> instances will be running as <code>hot-standby</code>. Each <code>etcd</code> instance serves as an independent single node cluster.
There are three options to setup an <code>active-passive</code> etcd.</p><details><summary>Option-1: Independent single node etcd clusters</summary>
<img src=/__resources/activepassive-option1_8e565b.png><p><code>Primary etcd</code> will periodically take a snapshot (full and delta) and will push these snapshots to the backup-bucket. <code>Hot-Standy etcd</code> instances will periodically query the backup-bucket and sync its database accordingly. If a new full snapshot is available which has a higher revision number than what is available in its local etcd database then it will restore from a full snapshot. It will additionally check if there are delta snapshots having a higher revision number. If that is the case then it will apply the delta snapshots directly to its local etcd database.</p><blockquote><p>NOTE: There is no need to run an embedded etcd to apply delta snapshots.</p></blockquote><p>For the sake of illustration only assume that there are two etcd pods <code>etcd-0</code> and <code>etcd-1</code> with corresponding labels which uniquely identify each pod. Assume that <code>etcd-0</code> is the current <code>primary/active</code> etcd instance.</p><p><code>etcd-druid</code> will take an additional responsibility to monitor the health of <code>etcd-0</code> and <code>etcd-1</code>. When it detects that the <code>etcd-0</code> is no longer healthy it will patch the <code>etcd</code> service to point to the <code>etcd-1</code> pod by updating the label/selector so that it becomes the <code>primary</code> etcd. It will then restart <code>etc-0</code> pod and henceforth that will serve as a <code>hot-standby</code>.</p><p><strong>Pros</strong></p><ul><li>There is no leader election, no quorum related issues to be handled. It is simpler to setup and manage.</li><li>Allows you to just have a total of two etcd nodes - one is active and another is passive. This allows high availability across zones in cases where regions only have 2 zones (e.g. CCloud and Azure regions that do not have more than 2 zones).</li><li>For all PUT calls the maximum cost in terms of network bandwidth is one call (cross-zonal) from Kube ApiServer to etcd instance which carries the payload with it. In comparison in a three member etcd cluster, the leader will have to send the PUT request to other members (cross zonal) in the etcd cluster which will be slightly more expensive than just having a single member etcd.</li></ul><p><strong>Cons</strong></p><ul><li>As compared to an <code>active-active</code> etcd cluster there is not much difference in cost of compute resources (CPU, Memory, Storage)</li><li>etcd-druid will have to periodically check the health of both the <code>primary</code> and <code>hot-standby</code> nodes and ensure that these are up and running.</li><li>There will be a potential delay in determining that a <code>primary</code> etcd instance is no longer healthy. Thereby increasing the delay in switching to the <code>hot-standy</code> etcd instance causing longer downtime. It is also possible that at the same time <code>hot-standy</code> also went down or is otherwise unhealthy resulting in a complete downtime. The amount of time it will take to recover from such a situation would be several minutes (time to start etcd pod + time to restore either from full snapshot or apply delta snapshots).</li><li>Synchronization is always via backup-bucket which will be less frequent as compared to an <code>active-active</code> etcd cluster where there is real-time synchronization done for any updates by the leader to majority or all of its followers. If the primary crashes, the time.</li><li>During the switchover from <code>primary</code> to <code>hot-standby</code> if the <code>hot-standy</code> etcd is in process of applying delta snaphots or restoring from a new full snapshot then <code>hot-standby</code> should ensure that the backup-restore container sets the readiness probe to indicate that it is not ready yet causing additional downtime.</li></ul></details><details><summary>Option-2: Perpetual Learner</summary>
<img src=/__resources/activepassive-option2_7533da.png><p>In this option etcd cluster and learner facilities are leveraged. <code>etcd-druid</code> will bootstrap a cluster with one member. Once this member is ready to serve client requests then an additional <code>learner</code> will be added which joins the etcd cluster as a non-voting member. Learner will reject client reads and writes requests and the clients will have to reattempt. Typically switching to another member in a cluster post retries is provided out-of-the-box by etcd <code>clientv3</code>. The only <code>member</code> who is also the leader will serve all client requests.</p><p><strong>Pros</strong></p><ul><li>All the pros in <code>Option-1</code> are also applicable for this option.</li><li>Leaner will be continuously updated by the leader and will remain in-sync with the leader. Using a learner retains historical data and its ordering and is therefore better in that aspect as compared to <code>Option-2</code>.</li></ul><p><strong>Cons</strong></p><ul><li>All the cons in <code>Option-1</code> are also applicable for this option.</li><li><code>etcd-druid</code> will now have to additionally play an active role in managing members of an etcd cluster by add new members as <code>learner</code> and promoting <code>learner</code> to an active member if the leader is no longer available. This will increase the complexity in <code>etcd-druid</code>.</li><li>To prevent clients from re-attempting to reach the <code>learner</code>, <code>etcd-druid</code> will have to ensure that the label on the learner are set differently than the <code>leader</code>. This needs to be done every time there is a switch in the leader/learner.</li><li>Since etcd only allows addition of 1 learner at a time, this means that the HA setup can only have one failover etcd node, limiting its capability to have more than one <code>hot-standby</code>.</li></ul></details><h3 id=topology-spread-constraints-evaluation-and-findings>Topology Spread Constraints evaluation and findings</h3><details><summary>Finding #1</summary><p>Consider the following setup:</p><p><em>Single zone & multiple nodes</em></p><img src=/__resources/singlezone-multinode_e38d39.png style=width:200px;height:250px><p>When the constraints defined above are applied then the following was the findings:</p><ul><li>With 3 replicas of etcd, all three got scheduled (one per node). This was a bit unexpected. As per the documentation if there are multiple constraints then they will be evaluated in conjunction. The first constraint should only allow 1 etcd pod per zone and the remaining 2 should not have been scheduled and should continue to be stuck in <code>pending</code> state. However all 3 etcd pods got scheduled and started successfully.</li></ul></details><details><summary>Finding #2</summary><p>Consider the following setup:</p><p>_<em>Multiple zones & multiple nodes</em></p><img src=/__resources/multizone-multinode_6dc42e.png style=width:450px;height:250px><p>When the constraints defined above are applied then the following was the findings:</p><ul><li>Both constraints are evaluated in <code>conjunction</code> and the scheduling is done as expected.</li><li>TSC behaves correctly till replicas=5. Beyond that TSC fails. This was reported as an issue <a href=https://github.com/kubernetes/kubernetes/issues/109364>kubernetes#109364</a></li></ul></details><details><summary>Finding #3</summary></details><blockquote><p>NOTE: Also see the <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#known-limitations><code>Known Limitations</code></a></p></blockquote><h3 id=availability-zone-outage-simulation>Availability Zone Outage simulation</h3><p>A zone outage was simulated by doing the following (Provider:AWS):</p><ul><li>Network ACL were replaced with empty ACL (which denies all ingress and egress). This was done for all subnets in a zone. Impact of denying all traffic:<ul><li>Kubelet running on the nodes in this zone will not be able to communicate to the Kube Apiserver. This will inturn result in <code>Kube-Controller-Manager</code> changing the status of the corresponding <code>Node</code> objects to <code>Unknown</code>.</li><li>Control plane components will not be able to communicate to the kubelet, thereby unable to drain the node.</li></ul></li><li>To simulate the scenario where <code>Machine-Controller-Manager</code> is unable to create/delete machines, <code>cloudprovider</code> credentials were changed so that any attempt to create/delete machines will be un-authorized.</li></ul><p>Worker groups were configured to use <code>region: eu-west-1</code> and <code>zones: eu-west-1a, eu-west-1b, eu-west-1c</code>. <code>eu-west-1a</code> zone was brought down following the above steps. State before and after the outage simulation is captured below.</p><details><summary>State before the outage simulation</summary><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get po -n &lt;shoot-control-ns&gt; <span style=color:green># list of pods in the shoot control plane</span>
</span></span></code></pre></div><table><thead><tr><th>NAME</th><th>READY</th><th>STATUS</th><th>NODE</th></tr></thead><tbody><tr><td>cert-controller-manager-6cf9787df6-wzq86</td><td>1/1</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>cloud-controller-manager-7748bcf697-n66t7</td><td>1/1</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>csi-driver-controller-6cd9bc7997-m7hr6</td><td>6/6</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>csi-snapshot-controller-5f774d57b4-2bghj</td><td>1/1</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>csi-snapshot-validation-7c99986c85-rr7zk</td><td>1/1</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>etcd-events-0</td><td>2/2</td><td>Running</td><td>ip-10-242-60-155.eu-west-1.compute.internal</td></tr><tr><td>etcd-events-1</td><td>2/2</td><td>Running</td><td>ip-10-242-73-89.eu-west-1.compute.internal</td></tr><tr><td>etcd-events-2</td><td>2/2</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>etcd-main-0</td><td>2/2</td><td>Running</td><td>ip-10-242-73-77.eu-west-1.compute.internal</td></tr><tr><td>etcd-main-1</td><td>2/2</td><td>Running</td><td>ip-10-242-22-85.eu-west-1.compute.internal</td></tr><tr><td>etcd-main-2</td><td>2/2</td><td>Running</td><td>ip-10-242-53-131.eu-west-1.compute.internal</td></tr><tr><td>gardener-resource-manager-7fff9f77f6-jwggx</td><td>1/1</td><td>Running</td><td>ip-10-242-73-89.eu-west-1.compute.internal</td></tr><tr><td>gardener-resource-manager-7fff9f77f6-jwggx</td><td>1/1</td><td>Running</td><td>ip-10-242-60-155.eu-west-1.compute.internal</td></tr><tr><td>gardener-resource-manager-7fff9f77f6-jwggx</td><td>1/1</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>grafana-operators-79b9cd58bb-z6hc2</td><td>1/1</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>grafana-operators-79b9cd58bb-z6hc2</td><td>1/1</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>kube-apiserver-5fcb7f4bff-7p4xc</td><td>1/1</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>kube-apiserver-5fcb7f4bff-845p7</td><td>1/1</td><td>Running</td><td>ip-10-242-73-89.eu-west-1.compute.internal</td></tr><tr><td>kube-apiserver-5fcb7f4bff-mrspt</td><td>1/1</td><td>Running</td><td>ip-10-242-60-155.eu-west-1.compute.internal</td></tr><tr><td>kube-controller-manager-6b94bcbc4-9bz8q</td><td>1/1</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>kube-scheduler-7f855ffbc4-8c9pg</td><td>1/1</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>kube-state-metrics-5446bb6d56-xqqnt</td><td>1/1</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>loki-0</td><td>4/4</td><td>Running</td><td>ip-10-242-60-155.eu-west-1.compute.internal</td></tr><tr><td>machine-controller-manager-967bc89b5-kgdwx</td><td>2/2</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>prometheus-0</td><td>3/3</td><td>Running</td><td>ip-10-242-60-155.eu-west-1.compute.internal</td></tr><tr><td>shoot-dns-service-75768bd764-4957h</td><td>1/1</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>vpa-admission-controller-6994f855c9-5vmh6</td><td>1/1</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>vpa-recommender-5bf4cfccb6-wft4b</td><td>1/1</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>vpa-updater-6f795d7bb8-snq67</td><td>1/1</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr><tr><td>vpn-seed-server-748674b7d8-qmjbm</td><td>2/2</td><td>Running</td><td>ip-10-242-20-17.eu-west-1.compute.internal</td></tr></tbody></table><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get nodes -o=custom-columns=<span style=color:#a31515>&#39;NAME:metadata.name,ZONE:metadata.labels.topology\.kubernetes\.io\/zone&#39;</span> <span style=color:green># list of nodes with name, zone and status (was taken separately)</span>
</span></span></code></pre></div><table><thead><tr><th>NAME</th><th>STATUS</th><th>ZONE</th></tr></thead><tbody><tr><td>ip-10-242-20-17.eu-west-1.compute.internal</td><td>Ready</td><td>eu-west-1a</td></tr><tr><td>ip-10-242-22-85.eu-west-1.compute.internal</td><td>Ready</td><td>eu-west-1a</td></tr><tr><td>ip-10-242-3-0.eu-west-1.compute.internal</td><td>Ready</td><td>eu-west-1a</td></tr><tr><td>ip-10-242-53-131.eu-west-1.compute.internal</td><td>Ready</td><td>eu-west-1b</td></tr><tr><td>ip-10-242-60-155.eu-west-1.compute.internal</td><td>Ready</td><td>eu-west-1b</td></tr><tr><td>ip-10-242-73-77.eu-west-1.compute.internal</td><td>Ready</td><td>eu-west-1c</td></tr><tr><td>ip-10-242-73-89.eu-west-1.compute.internal</td><td>Ready</td><td>eu-west-1c</td></tr></tbody></table><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get machines <span style=color:green># list of machines for the multi-AZ shoot control plane</span>
</span></span></code></pre></div><table><thead><tr><th>NAME</th><th>STATUS</th></tr></thead><tbody><tr><td>shoot&ndash;garden&ndash;aws-ha2-cpu-worker-etcd-z1-66659-sf4wt</td><td>Running</td></tr><tr><td>shoot&ndash;garden&ndash;aws-ha2-cpu-worker-etcd-z2-c767d-s8cmf</td><td>Running</td></tr><tr><td>shoot&ndash;garden&ndash;aws-ha2-cpu-worker-etcd-z3-9678d-6p8w5</td><td>Running</td></tr><tr><td>shoot&ndash;garden&ndash;aws-ha2-cpu-worker-z1-766bc-pjq6n</td><td>Running</td></tr><tr><td>shoot&ndash;garden&ndash;aws-ha2-cpu-worker-z2-85968-5qmjh</td><td>Running</td></tr><tr><td>shoot&ndash;garden&ndash;aws-ha2-cpu-worker-z3-5f499-hnrs6</td><td>Running</td></tr><tr><td>shoot&ndash;garden&ndash;aws-ha2-etcd-compaction-z1-6bd58-9ffc7</td><td>Running</td></tr></tbody></table></details><details><summary>State during outage</summary><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get po -n &lt;shoot-control-ns&gt; <span style=color:green># list of pods in the shoot control plane</span>
</span></span></code></pre></div><table><thead><tr><th>NAME</th><th>READY</th><th>STATUS</th><th>NODE</th></tr></thead><tbody><tr><td>cert-controller-manager-6cf9787df6-dt5nw</td><td>1/1</td><td>Running</td><td>ip-10-242-60-155.eu-west-1.compute.internal</td></tr><tr><td>cloud-controller-manager-7748bcf697-t2pn7</td><td>1/1</td><td>Running</td><td>ip-10-242-73-89.eu-west-1.compute.internal</td></tr><tr><td>csi-driver-controller-6cd9bc7997-bn82b</td><td>6/6</td><td>Running</td><td>ip-10-242-60-155.eu-west-1.compute.internal</td></tr><tr><td>csi-snapshot-controller-5f774d57b4-rskwj</td><td>1/1</td><td>Running</td><td>ip-10-242-73-89.eu-west-1.compute.internal</td></tr><tr><td>csi-snapshot-validation-7c99986c85-ft2qp</td><td>1/1</td><td>Running</td><td>ip-10-242-60-155.eu-west-1.compute.internal</td></tr><tr><td>etcd-events-0</td><td>2/2</td><td>Running</td><td>ip-10-242-60-155.eu-west-1.compute.internal</td></tr><tr><td>etcd-events-1</td><td>2/2</td><td>Running</td><td>ip-10-242-73-89.eu-west-1.compute.internal</td></tr><tr><td>etcd-events-2</td><td>0/2</td><td>Pending</td><td><none></td></tr><tr><td>etcd-main-0</td><td>2/2</td><td>Running</td><td>ip-10-242-73-77.eu-west-1.compute.internal</td></tr><tr><td>etcd-main-1</td><td>0/2</td><td>Pending</td><td><none></td></tr><tr><td>etcd-main-2</td><td>2/2</td><td>Running</td><td>ip-10-242-53-131.eu-west-1.compute.internal</td></tr><tr><td>gardener-resource-manager-7fff9f77f6-8wr5n</td><td>1/1</td><td>Running</td><td>ip-10-242-73-89.eu-west-1.compute.internal</td></tr><tr><td>gardener-resource-manager-7fff9f77f6-jwggx</td><td>1/1</td><td>Running</td><td>ip-10-242-73-89.eu-west-1.compute.internal</td></tr><tr><td>gardener-resource-manager-7fff9f77f6-lkgjh</td><td>1/1</td><td>Running</td><td>ip-10-242-60-155.eu-west-1.compute.internal</td></tr><tr><td>grafana-operators-79b9cd58bb-m55sx</td><td>1/1</td><td>Running</td><td>ip-10-242-60-155.eu-west-1.compute.internal</td></tr><tr><td>grafana-users-85c7b6856c-gx48n</td><td>1/1</td><td>Running</td><td>ip-10-242-73-89.eu-west-1.compute.internal</td></tr><tr><td>kube-apiserver-5fcb7f4bff-845p7</td><td>1/1</td><td>Running</td><td>ip-10-242-73-89.eu-west-1.compute.internal</td></tr><tr><td>kube-apiserver-5fcb7f4bff-mrspt</td><td>1/1</td><td>Running</td><td>ip-10-242-60-155.eu-west-1.compute.internal</td></tr><tr><td>kube-apiserver-5fcb7f4bff-vkrdh</td><td>1/1</td><td>Running</td><td>ip-10-242-73-89.eu-west-1.compute.internal</td></tr><tr><td>kube-controller-manager-6b94bcbc4-49v5x</td><td>1/1</td><td>Running</td><td>ip-10-242-60-155.eu-west-1.compute.internal</td></tr><tr><td>kube-scheduler-7f855ffbc4-6xnbk</td><td>1/1</td><td>Running</td><td>ip-10-242-73-89.eu-west-1.compute.internal</td></tr><tr><td>kube-state-metrics-5446bb6d56-g8wkp</td><td>1/1</td><td>Running</td><td>ip-10-242-73-89.eu-west-1.compute.internal</td></tr><tr><td>loki-0</td><td>4/4</td><td>Running</td><td>ip-10-242-73-89.eu-west-1.compute.internal</td></tr><tr><td>machine-controller-manager-967bc89b5-rr96r</td><td>2/2</td><td>Running</td><td>ip-10-242-60-155.eu-west-1.compute.internal</td></tr><tr><td>prometheus-0</td><td>3/3</td><td>Running</td><td>ip-10-242-60-155.eu-west-1.compute.internal</td></tr><tr><td>shoot-dns-service-75768bd764-7xhrw</td><td>1/1</td><td>Running</td><td>ip-10-242-73-89.eu-west-1.compute.internal</td></tr><tr><td>vpa-admission-controller-6994f855c9-7xt8p</td><td>1/1</td><td>Running</td><td>ip-10-242-73-89.eu-west-1.compute.internal</td></tr><tr><td>vpa-recommender-5bf4cfccb6-8wdpr</td><td>1/1</td><td>Running</td><td>ip-10-242-73-89.eu-west-1.compute.internal</td></tr><tr><td>vpa-updater-6f795d7bb8-gccv2</td><td>1/1</td><td>Running</td><td>ip-10-242-60-155.eu-west-1.compute.internal</td></tr><tr><td>vpn-seed-server-748674b7d8-cb8gh</td><td>2/2</td><td>Running</td><td>ip-10-242-60-155.eu-west-1.compute.internal</td></tr></tbody></table><p>Most of the pods except <code>etcd-events-2</code> and <code>etcd-main-1</code> are stuck in <code>Pending</code> state. Rest all of the pods are which were running on nodes in <code>eu-west-1a</code> zone were rescheduled automatically.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get nodes -o=custom-columns=<span style=color:#a31515>&#39;NAME:metadata.name,ZONE:metadata.labels.topology\.kubernetes\.io\/zone&#39;</span> <span style=color:green># list of nodes with name, zone and status (was taken separately)</span>
</span></span></code></pre></div><table><thead><tr><th>NAME</th><th>STATUS</th><th>ZONE</th></tr></thead><tbody><tr><td>ip-10-242-20-17.eu-west-1.compute.internal</td><td>NotReady</td><td>eu-west-1a</td></tr><tr><td>ip-10-242-22-85.eu-west-1.compute.internal</td><td>NotReady</td><td>eu-west-1a</td></tr><tr><td>ip-10-242-3-0.eu-west-1.compute.internal</td><td>NotReady</td><td>eu-west-1a</td></tr><tr><td>ip-10-242-53-131.eu-west-1.compute.internal</td><td>Ready</td><td>eu-west-1b</td></tr><tr><td>ip-10-242-60-155.eu-west-1.compute.internal</td><td>Ready</td><td>eu-west-1b</td></tr><tr><td>ip-10-242-73-77.eu-west-1.compute.internal</td><td>Ready</td><td>eu-west-1c</td></tr><tr><td>ip-10-242-73-89.eu-west-1.compute.internal</td><td>Ready</td><td>eu-west-1c</td></tr></tbody></table><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get machines <span style=color:green># list of machines for the multi-AZ shoot control plane</span>
</span></span></code></pre></div><table><thead><tr><th>NAME</th><th>STATUS</th><th>AGE</th></tr></thead><tbody><tr><td>shoot&ndash;garden&ndash;aws-ha2-cpu-worker-etcd-z1-66659-jlv56</td><td>Terminating</td><td>21m</td></tr><tr><td>shoot&ndash;garden&ndash;aws-ha2-cpu-worker-etcd-z1-66659-sf4wt</td><td>Unknown</td><td>3d</td></tr><tr><td>shoot&ndash;garden&ndash;aws-ha2-cpu-worker-etcd-z2-c767d-s8cmf</td><td>Running</td><td>3d</td></tr><tr><td>shoot&ndash;garden&ndash;aws-ha2-cpu-worker-etcd-z3-9678d-6p8w5</td><td>Running</td><td>3d</td></tr><tr><td>shoot&ndash;garden&ndash;aws-ha2-cpu-worker-z1-766bc-9m45j</td><td>CrashLoopBackOff</td><td>2m55s</td></tr><tr><td>shoot&ndash;garden&ndash;aws-ha2-cpu-worker-z1-766bc-pjq6n</td><td>Unknown</td><td>2d21h</td></tr><tr><td>shoot&ndash;garden&ndash;aws-ha2-cpu-worker-z1-766bc-vlflq</td><td>Terminating</td><td>28m</td></tr><tr><td>shoot&ndash;garden&ndash;aws-ha2-cpu-worker-z2-85968-5qmjh</td><td>Running</td><td>3d</td></tr><tr><td>shoot&ndash;garden&ndash;aws-ha2-cpu-worker-z2-85968-zs9lr</td><td>CrashLoopBackOff</td><td>7m26s</td></tr><tr><td>shoot&ndash;garden&ndash;aws-ha2-cpu-worker-z3-5f499-hnrs6</td><td>Running</td><td>2d21h</td></tr><tr><td>shoot&ndash;garden&ndash;aws-ha2-etcd-compaction-z1-6bd58-8qzln</td><td>CrashLoopBackOff</td><td>12m</td></tr><tr><td>shoot&ndash;garden&ndash;aws-ha2-etcd-compaction-z1-6bd58-9ffc7</td><td>Terminating</td><td>3d</td></tr></tbody></table><p>MCM attempts to delete the machines and since it is unable to the machines transition to <code>Terminating</code> state and are stuck there. It subsequently attempts to launch new machines which also fails and these machines transition to <code>CrashLoopBackOff</code> state.</p></details><h3 id=ingressegress-traffic-analysis-details>Ingress/Egress Traffic Analysis Details</h3><p>Consider the following etcd cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ etcdctl endpoint status --cluster -w table
</span></span></code></pre></div><table><thead><tr><th>ENDPOINT</th><th>ID</th><th>VERSION</th><th>DB SIZE</th><th>IS LEADER</th><th>IS LEARNER</th><th>RAFT TERM</th><th>RAFT INDEX</th><th>RAFT APPLIED INDEX</th></tr></thead><tbody><tr><td><code>https://etcd-main-0.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379</code></td><td>37e93e9d1dd2cc8e</td><td>3.4.13</td><td>7.6 MB</td><td>false</td><td>false</td><td>47</td><td>3863</td><td>3863</td></tr><tr><td><code>https://etcd-main-2.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379</code></td><td>65fe447d73e9dc58</td><td>3.4.13</td><td>7.6 MB</td><td>true</td><td>false</td><td>47</td><td>3863</td><td>3863</td></tr><tr><td><code>https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379</code></td><td>ad4fe89f4e731298</td><td>3.4.13</td><td>7.6 MB</td><td>false</td><td>false</td><td>47</td><td>3863</td><td>3863</td></tr></tbody></table><details><summary>Multi-zonal shoot control plane ingress/egress traffic in a fresh shoot cluster with no user activity</summary><p>The steady state traffic (post all controllers have made initial <code>list</code> requests to refresh their informer caches) is depicted below (span = 1hr):</p><img src=/__resources/etcdcluster-steadystate-traffic_4f7a2b.png><p><em>Observations:</em></p><ul><li>Leader to per follower max egress: ~20Kib/s</li><li>One Follower to Leader max egress: ~20Kib/s</li><li>Follower to follower max egress: ~2Kibs/s</li></ul><p>Total ingress + egress traffic amongst etcd members = ~84Kib/s.</p></details><details><summary>Traffic generated during PUT requests to etcd leader</summary><p>Generating a load 100 put requests/second for 30 seconds duration by targeting etcd leader, This will generate ~100KiB/s traffic (value size is 1kib).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span> benchmark put --target-leader  --rate 100 --conns=400 --clients=400 --sequential-keys --key-starts 0 --val-size=1024 --total=3000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>     --endpoints=https://etcd-main-client:2379 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>     --key=/var/etcd/ssl/client/client/tls.key <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>     --cacert=/var/etcd/ssl/client/ca/bundle.crt <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>     --cert=/var/etcd/ssl/client/client/tls.crt
</span></span></code></pre></div><img src=/__resources/put-request-targeting-leader_0fcdd1.png><p><em>Observations:</em></p><ul><li>Leader to per follower max egress: ~155 KiB/s (pattern duration: 50 secs)</li><li>One Follower to Leader max egress: ~50Kib/s (pattern duration: 50 secs)</li><li>Follower to follower max egress: ~2Kibs/s</li></ul><p>Total ingress + egress traffic amongst etcd members = ~412Kib/s.</p></details><details><summary>Traffic generated during PUT requests to etcd follower</summary>
Generating a load 100 put requests/second for 30 seconds duration by targeting etcd follower, This will generate ~100KiB/s traffic(value size is 1kib).<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>benchmark put  --rate 100 --conns=400 --clients=400 --sequential-keys --key-starts 3000 --val-size=1024 --total=3000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --key=/var/etcd/ssl/client/client/tls.key <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --cacert=/var/etcd/ssl/client/ca/bundle.crt <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --cert=/var/etcd/ssl/client/client/tls.crt
</span></span></code></pre></div><img src=/__resources/put-request-targeting-follower_faa2ab.png><p><em>Observations:</em></p><ul><li>In this case, the follower(<code>etcd-main-1</code>) redirects the put request to leader(<code>etcd-main2</code>) max egress: ~168 KiB/s</li><li>Leader to per follower max egress: ~150 KiB/s (pattern duration: 50 secs)</li><li>One Follower to Leader max egress: ~45Kib/s (pattern duration: 50 secs)</li><li>Follower to follower max egress: ~2Kibs/s</li></ul><p>Total ingress + egress traffic amongst etcd members = ~517KiB/s.</p></details><details><summary>Traffic generated during etcd follower initial sync with large revision difference</summary><p>Consider the following etcd cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ etcdctl endpoint status --cluster -w table
</span></span></code></pre></div><table><thead><tr><th>ENDPOINT</th><th>ID</th><th>VERSION</th><th>DB SIZE</th><th>IS LEADER</th><th>IS LEARNER</th><th>RAFT TERM</th><th>RAFT INDEX</th><th>RAFT APPLIED INDEX</th></tr></thead><tbody><tr><td><a href=https://etcd-main-0.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379>https://etcd-main-0.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379</a></td><td>37e93e9d1dd2cc8e</td><td>3.4.13</td><td>5.1 GB</td><td>false</td><td>false</td><td>48</td><td>47527</td><td>47527</td></tr><tr><td><a href=https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379>https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379</a></td><td>ad4fe89f4e731298</td><td>3.4.13</td><td>5.1 GB</td><td>true</td><td>false</td><td>48</td><td>47576</td><td>47576</td></tr></tbody></table><p>In this case, new follower or crashed follower <code>etcd-main-2</code> joins the etcd cluster with large revision difference (5.1 GB DB size). The etcd follower had its DB size = 14MB when it crashed. There was a flurry of activity and that increased the leader DB size to 5.1GB thus creating a huge revision difference.</p><blockquote><p>Scale up etcd member to 3</p></blockquote><p><code>kubectl scale statefulsets etcd-main --replicas=3 -n shoot--ash-garden--mz-neem</code></p><table><thead><tr><th>ENDPOINT</th><th>ID</th><th>VERSION</th><th>DB SIZE</th><th>IS LEADER</th><th>IS LEARNER</th><th>RAFT TERM</th><th>RAFT INDEX</th><th>RAFT APPLIED INDEX</th></tr></thead><tbody><tr><td><a href=https://etcd-main-0.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379>https://etcd-main-0.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379</a></td><td>37e93e9d1dd2cc8e</td><td>3.4.13</td><td>5.1 GB</td><td>false</td><td>false</td><td>48</td><td>50502</td><td>50502</td></tr><tr><td><a href=https://etcd-main-2.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379>https://etcd-main-2.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379</a></td><td>65fe447d73e9dc58</td><td>3.4.13</td><td>5.0 GB</td><td>false</td><td>false</td><td>48</td><td>50502</td><td>50502</td></tr><tr><td><a href=https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379>https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379</a></td><td>ad4fe89f4e731298</td><td>3.4.13</td><td>5.1 GB</td><td>true</td><td>false</td><td>48</td><td>50502</td><td>50502</td></tr></tbody></table><img src=/__resources/etcd-member-initial-sync-with-large-revision_a51b87.png><p><em>Observations:</em></p><ul><li>Leader to new follower or crashed follower <code>etcd-main-2</code> (which joins with large revision difference) max egress: ~120 MiB/s (noticeable pattern duration: 40 secs).</li><li>New follower <code>etcd-main-2</code> to Leader max egress: ~159 KiB/s (pattern duration: 40 secs).</li><li>Leader to another follower <code>etcd-main-0</code> max egress: &lt;20KiB/s.</li><li>Follower <code>etcd-main-0</code> to Leader max egress: &lt;20> Kib/s.</li><li>Follower to follower max egress: ~2Kibs/s</li></ul><p>Total ingress + egress traffic amongst etcd members = ~121MiB/s.</p></details><details><summary>Traffic generated during GET requests to etcd leader</summary><p>In this case, trying to get the keys which matches between 1 and 17999 by targeting leader <code>etcd-main-1</code>. This will dump both keys and values.</p><p>Executing the following command from <code>etcd-client</code> pod(running in same namespace).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>root@etcd-client:/#  etcdctl --endpoints=https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 get 1 17999 &gt; /tmp/range2.txt
</span></span><span style=display:flex><span>root@etcd-client:/# du -h  /tmp/range2.txt
</span></span><span style=display:flex><span>607M	/tmp/range2.txt
</span></span></code></pre></div><img src=/__resources/get-request-targeting-leader_cc22ea.png><p><em>Observations:</em></p><ul><li>Downloaded dump file is around 607 MiB.</li><li>Leader <code>etcd-main-1</code> to <code>etcd-client</code> max egress ~34MiBs.</li><li>Etcd intra cluster network traffic remains same, observed there is no change in network traffic pattern</li></ul></details><details><summary>Traffic generated during GET requests to etcd follower</summary><p>In this case, trying to get the keys which matches between 1 and 17999 by targeting follower<code>etcd-main-2</code>. This will dump both keys and values.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>root@etcd-client:/# etcdctl --endpoints=https://etcd-main-2.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 get 1 17999 &gt; /tmp/range.txt
</span></span><span style=display:flex><span>root@etcd-client:/# du -h  /tmp/range.txt
</span></span><span style=display:flex><span>607M	/tmp/range.txt
</span></span><span style=display:flex><span>root@etcd-client:/#
</span></span></code></pre></div><img src=/__resources/get-request-targeting-follower_12d930.png><p><em>Observations:</em></p><ul><li>Downloaded dump file is around 607 MiB.</li><li>Follower <code>etcd-main-2</code> to <code>etcd-client</code> max egress ~32MiBs.</li><li>Etcd intra cluster network traffic remains same, observed there is no change in network traffic pattern</li><li><strong>Watch requests are not forwarded to Leader <code>etcd-main-1</code> from <code>etcd-main-2</code></strong></li></ul></details><details><summary>Traffic generated during DELETE requests to etcd leader</summary><p>In this case, trying to delete the keys which matches between 0 and 99999 by targeting leader.</p><p>Executing the following command from <code>etcd-client</code> pod.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>root@etcd-client:/# time etcdctl --endpoints=https://etcd-main-2.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 del 0 99999    --dial-timeout=300s --command-timeout=300s
</span></span><span style=display:flex><span>99999
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>real	0m0.664s
</span></span><span style=display:flex><span>user	0m0.008s
</span></span><span style=display:flex><span>sys	0m0.016s
</span></span><span style=display:flex><span>root@etcd-client:/#
</span></span></code></pre></div><img src=/__resources/delete-request-targeting-leader_89e7a2.png><p><em>Observations:</em></p><ul><li>Downloaded dump file is around 607 MiB.</li><li>Leader <code>etcd-main-2</code> to <code>etcd-client</code> max egress ~226B/s.</li><li>Etcd intra cluster network traffic remains same, observed there is no change in network traffic pattern.</li></ul></details><details><summary>Traffic generated during DELETE requests to etcd follower</summary><p>In this case, trying to delete the keys which matches between 0 and 99999 by targeting follower.</p><p>Executing the following command from <code>etcd-client</code> pod.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>root@etcd-client:/# time etcdctl --endpoints=https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 del 0 99999    --dial-timeout=300s --command-timeout=300s
</span></span><span style=display:flex><span>99999
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>real	0m0.664s
</span></span><span style=display:flex><span>user	0m0.008s
</span></span><span style=display:flex><span>sys	0m0.016s
</span></span><span style=display:flex><span>root@etcd-client:/#
</span></span></code></pre></div><img src=/__resources/delete-request-targeting-follower_66ffbf.png><p><em>Observations:</em></p><ul><li>Downloaded dump file is around 607 MiB.</li><li>Leader <code>etcd-main-2</code> to <code>etcd-client</code> max egress ~222B/s.</li><li>Etcd intra cluster network traffic remains same, observed there is no change in network traffic pattern.</li></ul></details><details><summary>Traffic generated during WATCH requests to etcd</summary><p>Etcd cluster state</p><table><thead><tr><th>ENDPOINT</th><th>ID</th><th>VERSION</th><th>DB SIZE</th><th>IS LEADER</th><th>IS LEARNER</th><th>RAFT TERM</th><th>RAFT INDEX</th><th>RAFT APPLIED INDEX</th></tr></thead><tbody><tr><td><a href=https://etcd-main-0.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379>https://etcd-main-0.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379</a></td><td>37e93e9d1dd2cc8e</td><td>3.4.13</td><td>673 MB</td><td>false</td><td>false</td><td>388</td><td>970471</td><td>970471</td></tr><tr><td><a href=https://etcd-main-2.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379>https://etcd-main-2.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379</a></td><td>65fe447d73e9dc58</td><td>3.4.13</td><td>673 MB</td><td>true</td><td>false</td><td>388</td><td>970472</td><td>970472</td></tr><tr><td><a href=https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379>https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379</a></td><td>ad4fe89f4e731298</td><td>3.4.13</td><td>673 MB</td><td>false</td><td>false</td><td>388</td><td>970472</td><td>970472</td></tr></tbody></table><p>Watching the keys which matches between 0 and 99999 by targeting follower.</p><p>Executing the following command from <code>etcd-client</code> pod.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>time etcdctl --endpoints=https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 watch 0 99999    --dial-timeout=300s --command-timeout=300s
</span></span></code></pre></div><p>In parallel generating <code>100000</code> keys and each value size is 1Kib by targeting etcd leader for this case around(500rps)</p><pre tabindex=0><code>benchmark put --target-leader  --rate 500 --conns=400 --clients=800 --sequential-keys --key-starts 0 --val-size=1024 --total=100000 \
    --endpoints=https://etcd-main-client:2379 \
    --key=/var/etcd/ssl/client/client/tls.key \
    --cacert=/var/etcd/ssl/client/ca/bundle.crt \
    --cert=/var/etcd/ssl/client/client/tls.crt
</code></pre><img src=/__resources/watch-request-targeting-follower_b0c016.png><p><em>Observations:</em></p><ul><li>Etcd intra cluster network traffic remains same, observed there is no change in network traffic pattern.</li><li>Follower <code>etcd-main-1</code> to <code>etcd-client</code> max egress is 496 KiBs.</li><li><strong>Watch requests are not forwarded to Leader <code>etcd-main-2</code> from follower <code>etcd-main-1</code></strong> .</li></ul><p>Deleting the keys which matches between 0 and 99999 by targeting follower and in parallel watching the keys.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>root@etcd-client:/# time etcdctl --endpoints=https://etcd-main-1.etcd-main-peer.shoot--ash-garden--mz-neem.svc:2379 del 0 99999    --dial-timeout=300s --command-timeout=300s
</span></span><span style=display:flex><span>99999
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>real	0m0.590s
</span></span><span style=display:flex><span>user	0m0.018s
</span></span><span style=display:flex><span>sys	0m0.006s
</span></span></code></pre></div><img src=/__resources/watch-request-del-targeting-follower_f4a9b0.png><p><em>Observations:</em></p><ul><li>Etcd intra cluster network traffic remains same, observed there is no change in network traffic pattern.</li><li>Follower <code>etcd-main-1</code> to <code>etcd-client</code> max egress is 222B/s.</li><li>watch lists the keys which are deleted, not values.</li></ul></details></div><div class=td-content style=page-break-before:always><h1 id=pg-1fb3e5ef9afe10a6ed609201f5b717b7>16 - Monitoring Stack - Migrating to the prometheus-operator</h1><h1 id=gep-19-monitoring-stack---migrating-to-the-prometheus-operator>GEP-19: Monitoring Stack - Migrating to the prometheus-operator</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#gep-19-monitoring-stack---migrating-to-the-prometheus-operator>GEP-19: Monitoring Stack - Migrating to the prometheus-operator</a><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#summary>Summary</a></li><li><a href=#motivation>Motivation</a><ul><li><a href=#goals>Goals</a></li><li><a href=#non-goals>Non-Goals</a></li></ul></li><li><a href=#proposal>Proposal</a><ul><li><a href=#api>API</a></li><li><a href=#prometheus-operator-crds>Prometheus Operator CRDs</a></li><li><a href=#shoot-monitoring>Shoot Monitoring</a></li><li><a href=#seed-monitoring>Seed Monitoring</a></li><li><a href=#byomc-bring-your-own-monitoring-configuration>BYOMC (Bring your own monitoring configuration)</a></li><li><a href=#grafana-sidecar>Grafana Sidecar</a></li><li><a href=#migration>Migration</a></li></ul></li><li><a href=#alternatives>Alternatives</a></li></ul></li></ul><h2 id=summary>Summary</h2><p>As Gardener has grown, the monitoring configuration has also evolved with it.
Many components must be monitored and the configuration for these components
must also be managed. This poses a challenge because the configuration is
distributed across the Gardener project among different folders and even
different repositories (for example extensions). While it is not possible to
centralize the configuration, it is possible to improve the developer experience
and improve the general stability of the monitoring. This can be done by
introducing the <a href=https://github.com/prometheus-operator/prometheus-operator>prometheus-operator</a>. This operator will make it easier for
monitoring configuration to be discovered and picked up with the use of the
<a href=https://github.com/prometheus-operator/prometheus-operator#customresourcedefinitions>Custom Resources</a> provided by the prometheus-operator. These
resources can also be directly referenced in Go and be deployed by their
respective components, instead of creating <code>.yaml</code> files in Go, or templating
charts. With the addition of the prometheus-operator it should also be easier to
add new features, such as Thanos.</p><h2 id=motivation>Motivation</h2><p>Simplify monitoring changes and extensions with the use of the
<a href=https://github.com/prometheus-operator/prometheus-operator>prometheus-operator</a>. The current extension contract is described
<a href=/docs/gardener/extensions/logging-and-monitoring/>here</a>. This document aims to define a new contract.</p><p>Make it easier to add new monitoring features and make new changes. For example,
when using the <a href=https://github.com/prometheus-operator/prometheus-operator>prometheus-operator</a> components can bring their own monitoring
configuration and specify exactly how they should be monitored without needing
to add this configuration directly into Prometheus.</p><p>The prometheus-operator handles validation of monitoring configuration. It will
be more difficult to give Prometheus invalid config.</p><h3 id=goals>Goals</h3><ul><li><p>Migrate from the current monitoring stack to the prometheus-operator.</p></li><li><p>Improve the monitoring extensibility and improve developer experience when
adding or editing configuration. This includes the monitoring extensions in
addition to core Gardener components.</p></li><li><p>Provide a clear direction on how monitoring resources should be managed.
Currently, there are many ways that monitoring configuration is being deployed
and this should be unified.</p></li><li><p>Improve how dashboards are discovered and provisioned for Grafana. Currently,
all dashboards are appended into a single configmap. This can be an issue if
the maximum configmap size of 1MiB is ever exceeded.</p></li></ul><h3 id=non-goals>Non-Goals</h3><ul><li><p>Changes to the logging stack.</p></li><li><p>Feature parity between the current solution and the one proposed in this GEP.
The new stack should provide similar monitoring coverage, but it will be very
difficult to evaluate if there is feature parity. Perhaps some features will
be missing, but others may be added.</p></li></ul><h2 id=proposal>Proposal</h2><p>Today, Gardener provides monitoring for shoot clusters (i.e. system components
and the control plane) and for the seed cluster. The proposal is to migrate
these monitoring stacks to use the <a href=https://github.com/prometheus-operator/prometheus-operator>prometheus-operator</a>. The proposal is lined
out below:</p><h3 id=api>API</h3><p>Use the <a href=https://github.com/prometheus-operator/prometheus-operator/tree/main/pkg/apis/monitoring>API</a> provided by the <a href=https://github.com/prometheus-operator/prometheus-operator>prometheus-operator</a> and create Go structs.</p><h3 id=prometheus-operator-crds>Prometheus Operator CRDs</h3><p>Deploy the <a href=https://github.com/prometheus-operator/prometheus-operator>prometheus-operator</a> and its CRDs. These components can be deployed
via <code>ManagedResources</code>. The operator itself and some other components outlined
in the GEP will be deployed in a new namespace called <code>monitoring</code>. The CRDs for
the <a href=https://github.com/prometheus-operator/prometheus-operator>prometheus-operator</a> and the operator itself can be found
<a href=https://github.com/prometheus-operator/prometheus-operator/blob/main/bundle.yaml>here</a>. This step is a prerequisite for all other steps.</p><h3 id=shoot-monitoring>Shoot Monitoring</h3><p>Gardener will create a monitoring stack similar to the current one with the
<a href=https://github.com/prometheus-operator/prometheus-operator>prometheus-operator</a> custom resources.</p><ol><li><p>Most of the shoot monitoring is deployed via this
<a href=https://github.com/gardener/gardener/tree/master/charts/seed-monitoring/charts>chart</a>. The goal is to create a similar stack, but not
necessarily with feature parity, using the <a href=https://github.com/prometheus-operator/prometheus-operator>prometheus-operator</a>.</p><ul><li>An example Prometheus object that would be deployed in a shoot&rsquo;s
control plane.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: monitoring.coreos.com/v1
</span></span><span style=display:flex><span>kind: Prometheus
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: prometheus
</span></span><span style=display:flex><span>  name: prometheus
</span></span><span style=display:flex><span>  namespace: shoot--project--name
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  enableAdminAPI: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>  logFormat: logfmt
</span></span><span style=display:flex><span>  logLevel: info
</span></span><span style=display:flex><span>  image: image:tag
</span></span><span style=display:flex><span>  paused: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>  portName: web
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  retention: 30d
</span></span><span style=display:flex><span>  routePrefix: /
</span></span><span style=display:flex><span>  serviceAccountName: prometheus
</span></span><span style=display:flex><span>  serviceMonitorNamespaceSelector:
</span></span><span style=display:flex><span>    matchExpressions:
</span></span><span style=display:flex><span>    - key: kubernetes.io/metadata.name
</span></span><span style=display:flex><span>      operator: In
</span></span><span style=display:flex><span>      values:
</span></span><span style=display:flex><span>      - shoot--project--name
</span></span><span style=display:flex><span>  podMonitorNamespaceSelector:
</span></span><span style=display:flex><span>    matchExpressions:
</span></span><span style=display:flex><span>    - key: kubernetes.io/metadata.name
</span></span><span style=display:flex><span>      operator: In
</span></span><span style=display:flex><span>      values:
</span></span><span style=display:flex><span>      - shoot--project--name
</span></span><span style=display:flex><span>  ruleNamespaceSelector:
</span></span><span style=display:flex><span>    matchExpressions:
</span></span><span style=display:flex><span>    - key: kubernetes.io/metadata.name
</span></span><span style=display:flex><span>      operator: In
</span></span><span style=display:flex><span>      values:
</span></span><span style=display:flex><span>      - shoot--project--name
</span></span><span style=display:flex><span>  serviceMonitorSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      monitoring.gardener.cloud/monitoring-target: shoot-control-plane
</span></span><span style=display:flex><span>  podMonitorSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      monitoring.gardener.cloud/monitoring-target: shoot-control-plane
</span></span><span style=display:flex><span>  storage:
</span></span><span style=display:flex><span>    volumeClaimTemplate:
</span></span><span style=display:flex><span>      spec:
</span></span><span style=display:flex><span>        accessModes:
</span></span><span style=display:flex><span>        - ReadWriteOnce
</span></span><span style=display:flex><span>        resources:
</span></span><span style=display:flex><span>          requests:
</span></span><span style=display:flex><span>            storage: 20Gi
</span></span><span style=display:flex><span>  version: v2.35.0
</span></span></code></pre></div></li><li><p>Contract between the shoot <code>Prometheus</code> and its configuration.</p><ul><li><p><code>Prometheus</code> can discover <code>*Monitors</code> in different namespaces and also
by using labels.</p></li><li><p>In some cases, specific configuration is required (e.g. specific
configuration due to K8s versions). In this case, the configuration will
also be deployed in the shoot&rsquo;s namespace and Prometheus will also be able
to discover this configuration.</p></li><li><p>Prometheus must also distinguish between <code>*Monitors</code> relevant for shoot
control plane and shoot targets. This can be done with a
<code>serviceMonitorSelector</code> and <code>podMonitorSelector</code> where
<code>monitoring.gardener.cloud/monitoring-target=shoot-control-plane</code>. For a
<code>ServiceMonitor</code> it would look like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>serviceMonitorSelector:
</span></span><span style=display:flex><span>  matchLabels:
</span></span><span style=display:flex><span>    monitoring.gardener.cloud/monitoring-target: shoot-control-plane
</span></span></code></pre></div></li><li><p>In addition to a Prometheus, the configuration must also be created. To
do this, each <code>job</code> in the Prometheus configuration will need to be
replaced with either a <code>ServiceMonitor</code>, <code>PodMonitor</code>, or <code>Probe</code>. This
<code>ServiceMonitor</code> will be picked up by the Prometheus defined in the
previous step. This <code>ServiceMonitor</code> will scrape any
service that has the label <code>app=prometheus</code> on the port called <code>metrics</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: monitoring.coreos.com/v1
</span></span><span style=display:flex><span>kind: ServiceMonitor
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    monitoring.gardener.cloud/monitoring-target: shoot-control-plane
</span></span><span style=display:flex><span>  name: prometheus-job
</span></span><span style=display:flex><span>  namespace: shoot--project--name
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  endpoints:
</span></span><span style=display:flex><span>  - port: metrics
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: prometheus
</span></span></code></pre></div></li></ul></li><li><p>Prometheus needs to discover targets running in the shoot cluster.
Normally, this is done by changing the <code>api_server</code> field in the config
(<a href=https://github.com/gardener/gardener/blob/0f4d22270927e2aee8b821f858fb76162ccd8a86/charts/seed-monitoring/charts/core/charts/prometheus/templates/config.yaml#L311>example</a>). This is currently not possible with the
prometheus-operator, but there is an open <a href=https://github.com/prometheus-operator/prometheus-operator/issues/4828>issue</a>.</p><ul><li><p>Preferred approach: A second Prometheus can be created that is running
in <a href=https://prometheus.io/blog/2021/11/16/agent/>agent mode</a>. This Prometheus can also be deployed/managed by the
<a href=https://github.com/prometheus-operator/prometheus-operator>prometheus-operator</a>. The agent Prometheus can be configured to use
the API Server for the shoot cluster and use service discovery in the
shoot. The metrics can then be written via remote write to the
&ldquo;normal&rdquo; Prometheus or federated. This Prometheus will also discover
configuration in the same way as the other Prometheus with 1
difference. Instead of discovering configuration with the label
<code>monitoring.gardener.cloud/monitoring-target=shoot-control-plane</code> it will find configuration
with the label <code>monitoring.gardener.cloud/monitoring-target=shoot</code>.</p></li><li><p>Alternative: Use <a href=https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/additional-scrape-config.md>additional scrape config</a>. In this case, the
Prometheus config snippet is put into a secret and the
<a href=https://github.com/prometheus-operator/prometheus-operator>prometheus-operator</a> will append it to the config. The downside here is
that it is only possible to have 1 <code>additional-scrape-config</code> per
Prometheus. This could be an issue if multiple components will need to
use this.</p></li></ul></li><li><p>Deploy an optional <a href=https://github.com/gardener/gardener/tree/master/charts/seed-monitoring/charts/alertmanager>alertmanager</a> that is deployed
whenever the end-user <a href=/docs/gardener/monitoring/alerting/#alerting-for-users>specifies</a> alerting.</p><ul><li><p>Create an <code>Alertmanager</code> resource</p></li><li><p>Create the <code>AlertmanagerConfig</code></p></li></ul></li><li><p>Health checks - The gardenlet periodically checks the status of the
Prometheus <code>StatefulSet</code> among other components in the shoot care
controller. The gardenlet knows which component to check based on labels.
Since the gardenlet is no longer deploying the <code>StatefulSet</code> directly and
rather a <code>Prometheus</code> resource, it does not know which labels are
attached to the Prometheus <code>StatefulSet</code>. However, the
<a href=https://github.com/prometheus-operator/prometheus-operator>prometheus-operator</a> will create <code>StatefulSets</code> with the same labelset
that the <code>Prometheus</code> resource has. The gardenlet will be able to
discover the <code>StatefulSet</code> because it knows the labelset of the
<code>Prometheus</code> resource.</p></li></ol><h3 id=seed-monitoring>Seed Monitoring</h3><p>There is a monitoring stack deployed for each seed cluster. A similar setup must
also be provided using the <a href=https://github.com/prometheus-operator/prometheus-operator>prometheus-operator</a>. The steps for this are very
similar to the shoot monitoring.</p><ul><li><p>Replace the Prometheis and their configuration.</p></li><li><p>Replace the <a href=https://github.com/gardener/gardener/blob/0f4d22270927e2aee8b821f858fb76162ccd8a86/charts/seed-bootstrap/templates/alertmanager/alertmanager.yaml>alertmanager</a> and its configuration.</p></li></ul><h3 id=byomc-bring-your-own-monitoring-configuration>BYOMC (Bring your own monitoring configuration)</h3><ul><li><p>In general, components should bring their own monitoring configuration.
Gardener currently does this for some components such as the
<a href=https://github.com/gardener/gardener/blob/eec37223cb90475ec3e023136a7d5ba28ad48f0d/pkg/operation/botanist/component/resourcemanager/monitoring.go>gardener-resource-manager</a>. This configuration is then appended to the
existing Prometheus configuration. The goal is to replace the inline
<code>yaml</code> with <code>PodMonitors</code> and/or <code>ServiceMonitors</code> instead.</p></li><li><p>If alerting rules or recording rules need to be created for a component,
it can bring its own <code>PrometheusRules</code>.</p></li><li><p>The same thing can potentially be done for components such as
kube-state-metrics which are still currently deployed via the
[seed-bootstrap].</p></li><li><p>If an extension requires monitoring it must bring its own configuration in
the form of <code>PodMonitors</code>, <code>ServiceMonitors</code> or <code>PrometheusRules</code>.</p><ul><li><p>Adding monitoring config to the control plane: In some scenarios
extensions will add components to the controlplane that need to be
monitored. An example of this is the provider-aws extension that
deploys a <code>cloud-controller-manager</code>. In the current setup, if an
extension needs something to be monitored in the control plane, it
brings its own configmap with Prometheus config. The configmap has the
label <code>extensions.gardener.cloud/configuration=monitoring</code> to specify
that the config should be appended to the current Prometheus config.
Below is an example of what this looks like for the cloud controller
manager.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    extensions.gardener.cloud/configuration: monitoring
</span></span><span style=display:flex><span>  name: cloud-controller-manager-observability-config
</span></span><span style=display:flex><span>  namespace: shoot--project--name
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  alerting_rules: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    cloud-controller-manager.rules.yaml: |
</span></span></span><span style=display:flex><span><span style=color:#a31515>    groups:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    - name: cloud-controller-manager.rules
</span></span></span><span style=display:flex><span><span style=color:#a31515>      rules:
</span></span></span><span style=display:flex><span><span style=color:#a31515>      - alert: CloudControllerManagerDown
</span></span></span><span style=display:flex><span><span style=color:#a31515>      expr: absent(up{job=&#34;cloud-controller-manager&#34;} == 1)
</span></span></span><span style=display:flex><span><span style=color:#a31515>      for: 15m
</span></span></span><span style=display:flex><span><span style=color:#a31515>      labels:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        service: cloud-controller-manager
</span></span></span><span style=display:flex><span><span style=color:#a31515>        severity: critical
</span></span></span><span style=display:flex><span><span style=color:#a31515>        type: seed
</span></span></span><span style=display:flex><span><span style=color:#a31515>        visibility: all
</span></span></span><span style=display:flex><span><span style=color:#a31515>      annotations:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        description: All infrastruture specific operations cannot be completed (e.g. creating loadbalancers or persistent volumes).
</span></span></span><span style=display:flex><span><span style=color:#a31515>        summary: Cloud controller manager is down.</span>    
</span></span><span style=display:flex><span>  observedComponents: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    observedPods:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    - podPrefix: cloud-controller-manager
</span></span></span><span style=display:flex><span><span style=color:#a31515>    isExposedToUser: true</span>    
</span></span><span style=display:flex><span>  scrape_config: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    - job_name: cloud-controller-manager
</span></span></span><span style=display:flex><span><span style=color:#a31515>      scheme: https
</span></span></span><span style=display:flex><span><span style=color:#a31515>      tls_config:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        insecure_skip_verify: true
</span></span></span><span style=display:flex><span><span style=color:#a31515>      authorization:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        type: Bearer
</span></span></span><span style=display:flex><span><span style=color:#a31515>        credentials_file: /var/run/secrets/gardener.cloud/shoot/token/token
</span></span></span><span style=display:flex><span><span style=color:#a31515>      honor_labels: false
</span></span></span><span style=display:flex><span><span style=color:#a31515>      kubernetes_sd_configs:
</span></span></span><span style=display:flex><span><span style=color:#a31515>      - role: endpoints
</span></span></span><span style=display:flex><span><span style=color:#a31515>        namespaces:
</span></span></span><span style=display:flex><span><span style=color:#a31515>          names: [shoot--project--name]
</span></span></span><span style=display:flex><span><span style=color:#a31515>      relabel_configs:
</span></span></span><span style=display:flex><span><span style=color:#a31515>      - source_labels:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        - __meta_kubernetes_service_name
</span></span></span><span style=display:flex><span><span style=color:#a31515>        - __meta_kubernetes_endpoint_port_name
</span></span></span><span style=display:flex><span><span style=color:#a31515>        action: keep
</span></span></span><span style=display:flex><span><span style=color:#a31515>        regex: cloud-controller-manager;metrics
</span></span></span><span style=display:flex><span><span style=color:#a31515>      # common metrics
</span></span></span><span style=display:flex><span><span style=color:#a31515>      - action: labelmap
</span></span></span><span style=display:flex><span><span style=color:#a31515>          regex: __meta_kubernetes_service_label_(.+)
</span></span></span><span style=display:flex><span><span style=color:#a31515>      - source_labels: [ __meta_kubernetes_pod_name ]
</span></span></span><span style=display:flex><span><span style=color:#a31515>          target_label: pod
</span></span></span><span style=display:flex><span><span style=color:#a31515>      metric_relabel_configs:
</span></span></span><span style=display:flex><span><span style=color:#a31515>      - source_labels: [ __name__ ]
</span></span></span><span style=display:flex><span><span style=color:#a31515>        regex: ^(rest_client_requests_total|process_max_fds|process_open_fds)$
</span></span></span><span style=display:flex><span><span style=color:#a31515>        action: keep</span>    
</span></span></code></pre></div></li></ul></li><li><p>This configmap will be split up into 2 separate resources. One for the
<code>alerting_rules</code> and another for the <code>scrape_config</code>. The <code>alerting_rules</code>
should be converted into a <code>PrometheusRules</code> object. Since the
<code>scrape_config</code> only has one <code>job_name</code> we will only need one
<code>ServiceMonitor</code> or <code>PodMonitor</code> for this. The following is an example of
how this could be done. There are multiple ways to get the same results
and this is just one example.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: monitoring.coreos.com/v1
</span></span><span style=display:flex><span>kind: ServiceMonitor
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    cluster: shoot--project--name
</span></span><span style=display:flex><span>  name: cloud-controller-manager
</span></span><span style=display:flex><span>  namespace: shoot--project--name
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  endpoints:
</span></span><span style=display:flex><span>  - port: metrics <span style=color:green># scrape the service port with name `metrics`</span>
</span></span><span style=display:flex><span>    bearerTokenFile: /var/run/secrets/gardener.cloud/shoot/token/token <span style=color:green># could also be replaced with a secret</span>
</span></span><span style=display:flex><span>    metricRelabelings:
</span></span><span style=display:flex><span>    - sourceLabels: [ __name__ ]
</span></span><span style=display:flex><span>      regex: ^(rest_client_requests_total|process_max_fds|process_open_fds)$
</span></span><span style=display:flex><span>      action: keep
</span></span><span style=display:flex><span>  namespaceSelector:
</span></span><span style=display:flex><span>    matchNames:
</span></span><span style=display:flex><span>    - shoot--project--name
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      role: cloud-controller-manager <span style=color:green># discover any service with this label</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: monitoring.coreos.com/v1
</span></span><span style=display:flex><span>kind: PrometheusRule
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    cluster: shoot--project--name
</span></span><span style=display:flex><span>  name: cloud-controller-manager-rules
</span></span><span style=display:flex><span>  namespace: shoot--project--name
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  groups:
</span></span><span style=display:flex><span>  - name: cloud-controller-manager.rules
</span></span><span style=display:flex><span>    rules:
</span></span><span style=display:flex><span>    - alert: CloudControllerManagerDown
</span></span><span style=display:flex><span>      expr: absent(up{job=&#34;cloud-controller-manager&#34;} == 1)
</span></span><span style=display:flex><span>      for: 15m
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        service: cloud-controller-manager
</span></span><span style=display:flex><span>        severity: critical
</span></span><span style=display:flex><span>        type: seed
</span></span><span style=display:flex><span>        visibility: all
</span></span><span style=display:flex><span>      annotations:
</span></span><span style=display:flex><span>        description: All infrastruture specific operations cannot be completed (e.g. creating loadbalancers or persistent volumes).
</span></span><span style=display:flex><span>        summary: Cloud controller manager is down.
</span></span></code></pre></div></li><li><p>Adding meta monitoring for the extensions: If the extensions need to be
scraped for monitoring, the extensions must bring their own <a href=https://github.com/prometheus-operator/prometheus-operator#customresourcedefinitions>Custom
Resources</a>.</p><ul><li>Currently the contract between extensions and gardener is that
anything that needs to be scraped must have the labels:
<code>prometheus.io/scrape=true</code> and <code>prometheus.io/port=&lt;port></code>. This is
defined <a href=https://github.com/gardener/gardener/blob/201673c1f8a356a63b21505ca9c7f6efe725bd48/charts/seed-bootstrap/charts/monitoring/templates/config.yaml#L14-L36>here</a>. This is something that we can still
support with a <code>PodMonitor</code> that will scrape any pod in a specified
namespace with these labels.</li></ul></li></ul><h3 id=grafana-sidecar>Grafana Sidecar</h3><p>Add a <a href=https://github.com/kiwigrid/k8s-sidecar>sidecar</a> to Grafana that will pickup dashboards and provision them. Each dashboard gets its own configmap.</p><ul><li><p>Grafana in the control plane</p><ul><li><p>Most dashboards provisioned by Grafana are the same for each shoot
cluster. To avoid unnecessary duplication of configmaps, the dashboards
could be added once in a single namespace. These &ldquo;common&rdquo; dashboards can
then be discovered by each Grafana and provisioned.</p></li><li><p>In some cases, dashboards are more &ldquo;specific&rdquo; because they are related to
a certain Kubernetes version.</p></li><li><p>Contract between dashboards in configmaps and the Grafana sidecar.</p><ul><li><p>Label schema: <code>monitoring.gardener.cloud/dashboard-{seed,shoot,shoot-user}=true</code></p></li><li><p>Each common dashboard will be deployed in the <code>monitoring</code> namespace
as a configmap. If the dashboard should be provisioned by the user
Grafana in a shoot cluster it should have the label
<code>monitoring.gardener.cloud/dashboard-shoot-user=true</code>. For dashboards
that should be provisioned by the operator Grafana the label
<code>monitoring.gardener.cloud/dashboard-shoot=true</code> is required.</p></li><li><p>Each specific dashboard will be deployed in the shoot namespace. The
configmap will use the same label scheme.</p></li><li><p>The Grafana <a href=https://github.com/kiwigrid/k8s-sidecar>sidecar</a> must be <a href=https://github.com/kiwigrid/k8s-sidecar#configuration-environment-variables>configured</a> with:</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  env:
</span></span><span style=display:flex><span>  - name: METHOD
</span></span><span style=display:flex><span>    value: WATCH
</span></span><span style=display:flex><span>  - name: LABEL
</span></span><span style=display:flex><span>    value: monitoring.gardener.cloud/dashboard-shoot <span style=color:green># monitoring.gardener.cloud/dashboard-shoot-user for user Grafana</span>
</span></span><span style=display:flex><span>  - name: FOLDER
</span></span><span style=display:flex><span>    value: /tmp/dashboards
</span></span><span style=display:flex><span>  - name: NAMESPACE
</span></span><span style=display:flex><span>    value: monitoring,&lt;shoot namespace&gt;
</span></span></code></pre></div></li></ul></li><li><p>Grafana in the seed</p><ul><li><p>There is also a Grafana deployed in the seed. This Grafana will be
configured in a very similar way, except it will discover dashboards
with a different label.</p></li><li><p>The seed Grafana can discover configmaps labeled with
<code>monitoring.gardener.cloud/dashboard-seed</code>.</p></li><li><p>The sidecar will be configured in a similar way:</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  env:
</span></span><span style=display:flex><span>  - name: METHOD
</span></span><span style=display:flex><span>    value: WATCH
</span></span><span style=display:flex><span>  - name: LABEL
</span></span><span style=display:flex><span>    value: monitoring.gardener.cloud/dashboard-seed
</span></span><span style=display:flex><span>  - name: FOLDER
</span></span><span style=display:flex><span>    value: /tmp/dashboards
</span></span><span style=display:flex><span>  - name: NAMESPACE
</span></span><span style=display:flex><span>    value: monitoring,garden
</span></span></code></pre></div></li><li><p>Dashboards can have multiple labels and be provisioned in a seed and/or shoot Grafana.</p></li></ul><h3 id=migration>Migration</h3><ol><li>Deploy the <a href=https://github.com/prometheus-operator/prometheus-operator>prometheus-operator</a> and its custom resources.</li><li>Delete the old monitoring-stack.</li><li>Configure <code>Prometheus</code> to &ldquo;reuse&rdquo; the <code>pv</code> from the old Prometheus&rsquo;s
<code>pvc</code>. An init container will be temporarily needed for this migration.
This ensures that no data is lost and provides a clean migration.</li><li>Any extension or monitoring configuration that is not migrated to the <a href=https://github.com/prometheus-operator/prometheus-operator>prometheus-operator</a> right away will be collected and added to an <code>additionalScrapeConfig</code>. Once all extensions and components have migrated, this can be dropped.</li></ol><h2 id=alternatives>Alternatives</h2><ol><li>Continue using the current setup.</li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-6972c06e5d67dfd5b9bac676e2a230c2>17 - New Core Gardener Cloud APIs</h1><h1 id=new-coregardenercloudv1beta1-apis-required-to-extract-cloud-specificos-specific-knowledge-out-of-gardener-core>New <code>core.gardener.cloud/v1beta1</code> APIs required to extract cloud-specific/OS-specific knowledge out of Gardener core</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#summary>Summary</a></li><li><a href=#motivation>Motivation</a><ul><li><a href=#goals>Goals</a></li><li><a href=#non-goals>Non-Goals</a></li></ul></li><li><a href=#proposal>Proposal</a><ul><li><a href=#cloudprofile-resource><code>CloudProfile</code> resource</a></li><li><a href=#seed-resource><code>Seed</code> resource</a></li><li><a href=#project-resource><code>Project</code> resource</a></li><li><a href=#secretbinding-resource><code>SecretBinding</code> resource</a></li><li><a href=#quota-resource><code>Quota</code> resource</a></li><li><a href=#backupbucket-resource><code>BackupBucket</code> resource</a></li><li><a href=#backupentry-resource><code>BackupEntry</code> resource</a></li><li><a href=#shoot-resource><code>Shoot</code> resource</a></li><li><a href=#plant-resource><code>Plant</code> resource</a></li></ul></li></ul><h2 id=summary>Summary</h2><p>In <a href=/docs/gardener/proposals/01-extensibility/>GEP-1</a> we have proposed how to (re-)design Gardener to allow providers maintaining their provider-specific knowledge out of the core tree.
Meanwhile, we have progressed a lot and are about to remove the <a href=https://github.com/gardener/gardener/blob/de75a5bfcbedd16ba341ace0eb58be2a87049dcb/pkg/operation/cloudbotanist/types.go><code>CloudBotanist</code> interface</a> entirely.
The only missing aspect that will allow providers to really maintain their code out of the core is to design new APIs.</p><p>This proposal describes how the new <code>Shoot</code>, <code>Seed</code> etc. APIs will be re-designed to cope with the changes made with extensibility.
We already have the new <code>core.gardener.cloud/v1beta1</code> API group that will be the new default soon.</p><h2 id=motivation>Motivation</h2><p>We want to allow providers to individually maintain their specific knowledge without the necessity to touch the Gardener core code.
In order to achieve the same, we have to provide proper APIs.</p><h3 id=goals>Goals</h3><ul><li>Provide proper APIs to allow providers maintaining their code outside of the core codebase.</li><li>Do not complicate the APIs for end-users such that they can easily create, delete, and maintain shoot clusters.</li></ul><h3 id=non-goals>Non-Goals</h3><ul><li>Let&rsquo;s try to not split everything up into too many different resources. Instead, let&rsquo;s try to keep all relevant information in the same resources when possible/appropriate.</li></ul><h2 id=proposal>Proposal</h2><p>In GEP-1 we already have proposed a first version for new <code>CloudProfile</code> and <code>Shoot</code> resources.
In order to deprecate the existing/old <code>garden.sapcloud.io/v1beta1</code> API group (and remove it, eventually) we should move all existing resources to the new <code>core.gardener.cloud/v1beta1</code> API group.</p><h3 id=cloudprofile-resource><code>CloudProfile</code> resource</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cloudprofile1
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: &lt;some-provider-name&gt; <span style=color:green># {aws,azure,gcp,...}</span>
</span></span><span style=display:flex><span><span style=color:green># Optional list of labels on `Seed` resources that marks those seeds whose shoots may use this provider profile.</span>
</span></span><span style=display:flex><span><span style=color:green># An empty list means that all seeds of the same provider type are supported.</span>
</span></span><span style=display:flex><span><span style=color:green># This is useful for environments that are of the same type (like openstack) but may have different &#34;instances&#34;/landscapes.</span>
</span></span><span style=display:flex><span><span style=color:green># seedSelector:</span>
</span></span><span style=display:flex><span><span style=color:green>#   matchLabels:</span>
</span></span><span style=display:flex><span><span style=color:green>#     foo: bar</span>
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.12.1
</span></span><span style=display:flex><span>    - version: 1.11.0
</span></span><span style=display:flex><span>    - version: 1.10.6
</span></span><span style=display:flex><span>    - version: 1.10.5
</span></span><span style=display:flex><span>      expirationDate: 2020-04-05T01:02:03Z <span style=color:green># optional</span>
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: coreos
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 2023.5.0
</span></span><span style=display:flex><span>    - version: 1967.5.0
</span></span><span style=display:flex><span>      expirationDate: 2020-04-05T08:00:00Z
</span></span><span style=display:flex><span>  - name: ubuntu
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 18.04.201906170
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: m5.large
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 8Gi
</span></span><span style=display:flex><span>  <span style=color:green># storage: 20Gi # optional (not needed in every environment, may only be specified if no volumeTypes have been specified)</span>
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  volumeTypes: <span style=color:green># optional (not needed in every environment, may only be specified if no machineType has a `storage` field)</span>
</span></span><span style=display:flex><span>  - name: gp2
</span></span><span style=display:flex><span>    class: standard
</span></span><span style=display:flex><span>  - name: io1
</span></span><span style=display:flex><span>    class: premium
</span></span><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - name: europe-central-1
</span></span><span style=display:flex><span>    zones: <span style=color:green># optional (not needed in every environment)</span>
</span></span><span style=display:flex><span>    - name: europe-central-1a
</span></span><span style=display:flex><span>    - name: europe-central-1b
</span></span><span style=display:flex><span>    - name: europe-central-1c
</span></span><span style=display:flex><span>    <span style=color:green># unavailableMachineTypes: # optional, list of machine types defined above that are not available in this zone</span>
</span></span><span style=display:flex><span>    <span style=color:green># - m5.large</span>
</span></span><span style=display:flex><span>    <span style=color:green># unavailableVolumeTypes: # optional, list of volume types defined above that are not available in this zone</span>
</span></span><span style=display:flex><span>    <span style=color:green># - io1</span>
</span></span><span style=display:flex><span><span style=color:green># CA bundle that will be installed onto every shoot machine that is using this provider profile.</span>
</span></span><span style=display:flex><span><span style=color:green># caBundle: |</span>
</span></span><span style=display:flex><span><span style=color:green>#   -----BEGIN CERTIFICATE-----</span>
</span></span><span style=display:flex><span><span style=color:green>#   ...</span>
</span></span><span style=display:flex><span><span style=color:green>#   -----END CERTIFICATE-----</span>
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    &lt;some-provider-specific-cloudprofile-config&gt;
</span></span><span style=display:flex><span>    <span style=color:green># We don&#39;t have concrete examples for every existing provider yet, but these are the proposals:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># Example for Alicloud:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1</span>
</span></span><span style=display:flex><span>    <span style=color:green># kind: CloudProfileConfig</span>
</span></span><span style=display:flex><span>    <span style=color:green># machineImages:</span>
</span></span><span style=display:flex><span>    <span style=color:green># - name: coreos</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   version: 2023.5.0</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   id: coreos_2023_4_0_64_30G_alibase_20190319.vhd</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># Example for AWS:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1</span>
</span></span><span style=display:flex><span>    <span style=color:green># kind: CloudProfileConfig</span>
</span></span><span style=display:flex><span>    <span style=color:green># machineImages:</span>
</span></span><span style=display:flex><span>    <span style=color:green># - name: coreos</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   version: 1967.5.0</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   regions:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   - name: europe-central-1</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     ami: ami-0f46c2ed46d8157aa</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># Example for Azure:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1</span>
</span></span><span style=display:flex><span>    <span style=color:green># kind: CloudProfileConfig</span>
</span></span><span style=display:flex><span>    <span style=color:green># machineImages:</span>
</span></span><span style=display:flex><span>    <span style=color:green># - name: coreos</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   version: 1967.5.0</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   publisher: CoreOS</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   offer: CoreOS</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   sku: Stable</span>
</span></span><span style=display:flex><span>    <span style=color:green># countFaultDomains:</span>
</span></span><span style=display:flex><span>    <span style=color:green># - region: westeurope</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   count: 2</span>
</span></span><span style=display:flex><span>    <span style=color:green># countUpdateDomains:</span>
</span></span><span style=display:flex><span>    <span style=color:green># - region: westeurope</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   count: 5</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># Example for GCP:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1</span>
</span></span><span style=display:flex><span>    <span style=color:green># kind: CloudProfileConfig</span>
</span></span><span style=display:flex><span>    <span style=color:green># machineImages:</span>
</span></span><span style=display:flex><span>    <span style=color:green># - name: coreos</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   version: 2023.5.0</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   image: projects/coreos-cloud/global/images/coreos-stable-2023-5-0-v20190312</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># Example for OpenStack:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1</span>
</span></span><span style=display:flex><span>    <span style=color:green># kind: CloudProfileConfig</span>
</span></span><span style=display:flex><span>    <span style=color:green># machineImages:</span>
</span></span><span style=display:flex><span>    <span style=color:green># - name: coreos</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   version: 2023.5.0</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   image: coreos-2023.5.0</span>
</span></span><span style=display:flex><span>    <span style=color:green># keyStoneURL: https://url-to-keystone/v3/</span>
</span></span><span style=display:flex><span>    <span style=color:green># dnsServers:</span>
</span></span><span style=display:flex><span>    <span style=color:green># - 10.10.10.10</span>
</span></span><span style=display:flex><span>    <span style=color:green># - 10.10.10.11</span>
</span></span><span style=display:flex><span>    <span style=color:green># dhcpDomain: foo.bar</span>
</span></span><span style=display:flex><span>    <span style=color:green># requestTimeout: 30s</span>
</span></span><span style=display:flex><span>    <span style=color:green># constraints:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   loadBalancerProviders:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   - name: haproxy</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   floatingPools:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   - name: fip1</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     loadBalancerClasses:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     - name: class1</span>
</span></span><span style=display:flex><span>    <span style=color:green>#       floatingSubnetID: 04eed401-f85f-4610-8041-c4835c4beea6</span>
</span></span><span style=display:flex><span>    <span style=color:green>#       floatingNetworkID: 23949a30-1cdd-4732-ba47-d03ced950acc</span>
</span></span><span style=display:flex><span>    <span style=color:green>#       subnetID: ac46c204-9d0d-4a4c-a90d-afefe40cfc35</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># Example for Packet:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># apiVersion: packet.provider.extensions.gardener.cloud/v1alpha1</span>
</span></span><span style=display:flex><span>    <span style=color:green># kind: CloudProfileConfig</span>
</span></span><span style=display:flex><span>    <span style=color:green># machineImages:</span>
</span></span><span style=display:flex><span>    <span style=color:green># - name: coreos</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   version: 2079.3.0</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   id: d61c3912-8422-4daf-835e-854efa0062e4</span>
</span></span></code></pre></div><h3 id=seed-resource><code>Seed</code> resource</h3><p>Special note: The proposal contains fields that are not yet existing in the current <code>garden.sapcloud.io/v1beta1.Seed</code> resource, but they should be implemented (open issues that require them are linked).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: seed-secret
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  kubeconfig: base64(kubeconfig-for-seed-cluster)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: backup-secret
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  <span style=color:green># &lt;some-provider-specific data keys&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-backupbucket.yaml#L9-L11</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-infrastructure.yaml#L9-L10</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-backupbucket.yaml#L9-L10</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-backupbucket.yaml#L9</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-backupbucket.yaml#L9-L13</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Seed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: seed1
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: &lt;some-provider-name&gt; <span style=color:green># {aws,azure,gcp,...}</span>
</span></span><span style=display:flex><span>    region: europe-central-1
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: seed-secret
</span></span><span style=display:flex><span>    namespace: garden
</span></span><span style=display:flex><span>  <span style=color:green># Motivation for DNS section: https://github.com/gardener/gardener/issues/201.</span>
</span></span><span style=display:flex><span>  dns:
</span></span><span style=display:flex><span>    provider: &lt;some-provider-name&gt; <span style=color:green># {aws-route53, google-clouddns, ...}</span>
</span></span><span style=display:flex><span>    secretName: my-dns-secret <span style=color:green># must be in `garden` namespace</span>
</span></span><span style=display:flex><span>    ingressDomain: seed1.dev.example.com
</span></span><span style=display:flex><span>  volume: <span style=color:green># optional (introduced to get rid of `persistentvolume.garden.sapcloud.io/minimumSize` and `persistentvolume.garden.sapcloud.io/provider` annotations)</span>
</span></span><span style=display:flex><span>    minimumSize: 20Gi
</span></span><span style=display:flex><span>    providers:
</span></span><span style=display:flex><span>    - name: foo
</span></span><span style=display:flex><span>      purpose: etcd-main
</span></span><span style=display:flex><span>  networks: <span style=color:green># Seed and Shoot networks must be disjunct</span>
</span></span><span style=display:flex><span>    nodes: 10.240.0.0/16
</span></span><span style=display:flex><span>    pods: 10.241.128.0/17
</span></span><span style=display:flex><span>    services: 10.241.0.0/17
</span></span><span style=display:flex><span>  <span style=color:green># Shoot default networks, see also https://github.com/gardener/gardener/issues/895.</span>
</span></span><span style=display:flex><span>  <span style=color:green># shootDefaults:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   pods: 100.96.0.0/11</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   services: 100.64.0.0/13</span>
</span></span><span style=display:flex><span>  taints:
</span></span><span style=display:flex><span>  - key: seed.gardener.cloud/protected
</span></span><span style=display:flex><span>  - key: seed.gardener.cloud/invisible
</span></span><span style=display:flex><span>  blockCIDRs:
</span></span><span style=display:flex><span>  - 169.254.169.254/32
</span></span><span style=display:flex><span>  backup: <span style=color:green># See https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md.</span>
</span></span><span style=display:flex><span>    type: &lt;some-provider-name&gt; <span style=color:green># {aws,azure,gcp,...}</span>
</span></span><span style=display:flex><span>  <span style=color:green># region: eu-west-1</span>
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: backup-secret
</span></span><span style=display:flex><span>      namespace: garden
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - lastTransitionTime: <span style=color:#a31515>&#34;2020-07-14T19:16:42Z&#34;</span>
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#34;2020-07-14T19:18:17Z&#34;</span>
</span></span><span style=display:flex><span>    message: all checks passed
</span></span><span style=display:flex><span>    reason: Passed
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>    type: Available
</span></span><span style=display:flex><span>  gardener:
</span></span><span style=display:flex><span>    id: 4c9832b3823ee6784064877d3eb10c189fc26e98a1286c0d8a5bc82169ed702c
</span></span><span style=display:flex><span>    name: gardener-controller-manager-7fhn9ikan73n-7jhka
</span></span><span style=display:flex><span>    version: 1.0.0
</span></span><span style=display:flex><span>  observedGeneration: 1
</span></span></code></pre></div><h3 id=project-resource><code>Project</code> resource</h3><p>Special note: The <code>members</code> and <code>viewers</code> field of the <code>garden.sapcloud.io/v1beta1.Project</code> resource will be merged together into one <code>members</code> field.
Every member will have a role that is either <code>admin</code> or <code>viewer</code>.
This will allow us to add new roles without changing the API.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Project
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: example
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  description: Example project
</span></span><span style=display:flex><span>  members:
</span></span><span style=display:flex><span>  - apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>    kind: User
</span></span><span style=display:flex><span>    name: john.doe@example.com
</span></span><span style=display:flex><span>    role: admin
</span></span><span style=display:flex><span>  - apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>    kind: User
</span></span><span style=display:flex><span>    name: joe.doe@example.com
</span></span><span style=display:flex><span>    role: viewer
</span></span><span style=display:flex><span>  namespace: garden-example
</span></span><span style=display:flex><span>  owner:
</span></span><span style=display:flex><span>    apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>    kind: User
</span></span><span style=display:flex><span>    name: john.doe@example.com
</span></span><span style=display:flex><span>  purpose: Example project
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  observedGeneration: 1
</span></span><span style=display:flex><span>  phase: Ready
</span></span></code></pre></div><h3 id=secretbinding-resource><code>SecretBinding</code> resource</h3><p>Special note: No modifications needed compared to the current <code>garden.sapcloud.io/v1beta1.SecretBinding</code> resource.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: secret1
</span></span><span style=display:flex><span>  namespace: garden-core
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  <span style=color:green># &lt;some-provider-specific data keys&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-infrastructure.yaml#L14-L15</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-infrastructure.yaml#L9-L10</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-infrastructure.yaml#L14-L17</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-infrastructure.yaml#L14</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-infrastructure.yaml#L15-L18</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-packet/blob/master/example/30-infrastructure.yaml#L14-L15</span>
</span></span><span style=display:flex><span>  <span style=color:green>#</span>
</span></span><span style=display:flex><span>  <span style=color:green># If you use your own domain (not the default domain of your landscape) then you have to add additional keys to this secret.</span>
</span></span><span style=display:flex><span>  <span style=color:green># The reason is that the DNS management is not part of the Gardener core code base but externalized, hence, it might use other</span>
</span></span><span style=display:flex><span>  <span style=color:green># key names than Gardener itself.</span>
</span></span><span style=display:flex><span>  <span style=color:green># The actual values here depend on the DNS extension that is installed to your landscape.</span>
</span></span><span style=display:flex><span>  <span style=color:green># For example, check out https://github.com/gardener/external-dns-management and find a lot of example secret manifests here:</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/external-dns-management/tree/master/examples</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: SecretBinding
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: secretbinding1
</span></span><span style=display:flex><span>  namespace: garden-core
</span></span><span style=display:flex><span>secretRef:
</span></span><span style=display:flex><span>  name: secret1
</span></span><span style=display:flex><span><span style=color:green># namespace: namespace-other-than-&#39;garden-core&#39; // optional</span>
</span></span><span style=display:flex><span>quotas: []
</span></span><span style=display:flex><span><span style=color:green># - name: quota-1</span>
</span></span><span style=display:flex><span><span style=color:green># # namespace: namespace-other-than-&#39;garden-core&#39; // optional</span>
</span></span></code></pre></div><h3 id=quota-resource><code>Quota</code> resource</h3><p>Special note: No modifications needed compared to the current <code>garden.sapcloud.io/v1beta1.Quota</code> resource.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Quota
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: trial-quota
</span></span><span style=display:flex><span>  namespace: garden-trial
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  scope:
</span></span><span style=display:flex><span>    apiGroup: core.gardener.cloud
</span></span><span style=display:flex><span>    kind: Project
</span></span><span style=display:flex><span><span style=color:green># clusterLifetimeDays: 14</span>
</span></span><span style=display:flex><span>  metrics:
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;200&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;20&#34;</span>
</span></span><span style=display:flex><span>    memory: 4000Gi
</span></span><span style=display:flex><span>    storage.standard: 8000Gi
</span></span><span style=display:flex><span>    storage.premium: 2000Gi
</span></span><span style=display:flex><span>    loadbalancer: <span style=color:#a31515>&#34;100&#34;</span>
</span></span></code></pre></div><h3 id=backupbucket-resource><code>BackupBucket</code> resource</h3><p>Special note: This new resource is cluster-scoped.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green># See also: https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: backup-operator-provider
</span></span><span style=display:flex><span>  namespace: backup-garden
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  <span style=color:green># &lt;some-provider-specific data keys&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-backupbucket.yaml#L9-L11</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-backupbucket.yaml#L9-L10</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-backupbucket.yaml#L9-L10</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-backupbucket.yaml#L9</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-backupbucket.yaml#L9-L13</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: BackupBucket
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: &lt;seed-provider-type&gt;-&lt;region&gt;-&lt;seed-uid&gt;
</span></span><span style=display:flex><span>  ownerReferences:
</span></span><span style=display:flex><span>  - kind: Seed
</span></span><span style=display:flex><span>    name: seed1
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: &lt;some-provider-name&gt; <span style=color:green># {aws,azure,gcp,...}</span>
</span></span><span style=display:flex><span>    region: europe-central-1
</span></span><span style=display:flex><span>  seed: seed1
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: backup-operator-provider
</span></span><span style=display:flex><span>    namespace: backup-garden
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  lastOperation:
</span></span><span style=display:flex><span>    description: Backup bucket has been successfully reconciled.
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#39;2020-04-13T14:34:27Z&#39;</span>
</span></span><span style=display:flex><span>    progress: 100
</span></span><span style=display:flex><span>    state: Succeeded
</span></span><span style=display:flex><span>    type: Reconcile
</span></span><span style=display:flex><span>  observedGeneration: 1
</span></span></code></pre></div><h3 id=backupentry-resource><code>BackupEntry</code> resource</h3><p>Special note: This new resource is cluster-scoped.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green># See also: https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: backup-operator-provider
</span></span><span style=display:flex><span>  namespace: backup-garden
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  <span style=color:green># &lt;some-provider-specific data keys&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-backupbucket.yaml#L9-L11</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-backupbucket.yaml#L9-L10</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-backupbucket.yaml#L9-L10</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-backupbucket.yaml#L9</span>
</span></span><span style=display:flex><span>  <span style=color:green># https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-backupbucket.yaml#L9-L13</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: BackupEntry
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: shoot--core--crazy-botany--3ef42
</span></span><span style=display:flex><span>  namespace: garden-core
</span></span><span style=display:flex><span>  ownerReferences:
</span></span><span style=display:flex><span>  - apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>    blockOwnerDeletion: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>    controller: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    kind: Shoot
</span></span><span style=display:flex><span>    name: crazy-botany
</span></span><span style=display:flex><span>    uid: 19a9538b-5058-11e9-b5a6-5e696cab3bc8
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  bucketName: cloudprofile1-random[:5]
</span></span><span style=display:flex><span>  seed: seed1
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  lastOperation:
</span></span><span style=display:flex><span>    description: Backup entry has been successfully reconciled.
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#39;2020-04-13T14:34:27Z&#39;</span>
</span></span><span style=display:flex><span>    progress: 100
</span></span><span style=display:flex><span>    state: Succeeded
</span></span><span style=display:flex><span>    type: Reconcile
</span></span><span style=display:flex><span>  observedGeneration: 1
</span></span></code></pre></div><h3 id=shoot-resource><code>Shoot</code> resource</h3><p>Special notes:</p><ul><li><code>kubelet</code> configuration in the worker pools may override the default <code>.spec.kubernetes.kubelet</code> configuration (that applies for all worker pools if not overridden).</li><li>Moved remaining control plane configuration to new <code>.spec.provider.controlplane</code> section.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: crazy-botany
</span></span><span style=display:flex><span>  namespace: garden-core
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  secretBindingName: secretbinding1
</span></span><span style=display:flex><span>  cloudProfileName: cloudprofile1
</span></span><span style=display:flex><span>  region: europe-central-1
</span></span><span style=display:flex><span><span style=color:green># seedName: seed1</span>
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: &lt;some-provider-name&gt; <span style=color:green># {aws,azure,gcp,...}</span>
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      &lt;some-provider-specific-infrastructure-config&gt;
</span></span><span style=display:flex><span>      <span style=color:green># https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-infrastructure.yaml#L56-L64</span>
</span></span><span style=display:flex><span>      <span style=color:green># https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-infrastructure.yaml#L43-L53</span>
</span></span><span style=display:flex><span>      <span style=color:green># https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-infrastructure.yaml#L63-L71</span>
</span></span><span style=display:flex><span>      <span style=color:green># https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-infrastructure.yaml#L53-L57</span>
</span></span><span style=display:flex><span>      <span style=color:green># https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-infrastructure.yaml#L56-L64</span>
</span></span><span style=display:flex><span>      <span style=color:green># https://github.com/gardener/gardener-extension-provider-packet/blob/master/example/30-infrastructure.yaml#L48-L49</span>
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      &lt;some-provider-specific-controlplane-config&gt;
</span></span><span style=display:flex><span>      <span style=color:green># https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-controlplane.yaml#L60-L65</span>
</span></span><span style=display:flex><span>      <span style=color:green># https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-controlplane.yaml#L60-L64</span>
</span></span><span style=display:flex><span>      <span style=color:green># https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-controlplane.yaml#L61-L66</span>
</span></span><span style=display:flex><span>      <span style=color:green># https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-controlplane.yaml#L59-L64</span>
</span></span><span style=display:flex><span>      <span style=color:green># https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-controlplane.yaml#L64-L70</span>
</span></span><span style=display:flex><span>      <span style=color:green># https://github.com/gardener/gardener-extension-provider-packet/blob/master/example/30-controlplane.yaml#L60-L61</span>
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: cpu-worker
</span></span><span style=display:flex><span>      minimum: 3
</span></span><span style=display:flex><span>      maximum: 5
</span></span><span style=display:flex><span>    <span style=color:green># maxSurge: 1</span>
</span></span><span style=display:flex><span>    <span style=color:green># maxUnavailable: 0</span>
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: m5.large
</span></span><span style=display:flex><span>        image:
</span></span><span style=display:flex><span>          name: &lt;some-os-name&gt;
</span></span><span style=display:flex><span>          version: &lt;some-os-version&gt;
</span></span><span style=display:flex><span>        <span style=color:green># providerConfig:</span>
</span></span><span style=display:flex><span>        <span style=color:green>#   &lt;some-os-specific-configuration&gt;</span>
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        type: gp2
</span></span><span style=display:flex><span>        size: 20Gi
</span></span><span style=display:flex><span>    <span style=color:green># providerConfig:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   &lt;some-provider-specific-worker-config&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:green># labels:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   key: value</span>
</span></span><span style=display:flex><span>    <span style=color:green># annotations:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   key: value</span>
</span></span><span style=display:flex><span>    <span style=color:green># taints: # See also https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/</span>
</span></span><span style=display:flex><span>    <span style=color:green># - key: foo</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   value: bar</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   effect: NoSchedule</span>
</span></span><span style=display:flex><span>    <span style=color:green># caBundle: &lt;some-ca-bundle-to-be-installed-to-all-nodes-in-this-pool&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:green># kubernetes:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   kubelet:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     cpuCFSQuota: true</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     cpuManagerPolicy: none</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     podPidsLimit: 10</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     featureGates:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#       SomeKubernetesFeature: true</span>
</span></span><span style=display:flex><span>    <span style=color:green># zones: # optional, only relevant if the provider supports availability zones</span>
</span></span><span style=display:flex><span>    <span style=color:green># - europe-central-1a</span>
</span></span><span style=display:flex><span>    <span style=color:green># - europe-central-1b</span>
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.15.1
</span></span><span style=display:flex><span>  <span style=color:green># allowPrivilegedContainers: true # &#39;true&#39; means that all authenticated users can use the &#34;gardener.privileged&#34; PodSecurityPolicy, allowing full unrestricted access to Pod features.</span>
</span></span><span style=display:flex><span>  <span style=color:green># kubeAPIServer:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   featureGates:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     SomeKubernetesFeature: true</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   runtimeConfig:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     scheduling.k8s.io/v1alpha1: true</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   oidcConfig:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     caBundle: |</span>
</span></span><span style=display:flex><span>  <span style=color:green>#       -----BEGIN CERTIFICATE-----</span>
</span></span><span style=display:flex><span>  <span style=color:green>#       Li4u</span>
</span></span><span style=display:flex><span>  <span style=color:green>#       -----END CERTIFICATE-----</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     clientID: client-id</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     groupsClaim: groups-claim</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     groupsPrefix: groups-prefix</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     issuerURL: https://identity.example.com</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     usernameClaim: username-claim</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     usernamePrefix: username-prefix</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     signingAlgs: RS256,some-other-algorithm</span>
</span></span><span style=display:flex><span>  <span style=color:green>#-#-# only usable with Kubernetes &gt;= 1.11</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     requiredClaims:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#       key: value</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   admissionPlugins:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   - name: PodNodeSelector</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     config: |</span>
</span></span><span style=display:flex><span>  <span style=color:green>#       podNodeSelectorPluginConfig:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#         clusterDefaultNodeSelector: &lt;node-selectors-labels&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:green>#         namespace1: &lt;node-selectors-labels&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:green>#         namespace2: &lt;node-selectors-labels&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   auditConfig:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     auditPolicy:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#       configMapRef:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#         name: auditpolicy</span>
</span></span><span style=display:flex><span>  <span style=color:green># kubeControllerManager:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   featureGates:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     SomeKubernetesFeature: true</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   horizontalPodAutoscaler:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     syncPeriod: 30s</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     tolerance: 0.1</span>
</span></span><span style=display:flex><span>  <span style=color:green>#-#-# only usable with Kubernetes &lt; 1.12</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     downscaleDelay: 15m0s</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     upscaleDelay: 1m0s</span>
</span></span><span style=display:flex><span>  <span style=color:green>#-#-# only usable with Kubernetes &gt;= 1.12</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     downscaleStabilization: 5m0s</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     initialReadinessDelay: 30s</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     cpuInitializationPeriod: 5m0s</span>
</span></span><span style=display:flex><span>  <span style=color:green># kubeScheduler:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   featureGates:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     SomeKubernetesFeature: true</span>
</span></span><span style=display:flex><span>  <span style=color:green># kubeProxy:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   featureGates:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     SomeKubernetesFeature: true</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   mode: IPVS</span>
</span></span><span style=display:flex><span>  <span style=color:green># kubelet:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   cpuCFSQuota: true</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   cpuManagerPolicy: none</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   podPidsLimit: 10</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   featureGates:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     SomeKubernetesFeature: true</span>
</span></span><span style=display:flex><span>  <span style=color:green># clusterAutoscaler:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   scaleDownUtilizationThreshold: 0.5</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   scaleDownUnneededTime: 30m</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   scaleDownDelayAfterAdd: 60m</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   scaleDownDelayAfterFailure: 10m</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   scaleDownDelayAfterDelete: 10s</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   scanInterval: 10s</span>
</span></span><span style=display:flex><span>  dns:
</span></span><span style=display:flex><span>    <span style=color:green># When the shoot shall use a cluster domain no domain and no providers need to be provided - Gardener will</span>
</span></span><span style=display:flex><span>    <span style=color:green># automatically compute a correct domain.</span>
</span></span><span style=display:flex><span>    domain: crazy-botany.core.my-custom-domain.com
</span></span><span style=display:flex><span>    providers:
</span></span><span style=display:flex><span>    - type: aws-route53
</span></span><span style=display:flex><span>      secretName: my-custom-domain-secret
</span></span><span style=display:flex><span>      domains:
</span></span><span style=display:flex><span>        include:
</span></span><span style=display:flex><span>        - my-custom-domain.com
</span></span><span style=display:flex><span>        - my-other-custom-domain.com
</span></span><span style=display:flex><span>        exclude:
</span></span><span style=display:flex><span>        - yet-another-custom-domain.com
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>        include:
</span></span><span style=display:flex><span>        - zone-id-1
</span></span><span style=display:flex><span>        exclude:
</span></span><span style=display:flex><span>        - zone-id-2
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: foobar
</span></span><span style=display:flex><span>  <span style=color:green># providerConfig:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   apiVersion: foobar.extensions.gardener.cloud/v1alpha1</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   kind: FooBarConfiguration</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   foo: bar</span>
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>    pods: 100.96.0.0/11
</span></span><span style=display:flex><span>    services: 100.64.0.0/13
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>  <span style=color:green># providerConfig:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   apiVersion: calico.extensions.gardener.cloud/v1alpha1</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   kind: NetworkConfig</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   ipam:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     type: host-local</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     cidr: usePodCIDR</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   backend: bird</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   typha:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     enabled: true</span>
</span></span><span style=display:flex><span>  <span style=color:green># See also: https://github.com/gardener/gardener/blob/master/docs/proposals/03-networking.md</span>
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    timeWindow:
</span></span><span style=display:flex><span>      begin: 220000+0100
</span></span><span style=display:flex><span>      end: 230000+0100
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span><span style=color:green># hibernation:</span>
</span></span><span style=display:flex><span><span style=color:green>#   enabled: false</span>
</span></span><span style=display:flex><span><span style=color:green>#   schedules:</span>
</span></span><span style=display:flex><span><span style=color:green>#   - start: &#34;0 20 * * *&#34; # Start hibernation every day at 8PM</span>
</span></span><span style=display:flex><span><span style=color:green>#     end: &#34;0 6 * * *&#34;    # Stop hibernation every day at 6AM</span>
</span></span><span style=display:flex><span><span style=color:green>#     location: &#34;America/Los_Angeles&#34; # Specify a location for the cron to run in</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    nginx-ingress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>    <span style=color:green># loadBalancerSourceRanges: []</span>
</span></span><span style=display:flex><span>    kubernetes-dashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    <span style=color:green># authenticationMode: basic # allowed values: basic,token</span>
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - type: APIServerAvailable
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#39;True&#39;</span>
</span></span><span style=display:flex><span>    lastTransitionTime: <span style=color:#a31515>&#39;2020-01-30T10:38:15Z&#39;</span>
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#39;2020-04-13T14:35:21Z&#39;</span>
</span></span><span style=display:flex><span>    reason: HealthzRequestFailed
</span></span><span style=display:flex><span>    message: API server /healthz endpoint responded with success status code. [response_time:3ms]
</span></span><span style=display:flex><span>  - type: ControlPlaneHealthy
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#39;True&#39;</span>
</span></span><span style=display:flex><span>    lastTransitionTime: <span style=color:#a31515>&#39;2020-04-02T05:18:58Z&#39;</span>
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#39;2020-04-13T14:35:21Z&#39;</span>
</span></span><span style=display:flex><span>    reason: ControlPlaneRunning
</span></span><span style=display:flex><span>    message: All control plane components are healthy.
</span></span><span style=display:flex><span>  - type: EveryNodeReady
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#39;True&#39;</span>
</span></span><span style=display:flex><span>    lastTransitionTime: <span style=color:#a31515>&#39;2020-04-01T16:27:21Z&#39;</span>
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#39;2020-04-13T14:35:21Z&#39;</span>
</span></span><span style=display:flex><span>    reason: EveryNodeReady
</span></span><span style=display:flex><span>    message: Every node registered to the cluster is ready.
</span></span><span style=display:flex><span>  - type: SystemComponentsHealthy
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#39;True&#39;</span>
</span></span><span style=display:flex><span>    lastTransitionTime: <span style=color:#a31515>&#39;2020-04-03T18:26:28Z&#39;</span>
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#39;2020-04-13T14:35:21Z&#39;</span>
</span></span><span style=display:flex><span>    reason: SystemComponentsRunning
</span></span><span style=display:flex><span>    message: All system components are healthy.
</span></span><span style=display:flex><span>  gardener:
</span></span><span style=display:flex><span>    id: 4c9832b3823ee6784064877d3eb10c189fc26e98a1286c0d8a5bc82169ed702c
</span></span><span style=display:flex><span>    name: gardener-controller-manager-7fhn9ikan73n-7jhka
</span></span><span style=display:flex><span>    version: 1.0.0
</span></span><span style=display:flex><span>  lastOperation:
</span></span><span style=display:flex><span>    description: Shoot cluster state has been successfully reconciled.
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#39;2020-04-13T14:34:27Z&#39;</span>
</span></span><span style=display:flex><span>    progress: 100
</span></span><span style=display:flex><span>    state: Succeeded
</span></span><span style=display:flex><span>    type: Reconcile
</span></span><span style=display:flex><span>  observedGeneration: 1
</span></span><span style=display:flex><span>  seed: seed1
</span></span><span style=display:flex><span>  hibernated: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>  technicalID: shoot--core--crazy-botany
</span></span><span style=display:flex><span>  uid: d8608cfa-2856-11e8-8fdc-0a580af181af
</span></span></code></pre></div><h3 id=plant-resource><code>Plant</code> resource</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: crazy-plant-secret
</span></span><span style=display:flex><span>  namespace: garden-core
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  kubeconfig: base64(kubeconfig-for-plant-cluster)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Plant
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: crazy-plant
</span></span><span style=display:flex><span>  namespace: garden-core
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: crazy-plant-secret
</span></span><span style=display:flex><span>  endpoints:
</span></span><span style=display:flex><span>  - name: Cluster GitHub repository
</span></span><span style=display:flex><span>    purpose: management
</span></span><span style=display:flex><span>    url: https://github.com/my-org/my-cluster-repo
</span></span><span style=display:flex><span>  - name: GKE cluster page
</span></span><span style=display:flex><span>    purpose: management
</span></span><span style=display:flex><span>    url: https://console.cloud.google.com/kubernetes/clusters/details/europe-west1-b/plant?project=my-project&amp;authuser=1&amp;tab=details
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  clusterInfo:
</span></span><span style=display:flex><span>    provider:
</span></span><span style=display:flex><span>      type: gce
</span></span><span style=display:flex><span>      region: europe-west4-c
</span></span><span style=display:flex><span>    kubernetes:
</span></span><span style=display:flex><span>      version: v1.11.10-gke.5
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - lastTransitionTime: <span style=color:#a31515>&#34;2020-03-01T11:31:37Z&#34;</span>
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#34;2020-04-14T18:00:29Z&#34;</span>
</span></span><span style=display:flex><span>    message: API server /healthz endpoint responded with success status code. [response_time:8ms]
</span></span><span style=display:flex><span>    reason: HealthzRequestFailed
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>    type: APIServerAvailable
</span></span><span style=display:flex><span>  - lastTransitionTime: <span style=color:#a31515>&#34;2020-04-01T06:26:56Z&#34;</span>
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#a31515>&#34;2020-04-14T18:00:29Z&#34;</span>
</span></span><span style=display:flex><span>    message: Every node registered to the cluster is ready.
</span></span><span style=display:flex><span>    reason: EveryNodeReady
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>    type: EveryNodeReady
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-d9890d994643115baf0d880ffe41fc8f>18 - Reversed Cluster VPN</h1><h1 id=gep-14-reversed-cluster-vpn>GEP-14: Reversed Cluster VPN</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#Motivation>Motivation</a></li><li><a href=#Proposal>Proposal</a></li><li><a href=#Alternatives>Alternatives</a></li></ul><h2 id=motivation>Motivation</h2><p>It is necessary to describe the current VPN solution and outline its shortcomings in order to motivate this proposal.</p><h3 id=problem-statement>Problem Statement</h3><p>Today&rsquo;s Gardener cluster VPN solution has several issues including:</p><ol><li>Connection establishment is always from the seed cluster to the shoot cluster. This means that there needs to be connectivity both ways which is not desirable in many cases (OpenStack, VMware) and causes high effort in firewall configuration or extra infrastructure. These firewall configurations are prohibited in some cases due to security policies.</li><li>Shoot clusters must provide a VPN endpoint. This means extra cost for the endpoint (roughly €20/month on hyperscalers) or will consume scarce resources (limited number of VMware NSX-T load balancers).</li></ol><p>A first implementation has been provided to resolve the issues with the <a href=/docs/gardener/proposals/11-apiserver-network-proxy/>konnectivity server</a>. As we did find several shortcomings with the underlying technology component, the <a href=https://github.com/kubernetes-sigs/apiserver-network-proxy>apiserver-network-proxy</a> we believe that this is not a suitable way ahead. We have opened an <a href=https://github.com/kubernetes-sigs/apiserver-network-proxy/issues/180>issue</a> and provided two solution proposals to the community. We do see some remedies, e.g. using the <a href=https://de.wikipedia.org/wiki/Quick_UDP_Internet_Connections>Quick Protocol</a> instead of GRPC but we (a) consider the implementation effort significantly higher compared to this proposal and (b) would use an experimental protocol to solve a problem that can also be solved with existing and proven core network technologies.</p><p>We will therefore not continue to invest into this approach. We will however research a similar approach (see below in &ldquo;Further Research&rdquo;).</p><h3 id=current-solution-outline>Current Solution Outline</h3><p>The current solution consists of multiple VPN connections from each API server pod and the Prometheus pod of a control plane to an OpenVPN server running in the shoot cluster. This OpenVPN server is exposed via a load balancer that must have an IP address which is reachable from the seed cluster. The routing in the seed cluster pods is configured to route all traffic for the node, pod, and service ranges to the shoot cluster. This means that there is no address overlap allowed between seed- and shoot cluster node, pod, and service ranges.</p><p>In the seed cluster the <code>vpn-seed</code> container is a sidecar to the kube-apiserver and prometheus pods. OpenVPN acts as a TCP client connecting to an OpenVPN TCP server. This is not optimal (e.g. tunneling TCP over TCP is discouraged) but at the time of development there was no UDP load balancer available on at least one of the major hyperscalers. Connectivity could have been switched to UDP later but the development effort was not spent.</p><p>The solution is depicted in this diagram:</p><p><img src=/__resources/CurrentClusterVPN_095870.png alt="alt text" title="Overview Current Cluster VPN"></p><p>These are the essential parts of the OpenVPN client configuration in the <code>vpn-seed</code> sidecar container:</p><pre tabindex=0><code># use TCP instead of UDP (commonly not supported by load balancers)
proto tcp-client

[...]

# get all routing information from server
pull

tls-client
key &#34;/srv/secrets/vpn-seed/tls.key&#34;
cert &#34;/srv/secrets/vpn-seed/tls.crt&#34;
ca &#34;/srv/secrets/vpn-seed/ca.crt&#34;

tls-auth &#34;/srv/secrets/tlsauth/vpn.tlsauth&#34; 1
cipher AES-256-CBC

# https://openvpn.net/index.php/open-source/documentation/howto.html#mitm
remote-cert-tls server

# pull filter
pull-filter accept &#34;route 100.64.0.0 255.248.0.0&#34;
pull-filter accept &#34;route 100.96.0.0 255.224.0.0&#34;
pull-filter accept &#34;route 10.1.60.0 255.255.252.0&#34;
pull-filter accept &#34;route 192.168.123.&#34;
pull-filter ignore &#34;route&#34;
pull-filter ignore redirect-gateway
pull-filter ignore route-ipv6
pull-filter ignore redirect-gateway-ipv6
</code></pre><p>Encryption is based on SSL certificates with an additional HMAC signature to all SSL/TLS handshake packets. As multiple clients connect to the OpenVPN server in the shoot cluster, all clients must be assigned a unique IP address. This is done by the OpenVPN server pushing that configuration to the client (keyword <code>pull</code>). As this is potentially problematic because the OpenVPN server runs in an untrusted environment there are pull filters denying all but necessary routes for the pod, service, and node networks.</p><p>The OpenVPN server running in the shoot cluster is configured as follows:</p><pre tabindex=0><code>mode server
tls-server
proto tcp4-server
dev tun0

[...]

server 192.168.123.0 255.255.255.0

push &#34;route 10.243.0.0 255.255.128.0&#34;
push &#34;route 10.243.128.0 255.255.128.0&#34;

duplicate-cn

key &#34;/srv/secrets/vpn-shoot/tls.key&#34;
cert &#34;/srv/secrets/vpn-shoot/tls.crt&#34;
ca &#34;/srv/secrets/vpn-shoot/ca.crt&#34;
dh &#34;/srv/secrets/dh/dh2048.pem&#34;

tls-auth &#34;/srv/secrets/tlsauth/vpn.tlsauth&#34; 0
push &#34;route 10.242.0.0 255.255.0.0&#34;
</code></pre><p>It is a TCP TLS server and configured to automatically assign IP addresses for OpenVPN clients (<code>server</code> directive). In addition, it pushes the shoot cluster node-, pod-, and service ranges to the clients running in the seed cluster (<code>push</code> directive).</p><p><strong>Note:</strong> The network mesh spanned by OpenVPN uses the network range <code>192.168.123.0 - 192.168.123.255</code>. This network range cannot be used in either shoot-, or seed clusters. If it is used this might cause subtle problem due to network range overlaps. Unfortunately, this appears not to be well documented but this restriction exists since the very beginning. We should clean up this technical debt as part of the exercise.</p><h3 id=goals>Goals</h3><ul><li>We intend to supersede the current VPN solution with the solution outlined in this proposal.</li><li>We intend to remove the code for the konnectivity tunnel once this solution proposal has been validated.</li></ul><h3 id=non-goals>Non Goals</h3><ul><li>The solution is not a low latency, or high throughput solution. As the kube-apiserver to shoot cluster traffic does not demand these properties we do not intend to invest in improvements.</li><li>We do not intend to provide continuous availability to the shoot-seed VPN connection. We expect the availability to be comparable to the existing solution.</li></ul><h2 id=proposal>Proposal</h2><p>The proposal is depicted in the following diagram:</p><p><img src=/__resources/ReversedTunnelVPN_c0c514.png alt="alt text" title="Reversed Tunnel VPN"></p><p>We have added an OpenVPN server pod (<code>vpn-seed-server</code>) to each control plane. The OpenVPN client in the shoot cluster (<code>vpn-shoot-client</code>) connects to the OpenVPN server.</p><p>The two containers <code>vpn-seed-server</code> and <code>vpn-shoot-client</code> are new containers and are not related to containers in the github.com/gardener/vpn project. We will create a new project github.com/gardener/vpn2 for these containers. With this solution we intend to supersede the containers from the github.com/gardener/vpn project.</p><p>A service <code>vpn-seed-server</code> of type <code>ClusterIP</code> is created for each control plane in its namespace.</p><p>The <code>vpn-shoot-client</code> pod connects to the correct <code>vpn-seed-server</code> service via the SNI passthrough proxy introduced with <a href=/docs/gardener/proposals/08-shoot-apiserver-via-sni/>SNI Passthrough proxy for kube-apiservers</a> on port 8132.</p><p>Shoot OpenVPN clients (<code>vpn-shoot-client</code>) connect to the correct OpenVPN Server using the http proxy feature provided by OpenVPN. A configuration is added to the envoy proxy to detect http proxy requests and open a connection attempt to the correct OpenVPN server.</p><p>The <code>kube-apiserver</code> to shoot cluster connections are established using the API server proxy feature via an envoy proxy sidecar container of the <code>vpn-seed-server</code> container.</p><p>The restriction regarding the <code>192.168.123.0/24</code> network range in the current VPN solution still applies to this proposal. No other restrictions are introduced. In the context of this GEP a pull requst has been filed to block usage of that range by shoot clusters.</p><h3 id=performance-and-scalability>Performance and Scalability</h3><p>We do expect performance and throughput to be slightly lower compared to the existing solution. This is because the OpenVPN server acts as an additional hop and must decrypt and re-encrypt traffic that passes through. As there are no low latency, or high thoughput requirements for this connection we do not assume this to be an issue.</p><h3 id=availability-and-failure-scenarios>Availability and Failure Scenarios</h3><p>This solution re-uses multiple instances of the envoy component used for the kube-apiserver endpoints. We assume that the availability for kube-apiservers is good enough for the cluster VPN as well.</p><p>The OpenVPN client- and server pods are singleton pods in this approach and therefore are affected by potential failures and during cluster-, and control plane updates. Potential outages are only restricted to single shoot clusters and are comparable to the situation with the existing solution today.</p><h3 id=feature-gates-and-migration-strategy>Feature Gates and Migration Strategy</h3><p>We have introduced a gardenlet feature gate <code>ReversedVPN</code>. If <code>APIServerSNI</code> and <code>ReversedVPN</code> are enabled the proposed solution is automatically enabled for all shoot clusters hosted by the seed. If <code>ReversedVPN</code> is enabled but <code>APIServerSNI</code> is not the gardenlet will panic during startup as this is an invalid configuration. All existing shoot clusters will automatically be migrated during the next reconciliation. We assume that the <code>ReversedVPN</code> feature will work with Gardener as well as operator managed Istio.</p><p>We have also added a shoot annotation <code>alpha.featuregates.shoot.gardener.cloud/reversed-vpn</code> which can override the feature gate to enable or disable the solution for individual clusters. This is only respected if <code>APIServerSNI</code> is enabled, otherwise it is ignored.</p><h3 id=security-review>Security Review</h3><p>The change in the VPN solution will potentially open up new attack vectors. We will perform a thorough analysis outside of this document.</p><h2 id=alternatives>Alternatives</h2><h3 id=wireguard-and-kubelink-based-cluster-vpn>WireGuard and Kubelink based Cluster VPN</h3><p>We have done a detailed investigation and implementation of a reversed VPN based on WireGuard. While we believe that it is technically feasible and superior to the approach presented above there are some concerns with regards to scalability, and high availability. As the WireGuard scenario based on kubelink is relevant for other use cases we continue to improve this implementation and address the concerns but we concede that this might not be on time for the cluster VPN. We nevertheless keep the implementation and provide an outline as part of this proposal.</p><p>The general idea of the proposal is to keep the existing cluster VPN solution more or less as is, but change the underlying network used for the <code>vpn seed => vpn shoot</code> connection. The underlying network should be established in the reversed direction, i.e. the shoot cluster should initiate the network connection, but it nevertheless should work in both directions.</p><p>We achieve this by tunneling the open vpn connection through a WireGuard tunnel, which is established from the shoot to the seed (note that WireGuard uses UDP as protocol). Independent of that we can also use UDP for the OpenVPN connection, but we can also stay with TCP as it was before. While this might look like a big change, it only introduces minor changes to the existing solution, but let&rsquo;s look at the details. In essence, the OpenVPN connection does not require a public endpoint in the shoot cluster but it usees the internal endpoint provided by the WireGuard tunnel.</p><p>This is roughly depcited in this diagram. Note, that the <code>vpn-seed</code> and <code>vpn-shoot</code> containers only require very little changes and are fully backwards compatible.</p><p><img src=/__resources/WireGuardClusterVPN_ff18dd.png alt="alt text" title="Overview WireGuard Current Cluster VPN"></p><p>The WireGuard network needs a separate network range/CIDR. It has to be unique for the seed and all its shoot clusters. An example for an assumed workload of around 1000 shoot clusters would be <code>192.168.128.0/22</code> (1024 IP addresses), i.e. <code>192.168.128.0-192.168.131.255</code>. The IP addresses from this range need to be managed, but the IP address management (IPAM) using the Gardener Kubernetes objects like seed and shootstate as backing store is fairly straightforward. This is especially true as we do not expect large network ranges and only infrequent IP allocations. Hence, the IP address allocation can be quite simple, i.e. scan the range for a free IP address of all shoot clusters in a seed and allocate the first free address from the range.</p><p>There is another restriction: in case shoot clusters are configured to be seed clusters this network range must not overlap with the &ldquo;parent&rdquo; seed cluster. If the parent seed cluster uses <code>192.168.128.0/22</code> the child seed cluster can for example use <code>192.168.132.0/22</code>. Grandchildren can however use grandparent IP address ranges. Also 2 children seed clusters can use identical ranges.</p><p>This slightly adds to the restrictions described in the current solution outline. In that the arbitrary chosen <code>192.168.123.0/24</code> range is restricted. For the purpose of this implementation we propose to extend that restriction to <code>192.168.128.0/17</code> range. Most of it would be reserved for &ldquo;future use&rdquo; however. We are well aware that this adds to the burden of correctly configuring Gardener landscapes.</p><p>We do consider this to be a challenge that needs to be addressed by careful configuration of the Gardener seed cluster infrastructure. Together with the <code>192.168.123.0/24</code> address range these ranges should be automatically blocked for usage by shoots.</p><p>WireGuard can utilize the Linux kernel so that after initialization/configuration no user space processes are required. We propose to recommend the WireGuard kernel module as the default solution for all seeds. For shoot clusters, the WireGuard kernel based approach is also recommended, but the user space solution should also work as we expect less traffic on the shoot side. We expect the userspace implementation to work on all operating systems supported by Gardener in case no kernel module is available.</p><p>Almost all seed clusters are already managed by Gardener and we assume that those are configured with the WireGuard kernel module. There are however some cases where we use other Kubernetes distributions as seed cluster which may not have an operating system with WireGuard module available. We will therefore generally support the user space WireGuard process on seed cluster but place a size restriction on the number of control planes on those seeds.</p><p>There is a user space implementation of WireGuard, which can be used on Linux distributions without the WireGuard kernel module. (WireGuard moved into the standard Linux kernel 5.6.) Our proposal can handle the kernel/user space switch transparently, i.e. we include the user space binaries and use them only when required. However, especially for the seed the kernel based solution might be more attractive. Garden Linux 318.4.0 supports WireGuard.</p><p>We have looked at Ubuntu and SuSE chost:</p><ul><li>SuSE chost does not provide the WireGuard kernel module and it is not installable via zypper. It should however be straightforward for SuSE to include that in their next release.</li><li>Ubuntu does not provide the kernel module either but it can be installed using <code>apt-get install wireguard</code>. With that it appears straightforward to provide an image with WireGuard pre-installed.</li></ul><p>On the seed, we add a WireGuard device to one node on the host network. For all other nodes on the seed, we adapt the routes accordingly to route traffic destined for the WireGuard network to our WireGuard node. The Kubernetes pods managing the WireGuard device and routes are only used for initial configuration and later reconfiguration. During runtime, they can restart without any impact on the operation of the WireGuard network as the WireGuard device is managed by the Linux kernel.</p><p>With Calico as the networking solution it is not easily possible to put the WireGuard endpoint into a pod. Putting the WireGuard endpoint into a pod would require to define it as a gateway in the api server or prometheus pods but this is not possible since Calico does not span a proper subnet. While the defined CIDR in the pod network might be <code>100.96.0.0/11</code> the network visible from within a pod is only <code>100.96.0.5/32</code>. This restriction might not exist with other networking solutions.</p><p>The WireGuard endpoint on the seed is exposed via a load balancer. We propose to use <a href=https://github.com/mandelsoft/kubelink>kubelink</a> to manage the WireGuard configuration/device on the seed. We consider the management of the WireGuard endpoint to be complex especially in error situations which is the reason for utilizing kubelink as there is already significant experience managing an endpoint. We propose moving kubelink to the Gardener org in case it is used by this proposal.</p><p>Kubelink addresses three challenges managing WireGuard interfaces on cluster nodes. First, with WireGuard interfaces directly on the node (<code>hostNetwork=true</code>) the lifecycle of the interface is decoupled from the lifecycle of the pod that created it. This means that there will have to be means of cleaning up the interfaces and its configuration in case the interface moves to a different node. Second, additional routing information must be distributed across the cluster. The WireGuard CIDR is unknown to the network implementation so additional routes must be distributed on all nodes of the cluster. Third, kubelink dynamincally configures the Wireguard interface with endpoints and their public keys.</p><p>On the shoot, we create the keys and acquire the WireGuard IP in the standard secret generation. The data is added as a secret to the control plane and to the shootstate. The vpn shoot deployment is extended to include the WireGuard device setup inside the vpn shoot pod network. For certain infrastructures (AWS), we need a re-advertiser to resolve the seed WireGuard endpoint and evaluate whether the IP address changed.</p><p>While it is possible to configure a WireGuard device using DNS names only IP addresses can be stored in Linux Kernel data structures. A change of a load balancer IP address can therefore not be mitigated on that level. As WireGuard dynamically adapts endpoint IP addresses a change in load banlancer IPs is mitigated in most but not all cases. This is why a re-advertiser is required for public cloud providers such as AWS.</p><p>The load balancer exposing the OpenVPN endpoint in the shoot cluster is no longer required and therefore removed if this functionality is used.</p><p>As we want to slowly expand the usage of the WireGuard solution, we propose to introduce a feature gate for it. Furthermore, since the WireGuard network requires a separate network range, we propose to introduce a new section to the seed settings with two additional flags (enabled & cidr):</p><pre tabindex=0><code>apiVersion: core.gardener.cloud/v1beta1
kind: Seed
metadata:
  name: my-seed
  ...
spec:
  ...
  settings:
  ...
    wireguard:
      enabled: true
      cidr: 192.168.128.0/22
</code></pre><p>Last but not least, we propose to introduce an annotation to the shoots to enable/disable the WireGuard tunnel explicitly.</p><pre tabindex=0><code>apiVersion: core.gardener.cloud/v1beta1
kind: Shoot
metadata:
  name: my-shoot
  annotations:
    alpha.featuregates.shoot.gardener.cloud/wireguard-tunnel: &#34;true&#34;
  ...
</code></pre><p>Using this approach, it is easy to switch the solution on and off, i.e. migrate the shoot clusters automatically during ordinary reconciliation.</p><h4 id=high-availability>High Availability</h4><p>There is an issue if the node that hosts the WireGuard endpoint fails. The endpoint is migrated to another node however the time required to do this might exceed the budget for downtimes although one could argue that a disruption of less than 30 seconds to 1 minute does not qualify as a downtime and will in almost all cases not noticeable by end users.</p><p>In this case we also assume that TCP connections won&rsquo;t be interrupted - they would just appear to hang. We will confirm this behavior and the potential downtime as part of the development and testing effort as this is hard to predict.</p><p>As a possible mitigation we propose to instantiate 2 Kubelink instances in the seed cluster that are served by two different load balancers. The instances must run on different nodes (if possible but we assume a proper seed cluster has more than one node). Each shoot cluster connects to both endpoints. This means that the OpenVPN server is reachable with two different IP addresses. The VPN seed sidecars must attempt to connect to both of them and will continue to do so. The &ldquo;Persistent Keepalive&rdquo; feature is set to 21 seconds by default but could be reduced. Due to the redundancy this however appears not to be necessary.</p><p>It is desirable that both connections are used in an equal manner. One strategy could be to use the kubelink 1 connection if the first target WireGuard address is even (the last byte of the IPv4 address), otherwise the kubelink 2 connection. The <code>vpn-seed</code> sidecars can then use the following configuration in their OpenVPN configuration file:</p><pre tabindex=0><code>&lt;connection&gt;
remote 192.168.45.3 1194 udp
&lt;/connection&gt;

&lt;connection&gt;
remote 192.168.47.34 1194 udp
&lt;/connection&gt;
</code></pre><p>OpenVPN will go through the list sequentially and try to connect to these endpoints.</p><p>As an additional mitigation it appears possible to instantiate WireGuard devices on all hosts and replicate its relevant conntrack state across all cluster nodes. The relevant conntrack state keeps the state of all connections passing through the WireGuard interface (e.g. the WireGuard CIDR). conntrack and the tools to replicate conntrack state are part of the essential Linux netfilter tools package.</p><h4 id=load-considerations>Load Considerations</h4><p>What happens in case of a failure? In this case one router will end up owning all connections as the clients will attempt to use the next connection. This could be mitigated by adding a third redundant WireGuard connection. Using this strategy, the failure of one WireGuard endpoint would result in the equal distribution of connections to the two remaining interfaces. We believe however that this will not be necessary.</p><p>The cluster node running the Wireguard endpoint is essentially a router that routes all traffic to the various shoot clusters. This is established and proven technology that already exists since decades and has been highly optimized since then. This is also the technology that hyperscalers rely on to provide VPN connectivity to their customers. This said, hyperscalers essentially provide solutions based on IPsec which is known not to scale as well as Wireguard. Wireguard is a relatively new technology but we have no doubt that it is less stable than existing IPsec solution.</p><p>Regarding performance there is a lot of information on the Internet basically suggesting that Wireguard performs better than other VPN solutions such as IPsec or OpenVPN. One example is <a href=https://www.wireguard.com/performance/>https://www.wireguard.com/performance/</a> and <a href=https://www.net.in.tum.de/fileadmin/bibtex/publications/papers/2020-ifip-moonwire.pdf>https://www.net.in.tum.de/fileadmin/bibtex/publications/papers/2020-ifip-moonwire.pdf</a>.</p><p>Based on this, we have no reason to believe that one router will not be able to handle all traffic going to and coming from shoot clusters. Nevertheless, we will closely monitor the situation in our tests and will take action if necessary.</p><h4 id=further-research>Further Research</h4><p>Based on feedback on this proposal and while working on this implementation we identified two additinal approaches that we have not thought of so far. The first idea can be used to replace the &ldquo;inner&rdquo; OpenVPN implementation and the second can be used to replace WireGuard with OpenVPN and get rid of the single point of failure.</p><ol><li><p>Instead of using OpenVPN for the inner seed/shoot communication we can use the proxy protocol and use a TCP proxy (e.g. envoy) in the shoot cluster to broker the seed-shoot connections. The advantage is that with this solution seed- and shoot cluster network ranges are allowed to overlap. Disadvantages are increased implementation effort and less efficient network in terms of throughput and scalability. We believe however that the reduced network efficiency does not invalidate this option.</p></li><li><p>There is an option in OpenVPN to specify a tcp proxy as part of the endpoint configuration.</p></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-537ccca03886037e9fbdfd4d0c15d1a0>19 - Shoot APIServer via SNI</h1><h1 id=sni-passthrough-proxy-for-kube-apiservers>SNI Passthrough proxy for kube-apiservers</h1><p>This GEP tackles the problem that today a single <code>LoadBalancer</code> is needed for every single Shoot cluster&rsquo;s control plane.</p><h2 id=background>Background</h2><p>When the control plane of a Shoot cluster is provisioned, a dedicated LoadBalancer is created for it. It keeps the entire flow quite easy - the apiserver Pods are running and they are accessible via that LoadBalancer. It&rsquo;s hostnames / IP addresses are used for DNS records like <code>api.&lt;external-domain></code> and <code>api.&lt;shoot>.&lt;project>.&lt;internal-domain></code>. While this solution is simple it comes with several issues.</p><h2 id=motivation>Motivation</h2><p>There are several problems with the current setup.</p><ul><li>IaaS provider costs. For example <code>ClassicLoadBalancer</code> on AWS costs at minimum 17 USD / month.</li><li>Quotas can limit the amount of LoadBalancers you can get per account / project, limiting the number of clusters you can host under a single account.</li><li>Lack of support for better loadbalancing <a href=https://www.envoyproxy.io/docs/envoy/v1.10.0/intro/arch_overview/load_balancing/load_balancers#supported-load-balancers>algorithms than round-robin</a>.</li><li>Slow cluster provisioning time - depending on the provider a LoadBalancer provisioning could take quite a while.</li><li>Lower downtime when workload is shuffled in the clusters as the LoadBalancer is Kubernetes-aware.</li></ul><h2 id=goals>Goals</h2><ul><li>Only one LoadBalancer is used for all Shoot cluster API servers running in a Seed cluster.</li><li>Out-of-cluster (end-user / robot) communication to the API server is still possible.</li><li>In-cluster communication via the kubernetes master service (IPv4/v6 ClusterIP and the <code>kubernetes.default.svc.cluster.local</code>) is possible.</li><li>Client TLS authentication works without intermediate TLS termination (TLS is terminated by <code>kube-apiserver</code>).</li><li>Solution should be cloud-agnostic.</li></ul><h2 id=proposal>Proposal</h2><h3 id=seed-cluster>Seed cluster</h3><p>To solve the problem of having multiple <code>kube-apiservers</code> behind a single LoadBalancer, an intermediate proxy must be placed between the Cloud-Provider&rsquo;s LoadBalancer and <code>kube-apiservers</code>. This proxy is going to choose the Shoot API Server with the help of Server Name Indication. From <a href=https://en.wikipedia.org/wiki/Server_Name_Indication>wikipedia</a>:</p><blockquote><p>Server Name Indication (SNI) is an extension to the Transport Layer Security (TLS) computer networking protocol by which a client indicates which hostname it is attempting to connect to at the start of the handshaking process. This allows a server to present multiple certificates on the same IP address and TCP port number and hence allows multiple secure (HTTPS) websites (or any other service over TLS) to be served by the same IP address without requiring all those sites to use the same certificate. It is the conceptual equivalent to HTTP/1.1 name-based virtual hosting, but for HTTPS.</p></blockquote><p>A rough diagram of the flow of data:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>+-------------------------------+
</span></span><span style=display:flex><span>|                               |
</span></span><span style=display:flex><span>|           Network LB          | (accessible from clients)
</span></span><span style=display:flex><span>|                               |
</span></span><span style=display:flex><span>|                               |
</span></span><span style=display:flex><span>+-------------+-------+---------+                       +------------------+
</span></span><span style=display:flex><span>              |       |                                 |                  |
</span></span><span style=display:flex><span>              |       |            proxy + lb           | Shoot API Server |
</span></span><span style=display:flex><span>              |       |    +-------------+-------------&gt;+                  |
</span></span><span style=display:flex><span>              |       |    |                            | Cluster A        |
</span></span><span style=display:flex><span>              |       |    |                            |                  |
</span></span><span style=display:flex><span>              |       |    |                            +------------------+
</span></span><span style=display:flex><span>              |       |    |
</span></span><span style=display:flex><span>     +----------------v----+--+
</span></span><span style=display:flex><span>     |        |               |
</span></span><span style=display:flex><span>   +-+--------v----------+    |                         +------------------+
</span></span><span style=display:flex><span>   |                     |    |                         |                  |
</span></span><span style=display:flex><span>   |                     |    |       proxy + lb        | Shoot API Server |
</span></span><span style=display:flex><span>   |        Proxy        |    +-------------+----------&gt;+                  |
</span></span><span style=display:flex><span>   |                     |    |                         | Cluster B        |
</span></span><span style=display:flex><span>   |                     |    |                         |                  |
</span></span><span style=display:flex><span>   |                     +----+                         +------------------+
</span></span><span style=display:flex><span>   +----------------+----+
</span></span><span style=display:flex><span>                    |
</span></span><span style=display:flex><span>                    |
</span></span><span style=display:flex><span>                    |                                   +------------------+
</span></span><span style=display:flex><span>                    |                                   |                  |
</span></span><span style=display:flex><span>                    |             proxy + lb            | Shoot API Server |
</span></span><span style=display:flex><span>                    +-------------------+--------------&gt;+                  |
</span></span><span style=display:flex><span>                                                        | Cluster C        |
</span></span><span style=display:flex><span>                                                        |                  |
</span></span><span style=display:flex><span>                                                        +------------------+
</span></span></code></pre></div><p>Sequentially:</p><ol><li>client requests <code>Shoot Cluster A</code> and sets the <code>Server Name</code> in the TLS handshake to <code>api.shoot-a.foo.bar</code>.</li><li>this packet goes through the Network LB and it&rsquo;s forwarded to the Proxy server. (this loadbalancer should be a simple Layer-4 TCP proxy)</li><li>the proxy server reads the packet and see that client requests <code>api.shoot-a.foo.bar</code>.</li><li>based on its configuration, it maps <code>api.shoot-a.foo.bar</code> to <code>Shoot API Server Cluster A</code>.</li><li>it acts as TCP proxy and simply send the data <code>Shoot API Server Cluster A</code>.</li></ol><p>There are multiple OSS proxies for this case:</p><ul><li>nginx</li><li>HAProxy</li><li>Envoy</li><li>traefik</li><li><a href=https://github.com/linkerd/linkerd2-proxy>linkerd2-proxy</a></li></ul><p>To ease integration it should:</p><ul><li>be configurable via Kubernetes resources</li><li>not require restarting when configuration changes</li><li>be fast and with little overhead</li></ul><p>All things considered, <a href=http://envoyproxy.io/>Envoy proxy</a> is the most fitting solution as it provides all the features Gardener would like (no process reload being the most important one + battle tested in production by various companies).</p><p>While building a custom control plane for Envoy is <a href=https://github.com/envoyproxy/go-control-plane>quite simple</a>, an already established solution might be the better path forward. <a href=https://istio.io/docs/concepts/traffic-management/#pilot-and-envoy>Istio&rsquo;s Pilot</a> is one of the most feature-complete Envoy control plane solutions as it offers a way to configure edge ingress traffic for Envoy via <a href=https://istio.io/docs/reference/config/networking/v1alpha3/gateway/>Gateway</a> and <a href=https://istio.io/docs/reference/config/networking/v1alpha3/virtual-service/>VirtualService</a>.</p><p>The resources which needs to be created per Shoot clusters are the following:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.istio.io/v1alpha3
</span></span><span style=display:flex><span>kind: Gateway
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: kube-apiserver-gateway
</span></span><span style=display:flex><span>  namespace: &lt;shoot-namespace&gt;
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    istio: ingressgateway
</span></span><span style=display:flex><span>  servers:
</span></span><span style=display:flex><span>  - port:
</span></span><span style=display:flex><span>      number: 443
</span></span><span style=display:flex><span>      name: tls
</span></span><span style=display:flex><span>      protocol: TLS
</span></span><span style=display:flex><span>    tls:
</span></span><span style=display:flex><span>      mode: PASSTHROUGH
</span></span><span style=display:flex><span>    hosts:
</span></span><span style=display:flex><span>    - api.&lt;external-domain&gt;
</span></span><span style=display:flex><span>    - api.&lt;shoot&gt;.&lt;project&gt;.&lt;internal-domain&gt;
</span></span></code></pre></div><p>and correct <code>VirtualService</code> pointing to the correct API server:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.istio.io/v1alpha3
</span></span><span style=display:flex><span>kind: VirtualService
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: kube-apiserver
</span></span><span style=display:flex><span>  namespace: &lt;shoot-namespace&gt;
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  hosts:
</span></span><span style=display:flex><span>  - api.&lt;external-domain&gt;
</span></span><span style=display:flex><span>  - api.&lt;shoot&gt;.&lt;project&gt;.&lt;internal-domain&gt;
</span></span><span style=display:flex><span>  gateways:
</span></span><span style=display:flex><span>  - kube-apiserver-gateway
</span></span><span style=display:flex><span>  tls:
</span></span><span style=display:flex><span>  - match:
</span></span><span style=display:flex><span>    - port: 443
</span></span><span style=display:flex><span>      sniHosts:
</span></span><span style=display:flex><span>      - api.&lt;external-domain&gt;
</span></span><span style=display:flex><span>      - api.&lt;shoot&gt;.&lt;project&gt;.&lt;internal-domain&gt;
</span></span><span style=display:flex><span>    route:
</span></span><span style=display:flex><span>    - destination:
</span></span><span style=display:flex><span>        host: kube-apiserver.&lt;shoot-namespace&gt;.svc.cluster.local
</span></span><span style=display:flex><span>        port:
</span></span><span style=display:flex><span>          number: 443
</span></span></code></pre></div><p>The resources above configures Envoy to forward the raw TLS data (without termination) to the Shoot <code>kube-apiserver</code>.</p><p>Updated diagram:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>+-------------------------------+
</span></span><span style=display:flex><span>|                               |
</span></span><span style=display:flex><span>|           Network LB          | (accessible from clients)
</span></span><span style=display:flex><span>|                               |
</span></span><span style=display:flex><span>|                               |
</span></span><span style=display:flex><span>+-------------+-------+---------+                       +------------------+
</span></span><span style=display:flex><span>              |       |                                 |                  |
</span></span><span style=display:flex><span>              |       |            proxy + lb           | Shoot API Server |
</span></span><span style=display:flex><span>              |       |    +-------------+-------------&gt;+                  |
</span></span><span style=display:flex><span>              |       |    |                            | Cluster A        |
</span></span><span style=display:flex><span>              |       |    |                            |                  |
</span></span><span style=display:flex><span>              |       |    |                            +------------------+
</span></span><span style=display:flex><span>              |       |    |
</span></span><span style=display:flex><span>     +----------------v----+--+
</span></span><span style=display:flex><span>     |        |               |
</span></span><span style=display:flex><span>   +-+--------v----------+    |                         +------------------+
</span></span><span style=display:flex><span>   |                     |    |                         |                  |
</span></span><span style=display:flex><span>   |                     |    |       proxy + lb        | Shoot API Server |
</span></span><span style=display:flex><span>   |    Envoy Proxy      |    +-------------+----------&gt;+                  |
</span></span><span style=display:flex><span>   | (ingress Gateway)   |    |                         | Cluster B        |
</span></span><span style=display:flex><span>   |                     |    |                         |                  |
</span></span><span style=display:flex><span>   |                     +----+                         +------------------+
</span></span><span style=display:flex><span>   +-----+----------+----+
</span></span><span style=display:flex><span>         |          |
</span></span><span style=display:flex><span>         |          |
</span></span><span style=display:flex><span>         |          |                                   +------------------+
</span></span><span style=display:flex><span>         |          |                                   |                  |
</span></span><span style=display:flex><span>         |          |             proxy + lb            | Shoot API Server |
</span></span><span style=display:flex><span>         |          +-------------------+--------------&gt;+                  |
</span></span><span style=display:flex><span>         |   get                                        | Cluster C        |
</span></span><span style=display:flex><span>         | configuration                                |                  |
</span></span><span style=display:flex><span>         |                                              +------------------+
</span></span><span style=display:flex><span>         |
</span></span><span style=display:flex><span>         v                                                  Configure
</span></span><span style=display:flex><span>      +--+--------------+         +---------------------+   via Istio
</span></span><span style=display:flex><span>      |                 |         |                     |   Custom Resources
</span></span><span style=display:flex><span>      |     Pilot       +--------&gt;+   Seed API Server   +&lt;------------------+
</span></span><span style=display:flex><span>      |                 |         |                     |
</span></span><span style=display:flex><span>      |                 |         |                     |
</span></span><span style=display:flex><span>      +-----------------+         +---------------------+
</span></span></code></pre></div><p>In this case the <code>internal</code> and <code>external</code> <code>DNSEntries</code> should be changed to the Network LoadBalancer&rsquo;s IP.</p><h3 id=in-cluster-communication-to-the-apiserver>In-cluster communication to the apiserver</h3><p>In Kubernetes the API server is discoverable via the master service (<code>kubernetes</code> in <code>default</code> namespace). Today, this service can only be of type <code>ClusterIP</code> - making in-cluster communication to the API server impossible due to:</p><ul><li>the client doesn&rsquo;t set the <code>Server Name</code> in the TLS handshake, if it attempts to talk to an IP address. In this case, the TLS handshake reaches the Envoy IngressGateway proxy, but it&rsquo;s rejected by it.</li><li>Kubernetes services can be of type <code>ExternalName</code>, but the master service is not supported by <a href=https://github.com/gardener/gardener/issues/1135#issuecomment-505317932>kubelet</a>.<ul><li>even if this is fixed in future Kubernetes versions, this problem still exists for older versions where this functionality is not available.</li></ul></li></ul><p>Another issue occurs when the client tries to talk to the apiserver via the in-cluster DNS. For all Shoot API servers <code>kubernetes.default.svc.cluster.local</code> is the same and when a client tries to connect to that API server using that server name. This makes distinction between different in-cluster Shoot clients impossible by the Envoy IngressGateway.</p><p>To mitigate this problem an additional proxy must be deployed on every single Node. It does not terminate TLS and sends the traffic to the correct Shoot API Server. This is achieved by:</p><ul><li>the apiserver master service reconciler is started and pointing to the <code>kube-apiserver</code>&rsquo;s Cluster IP in the Seed cluster (e.g. <code>--advertise-address=10.1.2.3</code>).</li><li>the proxy runs in the host network of the <code>Node</code>.</li><li>the proxy has a sidecar container which:<ul><li>creates a dummy network interface and assigns the <code>10.1.2.3</code> to it.</li><li>removes connection tracking (conntrack) if iptables/nftables is enabled as the IP address is local to the <code>Node</code>.</li></ul></li><li>the proxy listens on the <code>10.1.2.3</code> and using the <a href=http://www.haproxy.org/download/2.0/doc/proxy-protocol.txt>PROXY protocol</a> it sends the data stream to the Envoy ingress gateway (EIGW).</li><li>EIGW listens for PROXY protocol on a dedicated <code>8443</code> port. EIGW reads the destination IP + port from the PROXY protocol and forwards traffic to the correct upstream apiserver.</li></ul><p>The sidecar is a standalone component. It&rsquo;s possible to transparently change the proxy implementation without any modifications to the sidecar. The simplified flow looks like:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>+------------------+                    +----------------+
</span></span><span style=display:flex><span>| Shoot API Server |       TCP          |   Envoy IGW    |
</span></span><span style=display:flex><span>|                  +&lt;-------------------+ PROXY listener |
</span></span><span style=display:flex><span>| Cluster A        |                    |     :8443      |
</span></span><span style=display:flex><span>+------------------+                    +-+--------------+
</span></span><span style=display:flex><span>                                          ^
</span></span><span style=display:flex><span>                                          |
</span></span><span style=display:flex><span>                                          |
</span></span><span style=display:flex><span>                                          |
</span></span><span style=display:flex><span>                                          |
</span></span><span style=display:flex><span>+-----------------------------------------------------------+
</span></span><span style=display:flex><span>                                          |   Single Node in
</span></span><span style=display:flex><span>                                          |   the Shoot cluster
</span></span><span style=display:flex><span>                                          |
</span></span><span style=display:flex><span>                                          | PROXY Protocol
</span></span><span style=display:flex><span>                                          |
</span></span><span style=display:flex><span>                                          |
</span></span><span style=display:flex><span>                                          |
</span></span><span style=display:flex><span> +---------------------+       +----------+----------+
</span></span><span style=display:flex><span> |  Pod talking to     |       |                     |
</span></span><span style=display:flex><span> |  the kubernetes     |       |       Proxy         |
</span></span><span style=display:flex><span> |  service            +------&gt;+  No TLS termination |
</span></span><span style=display:flex><span> |                     |       |                     |
</span></span><span style=display:flex><span> +---------------------+       +---------------------+
</span></span></code></pre></div><p>Multiple OSS solutions can be used:</p><ul><li>haproxy</li><li>nginx</li></ul><p>To add a PROXY lister with Istio several resources must be created - a dedicated <code>Gateway</code>, dummy <code>VirtualService</code> and <code>EnvoyFilter</code> which adds listener filter (<code>envoy.listener.proxy_protocol</code>) on <code>8443</code> port:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.istio.io/v1alpha3
</span></span><span style=display:flex><span>kind: Gateway
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: blackhole
</span></span><span style=display:flex><span>  namespace: istio-system
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    istio: ingressgateway
</span></span><span style=display:flex><span>  servers:
</span></span><span style=display:flex><span>  - port:
</span></span><span style=display:flex><span>      number: 8443
</span></span><span style=display:flex><span>      name: tcp
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>    hosts:
</span></span><span style=display:flex><span>    - <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>apiVersion: networking.istio.io/v1alpha3
</span></span><span style=display:flex><span>kind: VirtualService
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: blackhole
</span></span><span style=display:flex><span>  namespace: istio-system
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  hosts:
</span></span><span style=display:flex><span>  - blackhole.local
</span></span><span style=display:flex><span>  gateways:
</span></span><span style=display:flex><span>  - blackhole
</span></span><span style=display:flex><span>  tcp:
</span></span><span style=display:flex><span>  - match:
</span></span><span style=display:flex><span>    - port: 8443
</span></span><span style=display:flex><span>    route:
</span></span><span style=display:flex><span>    - destination:
</span></span><span style=display:flex><span>        host: localhost
</span></span><span style=display:flex><span>        port:
</span></span><span style=display:flex><span>          number: 9999 <span style=color:green># any dummy port will work</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>apiVersion: networking.istio.io/v1alpha3
</span></span><span style=display:flex><span>kind: EnvoyFilter
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: proxy-protocol
</span></span><span style=display:flex><span>  namespace: istio-system
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  workloadSelector:
</span></span><span style=display:flex><span>    labels:
</span></span><span style=display:flex><span>      istio: ingressgateway
</span></span><span style=display:flex><span>  configPatches:
</span></span><span style=display:flex><span>  - applyTo: LISTENER
</span></span><span style=display:flex><span>    match:
</span></span><span style=display:flex><span>      context: ANY
</span></span><span style=display:flex><span>      listener:
</span></span><span style=display:flex><span>        portNumber: 8443
</span></span><span style=display:flex><span>        name: 0.0.0.0_8443
</span></span><span style=display:flex><span>    patch:
</span></span><span style=display:flex><span>      operation: MERGE
</span></span><span style=display:flex><span>      value:
</span></span><span style=display:flex><span>        listener_filters:
</span></span><span style=display:flex><span>        - name: envoy.filters.listener.proxy_protocol
</span></span></code></pre></div><p>For each individual <code>Shoot</code> cluster, a dedicated <a href=https://www.envoyproxy.io/docs/envoy/v1.13.0/api-v2/api/v2/listener/listener_components.proto#listener-filterchainmatch>FilterChainMatch</a> is added. It ensures that only Shoot API servers can receive traffic from this listener:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.istio.io/v1alpha3
</span></span><span style=display:flex><span>kind: EnvoyFilter
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: &lt;shoot-namespace&gt;
</span></span><span style=display:flex><span>  namespace: istio-system
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  workloadSelector:
</span></span><span style=display:flex><span>    labels:
</span></span><span style=display:flex><span>      istio: ingressgateway
</span></span><span style=display:flex><span>  configPatches:
</span></span><span style=display:flex><span>  - applyTo: FILTER_CHAIN
</span></span><span style=display:flex><span>    match:
</span></span><span style=display:flex><span>      context: ANY
</span></span><span style=display:flex><span>      listener:
</span></span><span style=display:flex><span>        portNumber: 8443
</span></span><span style=display:flex><span>        name: 0.0.0.0_8443
</span></span><span style=display:flex><span>    patch:
</span></span><span style=display:flex><span>      operation: ADD
</span></span><span style=display:flex><span>      value:
</span></span><span style=display:flex><span>        filters:
</span></span><span style=display:flex><span>        - name: envoy.filters.network.tcp_proxy
</span></span><span style=display:flex><span>          typed_config:
</span></span><span style=display:flex><span>            &#34;@type&#34;: type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy
</span></span><span style=display:flex><span>            stat_prefix: outbound|443||kube-apiserver.&lt;shoot-namespace&gt;.svc.cluster.local
</span></span><span style=display:flex><span>            cluster: outbound|443||kube-apiserver.&lt;shoot-namespace&gt;.svc.cluster.local
</span></span><span style=display:flex><span>        filter_chain_match:
</span></span><span style=display:flex><span>          destination_port: 443
</span></span><span style=display:flex><span>          prefix_ranges:
</span></span><span style=display:flex><span>          - address_prefix: 10.1.2.3 <span style=color:green># kube-apiserver&#39;s cluster-ip</span>
</span></span><span style=display:flex><span>            prefix_len: 32
</span></span></code></pre></div><blockquote><p>Note: this additional <code>EnvoyFilter</code> can be removed when Istio supports full <a href=https://istio.io/docs/reference/config/networking/virtual-service/#L4MatchAttributes>L4 matching</a>.</p></blockquote><p>A nginx proxy client in the Shoot cluster on every node could have the following configuration:</p><pre tabindex=0><code class=language-conf data-lang=conf>error_log /dev/stdout;
stream {
    server {
        listen 10.1.2.3:443;
        proxy_pass api.&lt;external-domain&gt;:8443;
        proxy_protocol on;

        proxy_protocol_timeout 5s;
        resolver_timeout 5s;
        proxy_connect_timeout 5s;
    }
}

events { }
</code></pre><h3 id=in-cluster-communication-to-the-apiserver-when-exernalname-is-supported>In-cluster communication to the apiserver when ExernalName is supported</h3><p>Even if in future versions of Kubernetes, the master service of type <code>ExternalName</code> is supported, we still have the problem that in-cluster workload can talk to the server via DNS. For this to work we still need the above mentioned proxy (this time listening on another IP address <code>10.0.0.2</code>). An additional change to CoreDNS would be needed:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>default.svc.cluster.local.:8053 {
</span></span><span style=display:flex><span>    file kubernetes.default.svc.cluster.local
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>.:8053 {
</span></span><span style=display:flex><span>    errors
</span></span><span style=display:flex><span>    health
</span></span><span style=display:flex><span>    kubernetes cluster.local in-addr.arpa ip6.arpa {
</span></span><span style=display:flex><span>        pods insecure
</span></span><span style=display:flex><span>        upstream
</span></span><span style=display:flex><span>        fallthrough in-addr.arpa ip6.arpa
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    prometheus :9153
</span></span><span style=display:flex><span>    forward . /etc/resolv.conf
</span></span><span style=display:flex><span>    cache 30
</span></span><span style=display:flex><span>    loop
</span></span><span style=display:flex><span>    reload
</span></span><span style=display:flex><span>    loadbalance
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The content of the <code>kubernetes.default.svc.cluster.local</code> is going to be:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>$ORIGIN default.svc.cluster.local.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>@	30 IN	SOA local. local. (
</span></span><span style=display:flex><span>        2017042745 ; serial
</span></span><span style=display:flex><span>        1209600    ; refresh (2 hours)
</span></span><span style=display:flex><span>        1209600    ; retry (1 hour)
</span></span><span style=display:flex><span>        1209600    ; expire (2 weeks)
</span></span><span style=display:flex><span>        30         ; minimum (1 hour)
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  30 IN NS local.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubernetes     IN A     10.0.0.2
</span></span></code></pre></div><p>So when a client requests <code>kubernetes.default.svc.cluster.local</code>, it&rsquo;ll be send to the proxy listening on that IP address.</p><h2 id=future-work>Future work</h2><p>While out of scope of this GEP, several things can be improved:</p><ul><li>Make the sidecar work with eBPF and environments where iptables/nftables are not enabled.</li></ul><h2 id=references>References</h2><ul><li><a href=https://github.com/gardener/gardener/issues/1135>https://github.com/gardener/gardener/issues/1135</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-67211574c9c9627ba4e942fa77ceda03>20 - Shoot CA Rotation</h1><h1 id=gep-18-automated-shoot-ca-rotation>GEP-18: Automated Shoot CA Rotation</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#summary>Summary</a></li><li><a href=#motivation>Motivation</a><ul><li><a href=#goals>Goals</a></li><li><a href=#non-goals>Non-Goals</a></li></ul></li><li><a href=#proposal>Proposal</a></li><li><a href=#alternatives>Alternatives</a></li><li><a href=#open-questions>Open Questions</a></li></ul><h2 id=summary>Summary</h2><p>This proposal outlines an on-demand, multi-step approach to rotate all certificate authorities (CA) used in a Shoot cluster. This process includes creating new CAs, invalidating the old ones and recreating all certificates signed by the CAs.</p><p>We propose to bundle the rotation of <em>all</em> CAs in the Shoot together as one triggerable action. This includes the recreation and invalidation of the following CAs and all certificates signed by them:</p><ul><li>Cluster CA (currently used for signing <code>kube-apiserver</code> serving certificates and client certificates)</li><li><code>kubelet</code> CA (used for signing client certificates for talking to <code>kubelet</code> API, e.g. <code>kube-apiserver-kubelet</code>)</li><li><code>etcd</code> CA (used for signing <code>etcd</code> serving certificates and client certificates)</li><li>front-proxy CA (used for signing client certificates that <code>kube-aggregator</code> (part of <code>kube-apiserver</code>) uses to talk to extension API servers, filled into <code>extension-apiserver-authentication</code> ConfigMap and read by extension API servers to verify incoming <code>kube-aggregator</code> requests)</li><li><code>metrics-server</code> CA (used for signing serving certificates, filled into APIService <code>caBundle</code> field and read by <code>kube-aggregator</code> to verify the presented serving certificate)</li><li><code>ReversedVPN</code> CA (used for signing <code>vpn-seed-server</code> serving certificate and <code>vpn-shoot</code> client certificate)</li></ul><p>Out of scope for now:</p><ul><li><code>kubelet</code> serving CA is self-generated (valid for <code>1y</code>) and self-signed by <code>kubelet</code> on startup<ul><li><code>kube-apiserver</code> does not seem to verify the presented serving certificate</li><li><code>kubelet</code> can be configured to request serving certificate via CSR that can be verified by <code>kube-apiserver</code>, though, we consider this as a separate improvement outside of this GEP</li></ul></li><li>Legacy VPN solution uses the cluster CA for both serving and client certificates. As the solution is soon to be dropped in favor of the new <code>ReversedVPN</code> solution, we don&rsquo;t intend to introduce a dedicated CA for this component. If <code>ReversedVPN</code> is disabled and the CA rotation is triggered, we make sure to propagate the cluster CA to the relevant places in the legacy VPN solution.</li></ul><p>Naturally, not all certificates used for communication with the <code>kube-apiserver</code> are under control of Gardener. An example for a Gardener-controlled certificate is the kubelet client certificate used to communicate with the api server. An example for credentials not controlled by gardener are kubeconfigs or client certificates requested via <code>CertificateSigningRequest</code>s by the shoot owner.</p><p>We propose to use a two step approach to rotate CAs. The start of each phase is triggered by the shoot owner.
In summary the <strong>first phase</strong> is used to create new CAs (for example the new api server and client CA). Then we make sure that all servers and clients under Gardener&rsquo;s control trust <em>both</em> old and new CA. Next we renew all client certificates that are under Gardener&rsquo;s control so they are now signed by the new CAs. This includes a node rollout in order to propagate the certificates to kubelets and restart all pods. Afterwards the user needs to change their client credentials to trust both old and new cluster CA.
In the <strong>second phase</strong>, we remove all trust to the old CA for servers and clients under Gardener&rsquo;s control. This does not include a node rollout but all still running pods using <code>ServiceAccount</code>s will continue to trust the old CA until they restart. Also, the user needs to retrieve the new CA bundle to no longer trust the old CA.</p><p>A detailed overview of all steps required for each phase is given in the <a href=#proposal>proposal</a> section of this GEP.</p><p><em>Introducing a new client CA</em></p><p>Currently, client certificates and the kube-apiserver certificate are signed by the same CA. We propose to create a separate client CA when triggering the rotation. The client CA is used to sign certificates of clients talking to the API Server.</p><h2 id=motivation>Motivation</h2><p>There are a few reasons for rotating shoot cluster CAs:</p><ul><li>If we have to invalidate client certificates for the kube-apiserver or any other component we are forced to rotate the CA. The only way to invalidate them is to stop trusting all client certificates that are signed by the respective CA as kubernetes does not support revoking certificates.</li><li>If the CA itself got leaked.</li><li>If the CA is about to expire.</li><li>If a company policy requires to rotate a CA after a certain point in time.</li></ul><p>In each of those cases we currently need to basically manually recreate and replace all CAs and certificates. The process of rotating by hand is cumbersome and could lead to errors due to the many steps needing to be performed in the right order. By automating the process we want to create a way to securely and easily rotate shoot CAs.</p><h3 id=goals>Goals</h3><ul><li>Offer an automated and safe solution to rotate all CAs in a shoot cluster.</li><li>Offer a process that is easily understandable for developers and users.</li><li>Rotate the different CAs in the shoot with a similar process to reduce complexity.</li><li>Add visibility for Shoot owners when the last CA rotation happened</li></ul><h3 id=non-goals>Non-Goals</h3><ul><li>Offer an automated solution for rotating other static credentials (like static token).<ul><li>Later on, a similar two-phase approach could be implemented for the kubeconfig rotation. However, this is out of scope for this enhancement.</li></ul></li><li>Creating a process that runs fully automated without shoot owner interaction. As the shoot owner controls some secrets that would probably not even be possible.</li><li>Forcing the shoot owner to rotate after a certain time period. Our goal rather is to issue long-running certificates and let the user decide depending on their requirements to rotate as needed.</li><li>Configurable default CA lifetime</li></ul><h2 id=proposal>Proposal</h2><p>We will add a new feature gate <code>CARotation</code> for <code>gardener-apiserver</code> and <code>gardenlet</code> which allows to enable or disable the possibility to trigger the rotation.</p><h3 id=triggering-the-ca-rotation>Triggering the CA Rotation</h3><ul><li>Triggered via <code>gardener.cloud/operation</code> annotation in symmetry with other operations like reconciliation, kubeconfig rotation, etc.<ul><li>annotation increases the generation</li><li>value for triggering first phase: <code>start-ca-rotation</code></li><li>value for triggering the second phase: <code>complete-ca-rotation</code></li><li><code>gardener-apiserver</code> performs the needful validation: user can&rsquo;t trigger another rotation if one is already in progress, user can&rsquo;t trigger <code>complete-ca-rotation</code> if first phase has not been compeleted, etc.</li></ul></li><li>The annotation triggers a usual shoot reconciliation (just like a kubeconfig or SSH key rotation)</li><li>gardenlet begins the CA rotation sequence by setting the new status section <code>.status.credentials.caRotation</code> (probably in <code>updateShootStatusOperationStart</code>) and removes the annotation afterwards<ul><li>shoot reconciliation needs to be idemptotent to CA rotation phase, i.e. if a usual reconciliation or maintenance operation is triggered in between, no new CAs are generated or similar things that would interfere with the CA rotation sequence</li></ul></li></ul><h3 id=changing-the-shoot-status>Changing the Shoot Status</h3><p>A new section in the Shoot status is added when the first rotation is triggered:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  credentials:
</span></span><span style=display:flex><span>    rotation:
</span></span><span style=display:flex><span>      certificateAuthorities:
</span></span><span style=display:flex><span>        phase: Prepare <span style=color:green># Prepare|Finalize|Completed</span>
</span></span><span style=display:flex><span>        lastCompletion: 2022-02-07T14:23:44Z
</span></span><span style=display:flex><span>    <span style=color:green># kubeconfig:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   phase:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   lastCompletion:</span>
</span></span></code></pre></div><p>Later on, this section could be augmented with other information like the names of the credentials secrets (e.g. <a href=https://github.com/gardener/gardener/issues/1749>gardener/gardener#1749</a>)</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  credentials:
</span></span><span style=display:flex><span>    resources:
</span></span><span style=display:flex><span>    - type: kubeconfig
</span></span><span style=display:flex><span>      kind: Secret
</span></span><span style=display:flex><span>      name: shoot-foo.kubeconfig
</span></span></code></pre></div><h3 id=rotation-sequence-for-cluster-and-client-ca>Rotation Sequence for Cluster and Client CA</h3><p>The proposal section includes a detailed description of all steps involved for rotating from a given <code>CA0</code> to the target <code>CA1</code>.</p><p><code>t0</code>: Today&rsquo;s situation</p><ul><li><code>kube-apiserver</code> uses SERVER CERT signed by <code>CA0</code> and trusts CLIENT CERTS signed by <code>CA0</code></li><li><code>kube-controller-manager</code> issues new CLIENT CERTS signed by <code>CA0</code></li><li>kubeconfig trusts only <code>CA0</code></li><li><code>ServiceAccount</code> secrets trust only <code>CA0</code></li><li>kubelet uses CLIENT CERT signed by <code>CA0</code></li></ul><p><code>t1</code>: Shoot owner triggers first step of CA rotation process (&ndash;> phase one is started):</p><ul><li>Generate <code>CA1</code></li><li>Generate <code>CLIENT_CA1</code></li><li>Update <code>kube-apiserver</code>, <code>kube-scheduler</code>, etc. to trust CLIENT CERTS signed by both <code>CA0</code> and <code>CLIENT_CA1</code> (<code>--client-ca-file</code> flag)</li><li>Update <code>kube-controller-manager</code> to issue new CLIENT CERTS now with <code>CLIENT_CA1</code></li><li>Update kubeconfig so that its CA bundle contains both <code>CA0</code> and<code>CA1</code> (if kubeconfig still contains a legacy CLIENT CERT then rotate the kubeconfig)</li><li>Update <code>generic-token-kubeconfig</code> so that its CA bundle contains both <code>CA0</code> and<code>CA1</code></li><li>Update <code>kube-controller-manager</code> to populate both <code>CA0</code> and <code>CA1</code> in <code>ServiceAccount</code> secrets.</li><li>Restart control plane components so that their CA bundle contains both <code>CA0</code> and <code>CA1</code></li><li>Renew CLIENT CERTS (sign them with <code>CLIENT_CA1</code>) for the following control plane components: Prometheus, DWD, legacy VPN), if not dropped already in the context of <a href=https://github.com/gardener/gardener/issues/4661>gardener/gardener#4661</a></li><li>Trigger node rollout<ul><li>This issues new CLIENT CERTS for all kubelets signed by <code>CLIENT_CA1</code></li><li>This restarts all <code>Pod</code>s and propagates <code>CA0</code> and <code>CA1</code> into their mounted <code>ServiceAccount</code> secrets (note CAs can not be reloaded by go client, therefore we need a restart of pods.)</li></ul></li><li><em>Ask user to exchange all their client credentials (kubeconfig, CLIENT CERTS issued by <code>CertificateSigningRequest</code>s) to trust both CA0 and CA1</em></li></ul><p><code>t2</code>: Shoot owner triggers second step of CA rotation process (&ndash;> phase two is started):</p><p>Prerequisite: All Gardener-controlled actions listed in t1 were executed successfully (for example node rollout). The shoot owner has guaranteed that they exchanged their client credentials and triggered step 2 via an annotation.</p><ul><li>Renew SERVER CERTS (sign them with <code>CA1</code>) for <code>kube-apiserver</code>, <code>kube-controller-manager</code>, <code>cloud-controller-manager</code> etc.</li><li>Update <code>kube-apiserver</code>, <code>kube-scheduler</code>, etc. to trust only CLIENT CERTS signed by <code>CLIENT_CA1</code></li><li>Update kubeconfig so that its CA bundle contains only <code>CA1</code></li><li>Update <code>generic-token-kubeconfig</code> so that its CA bundle contains only <code>CA1</code></li><li>Update <code>kube-controller-manager</code> to only contain CA1. <code>ServiceAccount</code> secrets created after this point will get secrets that include only <code>CA1</code></li><li>Restart control plane components so that their CA bundle contains only <code>CA1</code></li><li>Restart kubelets so that the CA bundle in their kubeconfigs contain only <code>CA1</code></li><li>Delete <code>CA0</code></li><li><em>Ask user to optionally restart their <code>Pod</code>s since they still contain <code>CA0</code> in memory in order to eliminate trust to the old cluster CA.</em></li><li><em>Ask user to exchange all their client credentials (download kubeconfig containing only <code>CA1</code>; when using CLIENT CERTS trust only <code>CA1</code>)</em></li></ul><h3 id=rotation-sequence-of-other-cas>Rotation Sequence of Other CAs</h3><p>Apart from the kube-apiserver CA (and the client CA) we also use 5 other CAs as mentioned above in the gardener codebase. We propose to rotate those CAs together with the kube-apiserver CA following the same trigger.</p><p>ℹ️ Note for the front-proxy CA: users need to make sure, extension API servers have reloaded the <code>extension-apiserver-authentication</code> ConfigMap, before triggering the second phase.</p><p>You can find gardener managed CAs listed <a href=https://github.com/gardener/gardener/blob/04d2b3f459d198e8db0ab57180ca2fea18e84da9/pkg/operation/botanist/wanted_secrets.go#L48>here</a>.</p><p>Regarding the rotation steps we want to follow a similar approach to the one we defined for the kube-apiserver CA. Exemplary, we are going to show the timeline for ETCD_CA but the logic should be similiar for all the above listed CAs.</p><ul><li><code>t0</code><ul><li>etcd trusts client certificates signed by <code>ETCD_CA0</code> and uses a server certificate signed by <code>ETCD_CA0</code></li><li><code>kube-apiserver</code> and <code>backup-restore</code> use a client certificate signed by <code>ETCD_CA0</code> and trust <code>ETCD_CA0</code></li></ul></li><li><code>t1</code>:<ul><li>Generate <code>ETCD_CA1</code></li><li>Update <code>etcd</code> to trust CLIENT CERTS signed by both <code>ETCD_CA0</code> and <code>ETCD_CA1</code></li><li>Update <code>kube-apiserver</code> and <code>backup-restore</code>:<ul><li>Adapt CA bundle to trust both <code>ETCD_CA0</code> and <code>ETCD_CA1</code></li><li>Renew CLIENT CERTS (sign them with <code>ETCD_CA1</code>)</li></ul></li></ul></li><li><code>t2</code>:<ul><li>Update <code>etcd</code>:<ul><li>Trust only CLIENT CERTS signed by <code>ETCD_CA1</code></li><li>Renew SERVER CERT (sign it with <code>ETCD_CA1</code>)</li></ul></li><li>Update <code>kube-apiserver</code> and <code>backup-restore</code> so that their CA bundle contains only <code>ETCD_CA1</code></li></ul></li></ul><p>ℹ️ This means we are requiring two restarts of etcd in total.</p><h2 id=alternatives>Alternatives</h2><p>This section presents a different approach to rotate the CAs which is to <em>temporarily create a second set of api-servers utilizing the new CA</em> . After presenting the approach advantages and disadvantages of both approaches are listed.</p><p><code>t0</code>: Today&rsquo;s situation</p><ul><li><code>kube-apiserver</code> uses SERVER CERT signed by <code>CA0</code> and trusts CLIENT CERTS signed by <code>CA0</code></li><li><code>kube-controller-manager</code> issues new CLIENT CERTS with <code>CA0</code></li><li>kubeconfig contains only <code>CA0</code></li><li><code>ServiceAccount</code> secrets contain only <code>CA0</code></li><li>kubelet uses CLIENT CERT signed by <code>CA0</code></li></ul><p><code>t1</code>: User triggers first step of CA rotation process (&ndash;> phase one):</p><ul><li>Generate <code>CA1</code></li><li>Generate <code>CLIENT_CA1</code></li><li>Create new <code>DNSRecord</code>, <code>Service</code>, Istio configuration, etc. for second <code>kube-apiserver</code> deployment</li><li>Deploy second <code>kube-apiserver</code> deployment trusting only CLIENT CERTS signed by <code>CLIENT_CA1</code> and using SERVER CERT signed by <code>CA1</code></li><li>Update <code>kube-scheduler</code>, etc. to trust only CLIENT CERTS signed by <code>CLIENT_CA1</code> (<code>--client-ca-file</code> flag)</li><li>Update <code>kube-controller-manager</code> to issue new CLIENT CERTS with <code>CLIENT_CA1</code></li><li>Update kubeconfig so that it points to the new <code>DNSRecord</code> and its CA bundle contains only <code>CA1</code> (if kubeconfig still contains a legacy CLIENT CERT then rotate the kubeconfig)</li><li>Update <code>ServiceAccount</code> secrets so that their CA bundle contains both <code>CA0</code> and <code>CA1</code></li><li>Restart control plane components so that they point to the second <code>kube-apiserver</code> <code>Service</code> and so that their CA bundle contains only <code>CA1</code></li><li>Renew CLIENT CERTS (sign them with <code>CLIENT_CA1</code>) for control plane components (Prometheus, DWD, legacy VPN) and point them to the second <code>kube-apiserver</code> <code>Service</code></li><li>Adapt <code>apiserver-proxy-pod-mutator</code> to point <code>KUBERNETES_SERVICE_HOST</code> env variable to second <code>kube-apiserver</code></li><li>Trigger node rollout<ul><li>This issues new CLIENT CERTS for all kubelets signed by <code>CLIENT_CA1</code> and points them to the second <code>DNSRecord</code></li><li>This restarts all <code>Pod</code>s and propagates <code>CA0</code> and <code>CA1</code> into their mounted <code>ServiceAccount</code> secrets</li></ul></li><li><em>Ask user to exchange all their client credentials (kubeconfig, CLIENT CERTS issued by <code>CertificateSigningRequest</code>s)</em></li></ul><p><code>t2</code>: User triggers second step of CA rotation process (&ndash;> phase two):</p><ul><li>Update <code>ServiceAccount</code> secrets so that their CA bundle contains only <code>CA1</code></li><li>Update <code>apiserver-proxy</code> to talk to second <code>kube-apiserver</code></li><li>Drop first <code>DNSRecord</code>, <code>Service</code>, Istio configuration and first <code>kube-apiserver</code> deployment</li><li>Drop <code>CA0</code></li><li><em>Ask user to optionally restart their <code>Pod</code>s since they still contain <code>CA0</code> in memory.</em></li></ul><h4 id=advantagesdisadvantages-approach-two-api-servers>Advantages/Disadvantages approach two api servers</h4><ul><li>(+) User needs to adapt client credentials only once</li><li>(/) Unstable API server domain</li><li>(-) Probably more implementation effort</li><li>(-) More complex</li><li>(-) CA rotation process does not work similar for all CAs in our system</li></ul><h4 id=advantagesdisadvantages-of-currently-preferred-approach-see-proposal>Advantages/Disadvantages of currently preferred approach (see proposal)</h4><ul><li>(+) Implementation effort seems &ldquo;straight-forward&rdquo;</li><li>(+) CA rotation process works similar for all CAs in our system</li><li>(/) Stable API server domain</li><li>(-) User needs to adapt client credentials twice</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a3aad4401fad43af26be493b4a8c8b0d>21 - Utilize API Server Network Proxy to Invert Seed-to-Shoot Connectivity</h1><h1 id=utilize-api-server-network-proxy-to-invert-seed-to-shoot-connectivity>Utilize API Server Network Proxy to Invert Seed-to-Shoot Connectivity</h1><ul><li><a href=#utilize-api-server-network-proxy-to-invert-seed-to-shoot-connectivity>Utilize API Server Network Proxy to Invert Seed-to-Shoot Connectivity</a><ul><li><a href=#problem>Problem</a></li><li><a href=#proposal>Proposal</a><ul><li><a href=#api-server-network-proxy>API Server Network Proxy</a></li></ul></li><li><a href=#challenges>Challenges</a><ul><li><a href=#prometheus-to-shoot-connectivity>Prometheus to Shoot connectivity</a><ul><li><a href=#possible-solutions>Possible Solutions</a></li><li><a href=#port-forwarder-sidecar>Port-forwarder Sidecar</a></li><li><a href=#proxy-client-sidecar>Proxy Client Sidecar</a></li><li><a href=#proxy-sub-resource>Proxy sub-resource</a></li></ul></li><li><a href=#proxy-server-loadbalancer-sharing-and-re-advertising>Proxy-server Loadbalancer Sharing and Re-advertising</a><ul><li><a href=#possible-solution>Possible Solution</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></li></ul></li></ul><h2 id=problem>Problem</h2><p>Gardener&rsquo;s architecture for Kubernetes clusters relies on having the control-plane (e.g., kube-apiserver, kube-scheduler, kube-controller-manager, etc.) and the data-plane (e.g., kube-proxy, kubelet, etc.) of the cluster residing in separate places, this provides many benefits but poses some challenges, especially when API-server to system components communication is required. This problem is solved today in Gardener by <a href=https://github.com/gardener/vpn>making use of OpenVPN</a> to establish a VPN connection from the seed to the shoot. To do so, the following steps are required:</p><ul><li>Create a Loadbalancer service on the shoot.</li><li>Add a sidecar to the API server pod which knows the address of the newly created Loadbalancer.</li><li>Establish a connection over the internet to the VPN Loadbalancer</li><li>Install additional iptables rules that would redirect all the IPs of the shoot (i.e., service, pod, node CIDRs) to the established VPN tunnel</li></ul><p>There are however quite a few problems with the above approach, here are some:</p><ul><li>Every shoot would require an additional loadbalancer, this accounts for addition overhead in terms of both costs and troubleshooting efforts.</li><li>Private access use-cases would not be possible without having a seed residing in the same private domain as a hard requirement. For example, have a look at <a href=https://github.com/gardener/gardener-extension-provider-gcp/issues/56>this issue</a></li><li>Providing a public endpoint to access components in the shoot poses a security risk.</li></ul><h2 id=proposal>Proposal</h2><p>There are mutliple ways to tackle the directional connectivity issue mentioned above, one way would be to invert the connection between the API server and the system components, i.e., instead of having the API server side-car establish a tunnel, we would have an agent residing in the shoot cluster initiate the connection itself. This way we don&rsquo;t need a Loadbalancer for every shoot and from the security perspective, there is no ingress from outside, only controlled egress.</p><p>We want to replace this:</p><p><code>APIServer | VPN-seed ---> internet ---> LB --> VPN-Shoot (4314) --> Pods | Nodes | Services</code></p><p>With this:</p><p><code>APIServer &lt;-> Proxy-Server &lt;--- internet &lt;--- Proxy-Agent --> Pods | Nodes | Services</code></p><h3 id=api-server-network-proxy>API Server Network Proxy</h3><p>To solve this issue we can utilize the <a href=https://github.com/kubernetes-sigs/apiserver-network-proxy>apiserver-network-proxy</a> upstream implementation. Which provides a reference implementation for a reverse streaming server. The way it works is as follows:</p><ul><li>Proxy agent connects to proxy server to establish a sticky connection.</li><li>Traffic to the proxy server (residing in the seed) gets then re-directed to the agent (residing in the shoot) which forwards the traffic to in-cluster components.</li></ul><p>The initial motivation for the apiserver-network-proxy project is to get rid of provider-specific implementations that reside in the API-server (e.g., SSH), but it turns out that
it has other interesting use-cases such as data-plane connection decoupling, which is the main use-case for this proposal.</p><p>Starting with <strong>Kubernetes 1.18</strong> it&rsquo;s possible to make use of an <code>--egress-selector-config-file</code> flag, this helps point the API-server to traffic hook points based on traffic direction. For example, in the config below the API server would have to forward all cluster related traffic (e.g., logs, port-forward, exec, &mldr;etc.) to the <strong>proxy-server</strong> which then knows how to forward traffic to the shoot. For the rest of the traffic, e.g. API server to ETCD or other control-plane components <code>direct</code> is used which means legacy routing method, i.e., by-pass the proxy.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  egress-selector-configuration.yaml: |-<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    apiVersion: apiserver.k8s.io/v1alpha1
</span></span></span><span style=display:flex><span><span style=color:#a31515>    kind: EgressSelectorConfiguration
</span></span></span><span style=display:flex><span><span style=color:#a31515>    egressSelections:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    - name: cluster
</span></span></span><span style=display:flex><span><span style=color:#a31515>      connection:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        proxyProtocol: httpConnect
</span></span></span><span style=display:flex><span><span style=color:#a31515>        transport:
</span></span></span><span style=display:flex><span><span style=color:#a31515>          tcp:
</span></span></span><span style=display:flex><span><span style=color:#a31515>            url: https://proxy-server:8131
</span></span></span><span style=display:flex><span><span style=color:#a31515>    - name: master
</span></span></span><span style=display:flex><span><span style=color:#a31515>      connection:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        proxyProtocol: direct
</span></span></span><span style=display:flex><span><span style=color:#a31515>    - name: etcd
</span></span></span><span style=display:flex><span><span style=color:#a31515>      connection:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        proxyProtocol: direct</span>    
</span></span></code></pre></div><h2 id=challenges>Challenges</h2><h3 id=prometheus-to-shoot-connectivity>Prometheus to Shoot connectivity</h3><p>One challenge remains to completely eliminate the need for a VPN connection. In today&rsquo;s Gardener setup, each control-plane has a Prometheus instance that directly scrapes cluster components such as CoreDNS, Kubelets, cadvisor, etc. This works because in addition to the VPN side car attached to the API server pod, we have another one attached to prometheus which knows how to forward traffic to these endpoints. Once the VPN is eliminated, it is required to find other means to forward traffic to these components.</p><h4 id=possible-solutions>Possible Solutions</h4><p>There are currently two ways to solve this problem:</p><ul><li>Attach a port-forwarder side-car to prometheus.</li><li>Utilize the proxy subresource on the API server.</li></ul><h4 id=port-forwarder-sidecar>Port-forwarder Sidecar</h4><p>With this solution each prometheus instance would have a side-car that has the kubeconfig of the shoot cluster, and which establishes a port-forward connection to the endpoints residing in the shoot.</p><p>There are a many problems with this approach:</p><ul><li>the port-forward connection is not reliable.</li><li>the connection would break if the API server instance dies.</li><li>requires an additional component.</li><li>would need to expose every pod / service via port-forward.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>Prom Pod (Prometheus -&gt; Port-forwarder) &lt;-&gt; APIServer -&gt; Proxy-server &lt;--- internet &lt;--- Proxy-Agent --&gt; Pods | Nodes | Services
</span></span></code></pre></div><h4 id=proxy-client-sidecar>Proxy Client Sidecar</h4><p>Another solution would be to implement a proxy-client as a sidecar for every component that wishes to communicate with the shoot cluster. For this to work, means to re-direct / inject that proxy to handle the component&rsquo;s traffic is necessary (e.g., additional IPtable rules).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>Prometheus Pod (Prometheus -&gt; Proxy) &lt;-&gt; Proxy-Server &lt;--- internet &lt;--- Proxy-Agent --&gt; Pods | Nodes | Services
</span></span></code></pre></div><p>The problem with this approach is that it requires an additional sidecar (along with traffic redirection) to be attached to every client that wishes to communicate with the shoot cluster, this can cause:</p><ul><li>additional maintenance efforts (extra code).</li><li>other side-effects (e.g., if istio sidecar injection is enabled)</li></ul><h4 id=proxy-sub-resource>Proxy sub-resource</h4><p>Kubernetes supports proxying requests to nodes, services, and pod endpoints in the shoot cluster. This proxy connection can be utilized for scraping the necessary endpoints in the shoot.</p><p>This approach requires less components and is more reliable than the port-forward solution, however, it relies on having the API server supporting proxied connection for the required endpoints.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>Prometheus  &lt;-&gt; APIServer &lt;-&gt; Proxy-Server &lt;--- internet &lt;--- Proxy-Agent --&gt; Pods | Nodes | Services
</span></span></code></pre></div><p>As simple as it is, it has a downside that it relies on the availability of the API server.</p><h3 id=proxy-server-loadbalancer-sharing-and-re-advertising>Proxy-server Loadbalancer Sharing and Re-advertising</h3><p>With the proxy-server in place, we need to provide means to enable the proxy-agent in the shoot to establish the connection with the server. As a result, we need to provide a public endpoint through which this channel of communication can be established, i.e., we need a Loadbalancer(s).</p><h4 id=possible-solution>Possible Solution</h4><p>Using a Loadbalancer / proxy server would not make sense since this is a pain-point we are trying to eliminate in the first-place, doing so just moves the costs to the control-plane. A possible solution is to communicate over a shared loadbalancer in the seed, similar to what has been proposed <a href=/docs/gardener/proposals/08-shoot-apiserver-via-sni/>here</a>, this way we can prevent the extra-costs for load-balancers.</p><p>With this in mind, we still have other pain-points, namely:</p><ul><li>Advertising Loadbalancer public IPs to the shoot.</li><li>Directing the traffic to the corresponding shoot proxy-server.</li></ul><p>For advertising the Loadbalancer IP, a DNS entry can be created for the proxy loadbalancer (or re-use the DNS entry for the SNI proxy), along with necessary certificates, which is then used to connect to the loadbalancer. At this point we can decide on either one of the two approaches:</p><ol><li>One Proxy / API server with a shared loadbalancer.</li><li>Use one proxy server for all agents.</li></ol><p>In the first case, we will probably need a proxy for the proxy-server that knows how to direct traffic to the correct proxy server based on the corresponding shoot cluster. In the second case, we don&rsquo;t need another proxy if the proxy server is cluster-aware, i.e., can pool and identify connections coming from the same cluster and peer them with the correct API. Unfortunately, the second case is not supported today.</p><h3 id=summary>Summary</h3><ul><li>API server proxy can be utilized to invert the connection (only for clusters >= 1.18, for older clusters the old VPN solution will remain).</li><li>This is achieved by utilizing the <code>--egress-selector-config-file</code> flag on the api-server.</li><li>For monitoring endpoints, the proxy subresources would be the preferable methods to go, but in the future we can also support sidecar proxies that can communicate with the proxy-server.</li><li>For Directing traffic to the correct proxy-server we will re-use the SNI proxy along with the load-balancer from <a href=/docs/gardener/proposals/08-shoot-apiserver-via-sni/>the shoot API server via SNI GEP</a>.</li></ul></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2022 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid@8.13.4/dist/mermaid.min.js integrity="sha512-JERecFUBbsm75UpkVheAuDOE8NdHjQBrPACfEQYPwvPG+fjgCpHAz1Jw2ci9EXmd3DdfiWth3O3CQvcfEg8gsA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.7b24c0fb082ffb2de6cb14d6c95e9f8053053709ffcf8c761ef8e9ad2f8021e4.js integrity="sha256-eyTA+wgv+y3myxTWyV6fgFMFNwn/z4x2HvjprS+AIeQ=" crossorigin=anonymous></script></body></html>