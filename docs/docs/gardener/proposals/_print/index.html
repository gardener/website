<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.83.1"><link rel=canonical type=text/html href=https://gardener.cloud/docs/gardener/proposals/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/gardener/proposals/index.xml><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=icon type=image/x-icon href=https://gardener.cloud/images/favicon.ico><link rel=icon type=image/png href=https://gardener.cloud/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://gardener.cloud/images/favicon-16x16.png sizes=16x16><title>Proposals | Gardener</title><meta name=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:title" content="Proposals"><meta property="og:description" content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/gardener/proposals/"><meta itemprop=name content="Proposals"><meta itemprop=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta name=twitter:card content="summary"><meta name=twitter:title content="Proposals"><meta name=twitter:description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><link rel=preload href=/scss/main.min.ca2e9ddee7809848b536632b41e4e4df665800778ffe11b75edde5bdd6c78963.css as=style><link href=/scss/main.min.ca2e9ddee7809848b536632b41e4e4df665800778ffe11b75edde5bdd6c78963.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://unpkg.com/lunr@2.3.8/lunr.min.js integrity=sha384-vRQ9bDyE0Wnu+lMfm57BlYLO0/XauFuKpVsZPs7KEDwYKktWi5+Kz3MP8++DFlRY crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.2035d9813dafac83fe2a48b18d50f237.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/gardener/proposals/>Return to the regular view of this page</a>.</p></div><h1 class=title>Proposals</h1><div class=content><h1 id=gardener-enhancement-proposal-gep>Gardener Enhancement Proposal (GEP)</h1><p>Changes to the Gardener code base are often incorporated directly via pull requests which either themselves contain a description about the motivation and scope of a change or a linked GitHub issue does.</p><p>If a perspective feature has a bigger extent, requires the involvement of several parties or more discussion is needed before the actual implementation can be started, you may consider filing a pull request with a Gardener Enhancement Proposal (GEP) first.</p><p>GEPs are a measure to propose a change or to add a feature to Gardener, help you to describe the change(s) conceptionally, and to list the steps that are necessary to reach this goal. It helps the Gardener maintainers as well as the community to understand the motivation and scope around your proposed change(s) and encourages their contribution to discussions and future pull requests. If you are familiar with the Kubernetes community, GEPs are analogue to Kubernetes Enhancement Proposals (<a href=https://github.com/kubernetes/enhancements/tree/master/keps>KEPs</a>).</p><h2 id=reasons-for-a-gep>Reasons for a GEP</h2><p>You may consider filing a GEP for the following reasons:</p><ul><li>A Gardener architectural change is intended / necessary</li><li>Major changes to the Gardener code base</li><li>A phased implementation approach is expected because of the widespread scope of the change</li><li>Your proposed changes may be controversial</li></ul><p>We encourage you to take a look at already merged <a href=https://github.com/gardener/gardener/tree/master/docs/proposals>GEPs</a> since they give you a sense of what a typical GEP comprises.</p><h2 id=before-creating-a-gep>Before creating a GEP</h2><p>Before starting your work and creating a GEP, please take some time to familiarize yourself with our
general <a href=https://gardener.cloud/docs/contribute/>Gardener Contribution Guidelines</a>.</p><p>It is recommended to discuss and outline the motivation of your prospective GEP as a draft with the community before you take the investment of creating the actual GEP. This early briefing supports the understanding for the broad community and leads to a fast feedback for your proposal from the respective experts in the community.
An appropriate format for this may be the regular <a href=https://gardener.cloud/docs/contribute/#bi-weekly-meetings>Gardener community meetings</a>.</p><h2 id=how-to-file-a-gep>How to file a GEP</h2><p>GEPs should be created as Markdown <code>.md</code> files and are submitted through a GitHub pull request to their current home in <a href=https://github.com/gardener/gardener/tree/master/docs/proposals>docs/proposals</a>. Please use the provided <a href=/docs/gardener/proposals/00-template/>template</a> or follow the structure of existing <a href=https://github.com/gardener/gardener/tree/master/docs/proposals>GEPs</a> which makes reviewing easier and faster. Additionally, please link the new GEP in our documentation <a href=https://github.com/gardener/gardener/blob/master/docs/README.md#Proposals>index</a>.</p><p>If not already done, please present your GEP in the <a href=https://gardener.cloud/docs/contribute/#bi-weekly-meetings>regular community meetings</a> to brief the community about your proposal (we strive for personal communication :) ). Also consider that this may be an important step to raise awareness and understanding for everyone involved.</p><p>The GEP template contains a small set of metadata, which is helpful for keeping track of the enhancement
in general and especially of who is responsible for implementing and reviewing PRs that are part of
the enhancement.</p><h3 id=main-reviewers>Main Reviewers</h3><p>Apart from general metadata, the GEP should name at least one &ldquo;main reviewer&rdquo;.
You can find a main reviewer for your GEP either when discussing the proposal in the community meeting, by asking in our
<a href=https://gardener.cloud/docs/contribute/#slack-channel>Slack Channel</a> or at latest during the GEP PR review.
New GEPs should only be accepted once at least one main reviewer is nominated/assigned.</p><p>The main reviewers are charged with the following tasks:</p><ul><li>familiarizing themselves with the details of the proposal</li><li>reviewing the GEP PR itself and any further updates to the document</li><li>discussing design details and clarifying implementation questions with the author before and after
the proposal was accepted</li><li>reviewing PRs related to the GEP in-depth</li></ul><p>Other community members are of course also welcome to help the GEP author, review his work and raise
general concerns with the enhancement. Nevertheless, the main reviewers are supposed to focus on more
in-depth reviews and accompaning the whole GEP process end-to-end, which helps with getting more
high-quality reviews and faster feedback cycles instead of having more people looking at the process
with lower priority and less focus.</p><h2 id=gep-process>GEP Process</h2><ol><li>Pre-discussions about GEP (if necessary)</li><li>Find a main reviewer for your enhancement</li><li>GEP is filed through GitHub PR</li><li>Presentation in Gardener community meeting (if possible)</li><li>Review of GEP from maintainers/community</li><li>GEP is merged if accepted</li><li>Implementation of GEP</li><li>Consider keeping GEP up-to-date in case implementation differs essentially</li></ol></div></div><div class=td-content style=page-break-before:always><h1 id=pg-a871ba89d724217f0cabd24297c97c72>1 - 01 Extensibility</h1><h1 id=gardener-extensibility-and-extraction-of-cloud-specificos-specific-knowledge-308httpsgithubcomgardenergardenerissues308-262httpsgithubcomgardenergardenerissues262>Gardener extensibility and extraction of cloud-specific/OS-specific knowledge (<a href=https://github.com/gardener/gardener/issues/308>#308</a>, <a href=https://github.com/gardener/gardener/issues/262>#262</a>)</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#summary>Summary</a></li><li><a href=#motivation>Motivation</a><ul><li><a href=#goals>Goals</a></li><li><a href=#non-goals>Non-Goals</a></li></ul></li><li><a href=#proposal>Proposal</a><ul><li><a href=#modification-of-existing-cloudprofile-and-shoot-resources>Modification of existing <code>CloudProfile</code> and <code>Shoot</code> resources</a><ul><li><a href=#cloudprofiles>CloudProfiles</a></li><li><a href=#shoots>Shoots</a></li></ul></li><li><a href=#crd-definitions-and-workflow-adaptation>CRD definitions and workflow adaptation</a><ul><li><a href=#custom-resource-definitions>Custom resource definitions</a><ul><li><a href=#dns-records>DNS records</a></li><li><a href=#infrastructure-provisioning>Infrastructure provisioning</a></li><li><a href=#backup-infrastructure-provisioning>Backup infrastructure provisioning</a></li><li><a href=#cloud-config-user-data-for-bootstrapping-machines>Cloud config (user-data) for bootstrapping machines</a></li><li><a href=#worker-pools-definition>Worker pools definition</a></li><li><a href=#generic-resources>Generic resources</a></li></ul></li><li><a href=#shoot-state>Shoot state</a></li><li><a href=#shoot-health-checksconditions>Shoot health checks/conditions</a></li><li><a href=#reconciliation-flow>Reconciliation flow</a></li><li><a href=#deletion-flow>Deletion flow</a></li></ul></li><li><a href=#gardenlet>Gardenlet</a></li><li><a href=#shoot-control-plane-movementmigration>Shoot control plane movement/migration</a></li></ul></li><li><a href=#registration-of-external-controllers-at-gardener>Registration of external controllers at Gardener</a></li><li><a href=#other-cloud-specific-parts>Other cloud-specific parts</a><ul><li><a href=#defaulting-and-validation-admission-plugins>Defaulting and validation admission plugins</a></li><li><a href=#dns-hosted-zone-admission-plugin>DNS Hosted Zone admission plugin</a></li><li><a href=#shoot-quota-admission-plugin>Shoot Quota admission plugin</a></li><li><a href=#shoot-maintenance-controller>Shoot maintenance controller</a></li></ul></li><li><a href=#alternatives>Alternatives</a></li></ul><h2 id=summary>Summary</h2><p>Gardener has evolved to a large compound of packages containing lots of highly specific knowledge which makes it very hard to extend (supporting a new cloud provider, new OS, &mldr;, or behave differently depending on the underlying infrastructure).</p><p>This proposal aims to move out the cloud-specific implementations (called &ldquo;(cloud) botanists&rdquo;) and the OS-specifics into dedicated controllers, and simultaneously to allow deviation from the standard Gardener deployment.</p><h2 id=motivation>Motivation</h2><p>Currently, it is too hard to support additional cloud providers or operation systems/distributions as everything must be done in-tree which might affect the implementation of other cloud providers as well.
The various conditions and branches make the code hard to maintain and hard to test.
Every change must be done centrally, requires to completely rebuild Gardener, and cannot be deployed individually. Similar to the motivation for Kubernetes to extract their cloud-specifics into dedicated cloud-controller-managers or to extract the container/storage/network/&mldr; specifics into CRI/CSI/CNI/&mldr;, we aim to do the same right now.</p><h3 id=goals>Goals</h3><ul><li>Gardener does not contain any cloud-specific knowledge anymore but defines a clear contract allowing external controllers (botanists) to support different environments (AWS, Azure, GCP, &mldr;).</li><li>Gardener does not contain any operation system-specific knowledge anymore but defines a clear contract allowing external controllers to support different operation systems/distributions (CoreOS, SLES, Ubuntu, &mldr;).</li><li>It shall become much easier to move control planes of Shoot clusters between Seed clusters (<a href=https://github.com/gardener/gardener/issues/232>#232</a>) which is a necessary requirement of an automated setup for the Gardener Ring (<a href=https://github.com/gardener/gardener/issues/233>#233</a>).</li></ul><h3 id=non-goals>Non-Goals</h3><ul><li>We want to also factor out the specific knowledge of the addon deployments (nginx-ingress, kubernetes-dashboard, &mldr;), but we already have dedicated projects/issues for that: <a href=https://github.com/gardener/bouquet>https://github.com/gardener/bouquet</a> and <a href=https://github.com/gardener/gardener/issues/246>#246</a>. We will keep the addons in-tree as part of this proposal and tackle their extraction separately.</li><li>We do not want to make the Gardener a plain workflow engine that just executes a given template (which indeed would allow to be generic, open, and extensible in their highest forms but which would end-up in building a &ldquo;programming/scripting language&rdquo; inside a serialization format (YAML/JSON/&mldr;)). Rather, we want to have well-defined contracts and APIs, keeping Gardener responsible for the clusters management.</li></ul><h2 id=proposal>Proposal</h2><p>Gardener heavily relies on and implements Kubernetes principles, and its ultimate strategy is to use Kubernetes wherever applicable.
The extension concept in Kubernetes is based on (next to others) <code>CustomResourceDefinition</code>s, <code>ValidatingWebhookConfiguration</code>s and <code>MutatingWebhookConfiguration</code>s, and <code>InitializerConfiguration</code>s.
Consequently, Gardener&rsquo;s extensibility concept relies on these mechanisms.</p><p>Instead of implementing all aspects directly in Gardener it will deploy some CRDs to the Seed cluster which will be watched by dedicated controllers (also running in the Seed clusters), each one implementing one aspect of cluster management. This way one complex strongly coupled Gardener implementation covering all infrastructures is decomposed into a set of loosely coupled controllers implementing aspects of APIs defined by Gardener.
Gardener will just wait until the controllers report that they are done (or have faced an error) in the CRD&rsquo;s <code>.status</code> field instead of doing the respective tasks itself.
We will have one specific CRD for every specific operation (e.g., DNS, infrastructure provisioning, machine cloud config generation, &mldr;).
However, there are also parts inside Gardener which can be handled generically (not by cloud botanists) because they are the same or very similar for all the environments.
One example of those is the deployment of a <code>Namespace</code> in the Seed which will run the Shoot&rsquo;s control plane
Another one is the deployment of a <code>Service</code> for the Shoot&rsquo;s kube-apiserver.
In case a cloud botanist needs to cooperate and react on those operations it should register a <code>ValidatingWebhookConfiguration</code>, a <code>MutatingWebhookConfiguration</code>, or a <code>InitializerConfiguration</code>.
With this approach it can validate, modify, or react on any resource created by Gardener to make it cloud infrastructure specific.</p><p>The web hooks should be registered with <code>failurePolicy=Fail</code> to ensure that a request made by Gardener fails if the respective web hook is not available.</p><h3 id=modification-of-existing-cloudprofile-and-shoot-resources>Modification of existing <code>CloudProfile</code> and <code>Shoot</code> resources</h3><p>We will introduce the new API group <code>gardener.cloud</code>:</p><h4 id=cloudprofiles>CloudProfiles</h4><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>---
apiVersion: gardener.cloud/v1alpha1
kind: CloudProfile
metadata:
  name: aws
spec:
  type: aws
<span style=color:green># caBundle: |</span>
<span style=color:green>#   -----BEGIN CERTIFICATE-----</span>
<span style=color:green>#   ...</span>
<span style=color:green>#   -----END CERTIFICATE-----</span>
  dnsProviders:
  - type: aws-route53
  - type: unmanaged
  kubernetes:
    versions:
    - 1.12.1
    - 1.11.0
    - 1.10.5
  machineTypes:
  - name: m4.large
    cpu: <span style=color:#a31515>&#34;2&#34;</span>
    gpu: <span style=color:#a31515>&#34;0&#34;</span>
    memory: 8Gi
  <span style=color:green># storage: 20Gi   # optional (not needed in every environment, may only be specified if no volumeTypes have been specified)</span>
  ...
  volumeTypes:      <span style=color:green># optional (not needed in every environment, may only be specified if no machineType has a `storage` field)</span>
  - name: gp2
    class: standard
  - name: io1
    class: premium
  providerConfig:
    apiVersion: aws.cloud.gardener.cloud/v1alpha1
    kind: CloudProfileConfig
    constraints:
      minimumVolumeSize: 20Gi
      machineImages:
      - name: coreos
        regions:
        - name: eu-west-1
          ami: ami-32d1474b
        - name: us-east-1
          ami: ami-e582d29f
      zones:
      - region: eu-west-1
        zones:
        - name: eu-west-1a
          unavailableMachineTypes: <span style=color:green># list of machine types defined above that are not available in this zone</span>
          - name: m4.large
          unavailableVolumeTypes:  <span style=color:green># list of volume types defined above that are not available in this zone</span>
          - name: gp2
        - name: eu-west-1b
        - name: eu-west-1c
</code></pre></div><h4 id=shoots>Shoots</h4><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: gardener.cloud/v1alpha1
kind: Shoot
metadata:
  name: johndoe-aws
  namespace: garden-dev
spec:
  cloudProfileName: aws
  secretBindingName: core-aws
  cloud:
    type: aws
    region: eu-west-1
    providerConfig:
      apiVersion: aws.cloud.gardener.cloud/v1alpha1
      kind: InfrastructureConfig
      networks:
        vpc: <span style=color:green># specify either &#39;id&#39; or &#39;cidr&#39;</span>
        <span style=color:green># id: vpc-123456</span>
          cidr: 10.250.0.0/16
        internal:
        - 10.250.112.0/22
        public:
        - 10.250.96.0/22
        workers:
        - 10.250.0.0/19
      zones:
      - eu-west-1a
    workerPools:
    - name: pool-01
    <span style=color:green># Taints, labels, and annotations are not yet implemented. This requires interaction with the machine-controller-manager, see</span>
    <span style=color:green># https://github.com/gardener/machine-controller-manager/issues/174. It is only mentioned here as future proposal.</span>
    <span style=color:green># taints:</span>
    <span style=color:green># - key: foo</span>
    <span style=color:green>#   value: bar</span>
    <span style=color:green>#   effect: PreferNoSchedule</span>
    <span style=color:green># labels:</span>
    <span style=color:green># - key: bar</span>
    <span style=color:green>#   value: baz</span>
    <span style=color:green># annotations:</span>
    <span style=color:green># - key: foo</span>
    <span style=color:green>#   value: hugo</span>
      machineType: m4.large
      volume: <span style=color:green># optional, not needed in every environment, may only be specified if the referenced CloudProfile contains the volumeTypes field</span>
        type: gp2
        size: 20Gi
      providerConfig:
        apiVersion: aws.cloud.gardener.cloud/v1alpha1
        kind: WorkerPoolConfig
        machineImage:
          name: coreos
          ami: ami-d0dcef3
        zones:
        - eu-west-1a
      minimum: 2
      maximum: 2
      maxSurge: 1
      maxUnavailable: 0
  kubernetes:
    version: 1.11.0
    ...
  dns:
    provider: aws-route53
    domain: johndoe-aws.garden-dev.example.com
  maintenance:
    timeWindow:
      begin: 220000+0100
      end: 230000+0100
    autoUpdate:
      kubernetesVersion: <span style=color:#00f>true</span>
  backup:
    schedule: <span style=color:#a31515>&#34;*/5 * * * *&#34;</span>
    maximum: 7
  addons:
    kube2iam:
      enabled: <span style=color:#00f>false</span>
    kubernetes-dashboard:
      enabled: <span style=color:#00f>true</span>
    cluster-autoscaler:
      enabled: <span style=color:#00f>true</span>
    nginx-ingress:
      enabled: <span style=color:#00f>true</span>
      loadBalancerSourceRanges: []
    kube-lego:
      enabled: <span style=color:#00f>true</span>
      email: john.doe@example.com
</code></pre></div><p>ℹ The specifications for the other cloud providers Gardener already has an implementation for looks similar.</p><h3 id=crd-definitions-and-workflow-adaptation>CRD definitions and workflow adaptation</h3><p>In the following we are outlining the CRD definitions which define the API between Gardener and the dedicated controllers.
After that we will take a look at the current <a href=https://github.com/gardener/gardener/blob/master/pkg/gardenlet/controller/shoot/shoot_control_reconcile.go>reconciliation</a>/<a href=https://github.com/gardener/gardener/blob/master/pkg/gardenlet/controller/shoot/shoot_control_delete.go>deletion</a> flow and describe how it would look like in case we would implement this proposal.</p><h4 id=custom-resource-definitions>Custom resource definitions</h4><p>Every CRD has a <code>.spec.type</code> field containing the respective instance of the dimension the CRD represents, e.g. the cloud provider, the DNS provider or the operation system name.
Moreover, the <code>.status</code> field must contain</p><ul><li><code>observedGeneration</code> (<code>int64</code>), a field indicating on which generation the controller last worked on.</li><li><code>state</code> (<code>*runtime.RawExtension</code>), a field which is not interpreted by Gardener but persisted; it should be treated opaque and only be used by the respective CRD-specific controller (it can store anything it needs to re-construct its own state).</li><li><code>lastError</code> (<code>object</code>), a field which is optional and only present if the last operation ended with an error state.</li><li><code>lastOperation</code> (<code>object</code>), a field which always exists and which indicates what the last operation of the controller was.</li><li><code>conditions</code> (<code>list</code>), a field allowing the controller to report health checks for its area of responsibility.</li></ul><p>Some CRDs might have a <code>.spec.providerConfig</code> or a <code>.status.providerStatus</code> field containing controller-specific information that is treated opaque by Gardener and will only be copied to dependent or depending CRDs.</p><h5 id=dns-records>DNS records</h5><p>Every Shoot needs two DNS records (or three, depending on whether nginx-ingress addon is enabled), one so-called &ldquo;internal&rdquo; record that Gardener uses in the kubeconfigs of the Shoot cluster&rsquo;s system components, and one so-called &ldquo;external&rdquo; record which is used in the kubeconfig provided to the user.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>---
apiVersion: dns.gardener.cloud/v1alpha1
kind: DNSProvider
metadata:
  name: alicloud
  namespace: default
spec:
  type: alicloud-dns
  secretRef:
    name: alicloud-credentials
  domains:
    include:
    - my.own.domain.com
---
apiVersion: dns.gardener.cloud/v1alpha1
kind: DNSEntry
metadata:
  name: dns
  namespace: default
spec:
  dnsName: dns.my.own.domain.com
  ttl: 600
  targets:
  - 8.8.8.8
status:
  observedGeneration: 4
  state: some-state
  lastError:
    lastUpdateTime: 2018-04-04T07:08:51Z
    description: some-error message
    codes:
    - ERR_UNAUTHORIZED
  lastOperation:
    lastUpdateTime: 2018-04-04T07:24:51Z
    progress: 70
    type: Reconcile
    state: Processing
    description: Currently provisioning ...
  conditions:
  - lastTransitionTime: 2018-07-11T10:18:25Z
    message: DNS record has been created and is available.
    reason: RecordResolvable
    status: <span style=color:#a31515>&#34;True&#34;</span>
    type: Available
    propagate: <span style=color:#00f>false</span>
  providerStatus:
    apiVersion: aws.extensions.gardener.cloud/v1alpha1
    kind: DNSStatus
    ...
</code></pre></div><h5 id=infrastructure-provisioning>Infrastructure provisioning</h5><p>The <code>Infrastructure</code> CRD contains the information about VPC, networks, security groups, availability zones, &mldr;, basically, everything that needs to be prepared before an actual VMs/load balancers/&mldr; can be provisioned.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>---
apiVersion: extensions.gardener.cloud/v1alpha1
kind: Infrastructure
metadata:
  name: infrastructure
  namespace: shoot--core--aws-01
spec:
  type: aws
  providerConfig:
    apiVersion: aws.extensions.gardener.cloud/v1alpha1
    kind: InfrastructureConfig
    networks:
      vpc:
        cidr: 10.250.0.0/16
      internal:
      - 10.250.112.0/22
      public:
      - 10.250.96.0/22
      workers:
      - 10.250.0.0/19
    zones:
    - eu-west-1a
  dns:
    apiserver: api.aws-01.core.example.com
  region: eu-west-1
  secretRef:
    name: my-aws-credentials
  sshPublicKey: |<span style=color:#a31515>
</span><span style=color:#a31515>    </span>    base64(key)
status:
  observedGeneration: ...
  state: ...
  lastError: ..
  lastOperation: ...
  providerStatus:
    apiVersion: aws.extensions.gardener.cloud/v1alpha1
    kind: InfrastructureStatus
    vpc:
      id: vpc-1234
      subnets:
      - id: subnet-acbd1234
        name: workers
        zone: eu-west-1
      securityGroups:
      - id: sg-xyz12345
        name: workers
    iam:
      nodesRoleARN: &lt;some-arn&gt;
      instanceProfileName: foo
    ec2:
      keyName: bar
</code></pre></div><h5 id=backup-infrastructure-provisioning>Backup infrastructure provisioning</h5><p>The <code>BackupInfrastructure</code> CRD in the Seeds tells the cloud-specific controller to prepare a blob store bucket/container which can later be used to store etcd backups.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>---
apiVersion: extensions.gardener.cloud/v1alpha1
kind: BackupInfrastructure
metadata:
  name: etcd-backup
  namespace: shoot--core--aws-01
spec:
  type: aws
  region: eu-west-1
  storageContainerName: asdasjndasd-1293912378a-2213
  secretRef:
    name: my-aws-credentials
status:
  observedGeneration: ...
  state: ...
  lastError: ..
  lastOperation: ...
</code></pre></div><h5 id=cloud-config-user-data-for-bootstrapping-machines>Cloud config (user-data) for bootstrapping machines</h5><p>Gardener will continue to keep knowledge about the content of the cloud config scripts, but it will hand over it to the respective OS-specific controller which will generate the specific valid representation.
Gardener creates two <code>MachineCloudConfig</code> CRDs, one for the cloud-config-downloader (which will later flow into the <code>WorkerPool</code> CRD) and one for the real cloud-config (which will be stored as a <code>Secret</code> in the Shoot&rsquo;s <code>kube-system</code> namespace, and downloaded and executed from the cloud-config-downloader on the machines).</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>---
apiVersion: extensions.gardener.cloud/v1alpha1
kind: MachineCloudConfig
metadata:
  name: pool-01-downloader
  namespace: shoot--core--aws-01
spec:
  type: CoreOS
  units:
  - name: cloud-config-downloader.service
    command: start
    enable: <span style=color:#00f>true</span>
    content: |<span style=color:#a31515>
</span><span style=color:#a31515>      [Unit]
</span><span style=color:#a31515>      Description=Downloads the original cloud-config from Shoot API Server and executes it
</span><span style=color:#a31515>      After=docker.service docker.socket
</span><span style=color:#a31515>      Wants=docker.socket
</span><span style=color:#a31515>      [Service]
</span><span style=color:#a31515>      Restart=always
</span><span style=color:#a31515>      RestartSec=30
</span><span style=color:#a31515>      EnvironmentFile=/etc/environment
</span><span style=color:#a31515>      ExecStart=/bin/sh /var/lib/cloud-config-downloader/download-cloud-config.sh</span>      
  files:
  - path: /var/lib/cloud-config-downloader/credentials/kubeconfig
    permissions: 0644
    content:
      secretRef:
        name: cloud-config-downloader
        dataKey: kubeconfig
  - path: /var/lib/cloud-config-downloader/download-cloud-config.sh
    permissions: 0644
    content:
      inline:
        encoding: b64
        data: IyEvYmluL2Jhc2ggL...
status:
  observedGeneration: ...
  state: ...
  lastError: ..
  lastOperation: ...
  cloudConfig: | <span style=color:green># base64-encoded</span>
    <span style=color:green>#cloud-config</span>

    coreos:
      update:
        reboot-strategy: <span style=color:#00f>off</span>
      units:
      - name: cloud-config-downloader.service
        command: start
        enable: <span style=color:#00f>true</span>
        content: |<span style=color:#a31515>
</span><span style=color:#a31515>          [Unit]
</span><span style=color:#a31515>          Description=Downloads the original cloud-config from Shoot API Server and execute it
</span><span style=color:#a31515>          After=docker.service docker.socket
</span><span style=color:#a31515>          Wants=docker.socket
</span><span style=color:#a31515>          [Service]
</span><span style=color:#a31515>          Restart=always
</span><span style=color:#a31515>          RestartSec=30
</span><span style=color:#a31515>          ...</span>          
</code></pre></div><p>ℹ The cloud-config-downloader script does not only download the cloud-config initially but at regular intervals, e.g., every <code>30s</code>.
If it sees an updated cloud-config then it applies it again by reloading and restarting all systemd units in order to reflect the changes.
The way how this reloading of the cloud-config happens is OS-specific as well and not known to Gardener anymore, however, it must be part of the script already.
On CoreOS, you have to execute <code>/usr/bin/coreos-cloudinit --from-file=&lt;path></code> whereas on SLES you execute <code>cloud-init --file &lt;path> single -n write_files --frequency=once</code>.
As Gardener doesn&rsquo;t know these commands it will write a placeholder expression instead (e.g., <code>{RELOAD-CLOUD-CONFIG-WITH-PATH:&lt;path>}</code>) and the OS-specific controller is asked to replace it with the proper expression.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>---
apiVersion: extensions.gardener.cloud/v1alpha1
kind: MachineCloudConfig
metadata:
  name: pool-01-original <span style=color:green># stored as secret and downloaded later</span>
  namespace: shoot--core--aws-01
spec:
  type: CoreOS
  units:
  - name: docker.service
    drop-ins:
    - name: 10-docker-opts.conf
      content: |<span style=color:#a31515>
</span><span style=color:#a31515>        [Service]
</span><span style=color:#a31515>        Environment=&#34;DOCKER_OPTS=--log-opt max-size=60m --log-opt max-file=3&#34;</span>        
  - name: docker-monitor.service
    command: start
    enable: <span style=color:#00f>true</span>
    content: |<span style=color:#a31515>
</span><span style=color:#a31515>      [Unit]
</span><span style=color:#a31515>      Description=Docker-monitor daemon
</span><span style=color:#a31515>      After=kubelet.service
</span><span style=color:#a31515>      [Service]
</span><span style=color:#a31515>      Restart=always
</span><span style=color:#a31515>      EnvironmentFile=/etc/environment
</span><span style=color:#a31515>      ExecStart=/opt/bin/health-monitor docker</span>      
  - name: kubelet.service
    command: start
    enable: <span style=color:#00f>true</span>
    content: |<span style=color:#a31515>
</span><span style=color:#a31515>      [Unit]
</span><span style=color:#a31515>      Description=kubelet daemon
</span><span style=color:#a31515>      Documentation=https://kubernetes.io/docs/admin/kubelet
</span><span style=color:#a31515>      After=docker.service
</span><span style=color:#a31515>      Wants=docker.socket rpc-statd.service
</span><span style=color:#a31515>      [Service]
</span><span style=color:#a31515>      Restart=always
</span><span style=color:#a31515>      RestartSec=10
</span><span style=color:#a31515>      EnvironmentFile=/etc/environment
</span><span style=color:#a31515>      ExecStartPre=/bin/docker run --rm -v /opt/bin:/opt/bin:rw k8s.gcr.io/hyperkube:v1.11.2 cp /hyperkube /opt/bin/
</span><span style=color:#a31515>      ExecStartPre=/bin/sh -c &#39;hostnamectl set-hostname $(cat /etc/hostname | cut -d &#39;.&#39; -f 1)&#39;
</span><span style=color:#a31515>      ExecStart=/opt/bin/hyperkube kubelet \
</span><span style=color:#a31515>          --allow-privileged=true \
</span><span style=color:#a31515>          --bootstrap-kubeconfig=/var/lib/kubelet/kubeconfig-bootstrap \
</span><span style=color:#a31515>          ...</span>      
  files:
  - path: /var/lib/kubelet/ca.crt
    permissions: 0644
    content:
      secretRef:
        name: ca-kubelet
        dataKey: ca.crt
  - path: /var/lib/cloud-config-downloader/download-cloud-config.sh
    permissions: 0644
    content:
      inline:
        encoding: b64
        data: IyEvYmluL2Jhc2ggL...
  - path: /etc/sysctl.d/99-k8s-general.conf
    permissions: 0644
    content:
      inline:
        data: |<span style=color:#a31515>
</span><span style=color:#a31515>          vm.max_map_count = 135217728
</span><span style=color:#a31515>          kernel.softlockup_panic = 1
</span><span style=color:#a31515>          kernel.softlockup_all_cpu_backtrace = 1
</span><span style=color:#a31515>          ...</span>          
  - path: /opt/bin/health-monitor
    permissions: 0755
    content:
      inline:
        data: |<span style=color:#a31515>
</span><span style=color:#a31515>          #!/bin/bash
</span><span style=color:#a31515>          set -o nounset
</span><span style=color:#a31515>          set -o pipefail
</span><span style=color:#a31515>
</span><span style=color:#a31515>          function docker_monitoring {
</span><span style=color:#a31515>          ...</span>          
status:
  observedGeneration: ...
  state: ...
  lastError: ..
  lastOperation: ...
  cloudConfig: ...
</code></pre></div><p>Cloud-specific controllers which might need to add another kernel option or another flag to the kubelet, maybe even another file to the disk, can register a <code>MutatingWebhookConfiguration</code> to that resource and modify it upon creation/update.
The task of the <code>MachineCloudConfig</code> controller is to only generate the OS-specific cloud-config based on the <code>.spec</code> field, but not to add or change any logic related to Shoots.</p><h5 id=worker-pools-definition>Worker pools definition</h5><p>For every worker pool defined in the <code>Shoot</code> Gardener will create a <code>WorkerPool</code> CRD which shall be picked up by a cloud-specific controller and be translated to <code>MachineClass</code>es and <code>MachineDeployment</code>s.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>---
apiVersion: extensions.gardener.cloud/v1alpha1
kind: WorkerPool
metadata:
  name: pool-01
  namespace: shoot--core--aws-01
spec:
  cloudConfig: base64(downloader-cloud-config)
  infrastructureProviderStatus:
    apiVersion: aws.extensions.gardener.cloud/v1alpha1
    kind: InfrastructureStatus
    vpc:
      id: vpc-1234
      subnets:
      - id: subnet-acbd1234
        name: workers
        zone: eu-west-1
      securityGroups:
      - id: sg-xyz12345
        name: workers
    iam:
      nodesRoleARN: &lt;some-arn&gt;
      instanceProfileName: foo
    ec2:
      keyName: bar
  providerConfig:
    apiVersion: aws.cloud.gardener.cloud/v1alpha1
    kind: WorkerPoolConfig
    machineImage:
      name: CoreOS
      ami: ami-d0dcef3b
    machineType: m4.large
    volumeType: gp2
    volumeSize: 20Gi
    zones:
    - eu-west-1a
  region: eu-west-1
  secretRef:
    name: my-aws-credentials
  minimum: 2
  maximum: 2
status:
  observedGeneration: ...
  state: ...
  lastError: ..
  lastOperation: ...
</code></pre></div><h5 id=generic-resources>Generic resources</h5><p>Some components are cloud-specific and must be deployed by the cloud-specific botanists.
Others might need to deploy another pod next to the shoot&rsquo;s control plane or must do anything else.
Some of these might be important for a functional cluster (e.g., the cloud-controller-manager, or a CSI plugin in the future), and controllers should be able to report errors back to the user.
Consequently, in order to trigger the controllers to deploy these components Gardener would write a <code>Generic</code> CRD to the Seed to trigger the deployment.
No operation is depending on the status of these resources, however, the entire reconciliation flow is.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>---
apiVersion: extensions.gardener.cloud/v1alpha1
kind: Generic
metadata:
  name: cloud-components
  namespace: shoot--core--aws-01
spec:
  type: cloud-components
  secretRef:
    name: my-aws-credentials
  shootSpec:
    ...
status:
  observedGeneration: ...
  state: ...
  lastError: ..
  lastOperation: ...
</code></pre></div><h4 id=shoot-state>Shoot state</h4><p>In order to enable moving the control plane of a Shoot between Seed clusters (e.g., if a Seed cluster is not available anymore or entirely broken) Gardener must store some non-reconstructable state, potentially also the state written by the controllers.
Gardener watches these extension CRDs and copies the <code>.status.state</code> in a <code>ShootState</code> resource into the Garden cluster.
Any observed status change of the respective CRD-controllers must be immediately reflected in the <code>ShootState</code> resource.
The contract between Gardener and those controllers is: <strong>Every controller must be capable of reconstructing its own environment based on both the state it has written before and on the real world&rsquo;s conditions/state.</strong></p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>---
apiVersion: gardener.cloud/v1alpha1
kind: ShootState
metadata:
  name: shoot--core--aws-01
shootRef:
  name: aws-01
  project: core
state:
  secrets:
  - name: ca
    data: ...
  - name: kube-apiserver-cert
    data: ...
  resources:
  - kind: DNS
    name: record-1
    state: &lt;copied-state-of-dns-crd&gt;
  - kind: Infrastructure
    name: networks
    state: &lt;copied-state-of-infrastructure-crd&gt;
  ...
  &lt;other fields required to keep track of&gt;
</code></pre></div><p>We cannot assume that Gardener is always online to observe the most recent states the controllers have written to their resources.
Consequently, the information stored here must not be used as &ldquo;single point of truth&rdquo;, but the controllers must potentially check the real world&rsquo;s status to reconstruct themselves.
However, this must anyway be part of their normal reconciliation logic and is a general best practice for Kubernetes controllers.</p><h4 id=shoot-health-checksconditions>Shoot health checks/conditions</h4><p>Some of the existing conditions already contain specific code which shall be simplified as well.
All of the CRDs described above have a <code>.status.conditions</code> field to which the controllers may write relevant health information of their function area.
Gardener will pick them up and copy them over to the Shoots <code>.status.conditions</code> (only those conditions setting <code>propagate=true</code>).</p><h4 id=reconciliation-flow>Reconciliation flow</h4><p>We are now examining the current Shoot creation/reconciliation flow and describe how it could look like when applying this proposal:</p><table><thead><tr><th>Operation</th><th>Description</th></tr></thead><tbody><tr><td>botanist.DeployNamespace</td><td>Gardener creates the namespace for the Shoot in the Seed cluster.</td></tr><tr><td>botanist.DeployKubeAPIServerService</td><td>Gardener creates a Service of type <code>LoadBalancer</code> in the Seed.<br>AWS Botanist registers a Mutating Webhook and adds its AWS-specific annotation.</td></tr><tr><td>botanist.WaitUntilKubeAPIServerServiceIsReady</td><td>Gardener checks the <code>.status</code> object of the just created <code>Service</code> in the Seed. The contract is that also clouds not supporting load balancers must react on the <code>Service</code> object and modify the <code>.status</code> to correctly reflect the kube-apiserver&rsquo;s ingress IP.</td></tr><tr><td>botanist.DeploySecrets</td><td>Gardener creates the secrets/certificates it needs like it does today, but it provides utility functions that can be adopted by Botanists/other controllers if they need additional certificates/secrets created on their own. (We should also add labels to all secrets)</td></tr><tr><td>botanist.Shoot.Components.DNS.Internal{Provider/Entry}.Deploy</td><td>Gardener creates a DNS-specific CRD in the Seed, and the responsible DNS-controller picks it up and creates a corresponding DNS record (see CRD specification above).</td></tr><tr><td>botanist.Shoot.Components.DNS.External{Provider/Entry}.Deploy</td><td>Gardener creates a DNS-specific CRD in the Seed, and the responsible DNS-controller picks it up and creates a corresponding DNS record: (see CRD specification above).</td></tr><tr><td>shootCloudBotanist.DeployInfrastructure</td><td>Gardener creates a Infrastructure-specific CRD in the Seed, and the responsible Botanist picks it up and does its job: (see CRD above).</td></tr><tr><td>botanist.DeployBackupInfrastructure</td><td>Gardener creates a <code>BackupInfrastructure</code> resource in the Garden cluster.<br>(The BackupInfrastructure controller creates a BackupInfrastructure-specific CRD in the Seed, and the responsible Botanist picks it up and does its job: (see CRD above).)</td></tr><tr><td>botanist.WaitUntilBackupInfrastructureReconciled</td><td>Gardener checks the <code>.status</code> object of the just created <code>BackupInfrastructure</code> resource.</td></tr><tr><td>hybridBotanist.DeployETCD</td><td>Gardener does only deploy the etcd <code>StatefulSet</code> without backup-restore sidecar at all.<br>The cloud-specific Botanist registers a Mutating Webhook and adds the backup-restore sidecar, and it also creates the <code>Secret</code> needed by the backup-restore sidecar.</td></tr><tr><td>botanist.WaitUntilEtcdReady</td><td>Gardener checks the <code>.status</code> object of the etcd <code>Statefulset</code> and waits until readiness is indicated.</td></tr><tr><td>hybridBotanist.DeployCloudProviderConfig</td><td>Gardener does not execute this anymore because it doesn&rsquo;t know anything about cloud-specific configuration.</td></tr><tr><td>hybridBotanist.DeployKubeAPIServer</td><td>Gardener does only deploy the kube-apiserver <code>Deployment</code> without any cloud-specific flags/configuration.<br>The cloud-specific Botanist registers a Mutating Webhook and adds whatever is needed for the kube-apiserver to run in its cloud environment.</td></tr><tr><td>hybridBotanist.DeployKubeControllerManager</td><td>Gardener does only deploy the kube-controller-manager <code>Deployment</code> without any cloud-specific flags/configuration.<br>The cloud-specific Botanist registers a Mutating Webhook and adds whatever is needed for the kube-controller-manager to run in its cloud environment (e.g., the cloud-config).</td></tr><tr><td>hybridBotanist.DeployKubeScheduler</td><td>Gardener does only deploy the kube-scheduler <code>Deployment</code> without any cloud-specific flags/configuration.<br>The cloud-specific Botanist registers a Mutating Webhook and adds whatever is needed for the kube-scheduler to run in its cloud environment.</td></tr><tr><td>hybridBotanist.DeployCloudControllerManager</td><td>Gardener does not execute this anymore because it doesn&rsquo;t know anything about cloud-specific configuration. The Botanists would be responsible to deploy their own cloud-controller-manager now.<br>They would watch for the kube-apiserver Deployment to exist, and as soon as it does, they deploy the CCM.<br>(Side note: The Botanist would also be responsible to deploy further controllers needed for this cloud environment, e.g. F5-controllers or CSI plugins).</td></tr><tr><td>botanist.WaitUntilKubeAPIServerReady</td><td>Gardener checks the <code>.status</code> object of the kube-apiserver <code>Deployment</code> and waits until readiness is indicated.</td></tr><tr><td>botanist.InitializeShootClients</td><td>Unchanged; Gardener creates a Kubernetes client for the Shoot cluster.</td></tr><tr><td>botanist.DeployMachineControllerManager</td><td>Deleted, Gardener does no longer deploy MCM itself. See below.</td></tr><tr><td>hybridBotanist.ReconcileMachines</td><td>Gardener creates a <code>Worker</code> CRD in the Seed, and the responsible <code>Worker</code> controller picks it up and does its job (see CRD above). It also deploys the machine-controller-manager.<br>Gardener waits until the status indicates that the controller is done.</td></tr><tr><td>hybridBotanist.DeployKubeAddonManager</td><td>This function also computes the CoreOS cloud-config (because the secret storing it is managed by the kube-addon-manager).<br>Gardener would deploy the CloudConfig-specific CRD in the Seed, and the responsible OS controller picks it up and does its job (see CRD above).<br>The Botanists which would have to modify something would register a Webhook for this CloudConfig-specific resource and apply their changes.<br>The rest is mostly unchanged, Gardener generates the manifests for the addons and deploys the kube-addon-manager into the Seed.<br>AWS Botanist registers a Webhook for nginx-ingress.<br>Azure Botanist registers a Webhook for calico.<br>Gardener will no longer deploy the <code>StorageClass</code>es. Instead, the Botanists wait until the kube-apiserver is available and deploy them.<br><br>In the long term we want to get rid of optional addons inside the Gardener core and implement a sophisticated addon concept (see <a href=https://github.com/gardener/gardener/issues/246>#246</a>).</td></tr><tr><td>shootCloudBotanist.DeployKube2IAMResources</td><td>This function would be removed (currently Gardener would execute a Terraform job creating the IAM roles specified in the Shoot manifest). We cannot keep this behavior, the user would be responsible to create the needed IAM roles on its own.</td></tr><tr><td>botanist.Shoot.Components.Nginx.DNSEtnry</td><td>Gardener creates a DNS-specific CRD in the Seed, and the responsible DNS-controller picks it up and creates a corresponding DNS record (see CRD specification above).</td></tr><tr><td>botanist.WaitUntilVPNConnectionExists</td><td>Unchanged, Gardener checks that it is possible to port-forward to a Shoot pod.</td></tr><tr><td>seedCloudBotanist.ApplyCreateHook</td><td>This function would be removed (actually, only the AWS Botanist implements it).<br>AWS Botanist deploys the aws-lb-readvertiser once the API Server is deployed and updates the ELB health check protocol one the load balancer pointing to the API server is created.</td></tr><tr><td>botanist.DeploySeedMonitoring</td><td>Unchanged, Gardener deploys the monitoring stack into the Seed.</td></tr><tr><td>botanist.DeployClusterAutoscaler</td><td>Unchanged, Gardener deploys the cluster-autoscaler into the Seed.</td></tr></tbody></table><p>ℹ We can easily lift the contract later and allow dynamic network plugins or not using the VPN solution at all.
We could also introduce a dedicated <code>ControlPlane</code> CRD and leave the complete responsibility of deploying kube-apiserver, kube-controller-manager, etc. to other controllers (if we need it at some point in time).</p><h4 id=deletion-flow>Deletion flow</h4><p>We are now examining the current Shoot deletion flow and describe shortly how it could look like when applying this proposal:</p><table><thead><tr><th>Operation</th><th>Description</th></tr></thead><tbody><tr><td>botanist.DeploySecrets</td><td>This is just refreshing the cloud provider secret in the Shoot namespace in the Seed (in case the user has changed it before triggering the deletion). This function would stay as it is.</td></tr><tr><td>hybridBotanist.RefreshMachineClassSecrets</td><td>This function would disappear.<br>Worker Pool controller needs to watch the referenced secret and update the generated MachineClassSecrets immediately.</td></tr><tr><td>hybridBotanist.RefreshCloudProviderConfig</td><td>This function would disappear. Botanist needs to watch the referenced secret and update the generated cloud-provider-config immediately.</td></tr><tr><td>botanist.RefreshCloudControllerManagerChecksums</td><td>See &ldquo;hybridBotanist.RefreshCloudProviderConfig&rdquo;.</td></tr><tr><td>botanist.RefreshKubeControllerManagerChecksums</td><td>See &ldquo;hybridBotanist.RefreshCloudProviderConfig&rdquo;.</td></tr><tr><td>botanist.InitializeShootClients</td><td>Unchanged; Gardener creates a Kubernetes client for the Shoot cluster.</td></tr><tr><td>botanist.DeleteSeedMonitoring</td><td>Unchanged; Gardener deletes the monitoring stack.</td></tr><tr><td>botanist.DeleteKubeAddonManager</td><td>Unchanged; Gardener deletes the kube-addon-manager.</td></tr><tr><td>botanist.DeleteClusterAutoscaler</td><td>Unchanged; Gardener deletes the cluster-autoscaler.</td></tr><tr><td>botanist.WaitUntilKubeAddonManagerDeleted</td><td>Unchanged; Gardener waits until the kube-addon-manager is deleted.</td></tr><tr><td>botanist.CleanCustomResourceDefinitions</td><td>Unchanged, Gardener cleans the CRDs in the Shoot.</td></tr><tr><td>botanist.CleanKubernetesResources</td><td>Unchanged, Gardener cleans all remaining Kubernetes resources in the Shoot.</td></tr><tr><td>hybridBotanist.DestroyMachines</td><td>Gardener deletes the WorkerPool-specific CRD in the Seed, and the responsible WorkerPool-controller picks it up and does its job.<br>Gardener waits until the CRD is deleted.</td></tr><tr><td>shootCloudBotanist.DestroyKube2IAMResources</td><td>This function would disappear (currently Gardener would execute a Terraform job deleting the IAM roles specified in the <code>Shoot</code> manifest). We cannot keep this behavior, the user would be responsible to delete the needed IAM roles on its own.</td></tr><tr><td>shootCloudBotanist.DestroyInfrastructure</td><td>Gardener deletes the Infrastructure-specific CRD in the Seed, and the responsible Botanist picks it up and does its job.<br>Gardener waits until the CRD is deleted.</td></tr><tr><td>botanist.Shoot.Components.DNS.External{Provider/Entry}.Destroy</td><td>Gardener deletes the DNS-specific CRD in the Seed, and the responsible DNS-controller picks it up and does its job.<br>Gardener waits until the CRD is deleted.</td></tr><tr><td>botanist.DeleteKubeAPIServer</td><td>Unchanged; Gardener deletes the kube-apiserver.</td></tr><tr><td>botanist.DeleteBackupInfrastructure</td><td>Unchanged; Gardener deletes the <code>BackupInfrastructure</code> object in the Garden cluster.<br>(The BackupInfrastructure controller deletes the BackupInfrastructure-specific CRD in the Seed, and the responsible Botanist picks it up and does its job.<br>The BackupInfrastructure controller waits until the CRD is deleted.)</td></tr><tr><td>botanist.Shoot.Components.DNS.Internal{Provider/Entry}.Destroy</td><td>Gardener deletes the DNS-specific CRD in the Seed, and the responsible DNS-controller picks it up and does its job.<br>Gardener waits until the CRD is deleted.</td></tr><tr><td>botanist.DeleteNamespace</td><td>Unchanged; Gardener deletes the Shoot namespace in the Seed cluster.</td></tr><tr><td>botanist.WaitUntilSeedNamespaceDeleted</td><td>Unchanged; Gardener waits until the Shoot namespace in the Seed has been deleted.</td></tr><tr><td>botanist.DeleteGardenSecrets</td><td>Unchanged; Gardener deletes the kubeconfig/ssh-keypair <code>Secret</code> in the project namespace in the Garden.</td></tr></tbody></table><h3 id=gardenlet>Gardenlet</h3><p>One part of the whole extensibility work will also to further split Gardener itself.
Inspired from Kubernetes itself we plan to move the <code>Shoot</code> reconciliation/deletion controller loops as well as the <code>BackupInfrastructure</code> reconciliation/deletion controller loops into a dedicated &ldquo;gardenlet&rdquo; component that will run in the Seed cluster.
With that, it can talk locally to the responsible kube-apiserver and we do no longer need to perform every operation out of the Garden cluster.
This approach will also help us with scalability, performance, maintainability, testability in general.</p><p>This architectural change implies that the Kubernetes API server of the Garden cluster must be exposed publicly (or at least be reachable by the registered Seeds). The Gardener controller-manager will remain and will keep its <code>CloudProfile</code>, <code>SecretBinding</code>, <code>Quota</code>, <code>Project</code>, and <code>Seed</code> controller loops. One part of the seed controller could be to deploy the &ldquo;gardenlet&rdquo; into the Seeds, however, this would require network connectivity to the Seed cluster.</p><h3 id=shoot-control-plane-movementmigration>Shoot control plane movement/migration</h3><p>Automatically moving control planes is difficult with the current implementation as some resources created in the old Seed must be moved to the new one. However, some of them are not under Gardener&rsquo;s control (e.g., <code>Machine</code> resources). Moreover, the old control plane must be deactivated somehow to ensure that not two controllers work on the same things (e.g., virtual machines) from different environments.</p><p>Gardener does not only deploy a DNS controller into the Seeds but also into its own Garden cluster.
For every Shoot cluster, Gardener commissions it to create a DNS <code>TXT</code> record containing the name of the Seed responsible for the Shoot (holding the control plane), e.g.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>dig -t txt aws-01.core.garden.example.com

...
;; ANSWER SECTION:
aws-01.core.garden.example.com. 120 IN	TXT <span style=color:#a31515>&#34;Seed=seed-01&#34;</span>
...
</code></pre></div><p>Gardener always keeps the DNS record up-to-date based on which Seed is responsible.</p><p>In the above CRD examples one object in the <code>.spec</code> section was omitted as it is needed to get Shoot control plane movement/migration working (the field is only explained now in this section and not before; it was omitted on purpose to support focusing on the relevant specifications first).
Every CRD also has the following section in its <code>.spec</code>:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>leadership:
  record: aws-01.core.garden.example.com
  value: seed-01
  leaseSeconds: 60
</code></pre></div><p>Before every operation the CRD-controllers check this DNS record (based on the <code>.spec.leadership.leaseSeconds</code> configuration) and verify that its result is equal to the <code>.spec.leadership.value</code> field.
If both match they know that they should act on the resource, otherwise they stop doing anything.</p><p>ℹ We will provide an easy-to-use framework for the controllers containing all of these features out-of-the-box in order to allow the developers to focus on writing the actual controller logic.</p><p>When a Seed control plane move is triggered, the <code>.spec.cloud.seed</code> field of the respective <code>Shoot</code> is changed.
Gardener will change the respective DNS record&rsquo;s value (<code>aws-01.core.garden.example.com</code>) to contain the new Seed name.
After that it will wait <code>2*60s</code> to be sure that all controllers have observed the change.
Then it starts reconciling and applying the CRDs together with a preset <code>.status.state</code> into the new Seed (based on its last observations which were stored in the respective <code>ShootState</code> object stored in the Garden cluster).
The controllers are - as per contract - asked to reconstruct their own environment based on the <code>.status.state</code> they have written before and the real world&rsquo;s status.
Apart from that, the normal reconciliation flow gets executed.</p><p>Gardener stores the list of Seeds that were responsible for hosting a Shoots control plane at some time in the Shoots <code>.status.seeds</code> list so that it knows which Seeds must be cleaned up (i.e., where the control plane must be deleted because it has been moved).
Once cleaned up, the Seed&rsquo;s name will be removed from that list.</p><h3 id=backupinfrastructure-migration>BackupInfrastructure migration</h3><p>One part of the reconciliation flow above is the provisioning of the infrastructure for the Shoot&rsquo;s etcd backups (usually, this is a blob store bucket/container).
Gardener already uses a separate <code>BackupInfrastructure</code> resource that is written into the Garden cluster and picked up by a dedicated <code>BackupInfrastructure</code> controller (bundled into the Gardener controller manager).
This dedicated resource exists mainly for the reason to allow keeping backups for a certain &ldquo;grace period&rdquo; even after the Shoot deletion itself:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: gardener.cloud/v1alpha1
kind: BackupInfrastructure
metadata:
  name: aws-01-bucket
  namespace: garden-core
spec:
  seed: seed-01
  shootUID: uuid-of-shoot
</code></pre></div><p>The actual provisioning is executed in a corresponding Seed cluster as Gardener can only assume network connectivity to the underlying cloud environment in the Seed.
We would like to keep the created artifacts in the Seed (e.g., Terraform state) near to the control plane.
Consequently, when Gardener moves a control plane, it will update the <code>.spec.seed</code> field of the <code>BackupInfrastructure</code> resource as well.
With the exact same logic described above the <code>BackupInfrastructure</code> controller inside the Gardener will move to the new Seed.</p><h2 id=registration-of-external-controllers-at-gardener>Registration of external controllers at Gardener</h2><p>We want to have a dynamic registration process, i.e. we don&rsquo;t want to hard-code any information about which controllers shall be deployed.
The ideal solution would be to not even requiring a restart of Gardener when a new controller registers.</p><p>Every controller is registered by a <code>ControllerRegistration</code> resource that introduces every controller together with its supported resources (dimension (<code>kind</code>) and shape (<code>type</code>) combination) to Gardener:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: gardener.cloud/v1alpha1
kind: ControllerRegistration
metadata:
  name: dns-aws-route53
spec:
  resources:
  - kind: DNS
    type: aws-route53
<span style=color:green># deployment:</span>
<span style=color:green>#   type: helm</span>
<span style=color:green>#   providerConfig:</span>
<span style=color:green>#     chart.tgz: base64(helm-chart)</span>
<span style=color:green>#     values.yaml: |</span>
<span style=color:green>#       foo: bar</span>
</code></pre></div><p>Every <code>.kind</code>/<code>.type</code> combination may only exist once in the system.</p><p>When a <code>Shoot</code> shall be reconciled Gardener can identify based on the referenced <code>Seed</code> and the content of the <code>Shoot</code> specification which controllers are needed in the respective Seed cluster.
It will demand the operators in the Garden cluster to deploy the controllers they are responsible for to a specific Seed.
This kind of communication happens via CRDs as well:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: gardener.cloud/v1alpha1
kind: ControllerInstallation
metadata:
  name: dns-aws-route53
spec:
  registrationRef:
    name: dns-aws-route53
  seedRef:
    name: seed-01
status:
  conditions:
  - lastTransitionTime: 2018-08-07T15:09:23Z
    message: The controller has been successfully deployed to the seed.
    reason: ControllerDeployed
    status: <span style=color:#a31515>&#34;True&#34;</span>
    type: Available
</code></pre></div><p>The default scenario is that every controller is gets deployed by a dedicated operator that knows how to handle its lifecycle operations like deployment, update, upgrade, deletion.
This operator watches <code>ControllerInstallation</code> resources and reacts on those it is responsible for (that it has created earlier).
Gardener is responsible for writing the <code>.spec</code> field, the operator is responsible for providing information in the <code>.status</code> indicating whether the controller was successfully deployed and is ready to be used.
Gardener will be also able to ask for deletion of controllers from Seeds when they are not needed there anymore by deleting the corresponding <code>ControllerInstallation</code> object.</p><p>ℹ The provided easy-to-use framework for the controllers will also contain these needed features to implement corresponding operators.</p><p>For most cases the controller deployment is very simple (just deploying it into the seed with some static configuration).
In these cases it would produce unnecessary effort to ask for providing another component (the operator) that deploys the controller.
To simplify this situation Gardener will be able to react on <code>ControllerInstallation</code>s specifying <code>.spec.registration.deployment.type=helm</code>.
The controller would be registered with the <code>ControllerRegistration</code> resources that would contain a Helm chart with all resources needed to deploy this controller into a seed (plus some static values).
Gardener would render the Helm chart and deploy the resources into the seed.
It will not react if <code>.spec.registration.deployment.type!=helm</code> which allows to also use any other deployment mechanism. Controllers that are getting deployed by operators would not specify the <code>.spec.deployment</code> section in the <code>ControllerRegistration</code> at all.</p><p>ℹ Any controller requiring dynamic configuration values (e.g., based on the cloud provider or the region of the seed) must be installed with the operator approach.</p><h2 id=other-cloud-specific-parts>Other cloud-specific parts</h2><p>The Gardener API server has a few admission controllers that contain cloud-specific code as well. We have to replace these parts as well.</p><h3 id=defaulting-and-validation-admission-plugins>Defaulting and validation admission plugins</h3><p>Right now, the admission controllers inside the Gardener API server do perform a lot of validation and defaulting of fields in the Shoot specification.
The cloud-specific parts of these admission controllers will be replaced by mutating admission webhooks that will get called instead.
As we will have a dedicated operator running in the Garden cluster anyway it will also get the responsibility to register this webhook if it needs to validate/default parts of the Shoot specification.</p><p>Example: The <code>.spec.cloud.workerPools[*].providerConfig.machineImage</code> field in the new Shoot manifest mentioned above could be omitted by the user and would get defaulted by the cloud-specific operator.</p><h3 id=dns-hosted-zone-admission-plugin>DNS Hosted Zone admission plugin</h3><p>For the same reasons the existing DNS Hosted Zone admission plugin will be removed from the Gardener core and moved into the responsibility of the respective DNS-specific operators running in the Garden cluster.</p><h3 id=shoot-quota-admission-plugin>Shoot Quota admission plugin</h3><p>The Shoot quota admission plugin validates create or update requests on Shoots and checks that the specified machine/storage configuration is defined as per referenced <code>Quota</code> objects.
The cloud-specifics in this controller are no longer needed as the <code>CloudProfile</code> and the <code>Shoot</code> resource have been adapted:
The machine/storage configuration is no longer in cloud-specific sections but hard-wired fields in the general <code>Shoot</code> specification (see example resources above).
The quota admission plugin will be simplified and remains in the Gardener core.</p><h3 id=shoot-maintenance-controller>Shoot maintenance controller</h3><p>Every Shoot cluster can define a maintenance time window in which Gardener will update the Kubernetes patch version (if enabled) and the used machine image version in the Shoot resource.
While the Kubernetes version is not part of the <code>providerConfig</code> section in the <code>CloudProfile</code> resource, the <code>machineImage</code> field is, and thus Gardener can&rsquo;t understand it any longer.
In the future Gardener has to rely on the cloud-specific operator (probably the same doing the defaulting/validation mentioned before) to update this field.
In the maintenance time window the maintenance controller will update the Kubernetes patch version (if enabled) and add a <code>trigger.gardener.cloud=maintenance</code> annotation in the Shoot resource.
The already registered mutating web hook will call the operator who has to remove this annotation and update the <code>machineImage</code> in the <code>.spec.cloud.workerPools[*].providerConfig</code> sections.</p><h2 id=alternatives>Alternatives</h2><ul><li>Alternative to DNS approach for Shoot control plane movement/migration: We have thought about rotating the credentials when a move is triggered which would make all controllers ineffective immediately. However, one problem with this is that we require IAM privileges for the users infrastructure account which might be not desired. Another, more complicated problem is that we cannot assume API access in order to create technical users for all cloud environments that might be supported.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b5f9b425f9c9a3fa5345982414974ca5>2 - 02 Backupinfra</h1><h1 id=backup-infrastructure-crd-and-controller-redesign>Backup Infrastructure CRD and Controller Redesign</h1><h2 id=goal>Goal</h2><ul><li>As an operator, I would like to efficiently use the backup bucket for multiple clusters, thereby limiting the total number of buckets required.</li><li>As an operator, I would like to use different cloud provider for backup bucket provisioning other than cloud provider used for seed infrastructure.</li><li>Have seed independent backups, so that we can easily migrate a shoot from one seed to another.</li><li>Execute the backup operations (including bucket creation and deletion) from a seed, because network connectivity may only be ensured from the seeds (not necessarily from the garden cluster).</li><li>Preserve the garden cluster as source of truth (no information is missing in the garden cluster to reconstruct the state of the backups even if seed and shoots are lost completely).</li><li>Do not violate the infrastructure limits in regards to blob store limits/quotas.</li></ul><h2 id=motivation>Motivation</h2><p>Currently, every shoot cluster has its own etcd backup bucket with a centrally configured retention period. With the growing number of clusters, we are soon running out of the <a href=https://gist.github.com/swapnilgm/5c4d5506811e63c32ab3d73c4171d30f>quota limits of buckets on the cloud provider</a>. Moreover, even if the clusters are deleted, the backup buckets do exist, for a configured period of retention. Hence, there is need of minimizing the total count of buckets.</p><p>In addition, currently we use seed infrastructure credentials to provision the bucket for etcd backups. This results in binding backup bucket provider to seed infrastructure provider.</p><h2 id=terminology>Terminology</h2><ul><li><strong>Bucket</strong> : It is equivalent to s3 bucket, abs container, gcs bucket, swift container, alicloud bucket</li><li><strong>Object</strong> : It is equivalent s3 object, abs blob, gcs object, swift object, alicloud object, snapshot/backup of etcd on object store.</li><li><strong>Directory</strong> : As such there is no concept of directory in object store but usually the use directory as <code>/</code> separate common prefix for set of objects. Alternatively they use term folder for same.</li><li><strong>deletionGracePeriod</strong>: This means grace period or retention period for which backups will be persisted post deletion of shoot.</li></ul><h2 id=current-spec>Current Spec:</h2><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-YAML data-lang=YAML><span style=color:green>#BackupInfra spec</span>
Kind: BackupInfrastructure
Spec:
    seed: seedName
    shootUID : shoot.status.uid
</code></pre></div><h2 id=current-naming-conventions>Current naming conventions</h2><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>SeedNamespace :</td><td>Shoot&ndash;projectname&ndash;shootname</td></tr><tr><td>seed:</td><td>seedname</td></tr><tr><td>ShootUID :</td><td>shoot.status.UID</td></tr><tr><td>BackupInfraname:</td><td>seednamespce+sha(uid)[:5]</td></tr><tr><td>Backup-bucket-name:</td><td>BackupInfraName</td></tr><tr><td>BackupNamespace:</td><td>backup&ndash;BackupInfraName</td></tr></tbody></table><h2 id=proposal>Proposal</h2><p>Considering <a href=/docs/gardener/proposals/01-extensibility/#backup-infrastructure-provisioning>Gardener extension proposal</a> in mind, the backup infrastructure controller can be divided in two parts. There will be basically four backup infrastructure related CRD&rsquo;s. Two on the garden apiserver. And two on the seed cluster. Before going into to workflow, let&rsquo;s just first have look at the CRD.</p><h3 id=crd-on-garden-cluster>CRD on Garden cluster</h3><p>Just to give brief before going into the details, we will be sticking to the fact that Garden apiserver is always source of truth. Since backupInfra will be maintained post deletion of shoot, the info regarding this should always come from garden apiserver, we will continue to have BackupInfra resource on garden apiserver with some modifications.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: garden.cloud/v1alpha1
kind: BackupBucket
metadata:
  name: packet-region1-uid[:5]
  <span style=color:green># No namespace needed. This will be cluster scope resource.</span>
  ownerReferences:
  - kind: CloudProfile
    name: packet
spec:
  provider: aws
  region: eu-west-1
  secretRef: <span style=color:green># Required for root</span>
    name: backup-operator-aws
    namespace: garden
status:
  lastOperation: ...
  observedGeneration: ...
  seed: ...
</code></pre></div><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: garden.cloud/v1alpha1
kind: BackupEntry
metadata:
  name: shoot--dev--example--3ef42 <span style=color:green># Naming convention explained before</span>
  namespace: garden-dev
  ownerReferences:
  - apiVersion: core.gardener.cloud/v1beta1
    blockOwnerDeletion: <span style=color:#00f>false</span>
    controller: <span style=color:#00f>true</span>
    kind: Shoot
    name: example
    uid: 19a9538b-5058-11e9-b5a6-5e696cab3bc8
spec:
  shootUID: 19a9538b-5058-11e9-b5a6-5e696cab3bc8 <span style=color:green># Just for reference to find back associated shoot.</span>
  <span style=color:green># Following section comes from cloudProfile or seed yaml based on granularity decision.</span>
  bucketName: packet-region1-uid[:5]
status:
  lastOperation: ...
  observedGeneration: ...
  seed: ...
</code></pre></div><h3 id=crd-on-seed-cluster>CRD on Seed cluster</h3><p>Considering the extension proposal, we want individual component to be handled by controller inside seed cluster. We will have Backup related resource in registered seed cluster as well.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: extensions.gardener.cloud/v1alpha1
kind: BackupBucket
metadata:
  name: packet-random[:5]
  <span style=color:green># No namespace need. This will be cluster scope resource</span>
spec:
  type: aws
  region: eu-west-1
  secretRef:
    name: backup-operator-aws
    namespace: backup-garden
status:
  observedGeneration: ...
  state: ...
  lastError: ..
  lastOperation: ...
</code></pre></div><p>There are two points for introducing BackupEntry resource.</p><ol><li>Cloud provider specific code goes completely in seed cluster.</li><li>Network issue is also handled by moving deletion part to backup-extension-controller in seed cluster.</li></ol><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: extensions.gardener.cloud/v1alpha1
kind: BackupEntry
metadata:
  name: shoot--dev--example--3ef42 <span style=color:green># Naming convention explained later</span>
  <span style=color:green># No namespace need. This will be cluster scope resource</span>
spec:
  type: aws
  region: eu-west-1
  secretRef: <span style=color:green># Required for root</span>
    name: backup-operator-aws
    namespace: backup-garden
status:
  observedGeneration: ...
  state: ...
  lastError: ..
  lastOperation: ...
</code></pre></div><h3 id=workflow>Workflow</h3><ul><li>Gardener administrator will configure the cloudProfile with backup infra credentials and provider config as follows.</li></ul><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green># CloudProfile.yaml:</span>
Spec:
    backup:
        provider: aws
        region: eu-west-1
        secretRef:
            name: backup-operator-aws
            namespace: garden
</code></pre></div><p>Here CloudProfileController will interpret this spec as follows:</p><ul><li>If <code>spec.backup</code> is nil<ul><li>No backup for any shoot.</li></ul></li><li>If <code>spec.backup.region</code> is not nil,<ul><li>Then respect it, i.e. use the provider and unique region field mentioned there for BackupBucket.</li><li>Here Preferably, <code>spec.backup.region</code> field will be unique, since for cross provider, it doesn’t make much sense. Since region name will be different for different providers.</li></ul></li><li>Otherwise, spec.backup.region is nil then,<ul><li>If same provider case i.e. spec.backup.provider = spec.(type-of-provider) or nil,<ul><li>Then, for each region from <code>spec.(type-of-provider).constraints.regions</code> create a <code>BackupBucket</code> instance. This can be done lazily i.e. create <code>BackupBucket</code> instance for region only if some seed actually spawned in the region has been registered. This will avoid creating IaaS bucket even if no seed is registered in that region, but region is listed in <code>cloudprofile</code>.</li><li>Shoot controller will choose backup container as per the seed region. (With shoot control plane migration also, seed’s availability zone might change but the region will be remaining same as per current scope.)</li></ul></li><li>Otherwise cross provider case i.e. spec.backup.provider != spec.(type-of-provider)<ul><li>Report validation error: Since, for example, we can’t expect <code>spec.backup.provider</code> = <code>aws</code> to support region in, <code>spec.packet.constraint.region</code>. Where type-of-provider is <code>packet</code></li></ul></li></ul></li></ul><p>Following diagram represent overall flow in details:</p><p><img src=/__resources/02-backupinfra-provisioning-sequence-diagram_c08a3e.svg alt=sequence-diagram></p><h4 id=reconciliation>Reconciliation</h4><p>Reconciliation on backup entry in seed cluster mostly comes in picture at the time of deletion. But we can add initialization steps like creation of <a href=#terminology>directory</a> specific to shoot in backup bucket. We can simply create BackupEntry at the time of shoot deletion as well.</p><h4 id=deletion>Deletion</h4><ul><li>On shoot deletion, the BackupEntry instance i.e. shoot specific instance will get deletion timestamp because of ownerReference.</li><li>If <code>deletionGracePeriod</code> configured in GCM component configuration is expired, BackupInfrastructure Controller will delete the backup folder associated with it from backup object store.</li><li>Finally, it will remove the <code>finalizer</code> from backupEntry instance.</li></ul><h3 id=alternative>Alternative</h3><p><img src=/__resources/02-backupinfra-provisioning-with-deletion-job_e51f05.svg alt=sequence-diagram></p><h2 id=discussion-points--variations>Discussion points / variations</h2><h3 id=manual-vs-dynamic-bucket-creation>Manual vs dynamic bucket creation</h3><ul><li><p>As per limit observed on different cloud providers, we can have single bucket for backups on one cloud providers. So, we could avoid the little complexity introduced in above approach by pre-provisioning buckets as a part of landscape setup. But there won&rsquo;t be anybody to detect bucket existence and its reconciliation. Ideally this should be avoided.</p></li><li><p>Another thing we can have is, we can let administrator register the pool of root backup infra resource and let the controller schedule backup on one of this.</p></li><li><p>One more variation here could be to create bucket dynamically per hash of shoot UID.</p></li></ul><h3 id=sdk-vs-terraform>SDK vs Terraform</h3><p>Initial reason for going for terraform script is its stability and the provided parallelism/concurrency in resource creation. For backup infrastructure, Terraform scripts are very minimal right now. Its simply have bucket creation script. With shared bucket logic, if possible we might want to isolate access at <a href=#terminology>directory</a> level but again its additional one call. So, we will prefer switching to SDK for all object store operations.</p><h3 id=limiting-the-number-of-shoots-per-bucket>Limiting the number of shoots per bucket</h3><p>Again as per limit observed on different cloud providers, we can have single bucket for backups on one cloud providers. But if we want to limit the number of shoots associated with bucket, we can have central map of configuration in <code>gardener-controller-component-configuration.yaml</code>.
Where we will mark supported count of shoots per cloud provider. Most probable space could be,
<code>controller.backupInfrastructures.quota</code>. If limit is reached we can create new <code>BucketBucket</code> instance.</p><p>e.g.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: controllermanager.config.gardener.cloud/v1alpha1
kind: ControllerManagerConfiguration
controllers:
  backupInfrastructure:
    quota:
      - provider: aws
        limit: 100 <span style=color:green># Number mentioned here are random, just for example purpose.</span>
      - provider: azure
        limit: 80
      - provider: openstack
        limit: 100
      ...
</code></pre></div><h2 id=backward-compatibility>Backward compatibility</h2><h3 id=migration>Migration</h3><ul><li>Create shoot specific folder.</li><li>Transfer old objects.</li><li>Create manifest of objects on new bucket<ul><li>Each entry will have status: None,Copied, NotFound.</li><li>Copy objects one by one.</li></ul></li><li>Scale down etcd-main with old config. ⚠️ Cluster down time</li><li>Copy remaining objects</li><li>Scale up etcd-main with new config.</li><li>Destroy Old bucket and old backup namespace. It can be immediate or preferably <strong>lazy</strong> deletion.</li></ul><p><img src=/__resources/02-backupinfra-migration_a671d5.svg alt=backup-migration-sequence-diagram></p><h3 id=legacy-mode-alternative>Legacy Mode alternative</h3><ul><li>If Backup namespace present in seed cluster, then follow the legacy approach.</li><li>i.e. reconcile creation/existence of shoot specific bucket and backup namespace.</li><li>If backup namespace is not created, use shared bucket.</li><li><strong>Limitation</strong> Never know when the existing cluster will be deleted, and hence, it might be little difficult to maintain with next release of gardener. This might look simple and straight-forward for now but may become pain point in future, if in worst case, because of some new use cases or refactoring, we have to change the design again. Also, even after multiple garden release we won&rsquo;t be able to remove deprecated existing BackupInfrastructure CRD</li></ul><h3 id=references>References</h3><ul><li><a href=/docs/gardener/proposals/01-extensibility/#backup-infrastructure-provisioning>Gardener extension proposal</a></li><li><a href=https://gist.github.com/swapnilgm/5c4d5506811e63c32ab3d73c4171d30f>Cloud providers object store limit comparison</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-7f676322b31081c2c8e84da2b175e0a4>3 - 03 Networking Extensibility</h1><h1 id=network-extensibility>Network Extensibility</h1><p>Currently Gardener follows a mono network-plugin support model (i.e., Calico). Although this can seem to be the more stable approach, it does not completely reflect the real use-case. This proposal brings forth an effort to add an extra level of customizability to Gardener networking.</p><h2 id=motivation>Motivation</h2><p>Gardener is an open-source project that provides a nested user model. Basically, there are two types of services provided by Gardener to its users:</p><ul><li><strong>Managed</strong>: users only request a Kubernetes cluster (Clusters-as-a-Service)</li><li><strong>Hosted</strong>: users utilize Gardener to provide their own managed version of Kubernetes (Cluster-Provisioner-as-a-service)</li></ul><p>For the first set of users, the choice of network plugin might not be so important, however, for the second class of users (i.e., Hosted) it is important to be able to customize networking based on their needs.</p><p>Furthermore, Gardener provisions clusters on different cloud-providers with different networking requirements. For example, Azure does not support Calico Networking [1], this leads to the introduction of manual exceptions in static add-on charts which is error prune and can lead to failures during upgrades.</p><p>Finally, every provider is different, and thus the network always needs to adapt to the infrastructure needs to provider better performance. Consistency does not necessarily lie in the implementation but in the interface.</p><h2 id=gardener-network-extension>Gardener Network Extension</h2><p>The goal of the Gardener Network Extensions is to support different network plugin, therefore, the specification for the network resource won&rsquo;t be fixed and will be customized based on the underlying network plugin. To do so, a <code>NetworkConfig</code> field in the spec will be provided where each plugin will define. Below is an example for deploy Calico as the cluster network plugin.</p><h3 id=long-term-spec>Long Term Spec</h3><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>---
apiVersion: extensions.gardener.cloud/v1alpha1
kind: Network
metadata:
  name: calico-network
  namespace: shoot--core--test-01
spec:
  type: calico
  clusterCIDR: 192.168.0.0/24
  serviceCIDR:  10.96.0.0/24
  providerConfig:
    apiVersion: calico.extensions.gardener.cloud/v1alpha1
    kind: NetworkConfig
    ipam:
      type: host-local
      cidr: usePodCIDR
    backend: bird
    typha:
      enabled: <span style=color:#00f>true</span>
status:
  observedGeneration: ...
  state: ...
  lastError: ..
  lastOperation: ...
  providerStatus:
    apiVersion: calico.extensions.gardener.cloud/v1alpha1
    kind: NetworkStatus
    components:
      kubeControllers: <span style=color:#00f>true</span>
      calicoNodes: <span style=color:#00f>true</span>
    connectivityTests:
      pods: <span style=color:#00f>true</span>
      services: <span style=color:#00f>true</span>
    networkModules:
      arp_proxy: <span style=color:#00f>true</span>
    config:
      clusterCIDR: 192.168.0.0/24
      serviceCIDR:  10.96.0.0/24
      ipam:
        type: host-local
        cidr: usePodCIDR
</code></pre></div><h3 id=first-implementation-short-term>First Implementation (Short Term)</h3><p>As an initial implementation the network plugin type will be specified by the user e.g., Calico (without further configuration in the provider spec). This will then be used to generate
the <code>Network</code> resource in the seed. The Network operator will pick it up, and apply the configuration based on the <code>spec.cloudProvider</code> specified directly to the shoot or via the
Gardener resource manager (still in the works).</p><p>The <code>cloudProvider</code> field in the spec is just an initial catalyst but not meant to be stay long-term. In the future, the network provider configuration will be customized to match the best
needs of the infrastructure.</p><p>Here is how the simplified initial spec would look like:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>---
apiVersion: extensions.gardener.cloud/v1alpha1
kind: Network
metadata:
  name: calico-network
  namespace: shoot--core--test-01
spec:
  type: calico
  cloudProvider: {aws,azure,...}
status:
  observedGeneration: 2
  lastOperation: ...
  lastError: ...
</code></pre></div><h2 id=functionality>Functionality</h2><p>The network resource need to be created early-on during cluster provisioning. Once created, the Network operator residing in every seed will create all the necessary networking resources and apply them to the shoot cluster.</p><p>The status of the Network resource should reflect the health of the networking components as well as additional tests if required.</p><h2 id=references>References</h2><p>[1] <a href=https://docs.projectcalico.org/v3.0/reference/public-cloud/azure>Azure support for Calico Networking</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-6972c06e5d67dfd5b9bac676e2a230c2>4 - 04 New Core Gardener Cloud Apis</h1><h1 id=new-coregardenercloudv1beta1-apis-required-to-extract-cloud-specificos-specific-knowledge-out-of-gardener-core>New <code>core.gardener.cloud/v1beta1</code> APIs required to extract cloud-specific/OS-specific knowledge out of Gardener core</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#summary>Summary</a></li><li><a href=#motivation>Motivation</a><ul><li><a href=#goals>Goals</a></li><li><a href=#non-goals>Non-Goals</a></li></ul></li><li><a href=#proposal>Proposal</a><ul><li><a href=#cloudprofile-resource><code>CloudProfile</code> resource</a></li><li><a href=#seed-resource><code>Seed</code> resource</a></li><li><a href=#project-resource><code>Project</code> resource</a></li><li><a href=#secretbinding-resource><code>SecretBinding</code> resource</a></li><li><a href=#quota-resource><code>Quota</code> resource</a></li><li><a href=#backupbucket-resource><code>BackupBucket</code> resource</a></li><li><a href=#backupentry-resource><code>BackupEntry</code> resource</a></li><li><a href=#shoot-resource><code>Shoot</code> resource</a></li><li><a href=#plant-resource><code>Plant</code> resource</a></li></ul></li></ul><h2 id=summary>Summary</h2><p>In <a href=/docs/gardener/proposals/01-extensibility/>GEP-1</a> we have proposed how to (re-)design Gardener to allow providers maintaining their provider-specific knowledge out of the core tree.
Meanwhile, we have progressed a lot and are about to remove the <a href=https://github.com/gardener/gardener/blob/de75a5bfcbedd16ba341ace0eb58be2a87049dcb/pkg/operation/cloudbotanist/types.go><code>CloudBotanist</code> interface</a> entirely.
The only missing aspect that will allow providers to really maintain their code out of the core is to design new APIs.</p><p>This proposal describes how the new <code>Shoot</code>, <code>Seed</code> etc. APIs will be re-designed to cope with the changes made with extensibility.
We already have the new <code>core.gardener.cloud/v1beta1</code> API group that will be the new default soon.</p><h2 id=motivation>Motivation</h2><p>We want to allow providers to individually maintain their specific knowledge without the necessity to touch the Gardener core code.
In order to achieve the same, we have to provide proper APIs.</p><h3 id=goals>Goals</h3><ul><li>Provide proper APIs to allow providers maintaining their code outside of the core codebase.</li><li>Do not complicate the APIs for end-users such that they can easily create, delete, and maintain shoot clusters.</li></ul><h3 id=non-goals>Non-Goals</h3><ul><li>Let&rsquo;s try to not split everything up into too many different resources. Instead, let&rsquo;s try to keep all relevant information in the same resources when possible/appropriate.</li></ul><h2 id=proposal>Proposal</h2><p>In GEP-1 we already have proposed a first version for new <code>CloudProfile</code> and <code>Shoot</code> resources.
In order to deprecate the existing/old <code>garden.sapcloud.io/v1beta1</code> API group (and remove it, eventually) we should move all existing resources to the new <code>core.gardener.cloud/v1beta1</code> API group.</p><h3 id=cloudprofile-resource><code>CloudProfile</code> resource</h3><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: CloudProfile
metadata:
  name: cloudprofile1
spec:
  type: &lt;some-provider-name&gt; <span style=color:green># {aws,azure,gcp,...}</span>
<span style=color:green># Optional list of labels on `Seed` resources that marks those seeds whose shoots may use this provider profile.</span>
<span style=color:green># An empty list means that all seeds of the same provider type are supported.</span>
<span style=color:green># This is useful for environments that are of the same type (like openstack) but may have different &#34;instances&#34;/landscapes.</span>
<span style=color:green># seedSelector:</span>
<span style=color:green>#   matchLabels:</span>
<span style=color:green>#     foo: bar</span>
  kubernetes:
    versions:
    - version: 1.12.1
    - version: 1.11.0
    - version: 1.10.6
    - version: 1.10.5
      expirationDate: 2020-04-05T01:02:03Z <span style=color:green># optional</span>
  machineImages:
  - name: coreos
    versions:
    - version: 2023.5.0
    - version: 1967.5.0
      expirationDate: 2020-04-05T08:00:00Z
  - name: ubuntu
    versions:
    - version: 18.04.201906170
  machineTypes:
  - name: m5.large
    cpu: <span style=color:#a31515>&#34;2&#34;</span>
    gpu: <span style=color:#a31515>&#34;0&#34;</span>
    memory: 8Gi
  <span style=color:green># storage: 20Gi # optional (not needed in every environment, may only be specified if no volumeTypes have been specified)</span>
    usable: <span style=color:#00f>true</span>
  volumeTypes: <span style=color:green># optional (not needed in every environment, may only be specified if no machineType has a `storage` field)</span>
  - name: gp2
    class: standard
  - name: io1
    class: premium
  regions:
  - name: europe-central-1
    zones: <span style=color:green># optional (not needed in every environment)</span>
    - name: europe-central-1a
    - name: europe-central-1b
    - name: europe-central-1c
    <span style=color:green># unavailableMachineTypes: # optional, list of machine types defined above that are not available in this zone</span>
    <span style=color:green># - m5.large</span>
    <span style=color:green># unavailableVolumeTypes: # optional, list of volume types defined above that are not available in this zone</span>
    <span style=color:green># - io1</span>
<span style=color:green># CA bundle that will be installed onto every shoot machine that is using this provider profile.</span>
<span style=color:green># caBundle: |</span>
<span style=color:green>#   -----BEGIN CERTIFICATE-----</span>
<span style=color:green>#   ...</span>
<span style=color:green>#   -----END CERTIFICATE-----</span>
  providerConfig:
    &lt;some-provider-specific-cloudprofile-config&gt;
    <span style=color:green># We don&#39;t have concrete examples for every existing provider yet, but these are the proposals:</span>
    <span style=color:green>#</span>
    <span style=color:green># Example for Alicloud:</span>
    <span style=color:green>#</span>
    <span style=color:green># apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1</span>
    <span style=color:green># kind: CloudProfileConfig</span>
    <span style=color:green># machineImages:</span>
    <span style=color:green># - name: coreos</span>
    <span style=color:green>#   version: 2023.5.0</span>
    <span style=color:green>#   id: coreos_2023_4_0_64_30G_alibase_20190319.vhd</span>
    <span style=color:green>#</span>
    <span style=color:green>#</span>
    <span style=color:green># Example for AWS:</span>
    <span style=color:green>#</span>
    <span style=color:green># apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1</span>
    <span style=color:green># kind: CloudProfileConfig</span>
    <span style=color:green># machineImages:</span>
    <span style=color:green># - name: coreos</span>
    <span style=color:green>#   version: 1967.5.0</span>
    <span style=color:green>#   regions:</span>
    <span style=color:green>#   - name: europe-central-1</span>
    <span style=color:green>#     ami: ami-0f46c2ed46d8157aa</span>
    <span style=color:green>#</span>
    <span style=color:green>#</span>
    <span style=color:green># Example for Azure:</span>
    <span style=color:green>#</span>
    <span style=color:green># apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1</span>
    <span style=color:green># kind: CloudProfileConfig</span>
    <span style=color:green># machineImages:</span>
    <span style=color:green># - name: coreos</span>
    <span style=color:green>#   version: 1967.5.0</span>
    <span style=color:green>#   publisher: CoreOS</span>
    <span style=color:green>#   offer: CoreOS</span>
    <span style=color:green>#   sku: Stable</span>
    <span style=color:green># countFaultDomains:</span>
    <span style=color:green># - region: westeurope</span>
    <span style=color:green>#   count: 2</span>
    <span style=color:green># countUpdateDomains:</span>
    <span style=color:green># - region: westeurope</span>
    <span style=color:green>#   count: 5</span>
    <span style=color:green>#</span>
    <span style=color:green>#</span>
    <span style=color:green># Example for GCP:</span>
    <span style=color:green>#</span>
    <span style=color:green># apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1</span>
    <span style=color:green># kind: CloudProfileConfig</span>
    <span style=color:green># machineImages:</span>
    <span style=color:green># - name: coreos</span>
    <span style=color:green>#   version: 2023.5.0</span>
    <span style=color:green>#   image: projects/coreos-cloud/global/images/coreos-stable-2023-5-0-v20190312</span>
    <span style=color:green>#</span>
    <span style=color:green>#</span>
    <span style=color:green># Example for OpenStack:</span>
    <span style=color:green>#</span>
    <span style=color:green># apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1</span>
    <span style=color:green># kind: CloudProfileConfig</span>
    <span style=color:green># machineImages:</span>
    <span style=color:green># - name: coreos</span>
    <span style=color:green>#   version: 2023.5.0</span>
    <span style=color:green>#   image: coreos-2023.5.0</span>
    <span style=color:green># keyStoneURL: https://url-to-keystone/v3/</span>
    <span style=color:green># dnsServers:</span>
    <span style=color:green># - 10.10.10.10</span>
    <span style=color:green># - 10.10.10.11</span>
    <span style=color:green># dhcpDomain: foo.bar</span>
    <span style=color:green># requestTimeout: 30s</span>
    <span style=color:green># constraints:</span>
    <span style=color:green>#   loadBalancerProviders:</span>
    <span style=color:green>#   - name: haproxy</span>
    <span style=color:green>#   floatingPools:</span>
    <span style=color:green>#   - name: fip1</span>
    <span style=color:green>#     loadBalancerClasses:</span>
    <span style=color:green>#     - name: class1</span>
    <span style=color:green>#       floatingSubnetID: 04eed401-f85f-4610-8041-c4835c4beea6</span>
    <span style=color:green>#       floatingNetworkID: 23949a30-1cdd-4732-ba47-d03ced950acc</span>
    <span style=color:green>#       subnetID: ac46c204-9d0d-4a4c-a90d-afefe40cfc35</span>
    <span style=color:green>#</span>
    <span style=color:green>#</span>
    <span style=color:green># Example for Packet:</span>
    <span style=color:green>#</span>
    <span style=color:green># apiVersion: packet.provider.extensions.gardener.cloud/v1alpha1</span>
    <span style=color:green># kind: CloudProfileConfig</span>
    <span style=color:green># machineImages:</span>
    <span style=color:green># - name: coreos</span>
    <span style=color:green>#   version: 2079.3.0</span>
    <span style=color:green>#   id: d61c3912-8422-4daf-835e-854efa0062e4</span>
</code></pre></div><h3 id=seed-resource><code>Seed</code> resource</h3><p>Special note: The proposal contains fields that are not yet existing in the current <code>garden.sapcloud.io/v1beta1.Seed</code> resource, but they should be implemented (open issues that require them are linked).</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Secret
metadata:
  name: seed-secret
  namespace: garden
type: Opaque
data:
  kubeconfig: base64(kubeconfig-for-seed-cluster)

---
apiVersion: v1
kind: Secret
metadata:
  name: backup-secret
  namespace: garden
type: Opaque
data:
  <span style=color:green># &lt;some-provider-specific data keys&gt;</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-backupbucket.yaml#L9-L11</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-infrastructure.yaml#L9-L10</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-backupbucket.yaml#L9-L10</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-backupbucket.yaml#L9</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-backupbucket.yaml#L9-L13</span>

---
apiVersion: core.gardener.cloud/v1beta1
kind: Seed
metadata:
  name: seed1
spec:
  provider:
    type: &lt;some-provider-name&gt; <span style=color:green># {aws,azure,gcp,...}</span>
    region: europe-central-1
  secretRef:
    name: seed-secret
    namespace: garden
  <span style=color:green># Motivation for DNS section: https://github.com/gardener/gardener/issues/201.</span>
  dns:
    provider: &lt;some-provider-name&gt; <span style=color:green># {aws-route53, google-clouddns, ...}</span>
    secretName: my-dns-secret <span style=color:green># must be in `garden` namespace</span>
    ingressDomain: seed1.dev.example.com
  volume: <span style=color:green># optional (introduced to get rid of `persistentvolume.garden.sapcloud.io/minimumSize` and `persistentvolume.garden.sapcloud.io/provider` annotations)</span>
    minimumSize: 20Gi
    providers:
    - name: foo
      purpose: etcd-main
  networks: <span style=color:green># Seed and Shoot networks must be disjunct</span>
    nodes: 10.240.0.0/16
    pods: 10.241.128.0/17
    services: 10.241.0.0/17
  <span style=color:green># Shoot default networks, see also https://github.com/gardener/gardener/issues/895.</span>
  <span style=color:green># shootDefaults:</span>
  <span style=color:green>#   pods: 100.96.0.0/11</span>
  <span style=color:green>#   services: 100.64.0.0/13</span>
  taints:
  - key: seed.gardener.cloud/protected
  - key: seed.gardener.cloud/invisible
  blockCIDRs:
  - 169.254.169.254/32
  backup: <span style=color:green># See https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md.</span>
    type: &lt;some-provider-name&gt; <span style=color:green># {aws,azure,gcp,...}</span>
  <span style=color:green># region: eu-west-1</span>
    secretRef:
      name: backup-secret
      namespace: garden
status:
  conditions:
  - lastTransitionTime: <span style=color:#a31515>&#34;2020-07-14T19:16:42Z&#34;</span>
    lastUpdateTime: <span style=color:#a31515>&#34;2020-07-14T19:18:17Z&#34;</span>
    message: all checks passed
    reason: Passed
    status: <span style=color:#a31515>&#34;True&#34;</span>
    type: Available
  gardener:
    id: 4c9832b3823ee6784064877d3eb10c189fc26e98a1286c0d8a5bc82169ed702c
    name: gardener-controller-manager-7fhn9ikan73n-7jhka
    version: 1.0.0
  observedGeneration: 1
</code></pre></div><h3 id=project-resource><code>Project</code> resource</h3><p>Special note: The <code>members</code> and <code>viewers</code> field of the <code>garden.sapcloud.io/v1beta1.Project</code> resource will be merged together into one <code>members</code> field.
Every member will have a role that is either <code>admin</code> or <code>viewer</code>.
This will allow us to add new roles without changing the API.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: Project
metadata:
  name: example
spec:
  description: Example project
  members:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: john.doe@example.com
    role: admin
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: joe.doe@example.com
    role: viewer
  namespace: garden-example
  owner:
    apiGroup: rbac.authorization.k8s.io
    kind: User
    name: john.doe@example.com
  purpose: Example project
status:
  observedGeneration: 1
  phase: Ready
</code></pre></div><h3 id=secretbinding-resource><code>SecretBinding</code> resource</h3><p>Special note: No modifications needed compared to the current <code>garden.sapcloud.io/v1beta1.SecretBinding</code> resource.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Secret
metadata:
  name: secret1
  namespace: garden-core
type: Opaque
data:
  <span style=color:green># &lt;some-provider-specific data keys&gt;</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-infrastructure.yaml#L14-L15</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-infrastructure.yaml#L9-L10</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-infrastructure.yaml#L14-L17</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-infrastructure.yaml#L14</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-infrastructure.yaml#L15-L18</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-packet/blob/master/example/30-infrastructure.yaml#L14-L15</span>
  <span style=color:green>#</span>
  <span style=color:green># If you use your own domain (not the default domain of your landscape) then you have to add additional keys to this secret.</span>
  <span style=color:green># The reason is that the DNS management is not part of the Gardener core code base but externalized, hence, it might use other</span>
  <span style=color:green># key names than Gardener itself.</span>
  <span style=color:green># The actual values here depend on the DNS extension that is installed to your landscape.</span>
  <span style=color:green># For example, check out https://github.com/gardener/external-dns-management and find a lot of example secret manifests here:</span>
  <span style=color:green># https://github.com/gardener/external-dns-management/tree/master/examples</span>

---
apiVersion: core.gardener.cloud/v1beta1
kind: SecretBinding
metadata:
  name: secretbinding1
  namespace: garden-core
secretRef:
  name: secret1
<span style=color:green># namespace: namespace-other-than-&#39;garden-core&#39; // optional</span>
quotas: []
<span style=color:green># - name: quota-1</span>
<span style=color:green># # namespace: namespace-other-than-&#39;garden-core&#39; // optional</span>
</code></pre></div><h3 id=quota-resource><code>Quota</code> resource</h3><p>Special note: No modifications needed compared to the current <code>garden.sapcloud.io/v1beta1.Quota</code> resource.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: Quota
metadata:
  name: trial-quota
  namespace: garden-trial
spec:
  scope:
    apiGroup: core.gardener.cloud
    kind: Project
<span style=color:green># clusterLifetimeDays: 14</span>
  metrics:
    cpu: <span style=color:#a31515>&#34;200&#34;</span>
    gpu: <span style=color:#a31515>&#34;20&#34;</span>
    memory: 4000Gi
    storage.standard: 8000Gi
    storage.premium: 2000Gi
    loadbalancer: <span style=color:#a31515>&#34;100&#34;</span>
</code></pre></div><h3 id=backupbucket-resource><code>BackupBucket</code> resource</h3><p>Special note: This new resource is cluster-scoped.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green># See also: https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md.</span>

apiVersion: v1
kind: Secret
metadata:
  name: backup-operator-provider
  namespace: backup-garden
type: Opaque
data:
  <span style=color:green># &lt;some-provider-specific data keys&gt;</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-backupbucket.yaml#L9-L11</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-backupbucket.yaml#L9-L10</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-backupbucket.yaml#L9-L10</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-backupbucket.yaml#L9</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-backupbucket.yaml#L9-L13</span>

---
apiVersion: core.gardener.cloud/v1beta1
kind: BackupBucket
metadata:
  name: &lt;seed-provider-type&gt;-&lt;region&gt;-&lt;seed-uid&gt;
  ownerReferences:
  - kind: Seed
    name: seed1
spec:
  provider:
    type: &lt;some-provider-name&gt; <span style=color:green># {aws,azure,gcp,...}</span>
    region: europe-central-1
  seed: seed1
  secretRef:
    name: backup-operator-provider
    namespace: backup-garden
status:
  lastOperation:
    description: Backup bucket has been successfully reconciled.
    lastUpdateTime: <span style=color:#a31515>&#39;2020-04-13T14:34:27Z&#39;</span>
    progress: 100
    state: Succeeded
    type: Reconcile
  observedGeneration: 1
</code></pre></div><h3 id=backupentry-resource><code>BackupEntry</code> resource</h3><p>Special note: This new resource is cluster-scoped.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green># See also: https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md.</span>

apiVersion: v1
kind: Secret
metadata:
  name: backup-operator-provider
  namespace: backup-garden
type: Opaque
data:
  <span style=color:green># &lt;some-provider-specific data keys&gt;</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-backupbucket.yaml#L9-L11</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-backupbucket.yaml#L9-L10</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-backupbucket.yaml#L9-L10</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-backupbucket.yaml#L9</span>
  <span style=color:green># https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-backupbucket.yaml#L9-L13</span>

---
apiVersion: core.gardener.cloud/v1beta1
kind: BackupEntry
metadata:
  name: shoot--core--crazy-botany--3ef42
  namespace: garden-core
  ownerReferences:
  - apiVersion: core.gardener.cloud/v1beta1
    blockOwnerDeletion: <span style=color:#00f>false</span>
    controller: <span style=color:#00f>true</span>
    kind: Shoot
    name: crazy-botany
    uid: 19a9538b-5058-11e9-b5a6-5e696cab3bc8
spec:
  bucketName: cloudprofile1-random[:5]
  seed: seed1
status:
  lastOperation:
    description: Backup entry has been successfully reconciled.
    lastUpdateTime: <span style=color:#a31515>&#39;2020-04-13T14:34:27Z&#39;</span>
    progress: 100
    state: Succeeded
    type: Reconcile
  observedGeneration: 1
</code></pre></div><h3 id=shoot-resource><code>Shoot</code> resource</h3><p>Special notes:</p><ul><li><code>kubelet</code> configuration in the worker pools may override the default <code>.spec.kubernetes.kubelet</code> configuration (that applies for all worker pools if not overridden).</li><li>Moved remaining control plane configuration to new <code>.spec.provider.controlplane</code> section.</li></ul><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: Shoot
metadata:
  name: crazy-botany
  namespace: garden-core
spec:
  secretBindingName: secretbinding1
  cloudProfileName: cloudprofile1
  region: europe-central-1
<span style=color:green># seedName: seed1</span>
  provider:
    type: &lt;some-provider-name&gt; <span style=color:green># {aws,azure,gcp,...}</span>
    infrastructureConfig:
      &lt;some-provider-specific-infrastructure-config&gt;
      <span style=color:green># https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-infrastructure.yaml#L56-L64</span>
      <span style=color:green># https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-infrastructure.yaml#L43-L53</span>
      <span style=color:green># https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-infrastructure.yaml#L63-L71</span>
      <span style=color:green># https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-infrastructure.yaml#L53-L57</span>
      <span style=color:green># https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-infrastructure.yaml#L56-L64</span>
      <span style=color:green># https://github.com/gardener/gardener-extension-provider-packet/blob/master/example/30-infrastructure.yaml#L48-L49</span>
    controlPlaneConfig:
      &lt;some-provider-specific-controlplane-config&gt;
      <span style=color:green># https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-controlplane.yaml#L60-L65</span>
      <span style=color:green># https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-controlplane.yaml#L60-L64</span>
      <span style=color:green># https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/30-controlplane.yaml#L61-L66</span>
      <span style=color:green># https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-controlplane.yaml#L59-L64</span>
      <span style=color:green># https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/30-controlplane.yaml#L64-L70</span>
      <span style=color:green># https://github.com/gardener/gardener-extension-provider-packet/blob/master/example/30-controlplane.yaml#L60-L61</span>
    workers:
    - name: cpu-worker
      minimum: 3
      maximum: 5
    <span style=color:green># maxSurge: 1</span>
    <span style=color:green># maxUnavailable: 0</span>
      machine:
        type: m5.large
        image:
          name: &lt;some-os-name&gt;
          version: &lt;some-os-version&gt;
        <span style=color:green># providerConfig:</span>
        <span style=color:green>#   &lt;some-os-specific-configuration&gt;</span>
      volume:
        type: gp2
        size: 20Gi
    <span style=color:green># providerConfig:</span>
    <span style=color:green>#   &lt;some-provider-specific-worker-config&gt;</span>
    <span style=color:green># labels:</span>
    <span style=color:green>#   key: value</span>
    <span style=color:green># annotations:</span>
    <span style=color:green>#   key: value</span>
    <span style=color:green># taints: # See also https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/</span>
    <span style=color:green># - key: foo</span>
    <span style=color:green>#   value: bar</span>
    <span style=color:green>#   effect: NoSchedule</span>
    <span style=color:green># caBundle: &lt;some-ca-bundle-to-be-installed-to-all-nodes-in-this-pool&gt;</span>
    <span style=color:green># kubernetes:</span>
    <span style=color:green>#   kubelet:</span>
    <span style=color:green>#     cpuCFSQuota: true</span>
    <span style=color:green>#     cpuManagerPolicy: none</span>
    <span style=color:green>#     podPidsLimit: 10</span>
    <span style=color:green>#     featureGates:</span>
    <span style=color:green>#       SomeKubernetesFeature: true</span>
    <span style=color:green># zones: # optional, only relevant if the provider supports availability zones</span>
    <span style=color:green># - europe-central-1a</span>
    <span style=color:green># - europe-central-1b</span>
  kubernetes:
    version: 1.15.1
  <span style=color:green># allowPrivilegedContainers: true # &#39;true&#39; means that all authenticated users can use the &#34;gardener.privileged&#34; PodSecurityPolicy, allowing full unrestricted access to Pod features.</span>
  <span style=color:green># kubeAPIServer:</span>
  <span style=color:green>#   featureGates:</span>
  <span style=color:green>#     SomeKubernetesFeature: true</span>
  <span style=color:green>#   runtimeConfig:</span>
  <span style=color:green>#     scheduling.k8s.io/v1alpha1: true</span>
  <span style=color:green>#   oidcConfig:</span>
  <span style=color:green>#     caBundle: |</span>
  <span style=color:green>#       -----BEGIN CERTIFICATE-----</span>
  <span style=color:green>#       Li4u</span>
  <span style=color:green>#       -----END CERTIFICATE-----</span>
  <span style=color:green>#     clientID: client-id</span>
  <span style=color:green>#     groupsClaim: groups-claim</span>
  <span style=color:green>#     groupsPrefix: groups-prefix</span>
  <span style=color:green>#     issuerURL: https://identity.example.com</span>
  <span style=color:green>#     usernameClaim: username-claim</span>
  <span style=color:green>#     usernamePrefix: username-prefix</span>
  <span style=color:green>#     signingAlgs: RS256,some-other-algorithm</span>
  <span style=color:green>#-#-# only usable with Kubernetes &gt;= 1.11</span>
  <span style=color:green>#     requiredClaims:</span>
  <span style=color:green>#       key: value</span>
  <span style=color:green>#   admissionPlugins:</span>
  <span style=color:green>#   - name: PodNodeSelector</span>
  <span style=color:green>#     config: |</span>
  <span style=color:green>#       podNodeSelectorPluginConfig:</span>
  <span style=color:green>#         clusterDefaultNodeSelector: &lt;node-selectors-labels&gt;</span>
  <span style=color:green>#         namespace1: &lt;node-selectors-labels&gt;</span>
  <span style=color:green>#         namespace2: &lt;node-selectors-labels&gt;</span>
  <span style=color:green>#   auditConfig:</span>
  <span style=color:green>#     auditPolicy:</span>
  <span style=color:green>#       configMapRef:</span>
  <span style=color:green>#         name: auditpolicy</span>
  <span style=color:green># kubeControllerManager:</span>
  <span style=color:green>#   featureGates:</span>
  <span style=color:green>#     SomeKubernetesFeature: true</span>
  <span style=color:green>#   horizontalPodAutoscaler:</span>
  <span style=color:green>#     syncPeriod: 30s</span>
  <span style=color:green>#     tolerance: 0.1</span>
  <span style=color:green>#-#-# only usable with Kubernetes &lt; 1.12</span>
  <span style=color:green>#     downscaleDelay: 15m0s</span>
  <span style=color:green>#     upscaleDelay: 1m0s</span>
  <span style=color:green>#-#-# only usable with Kubernetes &gt;= 1.12</span>
  <span style=color:green>#     downscaleStabilization: 5m0s</span>
  <span style=color:green>#     initialReadinessDelay: 30s</span>
  <span style=color:green>#     cpuInitializationPeriod: 5m0s</span>
  <span style=color:green># kubeScheduler:</span>
  <span style=color:green>#   featureGates:</span>
  <span style=color:green>#     SomeKubernetesFeature: true</span>
  <span style=color:green># kubeProxy:</span>
  <span style=color:green>#   featureGates:</span>
  <span style=color:green>#     SomeKubernetesFeature: true</span>
  <span style=color:green>#   mode: IPVS</span>
  <span style=color:green># kubelet:</span>
  <span style=color:green>#   cpuCFSQuota: true</span>
  <span style=color:green>#   cpuManagerPolicy: none</span>
  <span style=color:green>#   podPidsLimit: 10</span>
  <span style=color:green>#   featureGates:</span>
  <span style=color:green>#     SomeKubernetesFeature: true</span>
  <span style=color:green># clusterAutoscaler:</span>
  <span style=color:green>#   scaleDownUtilizationThreshold: 0.5</span>
  <span style=color:green>#   scaleDownUnneededTime: 30m</span>
  <span style=color:green>#   scaleDownDelayAfterAdd: 60m</span>
  <span style=color:green>#   scaleDownDelayAfterFailure: 10m</span>
  <span style=color:green>#   scaleDownDelayAfterDelete: 10s</span>
  <span style=color:green>#   scanInterval: 10s</span>
  dns:
    <span style=color:green># When the shoot shall use a cluster domain no domain and no providers need to be provided - Gardener will</span>
    <span style=color:green># automatically compute a correct domain.</span>
    domain: crazy-botany.core.my-custom-domain.com
    providers:
    - type: aws-route53
      secretName: my-custom-domain-secret
      domains:
        include:
        - my-custom-domain.com
        - my-other-custom-domain.com
        exclude:
        - yet-another-custom-domain.com
      zones:
        include:
        - zone-id-1
        exclude:
        - zone-id-2
  extensions:
  - type: foobar
  <span style=color:green># providerConfig:</span>
  <span style=color:green>#   apiVersion: foobar.extensions.gardener.cloud/v1alpha1</span>
  <span style=color:green>#   kind: FooBarConfiguration</span>
  <span style=color:green>#   foo: bar</span>
  networking:
    type: calico
    pods: 100.96.0.0/11
    services: 100.64.0.0/13
    nodes: 10.250.0.0/16
  <span style=color:green># providerConfig:</span>
  <span style=color:green>#   apiVersion: calico.extensions.gardener.cloud/v1alpha1</span>
  <span style=color:green>#   kind: NetworkConfig</span>
  <span style=color:green>#   ipam:</span>
  <span style=color:green>#     type: host-local</span>
  <span style=color:green>#     cidr: usePodCIDR</span>
  <span style=color:green>#   backend: bird</span>
  <span style=color:green>#   typha:</span>
  <span style=color:green>#     enabled: true</span>
  <span style=color:green># See also: https://github.com/gardener/gardener/blob/master/docs/proposals/03-networking.md</span>
  maintenance:
    timeWindow:
      begin: 220000+0100
      end: 230000+0100
    autoUpdate:
      kubernetesVersion: <span style=color:#00f>true</span>
      machineImageVersion: <span style=color:#00f>true</span>
<span style=color:green># hibernation:</span>
<span style=color:green>#   enabled: false</span>
<span style=color:green>#   schedules:</span>
<span style=color:green>#   - start: &#34;0 20 * * *&#34; # Start hibernation every day at 8PM</span>
<span style=color:green>#     end: &#34;0 6 * * *&#34;    # Stop hibernation every day at 6AM</span>
<span style=color:green>#     location: &#34;America/Los_Angeles&#34; # Specify a location for the cron to run in</span>
  addons:
    nginx-ingress:
      enabled: <span style=color:#00f>false</span>
    <span style=color:green># loadBalancerSourceRanges: []</span>
    kubernetes-dashboard:
      enabled: <span style=color:#00f>true</span>
    <span style=color:green># authenticationMode: basic # allowed values: basic,token</span>
status:
  conditions:
  - type: APIServerAvailable
    status: <span style=color:#a31515>&#39;True&#39;</span>
    lastTransitionTime: <span style=color:#a31515>&#39;2020-01-30T10:38:15Z&#39;</span>
    lastUpdateTime: <span style=color:#a31515>&#39;2020-04-13T14:35:21Z&#39;</span>
    reason: HealthzRequestFailed
    message: API server /healthz endpoint responded with success status code. [response_time:3ms]
  - type: ControlPlaneHealthy
    status: <span style=color:#a31515>&#39;True&#39;</span>
    lastTransitionTime: <span style=color:#a31515>&#39;2020-04-02T05:18:58Z&#39;</span>
    lastUpdateTime: <span style=color:#a31515>&#39;2020-04-13T14:35:21Z&#39;</span>
    reason: ControlPlaneRunning
    message: All control plane components are healthy.
  - type: EveryNodeReady
    status: <span style=color:#a31515>&#39;True&#39;</span>
    lastTransitionTime: <span style=color:#a31515>&#39;2020-04-01T16:27:21Z&#39;</span>
    lastUpdateTime: <span style=color:#a31515>&#39;2020-04-13T14:35:21Z&#39;</span>
    reason: EveryNodeReady
    message: Every node registered to the cluster is ready.
  - type: SystemComponentsHealthy
    status: <span style=color:#a31515>&#39;True&#39;</span>
    lastTransitionTime: <span style=color:#a31515>&#39;2020-04-03T18:26:28Z&#39;</span>
    lastUpdateTime: <span style=color:#a31515>&#39;2020-04-13T14:35:21Z&#39;</span>
    reason: SystemComponentsRunning
    message: All system components are healthy.
  gardener:
    id: 4c9832b3823ee6784064877d3eb10c189fc26e98a1286c0d8a5bc82169ed702c
    name: gardener-controller-manager-7fhn9ikan73n-7jhka
    version: 1.0.0
  lastOperation:
    description: Shoot cluster state has been successfully reconciled.
    lastUpdateTime: <span style=color:#a31515>&#39;2020-04-13T14:34:27Z&#39;</span>
    progress: 100
    state: Succeeded
    type: Reconcile
  observedGeneration: 1
  seed: seed1
  hibernated: <span style=color:#00f>false</span>
  technicalID: shoot--core--crazy-botany
  uid: d8608cfa-2856-11e8-8fdc-0a580af181af
</code></pre></div><h3 id=plant-resource><code>Plant</code> resource</h3><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Secret
metadata:
  name: crazy-plant-secret
  namespace: garden-core
type: Opaque
data:
  kubeconfig: base64(kubeconfig-for-plant-cluster)

---
apiVersion: core.gardener.cloud/v1beta1
kind: Plant
metadata:
  name: crazy-plant
  namespace: garden-core
spec:
  secretRef:
    name: crazy-plant-secret
  endpoints:
  - name: Cluster GitHub repository
    purpose: management
    url: https://github.com/my-org/my-cluster-repo
  - name: GKE cluster page
    purpose: management
    url: https://console.cloud.google.com/kubernetes/clusters/details/europe-west1-b/plant?project=my-project&amp;authuser=1&amp;tab=details
status:
  clusterInfo:
    provider:
      type: gce
      region: europe-west4-c
    kubernetes:
      version: v1.11.10-gke.5
  conditions:
  - lastTransitionTime: <span style=color:#a31515>&#34;2020-03-01T11:31:37Z&#34;</span>
    lastUpdateTime: <span style=color:#a31515>&#34;2020-04-14T18:00:29Z&#34;</span>
    message: API server /healthz endpoint responded with success status code. [response_time:8ms]
    reason: HealthzRequestFailed
    status: <span style=color:#a31515>&#34;True&#34;</span>
    type: APIServerAvailable
  - lastTransitionTime: <span style=color:#a31515>&#34;2020-04-01T06:26:56Z&#34;</span>
    lastUpdateTime: <span style=color:#a31515>&#34;2020-04-14T18:00:29Z&#34;</span>
    message: Every node registered to the cluster is ready.
    reason: EveryNodeReady
    status: <span style=color:#a31515>&#34;True&#34;</span>
    type: EveryNodeReady
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-d76ec792a311963802e5b8c02ff3c5d5>5 - 05 Versioning Policy</h1><h1 id=gardener-versioning-policy>Gardener Versioning Policy</h1><p>Please refer to <a href=/docs/gardener/usage/shoot_versions/>this document</a> for the documentation of the implementation of this GEP.</p><h2 id=goal>Goal</h2><ul><li>As a Garden operator I would like to define a clear Kubernetes version policy, which informs my users about deprecated or expired Kubernetes versions.</li><li>As an user of Gardener, I would like to get information which Kubernetes version is supported for how long. I want to be able to get this information via API (cloudprofile) and also in the Dashboard.</li></ul><h2 id=motivation>Motivation</h2><p>The Kubernetes community releases <strong>minor</strong> versions roughly every three months and usually maintains <strong>three minor</strong> versions (the actual and the last two) with bug fixes and security updates. Patch releases are done more frequently. Operators of Gardener should be able to define their own Kubernetes version policy. This GEP suggests the possibility for operators to classify Kubernetes versions, while they are going through their &ldquo;maintenance life-cycle&rdquo;.</p><h2 id=kubernetes-version-classifications>Kubernetes Version Classifications</h2><p>An operator should be able to classify Kubernetes versions differently while they go through their &ldquo;maintenance life-cycle&rdquo;, starting with <strong>preview</strong>, <strong>supported</strong>, <strong>deprecated</strong>, and finally <strong>expired</strong>. This information should be programmatically available in the <code>cloudprofiles</code> of the Garden cluster as well as in the Dashboard. Please also note, that Gardener keeps the control plane and the workers on the same Kubernetes version.</p><p>For further explanation of the possible classifications, we assume that an operator wants to support four minor versions e.g. v1.16, v1.15, v1.14 and v1.13.</p><ul><li><p><strong>preview:</strong> After a fresh release of a new Kubernetes <strong>minor</strong> version (e.g. v1.17.0) the operator could tag it as <em>preview</em> until he has gained sufficient experience. It will not become the default in the Gardener Dashboard until he promotes that minor version to <em>supported</em>, which could happen a few weeks later with the first patch version.</p></li><li><p><strong>supported:</strong> The operator would tag the latest Kubernetes patch versions of the actual (if not still in <em>preview</em>) and the last three minor Kubernetes versions as <em>supported</em> (e.g. v1.16.1, v1.15.4, v1.14.9 and v1.13.12). The latest of these becomes the default in the Gardener Dashboard (e.g. v1.16.1).</p></li><li><p><strong>deprecated:</strong> The operator could decide, that he generally wants to classify every version that is not the latest patch version as <em>deprecated</em> and flag this versions accordingly (e.g. v1.16.0 and older, v1.15.3 and older, 1.14.8 and older as well as v1.13.11 and older). He could also tag all versions (latest or not) of every Kubernetes minor release that is neither the actual nor one of the last three minor Kubernetes versions as <em>deprecated</em>, too (e.g. v1.12.x and older). Deprecated versions will eventually expire (i.e., removed).</p></li><li><p><strong>expired:</strong> This state is a <em>logical</em> state only. It doesn&rsquo;t have to be maintained in the <code>cloudprofile</code>. All cluster versions whose <code>expirationDate</code> as defined in the <code>cloudprofile</code> is expired, are automatically in this <em>logical</em> state. After that date has passed, users cannot create new clusters with that version anymore and any cluster that is on that version will be forcefully migrated in its next maintenance time window, even if the owner has opted out of automatic cluster updates! The forceful update will pick the latest patch version of the current minor Kubernetes version. If the cluster was already on that latest patch version and the latest patch version is also expired, it will continue with latest patch version of the <strong>next minor Kubernetes version</strong>, so <strong>it will result in an update of a minor Kubernetes version, which is potentially harmful to your workload, so you should avoid that/plan ahead!</strong> If that&rsquo;s expired as well, the update process repeats until a non-expired Kubernetes version is reached, so <strong>depending on the circumstances described above, it can happen that the cluster receives multiple consecutive minor Kubernetes version updates!</strong></p></li></ul><p>To fulfill his specific versioning policy, the Garden operator should be able to classify his versions as well set the expiration date in the <code>cloudprofiles</code>. The user should see this classifiers as well as the expiration date in the dashboard.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-65e79b644bcfac2ee0ea485728885ad7>6 - 06 Etcd Drui</h1><h1 id=integrating-etcd-druid-with-gardener>Integrating etcd-druid with Gardener</h1><p>Etcd is currently deployed by garden-controller-manager as a Statefulset. The sidecar container spec contains details pertaining to cloud-provider object-store which is injected into the statefulset via a mutable webhook running as part of the gardener extension <a href=/docs/gardener/extensions/controlplane-webhooks/#what-needs-to-be-implemented-to-support-a-new-cloud-provider>story</a>. This approach restricts the operations on etcd such as scale-up and upgrade. Etcd-druid will eliminate the need to hijack statefulset creation to add cloudprovider details. It has been designed to provide an intricate control over the procedure of deploying and maintaining etcd. The roadmap for etcd-druid can be found <a href=https://github.com/gardener/etcd-druid/issues/2>here</a>.</p><p>This document explains how Gardener deploys etcd and what resources it creates for etcd-druid to deploy an etcd cluster.</p><h2 id=resources-required-by-etcd-druid-created-by-gardener>Resources required by etcd-druid (created by Gardener)</h2><ul><li>Secret containing credentials to access backup bucket in Cloud provider object store.</li><li>TLS server and client secrets for etcd and backup-sidecar</li><li>Etcd CRD resource that contains parameters pertaining to etcd, backup-sidecar and cloud-provider object store.</li></ul><p>When an etcd resource is created in the cluster, the druid acts on it by creating an etcd statefulset, a service and a configmap containing etcd bootstrap script. The secrets containing the infrastructure credentials and the TLS certificates are mounted as volumes. If no secret/information regarding backups is stated then etcd data backups are not taken. Only data corruption checks are performed prior to starting etcd.</p><p>Garden-controller-manager, being cloud agnostic, deploys the etcd resource. This will not contain any cloud-specific information other than the cloud-provider. The extension controller that contains the cloud specific implementation to create the backup bucket will create it if needed and create a secret containing the credentials to access the bucket. The etcd backup secret name should be exposed in the BackupEntry status. Then, Gardener can read it and write it into the ETCD resource. The secret will have to be made available in the namespace the etcd statefulset will be deployed. If etcd and backup-sidecar communicates over TLS then the CA certificates, server and client certificates, and keys will also have to be made available in the namespace as well. The etcd resource will have reference to these aforementioned secrets. etcd-druid will deploy the statefulset only if the secrets are available.</p><h2 id=workflow>Workflow</h2><ul><li>etcd-druid will be deployed and etcd CRD will be created as part of the seed bootstrap.</li><li>Garden-controller-manager creates backupBucket extension resource. Extension controller creates the backup bucket associated with the seed.</li><li>Garden-controller-manager creates backupentry associated with each shoot in the seed namespace.</li><li>Garden-controller-manager creates etcd resource with secretRefs and etcd information populated appropriately.</li><li>etcd-druid acts on the etcd resource; druid creates the statefulset, the service and the configmap.</li></ul><p><img src=/__resources/druid_integration_539d05.png alt=etcd-druid></p></div><div class=td-content style=page-break-before:always><h1 id=pg-8da6d960cda1d76486946f507800f02b>7 - 07 Shoot Control Plane Migration</h1><h1 id=shoot-control-plane-migration>Shoot Control Plane Migration</h1><h2 id=motivation>Motivation</h2><p>Currently moving the control plane of a shoot cluster can only be done manually and requires deep knowledge of how exactly to transfer the resources and state from one seed to another. This can make it slow and prone to errors.</p><p>Automatic migration can be very useful in a couple of scenarios:</p><ul><li>Seed goes down and can&rsquo;t be repaired (fast enough or at all) and it&rsquo;s control planes need to be brought to another seed</li><li>Seed needs to be changed, but this operation requires the recreation of the seed (e.g. turn a single-AZ seed into a multi-AZ seed)</li><li>Seeds need to be rebalanced</li><li>New seeds become available in a region closer to/in the region of the workers and the control plane should be moved there to improve latency</li><li>Gardener ring, which is a self-supporting setup/underlay for a highly available (usually cross-region) Gardener deployment</li></ul><h2 id=goals>Goals</h2><ul><li>Provide a mechanism to migrate the control plane of a shoot cluster from one seed to another</li><li>The mechanism should support migration from a seed which is no longer reachable (Disaster Recovery)</li><li>The shoot cluster nodes are preserved and continue to run the workload, but will talk to the new control plane after the migration completes</li><li>Extension controllers implement a mechanism which allows them to store their state or to be restored from an already existing state on a different seed cluster.</li><li>The already existing shoot reconciliation flow is reused for migration with minimum changes</li></ul><h2 id=terminology>Terminology</h2><p><strong>Source Seed</strong> is the seed which currently hosts the control plane of a Shoot Cluster</p><p><strong>Destination Seed</strong> is the seed to which the control plane is being migrated</p><h2 id=resources-and-controller-state-which-have-to-be-migrated-between-two-seeds>Resources and controller state which have to be migrated between two seeds:</h2><p><strong>Note:</strong> The following lists are just FYI and are meant to show the current resources which need to be moved to the <strong>Destination Seed</strong></p><h3 id=secrets>Secrets</h3><p>Gardener has preconfigured lists of needed secrets which are generated when a shoot is created and deployed in the seed. Following is a minimum set of secrets which must be migrated to the <strong>Destination Seed</strong>. Other secrets can be regenerated from them.</p><ul><li>ca</li><li>ca-front-proxy</li><li>static-token</li><li>ca-kubelet</li><li>ca-metrics-server</li><li>etcd-encryption-secret</li><li>kube-aggregator</li><li>kube-apiserver-basic-auth</li><li>kube-apiserver</li><li>service-account-key</li><li>ssh-keypair</li></ul><h3 id=custom-resources-and-state-of-extension-controllers>Custom Resources and state of extension controllers</h3><p>Gardenlet deploys custom resources in the <strong>Source Seed</strong> cluster during shoot reconciliation which are reconciled by extension controllers. The state of these controllers and any additional resources they create is independent of the gardenlet and must also be migrated to the <strong>Destination Seed</strong>. Following is a list of custom resources, and the state which is generated by them that has to be migrated.</p><ul><li><strong>BackupBucket</strong>: nothing relevant for migration</li><li><strong>BackupEntry</strong>: nothing relevant for migration</li><li><strong>ControlPlane</strong>: nothing relevant for migration</li><li><strong>DNSProvider</strong>/DNSEntry: nothing relevant for migration</li><li><strong>Extensions</strong>: migration of state needs to be handled individually</li><li><strong>Infrastructure</strong>: terraform state</li><li><strong>Network</strong>: nothing relevant for migration</li><li><strong>OperatingSystemConfig</strong>: nothing relevant for migration</li><li><strong>Worker</strong>: Machine-Controller-Manager related objects: machineclasses, machinedeployments, machinesets, machines</li></ul><p>This list depends on the currently installed extensions and can change in the future</p><h2 id=proposal>Proposal</h2><h3 id=custom-resource-on-the-garden-cluster>Custom Resource on the garden cluster</h3><p>The Garden cluster has a new Custom Resource which is stored in the project namespace of the Shoot called <code>ShootState</code>. It contains all the required data described above so that the control plane can be recreated on the <strong>Destination Seed</strong>.</p><p>This data is separated into two sections. The first is generated by the gardenlet and then either used to generate new resources (e.g secrets) or is directly deployed to the Shoot&rsquo;s control plane on the <strong>Destination Seed</strong>.</p><p>The second is generated by the extension controllers in the seed.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1alpha1
kind: ShootState
metadata:
  name: my-shoot
  namespace: garden-core
  ownerReference:
    apiVersion: core.gardener.cloud/v1beta1
    blockOwnerDeletion: <span style=color:#00f>true</span>
    controller: <span style=color:#00f>true</span>
    kind: Shoot
    name: my-shoot
    uid: ...
  finalizers:
  - gardener
gardenlet:
  secrets:
  - name: ca
    data:
      ca.crt: ...
      ca.key: ...
  - name: ssh-keypair
    data:
      id_rsa: ...
  - name:
...
extensions:
- kind: Infrastructure
  state: ... (Terraform state)
- kind: ControlPlane
  purpose: normal
  state: ... (Certificates generated by the extension)
- kind: Worker
  state: ... (Machine objects)
</code></pre></div><p>The state data is saved as a <code>runtime.RawExtension</code> type, which can be encoded/decoded by the corresponding extension controller.</p><p>There can be sensitive data in the <code>ShootState</code> which has to be hidden from the end-users. Hence, it will be recommended to provide an etcd encryption configuration to the Gardener API server in order to encrypt the <code>ShootState</code> resource.</p><h4 id=size-limitations>Size limitations</h4><p>There are limits on the size of the request bodies sent to the kubernetes API server when creating or updating resources: by default ETCD can only accept request bodies which do not exceed 1.5 MiB (this can be configured with the <code>--max-request-bytes</code> flag); the kubernetes API Server has a request body limit of 3 MiB which cannot be set from the outside (with a command line flag); the gRPC configuration used by the API server to talk to ETCD has a limit of 2 MiB per request body which cannot be configured from the outside; and <code>watch</code> requests have a 16 MiB limit on the buffer used to stream resources.</p><p>This means that if <code>ShootState</code> is bigger than 1.5 MiB, the ETCD max request bytes will have to be increased. However, there is still an upper limit of 2 MiB imposed by the gRPC configuration.</p><p>If <code>ShootState</code> exceeds this size limitation it must make use of configmap/secret references to store the state of extension controllers. This is an implementation detail of Gardener and can be done at a later time if necessary as extensions will not be affected.</p><p>Splitting the <code>ShootState</code> into multiple resources could have a positive benefit on performance as the Gardener API Server and Gardener Controller Manager would handle multiple small resources instead of one big resource.</p><h3 id=gardener-extensions-changes>Gardener extensions changes</h3><p>All extension controllers which require state migration must save their state in a new <code>status.state</code> field and act on an annotation <code>gardener.cloud/operation=restore</code> in the respective Custom Resources which should trigger a restoration operation instead of reconciliation. A restoration operation means that the extension has to restore its state in the Shoot&rsquo;s namespace on the <strong>Destination Seed</strong> from the <code>status.state</code> field.</p><p>As an example: the <code>Infrastructure</code> resource must save the terraform state.</p><pre><code>apiVersion: extensions.gardener.cloud/v1alpha1
kind: Infrastructure
metadata:
  name: infrastructure
  namespace: shoot--foo--bar
spec:
  type: azure
  region: eu-west-1
  secretRef:
    name: cloudprovider
    namespace: shoot--foo--bar
  providerConfig:
    apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
    kind: InfrastructureConfig
    resourceGroup:
      name: mygroup
    networks:
      vnet: # specify either 'name' or 'cidr'
      # name: my-vnet
        cidr: 10.250.0.0/16
      workers: 10.250.0.0/19
status:
  state: |
      {
          &quot;version&quot;: 3,
          &quot;terraform_version&quot;: &quot;0.11.14&quot;,
          &quot;serial&quot;: 2,
          &quot;lineage&quot;: &quot;3a1e2faa-e7b6-f5f0-5043-368dd8ea6c10&quot;,
          &quot;modules&quot;: [
              {
              }
          ]
          ...
      }
</code></pre><p>Extensions which do not require state migration should set <code>status.state=nil</code> in their Custom Resources and trigger a normal reconciliation operation if the CR contains the <code>core.gardener.cloud/operation=restore</code> annotation.</p><p>Similar to the contract for the <a href=/docs/gardener/extensions/reconcile-trigger/>reconcile operation</a>, the extension controller has to remove the <code>restore</code> annotation after the restoration operation has finished.</p><p>An additional annotation <code>gardener.cloud/operation=migrate</code> is added to the Custom Resources. It is used to tell the extension controllers in the <strong>Source Seed</strong> that they must stop reconciling resources (in case they are requeued due to errors) and should perform cleanup activities in the Shoot&rsquo;s control plane. These cleanup activities involve removing the finalizers on Custom Resources and deleting them without actually deleting any infrastructure resources.</p><p><strong>Note:</strong> The same size limitations from the previous section are relevant here as well.</p><h3 id=shoot-reconciliation-flow-changes>Shoot reconciliation flow changes</h3><p>The only data which must be stored in the <code>ShootState</code> by the gardenlet is secrets (e.g ca for the API server). Therefore the <code>botanist.DeploySecrets</code> step is changed. It is split into two functions which take a list of secrets that have to be generated.</p><ul><li><code>botanist.GenerateSecretState</code> Generates certificate authorities and other secrets which have to be persisted in the ShootState and must not be regenerated on the <strong>Destination Seed</strong>.</li><li><code>botanist.DeploySecrets</code> Takes secret data from the <code>ShootState</code>, generates new ones (e.g. client tls certificates from the saved certificate authorities) and deploys everything in the Shoot&rsquo;s control plane on the <strong>Destination Seed</strong></li></ul><h3 id=shootstate-synchronization-controller>ShootState synchronization controller</h3><p>The ShootState synchronization controller will become part of the gardenlet. It syncs the state of extension custom resources from the shoot namespace to the garden cluster and updates the corresponding <code>spec.extension.state</code> field in the <code>ShootState</code> resource. The controller can <code>watch</code> Custom Resources used by the extensions and update the <code>ShootState</code> only when changes occur.</p><h3 id=migration-workflow>Migration workflow</h3><ol><li>Starting migration<ul><li>Migration can only be started after a Shoot cluster has been successfully created so that the <code>status.seed</code> field in the <code>Shoot</code> resource has been set</li><li>The <code>Shoot</code> resource&rsquo;s field <code>spec.seedName="new-seed"</code> is edited to hold the name of the <strong>Destination Seed</strong> and reconciliation is automatically triggered</li><li>The Garden Controller Manager checks if the equality between <code>spec.seedName</code> and <code>status.seed</code>, detects that they are different and triggers migration.</li></ul></li><li>The Garden Controller Manager waits for the <strong>Destination Seed</strong> to be ready</li><li>Shoot&rsquo;s API server is stopped</li><li>Backup the Shoot&rsquo;s ETCD.</li><li>Extension resources in the <strong>Source Seed</strong> are annotated with <code>gardener.cloud/operation=migrate</code></li><li>Scale Down the Shoot&rsquo;s control plane in the <strong>Source Seed</strong>.</li><li>The gardenlet in the <strong>Destination Seed</strong> fetches the state of extension resources from the <code>ShootState</code> resource in the garden cluster.</li><li>Normal reconciliation flow is resumed in the <strong>Destination Seed</strong>. Extension resources are annotated with <code>gardener.cloud/operation=restore</code> to instruct the extension controllers to reconstruct their state.</li><li>The Shoot&rsquo;s namespace in <strong>Source Seed</strong> is deleted.</li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-537ccca03886037e9fbdfd4d0c15d1a0>8 - 08 Shoot Apiserver Via Sni</h1><h1 id=sni-passthrough-proxy-for-kube-apiservers>SNI Passthrough proxy for kube-apiservers</h1><p>This GEP tackles the problem that today a single <code>LoadBalancer</code> is needed for every single Shoot cluster&rsquo;s control plane.</p><h2 id=background>Background</h2><p>When the control plane of a Shoot cluster is provisioned, a dedicated LoadBalancer is created for it. It keeps the entire flow quite easy - the apiserver Pods are running and they are accessible via that LoadBalancer. It&rsquo;s hostnames / IP addresses are used for DNS records like <code>api.&lt;external-domain></code> and <code>api.&lt;shoot>.&lt;project>.&lt;internal-domain></code>. While this solution is simple it comes with several issues.</p><h2 id=motivation>Motivation</h2><p>There are several problems with the current setup.</p><ul><li>IaaS provider costs. For example <code>ClassicLoadBalancer</code> on AWS costs at minimum 17 USD / month.</li><li>Quotas can limit the amount of LoadBalancers you can get per account / project, limiting the number of clusters you can host under a single account.</li><li>Lack of support for better loadbalancing <a href=https://www.envoyproxy.io/docs/envoy/v1.10.0/intro/arch_overview/load_balancing/load_balancers#supported-load-balancers>algorithms than round-robin</a>.</li><li>Slow cluster provisioning time - depending on the provider a LoadBalancer provisioning could take quite a while.</li><li>Lower downtime when workload is shuffled in the clusters as the LoadBalancer is Kubernetes-aware.</li></ul><h2 id=goals>Goals</h2><ul><li>Only one LoadBalancer is used for all Shoot cluster API servers running in a Seed cluster.</li><li>Out-of-cluster (end-user / robot) communication to the API server is still possible.</li><li>In-cluster communication via the kubernetes master service (IPv4/v6 ClusterIP and the <code>kubernetes.default.svc.cluster.local</code>) is possible.</li><li>Client TLS authentication works without intermediate TLS termination (TLS is terminated by <code>kube-apiserver</code>).</li><li>Solution should be cloud-agnostic.</li></ul><h2 id=proposal>Proposal</h2><h3 id=seed-cluster>Seed cluster</h3><p>To solve the problem of having multiple <code>kube-apiservers</code> behind a single LoadBalancer, an intermediate proxy must be placed between the Cloud-Provider&rsquo;s LoadBalancer and <code>kube-apiservers</code>. This proxy is going to choose the Shoot API Server with the help of Server Name Indication. From <a href=https://en.wikipedia.org/wiki/Server_Name_Indication>wikipedia</a>:</p><blockquote><p>Server Name Indication (SNI) is an extension to the Transport Layer Security (TLS) computer networking protocol by which a client indicates which hostname it is attempting to connect to at the start of the handshaking process. This allows a server to present multiple certificates on the same IP address and TCP port number and hence allows multiple secure (HTTPS) websites (or any other service over TLS) to be served by the same IP address without requiring all those sites to use the same certificate. It is the conceptual equivalent to HTTP/1.1 name-based virtual hosting, but for HTTPS.</p></blockquote><p>A rough diagram of the flow of data:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>+-------------------------------+
|                               |
|           Network LB          | (accessible from clients)
|                               |
|                               |
+-------------+-------+---------+                       +------------------+
              |       |                                 |                  |
              |       |            proxy + lb           | Shoot API Server |
              |       |    +-------------+-------------&gt;+                  |
              |       |    |                            | Cluster A        |
              |       |    |                            |                  |
              |       |    |                            +------------------+
              |       |    |
     +----------------v----+--+
     |        |               |
   +-+--------v----------+    |                         +------------------+
   |                     |    |                         |                  |
   |                     |    |       proxy + lb        | Shoot API Server |
   |        Proxy        |    +-------------+----------&gt;+                  |
   |                     |    |                         | Cluster B        |
   |                     |    |                         |                  |
   |                     +----+                         +------------------+
   +----------------+----+
                    |
                    |
                    |                                   +------------------+
                    |                                   |                  |
                    |             proxy + lb            | Shoot API Server |
                    +-------------------+--------------&gt;+                  |
                                                        | Cluster C        |
                                                        |                  |
                                                        +------------------+
</code></pre></div><p>Sequentially:</p><ol><li>client requests <code>Shoot Cluster A</code> and sets the <code>Server Name</code> in the TLS handshake to <code>api.shoot-a.foo.bar</code>.</li><li>this packet goes through the Network LB and it&rsquo;s forwarded to the Proxy server. (this loadbalancer should be a simple Layer-4 TCP proxy)</li><li>the proxy server reads the packet and see that client requests <code>api.shoot-a.foo.bar</code>.</li><li>based on its configuration, it maps <code>api.shoot-a.foo.bar</code> to <code>Shoot API Server Cluster A</code>.</li><li>it acts as TCP proxy and simply send the data <code>Shoot API Server Cluster A</code>.</li></ol><p>There are multiple OSS proxies for this case:</p><ul><li>nginx</li><li>HAProxy</li><li>Envoy</li><li>traefik</li><li><a href=https://github.com/linkerd/linkerd2-proxy>linkerd2-proxy</a></li></ul><p>To ease integration it should:</p><ul><li>be configurable via Kubernetes resources</li><li>not require restarting when configuration changes</li><li>be fast and with little overhead</li></ul><p>All things considered, <a href=http://envoyproxy.io/>Envoy proxy</a> is the most fitting solution as it provides all the features Gardener would like (no process reload being the most important one + battle tested in production by various companies).</p><p>While building a custom control plane for Envoy is <a href=https://github.com/envoyproxy/go-control-plane>quite simple</a>, an already established solution might be the better path forward. <a href=https://istio.io/docs/concepts/traffic-management/#pilot-and-envoy>Istio&rsquo;s Pilot</a> is one of the most feature-complete Envoy control plane solutions as it offers a way to configure edge ingress traffic for Envoy via <a href=https://istio.io/docs/reference/config/networking/v1alpha3/gateway/>Gateway</a> and <a href=https://istio.io/docs/reference/config/networking/v1alpha3/virtual-service/>VirtualService</a>.</p><p>The resources which needs to be created per Shoot clusters are the following:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: kube-apiserver-gateway
  namespace: &lt;shoot-namespace&gt;
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 443
      name: tls
      protocol: TLS
    tls:
      mode: PASSTHROUGH
    hosts:
    - api.&lt;external-domain&gt;
    - api.&lt;shoot&gt;.&lt;project&gt;.&lt;internal-domain&gt;
</code></pre></div><p>and correct <code>VirtualService</code> pointing to the correct API server:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: kube-apiserver
  namespace: &lt;shoot-namespace&gt;
spec:
  hosts:
  - api.&lt;external-domain&gt;
  - api.&lt;shoot&gt;.&lt;project&gt;.&lt;internal-domain&gt;
  gateways:
  - kube-apiserver-gateway
  tls:
  - match:
    - port: 443
      sniHosts:
      - api.&lt;external-domain&gt;
      - api.&lt;shoot&gt;.&lt;project&gt;.&lt;internal-domain&gt;
    route:
    - destination:
        host: kube-apiserver.&lt;shoot-namespace&gt;.svc.cluster.local
        port:
          number: 443
</code></pre></div><p>The resources above configures Envoy to forward the raw TLS data (without termination) to the Shoot <code>kube-apiserver</code>.</p><p>Updated diagram:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>+-------------------------------+
|                               |
|           Network LB          | (accessible from clients)
|                               |
|                               |
+-------------+-------+---------+                       +------------------+
              |       |                                 |                  |
              |       |            proxy + lb           | Shoot API Server |
              |       |    +-------------+-------------&gt;+                  |
              |       |    |                            | Cluster A        |
              |       |    |                            |                  |
              |       |    |                            +------------------+
              |       |    |
     +----------------v----+--+
     |        |               |
   +-+--------v----------+    |                         +------------------+
   |                     |    |                         |                  |
   |                     |    |       proxy + lb        | Shoot API Server |
   |    Envoy Proxy      |    +-------------+----------&gt;+                  |
   | (ingress Gateway)   |    |                         | Cluster B        |
   |                     |    |                         |                  |
   |                     +----+                         +------------------+
   +-----+----------+----+
         |          |
         |          |
         |          |                                   +------------------+
         |          |                                   |                  |
         |          |             proxy + lb            | Shoot API Server |
         |          +-------------------+--------------&gt;+                  |
         |   get                                        | Cluster C        |
         | configuration                                |                  |
         |                                              +------------------+
         |
         v                                                  Configure
      +--+--------------+         +---------------------+   via Istio
      |                 |         |                     |   Custom Resources
      |     Pilot       +--------&gt;+   Seed API Server   +&lt;------------------+
      |                 |         |                     |
      |                 |         |                     |
      +-----------------+         +---------------------+
</code></pre></div><p>In this case the <code>internal</code> and <code>external</code> <code>DNSEntries</code> should be changed to the Network LoadBalancer&rsquo;s IP.</p><h3 id=in-cluster-communication-to-the-apiserver>In-cluster communication to the apiserver</h3><p>In Kubernetes the API server is discoverable via the master service (<code>kubernetes</code> in <code>default</code> namespace). Today, this service can only be of type <code>ClusterIP</code> - making in-cluster communication to the API server impossible due to:</p><ul><li>the client doesn&rsquo;t set the <code>Server Name</code> in the TLS handshake, if it attempts to talk to an IP address. In this case, the TLS handshake reaches the Envoy IngressGateway proxy, but it&rsquo;s rejected by it.</li><li>Kubernetes services can be of type <code>ExternalName</code>, but the master service is not supported by <a href=https://github.com/gardener/gardener/issues/1135#issuecomment-505317932>kubelet</a>.<ul><li>even if this is fixed in future Kubernetes versions, this problem still exists for older versions where this functionality is not available.</li></ul></li></ul><p>Another issue occurs when the client tries to talk to the apiserver via the in-cluster DNS. For all Shoot API servers <code>kubernetes.default.svc.cluster.local</code> is the same and when a client tries to connect to that API server using that server name. This makes distinction between different in-cluster Shoot clients impossible by the Envoy IngressGateway.</p><p>To mitigate this problem an additional proxy must be deployed on every single Node. It does not terminate TLS and sends the traffic to the correct Shoot API Server. This is achieved by:</p><ul><li>the apiserver master service reconciler is started and pointing to the <code>kube-apiserver</code>&rsquo;s Cluster IP in the Seed cluster (e.g. <code>--advertise-address=10.1.2.3</code>).</li><li>the proxy runs in the host network of the <code>Node</code>.</li><li>the proxy has a sidecar container which:<ul><li>creates a dummy network interface and assigns the <code>10.1.2.3</code> to it.</li><li>removes connection tracking (conntrack) if iptables/nftables is enabled as the IP address is local to the <code>Node</code>.</li></ul></li><li>the proxy listens on the <code>10.1.2.3</code> and using the <a href=http://www.haproxy.org/download/2.0/doc/proxy-protocol.txt>PROXY protocol</a> it sends the data stream to the Envoy ingress gateway (EIGW).</li><li>EIGW listens for PROXY protocol on a dedicated <code>8443</code> port. EIGW reads the destination IP + port from the PROXY protocol and forwards traffic to the correct upstream apiserver.</li></ul><p>The sidecar is a standalone component. It&rsquo;s possible to transparently change the proxy implementation without any modifications to the sidecar. The simplified flow looks like:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>+------------------+                    +----------------+
| Shoot API Server |       TCP          |   Envoy IGW    |
|                  +&lt;-------------------+ PROXY listener |
| Cluster A        |                    |     :8443      |
+------------------+                    +-+--------------+
                                          ^
                                          |
                                          |
                                          |
                                          |
+-----------------------------------------------------------+
                                          |   Single Node in
                                          |   the Shoot cluster
                                          |
                                          | PROXY Protocol
                                          |
                                          |
                                          |
 +---------------------+       +----------+----------+
 |  Pod talking to     |       |                     |
 |  the kubernetes     |       |       Proxy         |
 |  service            +------&gt;+  No TLS termination |
 |                     |       |                     |
 +---------------------+       +---------------------+
</code></pre></div><p>Multiple OSS solutions can be used:</p><ul><li>haproxy</li><li>nginx</li></ul><p>To add a PROXY lister with Istio several resources must be created - a dedicated <code>Gateway</code>, dummy <code>VirtualService</code> and <code>EnvoyFilter</code> which adds listener filter (<code>envoy.listener.proxy_protocol</code>) on <code>8443</code> port:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: blackhole
  namespace: istio-system
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 8443
      name: tcp
      protocol: TCP
    hosts:
    - <span style=color:#a31515>&#34;*&#34;</span>

---

apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: blackhole
  namespace: istio-system
spec:
  hosts:
  - blackhole.local
  gateways:
  - blackhole
  tcp:
  - match:
    - port: 8443
    route:
    - destination:
        host: localhost
        port:
          number: 9999 <span style=color:green># any dummy port will work</span>

---

apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: proxy-protocol
  namespace: istio-system
spec:
  workloadSelector:
    labels:
      istio: ingressgateway
  configPatches:
  - applyTo: LISTENER
    match:
      context: ANY
      listener:
        portNumber: 8443
        name: 0.0.0.0_8443
    patch:
      operation: MERGE
      value:
        listener_filters:
        - name: envoy.filters.listener.proxy_protocol
</code></pre></div><p>For each individual <code>Shoot</code> cluster, a dedicated <a href=https://www.envoyproxy.io/docs/envoy/v1.13.0/api-v2/api/v2/listener/listener_components.proto#listener-filterchainmatch>FilterChainMatch</a> is added. It ensures that only Shoot API servers can receive traffic from this listener:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: &lt;shoot-namespace&gt;
  namespace: istio-system
spec:
  workloadSelector:
    labels:
      istio: ingressgateway
  configPatches:
  - applyTo: FILTER_CHAIN
    match:
      context: ANY
      listener:
        portNumber: 8443
        name: 0.0.0.0_8443
    patch:
      operation: ADD
      value:
        filters:
        - name: envoy.filters.network.tcp_proxy
          typed_config:
            &#34;@type&#34;: type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy
            stat_prefix: outbound|443||kube-apiserver.&lt;shoot-namespace&gt;.svc.cluster.local
            cluster: outbound|443||kube-apiserver.&lt;shoot-namespace&gt;.svc.cluster.local
        filter_chain_match:
          destination_port: 443
          prefix_ranges:
          - address_prefix: 10.1.2.3 <span style=color:green># kube-apiserver&#39;s cluster-ip</span>
            prefix_len: 32
</code></pre></div><blockquote><p>Note: this additional <code>EnvoyFilter</code> can be removed when Istio supports full <a href=https://istio.io/docs/reference/config/networking/virtual-service/#L4MatchAttributes>L4 matching</a>.</p></blockquote><p>A nginx proxy client in the Shoot cluster on every node could have the following configuration:</p><pre><code class=language-conf data-lang=conf>error_log /dev/stdout;
stream {
    server {
        listen 10.1.2.3:443;
        proxy_pass api.&lt;external-domain&gt;:8443;
        proxy_protocol on;

        proxy_protocol_timeout 5s;
        resolver_timeout 5s;
        proxy_connect_timeout 5s;
    }
}

events { }
</code></pre><h3 id=in-cluster-communication-to-the-apiserver-when-exernalname-is-supported>In-cluster communication to the apiserver when ExernalName is supported</h3><p>Even if in future versions of Kubernetes, the master service of type <code>ExternalName</code> is supported, we still have the problem that in-cluster workload can talk to the server via DNS. For this to work we still need the above mentioned proxy (this time listening on another IP address <code>10.0.0.2</code>). An additional change to CoreDNS would be needed:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>default.svc.cluster.local.:8053 {
    file kubernetes.default.svc.cluster.local
}

.:8053 {
    errors
    health
    kubernetes cluster.local in-addr.arpa ip6.arpa {
        pods insecure
        upstream
        fallthrough in-addr.arpa ip6.arpa
    }
    prometheus :9153
    forward . /etc/resolv.conf
    cache 30
    loop
    reload
    loadbalance
}
</code></pre></div><p>The content of the <code>kubernetes.default.svc.cluster.local</code> is going to be:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>$ORIGIN default.svc.cluster.local.

@	30 IN	SOA local. local. (
        2017042745 ; serial
        1209600    ; refresh (2 hours)
        1209600    ; retry (1 hour)
        1209600    ; expire (2 weeks)
        30         ; minimum (1 hour)
        )

  30 IN NS local.

kubernetes     IN A     10.0.0.2
</code></pre></div><p>So when a client requests <code>kubernetes.default.svc.cluster.local</code>, it&rsquo;ll be send to the proxy listening on that IP address.</p><h2 id=future-work>Future work</h2><p>While out of scope of this GEP, several things can be improved:</p><ul><li>Make the sidecar work with eBPF and environments where iptables/nftables are not enabled.</li></ul><h2 id=references>References</h2><ul><li><a href=https://github.com/gardener/gardener/issues/1135>https://github.com/gardener/gardener/issues/1135</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-7581ec6f70a5cf65db6f716b423048e5>9 - 09 Test Framework</h1><h1 id=gardener-integration-test-framework>Gardener integration test framework</h1><h2 id=motivation>Motivation</h2><p>As we want to improve our code coverage in the next months we will need a simple and easy to use test framework.
The current testframework already contains a lot of general test functions that ease the work for writing new tests.
However there are multiple disadvantages with the current structure of the tests and the testframework:</p><ol><li>Every new test is an own testsuite and therefore needs its own <code>TestDef</code> (<a href=https://github.com/gardener/gardener/tree/master/.test-defs>https://github.com/gardener/gardener/tree/master/.test-defs</a>). With this approach there will be hundreds of test definitions, growing with every new test (or at least new test suite).
But in most cases new tests do not need their own special <code>TestDef</code>: it&rsquo;s just the wrong scope for the testmachinery and will result in unnecessary complex testruns and configurations. In addition it would result in additional maintenance for a huge number of <code>TestDefs</code>.</li><li>The testsuites currently have their own specific interface/configuration that they need in order to be executed correctly (see <a href=https://github.com/gardener/gardener/blob/master/.test-defs/ShootKubernetesUpdateTest.yaml#L14>K8s Update test</a>).
Consequently the configuration has to be defined in the testruns which result in one step per test with their very own configuration which means that the testmachinery cannot simply select testdefinitions by label.
As the testmachinery cannot make use of its ability to run labeled tests (e.g. run all tests labeled <code>default</code>), the testflow size increases with every new tests and the testruns have to be manually adjusted with every new test.</li><li>The current gardener test framework contains multiple test operations where some are just used for specific tests (e.g. <code>plant_operations</code>) and some are more general (<code>garden_operation</code>). Also the functions offered by the operations vary in their specialization as some are really specific to just one test e.g. shoot test operation with <code>WaitUntilGuestbookAppIsAvailable</code> whereas others are more general like <code>WaitUntilPodIsRunning</code>.<br>This structure makes it hard for developers to find commonly used functions and also hard to integrate as the common framework grows with specialized functions.</li></ol><h2 id=goals>Goals</h2><p>In order to clean the testframework, make it easier for new developers to write tests and easier to add and maintain test execution within the testmachinery, the following goals are defined:</p><ul><li>Have a small number of test suites (gardener, shoots see <a href=#test_flavors>test flavors</a>) to only maintain a fixed number of testdefinitions.</li><li>Use ginkgo test labels (inspired by the k8s e2e tests) to differentiate test behavior, test execution and test importance.</li><li>Use standardized configuration for all tests (differ depending on the test suite) but provide better tooling to dynamically read additional configuration from configuration files like the <code>cloudprofile</code>.</li><li>Clean the testframework to only contain general functionality and keep specific functions inside the tests</li></ul><h2 id=proposal>Proposal</h2><p>The proposed new test framework consists of the following changes to tackle the above described goals.
​</p><h4 id=test-flavors>Test Flavors</h4><p>Reducing the number of test definitions is done by ​combining the current specified test suites into the following 3 general ones:</p><ul><li><em>System test suite</em><ul><li>e.g. create-shoot, delete-shoot, hibernate</li><li>need their own testdef because they have a special meaning in the context of the testmachinery</li></ul></li><li><em>Gardener test suite</em><ul><li>e.g. RBAC, scheduler</li><li>All tests that only need a gardener installation but no shoot cluster</li><li>Possible functions/environment:<ul><li>New project for test suite (copy secret binding, cleanup)?</li></ul></li></ul></li><li><em>Shoot test suite</em><ul><li>e.g. shoot app, network</li><li>Test that require a running shoot</li><li>Possible functions:<ul><li>Namespace per test</li><li>cleanup of ns</li></ul></li></ul></li></ul><p>As inspired by the k8s e2e tests, test labels are used to differentiate the tests by their behavior, their execution and their importance.
Test labels means that tests are described using predefined labels in the test&rsquo;s text (e.g <code>ginkgo.It("[BETA] this is a test")</code>).
With this labeling strategy, it is also possible to see the test properties directly in the code and promoting a test can be done via a pullrequest and will then be automatically recognized by the testmachinery with the next release.</p><p>Using ginkgo focus to only run desired tests and combined testsuites, an example test definition will look like the following.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>kind: TestDefinition
metadata:
  name: gardener-beta-suite
spec:
  description: Test suite that runs all gardener tests that are labeled as beta
  activeDeadlineSeconds: 7200
  labels: [<span style=color:#a31515>&#34;gardener&#34;</span>, <span style=color:#a31515>&#34;beta&#34;</span>]
​
  command: [bash, -c]
  args:
  - &gt;-<span style=color:#a31515>
</span><span style=color:#a31515>    go test -timeout=0 -mod=vendor ./test/integration/suite
</span><span style=color:#a31515>    --v -ginkgo.v -ginkgo.progress -ginkgo.no-color
</span><span style=color:#a31515>    -ginkgo.focus=&#34;[GARDENER] [BETA]&#34;</span>    
</code></pre></div><p>Using this approach, the overall number of testsuites is then reduced to a fixed number (excluding the system steps) of <code>test suites * labelCombinations</code>.</p><h4 id=framework>Framework</h4><p>The new framework will consist of a common framework, a gardener framework (integrating the commom framework) and a shoot framework (integrating the gardener framework).</p><p>All of these frameworks will have their own configuration that is exposed via commandline flags so that for example the shoot test framework can be executed by <code>go test -timeout=0 -mod=vendor ./test/integration/suite --v -ginkgo.v -ginkgo.focus="[SHOOT]" --kubecfg=/path/to/config --shoot-name=xx</code>.</p><p>The available test labels should be declared in the code with predefined values and in a predefined order so that everyone is aware about possible labels and the tests are labeled similarly across all integration tests. This approach is somehow similar to what kubernetes is doing in their e2e test suite but with some more restrictions (compare <a href=https://github.com/kubernetes/kubernetes/blob/master/test/e2e/apps/deployment.go#L84>example k8s e2e test</a>).<br>A possible solution to have consistent labeling would be to define them with every new <code>ginkgo.It</code> definition: <code>f.Beta().Flaky().It("my test")</code> which internally orders them and would produce a ginkgo test with the text : <code>[BETA] [FLAKY] my test</code>.</p><p><strong>General Functions</strong>
The test framework should include some general functions that can and will be reused by every test.
These general functions may include:
​</p><ul><li>Logging</li><li>State Dump</li><li>Detailed test output (status, duration, etc..)</li><li>Cleanup handling per test (<code>It</code>)</li><li>General easy to use functions like <code>WaitUntilDeploymentCompleted</code>, <code>GetLogs</code>, <code>ExecCommand</code>, <code>AvailableCloudprofiles</code>, etc..
​</li></ul><h4 id=example>Example</h4><p>A possible test with the new test framework would look like:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#00f>var</span> _ = ginkgo.Describe(<span style=color:#a31515>&#34;Shoot network testing&#34;</span>, <span style=color:#00f>func</span>() {
  <span style=color:green>// the testframework registers some cleanup handling for a state dump on failure and maybe cleanup of created namespaces
</span><span style=color:green></span>  f := framework.NewShootFramework()
  f.CAfterEach(<span style=color:#00f>func</span>(ctx context.Context) {
    ginkgo.By(<span style=color:#a31515>&#34;cleanup network test daemonset&#34;</span>)
    err := f.ShootClient.Client().Delete(ctx, &amp;appsv1.DaemonSet{ObjectMeta: metav1.ObjectMeta{Name: name, Namespace: namespace}})
    <span style=color:#00f>if</span> err != <span style=color:#00f>nil</span> {
      <span style=color:#00f>if</span> !apierrors.IsNotFound(err) {
        Expect(err).To(HaveOccurred())
      }
    }
  }, FinalizationTimeout)
  f.Release().Default().CIt(<span style=color:#a31515>&#34;should reach all webservers on all nodes&#34;</span>, <span style=color:#00f>func</span>(ctx context.Context) {
    ginkgo.By(<span style=color:#a31515>&#34;Deploy the net test daemon set&#34;</span>)
    templateFilepath := filepath.Join(f.ResourcesDir, <span style=color:#a31515>&#34;templates&#34;</span>, nginxTemplateName)
    err := f.RenderAndDeployTemplate(f.Namespace(), tempalteFilepath)
    Expect(err).ToNot(HaveOccurred())
    err = f.WaitUntilDaemonSetIsRunning(ctx, f.ShootClient.Client(), name, namespace)
    Expect(err).NotTo(HaveOccurred())
    pods := &amp;corev1.PodList{}
    err = f.ShootClient.Client().List(ctx, pods, client.MatchingLabels{<span style=color:#a31515>&#34;app&#34;</span>: <span style=color:#a31515>&#34;net-nginx&#34;</span>})
    Expect(err).NotTo(HaveOccurred())
    <span style=color:green>// check if all webservers can be reached from all nodes
</span><span style=color:green></span>    ginkgo.By(<span style=color:#a31515>&#34;test connectivity to webservers&#34;</span>)
    shootRESTConfig := f.ShootClient.RESTConfig()
    <span style=color:#00f>var</span> res <span style=color:#2b91af>error</span>
    <span style=color:#00f>for</span> _, from := <span style=color:#00f>range</span> pods.Items {
      <span style=color:#00f>for</span> _, to := <span style=color:#00f>range</span> pods.Items {
        <span style=color:green>// test pods
</span><span style=color:green></span>        f.Logger.Infof(<span style=color:#a31515>&#34;%s to %s: %s&#34;</span>, from.GetName(), to.GetName(), data)
      }
    }
    Expect(res).ToNot(HaveOccurred())
  }, NetworkTestTimeout)
})
</code></pre></div><h2 id=future-plans>Future Plans</h2><h4 id=ownership>Ownership</h4><p>When the test coverage is increased and there will be more tests, we will need to track ownership for tests.
At the beginning the ownership will be shared across all maintainers of the residing repository but this is not suitable anymore as tests will grow and get more complex.</p><p>Therefore the test ownership should be tracked via subgroups (in kubernetes this would be a SIG (comp. <a href=https://github.com/kubernetes/kubernetes/blob/master/test/e2e/apps/framework.go#L22>sig apps e2e test</a>)). These subgroup will then be tracked via labels and the members of these groups will then be notified if tests fail.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3769be973f9bb9047a57a60178b9367c>10 - 10 Shoot Additional Container Runtimes</h1><h1 id=gardener-extensibility-to-support-shoot-additional-container-runtimes>Gardener extensibility to support shoot additional container runtimes</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#summary>Summary</a></li><li><a href=#motivation>Motivation</a><ul><li><a href=#goals>Goals</a></li><li><a href=#non-goals>Non-Goals</a></li></ul></li><li><a href=#proposal>Proposal</a></li><li><a href=#design-details>Design Details</a></li><li><a href=#alternatives>Alternatives</a></li></ul><h2 id=summary>Summary</h2><p>Gardener-managed Kubernetes clusters are sometimes used to run sensitive workloads, which sometimes are comprised of OCI images originating from untrusted sources. Additional use-cases want to leverage economy-of-scale to run workloads for multiple tenants on the same cluster. In some cases, Gardener users want to use operating systems which do not easily support the Docker engine.</p><p>This proposal aims to allow Gardener Shoot clusters to use CRI instead of the legacy Docker API, and to provide extension type for adding CRI shims (like <a href=https://gvisor.dev/>GVisor</a> and <a href=https://katacontainers.io/>Kata Containers</a>) which can be used to add support in Gardener Shoot clusters for these runtimes.</p><h2 id=motivation>Motivation</h2><p>While pods and containers are intended to create isolated areas for concurrently running workloads on nodes, this isolation is not as robust as could be expected. Containers leverage the core Linux CGroup and Namespace features to isolate workloads, and many kernel vulnerabilities have the potential to allow processes to escape from their isolation. Once a process has escaped from its container, any other process running on the same node is compromised. Several projects try to mitigate this problem; for example Kata Containers allow isolating a Kubernetes Pod in a micro-vm, gVisor reduces the kernel attack surface by adding another level of indirection between the actual payload and the real kernel.</p><p>Kubernetes supports running pods using these alternate runtimes via the <a href=https://kubernetes.io/docs/concepts/containers/runtime-class/>RuntimeClass</a> concept, which was promoted to Beta in Kubernetes 1.14. Once Kubernetes is configured to use the Container Runtime Interface to control pods, it becomes possible to leverage CRI and run specific pods using different Runtime Classes. Additionally, configuring Kubernetes to use CRI instead of the legacy Dockershim is <a href=https://events19.linuxfoundation.org/wp-content/uploads/2017/11/How-Container-Runtime-Matters-in-Kubernetes_-OSS-Kunal-Kushwaha.pdf>faster</a>.</p><p>The motivation behind this proposal is to make all of this functionality accessible to Shoot clusters managed by Gardener.</p><h3 id=goals>Goals</h3><ul><li>Gardener must allow to configue its managed clusters with the CRI interface instead of the legacy Dockershim.</li><li>Low-level runtimes like gVisor or Kata Containers are provided as gardener extensions which are (optionally) installed into a landscape by the Gardener operator. There must be no runtime-specific knowledge in the core Gardener code.</li><li>It shall be possible to configure multiple low-level runtimes in Shoot clusters, on the Worker Group level.</li></ul><h2 id=proposal>Proposal</h2><p>Gardener today assumes that all supported operating systems have Docker pre-installed in the base image. Starting with Docker Engine 1.11, Docker itself was <a href=https://www.docker.com/blog/docker-engine-1-11-runc/>refactored</a> and cleaned-up to be based on the <a href=https://containerd.io/>containerd</a> library. The first phase would be to allow the change of the Kubelet configuration as described <a href=https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd>here</a> so that Kubernetes would use containerd instead of the default Dockershim. This will be implemented for CoreOS, Ubuntu, and SuSE-CHost.</p><p>We will implement two Gardener extensions, providing gVisor and Kata Containers as options for Gardener landscapes.
The <code>WorkerGroup</code> specification will be extended to allow specifying the CRI name and a list of additional required Runtimes for nodes in that group. For example:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>workers:
- name: worker-b8jg5
  machineType: m5.large
  volumeType: gp2
  volumeSize: 50Gi
  autoScalerMin: 1
  autoScalerMax: 2
  maxSurge: 1
  cri:
    name: containerd
    containerRuntimes:
    - type: gvisor
    - type: kata-containers
  machineImage:
    name: coreos
    version: 2135.6.0
</code></pre></div><p>Each extension will need to address the following concern:</p><ol><li>Add the low-level runtime binaries to the worker nodes. Each extension should get the runtime binaries from a container.</li><li>Hook the runtime binary into the containerd configuration file, so that the runtime becomes available to containerd.</li><li>Apply a label to each node that allows identifying nodes where the runtime is available.</li><li>Apply the relevant <code>RuntimeClass</code> to the Shoot cluster, to expose the functionality to users.</li><li>Provide a separate binary with a <code>ValidatingWebhook</code> (deployable to the garden cluster) to catch invalid configurations. For example, Kata Containers on AWS requires a <code>machineType</code> of <code>i3.metal</code>, so any <code>Shoot</code> requests with a Kata Containers runtime and a different machine type on AWS should be rejected.</li></ol><h2 id=design-details>Design Details</h2><ol><li><p>Change the nodes container runtime to work with CRI and ContainerD (Only if specified in the Shoot spec):</p><ol><li><p>In order to configure each worker machine in the cluster to work with CRI, the following configurations should be done:</p><ol><li>Add kubelet execution flags:<ol><li>&ndash;container-runtime=remote</li><li>&ndash;container-runtime-endpoint=unix:///run/containerd/containerd.sock</li></ol></li><li>Make sure that default containerd configuration file exist in path /etc/containerd/config.toml.</li></ol></li><li><p>ContainerD and Docker configurations are different for each OS. To make sure the default configurations above works well in each worker machine, each OS extension would be responsible to configure them during the reconciliation of the
OperatingSystemConfig:</p><ol><li>os-ubuntu -<ol><li>Create ContainerD unit Drop-In to execute ContainerD with the default configurations file in path /etc/containerd/config.toml.</li><li>Create the container runtime metadata file with a OS path for binaries installations: /usr/bin.</li></ol></li><li>os-coreos -<ol><li>Create ContainerD unit Drop-In to execute ContainerD with the default configurations file in path /etc/containerd/config.toml.</li><li>Create Docker Drop-In unit to execute Docker with the correct socket path of ContainerD.</li><li>Create the container runtime metadata file with a OS path for binaries installations: /var/bin.</li></ol></li><li>os-suse-chost -<ol><li>Create ContainerD service unit and execute ContainerD with the default configurations file in path /etc/containerd/config.toml.</li><li>Download and install ctr-cli which is not shipped with the current SuSe image.</li><li>Create the container runtime metadata file with a OS path for binaries installations /usr/sbin.</li></ol></li></ol></li><li><p>To rotate the ContainerD (CRI) logs we will activate the kubelet feature flag: CRIContainerLogRotation=true.</p></li><li><p>Docker monitor service will be replaced with equivalent ContainerD monitor service.</p></li></ol></li><li><p>Validate workers additional runtime configurations:</p><ol><li>Disallow additional runtimes with shoots &lt; 1.14</li><li>kata-container validation: Machine type support nested virtualization.</li></ol></li><li><p>Add support for each additional container runtime in the cluster.</p><ol><li><p>In order to install each additional available runtime in the cluster we should:</p><ol><li>Install the runtime binaries in each Worker&rsquo;s pool nodes that specified the runtime support.</li><li>Apply the relevant RuntimeClass to the cluster.</li></ol></li><li><p>The installation above should be done by a new kind of extension: ContainerRuntime resource. For each container runtime type (Kata-container/gvisor) a dedicate extension controller will be created.</p><ol><li><p>A label for each container runtime support will be added to every node that belongs to the worker pool. This should be done similar
to the way labels created today for each node, through kubelet execution parameters (_kubelet.flags: &ndash;node-labels). When creating the OperatingSystemConfig (original) for the worker each container runtime support should be mapped to a label on the node.
For Example:
label: container.runtime.kata-containers=true (shoot.spec.cloud.<iaas>.worker.containerRuntimes.kata-container)
label: container.runtime.gvisor=true (shoot.spec.cloud.<iaas>.worker.containerRuntimes.gvisor)</p></li><li><p>During the Shoot reconciliation (Similar steps to the Extensions today) Gardener will create new ContainerRuntime resource if a container runtime exist in at least one worker spec:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: extensions.gardener.cloud/v1alpha1
kind: ContainerRuntime
metadata:
  name: kata-containers-runtime-extention
  namespace: shoot--foo--bar
spec:
  type: kata-containers
</code></pre></div><p>Gardener will wait that all ContainerRuntimes extensions will be reconciled by the appropriate extensions controllers.</p></li><li><p>Each runtime extension controller will be responsible to reconcile it&rsquo;s RuntimeContainer resource type.
rc-kata-containers extension controller will reconcile RuntimeContainer resource from type kata-container and rc-gvisor will reconcile RuntimeContainer resource from gvisor.
Reconciliation process by container runtime extension controllers:</p><ol><li>Runtime extension controller from specific type should apply a chart which responsible for the installation of the runtime container in the cluster:<ol><li>DaemonSet which will run a privileged pod on each node with the label: container.runtime.<type of the resource>:true The pod will be responsible for:<ol><li>Copy the runtime container binaries (From extension package ) to the relevant path in the host OS.</li><li>Add the relevant container runtime plugin section to the containerd configuration file (/etc/containerd/config.toml).</li><li>Restart containerd in the node.</li></ol></li><li>RuntimeClasses in the cluster to support the runtime class. for example:<div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: node.k8s.io/v1beta1
kind: RuntimeClass
metadata:
  name: gvisor
handler: runsc
</code></pre></div></li></ol></li><li>Update the status of the relevant RuntimeContainer resource to succeeded.</li></ol></li></ol></li></ol></li></ol><p>&ndash;></p><h2 id=alternatives>Alternatives</h2></div><div class=td-content style=page-break-before:always><h1 id=pg-f79a3d9aaa070750a3943e02881b340a>11 - 12 Oidc Webhook Authenticator</h1><h1 id=oidc-webhook-authenticator>OIDC Webhook Authenticator</h1><h2 id=problem>Problem</h2><p>In Kubernetes you can authenticate via several authentication strategies:</p><ul><li>x509 Client Certificates</li><li>Static Token Files</li><li>Bootstrap Tokens</li><li>Static Password File (Basic authentication - deprecated and removed in 1.19)</li><li>Service Account Tokens</li><li>OpenID Connect TOkens</li><li>Webhook Token Authentication</li><li>Authenticating Proxy</li></ul><p>End-users should use <a href=https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens>OpenID Connect (OIDC) Tokens</a> created by OIDC-compatible Identity Provider (IDP) and present <a href=https://openid.net/specs/openid-connect-core-1_0.html#IDToken>id_token</a> to the Kube APIServer. If the API server is configured to trust the IDP and the token is valid, then the user is authenticated and the <a href=https://github.com/kubernetes/kubernetes/blob/99019502bd6ed038dbd1c444974d5e8c6a8dda19/staging/src/k8s.io/api/authentication/v1/types.go#L100-L117>UserInfo</a> is send to the authorization stack.</p><p>Ideally, operators of the Gardener cluster should be able to authenticate to end-user Shoot clusters with <code>id_token</code> generated by OIDC IDP, but in many cases, end-users might have already configured OIDC for their cluster and more than one OIDC configurations are not allowed.</p><p>Another interesting application of multiple OIDC providers would be per <code>Project</code> OIDC provider where end-users of Gardener can add their own OIDC-compatible IDPs.</p><p>To workaround the one OIDC per Kube APIServer limitation, a new <code>OIDC Webhook Authenticator</code> (OWA) could be implemented.</p><h2 id=goals>Goals</h2><ul><li>Dynamic registrations of OpenID Connect configurations.</li><li>Close as possible to the Kubernetes build-in OIDC Authenticator.</li><li>Build as an optional extension and not required for functional Shoot or Gardener cluster.</li></ul><h2 id=non-goals>Non-goals</h2><ul><li><a href=https://kubernetes.io/docs/reference/access-authn-authz/webhook/>Dynamic Authorization</a> is out of scope.</li></ul><h2 id=proposal>Proposal</h2><p>The Kube APIServer can use <a href=https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication>Webhook Token Authentication</a> to send a <a href=https://tools.ietf.org/html/rfc6750#section-2.1>Bearer Tokens (id_token)</a> to external webhook for validation:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  &#34;apiVersion&#34;: <span style=color:#a31515>&#34;authentication.k8s.io/v1beta1&#34;</span>,
  &#34;kind&#34;: <span style=color:#a31515>&#34;TokenReview&#34;</span>,
  &#34;spec&#34;: {
    &#34;token&#34;: <span style=color:#a31515>&#34;(BEARERTOKEN)&#34;</span>
  }
}
</code></pre></div><p>Where upon verification, the remote webhook returns the identity of the user (if authentication succeeds):</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  &#34;apiVersion&#34;: <span style=color:#a31515>&#34;authentication.k8s.io/v1beta1&#34;</span>,
  &#34;kind&#34;: <span style=color:#a31515>&#34;TokenReview&#34;</span>,
  &#34;status&#34;: {
    &#34;authenticated&#34;: <span style=color:#00f>true</span>,
    &#34;user&#34;: {
      &#34;username&#34;: <span style=color:#a31515>&#34;janedoe@example.com&#34;</span>,
      &#34;uid&#34;: <span style=color:#a31515>&#34;42&#34;</span>,
      &#34;groups&#34;: [
        <span style=color:#a31515>&#34;developers&#34;</span>,
        <span style=color:#a31515>&#34;qa&#34;</span>
      ],
      &#34;extra&#34;: {
        &#34;extrafield1&#34;: [
          <span style=color:#a31515>&#34;extravalue1&#34;</span>,
          <span style=color:#a31515>&#34;extravalue2&#34;</span>
        ]
      }
    }
  }
}
</code></pre></div><h3 id=registration-of-new-openidconnect>Registration of new OpenIDConnect</h3><p>This new OWA can be configured with multiple OIDC providers and the entire flow can look like this:</p><ol><li><p>Admin adds a new <code>OpenIDConnect</code> resource (via CRD) to the cluster.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: authentication.gardener.cloud/v1alpha1
kind: OpenIDConnect
metadata:
  name: foo
spec:
  issuerURL: https://foo.bar
  clientID: some-client-id
  usernameClaim: email
  usernamePrefix: <span style=color:#a31515>&#34;test-&#34;</span>
  groupsClaim: groups
  groupsPrefix: <span style=color:#a31515>&#34;baz-&#34;</span>
  supportedSigningAlgs:
  - RS256
  requiredClaims:
    baz: bar
  caBundle: LS0tLS1CRUdJTiBDRVJU...base64-encoded CA certs for issuerURL.
</code></pre></div></li><li><ol><li>OWA watches for changes on this resource and does <a href=https://openid.net/specs/openid-connect-discovery-1_0.html>OIDC discovery</a>. The <a href=https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfigurationResponse>OIDC provider&rsquo;s configuration</a> has to be accessible under the <code>spec.issuerURL</code> with a <a href=https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig>well-known path (.well-known/openid-configuration)</a>.</li></ol></li><li><p>OWA uses the <code>jwks_uri</code> obtained from the OIDC providers configuration, to fetch the OIDC provider&rsquo;s public keys from that endpoint and stores them in the status of <code>OpenIDConnect</code>:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: authentication.gardener.cloud/v1alpha1
kind: OpenIDConnect
metadata:
  name: foo
spec:
  issuerURL: https://foo.bar
  ...
status:
  keys: f31deA9b... <span style=color:green>#the content of jwks_uri base64-encoded</span>
</code></pre></div></li><li><p>OWA uses those keys, issuer, client_id and other settings to add OIDC authenticator to a in-memory list of <a href="https://pkg.go.dev/k8s.io/apiserver/pkg/authentication/authenticator?tab=doc#Token">Token Authenticators</a>.</p></li></ol><p><img src=/__resources/registration_4bbe69.svg alt="alt text" title="Authentication with OIDC webhook"></p><h3 id=end-user-authentication-via-new-openidconnect-idp>End-user authentication via new OpenIDConnect IDP</h3><p>When a user presents an <code>id_token</code> obtained from a OpenID Connect the flow looks like this:</p><ol><li><p>The user authenticates in Custom IDP.</p></li><li><p><code>id_token</code> is obtained from Custom IDP.</p></li><li><p>The user uses <code>id_token</code> to perform an API call to Kube APIServer.</p></li><li><p>As the <code>id_token</code> is not matched by any build-in or configured authenticators in the Kube APIServer, it is send to OWA for validation.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  &#34;TokenReview&#34;: {
    &#34;kind&#34;: <span style=color:#a31515>&#34;TokenReview&#34;</span>,
    &#34;apiVersion&#34;: <span style=color:#a31515>&#34;authentication.k8s.io/v1beta1&#34;</span>,
    &#34;spec&#34;: {
      &#34;token&#34;: <span style=color:#a31515>&#34;ddeewfwef...&#34;</span>
    }
  }
}
</code></pre></div></li><li><p>OWA uses <code>TokenReview</code> to authenticate the calling API server (the Kube APIServer for delegation of authentication and authorization is different from the calling API server).</p><blockquote><p>Example: When a Shoot cluster&rsquo;s API Server is configured to verify tokens by OWA, that API server will be the callee API server. The Seed API server will be used for delegating authentication and authorization.</p></blockquote><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  &#34;TokenReview&#34;: {
    &#34;kind&#34;: <span style=color:#a31515>&#34;TokenReview&#34;</span>,
    &#34;apiVersion&#34;: <span style=color:#a31515>&#34;authentication.k8s.io/v1beta1&#34;</span>,
    &#34;spec&#34;: {
      &#34;token&#34;: <span style=color:#a31515>&#34;api-server-token...&#34;</span>
    }
  }
}
</code></pre></div></li><li><p>After the Authentication API server returns the identity of callee API server:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
    &#34;apiVersion&#34;: <span style=color:#a31515>&#34;authentication.k8s.io/v1&#34;</span>,
    &#34;kind&#34;: <span style=color:#a31515>&#34;TokenReview&#34;</span>,
    &#34;metadata&#34;: {
        &#34;creationTimestamp&#34;: <span style=color:#00f>null</span>
    },
    &#34;spec&#34;: {
        &#34;token&#34;: <span style=color:#a31515>&#34;eyJhbGciOiJSUzI1NiIsImtpZCI6InJocEdLTXZlYjV1OE5heD...&#34;</span>
    },
    &#34;status&#34;: {
        &#34;authenticated&#34;: <span style=color:#00f>true</span>,
        &#34;user&#34;: {
            &#34;groups&#34;: [
                <span style=color:#a31515>&#34;system:serviceaccounts&#34;</span>,
                <span style=color:#a31515>&#34;system:serviceaccounts:shoot--abcd&#34;</span>,
                <span style=color:#a31515>&#34;system:authenticated&#34;</span>
            ],
            &#34;uid&#34;: <span style=color:#a31515>&#34;14db103e-88bb-4fb3-8efd-ca9bec91c7bf&#34;</span>,
            &#34;username&#34;: <span style=color:#a31515>&#34;system:serviceaccount:shoot--abcd:kube-apiserver&#34;</span>
        }
    }
}
</code></pre></div><p>OWA makes a <code>SubjectAccessReview</code> call to the Authorization API server to ensure that callee API server is allowed to validate tokens:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  &#34;apiVersion&#34;: <span style=color:#a31515>&#34;authorization.k8s.io/v1&#34;</span>,
  &#34;kind&#34;: <span style=color:#a31515>&#34;SubjectAccessReview&#34;</span>,
  &#34;spec&#34;: {
    &#34;groups&#34;: [
      <span style=color:#a31515>&#34;system:serviceaccounts&#34;</span>,
      <span style=color:#a31515>&#34;system:serviceaccounts:shoot--abcd&#34;</span>,
      <span style=color:#a31515>&#34;system:authenticated&#34;</span>
    ],
    &#34;nonResourceAttributes&#34;: {
      &#34;path&#34;: <span style=color:#a31515>&#34;/validate-token&#34;</span>,
      &#34;verb&#34;: <span style=color:#a31515>&#34;post&#34;</span>
    },
    &#34;user&#34;: <span style=color:#a31515>&#34;system:serviceaccount:shoot--abcd:kube-apiserver&#34;</span>
  },
  &#34;status&#34;: {
    &#34;allowed&#34;: <span style=color:#00f>true</span>,
    &#34;reason&#34;: <span style=color:#a31515>&#34;RBAC: allowed by RoleBinding \&#34;kube-apiserver\&#34; of ClusterRole \&#34;kube-apiserver\&#34; to ServiceAccount \&#34;system:serviceaccount:shoot--abcd:kube-apiserver\&#34;&#34;</span>
  }
}
</code></pre></div></li><li><p>OWA then iterates over all registered <code>OpenIDConnect</code> Token authenticators and tries to validate the token.</p></li><li><p>Upon a successful validation it returns the <code>TokeReview</code> with user, groups and extra parameters:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  &#34;TokenReview&#34;: {
    &#34;kind&#34;: <span style=color:#a31515>&#34;TokenReview&#34;</span>,
    &#34;apiVersion&#34;: <span style=color:#a31515>&#34;authentication.k8s.io/v1beta1&#34;</span>,
    &#34;spec&#34;: {
      &#34;token&#34;: <span style=color:#a31515>&#34;ddeewfwef...&#34;</span>
    },
    &#34;status&#34;: {
      &#34;authenticated&#34;: <span style=color:#00f>true</span>,
      &#34;user&#34;: {
        &#34;username&#34;: <span style=color:#a31515>&#34;test-foo@bar.com&#34;</span>,
        &#34;groups&#34;: [
          <span style=color:#a31515>&#34;baz-employee&#34;</span>
        ],
        &#34;extra&#34;: {
          &#34;gardener.cloud/apiserver/groups&#34;: [
            <span style=color:#a31515>&#34;system:serviceaccounts&#34;</span>,
            <span style=color:#a31515>&#34;system:serviceaccounts:shoot--abcd&#34;</span>,
            <span style=color:#a31515>&#34;system:authenticated&#34;</span>
          ],
          &#34;gardener.cloud/apiserver/uid&#34;: [
            <span style=color:#a31515>&#34;system:serviceaccount:shoot--abcd:kube-apiserver&#34;</span>
          ],
          &#34;gardener.cloud/apiserver/username&#34;: [
            <span style=color:#a31515>&#34;system:serviceaccount:shoot--abcd:kube-apiserver&#34;</span>
          ],
          &#34;gardener.cloud/oidc/name&#34;: [
            <span style=color:#a31515>&#34;foo&#34;</span>
          ],
          &#34;gardener.cloud/oidc/uid&#34;: [
            <span style=color:#a31515>&#34;e5062528-e5a4-4b97-ad83-614d015b0979&#34;</span>
          ],
          &#34;gardener.cloud/oidc/resourceVersion&#34;: [
            <span style=color:#a31515>&#34;3355876311&#34;</span>
          ]
        }
      }
    }
  }
}
</code></pre></div><p>It also adds some extra information which can be used by custom authorizers later on:</p><ol><li><code>gardener.cloud/apiserver/groups</code> contains all the groups of the API server which is making the <code>TokenReview</code> request (it&rsquo;s the ServiceAccount of the API Server Pod in this case)</li><li><code>gardener.cloud/apiserver/uid</code> contains the UID of the API server which is making the <code>TokenReview</code> request (it&rsquo;s the ServiceAccount of the API Server Pod in this case)</li><li><code>gardener.cloud/apiserver/username</code> contains the username of the API server which is making the <code>TokenReview</code> request (it&rsquo;s the ServiceAccount of the API Server Pod in this case)</li><li><code>gardener.cloud/oidc/name</code> contains the name of the <code>OpenIDConnect</code> authenticator which was used.</li><li><code>gardener.cloud/oidc/uid</code> contains the <code>metadata.uid</code> of the <code>OpenIDConnect</code> authenticator which was used.</li><li><code>gardener.cloud/oidc/resourceVersion</code> contains the <code>metadata.resourceVersion</code> of the <code>OpenIDConnect</code> authenticator which was used.</li></ol></li><li><p>Kube APIServer proceeds with authorization checks and returns response.</p></li></ol><p>An overview of the flow:</p><p><img src=/__resources/authentication_eac090.svg alt="alt text" title="Authentication with OIDC webhook"></p><h2 id=deployment-for-shoot-clusters>Deployment for Shoot clusters</h2><p>To save cost, a single (multi-replica) deployment of OWA can be deployed in the <code>Seed</code> cluster. All Shoot API Servers are started with</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>--authentication-token-webhook-config-file=/etc/webhook/kubeconfig
</code></pre></div><p>where <code>/etc/webhook/kubeconfig</code> would contain a standard <code>kubeconfig</code>, with using for authentication the Service Account token of the API Server:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Config
clusters:
- name: authenticator
  cluster:
    certificate-authority-data: LS0tLS1CRU...
    server: https://oidc-webhook-authenticator/odic-authenticator-system.svc/validate-token
users:
- name: token
  user:
    tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
current-context: webhook
contexts:
- context:
    cluster: authenticator
    user: token
  name: webhook
</code></pre></div><p>Depending on the version of the Seed cluster and configuration, <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume projection</a> should be used instead of static ServiceAccount Tokens:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>volumes:
- name: oidc-authenticator-token
  projected:
    sources:
    - serviceAccountToken:
        path: oidc-authenticator-token
        expirationSeconds: 7200
        audience: oidc-authenticator
</code></pre></div><p>OWA is deployed via <code>ControllerRegistration</code>, which deploys the necessary components and inject the necessary shoot kube-apiserver configuration via a <code>MutatingWebhookConfiguration</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3a2b9eef5225b9d23e95c0373526624f>12 - 13 Automated Seed Management</h1><h1 id=automated-seed-management>Automated Seed Management</h1><p>Automated seed management involves automating certain aspects of managing seeds in Garden clusters, such as:</p><ul><li><a href=#ensuring-seeds-capacity-for-shoots-is-not-exceeded>Ensuring that the seeds capacity for shoots is not exceeded</a></li><li><a href=#managedseeds>Creating, deleting, and updating seeds declaratively as &ldquo;managed seeds&rdquo;</a></li><li><a href=#managedseedsets>Declaratively managing sets of similar &ldquo;managed seeds&rdquo; as &ldquo;managed seed sets&rdquo; which can be scaled up/down</a></li><li><a href=#auto-scaling-seeds>Auto-scaling seeds upon reaching capacity thresholds</a></li></ul><p>Implementing the above features would involve changes to various existing Gardener components, as well as perhaps introducing new ones. This document describes these features in more detail and proposes a design approach for some of them.</p><p>In Gardener, scheduling shoots onto seeds is quite similar to scheduling pods onto nodes in Kubernetes. Therefore, a guiding principle behind the proposed design approaches is taking advantage of best practices and existing components already used in Kubernetes.</p><h2 id=ensuring-seeds-capacity-for-shoots-is-not-exceeded>Ensuring Seeds Capacity for Shoots Is Not Exceeded</h2><p>Seeds have a practical limit of how many shoots they can accommodate. Exceeding this limit is undesirable as the system performance will be noticeably impacted. Therefore, it is important to ensure that a seed&rsquo;s capacity for shoots is not exceeded by introducing a maximum number of shoots that can be scheduled onto a seed and making sure that it is taken into account by the scheduler.</p><p>An initial discussion of this topic is available in <a href=https://github.com/gardener/gardener/issues/2938>Issue #2938</a>. The proposed solution is based on the following flow:</p><ul><li>The <code>gardenlet</code> is configured with certain <em>resources</em> and their total <em>capacity</em> (and, for certain resources, the amount reserved for Gardener).</li><li>The <code>gardenlet</code> seed controller updates the Seed status with the capacity of each resource and how much of it is actually available to be consumed by shoots, using <code>capacity</code> and <code>allocatable</code> fields that are very similar to the corresponding fields in <a href=https://github.com/kubernetes/api/blob/2c3c141c931c0ab1ce1396c3152c72852b3d37ee/core/v1/types.go#L4582-L4593>the Node status</a>.</li><li>When scheduling shoots, <code>gardener-scheduler</code> is influenced by the remaining capacity of the seed. In the simplest possible implementation, it never schedules shoots onto a seed that has already reached its capacity for a resource needed by the shoot.</li></ul><p>Initially, the only resource considered would be the maximum number of shoots that can be scheduled onto a seed. Later, more resources could be added to make more precise scheduling calculations.</p><p><strong>Note:</strong> Resources could also be requested by shoots, similarly to how pods can request node resources, and the scheduler could then ensure that such requests are taken into account when scheduling shoots onto seeds. However, the user is rarely, if at all, concerned with what resources does a shoot consume from a seed, and this should also be regarded as an implementation detail that could change in the future. Therefore, such resource requests are not included in this GEP.</p><p>In addition, an extensibility plugin framework could be introduced in the future in order to advertise custom resources, including provider-specific resources, so that <code>gardenlet</code> would be able to update the seed status with their capacity and allocatable values, for example load balancers on Azure. Such a concept is not described here in further details as it is sufficiently complex to require a separate GEP.</p><p>Example Seed status with <code>capacity</code> and <code>allocatable</code> fields:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>status:
  capacity:
    shoots: <span style=color:#a31515>&#34;100&#34;</span>
    persistent-volumes: <span style=color:#a31515>&#34;200&#34;</span> <span style=color:green># Built-in resource</span>
    azure.provider.extensions.gardener.cloud/load-balancers: <span style=color:#a31515>&#34;30&#34;</span> <span style=color:green># Custom resource advertised by an Azure-specific plugin</span>
  allocatable:
    shoots: <span style=color:#a31515>&#34;100&#34;</span>
    persistent-volumes: <span style=color:#a31515>&#34;197&#34;</span> <span style=color:green># 3 persistent volumes are reserved for Gardener</span>
    azure.provider.extensions.gardener.cloud/load-balancers: <span style=color:#a31515>&#34;300&#34;</span>
</code></pre></div><h3 id=gardenlet-configuration>Gardenlet Configuration</h3><p>As mentioned above, the total resource capacity for built-in resources such as the number of shoots is specified as part of the <code>gardenlet</code> configuration, not in the Seed spec. The <code>gardenlet</code> configuration itself could be specified in the spec of the newly introduced <a href=#managedseeds>ManagedSeed</a> resource. Here it is assumed that in the future this could become the recommended and most widely used way to manage seeds. If the same <code>gardenlet</code> is responsible for multiple seeds, they would all share the same capacity settings.</p><p>To specify the total resource capacity for built-in resources, as well as the amount of such resources reserved for Gardener, the 2 new fields <code>resources.capacity</code> and <code>resources.reserved</code> are introduced in the <code>GardenletConfiguration</code> resource. The <code>gardenlet</code> seed controller would then initialize the <code>capacity</code> and <code>allocatable</code> fields in the seed status as follows:</p><ul><li>The <code>capacity</code> value is set to the configured <code>resources.capacity</code>.</li><li>The <code>allocatable</code> value is set to the configured <code>resources.capacity</code> minus <code>resources.reserved</code>.</li></ul><p>Example <code>GardenletConfiguration</code> with <code>resources.capacity</code> and <code>resources.reserved</code> field:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>resources:
  capacity:
    shoots: 100
    persistent-volumes: 200
  reserved:
    persistent-volumes: 3
</code></pre></div><h3 id=scheduling-algorithm>Scheduling Algorithm</h3><p>Currently <code>gardener-scheduler</code> uses a simple non-extensible algorithm in order to schedule shoots onto seeds. It goes through the following stages:</p><ul><li>Filter out seeds that don&rsquo;t meet scheduling requirements such as being ready, matching cloud profile and shoot label selectors, matching the shoot provider, and not having taints that are not tolerated by the shoot.</li><li>From the remaining seeds, determine candidates that are considered best based on their region, by using a strategy that can be either &ldquo;same region&rdquo; or &ldquo;minimal distance&rdquo;.</li><li>Among these candidates, choose the one with the least number of shoots.</li></ul><p>This scheduling algorithm should be adapted in order to properly take into account resources capacity and requests. As a first step, during the filtering stage, any seeds that would exceed their capacity for shoots, or their capacity for any resources requested by the shoot, should simply be filtered out and not considered during the next stages.</p><p>Later, the scheduling algorithm could be further enhanced by replacing the step in which the region strategy is applied by a scoring step similar to the one in <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/>Kubernetes Scheduler</a>. In this scoring step, the scheduler would rank the remaining seeds to choose the most suitable shoot placement. It would assign a score to each seed that survived filtering based on a list of scoring rules. These rules might include for example <code>MinimalDistance</code> and <code>SeedResourcesLeastAllocated</code>, among others. Each rule would produce its own score for the seed, and the overall seed score would be calculated as a weighted sum of all such scores. Finally, the scheduler would assign the shoot to the seed with the highest ranking.</p><h2 id=managedseeds>ManagedSeeds</h2><p>When all or most of the existing seeds are near capacity, new seeds should be created in order to accommodate more shoots. Conversely, sometimes there could be too many seeds for the number of shoots, and so some of the seeds could be deleted to save resources. Currently, the process of creating a new seed involves a number of manual steps, such as creating a new shoot that meets certain criteria, and then registering it as a seed in Gardener. This could be automated to some extent by <a href=/docs/gardener/usage/shooted_seed/>annotating a shoot with the <code>use-as-seed</code> annotation</a>, in order to create a &ldquo;shooted seed&rdquo;. However, adding more than one similar seeds still requires manually creating all needed shoots, annotating them appropriately, and making sure that they are successfully reconciled and registered.</p><p>To create, delete, and update seeds effectively in a declarative way and allow auto-scaling, a &ldquo;creatable seed&rdquo; resource along with a &ldquo;set&rdquo; (and in the future, perhaps also a &ldquo;deployment&rdquo;) of such creatable seeds should be introduced, similar to Kubernetes <code>Pod</code>, <code>ReplicaSet</code>, and <code>Deployment</code> (or to MCM <code>Machine</code>, <code>MachineSet</code>, and <code>MachineDeployment</code>) resources. With such resources (and their respective controllers), creating a new seed based on a template would become as simple as increasing the <code>replicas</code> field in the &ldquo;set&rdquo; resource.</p><p>In <a href=https://github.com/gardener/gardener/issues/2181>Issue #2181</a> it is already proposed that the <code>use-as-seed</code> annotation is replaced by a dedicated <code>ShootedSeed</code> resource. The solution proposed here further elaborates on this idea.</p><h3 id=managedseed-resource>ManagedSeed Resource</h3><p>The <code>ManagedSeed</code> resource is a dedicated custom resource that represents an evolution of the &ldquo;shooted seed&rdquo; and properly replaces the <code>use-as-seed</code> annotation. This resource contains:</p><ul><li>The name of the Shoot that should be registered as a Seed.</li><li>An optional <code>seedTemplate</code> section that contains the Seed spec and parts of the metadata, such as labels and annotations.</li><li>An optional <code>gardenlet</code> section that contains:<ul><li><code>gardenlet</code> deployment parameters, such as the number of replicas, the image, etc.</li><li>The <code>GardenletConfiguration</code> resource that contains controllers configuration, feature gates, and a <code>seedConfig</code> section that contains the <code>Seed</code> spec and parts of its metadata.</li><li>Additional configuration parameters, such as the garden connection bootstrap mechanism (see <a href=/docs/gardener/concepts/gardenlet/#tls-bootstrapping>TLS Bootstrapping</a>), and whether to merge the provided configuration with the configuration of the parent <code>gardenlet</code>.</li></ul></li></ul><p>Either the <code>seedTemplate</code> or the <code>gardenlet</code> section must be specified, but not both:</p><ul><li>If the <code>seedTemplate</code> section is specified, <code>gardenlet</code> is not deployed to the shoot, and a new <code>Seed</code> resource is created based on the template.</li><li>If the <code>gardenlet</code> section is specified, <code>gardenlet</code> is deployed to the shoot, and it registers a new seed upon startup based on the <code>seedConfig</code> section of the <code>GardenletConfiguration</code> resource.</li></ul><p>A ManagedSeed allows fine-tuning the seed and the <code>gardenlet</code> configuration of shooted seeds in order to deviate from the global defaults, e.g. lower the concurrent sync for some of the seed&rsquo;s controllers or enable a feature gate only on certain seeds. Also, it simplifies the deletion protection of such seeds.</p><p>Also, the <code>ManagedSeed</code> resource is a more powerful alternative to the <code>use-as-seed</code> annotation. The implementation of the <code>use-as-seed</code> annotation itself could be refactored to use a <code>ManagedSeed</code> resource extracted from the annotation by a controller.</p><p>Although in this proposal a ManagedSeed is always a &ldquo;shooted seed&rdquo;, that is a Shoot that is registered as a Seed, this idea could be further extended in the future by adding a <code>type</code> field that could be either <code>Shoot</code> (implied in this proposal), or something different. Such an extension would allow to register and manage as Seed a cluster that is not a Shoot, e.g. a GKE cluster.</p><p>Last but not least, ManagedSeeds could be used as the basis for creating and deleting seeds automatically via the <code>ManagedSeedSet</code> resource that is described in <a href=#managedseedsets>ManagedSeedSets</a>.</p><p>Unlike the <code>Seed</code> resource, the <code>ManagedSeed</code> resource is namespaced. If created in the <code>garden</code> namespace, the resulting seed is globally available. If created in a project namespace, the resulting seed can be used as a &ldquo;private seed&rdquo; by shoots in the project, either by being decorated with project-specific taints and labels, or by being of the special <code>PrivateSeed</code> kind that is also namespaced. The concept of private seeds / cloudprofiles is described in <a href=https://github.com/gardener/gardener/issues/2874>Issue #2874</a>. Until this concept is implemented, <code>ManagedSeed</code> resources might need to be restricted to the <code>garden</code> namespace, similarly to how shoots with the <code>use-as-seed</code> annotation currently are.</p><p>Example <code>ManagedSeed</code> resource with a <code>seedTemplate</code> section:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: seedmanagement.gardener.cloud/v1alpha1
kind: ManagedSeed
metadata:
  name: crazy-botany
  namespace: garden
spec:
  shoot:
    name: crazy-botany <span style=color:green># Shoot that should be registered as a Seed</span>
  seedTemplate: <span style=color:green># Seed template, including spec and parts of the metadata</span>
    metadata:
      labels:
        foo: bar
    spec:
      provider:
        type: gcp
        region: europe-west1
      taints:
      - key: seed.gardener.cloud/protected
      ...
</code></pre></div><p>Example <code>ManagedSeed</code> resource with a <code>gardenlet</code> section:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: seedmanagement.gardener.cloud/v1alpha1
kind: ManagedSeed
metadata:
  name: crazy-botany
  namespace: garden
spec:
  shoot:
    name: crazy-botany <span style=color:green># Shoot that should be registered as a Seed</span>
  gardenlet: 
    deployment: <span style=color:green># Gardenlet deployment configuration</span>
      replicaCount: 1
      revisionHistoryLimit: 10
      serviceAccountName: gardenlet
      image:
        repository: eu.gcr.io/gardener-project/gardener/gardenlet
        tag: latest
        pullPolicy: IfNotPresent
      resources:
        ...
      podLabels:
        ...
      podAnnotations: 
        ...
      additionalVolumes:
        ...
      additionalVolumeMounts:
        ...
      env:
        ...
      vpa: <span style=color:#00f>false</span>
    config: <span style=color:green># GardenletConfiguration resource</span>
      apiVersion: gardenlet.config.gardener.cloud/v1alpha1
      kind: GardenletConfiguration
      seedConfig: <span style=color:green># Seed template, including spec and parts of the metadata</span>
        metadata:
          labels:
            foo: bar
        spec:
          provider:
            type: gcp
            region: europe-west1
          taints:
          - key: seed.gardener.cloud/protected
          ...
      controllers:
        shoot:
          concurrentSyncs: 20
      featureGates:
        CachedRuntimeClients: <span style=color:#00f>true</span>
      ...
    bootstrap: BootstrapToken
    mergeWithParent: <span style=color:#00f>true</span>
</code></pre></div><h3 id=managedseed-controller>ManagedSeed Controller</h3><p>ManagedSeeds are reconciled by a new <em>managed seed controller</em> in <code>gardenlet</code>. Its implementation is very similar to the current <a href=https://github.com/gardener/gardener/blob/master/pkg/gardenlet/controller/shoot/seed_registration_control.go>seed registration controller</a>, and in fact could be regarded as a refactoring of the latter, with the difference that it uses the <code>ManagedSeed</code> resource rather than the <code>use-as-seed</code> annotation on a Shoot. The <code>gardenlet</code> only reconciles ManagedSeeds that refer to Shoots scheduled on Seeds the <code>gardenlet</code> is responsible for.</p><p>Once this controller is considered sufficiently stable, the current <code>use-as-seed</code> annotation and the controller mentioned above should be marked as deprecated and eventually removed.</p><p>A <code>ManagedSeed</code> that is in use by shoots cannot be deleted, unless the shoots are either deleted or moved to other seeds first. The managed seed controller ensures that this is the case by only allowing a ManagedSeed to be deleted if its Seed has been already deleted.</p><h3 id=managedseed-admission-plugins>ManagedSeed Admission Plugins</h3><p>In addition to the managed seed controller mentioned above, new <code>gardener-apiserver</code> admission plugins should be introduced to properly validate the creation and update of ManagedSeeds, as well as the deletion of shoots registered as seeds. These plugins should ensure that:</p><ul><li>A <code>Shoot</code> that is being referred to by a <code>ManagedSeed</code> cannot be deleted.</li><li>Certain <code>Seed</code> spec fields, for example the provider type and region, networking CIDRs for pods, services, and nodes, etc., are the same as (or compatible with) the corresponding <code>Shoot</code> spec fields of the shoot that is being registered as seed.</li><li>If such <code>Seed</code> spec fields are omitted or empty, the plugins should supply proper defaults based on the values in the <code>Shoot</code> resource.</li></ul><h3 id=provider-specific-seed-bootstrapping-actions>Provider-specific Seed Bootstrapping Actions</h3><p>Bootstrapping a new seed might require additional provider-specific actions to the ones performed automatically by the managed seed controller. For example, on Azure this might include getting a new subscription, extending quotas, etc. This could eventually be automated by introducing an extension mechanism for the Gardener seed bootstrapping flow, to be handled by a new type of controller in the provider extensions. However, such an extension mechanism is not in the scope of this proposal and might require a separate GEP.</p><p>One idea that could be further explored is the use <em>shoot readiness gates</em>, similar to Kubernetes <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-readiness-gate>pod readiness gates</a>, in order to control whether a Shoot is considered <code>Ready</code> before it could be registered as a Seed. A provider-specific extension could set the special condition that is specified as a readiness gate to <code>True</code> only after it has successfully performed the provider-specific actions needed.</p><h3 id=changes-to-existing-controllers>Changes to Existing Controllers</h3><p>Since the Shoot registration as a Seed is decoupled from the Shoot reconciliation, existing <code>gardenlet</code> controllers would not have to be changed in order to properly support ManagedSeeds. The main change to <code>gardenlet</code> that would be needed is introducing the new <em>managed seed controller</em> mentioned above, and possibly retiring the old one at some point. In addition, the Shoot controller would need to be adapted as it currently performs certain actions differently if the shoot has a &ldquo;shooted seed&rdquo;.</p><p>The introduction of the <code>ManagedSeed</code> resource would also require no changes to existing <code>gardener-controller-manager</code> controllers that operate on Shoots (for example, shoot hibernation and maintenance controllers).</p><h2 id=managedseedsets>ManagedSeedSets</h2><p>Similarly to a <a href=https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/>ReplicaSet</a>, the purpose of a ManagedSeedSet is to maintain a stable set of replica <a href=#managedseeds>ManagedSeeds</a> available at any given time. As such, it is used to guarantee the availability of a specified number of identical ManagedSeeds, on an equal number of identical Shoots.</p><h3 id=managedseedset-resource>ManagedSeedSet Resource</h3><p>The <code>ManagedSeedSet</code> resource has a <code>selector</code> field that specifies how to identify ManagedSeeds it can acquire, a number of <code>replicas</code> indicating how many ManagedSeeds (and their corresponding Shoots) it should be maintaining, and a two templates:</p><ul><li>A ManagedSeed template (<code>template</code>) specifying the data of new ManagedSeeds it should create to meet the number of replicas criteria.</li><li>A Shoot template (<code>shootTemplate</code>) specifying the data of new Shoots it should create to host the ManagedSeeds.</li></ul><p>A ManagedSeedSet then fulfills its purpose by creating and deleting ManagedSeeds (and their corresponding Shoots) as needed to reach the desired number.</p><p>A ManagedSeedSet is linked to its ManagedSeeds and Shoots via the <code>metadata.ownerReferences</code> field, which specifies what resource the current object is owned by. All ManagedSeeds and Shoots acquired by a ManagedSeedSet have their owning ManagedSeedSet&rsquo;s identifying information within their <code>ownerReferences</code> field.</p><p>Example <code>ManagedSeedSet</code> resource:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: seedmanagement.gardener.cloud/v1alpha1
kind: ManagedSeedSet
metadata:
  name: crazy-botany
  namespace: garden
spec:
  replicas: 3
  selector:
    matchLabels:
      foo: bar
  updateStrategy:
    type: RollingUpdate <span style=color:green># Update strategy, must be `RollingUpdate`</span>
    rollingUpdate:
      partition: 2 <span style=color:green># Only update the last replica (#2), assuming there are no gaps (&#34;rolling out a canary&#34;)</span>
  template: <span style=color:green># ManagedSeed template, including spec and parts of the metadata</span>
    metadata:
      labels:
        foo: bar
    spec: 
      <span style=color:green># shoot.name is not specified since it&#39;s filled automatically by the controller</span>
      seedTemplate: <span style=color:green># Either a seed or a gardenlet section must be specified, see above</span>
        metadata:
          labels:
            foo: bar
        provider:
          type: gcp
          region: europe-west1
        taints:
        - key: seed.gardener.cloud/protected
        ...
  shootTemplate: <span style=color:green># Shoot template, including spec and parts of the metadata</span>
    metadata:
      labels:
        foo: bar
    spec:
      cloudProfileName: gcp
      secretBindingName: shoot-operator-gcp
      region: europe-west1
      provider:
        type: gcp
      ...
</code></pre></div><h3 id=managedseedset-controller>ManagedSeedSet Controller</h3><p>ManagedSeedSets are reconciled by a new <em>managed seed set controller</em> in <code>gardener-controller-manager</code>. During the reconciliation this controller creates and deletes ManagedSeeds and Shoots in response to changes to the <code>replicas</code> and <code>selector</code> fields.</p><p><strong>Note:</strong> The introduction of the <code>ManagedSeedSet</code> resource would not require any changes to <code>gardenlet</code> or to existing <code>gardener-controller-manager</code> controllers.</p><h3 id=managing-managedseed-updates>Managing ManagedSeed Updates</h3><p>To manage ManagedSeed updates, we considered two possible approaches:</p><ul><li>A ManagedSeedSet, similarly to a ReplicaSet, does not manage updates to its replicas in any way. In the future, we might introduce ManagedSeedDeployments, a higher-level concept that manages ManagedSeedSets and provides declarative updates to ManagedSeeds along with other useful features, similarly to a <a href=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/>Deployment</a>. Such a mechanism would involve creating new ManagedSeedSets, and therefore new seeds, behind the scenes, and moving existing shoots to them.</li><li>A ManagedSeedSet does manage updates to its replicas, similarly to a <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/>StatefulSet</a>. Updates are performed &ldquo;in-place&rdquo;, without creating new seeds and moving existing shoots to them. Such a mechanism could also take advantage of other StatefulSet features, such as ordered rolling updates and phased rollouts.</li></ul><p>There is an important difference between seeds and pods or nodes in that seeds are more &ldquo;heavyweight&rdquo; and therefore updating a set of seeds by introducing new seeds and moving shoots to them tends to be much more complex, time-consuming, and prone to failures compared to updating the seeds &ldquo;in place&rdquo;. Furthermore, updating seeds in this way depends on a mature implementation of <a href=/docs/gardener/proposals/07-shoot-control-plane-migration/>GEP-7: Shoot Control Plane Migration</a>, which is not available right now. Due to these considerations, we favor the second approach over the first one.</p><h4 id=managedseed-identity-and-order>ManagedSeed Identity and Order</h4><p>A StatefulSet manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods. It maintains a <em>stable identity</em> (including network identity) for each of their Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.</p><p>A StatefulSet achieves the above by associating each replica with an <em>ordinal number</em>. With n replicas, these ordinal numbers range from 0 to n-1. When scaling out, newly added replicas always have ordinal numbers larger than those of previously existing replicas. When scaling in, it is the replicas with the largest original numbers that are removed.</p><p>Besides stable identity and persistent storage, these ordinal numbers are also used to implement the following StatefulSet features:</p><ul><li>Ordered, graceful deployment and scaling.</li><li>Ordered, automated rolling updates. Such rolling updates can be <em>partitioned</em> (limited to replicas with ordinal numbers greater than or equal to the &ldquo;partition&rdquo;) to achieve <em>phased rollouts</em>.</li></ul><p>A ManagedSeedSet, unlike a StatefulSet, does not need to maintain a stable identity for its ManagedSeeds. Furthermore, it would not be practical to always remove the replicas with the largest ordinal numbers when scaling in, since the corresponding seeds may have shoots scheduled onto them, while other seeds, with lower ordinals, may have fewer shoots (or none), and therefore be much better candidates for being removed.</p><p>On the other hand, it would be beneficial if a ManagedSeedSet, like a StatefulSet, provides ordered deployment and scaling, ordered rolling updates, and phased rollouts. The main advantage of these features is that a deployment or update failure would affect fewer replicas (ideally just one), containing any potential damage and making the situation easier to handle, thus achieving some of the goals stated in <a href=https://github.com/gardener/gardener/issues/87>Issue #87</a>. They could also help to contain seed rolling updates outside business hours.</p><p>Based on the above considerations, we propose the following mechanism for handling ManagedSeed identity and order:</p><ul><li>A ManagedSeedSet uses <em>ordinal numbers generated by an increasing sequence</em> to identify ManagedSeeds and Shoots it creates and manages. These numbers always start from 0 and are incremented by 1 for each newly added replica.</li><li>Replicas (both ManagedSeeds and Shoots) are named after the ManagedSeedSet with the ordinal number appended. For example, for a ManagedSeedSet named <code>test</code> its replicas are named <code>test-0</code>, <code>test-1</code>, etc.</li><li>Gaps in the sequence created by removing replicas with ordinal numbers in the middle of the range are never filled in. A newly added replica always receives a number that is not only free, but also unique to itself. For example, if there are 2 replicas named <code>test-0</code> and <code>test-1</code> and any one of them is removed, a newly added replica will still be named <code>test-2</code>.</li></ul><p>Although such ordinal numbers can also provide some form of stable identity, in this case it is much more important that they can provide a predictable ordering for deployments and updates, and can also be used to partition rolling updates similarly to StatefulSet ordinal numbers.</p><h4 id=update-strategies>Update Strategies</h4><p>The ManagedSeedSet&rsquo;s <code>.spec.updateStrategy</code> field allows configuring automated rolling updates for the ManagedSeeds and Shoots in a ManagedSeedSet.</p><p><strong>Rolling Updates</strong></p><p>The <code>RollingUpdate</code> update strategy implements automated, rolling update for the ManagedSeeds and Shoots in a ManagedSeedSet. With this strategy, the ManagedSeedSet controller will update each ManagedSeed and Shoot in the ManagedSeedSet. It will proceed from the largest number to the smallest, updating each ManagedSeed and its corresponding Shoot one at a time. It will wait until both the Shoot and the Seed of an updated ManagedSeed are Ready prior to updating its predecessor.</p><p>As a further improvement upon the above, the controller could check not only the ManagedSeeds and their corresponding Shoots for readiness, but also the Shoots scheduled onto these ManagedSeeds. The rollout would then only continue if no more than X percent of these Shoots are not reconciled and Ready. Since checking all these additional conditions might require some complex logic, it should be performed by an independent <em>managed seed care controller</em> that updates the ManagedSeed resource with the readiness of its Seed and all Shoots scheduled onto the Seed.</p><p>Note that unlike a StatefulSet, an <code>OnDelete</code> update strategy is not supported.</p><p><strong>Partitions</strong></p><p>The <code>RollingUpdate</code> update strategy can be partitioned, by specifying a <code>.spec.updateStrategy.rollingUpdate.partition</code>. If a partition is specified, only ManagedSeeds and Shoots with ordinals greater than or equal to the partition will be updated when any of the ManagedSeedSet&rsquo;s templates is updated. All remaining ManagedSeeds and Shoots will not be updated. If a ManagedSeedSet&rsquo;s <code>.spec.updateStrategy.rollingUpdate.partition</code> is greater than the largest ordinal number in use by a replica, updates to its templates will not be propagated to its replicas (but newly added replicas may still use the updated templates depending on the partition value).</p><h4 id=keeping-track-of-revision-history-and-performing-rollbacks>Keeping Track of Revision History and Performing Rollbacks</h4><p>Similarly to a StatefulSet, the ManagedSeedSet controller uses <a href=https://pkg.go.dev/k8s.io/api/apps/v1#ControllerRevision>ControllerRevisions</a> to keep track of the revision history, and <code>controller-revision-hash</code> labels to maintain an association between a ManagedSeed or a Shoot and the concrete template revisions based on which they were created or last updated. These are used for the following purposes:</p><ul><li>During an update, determine which replicas are still not on the latest revision and therefore should be updated.</li><li>Display the revision history of a ManagedSeedSet via <code>kubectl rollout history</code>.</li><li>Roll back all ManagedSeedSet replicas to a specific revision via <code>kubectl rollout undo</code></li></ul><p><strong>Note:</strong> The above <code>kubectl rollout</code> commands will not work with custom resources such as ManagedSeedSets out of the box (the <a href=https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#rollout>documentation</a> says explicitly that valid resource types are only deployments, daemonsets, and statefulsets), but it should be possible to eventually support such commands for ManagedSeedSets via a <a href=https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/>kubectl plugin</a>.</p><h3 id=scaling-in-managedseedsets>Scaling-in ManagedSeedSets</h3><p>Deleting ManagedSeeds in response to decreasing the replicas of a ManagedSeedSet deserves special attention for two reasons:</p><ul><li>A seed that is already in use by shoots cannot be deleted, unless the shoots are either deleted or moved to other seeds first.</li><li>When there are more empty seeds than requested for deletion, determining which seeds to delete might not be as straightforward as with pods or nodes.</li></ul><p>The above challenges could be addressed as follows:</p><ul><li>In order to scale in a ManagedSeedSet successfully, there should be at least as many empty ManagedSeeds as the difference between the old and the new replicas. In some cases, the user might need to ensure that this is the case by draining some seeds manually before decreasing the replicas field.</li><li>It should be possible to protect ManagedSeeds from deletion even if they are empty, perhaps via an annotation such as <code>seedmanagement.gardener.cloud/protect-from-deletion</code>. Such seeds are not taken into account when determining whether the scale in operation can succeed.</li><li>The decision which seeds to delete among the ManagedSeeds that are empty and not protected should be based on hints, perhaps again in the form of annotations, that could be added manually by the user, as well as other factors, see <a href=#prioritizing-managedseed-deletion>Prioritizing ManagedSeed Deletion</a>.</li></ul><h4 id=prioritizing-managedseed-deletion>Prioritizing ManagedSeed Deletion</h4><p>To help the controller decide which empty ManagedSeeds are to be deleted first, the user could manually annotate ManagedSeeds with a <em>seed priority annotation</em> such as <code>seedmanagement.gardener.cloud/priority</code>. ManagedSeeds with lower priority are more likely to be deleted first. If not specified, a certain default value is assumed, for example 3.</p><p>Besides this annotation, the controller should take into account also other factors, such as the current seed conditions (<code>NotReady</code> should be preferred for deletion over <code>Ready</code>), as well as its age (older should be preferred for deletion over newer).</p><h2 id=auto-scaling-seeds>Auto-scaling Seeds</h2><p>The most interesting and advanced automated seed management feature is making sure that a Garden cluster has enough seeds registered to schedule new shoots (and, in the future, reschedule shoots from drained seeds) without exceeding the seeds capacity for shoots, but not more than actually needed at any given moment. This would involve introducing an auto-scaling mechanism for seeds in Garden clusters.</p><p>The proposed solution builds upon the ideas introduced earlier. The <a href=#managedseeds><code>ManagedSeedSet</code></a> resource (and in the future, also the <code>ManagedSeedDeployment</code> resource) could have a <code>scale</code> subresource that changes the <code>replicas</code> field. This would allow a new &ldquo;seed autoscaler&rdquo; controller to scale these resources via a special &ldquo;autoscaler&rdquo; resource (for example <code>SeedAutoscaler</code>), similarly to how the Kubernetes <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/>Horizontal Pod Autoscaler</a> controller scales pods, as described in <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/>Horizontal Pod Autoscaler Walkthrough</a>.</p><p>The primary metric used for scaling should be the number of shoots already scheduled onto that seed either as a direct value or as a percentage of the seed&rsquo;s capacity for shoots introduced in <a href=#ensuring-seeds-capacity-for-shoots-is-not-exceeded>Ensuring Seeds Capacity for Shoots Is Not Exceeded</a> (<em>utilization</em>). Later, custom metrics based on other resources, including provider-specific resources, could be considered as well.</p><p><strong>Note:</strong> Even if the controller is called <em>Horizontal Pod Autoscaler</em>, it is capable of scaling any resource with a <code>scale</code> subresource, using any custom metric. Therefore, initially it was proposed to use this controller directly. However, a number of important drawbacks were identified with this approach, and so it is no longer proposed here.</p><h3 id=seedautoscaler-resource>SeedAutoscaler Resource</h3><p>The SeedAutoscaler automatically scales the number of <a href=#managedseeds>ManagedSeeds</a> in a <a href=#managedseedsets>ManagedSeedSet</a> based on observed resource utilization. The resource could be any resource that is tracked via the <code>capacity</code> and <code>allocatable</code> fields in the Seed status, including in particular the number of shoots already scheduled onto the seed.</p><p>The SeedAutoscaler is implemented as a custom resource and a new controller. The resource determines the behavior of the controller. The <code>SeedAutoscaler</code> resource has a <code>scaleTargetRef</code> that specifies the target resource to be scaled, the minimum and maximum number of replicas, as well as a list of metrics. The only supported metric type initially is <code>Resource</code> for resources that are tracked via the <code>capacity</code> and <code>allocatable</code> fields in the Seed status. The resource target can be of type <code>Utilization</code> or <code>AverageValue</code>.</p><p>Example <code>SeedAutoscaler</code> resource:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: seedmanagement.gardener.cloud/v1alpha1
kind: SeedAutoscaler
metadata:
  name: crazy-botany
  namespace: garden
spec:
  scaleTargetRef:
    apiVersion: seedmanagement.gardener.cloud/v1alpha1
    kind: ManagedSeedSet
    name: crazy-botany
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource <span style=color:green># Only Resource is supported</span>
    resource:
      name: shoots
      target:
        type: Utilization <span style=color:green># Utilization or AverageValue</span>
        averageUtilization: 50
</code></pre></div><h3 id=seedautoscaler-controller>SeedAutoscaler Controller</h3><p><code>SeedAutoscaler</code> resources are reconciled by a new <em>seed autoscaler controller</em>, either in <code>gardener-controller-manager</code> or out-of-tree, similarly to <a href=https://github.com/gardener/autoscaler>cluster-autoscaler</a>. The controller periodically adjusts the number of replicas in a ManagedSeedSet to match the observed average resource utilization to the target specified by user.</p><p><strong>Note:</strong> The SeedAutoscaler controller should perhaps not be limited to evaluating only metrics, it could also take into account also taints, label selectors, etc. This is not yet reflected in the example <code>SeedAutoscaler</code> resource above. Such details are intentionally not specified in this GEP, they should be further explored in the issues created to track the actual implementation.</p><h4 id=evaluating-metrics-for-autoscaling>Evaluating Metrics for Autoscaling</h4><p>The metrics used by the controller, for example the <code>shoots</code> metric above, could be evaluated in one of the following ways:</p><ul><li>Directly, by looking at the <code>capacity</code> and <code>allocatable</code> fields in the Seed status and comparing to the actual resource consumption calculated by simply counting all shoots that meet a certain criteria (e.g. shoots that are scheduled onto the seed), then taking an average over all seeds in the set.</li><li>By sampling existing metrics exported for example by <a href=https://github.com/gardener/gardener-metrics-exporter><code>gardener-metrics-exporter</code></a>.</li></ul><p>The second approach decouples the seed autoscaler controller from the actual metrics evaluation, and therefore allows plugging in new metrics more easily. It also has the advantage that the exported metrics could also be used for other purposes, e.g. for triggering Prometheus alerts or building Grafana dashboards. It has the disadvantage that the seed autoscaler controller would depend on the metrics exporter to do its job properly.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-37ede2ec02b1dbd34b37081524495a25>13 - 17 Shoot Control Plane Migration Bad Case</h1><h1 id=shoot-control-plane-migration-bad-case-scenario>Shoot Control Plane Migration &ldquo;Bad Case&rdquo; Scenario</h1><p>The <a href=/docs/gardener/proposals/07-shoot-control-plane-migration/#migration-workflow>migration flow</a> described as part of <a href=/docs/gardener/proposals/07-shoot-control-plane-migration/>GEP-7</a> can only be executed if both the Garden cluster and source seed cluster are healthy, and <code>gardenlet</code> in the source seed cluster can connect to the Garden cluster. In this case, <code>gardenlet</code> can directly scale down the shoot&rsquo;s control plane in the source seed, after checking the <code>spec.seedName</code> field.</p><p>However, there might be situations in which <code>gardenlet</code> in the source seed cluster can&rsquo;t connect to the Garden cluster and determine that <code>spec.seedName</code> has changed. Similarly, the connection to the seed <code>kube-apiserver</code> could also be broken. This might be caused by issues with the seed cluster itself. In other situations, the migration flow steps in the source seed might have started but might not be able to finish successfully. In all such cases, it should still be possible to migrate a shoot&rsquo;s control plane to a different seed, even though executing the migration flow steps in the source seed might not be possible. The potential &ldquo;split brain&rdquo; situation caused by having the shoot&rsquo;s control plane components attempting to reconcile the shoot resources in two different seeds must still be avoided, by ensuring that the shoot&rsquo;s control plane in the source seed is deactivated before it is activated in the destination seed.</p><p>The mechanisms and adaptations described below have been tested as part of a PoC prior to describing them here.</p><h2 id=owner-election--copying-snapshots>Owner Election / Copying Snapshots</h2><p>To achieve the goals outlined above, an &ldquo;owner election&rdquo; (or rather, &ldquo;ownership passing&rdquo;) mechanism is introduced to ensure that the source and destination seeds are able to successfully negotiate a single &ldquo;owner&rdquo; during the migration. This mechanism is based on special <em>owner DNS records</em> that uniquely identify the seed that currently hosts the shoot&rsquo;s control plane (&ldquo;owns&rdquo; the shoot).</p><p>For example, for a shoot named <code>i500152-gcp</code> in project <code>dev</code> that uses an internal domain suffix <code>internal.dev.k8s.ondemand.com</code> and is scheduled on a seed with an identity <code>shoot--i500152--gcp2-0841c87f-8db9-4d04-a603-35570da6341f-sap-landscape-dev</code>, the owner DNS record is a TXT record with a domain name <code>owner.i500152-gcp.dev.internal.dev.k8s.ondemand.com</code> and a single value <code>shoot--i500152--gcp2-0841c87f-8db9-4d04-a603-35570da6341f-sap-landscape-dev</code>. The owner DNS record is created and maintained by reconciling an <code>owner</code> DNSRecord resource, if the recently introduced DNSRecords feature is enabled via the <code>UseDNSRecords</code> feature gate.</p><p>Unlike other extension resources, the <code>owner</code> DNSRecord resource is not reconciled every time the shoot is reconciled, but only when the resource is created. Therefore, the owner DNS record value (the owner ID) is updated only when the shoot is migrated to a different seed. For more information, see <a href=https://github.com/gardener/gardener/pull/4307>Add handling of owner DNSRecord resources</a>.</p><p>The owner DNS record domain name and owner ID are passed to components that need to perform ownership checks, such as the <code>backup-restore</code> container of the <code>etcd-main</code> StatefulSet, and all extension controllers. These components then check regularly whether the actual owner ID (the value of the record) matches the passed ID. If they don&rsquo;t, the ownership check is considered failed, which causes the special behavior described below.</p><p><strong>Note:</strong> A previous revision of this document proposed using &ldquo;sync objects&rdquo; written to and read from the backup container of the source seed as JSON files by the <code>etcd-backup-restore</code> processes in both seeds. With the introduction of owner DNS records such sync objects are no longer needed.</p><p>For the destination seed to actually become the owner, it needs to acquire the shoot&rsquo;s etcd data by copying the final full snapshot (and potentially also older snapshots) from the backup container of the source seed.</p><p>The mechanism to copy the snapshots and pass the ownership from the source to the destination seed consists of the following steps:</p><ol><li><p>The reconciliation flow (&ldquo;restore&rdquo; phase) is triggered in the destination seed without first executing the migration flow in the source seed (or perhaps it was executed, but it failed, and its state is currently unknown).</p></li><li><p>The <code>owner</code> DNSRecord resource is created in the destination seed. As a result, the actual owner DNS record is updated with the destination seed ID. From this point, ownership checks by the <code>etcd-backup-restore</code> process and <a href=#extension-controller-watchdogs>extension controller watchdogs</a> in the source seed will fail, which will cause the special behavior described below.</p></li><li><p>An additional &ldquo;source&rdquo; backup entry referencing the source seed backup bucket is deployed to the Garden cluster and the destination seed and reconciled by the backup entry controller. As a result, a secret with the appropriate credentials for accessing the source seed backup container named <code>source-etcd-backup</code> is created in the destination seed. The normal backup entry (referencing the destination seed backup container) is also deployed and reconciled, as usual, resulting in the usual <code>etcd-backup</code> secret being created.</p></li><li><p>A special &ldquo;copy&rdquo; version of the <code>etcd-main</code> Etcd resource is deployed to the destination seed. In its <code>backup</code> section, this resource contains a <code>sourceStore</code> in addition to the usual <code>store</code>, which contains the parameters needed to use the source seed backup container, such as its name and the secret created in the previous step.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:
  backup:
    ...
    store:
      container: 408740b8-6491-415e-98e6-76e92e5956ac
      secretRef:
        name: etcd-backup
      ...
    sourceStore:
      container: d1435fea-cd5e-4d5b-a198-81f4025454ff
      secretRef:
        name: source-etcd-backup
      ...
</code></pre></div></li><li><p>The <code>etcd-druid</code> in the destination seed reconciles the above resource by deploying a <code>etcd-copy</code> Job that contains a single <code>backup-restore</code> container. It executes the newly introduced <code>copy</code> command of <code>etcd-backup-restore</code> that copies the snapshots from the source to the destination backup container.</p></li><li><p>Before starting the copy itself, the <code>etcd-backup-restore</code> process in the destination seed checks if a final full snapshot (a full snapshot marked as <code>final=true</code>) exists in the backup container. If such a snapshot is not found, it waits for it to appear in order to proceed. This waiting is up to a certain timeout that should be sufficient for a full snapshot to be taken; after this timeout has elapsed, it proceeds anyway, and the reconciliation flow continues from step 9. As described in <a href=#handling-inability-to-access-the-backup-container>Handling Inability to Access the Backup Container</a> below, this is safe to do.</p></li><li><p>The <code>etcd-backup-restore</code> process in the source seed detects that the owner ID in the owner DNS record is different from the expected owner ID (because it was updated in step 2) and switches to a special &ldquo;final snapshot&rdquo; mode. In this mode the regular snapshotter is stopped, the readiness probe of the main <code>etcd</code> container starts returning 503, and one final full snapshot is taken. This snapshot is marked as <code>final=true</code> in order to ensure that it&rsquo;s only taken once, and in order to enable the <code>etcd-backup-restore</code> process in the destination seed to find it (see step 6).</p><p><strong>Note:</strong> While testing our PoC, we noticed that simply making the readiness probe of the main <code>etcd</code> container fail doesn&rsquo;t terminate the existing open connections from <code>kube-apiserver</code> to <code>etcd</code>. For this to happen, either the <code>kube-apiserver</code> or the <code>etcd</code> process has to be restarted at least once. Therefore, when the snapshotter is stopped because an ownership change has been detected, the main <code>etcd</code> process is killed (using <code>SIGTERM</code> to allow graceful termination) to ensure that any open connections from <code>kube-apiserver</code> are terminated. For this to work, the 2 containers must <a href=https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/>share the process namespace</a>.</p></li><li><p>Since the <code>kube-apiserver</code> process in the source seed is no longer able to connect to <code>etcd</code>, all shoot control plane controllers (<code>kube-controller-manager</code>, <code>kube-scheduler</code>, <code>machine-controller-manager</code>, etc.) and extension controllers reconciling shoot resources in the source seed that require a connection to the shoot in order to work start failing. All remaining extension controllers are prevented from reconciling shoot resources via the <a href=#extension-controller-watchdogs>watchdogs</a> mechanism. At this point, the source seed has effectively lost its ownership of the shoot, and it is safe for the destination seed to assume the ownership.</p></li><li><p>After the <code>etcd-backup-restore</code> process in the destination seed detects that a final full snapshot exists, it copies all snapshots (or a subset of all snapshots) from the source to the destination backup container. When this is done, the Job finishes successfully which signals to the reconciliation flow that the snapshots have been copied.</p><p><strong>Note:</strong> To save time, only the final full snapshot taken in step 6, or a subset defined by some criteria, could be copied, instead of all snapshots.</p></li><li><p>The special &ldquo;copy&rdquo; version of the <code>etcd-main</code> Etcd resource is deleted from the source seed, and as a result the <code>etcd-copy</code> Job is also deleted by <code>etcd-druid</code>.</p></li><li><p>The additional &ldquo;source&rdquo; backup entry referencing the source seed backup container is deleted from the Garden cluster and the destination seed. As a result, its corresponding <code>source-etcd-backup</code> secret is also deleted from the destination seed.</p></li><li><p>From this point, the reconciliation flow proceeds as already described in <a href=/docs/gardener/proposals/07-shoot-control-plane-migration/>GEP-7</a>. This is safe, since the source seed cluster is no longer able to interfere with the shoot.</p></li></ol><h2 id=handling-inability-to-access-the-backup-container>Handling Inability to Access the Backup Container</h2><p>The mechanism described above assumes that the <code>etcd-backup-restore</code> process in the source seed is able to access its backup container in order to take snapshots. If this is not the case, but an ownership change was detected, the <code>etcd-backup-restore</code> process still sets the readiness probe status of the main <code>etcd</code> container to 503, and kills the main <code>etcd</code> process as described above to ensure that any open connections from <code>kube-apiserver</code> are terminated. This effectively deactivates the source seed control plane to ensure that the ownership of the shoot can be passed to a different seed.</p><p>Because of this, <code>etcd-backup-restore</code> process in the destination seed responsible for copying the snapshots can avoid waiting forever for a final full snapshot to appear. Instead, after a certain timeout has elapsed, it can proceed with the copying. In this situation, whatever latest snapshot is found in the source backup container will be restored in the destination seed. The shoot is still migrated to a healthy seed at the cost of losing the etcd data that accumulated between the point in time when the connection to the source backup container was lost, and the point in time when the source seed cluster was deactivated.</p><p>When the connection to the backup container is restored in the source seed, a final full snapshot will be eventually taken. Depending on the stage of the restoration flow in the destination seed, this snapshot may be copied to the destination seed and restored, or it may simply be ignored since the snapshots have already been copied.</p><h2 id=handling-inability-to-resolve-the-owner-dns-record>Handling Inability to Resolve the Owner DNS Record</h2><p>The situation when the owner DNS record cannot be resolved is treated similarly to a failed ownership check: the <code>etcd-backup-restore</code> process sets the readiness probe status of the main <code>etcd</code> container to 503, and kills the main <code>etcd</code> process as described above to ensure that any open connections from <code>kube-apiserver</code> are terminated, effectively deactivating the source seed control plane. The final full snapshot is not taken in this case to ensure that the control plane can be re-activated if needed.</p><p>When the owner DNS record can be resolved again, the following 2 situations are possible:</p><ul><li>If the source seed is still the owner of the shoot, the <code>etcd-backup-restore</code> process will set the readiness probe status of the main <code>etcd</code> container to 200, so <code>kube-apiserver</code> will be able to connect to <code>etcd</code> and the source seed control plane will be activated again.</li><li>If the source seed is no longer the owner of the shoot, the etcd readiness probe will continue to fail, and the source seed control plane will remain inactive. In addition, the final full snapshot will be taken at this time, for the same reason as described in <a href=#handling-inability-to-access-the-backup-container>Handling Inability to Access the Backup Container</a>.</li></ul><p><strong>Note:</strong> We expect that actual DNS outages are extremely unlikely. A more likely reason for an inability to resolve a DNS record could be network issues with the underlying infrastructure. In such cases, the shoot would usually not be usable / reachable anyway, so deactivating its control plane would not cause a worse outage.</p><h2 id=migration-flow-adaptations>Migration Flow Adaptations</h2><p>Certain changes to the migration flow are needed in order to ensure that it is compatible with the <a href=#owner-election--copying-snapshots>owner election</a> mechanism described above. Instead of taking a full snapshot of the source seed etcd, the flow deletes the owner DNS record by deleting the <code>owner</code> DNSRecord resource. This causes the ownership check by <code>etcd-backup-restore</code> to fail, and the final full snapshot to be eventually taken, so the migration flow waits for a final full snapshot to appear as the last step before deleting the shoot namespace in the source seed. This ensures that the reconciliation flow described above will find a final full snapshot waiting to be copied at step 6.</p><p>Checking for the final full snapshot is performed by calling the already existing <code>etcd-backup-restore</code> endpoint <code>snapshot/latest</code>. This is possible, since the <code>backup-restore</code> container is always running at this point.</p><p>After the final full snapshot has been taken, the readiness probe of the main <code>etcd</code> container starts failing, which means that if the migration flow is retried due to an error it must skip the step that waits for <code>etcd-main</code> to become ready. To determine if this is the case, a check whether the final full snapshot has been taken or not is performed by calling the same <code>etcd-backup-restore</code> endpoint, e.g. <code>snapshot/latest</code>. This is possible if the <code>etcd-main</code> Etcd resource exists with non-zero replicas. Otherwise:</p><ul><li>If the resource doesn&rsquo;t exist, it must have been already deleted, so the final full snapshot n must have been already taken.</li><li>If it exists with zero replicas, the shoot must be hibernated, and the migration flow must have never been executed (since it scales up etcd as one of its first steps), so the final full snapshot must not have been taken yet.</li></ul><h2 id=extension-controller-watchdogs>Extension Controller Watchdogs</h2><p>Some extension controllers will stop reconciling shoot resources after the connection to the shoot&rsquo;s <code>kube-apiserver</code> is lost. Others, most notably the infrastructure controller, will not be affected. Even though new shoot reconciliations won&rsquo;t be performed by <code>gardenlet</code>, such extension controllers might be stuck in a retry loop triggered by a previous reconciliation, which may cause them to reconcile their resources after <code>gardenlet</code> has already stopped reconciling the shoot. In addition, a reconciliation started when the seed still owned the shoot might take some time and therefore might still be running after the ownership has changed. To ensure that the source seed is completely deactivated, an additional safety mechanism is needed.</p><p>This mechanism should handle the following interesting cases:</p><ul><li><code>gardenlet</code> cannot connect to the Garden <code>kube-apiserver</code>. In this case it cannot fetch shoots and therefore does not know if control plane migration has been triggered. Even though <code>gardenlet</code> will not trigger new reconciliations, extension controllers could still attempt to reconcile their resources if they are stuck a retry loop from a previous reconciliation, and already running reconciliations will not be stopped.</li><li><code>gardenlet</code> cannot connect to the seed&rsquo;s <code>kube-apiserver</code>. In this case <code>gardenlet</code> knows if migration has been triggered, but it will not start shoot migration or reconciliation as it will first check the seed conditions and try to update the <code>Cluster</code> resource, both of which will fail. Extension controllers could still be able to connect to the seed&rsquo;s <code>kube-apiserver</code> (if they are not running where <code>gardenlet</code> is running), and similarly to the previous case, they could still attempt to reconcile their resources.</li><li>The seed components (<code>etcd-druid</code>, extension controllers, etc) cannot connect to the seed&rsquo;s <code>kube-apiserver</code>. In this case extension controllers would not be able to reconcile their resources as they cannot fetch them from the seed&rsquo;s <code>kube-apiserver</code>. When the connection to the <code>kube-apiserver</code> comes back, the controllers might be stuck in a retry loop from a previous reconciliation, or the resources could still be annotated with <code>gardener.cloud/operation=reconcile</code>. This could lead to a race condition depending on who manages to <code>update</code> or <code>get</code> the resources first. If <code>gardenlet</code> manages to update the resources before they are read by the extension controllers, they would be properly updated with <code>gardener.cloud/operation=migrate</code>. Otherwise, they would be reconciled as usual.</li></ul><p><strong>Note:</strong> A previous revision of this document proposed using &ldquo;cluster leases&rdquo; as such an additional safety mechanism. With the introduction of owner DNS records cluster leases are no longer needed.</p><p>The safety mechanism is based on <em>extension controller watchdogs</em>. These are simply additional goroutines that are started when a reconciliation is started by an extension controller. These goroutines perform an ownership check on a regular basis using the owner DNS record, similar to the check performed by the <code>etcd-backup-restore</code> process described above. If the check fails, the watchdog cancels the reconciliation context, which immediately aborts the reconciliation.</p><p><strong>Note:</strong> The <code>dns-external</code> extension controller is the only extension controller that neither needs the shoot&rsquo;s <code>kube-apiserver</code>, nor uses the watchdog mechanism described here. Therefore, this controller will continue reconciling <code>DNSEntry</code> resources even after the source seed has lost the ownership of the shoot. With the PoC, we manually delete the <code>DNSOwner</code> resources from the source seed cluster to prevent this from happening. Eventually, the <code>dns-external</code> controller should be adapted to use the owner DNS records to ensure that it disables itself after the seed has lost the ownership of the shoot. Changes in this direction have already been agreed and relevant PRs proposed.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-790545861fe7f3b616f4d087f63febbb>14 - Bastion Management and SSH Key Pair Rotation</h1><h1 id=gep-15-bastion-management-and-ssh-key-pair-rotation>GEP-15: Bastion Management and SSH Key Pair Rotation</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#motivation>Motivation</a><ul><li><a href=#goals>Goals</a></li><li><a href=#non-goals>Non-Goals</a></li></ul></li><li><a href=#proposal>Proposal</a><ul><li><a href=#involved-components>Involved Components</a></li><li><a href=#ssh-flow>SSH Flow</a></li><li><a href=#resource-example>Resource Example</a></li></ul></li><li><a href=#ssh-key-pair-rotation>SSH Key Pair Rotation</a><ul><li><a href=#rotation-proposal>Rotation Proposal</a></li></ul></li></ul><h2 id=motivation>Motivation</h2><p><code>gardenctl</code> (v1) has the functionality to setup <code>ssh</code> sessions to the targeted shoot cluster (nodes). To this end, infrastructure resources like VMs, public IPs, firewall rules, etc. have to be created. <code>gardenctl</code> will clean up the resources after termination of the <code>ssh</code> session (or rather when the operator is done with her work). However, there were issues in the past where these infrastructure resources were not properly cleaned up afterwards, e.g. due to some error (no retries either). Hence, the proposal is to have a dedicated controller (for each infrastructure) that manages the infrastructure resources and their cleanup. The current <code>gardenctl</code> also re-used the <code>ssh</code> node credentials for the bastion host. While that&rsquo;s possible, it would be safer to rather use personal or generated <code>ssh</code> key pairs to access the bastion host.
The static shoot-specific <code>ssh</code> key pair should be rotated regularly, e.g. once in the maintenance time window. This also means that we cannot create the node VMs anymore with infrastructure public keys as these cannot be revoked or rotated (e.g. in AWS) without terminating the VM itself.</p><p>Changes to the <code>Bastion</code> resource should only be allowed for controllers on seeds that are responsible for it. This cannot be restricted when using custom resources.
The proposal, as outlined below, suggests to implement the necessary changes in the gardener core components and to adapt the <a href=https://github.com/gardener/gardener/issues/1723>SeedAuthorizer</a> to consider <code>Bastion</code> resources that the Gardener API Server serves.</p><h3 id=goals>Goals</h3><ul><li>Operators can request and will be granted time-limited <code>ssh</code> access to shoot cluster nodes via bastion hosts.</li><li>To that end, requestors must present their public <code>ssh</code> key and only this will be installed into <code>sshd</code> on the bastion hosts.</li><li>The bastion hosts will be firewalled and ingress traffic will be permitted only from the client IP of the requestor. Except for traffic on port 22 to the cluster worker nodes, no egress from the bastion is allowed.</li><li>The actual node <code>ssh</code> private key (resp. key pair) will be rotated by Gardener and access to the nodes is only possible with this constantly rotated key pair and not with the personal one that is used only for the bastion host.</li><li>Bastion host and access is granted only for the extent of this operator request (of course multiple <code>ssh</code> sessions are possible, in parallel or repeatedly, but after &ldquo;the time is up&rdquo;, access is no longer possible).</li><li>By these means (personal public key and allow-listed client IP) nobody else can use (a.k.a. impersonate) the requestor (not even other operators).</li><li>Necessary infrastructure resources for <code>ssh</code> access (such as VMs, public IPs, firewall rules, etc.) are automatically created and also terminated after usage, but at the latest after the above mentioned time span is up.</li></ul><h3 id=non-goals>Non-Goals</h3><ul><li>Node-specific access</li><li>Auditability on operating system level (not only auditing the <code>ssh</code> login, but everything that is done on a node and other respective resources, e.g. by using dedicated operating system users)</li><li>Reuse of temporarily created necessary infrastructure resources by different users</li></ul><h2 id=proposal>Proposal</h2><h3 id=involved-components>Involved Components</h3><p>The following is a list of involved components, that either need to be newly introduced or extended if already existing</p><ul><li>Gardener API Server (<code>GAPI</code>)<ul><li>New <code>operations.gardener.cloud</code> API Group</li><li>New resource type <code>Bastion</code>, see <a href=#resource-example>resource example</a> below</li><li>New Admission Webhooks for <code>Bastion</code> resource</li><li><code>SeedAuthorizer</code>: The <code>SeedAuthorizer</code> and dependency graph needs to be extended to consider the <code>Bastion</code> resource <a href=https://github.com/gardener/gardener/tree/master/pkg/admissioncontroller/webhooks/auth/seed/graph>https://github.com/gardener/gardener/tree/master/pkg/admissioncontroller/webhooks/auth/seed/graph</a></li><li>Is configured with <code>timeToLive</code>, the time to add to the current time on each heartbeat</li></ul></li><li><code>gardenlet</code><ul><li>Deploys <code>Bastion</code> CRD under the <code>extensions.gardener.cloud</code> API Group to the Seed, see <a href=#resource-example>resource example</a> below</li><li>Similar to <code>BackupBucket</code>s or <code>BackupEntry</code>, the <code>gardenlet</code> watches the <code>Bastion</code> resource in the garden cluster and creates a seed-local <code>Bastion</code> resource, on which the provider specific bastion controller acts upon</li></ul></li><li><code>gardenctlv2</code> (or any other client)<ul><li>Creates <code>Bastion</code> resource in the garden cluster</li><li>Establishes an <code>ssh</code> connection to a shoot node, using a bastion host as proxy</li><li>Heartbeats / keeps alive the <code>Bastion</code> resource during <code>ssh</code> connection</li></ul></li><li>Gardener extension provider <infra><ul><li>Provider specific bastion controller</li><li>Should be added to gardener-extension-provider-<infra> repos, e.g. <a href=https://github.com/gardener/gardener-extension-provider-aws/tree/master/pkg/controller>https://github.com/gardener/gardener-extension-provider-aws/tree/master/pkg/controller</a></li><li>Has the permission to update the <code>Bastion/status</code> subresource on the seed cluster</li><li>Runs on seed (of course)</li></ul></li><li>Gardener Controller Manager (<code>GCM</code>)<ul><li><code>Bastion</code> heartbeat controller<ul><li>Cleans up <code>Bastion</code> resource on missing heartbeat.</li><li>Is configured with a <code>maxLifetime</code> for the <code>Bastion</code> resource</li></ul></li></ul></li><li>Gardener (RBAC)<ul><li>The project <code>admin</code> role should be extended to allow CRUD operations on the <code>Bastion</code> resource. The <code>gardener.cloud:system:project-member-aggregation</code> <code>ClusterRole</code> needs to be updated accordingly (<a href=https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/charts/application/templates/rbac-user.yaml>https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/charts/application/templates/rbac-user.yaml</a>)</li></ul></li></ul><h3 id=ssh-flow>SSH Flow</h3><ol start=0><li>Users should only get the RBAC permission to <code>create</code> / <code>update</code> <code>Bastion</code> resources for a namespace, if they should be allowed to <code>ssh</code> onto the shoot nodes in this namespace. A project member with <code>admin</code> role will have these permissions.</li><li>User/<code>gardenctlv2</code> creates <code>Bastion</code> resource in garden cluster (see <a href=#resource-example>resource example</a> below)<ul><li>First, gardenctl would figure out the own public IP of the user&rsquo;s machine. Either by calling an external service (gardenctl (v1) uses <a href=https://github.com/gardener/gardenctl/blob/master/pkg/cmd/miscellaneous.go#L226>https://github.com/gardener/gardenctl/blob/master/pkg/cmd/miscellaneous.go#L226</a>) or by calling a binary that prints the public IP(s) to stdout. The binary should be configurable. The result is set under <code>spec.ingress[].ipBlock.cidr</code></li><li>Creates new <code>ssh</code> key pair. The newly created key pair is used only once for each bastion host, so it has a 1:1 relationship to it. It is cleaned up after it is not used anymore, e.g. if the <code>Bastion</code> resource was deleted.</li><li>The public <code>ssh</code> key is set under <code>spec.sshPublicKey</code></li><li>The targeted shoot is set under <code>spec.shootRef</code></li></ul></li><li>GAPI Admission Plugin for the <code>Bastion</code> resource in the garden cluster<ul><li>on creation, sets <code>metadata.annotations["gardener.cloud/created-by"]</code> according to the user that created the resource</li><li>when <code>gardener.cloud/operation: keepalive</code> is set it will be removed by GAPI from the annotations and <code>status.lastHeartbeatTimestamp</code> will be set with the current timestamp. The <code>status.expirationTimestamp</code> will be calculated by taking the last heartbeat timestamp and adding <code>x</code> minutes (configurable, default <code>60</code> Minutes).</li><li>validates that only the creator of the bastion (see <code>gardener.cloud/created-by</code> annotation) can update <code>spec.ingress</code></li><li>validates that a Bastion can only be created for a Shoot if that Shoot is already assigned to a Seed</li><li>sets <code>spec.seedName</code> and <code>spec.providerType</code> based on the <code>spec.shootRef</code></li></ul></li><li><code>gardenlet</code><ul><li>Watches <code>Bastion</code> resource for own seed under api group <code>operations.gardener.cloud</code> in the garden cluster</li><li>Creates <code>Bastion</code> custom resource under api group <code>extensions.gardener.cloud/v1alpha1</code> in the seed cluster<ul><li>Populates bastion user data under field under <code>spec.userData</code> similar to <a href=https://github.com/gardener/gardenctl/blob/1e3e5fa1d5603e2161f45046ba7c6b5b4107369e/pkg/cmd/ssh.go#L160-L171>https://github.com/gardener/gardenctl/blob/1e3e5fa1d5603e2161f45046ba7c6b5b4107369e/pkg/cmd/ssh.go#L160-L171</a>. By this means the <code>spec.sshPublicKey</code> from the <code>Bastion</code> resource in the garden cluster will end up in the <code>authorized_keys</code> file on the bastion host.</li></ul></li></ul></li><li>Gardener extension provider <infra>/ Bastion Controller on Seed:<ul><li>With own <code>Bastion</code> Custom Resource Definition in the seed under the api group <code>extensions.gardener.cloud/v1alpha1</code></li><li>Watches <code>Bastion</code> custom resources that are created by the <code>gardenlet</code> in the seed</li><li>Controller reads <code>cloudprovider</code> credentials from seed-shoot namespace</li><li>Deploy infrastructure resources<ul><li>Bastion VM. Uses user data from <code>spec.userData</code></li><li>attaches public IP, creates security group, firewall rules, etc.</li></ul></li><li>Updates status of <code>Bastion</code> resource:<ul><li>With bastion IP under <code>status.ingress.ip</code> or hostname under <code>status.ingress.hostname</code></li><li>Updates the <code>status.lastOperation</code> with the status of the last reconcile operation</li></ul></li></ul></li><li><code>gardenlet</code><ul><li>Syncs back the <code>status.ingress</code> and <code>status.conditions</code> of the <code>Bastion</code> resource in the seed to the garden cluster in case it changed</li></ul></li><li><code>gardenctl</code><ul><li>initiates <code>ssh</code> session once <code>status.conditions['BastionReady']</code> is true of the <code>Bastion</code> resource in the garden cluster<ul><li>locates private <code>ssh</code> key matching <code>spec["sshPublicKey"]</code> which was configured beforehand by the user</li><li>reads bastion IP (<code>status.ingress.ip</code>) or hostname (<code>status.ingress.hostname</code>)</li><li>reads the private key from the <code>ssh</code> key pair for the shoot node</li><li>opens <code>ssh</code> connection to the bastion and from there to the respective shoot node</li></ul></li><li>runs heartbeat in parallel as long as the <code>ssh</code> session is open by annotating the <code>Bastion</code> resource with <code>gardener.cloud/operation: keepalive</code></li></ul></li><li><code>GCM</code>:<ul><li>Once <code>status.expirationTimestamp</code> is reached, the <code>Bastion</code> will be marked for deletion</li></ul></li><li><code>gardenlet</code>:<ul><li>Once the <code>Bastion</code> resource in the garden cluster is marked for deletion, it marks the <code>Bastion</code> resource in the seed for deletion</li></ul></li><li>Gardener extension provider <infra>/ Bastion Controller on Seed:<ul><li>all created resources will be cleaned up</li><li>On succes, removes finalizer on <code>Bastion</code> resource in seed</li></ul></li><li><code>gardenlet</code>:<ul><li>removes finalizer on <code>Bastion</code> resource in garden cluster</li></ul></li></ol><h3 id=resource-example>Resource Example</h3><p><code>Bastion</code> resource in the garden cluster</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: operations.gardener.cloud/v1alpha1
kind: Bastion
metadata:
  generateName: cli-
  name: cli-abcdef
  namespace: garden-myproject
  annotations:
    gardener.cloud/created-by: foo <span style=color:green># immutable, set by the GAPI Admission Plugin</span>
    <span style=color:green># gardener.cloud/operation: keepalive # this annotation is removed by the GAPI and the status.lastHeartbeatTimestamp and status.expirationTimestamp will be updated accordingly</span>
spec:
  shootRef: <span style=color:green># namespace cannot be set / it&#39;s the same as .metadata.namespace</span>
    name: my-cluster <span style=color:green># immutable</span>

  <span style=color:green># the following fields are set by the GAPI</span>
  seedName: aws-eu2
  providerType: aws

  sshPublicKey: c3NoLXJzYSAuLi4K <span style=color:green># immutable, public `ssh` key of the user</span>

  ingress: <span style=color:green># can only be updated by the creator of the bastion</span>
  - ipBlock:
      cidr: 1.2.3.4/32 <span style=color:green># public IP of the user. CIDR is a string representing the IP Block. Valid examples are &#34;192.168.1.1/24&#34; or &#34;2001:db9::/64&#34;</span>

status:
  observedGeneration: 1

  <span style=color:green># the following fields are managed by the controller in the seed and synced by gardenlet</span>
  ingress: <span style=color:green># IP or hostname of the bastion</span>
    ip: 1.2.3.5
    <span style=color:green># hostname: foo.bar</span>

  conditions:
  - type: BastionReady <span style=color:green># when the `status` is true of condition type `BastionReady`, the client can initiate the `ssh` connection</span>
    status: <span style=color:#a31515>&#39;True&#39;</span>
    lastTransitionTime: <span style=color:#a31515>&#34;2021-03-19T11:59:00Z&#34;</span>
    lastUpdateTime: <span style=color:#a31515>&#34;2021-03-19T11:59:00Z&#34;</span>
    reason: BastionReady
    message: Bastion for the cluster is ready.

  <span style=color:green># the following fields are only set by the GAPI</span>
  lastHeartbeatTimestamp: <span style=color:#a31515>&#34;2021-03-19T11:58:00Z&#34;</span> <span style=color:green># will be set when setting the annotation gardener.cloud/operation: keepalive</span>
  expirationTimestamp: <span style=color:#a31515>&#34;2021-03-19T12:58:00Z&#34;</span> <span style=color:green># extended on each keepalive</span>
</code></pre></div><p><code>Bastion</code> custom resource in the seed cluster</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: extensions.gardener.cloud/v1alpha1
kind: Bastion
metadata:
  name: cli-abcdef
  namespace: shoot--myproject--mycluster
spec:
  userData: |- <span style=color:green># this is normally base64-encoded, but decoded for the example. Contains spec.sshPublicKey from Bastion resource in garden cluster</span>
    <span style=color:green>#!/bin/bash</span>
    <span style=color:green># create user</span>
    <span style=color:green># add ssh public key to authorized_keys</span>
    <span style=color:green># ...</span>

  ingress:
  - ipBlock:
      cidr: 1.2.3.4/32

  type: aws <span style=color:green># from extensionsv1alpha1.DefaultSpec</span>

status:
  observedGeneration: 1
  ingress:
    ip: 1.2.3.5
    <span style=color:green># hostname: foo.bar</span>
  conditions:
  - type: BastionReady
    status: <span style=color:#a31515>&#39;True&#39;</span>
    lastTransitionTime: <span style=color:#a31515>&#34;2021-03-19T11:59:00Z&#34;</span>
    lastUpdateTime: <span style=color:#a31515>&#34;2021-03-19T11:59:00Z&#34;</span>
    reason: BastionReady
    message: Bastion for the cluster is ready.
</code></pre></div><h2 id=ssh-key-pair-rotation>SSH Key Pair Rotation</h2><p>Currently, the <code>ssh</code> key pair for the shoot nodes are created once during shoot cluster creation. These key pairs should be rotated on a regular basis.</p><h3 id=rotation-proposal>Rotation Proposal</h3><ul><li><code>gardeneruser</code> original user data <a href=https://github.com/gardener/gardener/tree/master/pkg/operation/botanist/component/extensions/operatingsystemconfig/original/components/gardeneruser>component</a>:<ul><li>The <code>gardeneruser</code> create script should be changed into a reconcile script script, and renamed accordingly. It needs to be adapted so that the <code>authorized_keys</code> file will be updated / overwritten with the current and old <code>ssh</code> public key from the cloud-config user data.</li></ul></li><li>Rotation trigger:<ul><li>Once in the maintenance time window</li><li>On demand, by annotating the shoot with <code>gardener.cloud/operation: rotate-ssh-keypair</code></li></ul></li><li>On rotation trigger:<ul><li><code>gardenlet</code><ul><li>Prerequisite of <code>ssh</code> key pair rotation: all nodes of all the worker pools have successfully applied the desired version of their cloud-config user data</li><li>Creates or updates the secret <code>ssh-keypair.old</code> with the content of <code>ssh-keypair</code> in the seed-shoot namespace. The old private key can be used by clients as fallback, in case the new <code>ssh</code> public key is not yet applied on the node</li><li>Generates new <code>ssh-keypair</code> secret</li><li>The <code>OperatingSystemConfig</code> needs to be re-generated and deployed with the new and old <code>ssh</code> public key</li></ul></li><li>As usual (for more details, see <a href=/docs/gardener/extensions/operatingsystemconfig/>here</a>):<ul><li>Once the <code>cloud-config-&lt;X></code> secret in the <code>kube-system</code> namespace of the shoot cluster is updated, it will be picked up by the <a href=https://github.com/gardener/gardener/blob/master/pkg/operation/botanist/component/extensions/operatingsystemconfig/downloader/templates/scripts/download-cloud-config.tpl.sh><code>downloader</code> script</a> (checks every 30s for updates)</li><li>The <code>downloader</code> runs the <a href=https://github.com/gardener/gardener/blob/master/pkg/operation/botanist/component/extensions/operatingsystemconfig/executor/templates/scripts/execute-cloud-config.tpl.sh>&ldquo;execution&rdquo; script</a> from the <code>cloud-config-&lt;X></code> secret</li><li>The &ldquo;execution&rdquo; script includes also the original user data script, which it writes to <code>PATH_CLOUDCONFIG</code>, compares it against the previous cloud config and runs the script in case it has changed</li><li>Running the <a href=https://github.com/gardener/gardener/tree/master/pkg/operation/botanist/component/extensions/operatingsystemconfig/original>original user data</a> script will also run the <code>gardeneruser</code> component, where the <code>authorized_keys</code> file will be updated</li><li>After the most recent cloud-config user data was applied, the &ldquo;execution&rdquo; script annotates the node with <code>checksum/cloud-config-data: &lt;cloud-config-checksum></code> to indicate the success</li></ul></li></ul></li></ul><h3 id=limitations>Limitations</h3><p>Each operating system has its own default user (e.g. <code>core</code>, <code>admin</code>, <code>ec2-user</code> etc). These users get their SSH keys during VM creation (however there is a different handling on Google Cloud Platform as stated below). These keys currently do not get rotated respectively are not removed from the <code>authorized_keys</code> file. This means that the initial <code>ssh</code> key will still be valid for the default operating system user.</p><p>On Google Cloud Platform, the VMs do not have any static users (i.e. no <code>gardener</code> user) and there is an agent on the nodes that syncs the users with their SSH keypairs from the GCP IAM service.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-fb3867149339478f45ee6b887f9089ff>15 - Dynamic kubeconfig generation for Shoot clusters</h1><h1 id=gep-16-dynamic-kubeconfig-generation-for-shoot-clusters>GEP-16: Dynamic kubeconfig generation for Shoot clusters</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#gep-16-dynamic-kubeconfig-generation-for-shoot-clusters>GEP-16: Dynamic kubeconfig generation for Shoot clusters</a><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#summary>Summary</a></li><li><a href=#motivation>Motivation</a><ul><li><a href=#goals>Goals</a></li><li><a href=#non-goals>Non-Goals</a></li></ul></li><li><a href=#proposal>Proposal</a></li><li><a href=#alternatives>Alternatives</a></li></ul></li></ul><h2 id=summary>Summary</h2><p>This <code>GEP</code> introduces new <code>Shoot</code> subresource called <code>AdminKubeconfigRequest</code> allowing for users to dynamically generate a short-lived <code>kubeconfig</code> that can be used to access the <code>Shoot</code> cluster as <code>cluster-admin</code>.</p><h2 id=motivation>Motivation</h2><p>Today, when access to the created <code>Shoot</code> clusters is needed, a <code>kubeconfig</code> with static token credentials is used. This static token is in the <code>system:masters</code> group, granting it <code>cluster-admin</code> privileges. The <code>kubeconfig</code> is generated when the cluster is reconciled, stored in <code>ShootState</code> and replicated in the <code>Project</code>&rsquo;s namespace in a <code>Secret</code>. End-users can fetch the secret and use the <code>kubeconfig</code> inside it.</p><p>There are several problems with this approach:</p><ul><li>The token in the <code>kubeconfig</code> does not have any expiration, so end-users have to request a <code>kubeconfig</code> credential rotation if they want revoke the token.</li><li>There is no user identity in the token. e.g. if user <code>Joe</code> gets the <code>kubeconfig</code> from the <code>Secret</code>, user in that token would be <code>system:cluster-admin</code> and not <code>Joe</code> when accessing the <code>Shoot</code> cluster with it. This makes auditing events in the cluster almost impossible.</li></ul><h3 id=goals>Goals</h3><ul><li><p>Add a <code>Shoot</code> subresource called <code>adminkubeconfig</code> that would produce a <code>kubeconfig</code> used to access that <code>Shoot</code> cluster.</p></li><li><p>The <code>kubeconfig</code> is not stored in the API Server, but generated for each request.</p></li><li><p>In the <code>AdminKubeconfigRequest</code> send to that subresource, end-users can specify the expiration time of the credential.</p></li><li><p>The identity (user) in the Gardener cluster would be part of the identity (x509 client certificate). E.g if <code>Joe</code> authenticates against the Gardener API server, the generated certificate for <code>Shoot</code> authentication would have the following subject:</p><ul><li>Common Name: <code>Joe</code></li><li>Organisation: <code>system:masters</code></li></ul></li><li><p>The maximum validity of the certificate can be enforced by setting a flag on the <code>gardener-apiserver</code>.</p></li><li><p>Deprecate and remove the old <code>{shoot-name}.kubeconfig</code> secrets in each <code>Project</code> namespace.</p></li></ul><h3 id=non-goals>Non-Goals</h3><ul><li>Generate <code>OpenID Connect</code> kubeconfigs</li></ul><h2 id=proposal>Proposal</h2><p>The <code>gardener-apiserver</code> would serve a new <code>shoots/adminkubeconfig</code> resource. It can only accept <code>CREATE</code> calls and accept <code>AdminKubeconfigRequest</code>. A <code>AdminKubeconfigRequest</code> would have the following structure:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: authentication.gardener.cloud/v1alpha1
kind: AdminKubeconfigRequest
spec:
  expirationSeconds: 3600
</code></pre></div><p>Where <code>expirationSeconds</code> is the validity of the certificate in seconds. In this case it would be <code>1 hour</code>. The maximum validity of a <code>AdminKubeconfigRequest</code> is configured by <code>--shoot-admin-kubeconfig-max-expiration</code> flag in the <code>gardener-apiserver</code>.</p><p>When such request is received, the API server would find the <code>ShootState</code> associated with that cluster and generate a <code>kubeconfig</code>. The x509 client certificate would be signed by the <code>Shoot</code> cluster&rsquo;s CA and the user used in the subject&rsquo;s common name would be from the <code>User.Info</code> used to make the request.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: authentication.gardener.cloud/v1alpha1
kind: AdminKubeconfigRequest
spec:
  expirationSeconds: 3600
status:
  expirationTimestamp: <span style=color:#a31515>&#34;2021-02-22T09:06:51Z&#34;</span>
  kubeConfig: <span style=color:green># this is normally base64-encoded, but decoded for the example</span>
    apiVersion: v1
    clusters:
    - cluster:
        certificate-authority-data: LS0tLS1....
        server: https://api.shoot-cluster
      name: shoot-cluster-a
    contexts:
    - context:
        cluster: shoot-cluster-a
        user: shoot-cluster-a
      name: shoot-cluster-a
    current-context: shoot-cluster-a
    kind: Config
    preferences: {}
    users:
    - name: shoot-cluster-a
      user:
        client-certificate-data: LS0tLS1CRUd...
        client-key-data: LS0tLS1CRUd...
</code></pre></div><p>New feature gate called <code>AdminKubeconfigRequest</code> enables the above mentioned API in the <code>gardener-apiserver</code>. The old <code>{shoot-name}.kubeconfig</code> is kept, but deprecated and will be removed in the future.</p><p>In order to get the server&rsquo;s address used in the <code>kubeconfig</code>, the Shoot&rsquo;s <code>status</code> should be updated with new entries:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: Shoot
metadata:
  name: crazy-botany
  namespace: garden-dev
spec: {}
status:
  advertisedAddresses:
  - name: external
    url: https://api.shoot-cluster.external.foo
  - name: internal
    url: https://api.shoot-cluster.internal.foo
  - name: ip
    url: https://1.2.3.4
</code></pre></div><p>This is needed, because the Gardener API server might not know on which IP address the API server is advertised on (e.g. DNS is disabled).</p><p>If there are multiple entries, each would be added in a separate <code>cluster</code> in the <code>kubeconfig</code> and a <code>context</code> with the same name would be added added as well. The current context would be selected as the first entry in the <code>advertisedAddresses</code> list (<code>.status.advertisedAddresses[0]</code>).</p><h2 id=alternatives>Alternatives</h2><ul><li><a href=https://github.com/gardener/oidc-webhook-authenticator>Dynamic OpenID Connect Webhook Authenticator</a> can be used instead. Ideally cluster admins can enable either or both.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-bd5352a75eaa367044f38d5f9e2080ac>16 - GEP Title</h1><h1 id=gep-nnnn-your-short-descriptive-title>GEP-NNNN: Your short, descriptive title</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#summary>Summary</a></li><li><a href=#motivation>Motivation</a><ul><li><a href=#goals>Goals</a></li><li><a href=#non-goals>Non-Goals</a></li></ul></li><li><a href=#proposal>Proposal</a></li><li><a href=#alternatives>Alternatives</a></li></ul><h2 id=summary>Summary</h2><h2 id=motivation>Motivation</h2><h3 id=goals>Goals</h3><h3 id=non-goals>Non-Goals</h3><h2 id=proposal>Proposal</h2><h2 id=alternatives>Alternatives</h2></div><div class=td-content style=page-break-before:always><h1 id=pg-d9890d994643115baf0d880ffe41fc8f>17 - Reversed Cluster VPN</h1><h1 id=gep-14-reversed-cluster-vpn>GEP-14: Reversed Cluster VPN</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#Motivation>Motivation</a></li><li><a href=#Proposal>Proposal</a></li><li><a href=#Alternatives>Alternatives</a></li></ul><h2 id=motivation>Motivation</h2><p>It is necessary to describe the current VPN solution and outline its shortcomings in order to motivate this proposal.</p><h3 id=problem-statement>Problem Statement</h3><p>Today&rsquo;s Gardener cluster VPN solution has several issues including:</p><ol><li>Connection establishment is always from the seed cluster to the shoot cluster. This means that there needs to be connectivity both ways which is not desirable in many cases (OpenStack, VMware) and causes high effort in firewall configuration or extra infrastructure. These firewall configurations are prohibited in some cases due to security policies.</li><li>Shoot clusters must provide a VPN endpoint. This means extra cost for the endpoint (roughly €20/month on hyperscalers) or will consume scarce resources (limited number of VMware NSX-T load balancers).</li></ol><p>A first implementation has been provided to resolve the issues with the <a href=/docs/gardener/proposals/11-apiserver-network-proxy/>konnectivity server</a>. As we did find several shortcomings with the underlying technology component, the <a href=https://github.com/kubernetes-sigs/apiserver-network-proxy>apiserver-network-proxy</a> we believe that this is not a suitable way ahead. We have opened an <a href=https://github.com/kubernetes-sigs/apiserver-network-proxy/issues/180>issue</a> and provided two solution proposals to the community. We do see some remedies, e.g. using the <a href=https://de.wikipedia.org/wiki/Quick_UDP_Internet_Connections>Quick Protocol</a> instead of GRPC but we (a) consider the implementation effort significantly higher compared to this proposal and (b) would use an experimental protocol to solve a problem that can also be solved with existing and proven core network technologies.</p><p>We will therefore not continue to invest into this approach. We will however research a similar approach (see below in &ldquo;Further Research&rdquo;).</p><h3 id=current-solution-outline>Current Solution Outline</h3><p>The current solution consists of multiple VPN connections from each API server pod and the Prometheus pod of a control plane to an OpenVPN server running in the shoot cluster. This OpenVPN server is exposed via a load balancer that must have an IP address which is reachable from the seed cluster. The routing in the seed cluster pods is configured to route all traffic for the node, pod, and service ranges to the shoot cluster. This means that there is no address overlap allowed between seed- and shoot cluster node, pod, and service ranges.</p><p>In the seed cluster the <code>vpn-seed</code> container is a sidecar to the kube-apiserver and prometheus pods. OpenVPN acts as a TCP client connecting to an OpenVPN TCP server. This is not optimal (e.g. tunneling TCP over TCP is discouraged) but at the time of development there was no UDP load balancer available on at least one of the major hyperscalers. Connectivity could have been switched to UDP later but the development effort was not spent.</p><p>The solution is depicted in this diagram:</p><p><img src=/__resources/CurrentClusterVPN_095870.png alt="alt text" title="Overview Current Cluster VPN"></p><p>These are the essential parts of the OpenVPN client configuration in the <code>vpn-seed</code> sidecar container:</p><pre><code># use TCP instead of UDP (commonly not supported by load balancers)
proto tcp-client

[...]

# get all routing information from server
pull

tls-client
key &quot;/srv/secrets/vpn-seed/tls.key&quot;
cert &quot;/srv/secrets/vpn-seed/tls.crt&quot;
ca &quot;/srv/secrets/vpn-seed/ca.crt&quot;

tls-auth &quot;/srv/secrets/tlsauth/vpn.tlsauth&quot; 1
cipher AES-256-CBC

# https://openvpn.net/index.php/open-source/documentation/howto.html#mitm
remote-cert-tls server

# pull filter
pull-filter accept &quot;route 100.64.0.0 255.248.0.0&quot;
pull-filter accept &quot;route 100.96.0.0 255.224.0.0&quot;
pull-filter accept &quot;route 10.1.60.0 255.255.252.0&quot;
pull-filter accept &quot;route 192.168.123.&quot;
pull-filter ignore &quot;route&quot;
pull-filter ignore redirect-gateway
pull-filter ignore route-ipv6
pull-filter ignore redirect-gateway-ipv6
</code></pre><p>Encryption is based on SSL certificates with an additional HMAC signature to all SSL/TLS handshake packets. As multiple clients connect to the OpenVPN server in the shoot cluster, all clients must be assigned a unique IP address. This is done by the OpenVPN server pushing that configuration to the client (keyword <code>pull</code>). As this is potentially problematic because the OpenVPN server runs in an untrusted environment there are pull filters denying all but necessary routes for the pod, service, and node networks.</p><p>The OpenVPN server running in the shoot cluster is configured as follows:</p><pre><code>mode server
tls-server
proto tcp4-server
dev tun0

[...]

server 192.168.123.0 255.255.255.0

push &quot;route 10.243.0.0 255.255.128.0&quot;
push &quot;route 10.243.128.0 255.255.128.0&quot;

duplicate-cn

key &quot;/srv/secrets/vpn-shoot/tls.key&quot;
cert &quot;/srv/secrets/vpn-shoot/tls.crt&quot;
ca &quot;/srv/secrets/vpn-shoot/ca.crt&quot;
dh &quot;/srv/secrets/dh/dh2048.pem&quot;

tls-auth &quot;/srv/secrets/tlsauth/vpn.tlsauth&quot; 0
push &quot;route 10.242.0.0 255.255.0.0&quot;
</code></pre><p>It is a TCP TLS server and configured to automatically assign IP addresses for OpenVPN clients (<code>server</code> directive). In addition, it pushes the shoot cluster node-, pod-, and service ranges to the clients running in the seed cluster (<code>push</code> directive).</p><p><strong>Note:</strong> The network mesh spanned by OpenVPN uses the network range <code>192.168.123.0 - 192.168.123.255</code>. This network range cannot be used in either shoot-, or seed clusters. If it is used this might cause subtle problem due to network range overlaps. Unfortunately, this appears not to be well documented but this restriction exists since the very beginning. We should clean up this technical debt as part of the exercise.</p><h3 id=goals>Goals</h3><ul><li>We intend to supersede the current VPN solution with the solution outlined in this proposal.</li><li>We intend to remove the code for the konnectivity tunnel once this solution proposal has been validated.</li></ul><h3 id=non-goals>Non Goals</h3><ul><li>The solution is not a low latency, or high throughput solution. As the kube-apiserver to shoot cluster traffic does not demand these properties we do not intend to invest in improvements.</li><li>We do not intend to provide continuous availability to the shoot-seed VPN connection. We expect the availability to be comparable to the existing solution.</li></ul><h2 id=proposal>Proposal</h2><p>The proposal is depicted in the following diagram:</p><p><img src=/__resources/ReversedTunnelVPN_c0c514.png alt="alt text" title="Reversed Tunnel VPN"></p><p>We have added an OpenVPN server pod (<code>vpn-seed-server</code>) to each control plane. The OpenVPN client in the shoot cluster (<code>vpn-shoot-client</code>) connects to the OpenVPN server.</p><p>The two containers <code>vpn-seed-server</code> and <code>vpn-shoot-client</code> are new containers and are not related to containers in the github.com/gardener/vpn project. We will create a new project github.com/gardener/vpn2 for these containers. With this solution we intend to supersede the containers from the github.com/gardener/vpn project.</p><p>A service <code>vpn-seed-server</code> of type <code>ClusterIP</code> is created for each control plane in its namespace.</p><p>The <code>vpn-shoot-client</code> pod connects to the correct <code>vpn-seed-server</code> service via the SNI passthrough proxy introduced with <a href=/docs/gardener/proposals/08-shoot-apiserver-via-sni/>SNI Passthrough proxy for kube-apiservers</a> on port 8132.</p><p>Shoot OpenVPN clients (<code>vpn-shoot-client</code>) connect to the correct OpenVPN Server using the http proxy feature provided by OpenVPN. A configuration is added to the envoy proxy to detect http proxy requests and open a connection attempt to the correct OpenVPN server.</p><p>The <code>kube-apiserver</code> to shoot cluster connections are established using the API server proxy feature via an envoy proxy sidecar container of the <code>vpn-seed-server</code> container.</p><p>The restriction regarding the <code>192.168.123.0/24</code> network range in the current VPN solution still applies to this proposal. No other restrictions are introduced. In the context of this GEP a pull requst has been filed to block usage of that range by shoot clusters.</p><h3 id=performance-and-scalability>Performance and Scalability</h3><p>We do expect performance and throughput to be slightly lower compared to the existing solution. This is because the OpenVPN server acts as an additional hop and must decrypt and re-encrypt traffic that passes through. As there are no low latency, or high thoughput requirements for this connection we do not assume this to be an issue.</p><h3 id=availability-and-failure-scenarios>Availability and Failure Scenarios</h3><p>This solution re-uses multiple instances of the envoy component used for the kube-apiserver endpoints. We assume that the availability for kube-apiservers is good enough for the cluster VPN as well.</p><p>The OpenVPN client- and server pods are singleton pods in this approach and therefore are affected by potential failures and during cluster-, and control plane updates. Potential outages are only restricted to single shoot clusters and are comparable to the situation with the existing solution today.</p><h3 id=feature-gates-and-migration-strategy>Feature Gates and Migration Strategy</h3><p>We have introduced a gardenlet feature gate <code>ReversedVPN</code>. If <code>APIServerSNI</code> and <code>ReversedVPN</code> are enabled the proposed solution is automatically enabled for all shoot clusters hosted by the seed. If <code>ReversedVPN</code> is enabled but <code>APIServerSNI</code> is not the gardenlet will panic during startup as this is an invalid configuration. All existing shoot clusters will automatically be migrated during the next reconciliation. We assume that the <code>ReversedVPN</code> feature will work with Gardener as well as operator managed Istio.</p><p>We have also added a shoot annotation <code>alpha.featuregates.shoot.gardener.cloud/reversed-vpn</code> which can override the feature gate to enable or disable the solution for individual clusters. This is only respected if <code>APIServerSNI</code> is enabled, otherwise it is ignored.</p><h3 id=security-review>Security Review</h3><p>The change in the VPN solution will potentially open up new attack vectors. We will perform a thorough analysis outside of this document.</p><h2 id=alternatives>Alternatives</h2><h3 id=wireguard-and-kubelink-based-cluster-vpn>WireGuard and Kubelink based Cluster VPN</h3><p>We have done a detailed investigation and implementation of a reversed VPN based on WireGuard. While we believe that it is technically feasible and superior to the approach presented above there are some concerns with regards to scalability, and high availability. As the WireGuard scenario based on kubelink is relevant for other use cases we continue to improve this implementation and address the concerns but we concede that this might not be on time for the cluster VPN. We nevertheless keep the implementation and provide an outline as part of this proposal.</p><p>The general idea of the proposal is to keep the existing cluster VPN solution more or less as is, but change the underlying network used for the <code>vpn seed => vpn shoot</code> connection. The underlying network should be established in the reversed direction, i.e. the shoot cluster should initiate the network connection, but it nevertheless should work in both directions.</p><p>We achieve this by tunneling the open vpn connection through a WireGuard tunnel, which is established from the shoot to the seed (note that WireGuard uses UDP as protocol). Independent of that we can also use UDP for the OpenVPN connection, but we can also stay with TCP as it was before. While this might look like a big change, it only introduces minor changes to the existing solution, but let&rsquo;s look at the details. In essence, the OpenVPN connection does not require a public endpoint in the shoot cluster but it usees the internal endpoint provided by the WireGuard tunnel.</p><p>This is roughly depcited in this diagram. Note, that the <code>vpn-seed</code> and <code>vpn-shoot</code> containers only require very little changes and are fully backwards compatible.</p><p><img src=/__resources/WireGuardClusterVPN_ff18dd.png alt="alt text" title="Overview WireGuard Current Cluster VPN"></p><p>The WireGuard network needs a separate network range/CIDR. It has to be unique for the seed and all its shoot clusters. An example for an assumed workload of around 1000 shoot clusters would be <code>192.168.128.0/22</code> (1024 IP addresses), i.e. <code>192.168.128.0-192.168.131.255</code>. The IP addresses from this range need to be managed, but the IP address management (IPAM) using the Gardener Kubernetes objects like seed and shootstate as backing store is fairly straightforward. This is especially true as we do not expect large network ranges and only infrequent IP allocations. Hence, the IP address allocation can be quite simple, i.e. scan the range for a free IP address of all shoot clusters in a seed and allocate the first free address from the range.</p><p>There is another restriction: in case shoot clusters are configured to be seed clusters this network range must not overlap with the &ldquo;parent&rdquo; seed cluster. If the parent seed cluster uses <code>192.168.128.0/22</code> the child seed cluster can for example use <code>192.168.132.0/22</code>. Grandchildren can however use grandparent IP address ranges. Also 2 children seed clusters can use identical ranges.</p><p>This slightly adds to the restrictions described in the current solution outline. In that the arbitrary chosen <code>192.168.123.0/24</code> range is restricted. For the purpose of this implementation we propose to extend that restriction to <code>192.168.128.0/17</code> range. Most of it would be reserved for &ldquo;future use&rdquo; however. We are well aware that this adds to the burden of correctly configuring Gardener landscapes.</p><p>We do consider this to be a challenge that needs to be addressed by careful configuration of the Gardener seed cluster infrastructure. Together with the <code>192.168.123.0/24</code> address range these ranges should be automatically blocked for usage by shoots.</p><p>WireGuard can utilize the Linux kernel so that after initialization/configuration no user space processes are required. We propose to recommend the WireGuard kernel module as the default solution for all seeds. For shoot clusters, the WireGuard kernel based approach is also recommended, but the user space solution should also work as we expect less traffic on the shoot side. We expect the userspace implementation to work on all operating systems supported by Gardener in case no kernel module is available.</p><p>Almost all seed clusters are already managed by Gardener and we assume that those are configured with the WireGuard kernel module. There are however some cases where we use other Kubernetes distributions as seed cluster which may not have an operating system with WireGuard module available. We will therefore generally support the user space WireGuard process on seed cluster but place a size restriction on the number of control planes on those seeds.</p><p>There is a user space implementation of WireGuard, which can be used on Linux distributions without the WireGuard kernel module. (WireGuard moved into the standard Linux kernel 5.6.) Our proposal can handle the kernel/user space switch transparently, i.e. we include the user space binaries and use them only when required. However, especially for the seed the kernel based solution might be more attractive. Garden Linux 318.4.0 supports WireGuard.</p><p>We have looked at Ubuntu and SuSE chost:</p><ul><li>SuSE chost does not provide the WireGuard kernel module and it is not installable via zypper. It should however be straightforward for SuSE to include that in their next release.</li><li>Ubuntu does not provide the kernel module either but it can be installed using <code>apt-get install wireguard</code>. With that it appears straightforward to provide an image with WireGuard pre-installed.</li></ul><p>On the seed, we add a WireGuard device to one node on the host network. For all other nodes on the seed, we adapt the routes accordingly to route traffic destined for the WireGuard network to our WireGuard node. The Kubernetes pods managing the WireGuard device and routes are only used for initial configuration and later reconfiguration. During runtime, they can restart without any impact on the operation of the WireGuard network as the WireGuard device is managed by the Linux kernel.</p><p>With Calico as the networking solution it is not easily possible to put the WireGuard endpoint into a pod. Putting the WireGuard endpoint into a pod would require to define it as a gateway in the api server or prometheus pods but this is not possible since Calico does not span a proper subnet. While the defined CIDR in the pod network might be <code>100.96.0.0/11</code> the network visible from within a pod is only <code>100.96.0.5/32</code>. This restriction might not exist with other networking solutions.</p><p>The WireGuard endpoint on the seed is exposed via a load balancer. We propose to use <a href=https://github.com/mandelsoft/kubelink>kubelink</a> to manage the WireGuard configuration/device on the seed. We consider the management of the WireGuard endpoint to be complex especially in error situations which is the reason for utilizing kubelink as there is already significant experience managing an endpoint. We propose moving kubelink to the Gardener org in case it is used by this proposal.</p><p>Kubelink addresses three challenges managing WireGuard interfaces on cluster nodes. First, with WireGuard interfaces directly on the node (<code>hostNetwork=true</code>) the lifecycle of the interface is decoupled from the lifecycle of the pod that created it. This means that there will have to be means of cleaning up the interfaces and its configuration in case the interface moves to a different node. Second, additional routing information must be distributed across the cluster. The WireGuard CIDR is unknown to the network implementation so additional routes must be distributed on all nodes of the cluster. Third, kubelink dynamincally configures the Wireguard interface with endpoints and their public keys.</p><p>On the shoot, we create the keys and acquire the WireGuard IP in the standard secret generation. The data is added as a secret to the control plane and to the shootstate. The vpn shoot deployment is extended to include the WireGuard device setup inside the vpn shoot pod network. For certain infrastructures (AWS), we need a re-advertiser to resolve the seed WireGuard endpoint and evaluate whether the IP address changed.</p><p>While it is possible to configure a WireGuard device using DNS names only IP addresses can be stored in Linux Kernel data structures. A change of a load balancer IP address can therefore not be mitigated on that level. As WireGuard dynamically adapts endpoint IP addresses a change in load banlancer IPs is mitigated in most but not all cases. This is why a re-advertiser is required for public cloud providers such as AWS.</p><p>The load balancer exposing the OpenVPN endpoint in the shoot cluster is no longer required and therefore removed if this functionality is used.</p><p>As we want to slowly expand the usage of the WireGuard solution, we propose to introduce a feature gate for it. Furthermore, since the WireGuard network requires a separate network range, we propose to introduce a new section to the seed settings with two additional flags (enabled & cidr):</p><pre><code>apiVersion: core.gardener.cloud/v1beta1
kind: Seed
metadata:
  name: my-seed
  ...
spec:
  ...
  settings:
  ...
    wireguard:
      enabled: true
      cidr: 192.168.128.0/22
</code></pre><p>Last but not least, we propose to introduce an annotation to the shoots to enable/disable the WireGuard tunnel explicitly.</p><pre><code>apiVersion: core.gardener.cloud/v1beta1
kind: Shoot
metadata:
  name: my-shoot
  annotations:
    alpha.featuregates.shoot.gardener.cloud/wireguard-tunnel: &quot;true&quot;
  ...
</code></pre><p>Using this approach, it is easy to switch the solution on and off, i.e. migrate the shoot clusters automatically during ordinary reconciliation.</p><h4 id=high-availability>High Availability</h4><p>There is an issue if the node that hosts the WireGuard endpoint fails. The endpoint is migrated to another node however the time required to do this might exceed the budget for downtimes although one could argue that a disruption of less than 30 seconds to 1 minute does not qualify as a downtime and will in almost all cases not noticeable by end users.</p><p>In this case we also assume that TCP connections won&rsquo;t be interrupted - they would just appear to hang. We will confirm this behavior and the potential downtime as part of the development and testing effort as this is hard to predict.</p><p>As a possible mitigation we propose to instantiate 2 Kubelink instances in the seed cluster that are served by two different load balancers. The instances must run on different nodes (if possible but we assume a proper seed cluster has more than one node). Each shoot cluster connects to both endpoints. This means that the OpenVPN server is reachable with two different IP addresses. The VPN seed sidecars must attempt to connect to both of them and will continue to do so. The &ldquo;Persistent Keepalive&rdquo; feature is set to 21 seconds by default but could be reduced. Due to the redundancy this however appears not to be necessary.</p><p>It is desirable that both connections are used in an equal manner. One strategy could be to use the kubelink 1 connection if the first target WireGuard address is even (the last byte of the IPv4 address), otherwise the kubelink 2 connection. The <code>vpn-seed</code> sidecars can then use the following configuration in their OpenVPN configuration file:</p><pre><code>&lt;connection&gt;
remote 192.168.45.3 1194 udp
&lt;/connection&gt;

&lt;connection&gt;
remote 192.168.47.34 1194 udp
&lt;/connection&gt;
</code></pre><p>OpenVPN will go through the list sequentially and try to connect to these endpoints.</p><p>As an additional mitigation it appears possible to instantiate WireGuard devices on all hosts and replicate its relevant conntrack state across all cluster nodes. The relevant conntrack state keeps the state of all connections passing through the WireGuard interface (e.g. the WireGuard CIDR). conntrack and the tools to replicate conntrack state are part of the essential Linux netfilter tools package.</p><h4 id=load-considerations>Load Considerations</h4><p>What happens in case of a failure? In this case one router will end up owning all connections as the clients will attempt to use the next connection. This could be mitigated by adding a third redundant WireGuard connection. Using this strategy, the failure of one WireGuard endpoint would result in the equal distribution of connections to the two remaining interfaces. We believe however that this will not be necessary.</p><p>The cluster node running the Wireguard endpoint is essentially a router that routes all traffic to the various shoot clusters. This is established and proven technology that already exists since decades and has been highly optimized since then. This is also the technology that hyperscalers rely on to provide VPN connectivity to their customers. This said, hyperscalers essentially provide solutions based on IPsec which is known not to scale as well as Wireguard. Wireguard is a relatively new technology but we have no doubt that it is less stable than existing IPsec solution.</p><p>Regarding performance there is a lot of information on the Internet basically suggesting that Wireguard performs better than other VPN solutions such as IPsec or OpenVPN. One example is <a href=https://www.wireguard.com/performance/>https://www.wireguard.com/performance/</a> and <a href=https://www.net.in.tum.de/fileadmin/bibtex/publications/papers/2020-ifip-moonwire.pdf>https://www.net.in.tum.de/fileadmin/bibtex/publications/papers/2020-ifip-moonwire.pdf</a>.</p><p>Based on this, we have no reason to believe that one router will not be able to handle all traffic going to and coming from shoot clusters. Nevertheless, we will closely monitor the situation in our tests and will take action if necessary.</p><h4 id=further-research>Further Research</h4><p>Based on feedback on this proposal and while working on this implementation we identified two additinal approaches that we have not thought of so far. The first idea can be used to replace the &ldquo;inner&rdquo; OpenVPN implementation and the second can be used to replace WireGuard with OpenVPN and get rid of the single point of failure.</p><ol><li><p>Instead of using OpenVPN for the inner seed/shoot communication we can use the proxy protocol and use a TCP proxy (e.g. envoy) in the shoot cluster to broker the seed-shoot connections. The advantage is that with this solution seed- and shoot cluster network ranges are allowed to overlap. Disadvantages are increased implementation effort and less efficient network in terms of throughput and scalability. We believe however that the reduced network efficiency does not invalidate this option.</p></li><li><p>There is an option in OpenVPN to specify a tcp proxy as part of the endpoint configuration.</p></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-67211574c9c9627ba4e942fa77ceda03>18 - Shoot CA Rotation</h1><h1 id=gep-18-automated-shoot-ca-rotation>GEP-18: Automated Shoot CA Rotation</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=#summary>Summary</a></li><li><a href=#motivation>Motivation</a><ul><li><a href=#goals>Goals</a></li><li><a href=#non-goals>Non-Goals</a></li></ul></li><li><a href=#proposal>Proposal</a></li><li><a href=#alternatives>Alternatives</a></li><li><a href=#open-questions>Open Questions</a></li></ul><h2 id=summary>Summary</h2><p>This proposal outlines an on-demand, multi-step approach to rotate all certificate authorities (CA) used in a Shoot cluster. This process includes creating new CAs, invalidating the old ones and recreating all certificates signed by the CAs.</p><p>We propose to bundle the rotation of <em>all</em> CAs in the Shoot together as one triggerable action. This includes the recreation and invalidation of the following CAs and all certificates signed by them:</p><ul><li>Cluster CA (currently used for signing <code>kube-apiserver</code> serving certificates and client certificates)</li><li><code>kubelet</code> CA (used for signing client certificates for talking to <code>kubelet</code> API, e.g. <code>kube-apiserver-kubelet</code>)</li><li><code>etcd</code> CA (used for signing <code>etcd</code> serving certificates and client certificates)</li><li>front-proxy CA (used for signing client certificates that <code>kube-aggregator</code> (part of <code>kube-apiserver</code>) uses to talk to extension API servers, filled into <code>extension-apiserver-authentication</code> ConfigMap and read by extension API servers to verify incoming <code>kube-aggregator</code> requests)</li><li><code>metrics-server</code> CA (used for signing serving certificates, filled into APIService <code>caBundle</code> field and read by <code>kube-aggregator</code> to verify the presented serving certificate)</li><li><code>ReversedVPN</code> CA (used for signing <code>vpn-seed-server</code> serving certificate and <code>vpn-shoot</code> client certificate)</li></ul><p>Out of scope for now:</p><ul><li><code>kubelet</code> serving CA is self-generated (valid for <code>1y</code>) and self-signed by <code>kubelet</code> on startup<ul><li><code>kube-apiserver</code> does not seem to verify the presented serving certificate</li><li><code>kubelet</code> can be configured to request serving certificate via CSR that can be verified by <code>kube-apiserver</code>, though, we consider this as a separate improvement outside of this GEP</li></ul></li><li>Legacy VPN solution uses the cluster CA for both serving and client certificates. As the solution is soon to be dropped in favor of the new <code>ReversedVPN</code> solution, we don&rsquo;t intend to introduce a dedicated CA for this component. If <code>ReversedVPN</code> is disabled and the CA rotation is triggered, we make sure to propagate the cluster CA to the relevant places in the legacy VPN solution.</li></ul><p>Naturally, not all certificates used for communication with the <code>kube-apiserver</code> are under control of Gardener. An example for a Gardener-controlled certificate is the kubelet client certificate used to communicate with the api server. An example for credentials not controlled by gardener are kubeconfigs or client certificates requested via <code>CertificateSigningRequest</code>s by the shoot owner.</p><p>We propose to use a two step approach to rotate CAs. The start of each phase is triggered by the shoot owner.
In summary the <strong>first phase</strong> is used to create new CAs (for example the new api server and client CA). Then we make sure that all servers and clients under Gardener&rsquo;s control trust <em>both</em> old and new CA. Next we renew all client certificates that are under Gardener&rsquo;s control so they are now signed by the new CAs. This includes a node rollout in order to propagate the certificates to kubelets and restart all pods. Afterwards the user needs to change their client credentials to trust both old and new cluster CA.
In the <strong>second phase</strong>, we remove all trust to the old CA for servers and clients under Gardener&rsquo;s control. This does not include a node rollout but all still running pods using <code>ServiceAccount</code>s will continue to trust the old CA until they restart. Also, the user needs to retrieve the new CA bundle to no longer trust the old CA.</p><p>A detailed overview of all steps required for each phase is given in the <a href=#proposal>proposal</a> section of this GEP.</p><p><em>Introducing a new client CA</em></p><p>Currently, client certificates and the kube-apiserver certificate are signed by the same CA. We propose to create a separate client CA when triggering the rotation. The client CA is used to sign certificates of clients talking to the API Server.</p><h2 id=motivation>Motivation</h2><p>There are a few reasons for rotating shoot cluster CAs:</p><ul><li>If we have to invalidate client certificates for the kube-apiserver or any other component we are forced to rotate the CA. The only way to invalidate them is to stop trusting all client certificates that are signed by the respective CA as kubernetes does not support revoking certificates.</li><li>If the CA itself got leaked.</li><li>If the CA is about to expire.</li><li>If a company policy requires to rotate a CA after a certain point in time.</li></ul><p>In each of those cases we currently need to basically manually recreate and replace all CAs and certificates. The process of rotating by hand is cumbersome and could lead to errors due to the many steps needing to be performed in the right order. By automating the process we want to create a way to securely and easily rotate shoot CAs.</p><h3 id=goals>Goals</h3><ul><li>Offer an automated and safe solution to rotate all CAs in a shoot cluster.</li><li>Offer a process that is easily understandable for developers and users.</li><li>Rotate the different CAs in the shoot with a similar process to reduce complexity.</li><li>Add visibility for Shoot owners when the last CA rotation happened</li></ul><h3 id=non-goals>Non-Goals</h3><ul><li>Offer an automated solution for rotating other static credentials (like static token).<ul><li>Later on, a similar two-phase approach could be implemented for the kubeconfig rotation. However, this is out of scope for this enhancement.</li></ul></li><li>Creating a process that runs fully automated without shoot owner interaction. As the shoot owner controls some secrets that would probably not even be possible.</li><li>Forcing the shoot owner to rotate after a certain time period. Our goal rather is to issue long-running certificates and let the user decide depending on their requirements to rotate as needed.</li><li>Configurable default CA lifetime</li></ul><h2 id=proposal>Proposal</h2><p>We will add a new feature gate <code>CARotation</code> for <code>gardener-apiserver</code> and <code>gardenlet</code> which allows to enable or disable the possibility to trigger the rotation.</p><h3 id=triggering-the-ca-rotation>Triggering the CA Rotation</h3><ul><li>Triggered via <code>gardener.cloud/operation</code> annotation in symmetry with other operations like reconciliation, kubeconfig rotation, etc.<ul><li>annotation increases the generation</li><li>value for triggering first phase: <code>start-ca-rotation</code></li><li>value for triggering the second phase: <code>complete-ca-rotation</code></li><li><code>gardener-apiserver</code> performs the needful validation: user can&rsquo;t trigger another rotation if one is already in progress, user can&rsquo;t trigger <code>complete-ca-rotation</code> if first phase has not been compeleted, etc.</li></ul></li><li>The annotation triggers a usual shoot reconciliation (just like a kubeconfig or SSH key rotation)</li><li>gardenlet begins the CA rotation sequence by setting the new status section <code>.status.credentials.caRotation</code> (probably in <code>updateShootStatusOperationStart</code>) and removes the annotation afterwards<ul><li>shoot reconciliation needs to be idemptotent to CA rotation phase, i.e. if a usual reconciliation or maintenance operation is triggered in between, no new CAs are generated or similar things that would interfere with the CA rotation sequence</li></ul></li></ul><h3 id=changing-the-shoot-status>Changing the Shoot Status</h3><p>A new section in the Shoot status is added when the first rotation is triggered:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>status:
  credentials:
    rotation:
      certificateAuthorities:
        phase: Prepare <span style=color:green># Prepare|Finalize|Completed</span>
        lastCompletion: 2022-02-07T14:23:44Z
    <span style=color:green># kubeconfig:</span>
    <span style=color:green>#   phase:</span>
    <span style=color:green>#   lastCompletion:</span>
</code></pre></div><p>Later on, this section could be augmented with other information like the names of the credentials secrets (e.g. <a href=https://github.com/gardener/gardener/issues/1749>gardener/gardener#1749</a>)</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>status:
  credentials:
    resources:
    - type: kubeconfig
      kind: Secret
      name: shoot-foo.kubeconfig
</code></pre></div><h3 id=rotation-sequence-for-cluster-and-client-ca>Rotation Sequence for Cluster and Client CA</h3><p>The proposal section includes a detailed description of all steps involved for rotating from a given <code>CA0</code> to the target <code>CA1</code>.</p><p><code>t0</code>: Today&rsquo;s situation</p><ul><li><code>kube-apiserver</code> uses SERVER CERT signed by <code>CA0</code> and trusts CLIENT CERTS signed by <code>CA0</code></li><li><code>kube-controller-manager</code> issues new CLIENT CERTS signed by <code>CA0</code></li><li>kubeconfig trusts only <code>CA0</code></li><li><code>ServiceAccount</code> secrets trust only <code>CA0</code></li><li>kubelet uses CLIENT CERT signed by <code>CA0</code></li></ul><p><code>t1</code>: Shoot owner triggers first step of CA rotation process (&ndash;> phase one is started):</p><ul><li>Generate <code>CA1</code></li><li>Generate <code>CLIENT_CA1</code></li><li>Update <code>kube-apiserver</code>, <code>kube-scheduler</code>, etc. to trust CLIENT CERTS signed by both <code>CA0</code> and <code>CLIENT_CA1</code> (<code>--client-ca-file</code> flag)</li><li>Update <code>kube-controller-manager</code> to issue new CLIENT CERTS now with <code>CLIENT_CA1</code></li><li>Update kubeconfig so that its CA bundle contains both <code>CA0</code> and<code>CA1</code> (if kubeconfig still contains a legacy CLIENT CERT then rotate the kubeconfig)</li><li>Update <code>kube-controller-manager</code> to populate both <code>CA0</code> and <code>CA1</code> in <code>ServiceAccount</code> secrets.</li><li>Restart control plane components so that their CA bundle contains both <code>CA0</code> and <code>CA1</code></li><li>Renew CLIENT CERTS (sign them with <code>CLIENT_CA1</code>) for the following control plane components: Prometheus, DWD, legacy VPN), if not dropped already in the context of <a href=https://github.com/gardener/gardener/issues/4661>gardener/gardener#4661</a></li><li>Trigger node rollout<ul><li>This issues new CLIENT CERTS for all kubelets signed by <code>CLIENT_CA1</code></li><li>This restarts all <code>Pod</code>s and propagates <code>CA0</code> and <code>CA1</code> into their mounted <code>ServiceAccount</code> secrets (note CAs can not be reloaded by go client, therefore we need a restart of pods.)</li></ul></li><li><em>Ask user to exchange all their client credentials (kubeconfig, CLIENT CERTS issued by <code>CertificateSigningRequest</code>s) to trust both CA0 and CA1</em></li></ul><p><code>t2</code>: Shoot owner triggers second step of CA rotation process (&ndash;> phase two is started):</p><p>Prerequisite: All Gardener-controlled actions listed in t1 were executed successfully (for example node rollout). The shoot owner has guaranteed that they exchanged their client credentials and triggered step 2 via an annotation.</p><ul><li>Renew SERVER CERTS (sign them with <code>CA1</code>) for <code>kube-apiserver</code>, etc.</li><li>Update <code>kube-apiserver</code>, <code>kube-scheduler</code>, etc. to trust only CLIENT CERTS signed by <code>CLIENT_CA1</code></li><li>Update kubeconfig so that its CA bundle contains only <code>CA1</code></li><li>Update <code>kube-controller-manager</code> to only contain CA1. <code>ServiceAccount</code> secrets created after this point will get secrets that include only <code>CA1</code></li><li>Restart control plane components so that their CA bundle contains only <code>CA1</code></li><li>Restart kubelets so that the CA bundle in their kubeconfigs contain only <code>CA1</code></li><li>Delete <code>CA0</code></li><li><em>Ask user to optionally restart their <code>Pod</code>s since they still contain <code>CA0</code> in memory in order to eliminate trust to the old cluster CA.</em></li><li><em>Ask user to exchange all their client credentials (download kubeconfig containing only <code>CA1</code>; when using CLIENT CERTS trust only <code>CA1</code>)</em></li></ul><h3 id=rotation-sequence-of-other-cas>Rotation Sequence of Other CAs</h3><p>Apart from the kube-apiserver CA (and the client CA) we also use 5 other CAs as mentioned above in the gardener codebase. We propose to rotate those CAs together with the kube-apiserver CA following the same trigger.</p><p>ℹ️ Note for the front-proxy CA: users need to make sure, extension API servers have reloaded the <code>extension-apiserver-authentication</code> ConfigMap, before triggering the second phase.</p><p>You can find gardener managed CAs listed <a href=https://github.com/gardener/gardener/blob/04d2b3f459d198e8db0ab57180ca2fea18e84da9/pkg/operation/botanist/wanted_secrets.go#L48>here</a>.</p><p>Regarding the rotation steps we want to follow a similar approach to the one we defined for the kube-apiserver CA. Exemplary, we are going to show the timeline for ETCD_CA but the logic should be similiar for all the above listed CAs.</p><ul><li><code>t0</code><ul><li>etcd trusts client certificates signed by <code>ETCD_CA0</code> and uses a server certificate signed by <code>ETCD_CA0</code></li><li><code>kube-apiserver</code> and <code>backup-restore</code> use a client certificate signed by <code>ETCD_CA0</code> and trust <code>ETCD_CA0</code></li></ul></li><li><code>t1</code>:<ul><li>Generate <code>ETCD_CA1</code></li><li>Update <code>etcd</code> to trust CLIENT CERTS signed by both <code>ETCD_CA0</code> and <code>ETCD_CA1</code></li><li>Update <code>kube-apiserver</code> and <code>backup-restore</code>:<ul><li>Adapt CA bundle to trust both <code>ETCD_CA0</code> and <code>ETCD_CA1</code></li><li>Renew CLIENT CERTS (sign them with <code>ETCD_CA1</code>)</li></ul></li></ul></li><li><code>t2</code>:<ul><li>Update <code>etcd</code>:<ul><li>Trust only CLIENT CERTS signed by <code>ETCD_CA1</code></li><li>Renew SERVER CERT (sign it with <code>ETCD_CA1</code>)</li></ul></li><li>Update <code>kube-apiserver</code> and <code>backup-restore</code> so that their CA bundle contains only <code>ETCD_CA1</code></li></ul></li></ul><p>ℹ️ This means we are requiring two restarts of etcd in total.</p><h2 id=alternatives>Alternatives</h2><p>This section presents a different approach to rotate the CAs which is to <em>temporarily create a second set of api-servers utilizing the new CA</em> . After presenting the approach advantages and disadvantages of both approaches are listed.</p><p><code>t0</code>: Today&rsquo;s situation</p><ul><li><code>kube-apiserver</code> uses SERVER CERT signed by <code>CA0</code> and trusts CLIENT CERTS signed by <code>CA0</code></li><li><code>kube-controller-manager</code> issues new CLIENT CERTS with <code>CA0</code></li><li>kubeconfig contains only <code>CA0</code></li><li><code>ServiceAccount</code> secrets contain only <code>CA0</code></li><li>kubelet uses CLIENT CERT signed by <code>CA0</code></li></ul><p><code>t1</code>: User triggers first step of CA rotation process (&ndash;> phase one):</p><ul><li>Generate <code>CA1</code></li><li>Generate <code>CLIENT_CA1</code></li><li>Create new <code>DNSRecord</code>, <code>Service</code>, Istio configuration, etc. for second <code>kube-apiserver</code> deployment</li><li>Deploy second <code>kube-apiserver</code> deployment trusting only CLIENT CERTS signed by <code>CLIENT_CA1</code> and using SERVER CERT signed by <code>CA1</code></li><li>Update <code>kube-scheduler</code>, etc. to trust only CLIENT CERTS signed by <code>CLIENT_CA1</code> (<code>--client-ca-file</code> flag)</li><li>Update <code>kube-controller-manager</code> to issue new CLIENT CERTS with <code>CLIENT_CA1</code></li><li>Update kubeconfig so that it points to the new <code>DNSRecord</code> and its CA bundle contains only <code>CA1</code> (if kubeconfig still contains a legacy CLIENT CERT then rotate the kubeconfig)</li><li>Update <code>ServiceAccount</code> secrets so that their CA bundle contains both <code>CA0</code> and <code>CA1</code></li><li>Restart control plane components so that they point to the second <code>kube-apiserver</code> <code>Service</code> and so that their CA bundle contains only <code>CA1</code></li><li>Renew CLIENT CERTS (sign them with <code>CLIENT_CA1</code>) for control plane components (Prometheus, DWD, legacy VPN) and point them to the second <code>kube-apiserver</code> <code>Service</code></li><li>Adapt <code>apiserver-proxy-pod-mutator</code> to point <code>KUBERNETES_SERVICE_HOST</code> env variable to second <code>kube-apiserver</code></li><li>Trigger node rollout<ul><li>This issues new CLIENT CERTS for all kubelets signed by <code>CLIENT_CA1</code> and points them to the second <code>DNSRecord</code></li><li>This restarts all <code>Pod</code>s and propagates <code>CA0</code> and <code>CA1</code> into their mounted <code>ServiceAccount</code> secrets</li></ul></li><li><em>Ask user to exchange all their client credentials (kubeconfig, CLIENT CERTS issued by <code>CertificateSigningRequest</code>s)</em></li></ul><p><code>t2</code>: User triggers second step of CA rotation process (&ndash;> phase two):</p><ul><li>Update <code>ServiceAccount</code> secrets so that their CA bundle contains only <code>CA1</code></li><li>Update <code>apiserver-proxy</code> to talk to second <code>kube-apiserver</code></li><li>Drop first <code>DNSRecord</code>, <code>Service</code>, Istio configuration and first <code>kube-apiserver</code> deployment</li><li>Drop <code>CA0</code></li><li><em>Ask user to optionally restart their <code>Pod</code>s since they still contain <code>CA0</code> in memory.</em></li></ul><h4 id=advantagesdisadvantages-approach-two-api-servers>Advantages/Disadvantages approach two api servers</h4><ul><li>(+) User needs to adapt client credentials only once</li><li>(/) Unstable API server domain</li><li>(-) Probably more implementation effort</li><li>(-) More complex</li><li>(-) CA rotation process does not work similar for all CAs in our system</li></ul><h4 id=advantagesdisadvantages-of-currently-preferred-approach-see-proposal>Advantages/Disadvantages of currently preferred approach (see proposal)</h4><ul><li>(+) Implementation effort seems &ldquo;straight-forward&rdquo;</li><li>(+) CA rotation process works similar for all CAs in our system</li><li>(/) Stable API server domain</li><li>(-) User needs to adapt client credentials twice</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a3aad4401fad43af26be493b4a8c8b0d>19 - Utilize API Server Network Proxy to Invert Seed-to-Shoot Connectivity</h1><h1 id=utilize-api-server-network-proxy-to-invert-seed-to-shoot-connectivity>Utilize API Server Network Proxy to Invert Seed-to-Shoot Connectivity</h1><ul><li><a href=#utilize-api-server-network-proxy-to-invert-seed-to-shoot-connectivity>Utilize API Server Network Proxy to Invert Seed-to-Shoot Connectivity</a><ul><li><a href=#problem>Problem</a></li><li><a href=#proposal>Proposal</a><ul><li><a href=#api-server-network-proxy>API Server Network Proxy</a></li></ul></li><li><a href=#challenges>Challenges</a><ul><li><a href=#prometheus-to-shoot-connectivity>Prometheus to Shoot connectivity</a><ul><li><a href=#possible-solutions>Possible Solutions</a></li><li><a href=#port-forwarder-sidecar>Port-forwarder Sidecar</a></li><li><a href=#proxy-client-sidecar>Proxy Client Sidecar</a></li><li><a href=#proxy-sub-resource>Proxy sub-resource</a></li></ul></li><li><a href=#proxy-server-loadbalancer-sharing-and-re-advertising>Proxy-server Loadbalancer Sharing and Re-advertising</a><ul><li><a href=#possible-solution>Possible Solution</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></li></ul></li></ul><h2 id=problem>Problem</h2><p>Gardener&rsquo;s architecture for Kubernetes clusters relies on having the control-plane (e.g., kube-apiserver, kube-scheduler, kube-controller-manager, etc.) and the data-plane (e.g., kube-proxy, kubelet, etc.) of the cluster residing in separate places, this provides many benefits but poses some challenges, especially when API-server to system components communication is required. This problem is solved today in Gardener by <a href=https://github.com/gardener/vpn>making use of OpenVPN</a> to establish a VPN connection from the seed to the shoot. To do so, the following steps are required:</p><ul><li>Create a Loadbalancer service on the shoot.</li><li>Add a sidecar to the API server pod which knows the address of the newly created Loadbalancer.</li><li>Establish a connection over the internet to the VPN Loadbalancer</li><li>Install additional iptables rules that would redirect all the IPs of the shoot (i.e., service, pod, node CIDRs) to the established VPN tunnel</li></ul><p>There are however quite a few problems with the above approach, here are some:</p><ul><li>Every shoot would require an additional loadbalancer, this accounts for addition overhead in terms of both costs and troubleshooting efforts.</li><li>Private access use-cases would not be possible without having a seed residing in the same private domain as a hard requirement. For example, have a look at <a href=https://github.com/gardener/gardener-extension-provider-gcp/issues/56>this issue</a></li><li>Providing a public endpoint to access components in the shoot poses a security risk.</li></ul><h2 id=proposal>Proposal</h2><p>There are mutliple ways to tackle the directional connectivity issue mentioned above, one way would be to invert the connection between the API server and the system components, i.e., instead of having the API server side-car establish a tunnel, we would have an agent residing in the shoot cluster initiate the connection itself. This way we don&rsquo;t need a Loadbalancer for every shoot and from the security perspective, there is no ingress from outside, only controlled egress.</p><p>We want to replace this:</p><p><code>APIServer | VPN-seed ---> internet ---> LB --> VPN-Shoot (4314) --> Pods | Nodes | Services</code></p><p>With this:</p><p><code>APIServer &lt;-> Proxy-Server &lt;--- internet &lt;--- Proxy-Agent --> Pods | Nodes | Services</code></p><h3 id=api-server-network-proxy>API Server Network Proxy</h3><p>To solve this issue we can utilize the <a href=https://github.com/kubernetes-sigs/apiserver-network-proxy>apiserver-network-proxy</a> upstream implementation. Which provides a reference implementation for a reverse streaming server. The way it works is as follows:</p><ul><li>Proxy agent connects to proxy server to establish a sticky connection.</li><li>Traffic to the proxy server (residing in the seed) gets then re-directed to the agent (residing in the shoot) which forwards the traffic to in-cluster components.</li></ul><p>The initial motivation for the apiserver-network-proxy project is to get rid of provider-specific implementations that reside in the API-server (e.g., SSH), but it turns out that
it has other interesting use-cases such as data-plane connection decoupling, which is the main use-case for this proposal.</p><p>Starting with <strong>Kubernetes 1.18</strong> it&rsquo;s possible to make use of an <code>--egress-selector-config-file</code> flag, this helps point the API-server to traffic hook points based on traffic direction. For example, in the config below the API server would have to forward all cluster related traffic (e.g., logs, port-forward, exec, &mldr;etc.) to the <strong>proxy-server</strong> which then knows how to forward traffic to the shoot. For the rest of the traffic, e.g. API server to ETCD or other control-plane components <code>direct</code> is used which means legacy routing method, i.e., by-pass the proxy.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>  egress-selector-configuration.yaml: |-<span style=color:#a31515>
</span><span style=color:#a31515>    apiVersion: apiserver.k8s.io/v1alpha1
</span><span style=color:#a31515>    kind: EgressSelectorConfiguration
</span><span style=color:#a31515>    egressSelections:
</span><span style=color:#a31515>    - name: cluster
</span><span style=color:#a31515>      connection:
</span><span style=color:#a31515>        proxyProtocol: httpConnect
</span><span style=color:#a31515>        transport:
</span><span style=color:#a31515>          tcp:
</span><span style=color:#a31515>            url: https://proxy-server:8131
</span><span style=color:#a31515>    - name: master
</span><span style=color:#a31515>      connection:
</span><span style=color:#a31515>        proxyProtocol: direct
</span><span style=color:#a31515>    - name: etcd
</span><span style=color:#a31515>      connection:
</span><span style=color:#a31515>        proxyProtocol: direct</span>    
</code></pre></div><h2 id=challenges>Challenges</h2><h3 id=prometheus-to-shoot-connectivity>Prometheus to Shoot connectivity</h3><p>One challenge remains to completely eliminate the need for a VPN connection. In today&rsquo;s Gardener setup, each control-plane has a Prometheus instance that directly scrapes cluster components such as CoreDNS, Kubelets, cadvisor, etc. This works because in addition to the VPN side car attached to the API server pod, we have another one attached to prometheus which knows how to forward traffic to these endpoints. Once the VPN is eliminated, it is required to find other means to forward traffic to these components.</p><h4 id=possible-solutions>Possible Solutions</h4><p>There are currently two ways to solve this problem:</p><ul><li>Attach a port-forwarder side-car to prometheus.</li><li>Utilize the proxy subresource on the API server.</li></ul><h4 id=port-forwarder-sidecar>Port-forwarder Sidecar</h4><p>With this solution each prometheus instance would have a side-car that has the kubeconfig of the shoot cluster, and which establishes a port-forward connection to the endpoints residing in the shoot.</p><p>There are a many problems with this approach:</p><ul><li>the port-forward connection is not reliable.</li><li>the connection would break if the API server instance dies.</li><li>requires an additional component.</li><li>would need to expose every pod / service via port-forward.</li></ul><pre><code class=language-console data-lang=console>Prom Pod (Prometheus -&gt; Port-forwarder) &lt;-&gt; APIServer -&gt; Proxy-server &lt;--- internet &lt;--- Proxy-Agent --&gt; Pods | Nodes | Services
</code></pre><h4 id=proxy-client-sidecar>Proxy Client Sidecar</h4><p>Another solution would be to implement a proxy-client as a sidecar for every component that wishes to communicate with the shoot cluster. For this to work, means to re-direct / inject that proxy to handle the component&rsquo;s traffic is necessary (e.g., additional IPtable rules).</p><pre><code class=language-console data-lang=console>Prometheus Pod (Prometheus -&gt; Proxy) &lt;-&gt; Proxy-Server &lt;--- internet &lt;--- Proxy-Agent --&gt; Pods | Nodes | Services
</code></pre><p>The problem with this approach is that it requires an additional sidecar (along with traffic redirection) to be attached to every client that wishes to communicate with the shoot cluster, this can cause:</p><ul><li>additional maintenance efforts (extra code).</li><li>other side-effects (e.g., if istio sidecar injection is enabled)</li></ul><h4 id=proxy-sub-resource>Proxy sub-resource</h4><p>Kubernetes supports proxying requests to nodes, services, and pod endpoints in the shoot cluster. This proxy connection can be utilized for scraping the necessary endpoints in the shoot.</p><p>This approach requires less components and is more reliable than the port-forward solution, however, it relies on having the API server supporting proxied connection for the required endpoints.</p><pre><code class=language-console data-lang=console>Prometheus  &lt;-&gt; APIServer &lt;-&gt; Proxy-Server &lt;--- internet &lt;--- Proxy-Agent --&gt; Pods | Nodes | Services
</code></pre><p>As simple as it is, it has a downside that it relies on the availability of the API server.</p><h3 id=proxy-server-loadbalancer-sharing-and-re-advertising>Proxy-server Loadbalancer Sharing and Re-advertising</h3><p>With the proxy-server in place, we need to provide means to enable the proxy-agent in the shoot to establish the connection with the server. As a result, we need to provide a public endpoint through which this channel of communication can be established, i.e., we need a Loadbalancer(s).</p><h4 id=possible-solution>Possible Solution</h4><p>Using a Loadbalancer / proxy server would not make sense since this is a pain-point we are trying to eliminate in the first-place, doing so just moves the costs to the control-plane. A possible solution is to communicate over a shared loadbalancer in the seed, similar to what has been proposed <a href=/docs/gardener/proposals/08-shoot-apiserver-via-sni/>here</a>, this way we can prevent the extra-costs for load-balancers.</p><p>With this in mind, we still have other pain-points, namely:</p><ul><li>Advertising Loadbalancer public IPs to the shoot.</li><li>Directing the traffic to the corresponding shoot proxy-server.</li></ul><p>For advertising the Loadbalancer IP, a DNS entry can be created for the proxy loadbalancer (or re-use the DNS entry for the SNI proxy), along with necessary certificates, which is then used to connect to the loadbalancer. At this point we can decide on either one of the two approaches:</p><ol><li>One Proxy / API server with a shared loadbalancer.</li><li>Use one proxy server for all agents.</li></ol><p>In the first case, we will probably need a proxy for the proxy-server that knows how to direct traffic to the correct proxy server based on the corresponding shoot cluster. In the second case, we don&rsquo;t need another proxy if the proxy server is cluster-aware, i.e., can pool and identify connections coming from the same cluster and peer them with the correct API. Unfortunately, the second case is not supported today.</p><h3 id=summary>Summary</h3><ul><li>API server proxy can be utilized to invert the connection (only for clusters >= 1.18, for older clusters the old VPN solution will remain).</li><li>This is achieved by utilizing the <code>--egress-selector-config-file</code> flag on the api-server.</li><li>For monitoring endpoints, the proxy subresources would be the preferable methods to go, but in the future we can also support sidecar proxies that can communicate with the proxy-server.</li><li>For Directing traffic to the correct proxy-server we will re-use the SNI proxy along with the load-balancer from <a href=/docs/gardener/proposals/08-shoot-apiserver-via-sni/>the shoot API server via SNI GEP</a>.</li></ul></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2022 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js integrity=sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/mermaid@8.9.2/dist/mermaid.min.js integrity=sha384-uQikAXnCAqsMb3ygtdqBYvcwvHUkzGIpjdGyy9owhURXHUxLC5LgTcSxJQH/RzjK crossorigin=anonymous></script><script src=/js/main.min.ef8e0714aff556fd5a9768ed6ecabd2964dd962cd9f89762a373947bb53bc742.js integrity="sha256-744HFK/1Vv1al2jtbsq9KWTdlizZ+Jdio3OUe7U7x0I=" crossorigin=anonymous></script></body></html>