<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.95.0"><link rel=canonical type=text/html href=https://gardener.cloud/docs/other-components/etcd-backup-restore/proposals/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/other-components/etcd-backup-restore/proposals/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Proposals | Gardener</title><meta name=description content><meta property="og:title" content="Proposals"><meta property="og:description" content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/other-components/etcd-backup-restore/proposals/"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="Proposals"><meta itemprop=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="Proposals"><meta name=twitter:description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><link rel=preload href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css as=style><link href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7N3XF5XLGV"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7N3XF5XLGV",{anonymize_ip:!1})}</script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.cef0980fb8e4cf522414dcbf02b2220a.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/other-components/etcd-backup-restore/proposals/>Return to the regular view of this page</a>.</p></div><h1 class=title>Proposals</h1><div class=content></div></div><div class=td-content><h1 id=pg-544f5cd1f7b51e199bec5d6920882be3>1 - Design</h1><h1 id=etcd-backup-restore-design>Etcd backup restore design</h1><h2 id=goal>Goal</h2><p>Main goal of this project to provide a solution to make <a href=https://github.com/etcd-io/etcd>etcd</a> instance backing the kubernetes cluster robust to failures. Etcd data is backed up at regular intervals. The etcd instance runs on a Seed Kubernetes cluster and stores the state of the Shoot Kubernetes clusters. In case of etcd instance failures, the etcd instance is reconciled and in the extreme case restored from the latest non-corrupt backup available.</p><h2 id=non-goal>Non Goal</h2><ul><li>Volume failures like EBS volume failures, PVC and PV deletion after etcd deletion (when the shoot goes down) are not handled at this moment. However, checks and remedial steps for the failure types mentioned previously will be implemented over time/later.</li><li>Allowing the user to pick from a list of backups or even point-in-time-recovery (PiTR) is not planned, because there is no reliable infrastructure reconciliation implemented in Kubernetes and the CRD extension concept (or API server extensions) allows to have resources with impact on the external/physical world that would not be reconciled either (if we are to restore a backup which is not the latest).</li><li>Backup validation is not repeatedly done, i.e. once a backup is taken, we assume the infrastructure preserves it in a healthy state (we do not validate backups by rehashing them and comparing them with initially taken hashes).</li></ul><h2 id=design>Design</h2><h3 id=assumption>Assumption</h3><ul><li><p>Etcd cluster is a single member cluster (Shoot cluster environment, much like a master in a Borg-hosted GKE cluster).</p></li><li><p>Etcd instance will be deployed as a StatefulSet with a PVC for its persistence, which will bring free reconciliation.</p></li></ul><h3 id=requirements>Requirements</h3><ul><li><p>ETCD pod fails and PVC with valid data directory is available: Etcd pod should restart and attach to same PVC to continue from last state before failure.</p></li><li><p>Data corruption check: There should be mechanism to check etcd failure due to etcd data directory corruption.</p></li><li><p>Data directory unavailable but backups available in the cloud store: Restore Etcd from the latest backup.</p></li><li><p>Backup etcd snapshots on different cloud object-stores.</p></li></ul><h3 id=rely-on-k8s-for-the-following>Rely on K8s for the Following</h3><ul><li>PVC deleted - Doesn&rsquo;t necessarily detach volume if in use.</li><li>IaaS failure<ul><li>Storage Volume detached: PV reattaches to the same volume. Restoration of Etcd data directory not required.</li><li>Storage Volume deleted: Expect StatefulSet to be robust to such failures. Restoration of etcd data directory necessary.</li><li>Node failure: K8s should reschedule the pod on a different node. Restoration of Etcd data directory not required.</li><li>ETCD Pod scheduling failed: Can’t do much (look pod configuration parameter for critical-pods).</li></ul></li></ul><h2 id=architecture>Architecture</h2><p><img src=/__resources/etcd-backup-restore_f13a0f.jpg alt=architecture></p><p>We will have a StatefulSet for etcd with two containers in it.</p><ul><li>ETCD container</li><li>Sidecar container</li></ul><h3 id=etcd-container>ETCD Container</h3><ul><li>Request the sidecar to validate/initialize the data directory.</li><li>The etcd process is started only if the <code>initialize</code> request to sidecar returns a success.</li></ul><h3 id=sidecar-container>Sidecar Container</h3><p>Sidecar container has two components</p><ul><li>Initializer</li><li>Prober</li></ul><h4 id=initializer>Initializer</h4><ul><li>On request from the Etcd container, check the data directory for data corruption.</li><li>if data directory is corrupt, restore the data directory from the latest snapshot.</li><li>return successful response on validation/restoration of data directory.</li></ul><h4 id=prober>Prober</h4><ul><li>Probe etcd container for liveliness of etcd process.</li><li>Probe is required to ensure that etcd is live before backups are triggered.</li><li>Schedule the backup operation (probably using cron library) which triggers full snapshot at regular intervals.</li><li>Store the snapshot in the configured cloud object store.</li></ul><p><strong>Init container is not used for the validation/restoration of etcd data directory. The rationale behind the decision was to avoid baking in pod restart logic in sidecar container in the event etcd process had died. In case etcd container died, init-container had to be run before etcd container was run to ensure that data directory was valid. This required the pod to be restarted. With the current design, the sidecar handles the data directory validation/restoration and periodic backups. Pod restart is not required.</strong></p><h2 id=workflow>Workflow</h2><p><img src=/__resources/etcd-backup-restore-sequence-diagram_12616f.jpg alt=sequence-diagram></p><h3 id=etcd-container-1>Etcd container</h3><ol><li>Etcd container starts and requests the sidecar for data directory initialization. It waits for the response.</li><li>On response,<ol><li>in case of success, start etcd process.</li><li>in case of failure/timeout, exit with error. (Container restarts)</li></ol></li></ol><h3 id=sidecar-container-1>Sidecar container</h3><ol><li>Start periodic liveliness prober on etcd. Start http server to serve <code>initialize</code> requests.</li><li>On receiving an <code>initialize</code> request, check the data directory for corruption.<ol><li>In case of data directory corruption, restore data directory from the latest cloud snapshot. Return success.</li><li>In case data directory is valid, return success.</li><li>In all other cases, return failure.</li></ol></li><li>Once the <code>initialize</code> request returns success, etcd process can be expected to start up in some time. The prober would then receive a successful probe of etcd&rsquo;s liveliness.</li><li>On successful probe, start taking periodic backup of etcd and store the snapshot to the cloud object store. Stop prober.<ul><li>In case of a failure to take a backup, exit with error. (Container restarts)</li></ul></li></ol><h3 id=handling-of-different-scenariosissues>Handling of Different Scenarios/Issues</h3><ul><li>DNS latency: Should not matter for single member Etcd cluster.</li><li>Etcd upgrade and downgrade for K8s compatibility: Should not be issue for v3.* series released so far. Simply restart pod. No data format change.</li><li>IaaS issue: Issues like unreachable object store, will be taken care by init container and backup container. Both container will keep retrying to reach out object store with exponential timeouts.</li><li>Corrupt backup: StatefulSet go in restart loop, and human operator will with customers concern delete the last corrupt backup from object store manually. So that, in next iteration it will recover from previous non-corrupt backup.</li></ul><h2 id=outlook>Outlook</h2><p>We want to develop incremental/continuous etcd backups (write watch logs in between full backups), to ensure our backups are fresh enough to avoid discrepancies between etcd backup and external/physical world.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-8becec8126a0477f480a7d4313991617>2 - High Watch Event Ingress Rate</h1><h1 id=handle-high-watch-event-ingress-rate>Handle high watch event ingress rate</h1><h2 id=problem-statement>Problem statement</h2><p>For large key-value sizes of etcd and watch on etcd yielding responses at a high rates, we see that the backup sidecar memory shoots up drastically but linearly.</p><p><img src=/__resources/profile-high-rate-events_b2b269.png alt=profile></p><h2 id=reason>Reason</h2><p>The memory consumption surge is associated with the watch implementation in the client library.
As can be seen <a href=https://github.com/etcd-io/etcd/blob/master/clientv3/watch.go#L814>here</a>, when the ingress rate of the events are more than the egress rate from the watch response channel the buffers holding the events tend to bloat. There is no throttling done between etcd and the client. Even if throttling was done, we would not be able to depend on it. As the revision of etcd would have moved ahead and due to throttling the watch events would be far behind. This lag could get progressively worse if the load on etcd persists. We would have to implement a different approach to watches if we stumble upon this issue in realtime.</p><p>When the delta snapshot logic was changed to run multiple goroutines to drain the watch channel and to short circuit writing the delta to cloud but instead just drop it, we see that the memory pressure on the watch client is drastically reduced. The finding is shown in the image below.</p><p><img src=/__resources/profile-fast-drain_a0257d.png alt=profile></p><h2 id=solution>Solution</h2><p>There are two areas to address in order to arrive at a solution:</p><ul><li>Fix the watch implementation of etcd client to throttle the ingress of watch response events from etcd. Implement a fix avoid the unchecked growth of <a href=https://github.com/etcd-io/etcd/blob/master/clientv3/watch.go#L814>buffer</a> so as to prevent the OOM Kill.</li><li>Try a polling-based approach (with parallel download of chunks of revisions) as against the watch-based approach now. The delta revisions yet to be archived in the cloud store can be polled in chunks. The amount of memory consumed from as part of the poll based events fetch can be done in a controlled manner.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-7bdeb069310894a2eed087bffee2946b>3 - Restoration</h1><h1 id=etcd-data-restoration>Etcd data restoration</h1><p>Etcd-backup-restore provides a simple and automated method of restoring single-node etcd clusters from their backups (snapshots) stored via the snapshotting process. Restoration kicks in when etcd data gets corrupted, either by the etcd process itself or due to PV corruption, or if the data is manipulated or erased due to human intervention. The restorer relies on the backups stored in an object storage bucket to restore the etcd data to its latest backed-up non-corrupt state.</p><p>In the context of etcd-backup-restore server, restoration is a conditional part of the initialization workflow, in which restoration is triggered when etcd data validation deems the data directory invalid or corrupt.</p><p>To learn about starting an etcd-backup-restore server, please follow the <a href=/docs/other-components/etcd-backup-restore/deployment/getting_started/>getting started guide</a>.</p><h2 id=assumption>Assumption</h2><ul><li>Data directory must not contain <code>member</code> directory before triggering restoration</li><li>Snapshotter should NOT be running<ul><li>This is taken care of by etcd-backup-restore when started in <code>server</code> mode</li><li>If running in <code>snapshot</code> mode, one must take precautions to stop the snapshotter before attempting to restore</li></ul></li></ul><h2 id=requirements>Requirements</h2><ul><li>Etcd backup stored in an object storage bucket like AWS S3, GCP GCS, Azure ABS, Openstack Swift or Alicloud OSS</li><li>The backup format must conform to that of etcd-backup-restore&rsquo;s snapshotter<ul><li>Each backup must have a full snapshot, optionally followed by a set of delta/incremental snapshots</li></ul></li></ul><h2 id=workflow>Workflow</h2><p><img src=/__resources/restorer-sequence-diagram_f768e9.png alt=workflow></p><ul><li>Full snapshot is essentially the etcd DB file encapsulated in a JSON object</li><li>The full snapshot is downloaded only if available. If not, the embedded etcd server takes care of initializing the DB file</li><li>Delta snapshots are downloaded and applied only if available. If not, this step is skipped and restoration is marked as complete.</li></ul><p>Restorer fetches the latest full snapshot present in the snapstore, named <code>etcd-main/v1/Backup-xxxxxxxxxx/Full-aaaaaaaa-bbbbbbbb-xxxxxxxxxx</code>, and restores from it. It then fetches the subsequent delta snapshots and applies them sequentially on top of the partially restored data directory. These delta snapshots are named <code>etcd-main/v1/Backup-xxxxxxxxxx/Incr-bbbbbbbb-cccccccc-yyyyyyyyyy</code> and so on. Please consider the following while reading the aforementioned snapshot naming patterns: <code>xxxxxxxxxx</code>,<code>yyyyyyyyyy</code> are UTC timestamps and <code>aaaaaaaa</code>,<code>bbbbbbbb</code>,<code>cccccccc</code> are etcd revision numbers. Each snapshot contains etcd events between two revisions inclusive, as specified in the snapshot name.</p><h2 id=automatic-restoration>Automatic restoration</h2><p>Restoration is an essential part of etcd-backup-restore&rsquo;s <code>initialization</code> workflow, which is triggered as the first step in the <code>server</code> sub-command workflow. Ideally, operators need not worry about restoration when deploying etcd-backup-restore via the provided helm chart, where the tool is deployed in the <code>server</code> mode, as restoration will be triggered if the etcd data directory is found to be corrupt.</p><h2 id=manual-restoration>Manual restoration</h2><p>Please refer to <a href=/docs/other-components/etcd-backup-restore/operations/manual_restoration/>this guide</a> to know how to manually restore the etcd data, the caveats that come with it, and how to avoid common pitfalls.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-aa14d387a9e60db3ae269a07fe96be99>4 - Validation</h1><h1 id=etcd-data-validation>Etcd data validation</h1><p>As etcd is being used to store the state of the K8s cluster, it is mandatory that etcd deployment has to be hardened against data loss. Sufficient checks have to be in place to prevent etcd from erroneously starting with stale/corrupt data and taking stale snapshots to the backing store. We have a data validation flow in place which prevents etcd from starting in case of data corruption.</p><h2 id=directory-validation>Directory validation</h2><p>The etcd data directory validation comprises of multiple checks as mentioned below:</p><h3 id=structure-validation>Structure validation</h3><p>The member directory, snap directory and wal directory are checked to ascertain that they adhere to the directory structure followed by etcd.</p><h3 id=content-validation>Content validation</h3><h4 id=corruption-check>Corruption check</h4><p>The contents for the data directory(db file, snap files and wal file) are checked for data corruption.</p><h4 id=revision-check>Revision check</h4><p>The revision of etcd data in the db file is checked with the revision of the latest snapshot in the backing store. If the revison in the backing store is greater than that of etcd data in the db file, etcd data is considered stale. This is to prevent etcd snapshots for stale revisions from overwriting legit recent snapshots.</p><h2 id=validation-flow>Validation flow</h2><p>Not all validation steps take the same time to complete. Some validation steps are dependent on the size of etcd data(eg. db file). If the db file is checked for data corruption before etcd startup, it would take longer for etcd to become servicable. Therefore, it is only imperative to perform validation checks on abnormal etcd events like etcd restart after a crash. The validation flow mentioned below is modeled with the aforementioned rationale in mind.</p><ul><li>Is the validation marker file present?</li><li>No<ul><li>Do directory structure validation.</li><li>Do directory content validation.</li><li>Start etcd</li></ul></li><li>Yes<ul><li>Check if previous exit was normal from the validation marker file<ul><li>Yes<ul><li>Do revision check</li><li>Do directory structure validation.</li><li>Start etcd</li></ul></li><li>No<ul><li>Do directory structure validation.</li><li>Do directory content validation.</li><li>Start etcd</li></ul></li></ul></li></ul></li></ul><h2 id=addition-design-decisions-to-be-made>Addition design decisions to be made</h2><p>Currently, we have the validation check triggered from a bash script in the etcd container. The status of the validation check is polled till its completed and based on the validation status, it is decided whether it is safe to start etcd. During validation if etcd directory is found to be corrupt or stale, the latest snapshot in the backing store is used to restore etcd data to the latest revision.</p><h3 id=question-1-should-the-sidecar-container-be-able-to-act-on-the-status-of-previous-etcd-run-status>Question 1: Should the sidecar container be able to act on the status of previous etcd run status?</h3><ul><li><p><strong>Option 1</strong>: Yes. The information of previous etcd run may be made available to the sidecar container via configmaps. The idea is that <code>validate</code> REST endpoint shall check the shared configmap for status, perform necessary validation and restore steps before etcd start.</p></li><li><p><strong>Option 2</strong>: No. If the above-mentioned level of granularity is to be available for validation checks, we would need to modify the REST endpoints to trigger the validation sub-checks. Should we modify the bash script to handle the cases and let the sidecar be agnostic to the status of the previous etcd run?</p></li></ul><p>We have chosen the approach were the script decides on the previous exit status of etcd, to call the necessary validation step. If etcd terminated normally then sanity validation is performed else we perform a full etcd data validation.</p><h3 id=question-2-how-should-status-for-previous-etcd-run-be-identified>Question 2: How should status for previous etcd run be identified?</h3><ul><li><strong>Option 1</strong>: The error logs of the etcd run can be dumped to an log file in the persistent disk. This can be checked on subsequent validation steps to identify the status of previous etcd run.</li><li><strong>Option 2</strong>: Via exit code stored in a file in the persistent disk. This can be checked on subsequent validation steps to identify the status of previous etcd run.</li></ul><p>Since we are do not do an analysis of the logs at this point of time, the log dump and subsequent analysis steps can be taken care of in the necessary PR.</p></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2023 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid@8.13.4/dist/mermaid.min.js integrity="sha512-JERecFUBbsm75UpkVheAuDOE8NdHjQBrPACfEQYPwvPG+fjgCpHAz1Jw2ci9EXmd3DdfiWth3O3CQvcfEg8gsA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.7b24c0fb082ffb2de6cb14d6c95e9f8053053709ffcf8c761ef8e9ad2f8021e4.js integrity="sha256-eyTA+wgv+y3myxTWyV6fgFMFNwn/z4x2HvjprS+AIeQ=" crossorigin=anonymous></script></body></html>