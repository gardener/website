<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://gardener.cloud/docs/gardener/autoscaling/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/gardener/autoscaling/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Autoscaling | Gardener</title>
<meta name=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:url" content="https://gardener.cloud/docs/gardener/autoscaling/"><meta property="og:site_name" content="Gardener"><meta property="og:title" content="Autoscaling"><meta property="og:description" content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:locale" content="en_US"><meta property="og:type" content="website"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="Autoscaling"><meta itemprop=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta itemprop=image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="Autoscaling"><meta name=twitter:description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><link rel=preload href=/scss/main.min.1d5ea92d936c3f696a972447493d291f3b21bcba71aba5c7967af37ea482eba6.css as=style integrity="sha256-HV6pLZNsP2lqlyRHST0pHzshvLpxq6XHlnrzfqSC66Y=" crossorigin=anonymous><link href=/scss/main.min.1d5ea92d936c3f696a972447493d291f3b21bcba71aba5c7967af37ea482eba6.css rel=stylesheet integrity="sha256-HV6pLZNsP2lqlyRHST0pHzshvLpxq6XHlnrzfqSC66Y=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><style>.nav-link:hover{text-decoration:none}.ml-md-auto{margin-left:auto!important}.td-search__icon{color:#fff!important}.td-search__input.form-control::placeholder{color:#fff;border:1px;border-radius:20px}.td-search__input.form-control{border:1px;border-radius:20px}.td-search__input{max-width:90%}.td-search:not(:focus-within){color:#fff!important}</style><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://demo.gardener.cloud target=_blank><span>Demo</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><div class=dropdown><a href=/docs class=nav-link>Documentation</a><div class=dropdown-content><a class=taxonomy-term href=/docs>Users</a>
<a class=taxonomy-term href=/docs>Operators</a>
<a class=taxonomy-term href=/docs>Developers</a>
<a class=taxonomy-term href=/docs>All</a></div></div></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.080e494772bc09467b645f4393c6841d.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/gardener/autoscaling/>Return to the regular view of this page</a>.</p></div><h1 class=title>Autoscaling</h1><div class=content></div></div><div class=td-content><h1 id=pg-605bcd622eb0c8ad243eb175ff4fb8b7>1 - DNS Autoscaling</h1><h1 id=dns-autoscaling>DNS Autoscaling</h1><p>This is a short guide describing different options how to automatically scale CoreDNS in the shoot cluster.</p><h2 id=background>Background</h2><p>Currently, Gardener uses CoreDNS as DNS server. Per default, it is installed as a deployment into the shoot cluster that is auto-scaled horizontally to cover for QPS-intensive applications. However, doing so does not seem to be enough to completely circumvent DNS bottlenecks such as:</p><ul><li>Cloud provider limits for DNS lookups.</li><li>Unreliable UDP connections that forces a period of timeout in case packets are dropped.</li><li>Unnecessary node hopping since CoreDNS is not deployed on all nodes, and as a result DNS queries end-up traversing multiple nodes before reaching the destination server.</li><li>Inefficient load-balancing of services (e.g., round-robin might not be enough when using IPTables mode).</li><li>Overload of the CoreDNS replicas as the maximum amount of replicas is fixed.</li><li>and more &mldr;</li></ul><p>As an alternative with extended configuration options, Gardener provides cluster-proportional autoscaling of CoreDNS. This guide focuses on the configuration of cluster-proportional autoscaling of CoreDNS and its advantages/disadvantages compared to the horizontal
autoscaling.
Please note that there is also the option to use a <a href=/docs/gardener/networking/node-local-dns/>node-local DNS cache</a>, which helps mitigate potential DNS bottlenecks (see <a href=/docs/gardener/autoscaling/dns-autoscaling/#trade-offs-in-conjunction-with-nodelocaldns>Trade-offs in conjunction with NodeLocalDNS</a> for considerations regarding using NodeLocalDNS together with one of the CoreDNS autoscaling approaches).</p><h2 id=configuring-cluster-proportional-dns-autoscaling>Configuring Cluster-Proportional DNS Autoscaling</h2><p>All that needs to be done to enable the usage of cluster-proportional autoscaling of CoreDNS is to set the corresponding option (<code>spec.systemComponents.coreDNS.autoscaling.mode</code>) in the <code>Shoot</code> resource to <code>cluster-proportional</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  systemComponents:
</span></span><span style=display:flex><span>    coreDNS:
</span></span><span style=display:flex><span>      autoscaling:
</span></span><span style=display:flex><span>        mode: cluster-proportional
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>To switch back to horizontal DNS autoscaling, you can set the <code>spec.systemComponents.coreDNS.autoscaling.mode</code> to <code>horizontal</code> (or remove the <code>coreDNS</code> section).</p><p>Once the cluster-proportional autoscaling of CoreDNS has been enabled and the Shoot cluster has been reconciled afterwards, a ConfigMap called <code>coredns-autoscaler</code> will be created in the <code>kube-system</code> namespace with the default settings. The content will be similar to the following:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>linear: <span style=color:#a31515>&#39;{&#34;coresPerReplica&#34;:256,&#34;min&#34;:2,&#34;nodesPerReplica&#34;:16}&#39;</span>
</span></span></code></pre></div><p>It is possible to adapt the ConfigMap according to your needs in case the defaults do not work as desired. The number of CoreDNS replicas is calculated according to the following formula:</p><pre tabindex=0><code>replicas = max( ceil( cores × 1 / coresPerReplica ) , ceil( nodes × 1 / nodesPerReplica ) )
</code></pre><p>Depending on your needs, you can adjust <code>coresPerReplica</code> or <code>nodesPerReplica</code>, but it is also possible to override <code>min</code> if required.</p><h2 id=trade-offs-of-horizontal-and-cluster-proportional-dns-autoscaling>Trade-Offs of Horizontal and Cluster-Proportional DNS Autoscaling</h2><p>The horizontal autoscaling of CoreDNS as implemented by Gardener is fully managed, i.e., you do not need to perform any configuration changes. It scales according to the CPU usage of CoreDNS replicas, meaning that it will create new replicas if the existing ones are under heavy load. This approach scales between 2 and 5 instances, which is sufficient for most workloads. In case this is not enough, the cluster-proportional autoscaling approach can be used instead, with its more flexible configuration options.</p><p>The cluster-proportional autoscaling of CoreDNS as implemented by Gardener is fully managed, but allows more configuration options to adjust the default settings to your individual needs. It scales according to the cluster size, i.e., if your cluster grows in terms of cores/nodes so will the amount of CoreDNS replicas. However, it does not take the actual workload, e.g., CPU consumption, into account.</p><p>Experience shows that the horizontal autoscaling of CoreDNS works for a variety of workloads. It does reach its limits if a cluster has a high amount of DNS requests, though. The cluster-proportional autoscaling approach allows to fine-tune the amount of CoreDNS replicas. It helps to scale in clusters of changing size. However, please keep in mind that you need to cater for the maximum amount of DNS requests as the replicas will not be adapted according to the workload, but only according to the cluster size (cores/nodes).</p><h2 id=trade-offs-in-conjunction-with-nodelocaldns>Trade-Offs in Conjunction with NodeLocalDNS</h2><p>Using a <a href=/docs/gardener/networking/node-local-dns/>node-local DNS cache</a> can mitigate a lot of the potential DNS related problems. It works fine with a DNS workload that can be handle through the cache and reduces the inter-node DNS communication. As <a href=/docs/gardener/networking/node-local-dns/>node-local DNS cache</a> reduces the amount of traffic being sent to the cluster&rsquo;s CoreDNS replicas, it usually works fine with horizontally scaled CoreDNS. Nevertheless, it also works with CoreDNS scaled in a cluster-proportional approach. In this mode, though, it might make sense to adapt the default settings as the CoreDNS workload is likely significantly reduced.</p><p>Overall, you can view the DNS options on a scale. Horizontally scaled DNS provides a small amount of DNS servers. Especially for bigger clusters, a cluster-proportional approach will yield more CoreDNS instances and hence may yield a more balanced DNS solution. By adapting the settings you can further increase the amount of CoreDNS replicas. On the other end of the spectrum, a <a href=/docs/gardener/networking/node-local-dns/>node-local DNS cache</a> provides DNS on every node and allows to reduce the amount of (backend) CoreDNS instances regardless if they are horizontally or cluster-proportionally scaled.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-aee348cd1cdd18936d8abea35f2b24c1>2 - Shoot Autoscaling</h1><div class=lead>The basics of horizontal Node and vertical Pod auto-scaling</div><h1 id=auto-scaling-in-shoot-clusters>Auto-Scaling in Shoot Clusters</h1><p>There are three auto-scaling scenarios of relevance in Kubernetes clusters in general and Gardener shoot clusters in particular:</p><ul><li>Horizontal node auto-scaling, i.e., dynamically adding and removing worker nodes.</li><li>Horizontal pod auto-scaling, i.e., dynamically adding and removing pod replicas.</li><li>Vertical pod auto-scaling, i.e., dynamically raising or shrinking the resource requests/limits of pods.</li></ul><p>This document provides an overview of these scenarios and how the respective auto-scaling components can be enabled and configured. For more details, please see our <a href=/docs/gardener/autoscaling/shoot_pod_autoscaling_best_practices/>pod auto-scaling best practices</a>.</p><h2 id=horizontal-node-auto-scaling>Horizontal Node Auto-Scaling</h2><p>Every shoot cluster that has at least one worker pool with <code>minimum &lt; maximum</code> nodes configuration will get a <code>cluster-autoscaler</code> deployment.
Gardener is leveraging the upstream community Kubernetes <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler><code>cluster-autoscaler</code> component</a>.
We have forked it to <a href=https://github.com/gardener/autoscaler/>gardener/autoscaler</a> so that it supports the way how Gardener manages the worker nodes (leveraging <a href=https://github.com/gardener/machine-controller-manager>gardener/machine-controller-manager</a>).
However, we have not touched the logic how it performs auto-scaling decisions.
Consequently, please refer to the <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#faqdocumentation>official documentation</a> for this component.</p><p>The <code>Shoot</code> API allows to configure a few flags of the <code>cluster-autoscaler</code>:</p><p>There are <a href=/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.ClusterAutoscaler>general options for <code>cluster-autoscaler</code></a>, and these values will be used for all worker groups except for those overwriting them. Additionally, there are some <a href=/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.ClusterAutoscalerOptions><code>cluster-autoscaler</code> flags to be set per worker pool</a>. They override any general value such as those specified in the general flags above.</p><blockquote><p>Only some <code>cluster-autoscaler</code> flags can be configured per worker pool, and is limited by NodeGroupAutoscalingOptions of the upstream community Kubernetes repository. This list can be found <a href=https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/config/autoscaling_options.go#L37-L55>here</a>.</p></blockquote><h2 id=horizontal-pod-auto-scaling>Horizontal Pod Auto-Scaling</h2><p>This functionality (HPA) is a standard functionality of any Kubernetes cluster (implemented as part of the <code>kube-controller-manager</code> that all Kubernetes clusters have). It is always enabled.</p><p>The <code>Shoot</code> API allows to configure most of the <a href=/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.HorizontalPodAutoscalerConfig>flags of the <code>horizontal-pod-autoscaler</code></a>.</p><h2 id=vertical-pod-auto-scaling>Vertical Pod Auto-Scaling</h2><p>This form of auto-scaling (VPA) is enabled by default, but it can be switched off in the <code>Shoot</code> by setting <code>.spec.kubernetes.verticalPodAutoscaler.enabled=false</code> in case you deploy your own VPA into your cluster (having more than one VPA on the same set of pods will lead to issues, eventually).</p><p>Gardener is leveraging the upstream community Kubernetes <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler><code>vertical-pod-autoscaler</code></a>.
If enabled, Gardener will deploy it as part of the control plane into the seed cluster.
It will also be used for the vertical autoscaling of Gardener&rsquo;s system components deployed into the <code>kube-system</code> namespace of shoot clusters, for example, <code>kube-proxy</code> or <code>metrics-server</code>.</p><p>You might want to refer to the <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md>official documentation</a> for this component to get more information how to use it.</p><p>The <code>Shoot</code> API allows to configure a few <a href=/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.VerticalPodAutoscaler>flags of the <code>vertical-pod-autoscaler</code></a>.</p><p>⚠️ Please note that if you disable VPA, the related <code>CustomResourceDefinition</code>s (ours and yours) will remain in your shoot cluster (whether someone acts on them or not).
You can delete these <code>CustomResourceDefinition</code>s yourself using <code>kubectl delete crd</code> if you want to get rid of them (in case you statically size all resources, which we do not recommend).</p><h1 id=pod-auto-scaling-best-practices>Pod Auto-Scaling Best Practices</h1><p>Please continue reading our <a href=/docs/gardener/autoscaling/shoot_pod_autoscaling_best_practices/>pod auto-scaling best practices</a> for more details and recommendations.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-63667bf36c177cbd64e7f9b2289d3838>3 - Shoot Pod Autoscaling Best Practices</h1><h1 id=introduction>Introduction</h1><p>There are two types of pod autoscaling in Kubernetes: Horizontal Pod Autoscaling (HPA) and Vertical Pod Autoscaling (VPA). HPA (implemented as part of the kube-controller-manager) scales the number of pod replicas, while VPA (implemented as independent community project) adjusts the CPU and memory requests for the pods. Both types of autoscaling aim to optimize resource usage/costs and maintain the performance and (high) availability of applications running on Kubernetes.</p><h2 id=horizontal-pod-autoscaling-hpahttpskubernetesiodocstasksrun-applicationhorizontal-pod-autoscale><a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale>Horizontal Pod Autoscaling (HPA)</a></h2><p>Horizontal Pod Autoscaling involves increasing or decreasing the number of pod replicas in a deployment, replica set, stateful set, or <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-autoscaling/853-configurable-hpa-scale-velocity/README.md#summary>anything really with a scale subresource that manages pods</a>. HPA adjusts the number of replicas based on specified metrics, such as CPU or memory average utilization (usage divided by requests; most common) or average value (usage; less common). When the demand on your application increases, HPA automatically scales out the number of pods to meet the demand. Conversely, when the demand decreases, it scales in the number of pods to reduce resource usage.</p><p>HPA targets (mostly stateless) applications where adding more instances of the application can linearly increase the ability to handle additional load. It is very useful for applications that experience variable traffic patterns, as it allows for real-time scaling without the need for manual intervention.</p><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>HPA continuously monitors the metrics of the targeted pods and adjusts the number of replicas based on the observed metrics. It operates solely on the current metrics when it calculates the averages across all pods, meaning it reacts to the immediate resource usage without considering past trends or patterns. Also, all pods are treated equally based on the average metrics. This could potentially lead to situations where some pods are under high load while others are underutilized. Therefore, particular care must be applied to (fair) load-balancing (connection vs. request vs. actual resource load balancing are crucial).</p></blockquote><h3 id=a-few-words-on-the-cluster-proportional-horizontal-autoscaler-cpahttpsgithubcomkubernetes-sigscluster-proportional-autoscaler-and-the-cluster-proportional-vertical-autoscaler-cpvahttpsgithubcomkubernetes-sigscluster-proportional-vertical-autoscaler>A Few Words on the <a href=https://github.com/kubernetes-sigs/cluster-proportional-autoscaler>Cluster-Proportional (Horizontal) Autoscaler (CPA)</a> and the <a href=https://github.com/kubernetes-sigs/cluster-proportional-vertical-autoscaler>Cluster-Proportional Vertical Autoscaler (CPVA)</a></h3><p>Besides HPA and VPA, CPA and CPVA are further options for scaling horizontally or vertically (neither is deployed by Gardener and must be deployed by the user). Unlike HPA and VPA, CPA and CPVA do not monitor the actual pod metrics, but scale solely on the number of nodes or CPU cores in the cluster. While this approach may be helpful and sufficient in a few rare cases, it is often a risky and crude scaling scheme that we do not recommend. More often than not, cluster-proportional scaling results in either under- or over-reserving your resources.</p><h2 id=vertical-pod-autoscaling-vpahttpsgithubcomkubernetesautoscalertreemastervertical-pod-autoscalerreadme><a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#readme>Vertical Pod Autoscaling (VPA)</a></h2><p>Vertical Pod Autoscaling, on the other hand, focuses on adjusting the CPU and memory resources allocated to the pods themselves. Instead of changing the number of replicas, VPA tweaks the resource requests (and limits, but only <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md#keeping-limit-proportional-to-request>proportionally</a>, if configured) for the pods in a deployment, replica set, stateful set, daemon set, or <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/FAQ.md#how-can-i-apply-vpa-to-my-custom-resource>anything really with a scale subresource that manages pods</a>. This means that each pod can be given more, or fewer resources as needed.</p><p>VPA is very useful for optimizing the resource requests of pods that have dynamic resource needs over time. It does so by mutating pod requests (unfortunately, <a href=https://github.com/kubernetes/design-proposals-archive/blob/main/autoscaling/vertical-pod-autoscaler.md#in-place-updates>not in-place</a>). Therefore, in order to apply new recommendations, pods that are &ldquo;out of bounds&rdquo; (i.e. below a configured/computed lower or above a configured/computed upper recommendation percentile) will be evicted proactively, but also pods that are &ldquo;within bounds&rdquo; may be evicted after a grace period. The corresponding higher-level replication controller will then recreate a new pod that VPA will then mutate to set the currently recommended requests (and proportional limits, if configured).</p><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>VPA continuously monitors all targeted pods and calculates recommendations based on their usage (one recommendation for the entire target). This calculation is influenced by configurable percentiles, with a greater emphasis on recent usage data and a gradual decrease (=decay) in the relevance of older data. However, this means, that VPA doesn&rsquo;t take into account individual needs of single pods - eventually, all pods will receive the same recommendation, which may lead to considerable resource waste. Ideally, VPA would update pods <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/1287-in-place-update-pod-resources/README.md>in-place</a> depending on their individual needs, but that&rsquo;s (individual recommendations) not in its design, even if <a href=https://github.com/kubernetes/design-proposals-archive/blob/main/autoscaling/vertical-pod-autoscaler.md#in-place-updates>in-place updates</a> get implemented, which may be years away for VPA based on current activity on the component.</p></blockquote><h2 id=selecting-the-appropriate-autoscaler>Selecting the Appropriate Autoscaler</h2><p>Before deciding on an autoscaling strategy, it&rsquo;s important to understand the characteristics of your application:</p><ul><li><strong>Interruptibility:</strong> Most importantly, if the clients of your workload are too sensitive to disruptions/cannot cope well with terminating pods, then maybe neither HPA nor VPA is an option (both, HPA and VPA cause pods and connections to be terminated, though VPA even more frequently). Clients must retry on disruptions, which is a reasonable ask in a highly dynamic (and self-healing) environment such as Kubernetes, but this is often not respected (or expected) by your clients (they may not know or care you run the workload in a Kubernetes cluster and have different expectations to the stability of the workload unless you communicated those through <a href=https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos>SLIs/SLOs/SLAs</a>).</li><li><strong>Statelessness:</strong> Is your application stateless or stateful? Stateless applications are typically better candidates for HPA as they can be easily scaled out by adding more replicas without worrying about maintaining state.</li><li><strong>Traffic Patterns:</strong> Does your application experience variable traffic? If so, HPA can help manage these fluctuations by adjusting the number of replicas to handle the load.</li><li><strong>Resource Usage:</strong> Does your application&rsquo;s resource usage change over time? VPA can adjust the CPU and memory reservations dynamically, which is beneficial for applications with non-uniform resource requirements.</li><li><strong>Scalability:</strong> Can your application handle increased load by scaling vertically (more resources per pod) or does it require horizontal scaling (more pod instances)?</li></ul><p>HPA is the right choice if:</p><ul><li>Your application is stateless and can handle increased load by adding more instances.</li><li>You experience short-term fluctuations in traffic that require quick scaling responses.</li><li>You want to maintain a specific performance metric, such as requests per second per pod.</li></ul><p>VPA is the right choice if:</p><ul><li>Your application&rsquo;s resource requirements change over time, and you want to optimize resource usage without manual intervention.</li><li>You want to avoid the complexity of managing resource requests for each pod, especially when they run code where it&rsquo;s impossible for you to suggest static requests.</li></ul><p>In essence:</p><ul><li>For applications that can handle increased load by simply adding more replicas, HPA should be used to handle short-term fluctuations in load by scaling the number of replicas.</li><li>For applications that require more resources per pod to handle additional work, VPA should be used to adjust the resource allocation for longer-term trends in resource usage.</li></ul><p>Consequently, if both cases apply (VPA often applies), HPA and VPA can also be combined. However, combining both, especially on the same metrics (CPU and memory), requires understanding and care to avoid conflicts and ensure that the autoscaling actions do not interfere with and rather complement each other. For more details, see <a href=/docs/gardener/autoscaling/shoot_pod_autoscaling_best_practices/#combining-hpa-and-vpa>Combining HPA and VPA</a>.</p><h1 id=horizontal-pod-autoscaler-hpa>Horizontal Pod Autoscaler (HPA)</h1><p>HPA operates by monitoring resource metrics for all pods in a target. It computes the desired number of replicas from the current average metrics and the desired user-defined metrics <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details>as follows</a>:</p><p><code>desiredReplicas = ceil[currentReplicas * (currentMetricValue / desiredMetricValue)]</code></p><p>HPA checks the metrics at regular intervals, which can be configured by the user. <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-metrics-apis>Several types of metrics</a> are supported (classical resource metrics like CPU and memory, but also custom and external metrics like requests per second or queue length can be configured, if available). If a scaling event is necessary, HPA adjusts the replica count for the targeted resource.</p><h2 id=defining-an-hpa-resource>Defining an HPA Resource</h2><p>To configure HPA, you need to create an HPA resource in your cluster. This resource specifies the target to scale, the metrics to be used for scaling decisions, and the desired thresholds. Here&rsquo;s an example of an HPA configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: autoscaling/v2
</span></span><span style=display:flex><span>kind: HorizontalPodAutoscaler
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: foo-hpa
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  scaleTargetRef:
</span></span><span style=display:flex><span>    apiVersion: apps/v1
</span></span><span style=display:flex><span>    kind: Deployment
</span></span><span style=display:flex><span>    name: foo-deployment
</span></span><span style=display:flex><span>  minReplicas: 1
</span></span><span style=display:flex><span>  maxReplicas: 10
</span></span><span style=display:flex><span>  metrics:
</span></span><span style=display:flex><span>  - type: Resource
</span></span><span style=display:flex><span>    resource:
</span></span><span style=display:flex><span>      name: cpu
</span></span><span style=display:flex><span>      target:
</span></span><span style=display:flex><span>        type: AverageValue
</span></span><span style=display:flex><span>        averageValue: 2
</span></span><span style=display:flex><span>  - type: Resource
</span></span><span style=display:flex><span>    resource:
</span></span><span style=display:flex><span>      name: memory
</span></span><span style=display:flex><span>      target:
</span></span><span style=display:flex><span>        type: AverageValue
</span></span><span style=display:flex><span>        averageValue: 8G
</span></span><span style=display:flex><span>  behavior:
</span></span><span style=display:flex><span>    scaleUp:
</span></span><span style=display:flex><span>      stabilizationWindowSeconds: 30
</span></span><span style=display:flex><span>      policies:
</span></span><span style=display:flex><span>      - type: Percent
</span></span><span style=display:flex><span>        value: 100
</span></span><span style=display:flex><span>        periodSeconds: 60
</span></span><span style=display:flex><span>    scaleDown:
</span></span><span style=display:flex><span>      stabilizationWindowSeconds: 1800
</span></span><span style=display:flex><span>      policies:
</span></span><span style=display:flex><span>      - type: Pods
</span></span><span style=display:flex><span>        value: 1
</span></span><span style=display:flex><span>        periodSeconds: 300
</span></span></code></pre></div><p>In this example, HPA is configured to scale <code>foo-deployment</code> based on pod average CPU and memory usage. It will maintain an average CPU and memory usage (not utilization, which is usage divided by requests!) across all replicas of 2 CPUs and 8G or lower with as few replicas as possible. The number of replicas will be scaled between a minimum of 1 and a maximum of 10 based on this target.</p><p>Since a while, you can also <a href=https://kubernetes.io/blog/2023/05/02/hpa-container-resource-metric>configure the autoscaling based on the resource usage of individual containers</a>, not only on the resource usage of the entire pod. All you need to do is to switch the <code>type</code> from <code>Resource</code> to <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#container-resource-metrics><code>ContainerResource</code> and specify the container name</a>.</p><p>In the official documentation (<a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale>[1]</a> and <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough>[2]</a>) you will find examples with average utilization (<code>averageUtilization</code>), not average usage (<code>averageValue</code>), but this is not particularly helpful, especially if you plan to combine HPA together with VPA on the same metrics (generally discouraged in the documentation). If you want to safely combine both on the same metrics, you should scale on average usage (<code>averageValue</code>) as shown above. For more details, see <a href=/docs/gardener/autoscaling/shoot_pod_autoscaling_best_practices/#combining-hpa-and-vpa>Combining HPA and VPA</a>.</p><p>Finally, the behavior section influences how fast you scale up and down. Most of the time (depends on your workload), you like to scale out faster than you scale in. In this example, the configuration will trigger a scale-out only after observing the need to scale out for 30s (<code>stabilizationWindowSeconds</code>) and will then only scale out at most 100% (<code>value</code> + <code>type</code>) of the current number of replicas every 60s (<code>periodSeconds</code>). The configuration will trigger a scale-in only after observing the need to scale in for 1800s (<code>stabilizationWindowSeconds</code>) and will then only scale in at most 1 pod (<code>value</code> + <code>type</code>) every 300s (<code>periodSeconds</code>). As you can see, scale-out happens quicker than scale-in in this example.</p><h2 id=hpa-actually-kcm-options>HPA (actually KCM) Options</h2><p>HPA is a function of the kube-controller-manager (KCM).</p><p>You can read up the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager>full KCM options</a> online and set most of them conveniently in your <a href=https://github.com/gardener/gardener/blob/957e4c7/example/90-shoot.yaml#L221-L226>Gardener shoot cluster spec</a>:</p><ul><li><code>downscaleStabilization</code> (default 5m): HPA will scale out whenever the formula (in accordance with the behavior section, if present in the HPA resource) yields a higher replica count, but it won&rsquo;t scale in just as eagerly. This option lets you define a trailing time window that HPA must check and only if the recommended replica count is consistently lower throughout the entire time window, HPA will scale in (in accordance with the behavior section, if present in the HPA resource). If at any point in time in that trailing time window the recommended replica count isn&rsquo;t lower, scale-in won&rsquo;t happen. This setting is just a default, if nothing is defined in the behavior section of an HPA resource. The default for the upscale stabilization is 0s and it cannot be set via a KCM option (<a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-autoscaling/853-configurable-hpa-scale-velocity/README.md>downscale stabilization was historically more important than upscale stabilization and when later the behavior sections were added to the HPA resources, upscale stabilization remained missing from the KCM options</a>).</li><li><code>tolerance</code> (default +/-10%): HPA will not scale out or in if the desired replica count is (mathematically as a float) near the actual replica count (see <a href=https://github.com/kubernetes/kubernetes/blob/f3f5dd99ac7bdc61c61c3d587575090c3473ab5a/pkg/controller/podautoscaler/replica_calculator.go#L97-L103>source code</a> for details), which is a form of hysteresis to avoid replica flapping around a threshold.</li></ul><p>There are a few more configurable options of lesser interest:</p><ul><li><p><code>syncPeriod</code> (default 15s): How often HPA retrieves the pods and metrics respectively how often it recomputes and sets the desired replica count.</p></li><li><p><code>cpuInitializationPeriod</code> (default 30s) and <code>initialReadinessDelay</code> (default 5m): Both settings only affect whether or not CPU metrics are considered for scaling decisions. They can be easily misinterpreted as the <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details>official docs</a> are <a href=https://github.com/kubernetes/website/issues/12657>somewhat hard to read</a> (see <a href=https://github.com/kubernetes/kubernetes/blob/f3f5dd99ac7bdc61c61c3d587575090c3473ab5a/pkg/controller/podautoscaler/replica_calculator.go#L399-L418>source code</a> for details, which is more readable, if you ignore the comments). Normally, you have little reason to modify them, but here is what they do:</p><ul><li><code>cpuInitializationPeriod</code>: Defines a grace period after a pod starts during which HPA won&rsquo;t consider CPU metrics of the pod for scaling if the pod is either not ready <strong>or</strong> it is ready, but a given CPU metric is older than the last state transition (to ready). This is to ignore CPU metrics that predate the current readiness while still in initialization to not make scaling decisions based on potentially misleading data. If the pod is ready and a CPU metric was collected after it became ready, it is considered also within this grace period.</li><li><code>initialReadinessDelay</code>: Defines another grace period after a pod starts during which HPA won&rsquo;t consider CPU metrics of the pod for scaling if the pod is not ready <strong>and</strong> it became not ready within this grace period (the docs/comments want to check whether the pod was ever ready, but the <a href=https://github.com/kubernetes/kubernetes/blob/f3f5dd99ac7bdc61c61c3d587575090c3473ab5a/pkg/controller/podautoscaler/replica_calculator.go#L411>code only checks whether the pod condition last transition time to not ready happened within that grace period</a> which it could have <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions>from being ready or simply unknown before</a>). This is to ignore not (ever have been) ready pods while still in initialization to not make scaling decisions based on potentially misleading data. If the pod is ready, it is considered also within this grace period.</li></ul><p>So, regardless of the values of these settings, if a pod is reporting ready and it has a CPU metric from the time after it became ready, that pod and its metric will be considered. This holds true even if the pod becomes ready very early into its initialization. These settings cannot be used to &ldquo;black-out&rdquo; pods for a certain duration before being considered for scaling decisions. Instead, if it is your goal to ignore a potentially resource-intensive initialization phase that could wrongly lead to further scale-out, you would need to configure your pods to not report as ready until that resource-intensive initialization phase is over.</p></li></ul><h2 id=considerations-when-using-hpa>Considerations When Using HPA</h2><ul><li><strong>Selection of metrics:</strong> Besides CPU and memory, HPA can also target custom or external metrics. Pick those (in addition or exclusively), if you guarantee certain SLOs in your SLAs.</li><li><strong>Targeting usage or utilization:</strong> HPA supports usage (absolute) and utilization (relative). Utilization is often preferred in simple examples, but usage is more precise and versatile.</li><li><strong>Compatibility with VPA:</strong> Care must be taken when using HPA in conjunction with VPA, as they can potentially interfere with each other&rsquo;s scaling decisions.</li></ul><h1 id=vertical-pod-autoscaler-vpa>Vertical Pod Autoscaler (VPA)</h1><p>VPA operates by monitoring resource metrics for all pods in a target. It computes a resource requests recommendation from the historic and current resource metrics. VPA checks the metrics at regular intervals, which can be configured by the user. Only CPU and memory are supported. If VPA detects that a pod&rsquo;s resource allocation is too high or too low, it may evict pods (if within the permitted disruption budget), which will trigger the creation of a new pod by the corresponding higher-level replication controller, which will then be mutated by VPA to match resource requests recommendation. This happens in three different components that work together:</p><ul><li><strong>VPA Recommender:</strong> The Recommender observes the historic and current resource metrics of pods and generates recommendations based on this data.</li><li><strong>VPA Updater:</strong> The Updater component checks the recommendations from the Recommender and decides whether any pod&rsquo;s resource requests need to be updated. If an update is needed, the Updater will evict the pod.</li><li><strong>VPA Admission Controller:</strong> When a pod is (re-)created, the Admission Controller modifies the pod&rsquo;s resource requests based on the recommendations from the Recommender. This ensures that the pod starts with the optimal amount of resources.</li></ul><p>Since VPA doesn&rsquo;t support in-place updates, pods will be evicted. You will want to control voluntary evictions by means of <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb>Pod Disruption Budgets (PDBs)</a>. Please make yourself familiar with those and use them.</p><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>PDBs will not always work as expected and can also get in your way, e.g. if the PDB is violated or would be violated, it may possibly block evictions that would actually help your workload, e.g. to get a pod out of an <code>OOMKilled</code> <code>CrashLoopBackoff</code> (if the PDB is or would be violated, not even unhealthy pods would be evicted as they could theoretically become healthy again, which VPA doesn&rsquo;t know). In order to overcome this issue, it is now possible (alpha since Kubernetes <code>v1.26</code> in combination with the feature gate <code>PDBUnhealthyPodEvictionPolicy</code> on the API server, beta and enabled by default since Kubernetes <code>v1.27</code>) to configure the so-called <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy>unhealthy pod eviction policy</a>. The default is still <code>IfHealthyBudget</code> as a change in default would have changed the behavior (as described above), but you can now also set <code>AlwaysAllow</code> at the PDB (<code>spec.unhealthyPodEvictionPolicy</code>). For more information, please check out <a href=https://github.com/kubernetes/kubernetes/issues/72320>this discussion</a>, <a href=https://github.com/kubernetes/kubernetes/pull/105296>the PR</a> and <a href="https://groups.google.com/g/kubernetes-sig-apps/c/_joO4swogKY?pli=1">this document</a> and balance the pros and cons for yourself. In short, the new <code>AlwaysAllow</code> option is probably the better choice in most of the cases while <code>IfHealthyBudget</code> is useful only if you have frequent temporary transitions or for special cases where you have already implemented controllers that depend on the old behavior.</p></blockquote><h2 id=defining-a-vpa-resource>Defining a VPA Resource</h2><p>To configure VPA, you need to create a VPA resource in your cluster. This resource specifies the target to scale, the metrics to be used for scaling decisions, and the policies for resource updates. Here&rsquo;s an example of an VPA configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: autoscaling.k8s.io/v1
</span></span><span style=display:flex><span>kind: VerticalPodAutoscaler
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: foo-vpa
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  targetRef:
</span></span><span style=display:flex><span>    apiVersion: <span style=color:#a31515>&#34;apps/v1&#34;</span>
</span></span><span style=display:flex><span>    kind:       Deployment
</span></span><span style=display:flex><span>    name:       foo-deployment
</span></span><span style=display:flex><span>  updatePolicy:
</span></span><span style=display:flex><span>    updateMode: <span style=color:#a31515>&#34;Auto&#34;</span>
</span></span><span style=display:flex><span>  resourcePolicy:
</span></span><span style=display:flex><span>    containerPolicies:
</span></span><span style=display:flex><span>    - containerName: foo-container
</span></span><span style=display:flex><span>      controlledValues: RequestsOnly
</span></span><span style=display:flex><span>      minAllowed:
</span></span><span style=display:flex><span>        cpu: 50m
</span></span><span style=display:flex><span>        memory: 200M
</span></span><span style=display:flex><span>      maxAllowed:
</span></span><span style=display:flex><span>        cpu: 4
</span></span><span style=display:flex><span>        memory: 16G
</span></span></code></pre></div><p>In this example, VPA is configured to scale <code>foo-deployment</code> requests (<code>RequestsOnly</code>) from 50m cores (<code>minAllowed</code>) up to 4 cores (<code>maxAllowed</code>) and 200M memory (<code>minAllowed</code>) up to 16G memory (<code>maxAllowed</code>) automatically (<code>updateMode</code>). VPA doesn&rsquo;t support in-place updates, so in <code>updateMode</code> <code>Auto</code> it will evict pods under certain conditions and then mutate the requests (and possibly limits if you omit <code>controlledValues</code> or set it to <code>RequestsAndLimits</code>, which is the default) of upcoming new pods.</p><p><a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md#quick-start>Multiple update modes exist</a>. They influence eviction and mutation. The most important ones are:</p><ul><li><code>Off</code>: In this mode, recommendations are computed, but never applied. This mode is useful, if you want to learn more about your workload or if you have a custom controller that depends on VPA&rsquo;s recommendations but shall act instead of VPA.</li><li><code>Initial</code>: In this mode, recommendations are computed and applied, but pods are never proactively evicted to enforce new recommendations over time. This mode is useful, if you want to control pod evictions yourself (similar to the <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies><code>StatefulSet</code> <code>updateStrategy</code> <code>OnDelete</code></a>) or your workload is sensitive to evictions, e.g. some brownfield singleton application or a daemon set pod that is critical for the node.</li><li><code>Auto</code> (default): In this mode, recommendations are computed, applied, and pods are even proactively evicted to enforce new recommendations over time. This applies recommendations continuously without you having to worry too much.</li></ul><p>As mentioned, <code>controlledValues</code> influences whether only requests or requests and limits are scaled:</p><ul><li><code>RequestsOnly</code>: Updates only requests and doesn&rsquo;t change limits. Useful if you have defined absolute limits (unrelated to the requests).</li><li><code>RequestsAndLimits</code> (default): Updates requests and proportionally scales limits along with the requests. Useful if you have defined relative limits (related to the requests). In this case, the gap between requests and limits should be either zero for QoS <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#guaranteed><code>Guaranteed</code></a> or small for QoS <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#burstable><code>Burstable</code></a> to avoid useless (way beyond the threshold of unhealthy behavior) or absurd (larger than node capacity) values.</li></ul><p>VPA doesn&rsquo;t offer many more settings that can be tuned per VPA resource than you see above (different than HPA&rsquo;s <code>behavior</code> section). However, there is one more that isn&rsquo;t shown above, which allows to <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md#controlling-eviction-behavior-based-on-scaling-direction-and-resource>scale only up or only down (<code>evictionRequirements[].changeRequirement</code>)</a>, in case you need that, e.g. to provide resources when needed, but avoid disruptions otherwise.</p><h2 id=vpa-options>VPA Options</h2><p>VPA is an independent community project that <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md#components-of-vpa>consists of</a> a recommender (computing target recommendations and bounds), an updater (evicting pods that are out of recommendation bounds), and an admission controller (mutating webhook applying the target recommendation to newly created pods). As such, they have independent options.</p><h3 id=vpa-recommender-options>VPA Recommender Options</h3><p>You can read up the <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/FAQ.md#what-are-the-parameters-to-vpa-recommender>full VPA recommender options</a> online and set some of them conveniently in your <a href=https://github.com/gardener/gardener/blob/957e4c7/example/90-shoot.yaml#L298-L307>Gardener shoot cluster spec</a>:</p><ul><li><code>recommendationMarginFraction</code> (default 15%): Safety margin that will be added to the recommended requests.</li><li><code>targetCPUPercentile</code> (default 90%): CPU usage percentile that will be targeted with the CPU recommendation (i.e. recommendation will &ldquo;fit&rdquo; e.g. 90% of the observed CPU usages). This setting is relevant for balancing your requests reservations vs. your costs. If you want to reduce costs, you can reduce this value (higher risk because of potential under-reservation, but lower costs), because CPU is compressible, but then VPA may lack the necessary signals for scale-up as throttling on an otherwise fully utilized node will go unnoticed by VPA. If you want to err on the safe side, you can increase this value, but you will then target more and more a worst case scenario, quickly (maybe even exponentially) increasing the costs.</li><li><code>targetMemoryPercentile</code> (default 90%): Memory usage percentile that will be targeted with the memory recommendation (i.e. recommendation will &ldquo;fit&rdquo; e.g. 90% of the observed memory usages). This setting is relevant for balancing your requests reservations vs. your costs. If you want to reduce costs, you can reduce this value (higher risk because of potential under-reservation, but lower costs), because OOMs will trigger bump-ups, but those will disrupt the workload. If you want to err on the safe side, you can increase this value, but you will then target more and more a worst case scenario, quickly (maybe even exponentially) increasing the costs.</li></ul><p>There are a few more configurable options of lesser interest:</p><ul><li><code>recommenderInterval</code> (default 1m): How often VPA retrieves the pods and metrics respectively how often it recomputes the recommendations and bounds.</li></ul><p>There are many more options that you can only configure if you deploy your own VPA and which we will not discuss here, but you can check them out <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/FAQ.md#what-are-the-parameters-to-vpa-recommender>here</a>.</p><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>Due to an implementation detail (smallest bucket size), VPA cannot create recommendations below <a href=https://github.com/kubernetes/autoscaler/blob/1f89ff92cf87dd3700f74f9b387ae4846aa51846/vertical-pod-autoscaler/pkg/recommender/model/aggregations_config.go#L89-L99>10m cores</a> and <a href=https://github.com/kubernetes/autoscaler/blob/1f89ff92cf87dd3700f74f9b387ae4846aa51846/vertical-pod-autoscaler/pkg/recommender/model/aggregations_config.go#L101-L111>10M memory</a> even if <code>minAllowed</code> is lower.</p></blockquote><h3 id=vpa-updater-options>VPA Updater Options</h3><p>You can read up the <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/FAQ.md#what-are-the-parameters-to-vpa-updater>full VPA updater options</a> online and set some of them conveniently in your <a href=https://github.com/gardener/gardener/blob/957e4c7/example/90-shoot.yaml#L298-L307>Gardener shoot cluster spec</a>:</p><ul><li><code>evictAfterOOMThreshold</code> (default 10m): Pods where at least one container OOMs within this time period since its start will be actively evicted, which will implicitly apply the new target recommendation that will have been <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md#custom-memory-bump-up-after-oomkill>bumped up after <code>OOMKill</code></a>. Please note, the kubelet may evict pods even before an OOM, but only if <code>kube-reserved</code> is underrun, i.e. <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#pod-selection-for-kubelet-eviction>node-level resources are running low</a>. In these cases, eviction will happen first by pod priority and second by how much the usage overruns the requests.</li><li><code>evictionTolerance</code> (default 50%): Defines a threshold below which no further eligible pod will be evited anymore, i.e. limits how many eligible pods may be in eviction in parallel (but at least 1). The <a href=https://github.com/kubernetes/autoscaler/blob/4d0511363eeeff657119797dd8d26e851dcc3459/vertical-pod-autoscaler/pkg/updater/eviction/pods_eviction_restriction.go#L108-L117>threshold is computed as follows</a>: <code>running - evicted > replicas - tolerance</code>. Example: 10 replicas, 9 running, 8 eligible for eviction, 20% tolerance with 10 replicas which amounts to 2 pods, and no pod evicted in this round yet, then <code>9 - 0 > 10 - 2</code> is true and a pod would be evicted, but the next one would be in violation as <code>9 - 1 = 10 - 2</code> and no further pod would be evicted anymore in this round.</li><li><code>evictionRateBurst</code> (default 1): Defines how many eligible pods may be evicted in one go.</li><li><code>evictionRateLimit</code> (default disabled): Defines how many eligible pods may be evicted per second (a value of 0 or -1 disables the rate limiting).</li></ul><p>In general, avoid modifying these eviction settings unless you have good reasons and try to rely on <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb>Pod Disruption Budgets (PDBs)</a> instead. However, <a href=https://github.com/kubernetes/kubernetes/issues/108124>PDBs are not available for daemon sets</a>.</p><p>There are a few more configurable options of lesser interest:</p><ul><li><code>updaterInterval</code> (default 1m): How often VPA evicts the pods.</li></ul><p>There are many more options that you can only configure if you deploy your own VPA and which we will not discuss here, but you can check them out <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/FAQ.md#what-are-the-parameters-to-vpa-updater>here</a>.</p><h2 id=considerations-when-using-vpa>Considerations When Using VPA</h2><ul><li><strong>Initial Resource Estimates:</strong> VPA requires historical resource usage data to base its recommendations on. Until they kick in, your initial resource requests apply and should be sensible.</li><li><strong>Pod Disruption:</strong> When VPA adjusts the resources for a pod, it may need to &ldquo;recreate&rdquo; the pod, which can cause temporary disruptions. This should be taken into account.</li><li><strong>Compatibility with HPA:</strong> Care must be taken when using VPA in conjunction with HPA, as they can potentially interfere with each other&rsquo;s scaling decisions.</li></ul><h1 id=combining-hpa-and-vpa>Combining HPA and VPA</h1><p>HPA and VPA serve different purposes and operate on different axes of scaling. HPA increases or decreases the number of pod replicas based on metrics like CPU or memory usage, effectively scaling the application out or in. VPA, on the other hand, adjusts the CPU and memory reservations of individual pods, scaling the application up or down.</p><p>When used together, these autoscalers can provide both horizontal and vertical scaling. However, they can also conflict with each other if used on the same metrics (e.g. both on CPU or both on memory). In particular, if VPA adjusts the requests, the utilization, i.e. the ratio between usage and requests, will approach 100% (for various reasons not exactly right, but for this consideration, close enough), which may trigger HPA to scale out, if it&rsquo;s configured to scale on utilization below 100% (often seen in simple examples), which will spread the load across more pods, which may trigger VPA again to adjust the requests to match the new pod usages.</p><p>This is a feedback loop and it stems from <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details>HPA&rsquo;s method of calculating the desired number of replicas</a>, which is:</p><p><code>desiredReplicas = ceil[currentReplicas * (currentMetricValue / desiredMetricValue)]</code></p><p>If <code>desiredMetricValue</code> is utilization and VPA adjusts the requests, which changes the utilization, this may inadvertently trigger HPA and create said feedback loop. On the other hand, if <code>desiredMetricValue</code> is usage and VPA adjusts the requests now, this will have no impact on HPA anymore (HPA will always influence VPA, but we can control whether VPA influences HPA).</p><p>Therefore, to safely combine HPA and VPA, consider the following strategies:</p><ul><li><strong>Configure HPA and VPA on different metrics:</strong> One way to avoid conflicts is to use HPA and VPA based on different metrics. For instance, you could configure HPA to scale based on requests per seconds (or another representative custom/external metric) and VPA to adjust CPU and memory requests. This way, each autoscaler operates independently based on its specific metric(s).</li><li><strong>Configure HPA to scale on usage, not utilization, when used with VPA:</strong> Another way to avoid conflicts is to use HPA not on average utilization (<code>averageUtilization</code>), but instead on average usage (<code>averageValue</code>) as replicas driver, which is an absolute metric (requests don&rsquo;t affect usage). This way, you can combine both autoscalers even on the same metrics.</li></ul><h1 id=pod-autoscaling-and-cluster-autoscaler>Pod Autoscaling and Cluster Autoscaler</h1><p>Autoscaling within Kubernetes can be implemented at different levels: pod autoscaling (HPA and VPA) and cluster autoscaling (CA). While pod autoscaling adjusts the number of pod replicas or their resource reservations, cluster autoscaling focuses on the number of nodes in the cluster, so that your pods can be hosted. If your workload isn&rsquo;t static and especially if you make use of pod autoscaling, it only works if you have sufficient node capacity available. The most effective way to do that, without running a worst-case number of nodes, is to configure burstable worker pools in your shoot spec, i.e. define a true minimum node count and a worst-case maximum node count and leave the node autoscaling to Gardener that internally uses the Cluster Autoscaler to provision and deprovision nodes as needed.</p><p>Cluster Autoscaler automatically adjusts the number of nodes by adding or removing nodes based on the demands of the workloads and the available resources. It interacts with the cloud provider&rsquo;s APIs to provision or deprovision nodes as needed. Cluster Autoscaler monitors the utilization of nodes and the scheduling of pods. If it detects that pods cannot be scheduled due to a lack of resources, it will trigger the addition of new nodes to the cluster. Conversely, if nodes are underutilized for some time and their pods can be placed on other nodes, it will remove those nodes to reduce costs and improve resource efficiency.</p><p>Best Practices:</p><ul><li><strong>Resource Buffering:</strong> Maintain a buffer of resources to accommodate temporary spikes in demand without waiting for node provisioning. This can be done by <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-can-i-configure-overprovisioning-with-cluster-autoscaler>deploying pods with low priority that can be preempted when real workloads require resources</a>. This helps in faster pod scheduling and avoids delays in scaling out or up.</li><li><strong>Pod Disruption Budgets (PDBs):</strong> Use <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb>PDBs</a> to ensure that during scale-down events, the availability of applications is maintained as the Cluster Autoscaler will not voluntarily evict a pod if a PDB would be violated.</li></ul><h2 id=interesting-ca-options>Interesting CA Options</h2><p>CA can be configured in your <a href=https://github.com/gardener/gardener/blob/957e4c7/example/90-shoot.yaml#L281-L297>Gardener shoot cluster spec globally</a> and also in parts <a href=https://github.com/gardener/gardener/blob/957e4c7/example/90-shoot.yaml#L48-L53>per worker pool</a>:</p><ul><li>Can only be configured globally:<ul><li><code>expander</code> (default least-waste): Defines the &ldquo;expander&rdquo; algorithm to use during scale-up, see <a href=https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders>FAQ</a>.</li><li><code>scaleDownDelayAfterAdd</code> (default 1h): Defines how long after scaling up a node, a node may be scaled down.</li><li><code>scaleDownDelayAfterFailure</code> (default 3m): Defines how long after scaling down a node failed, scaling down will be resumed.</li><li><code>scaleDownDelayAfterDelete</code> (default 0s): Defines how long after scaling down a node, another node may be scaled down.</li></ul></li><li>Can be configured globally and also overwritten individually per worker pool:<ul><li><code>scaleDownUtilizationThreshold</code> (default 50%): Defines the threshold below which a node becomes eligible for scaling down.</li><li><code>scaleDownUnneededTime</code> (default 30m): Defines the trailing time window the node must be consistently below a certain utilization threshold before it can finally be scaled down.</li></ul></li></ul><p>There are many more options that you can only configure if you deploy your own CA and which we will not discuss here, but you can check them out <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-are-the-parameters-to-ca>here</a>.</p><h1 id=importance-of-monitoring>Importance of Monitoring</h1><p>Monitoring is a critical component of autoscaling for several reasons:</p><ul><li><strong>Performance Insights:</strong> It provides insights into how well your autoscaling strategy is meeting the performance requirements of your applications.</li><li><strong>Resource Utilization:</strong> It helps you understand resource utilization patterns, enabling you to optimize resource allocation and reduce waste.</li><li><strong>Cost Management:</strong> It allows you to track the cost implications of scaling actions, helping you to maintain control over your cloud spending.</li><li><strong>Troubleshooting:</strong> It enables you to quickly identify and address issues with autoscaling, such as unexpected scaling behavior or resource bottlenecks.</li></ul><p>To effectively monitor autoscaling, you should leverage the following tools and metrics:</p><ul><li><strong><a href=https://sigs.k8s.io/metrics-server>Kubernetes Metrics Server</a>:</strong> Collects resource metrics from kubelets and provides them to HPA and VPA for autoscaling decisions (automatically provided by Gardener).</li><li><strong>Prometheus:</strong> An open-source monitoring system that can collect and store custom metrics, providing a rich dataset for autoscaling decisions.</li><li><strong>Grafana/Plutono:</strong> A visualization tool that integrates with Prometheus to create dashboards for monitoring autoscaling metrics and events.</li><li><strong>Cloud Provider Tools:</strong> Most cloud providers offer native monitoring solutions that can be used to track the performance and costs associated with autoscaling.</li></ul><p>Key metrics to monitor include:</p><ul><li><strong>CPU and Memory Utilization:</strong> Track the resource utilization of your pods and nodes to understand how they correlate with scaling events.</li><li><strong>Pod Count:</strong> Monitor the number of pod replicas over time to see how HPA is responding to changes in load.</li><li><strong>Scaling Events:</strong> Keep an eye on scaling events triggered by HPA and VPA to ensure they align with expected behavior.</li><li><strong>Application Performance Metrics:</strong> Track application-specific metrics such as response times, error rates, and throughput.</li></ul><p>Based on the insights gained from monitoring, you may need to adjust your autoscaling configurations:</p><ul><li><strong>Refine Thresholds:</strong> If you notice frequent scaling actions or periods of underutilization or overutilization, adjust the thresholds used by HPA and VPA to better match the workload patterns.</li><li><strong>Update Policies:</strong> Modify VPA update policies if you observe that the current settings are causing too much or too little pod disruption.</li><li><strong>Custom Metrics:</strong> If using custom metrics, ensure they accurately reflect the load on your application and adjust them if they do not.</li><li><strong>Scaling Limits:</strong> Review and adjust the minimum and maximum scaling limits to prevent over-scaling or under-scaling based on the capacity of your cluster and the criticality of your applications.</li></ul><h1 id=quality-of-service-qos>Quality of Service (QoS)</h1><p>A few words on the <a href=https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod>quality of service for pods</a>. Basically, there are 3 classes of QoS and they influence the eviction of pods when <code>kube-reserved</code> is underrun, i.e. <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#pod-selection-for-kubelet-eviction>node-level resources are running low</a>:</p><ul><li><a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#besteffort><code>BestEffort</code></a>, i.e. pods where no container has CPU or memory requests or limits: <strong>Avoid them</strong> unless you have really good reasons. The kube-scheduler will place them just anywhere according to its policy, e.g. <code>balanced</code> or <code>bin-packing</code>, but whatever resources these pods consume, may bring other pods into trouble or even the kubelet and the container runtime itself, if it happens very suddenly.</li><li><a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#burstable><code>Burstable</code></a>, i.e. pods where at least one container has CPU or memory requests and at least one has no limits or limits that don&rsquo;t match the requests: <strong>Prefer them</strong> unless you have really good reasons for the other QoS classes. Always specify proper requests or use VPA to recommend those. <a href=https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-requests-are-scheduled>This helps the kube-scheduler to make the right scheduling decisions</a>. Not having limits will additionally provide upward resource flexibility, if the node is not under pressure.</li><li><a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#guaranteed><code>Guaranteed</code></a>, i.e. pods where all containers have CPU and memory requests and equal limits: <strong>Avoid them</strong> unless you really know the limits or throttling/killing is intended. While &ldquo;Guaranteed&rdquo; sounds like something &ldquo;positive&rdquo; in the English language, this class comes with the downside, that pods will be actively CPU-throttled and will actively go OOM, even if the node is not under pressure and has excess capacity left. Worse, if containers in the pod are under VPA, their CPU requests/limits will often not be scaled up as CPU throttling will go unnoticed by VPA.</li></ul><h1 id=summary>Summary</h1><ul><li>As a rule of thumb, always set CPU and memory requests (or let VPA do that) and always avoid CPU and memory limits.<ul><li>CPU limits aren&rsquo;t helpful on an under-utilized node (=may result in needless outages) and even suppress the signals for VPA to act. On a nearly or fully utilized node, CPU limits are practically irrelevant as only the requests matter, which are translated into CPU shares that provide a fair use of the CPU anyway (see <a href=https://docs.kernel.org/scheduler/sched-design-CFS.html>CFS</a>).<br>Therefore, if you do not know the healthy range, do not set CPU limits. If you as author of the source code know its healthy range, set them to the upper threshold of that healthy range (everything above, from your knowledge of that code, is definitely an unbound busy loop or similar, which is the main reason for CPU limits, besides batch jobs where throttling is acceptable or even desired).</li><li>Memory limits may be more useful, but suffer a similar, though not as negative downside. As with CPU limits, memory limits aren&rsquo;t helpful on an under-utilized node (=may result in needless outages), but different than CPU limits, they result in an OOM, which triggers VPA to provide more memory suddenly (modifies the currently computed recommendations by a configurable factor, defaulting to +20%, see <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md#custom-memory-bump-up-after-oomkill>docs</a>).<br>Therefore, if you do not know the healthy range, do not set memory limits. If you as author of the source code know its healthy range, set them to the upper threshold of that healthy range (everything above, from your knowledge of that code, is definitely an unbound memory leak or similar, which is the main reason for memory limits)</li></ul></li><li>Horizontal Pod Autoscaling (HPA): Use for pods that support horizontal scaling. Prefer scaling on usage, not utilization, as this is more predictable (not dependent on a second variable, namely the current requests) and conflict-free with vertical pod autoscaling (VPA).</li><li>As a rule of thumb, set the initial replicas to the 5th percentile of the actually observed replica count in production. Since HPA reacts fast, this is not as critical, but may help reduce initial load on the control plane early after deployment. However, be cautious when you update the higher-level resource not to inadvertently reset the current HPA-controlled replica count (very easy to make mistake that can lead to catastrophic loss of pods). HPA modifies the replica count directly in the spec and you do not want to overwrite that. Even if it reacts fast, it is not instant (not via a mutating webhook as VPA operates) and the damage may already be done.</li><li>As for minimum and maximum, let your high availability requirements determine the minimum and your theoretical maximum load determine the maximum, flanked with alerts to detect erroneous run-away out-scaling or the actual nearing of your practical maximum load, so that you can intervene.</li><li>Vertical Pod Autoscaling (VPA): Use for containers that have a significant usage (e.g. any container above 50m CPU or 100M memory) and a significant usage spread over time (by more than 2x), i.e. ignore small (e.g. side-cars) or static (e.g. Java statically allocated heap) containers, but otherwise use it to provide the resources needed on the one hand and keep the costs in check on the other hand.</li><li>As a rule of thumb, set the initial requests to the 5th percentile of the actually observed CPU resp. memory usage in production. Since VPA may need some time at first to respond and evict pods, this is especially critical early after deployment. The lower bound, below which pods will be immediately evicted, converges much faster than the upper bound, above which pods will be immediately evicted, but it isn&rsquo;t instant, e.g. after 5 minutes the lower bound is just at 60% of the computed lower bound; after 12 hours the upper bound is still at 300% of the computed upper bound (see <a href=https://github.com/kubernetes/autoscaler/blob/b3a501cbe11e46bea1f8879d39c8436ef03e7139/vertical-pod-autoscaler/pkg/recommender/logic/recommender.go#L118-L143>code</a>). Unlike with HPA, you don&rsquo;t need to be as cautious when updating the higher-level resource in the case of VPA. As long as VPA&rsquo;s mutating webhook (VPA Admission Controller) is operational (which also the VPA Updater checks before evicting pods), it&rsquo;s generally safe to update the higher-level resource. However, if it&rsquo;s not up and running, any new pods that are spawned (e.g. as a consequence of a rolling update of the higher-level resource or for any other reason) will not be mutated. Instead, they will receive whatever requests are currently configured at the higher-level resource, which can lead to catastrophic resource under-reservation. Gardener deploys the VPA Admission Controller in HA - if unhealthy, it is reported under the <code>ControlPlaneHealthy</code> shoot status condition.</li><li>If you have defined absolute limits (unrelated to the requests), configure VPA to only scale the requests or else it will proportionally scale the limits as well, which can easily become useless (way beyond the threshold of unhealthy behavior) or absurd (larger than node capacity):<pre tabindex=0><code>spec:
  resourcePolicy:
    containerPolicies:
    - controlledValues: RequestsOnly
      ...
</code></pre>If you have defined relative limits (related to the requests), the default policy to scale the limits proportionally with the requests is fine, but the gap between requests and limits must be zero for QoS <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#guaranteed><code>Guaranteed</code></a> and should best be small for QoS <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#burstable><code>Burstable</code></a> to avoid useless or absurd limits either, e.g. prefer limits being 5 to at most 20% larger than requests as opposed to being 100% larger or more.</li><li>As a rule of thumb, set <code>minAllowed</code> to the highest observed VPA recommendation (usually during the initialization phase or during any periodical activity) for an otherwise practically idle container, so that you avoid needless trashing (e.g. resource usage calms down over time and recommendations drop consecutively until eviction, which will then lead again to initialization or later periodical activity and higher recommendations and new evictions).<br>⚠️ You may want to provide higher <code>minAllowed</code> values, if you observe that up-scaling takes too long for CPU or memory for a too large percentile of your workload. This will get you out of the danger zone of too few resources for too many pods at the expense of providing too many resources for a few pods. Memory may react faster than CPU, because CPU throttling is not visible and memory gets aided by OOM bump-up incidents, but still, if you observe that up-scaling takes too long, you may want to increase <code>minAllowed</code> accordingly.</li><li>As a rule of thumb, set <code>maxAllowed</code> to your theoretical maximum load, flanked with alerts to detect erroneous run-away usage or the actual nearing of your practical maximum load, so that you can intervene. However, VPA can easily recommend requests larger than what is allocatable on a node, so you must either ensure large enough nodes (Gardener can scale up from zero, in case you like to <a href=https://gardener.cloud/docs/getting-started/features/cluster-autoscaler/#scaling-by-priority>define a low-priority worker pool with more resources</a> for very large pods) and/or cap VPA&rsquo;s target recommendations using <code>maxAllowed</code> at the node allocatable remainder (after daemon set pods) of the largest eligible machine type (may result in under-provisioning resources for a pod). Use your monitoring and check maximum pod usage to decide about the maximum machine type.</li></ul><h2 id=recommendations-in-a-box>Recommendations in a Box</h2><table><thead><tr><th>Container</th><th>When to use</th><th>Value</th></tr></thead><tbody><tr><td>Requests</td><td>- <strong>Set them (recommended)</strong> unless:<br>- Do not set requests for QoS <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#besteffort><code>BestEffort</code></a>; useful only if pod can be evicted as often as needed <strong>and</strong> pod can pick up where it left off without any penalty</td><td>Set <code>requests</code> to <strong>95th percentile (w/o VPA)</strong> of the actually observed CPU resp. memory usage in production resp. <strong>5th percentile (w/ VPA)</strong> (see below)</td></tr><tr><td>Limits</td><td>- <strong>Avoid them (recommended)</strong> unless:<br>- Set limits for QoS <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#guaranteed><code>Guaranteed</code></a>; useful only if pod has strictly static resource requirements<br>- Set CPU limits if you want to throttle CPU usage for containers that can be throttled w/o any other disadvantage than processing time (never do that when time-critical operations like leases are involved)<br>- Set limits if you know the healthy range and want to shield against unbound busy loops, unbound memory leaks, or similar</td><td>If you really can (otherwise not), set <code>limits</code> to healthy theoretical max load</td></tr></tbody></table><table><thead><tr><th>Scaler</th><th>When to use</th><th>Initial</th><th>Minimum</th><th>Maximum</th></tr></thead><tbody><tr><td>HPA</td><td><strong>Use for pods that support horizontal scaling</strong></td><td>Set initial <code>replicas</code> to 5th percentile of the actually observed replica count in production (prefer scaling on usage, not utilization) and make sure to never overwrite it later when controlled by HPA</td><td>Set <code>minReplicas</code> to 0 (requires <a href=https://github.com/kubernetes/kubernetes/blob/4c7960a67c29b7954cccc6c7d77a62133af3484f/pkg/features/kube_features.go#L266-L267>feature gate</a> and <a href=https://github.com/kubernetes/kubernetes/pull/74526>custom/external metrics</a>), to 1 (regular HPA minimum), or whatever the high availability requirements of the workload demand</td><td>Set <code>maxReplicas</code> to healthy theoretical max load</td></tr><tr><td>VPA</td><td><strong>Use for containers that have a significant usage</strong> (>50m/100M) <strong>and a significant usage spread over time</strong> (>2x)</td><td>Set initial <code>requests</code> to 5th percentile of the actually observed CPU resp. memory usage in production</td><td>Set <code>minAllowed</code> to highest observed VPA recommendation (includes start-up phase) for an otherwise practically idle container (avoids pod trashing when pod gets evicted after idling)</td><td>Set <code>maxAllowed</code> to fresh node allocatable remainder after daemonset pods (avoids pending pods when requests exceed fresh node allocatable remainder) or, if you really can (otherwise not), to healthy theoretical max load (less disruptive than limits as no throttling or OOM happens on under-utilized nodes)</td></tr><tr><td>CA</td><td><strong>Use for dynamic workloads</strong>, definitely if you use HPA and/or VPA</td><td>N/A</td><td>Set <code>minimum</code> to 0 or number of nodes required right after cluster creation or wake-up</td><td>Set <code>maximum</code> to healthy theoretical max load</td></tr></tbody></table><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>Theoretical max load may be very difficult to ascertain, especially with modern software that consists of building blocks you do not own or know in detail. If you have comprehensive monitoring in place, you may be tempted to pick the observed maximum and add a safety margin or even factor on top (2x, 4x, or any other number), but this is not to be confused with &ldquo;theoretical max load&rdquo; (solely depending on the code, not observations from the outside). At any point in time, your numbers may change, e.g. because you updated a software component or your usage increased. If you decide to use numbers that are set based only on observations, make sure to flank those numbers with monitoring alerts, so that you have sufficient time to investigate, revise, and readjust if necessary.</p></blockquote><h1 id=conclusion>Conclusion</h1><p>Pod autoscaling is a dynamic and complex aspect of Kubernetes, but it is also one of the most powerful tools at your disposal for maintaining efficient, reliable, and cost-effective applications. By carefully selecting the appropriate autoscaler, setting well-considered thresholds, and continuously monitoring and adjusting your strategies, you can ensure that your Kubernetes deployments are well-equipped to handle your resource demands while not over-paying for the provided resources at the same time.</p><p>As Kubernetes continues to evolve (e.g. <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/1287-in-place-update-pod-resources/README.md>in-place updates</a>) and as new patterns and practices emerge, the approaches to autoscaling may also change. However, the principles discussed above will remain foundational to creating scalable and resilient Kubernetes workloads. Whether you&rsquo;re a developer or operations engineer, a solid understanding of pod autoscaling will be instrumental in the successful deployment and management of containerized applications.</p></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2025 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=/js/main.min.69e2c1ae9320465ab10236d9ef752c6a4442c54b48b883b17c497b7c7d96a796.js integrity="sha256-aeLBrpMgRlqxAjbZ73UsakRCxUtIuIOxfEl7fH2Wp5Y=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>