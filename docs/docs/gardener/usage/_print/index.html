<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.83.1"><link rel=canonical type=text/html href=https://gardener.cloud/docs/gardener/usage/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/gardener/usage/index.xml><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=icon type=image/x-icon href=https://gardener.cloud/images/favicon.ico><link rel=icon type=image/png href=https://gardener.cloud/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://gardener.cloud/images/favicon-16x16.png sizes=16x16><title>Usage | Gardener</title><meta name=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:title" content="Usage"><meta property="og:description" content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/gardener/usage/"><meta itemprop=name content="Usage"><meta itemprop=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta name=twitter:card content="summary"><meta name=twitter:title content="Usage"><meta name=twitter:description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><link rel=preload href=/scss/main.min.ca2e9ddee7809848b536632b41e4e4df665800778ffe11b75edde5bdd6c78963.css as=style><link href=/scss/main.min.ca2e9ddee7809848b536632b41e4e4df665800778ffe11b75edde5bdd6c78963.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://unpkg.com/lunr@2.3.8/lunr.min.js integrity=sha384-vRQ9bDyE0Wnu+lMfm57BlYLO0/XauFuKpVsZPs7KEDwYKktWi5+Kz3MP8++DFlRY crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.2035d9813dafac83fe2a48b18d50f237.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/gardener/usage/>Return to the regular view of this page</a>.</p></div><h1 class=title>Usage</h1><div class=content></div></div><div class=td-content><h1 id=pg-5092bba8dce7cbbc8af74c9d34288317>1 - Hibernate a Cluster</h1><h1 id=hibernate-a-cluster>Hibernate a Cluster</h1><p>Clusters are only needed 24 hours a day if they run productive workload. So whenever you do development in a cluster, or just use it for tests or demo purposes, you can save much money if you scale-down your Kubernetes resources whenever you don&rsquo;t need them. However, scaling them down manually can become time-consuming the more resources you have.</p><p>Gardener offers a clever way to automatically scale-down all resources to zero: cluster hibernation. You can either hibernate a cluster by pushing a button or by defining a hibernation schedule.</p><blockquote><p>To save costs, it&rsquo;s recommended to define a hibernation schedule before the creation of a cluster. You can hibernate your cluster or wake up your cluster manually even if there&rsquo;s a schedule for its hibernation.</p></blockquote><ul><li><a href=#what-is-hibernated>What is hibernated?</a></li><li><a href=#what-isnt-affected-by-the-hibernation>What isn’t affected by the hibernation?</a></li><li><a href=#hibernate-your-cluster-manually>Hibernate your cluster manually</a></li><li><a href=#wake-up-your-cluster-manually>Wake up your cluster manually</a></li><li><a href=#create-a-schedule-to-hibernate-your-cluster>Create a schedule to hibernate your cluster</a></li></ul><h2 id=what-is-hibernated>What is hibernated?</h2><p>When a cluster is hibernated, Gardener scales down worker nodes and the cluster&rsquo;s control plane to free resources at the IaaS provider. This affects:</p><ul><li>Your workload, for example, pods, deployments, custom resources.</li><li>The virtual machines running your workload.</li><li>The resources of the control plane of your cluster.</li></ul><h2 id=what-isnt-affected-by-the-hibernation>What isn’t affected by the hibernation?</h2><p>To scale up everything where it was before hibernation, Gardener doesn’t delete state-related information, that is, information stored in persistent volumes. The cluster state as persistent in <code>etcd</code> is also preserved.</p><h2 id=hibernate-your-cluster-manually>Hibernate your cluster manually</h2><p>To hibernate your cluster you can run the following <code>kubectl</code> command:</p><pre><code>$ kubectl patch shoot -n $NAMESPACE $SHOOT_NAME -p '{&quot;spec&quot;:{&quot;hibernation&quot;:{&quot;enabled&quot;: true}}}'
</code></pre><h2 id=wake-up-your-cluster-manually>Wake up your cluster manually</h2><p>To wake up your cluster you can run the following <code>kubectl</code> command:</p><pre><code>$ kubectl patch shoot -n $NAMESPACE $SHOOT_NAME -p '{&quot;spec&quot;:{&quot;hibernation&quot;:{&quot;enabled&quot;: false}}}'
</code></pre><p><strong>Hibernation schedule is also supported. More details can be found <a href=https://github.com/gardener/gardener/blob/master/pkg/apis/core/v1beta1/types_shoot.go#L335-L348>here</a></strong></p></div><div class=td-content style=page-break-before:always><h1 id=pg-5cc140bc78ef125e0e6636c2d7cd748a>2 - Apiserver Sni Injection</h1><h1 id=apiserversni-environment-variable-injection>APIServerSNI environment variable injection</h1><p>If the Gardener administrator has enabled <code>APIServerSNI</code> feature gate for a particular Seed cluster, then in each Shoot cluster&rsquo;s <code>kube-system</code> namespace a <code>DaemonSet</code> called <code>apiserver-proxy</code> is deployed. It routes traffic to the upstream Shoot Kube APIServer. See the <a href=/docs/gardener/proposals/08-shoot-apiserver-via-sni/>APIServer SNI GEP</a> for more details.</p><p>To skip this extra network hop, a <a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook>mutating webhook</a> called <code>apiserver-proxy.networking.gardener.cloud</code> is deployed next to the API server in the Seed. It adds <code>KUBERNETES_SERVICE_HOST</code> environment variable to each container and init container that do not specify it. See the webhook <a href=https://github.com/gardener/apiserver-proxy/>repository</a> for more information.</p><h2 id=opt-out-of-pod-injection>Opt-out of pod injection</h2><p>In some cases it&rsquo;s desirable to opt-out of Pod injection:</p><ul><li>DNS is disabled on that individual Pod, but it still needs to talk to the kube-apiserver.</li><li>Want to test the <code>kube-proxy</code> and <code>kubelet</code> in-cluster discovery.</li></ul><h3 id=opt-out-of-pod-injection-for-specific-pods>Opt-out of pod injection for specific pods</h3><p>To opt out of the injection, the Pod should be labeled with <code>apiserver-proxy.networking.gardener.cloud/inject: disable</code> e.g.:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
        apiserver-proxy.networking.gardener.cloud/inject: disable
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
</code></pre></div><h3 id=opt-out-of-pod-injection-on-namespace-level>Opt-out of pod injection on namespace level</h3><p>To opt out of the injection of <strong>all</strong> Pods in a namespace, you should label your namespace with <code>apiserver-proxy.networking.gardener.cloud/inject: disable</code> e.g.:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Namespace
metadata:
  labels:
    apiserver-proxy.networking.gardener.cloud/inject: disable
  name: my-namespace
</code></pre></div><p>or via <code>kubectl</code> for existing namespace:</p><pre><code class=language-console data-lang=console>kubectl label namespace my-namespace apiserver-proxy.networking.gardener.cloud/inject=disable
</code></pre><blockquote><p>NOTE: Please be aware that it&rsquo;s not possible to disable injection on namespace level and enable it for individual pods in it.</p></blockquote><h3 id=opt-out-of-pod-injection-for-the-entire-cluster>Opt-out of pod injection for the entire cluster</h3><p>If the injection is causing problems for different workloads and ignoring individual pods or namespaces is not possible, then the feature could be disabled for the entire cluster with the <code>alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector</code> annotation with value <code>disable</code> on the <code>Shoot</code> resource itself:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: Shoot
metadata:
  annotations:
    alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector: <span style=color:#a31515>&#39;disable&#39;</span>
  name: my-cluster
</code></pre></div><p>or via <code>kubectl</code> for existing shoot cluster:</p><pre><code class=language-console data-lang=console>kubectl label shoot my-cluster alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector=disable
</code></pre><blockquote><p>NOTE: Please be aware that it&rsquo;s not possible to disable injection on cluster level and enable it for individual pods in it.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-7e7022c7d80b227a07794be1c66352e7>3 - Configuration</h1><h1 id=gardener-configuration-and-usage>Gardener Configuration and Usage</h1><p>Gardener automates the full lifecycle of Kubernetes clusters as a service.
Additionally, it has several extension points allowing external controllers to plug-in to the lifecycle.
As a consequence, there are several configuration options for the various custom resources that are partially required.</p><p>This document describes the</p><ol><li><a href=#configuration-and-usage-of-gardener-as-operatoradministrator>configuration and usage of Gardener as operator/administrator</a>.</li><li><a href=#configuration-and-usage-of-gardener-as-end-userstakeholdercustomer>configuration and usage of Gardener as end-user/stakeholder/customer</a>.</li></ol><h2 id=configuration-and-usage-of-gardener-as-operatoradministrator>Configuration and Usage of Gardener as Operator/Administrator</h2><p>When we use the terms &ldquo;operator/administrator&rdquo; we refer to both the people deploying and operating Gardener.
Gardener consists of the following components:</p><ol><li><code>gardener-apiserver</code>, a Kubernetes-native API extension that serves custom resources in the Kubernetes-style (like <code>Seed</code>s and <code>Shoot</code>s), and a component that contains multiple admission plugins.</li><li><code>gardener-admission-controller</code>, an HTTP(S) server with several handlers to be used in a <a href=https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/charts/application/templates/validatingwebhook-admission-controller.yaml>ValidatingWebhookConfiguration</a>.</li><li><code>gardener-controller-manager</code>, a component consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for maintaining <code>Shoot</code>s, reconciling <code>Plant</code>s, etc.).</li><li><code>gardener-scheduler</code>, a component that assigns newly created <code>Shoot</code> clusters to appropriate <code>Seed</code> clusters.</li><li><code>gardenlet</code>, a component running in seed clusters and consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for reconciliation and deletion of <code>Shoot</code>s).</li></ol><p>Each of these components have various configuration options.
The <code>gardener-apiserver</code> uses the standard API server library maintained by the Kubernetes community, and as such it mainly supports command line flags.
Other components use so-called componentconfig files that describe their configuration in a Kubernetes-style versioned object.</p><h3 id=configuration-file-for-gardener-admission-controller>Configuration file for Gardener admission controller</h3><p>The Gardener admission controller does only support one command line flag which should be a path to a valid admission-controller configuration file.
Please take a look at <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-admission-controller.yaml>this</a> example configuration.</p><h3 id=configuration-file-for-gardener-controller-manager>Configuration file for Gardener controller manager</h3><p>The Gardener controller manager does only support one command line flag which should be a path to a valid controller-manager configuration file.
Please take a look at <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-controller-manager.yaml>this</a> example configuration.</p><h3 id=configuration-file-for-gardener-scheduler>Configuration file for Gardener scheduler</h3><p>The Gardener scheduler also only supports one command line flag which should be a path to a valid scheduler configuration file.
Please take a look at <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-scheduler.yaml>this</a> example configuration.
Information about the concepts of the Gardener scheduler can be found <a href=/docs/gardener/concepts/scheduler/>here</a></p><h3 id=configuration-file-for-gardenlet>Configuration file for Gardenlet</h3><p>The Gardenlet also only supports one command line flag which should be a path to a valid gardenlet configuration file.
Please take a look at <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml>this</a> example configuration.
Information about the concepts of the Gardenlet can be found <a href=/docs/gardener/concepts/gardenlet/>here</a></p><h3 id=system-configuration>System configuration</h3><p>After successful deployment of the four components you need to setup the system.
Let&rsquo;s first focus on some &ldquo;static&rdquo; configuration.
When the <code>gardenlet</code> starts it scans the <code>garden</code> namespace of the garden cluster for <code>Secret</code>s that have influence on its reconciliation loops, mainly the <code>Shoot</code> reconciliation:</p><ul><li><p><strong>Internal domain secret</strong>, contains the DNS provider credentials (having appropriate privileges) which will be used to create/delete so-called &ldquo;internal&rdquo; DNS records for the Shoot clusters, please see <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-internal-domain.yaml>this</a> for an example.</p><ul><li>This secret is used in order to establish a stable endpoint for shoot clusters which is used internally by all control plane components.</li><li>The DNS records are normal DNS records but called &ldquo;internal&rdquo; in our scenario because only the kubeconfigs for the control plane components use this endpoint when talking to the shoot clusters.</li><li>It is forbidden to change the internal domain secret if there are existing shoot clusters.</li></ul></li><li><p><strong>Default domain secrets</strong> (optional), contain the DNS provider credentials (having appropriate privileges) which will be used to create/delete DNS records for a default domain for shoots (e.g., <code>example.com</code>), please see <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-default-domain.yaml>this</a> for an example.</p><ul><li>Not every end-user/stakeholder/customer has its own domain, however, Gardener needs to create a DNS record for every shoot cluster.</li><li>As landscape operator you might want to define a default domain owned and controlled by you that is used for all shoot clusters that don&rsquo;t specify their own domain.</li><li>If you have multiple default domain secrets defined you can add a priority as an annotation (<code>dns.gardener.cloud/domain-default-priority</code>) to select which domain should be used for new shoots while creation. The domain with the highest priority is selected while shoot creation. If there is no annotation defined the default priority is <code>0</code>, also all non integer values are considered as priority <code>0</code>.</li></ul></li></ul><p>⚠️ Please note that the mentioned domain secrets are only needed if you have at least one seed cluster that is not specifing <code>.spec.settings.shootDNS.enabled=false</code>.
Seeds with this taint don&rsquo;t create any DNS records for shoots scheduled on it, hence, if you only have such seeds, you don&rsquo;t need to create the domain secrets.</p><ul><li><p><strong>Alerting secrets</strong> (optional), contain the alerting configuration and credentials for the <a href=https://prometheus.io/docs/alerting/alertmanager/>AlertManager</a> to send email alerts. It is also possible to configure the monitoring stack to send alerts to an AlertManager not deployed by Gardener to handle alerting. Please see <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-alerting.yaml>this</a> for an example.</p><ul><li>If email alerting is configured:<ul><li>An AlertManager is deployed into each seed cluster that handles the alerting for all shoots on the seed cluster.</li><li>Gardener will inject the SMTP credentials into the configuration of the AlertManager.</li><li>The AlertManager will send emails to the configured email address in case any alerts are firing.</li></ul></li><li>If an external AlertManager is configured:<ul><li>Each shoot has a <a href=https://prometheus.io/docs/introduction/overview/>Prometheus</a> responsible for monitoring components and sending out alerts. The alerts will be sent to a URL configured in the alerting secret.</li><li>This external AlertManager is not managed by Gardener and can be configured however the operator sees fit.</li><li>Supported authentication types are no authentication, basic, or mutual TLS.</li></ul></li></ul></li><li><p><strong>OpenVPN Diffie-Hellmann Key secret</strong> (optional), contains the self-generated Diffie-Hellmann key used by OpenVPN in your landscape, please see <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-openvpn-diffie-hellman.yaml>this</a> for an example.</p><ul><li>If you don&rsquo;t specify a custom key then a default key is used, but for productive landscapes it&rsquo;s recommend to create a landscape-specific key and define it.</li></ul></li><li><p><strong>Global monitoring secrets</strong> (optional), contains basic authentication credentials for the Prometheus aggregating metrics for all clusters.</p><ul><li>These secrets are synced to each seed cluster and used to gain access to the aggregate monitoring components.</li></ul></li></ul><p>Apart from this &ldquo;static&rdquo; configuration there are several custom resources extending the Kubernetes API and used by Gardener.
As an operator/administrator you have to configure some of them to make the system work.</p><h3 id=configuration-and-usage-of-gardener-as-end-userstakeholdercustomer>Configuration and Usage of Gardener as End-User/Stakeholder/Customer</h3><p>As an end-user/stakeholder/customer you are using a Gardener landscape that has been setup for you by another team.
You don&rsquo;t need to care about how Gardener itself has to be configured or how it has to be deployed.
Take a look at <a href=/docs/gardener/concepts/apiserver/>this document</a> - it describes which resources are offered by Gardener.
You may want to have a more detailed look for <code>Project</code>s, <code>SecretBinding</code>s, <code>Shoot</code>s, <code>Plant</code>s, and <code>(Cluster)OpenIDConnectPreset</code>s.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-557189bc540e3079a8a290a1db4e319f>4 - Control Plane Migration</h1><h1 id=control-plane-migration>Control Plane Migration</h1><h2 id=preconditions>Preconditions</h2><p>To be able to use this feature you need to enable the feature gate <code>SeedChange</code> in your <code>gardener-apiserver</code>
by adding the following command flag: <code>--feature-gates=SeedChange=true</code>.</p><p>Also, the involved Seeds need to have enabled BackupBuckets.</p><h2 id=shootstate>ShootState</h2><p><code>ShootState</code> is an API resource which stores non-reconstructible state and data required to completely recreate a <code>Shoot</code>&rsquo;s control plane on a new <code>Seed</code>. The <code>ShootState</code> resource is created on <code>Shoot</code> creation in its <code>Project</code> namespace and the required state/data is persisted during <code>Shoot</code> creation or reconciliation.</p><h2 id=shoot-control-plane-migration>Shoot Control Plane Migration</h2><p>Triggering the migration is done by changing the <code>Shoot</code>&rsquo;s <code>.spec.seedName</code> to a <code>Seed</code> that differs from the <code>.status.seedName</code>, we call this <code>Seed</code> <code>"Destination Seed"</code>. If the Destination <code>Seed</code> does not have a backup and restore configuration, the change to <code>spec.seedName</code> is rejected. Additionally, this Seed must not be set for deletion and must be healthy.</p><p>If the <code>Shoot</code> has different <code>.spec.seedName</code> and <code>.status.seedName</code> a process is started to prepare the Control Plane for migration:</p><ol><li><code>.status.lastOperation</code> is changed to <code>Migrate</code>.</li><li>Kubernetes API Server is stopped and the extension resources are annotated with <code>gardener.cloud/operation=migrate</code>.</li><li>Full snapshot of the ETCD is created and terminating of the Control Plane in the <code>Source Seed</code> is initiated.</li></ol><p>If the process is successful, we update the status of the <code>Shoot</code> by setting the <code>.status.seedName</code> to the null value. That way, a restoration is triggered in the <code>Destination Seed</code> and <code>.status.lastOperation</code> is changed to <code>Restore</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3a01742bcc319ec4dfb0ee2cdce04deb>5 - Csi Components</h1><h1 id=custom-csi-components>(Custom) CSI Components</h1><p>Some provider extensions for Gardener are using CSI components to manage persistent volumes in the shoot clusters.
Additionally, most of the provider extensions are deploying controllers for taking volume snapshots (CSI snapshotter).</p><p>End-users can deploy their own CSI components and controllers into shoot clusters.
In such situations, there are multiple controllers acting on the <code>VolumeSnapshot</code> custom resources (each responsible for those instances associated with their respective driver provisioner types).</p><p>However, this might lead to operational conflicts that cannot be overcome by Gardener alone.
Concretely, Gardener cannot know which custom CSI components were installed by end-users which can lead to issues, especially during shoot cluster deletion.
You can add a label to your custom CSI components indicating that Gardener should not try to remove them during shoot cluster deletion. This means you have to take care of the lifecycle for these components yourself!</p><h2 id=recommendations>Recommendations</h2><p>Custom CSI components are typically regular <code>Deployment</code>s running in the shoot clusters.</p><p><strong>Please label them with the <code>shoot.gardener.cloud/no-cleanup=true</code> label.</strong></p><h2 id=background-information>Background Information</h2><p>When a shoot cluster is deleted, Gardener deletes most Kubernetes resources (<code>Deployment</code>s, <code>DaemonSet</code>s, <code>StatefulSet</code>s, etc.). Gardener will also try to delete CSI components if they are not marked with the above mentioned label.</p><p>This can result in <code>VolumeSnapshot</code> resources still having finalizers that will never be cleaned up.
Consequently, manual intervention is required to clean them up before the cluster deletion can continue.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-84651b4d4651fa66c31fb34b1b24d215>6 - Custom Dns</h1><h1 id=custom-dns-configuration>Custom DNS Configuration</h1><p>Gardener provides Kubernetes-Clusters-As-A-Service where all the system components (e.g., kube-proxy, networking, dns, &mldr;) are managed.
As a result, Gardener needs to ensure and auto-correct additional configuration to those system components to avoid unnecessary down-time.</p><p>In some cases, auto-correcting system components can prevent users from deploying applications on top of the cluster that requires bits of customization, DNS configuration can be a good example.</p><p>To allow for customizations for DNS configuration (that could potentially lead to downtime) while having the option to &ldquo;undo&rdquo;, we utilize the <code>import</code> plugin from CoreDNS [1].
which enables in-line configuration changes.</p><h2 id=how-to-use>How to use</h2><p>To customize your CoreDNS cluster config, you can simply edit a <code>ConfigMap</code> named <code>coredns-custom</code> in the <code>kube-system</code> namespace.
By editing, this <code>ConfigMap</code>, you are modifying CoreDNS configuration, therefore care is advised.</p><p>For example, to apply new config to CoreDNS that would point all <code>.global</code> DNS requests to another DNS pod, simply edit the configuration as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns-custom
  namespace: kube-system
data:
  istio.server: |<span style=color:#a31515>
</span><span style=color:#a31515>    global:8053 {
</span><span style=color:#a31515>            errors
</span><span style=color:#a31515>            cache 30
</span><span style=color:#a31515>            forward . 1.2.3.4
</span><span style=color:#a31515>        }</span>    
  corefile.override: |<span style=color:#a31515>
</span><span style=color:#a31515>         # &lt;some-plugin&gt; &lt;some-plugin-config&gt;
</span><span style=color:#a31515>         debug
</span><span style=color:#a31515>         whoami</span>         
</code></pre></div><p>It is important to have the <code>ConfigMap</code> keys ending with <code>*.server</code> (if you would like to add a new server) or <code>*.override</code>
if you want to customize the current server configuration (it is optional setting both).</p><h2 id=optional-reload-coredns>[Optional] Reload CoreDNS</h2><p>As Gardener is configuring the <code>reload</code> <a href=https://coredns.io/plugins/reload/>plugin</a> of CoreDNS a restart of the CoreDNS components is typically not necessary to propagate <code>ConfigMap</code> changes. However, if you don&rsquo;t want to wait for the default (30s) to kick in, you can roll-out your CoreDNS deployment using:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl -n kube-system rollout restart deploy coredns
</code></pre></div><p>This will reload the config into CoreDNS.</p><p>The approach we follow here was inspired by AKS&rsquo;s approach [2].</p><h2 id=anti-pattern>Anti-Pattern</h2><p>Applying a configuration that is in-compatible with the running version of CoreDNS is an anti-pattern (sometimes plugin configuration changes,
simply applying a configuration can break DNS).</p><p>If incompatible changes are applied by mistake, simply delete the content of the <code>ConfigMap</code> and re-apply.
This should bring the cluster DNS back to functioning state.</p><h2 id=references>References</h2><p>[1] <a href=https://github.com/coredns/coredns/tree/master/plugin/import>Import plugin</a>
[2] <a href=https://docs.microsoft.com/en-us/azure/aks/coredns-custom>AKS Custom DNS</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-da5e6c1f64dc67e784358df56a1ec88d>7 - Dns Autoscaling</h1><h1 id=dns-autoscaling>DNS Autoscaling</h1><p>This is a short guide describing different options how to automatically scale CoreDNS in the shoot cluster.</p><h2 id=background>Background</h2><p>Currently, Gardener uses CoreDNS as DNS server. Per default, it is installed as a deployment into the shoot cluster that is auto-scaled horizontally to cover for QPS-intensive applications. However, doing so does not seem to be enough to completely circumvent DNS bottlenecks such as:</p><ul><li>Cloud provider limits for DNS lookups.</li><li>Unreliable UDP connections that forces a period of timeout in case packets are dropped.</li><li>Unnecessary node hopping since CoreDNS is not deployed on all nodes, and as a result DNS queries end-up traversing multiple nodes before reaching the destination server.</li><li>Inefficient load-balancing of services (e.g., round-robin might not be enough when using IPTables mode).</li><li>Overload of the CoreDNS replicas as the maximum amount of replicas is fixed.</li><li>and more &mldr;</li></ul><p>As an alternative with extended configuration options, Gardener provides cluster-proportional autoscaling of CoreDNS. This guide focuses on the configuration of cluster-proportional autoscaling of CoreDNS and its advantages/disadvantages compared to the horizontal
autoscaling.
Please note that there is also the option to use a <a href=/docs/gardener/usage/node-local-dns/>node-local DNS cache</a>, which helps mitigate potential DNS bottlenecks (see <a href=#trade-offs-in-conjunction-with-nodelocaldns>Trade-offs in conjunction with NodeLocalDNS</a> for considerations regarding using NodeLocalDNS together with one of the CoreDNS autoscaling approaches).</p><h2 id=configuring-cluster-proportional-dns-autoscaling>Configuring cluster-proportional DNS Autoscaling</h2><p>All that needs to be done to enable the usage of cluster-proportional autoscaling of CoreDNS is to set the corresponding option (<code>spec.systemComponents.coreDNS.autoscaling.mode</code>) in the <code>Shoot</code> resource to <code>cluster-proportional</code>:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>...
spec:
  ...
  systemComponents:
    coreDNS:
      autoscaling:
        mode: cluster-proportional
...
</code></pre></div><p>To switch back to horizontal DNS autoscaling you can set the <code>spec.systemComponents.coreDNS.autoscaling.mode</code> to <code>horizontal</code> (or remove the <code>coreDNS</code> section).</p><p>Once the cluster-proportional autoscaling of CoreDNS has been enabled and the Shoot cluster has been reconciled afterwards, a ConfigMap called <code>coredns-autoscaler</code> will be created in the <code>kube-system</code> namespace with the default settings. The content will be similar to the following:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>linear: <span style=color:#a31515>&#39;{&#34;coresPerReplica&#34;:256,&#34;min&#34;:2,&#34;nodesPerReplica&#34;:16}&#39;</span>
</code></pre></div><p>It is possible to adapt the ConfigMap according to your needs in case the defaults do not work as desired. The number of CoreDNS replicas is calculated according to the following formula:</p><pre><code>replicas = max( ceil( cores × 1 / coresPerReplica ) , ceil( nodes × 1 / nodesPerReplica ) )
</code></pre><p>Depending on your needs, you can adjust <code>coresPerReplica</code> or <code>nodesPerReplica</code>, but it is also possible to override <code>min</code> if required.</p><h2 id=trade-offs-of-horizontal-and-cluster-proportional-dns-autoscaling>Trade-offs of horizontal and cluster-proportional DNS Autoscaling</h2><p>The horizontal autoscaling of CoreDNS as implemented by Gardener is fully managed, i.e. you do not need to perform any configuration changes. It scales according to the CPU usage of CoreDNS replicas meaning that it will create new replicas if the existing ones are under heavy load. This approach scales between 2 and 5 instances, which is sufficient for most workloads. In case this is not enough, the cluster-proportional autoscaling approach can be used instead with its more flexible configuration options.</p><p>The cluster-proportional autoscaling of CoreDNS as implemented by Gardener is fully managed, but allows more configuration options to adjust the default settings to your individual needs. It scales according to the cluster size, i.e. if your cluster grows in terms of cores/nodes so will the amount of CoreDNS replicas. However, it does not take the actual workload, e.g. CPU consumption, into account.</p><p>Experience shows that the horizontal autoscaling of CoreDNS works for a variety of workloads. It does reach its limits if a cluster has a high amount of DNS requests, though. The cluster-proportional autoscaling approach allows to fine-tune the amount of CoreDNS replicas. It helps to scale in clusters of changing size. However, please keep in mind that you need to cater for the maximum amount of DNS requests as the replicas will not be adapted according to the workload, but only according to the cluster size (cores/nodes).</p><h2 id=trade-offs-in-conjunction-with-nodelocaldns>Trade-offs in conjunction with NodeLocalDNS</h2><p>Using a <a href=/docs/gardener/usage/node-local-dns/>node-local DNS cache</a> can mitigate a lot of the potential DNS related problems. It works fine with a DNS workload that can be handle through the cache and reduces the inter-node DNS communication. As <a href=/docs/gardener/usage/node-local-dns/>node-local DNS cache</a> reduces the amount of traffic being sent to the cluster&rsquo;s CoreDNS replicas, it usually works fine with horizontally scaled CoreDNS. Nevertheless, it also works with CoreDNS scaled in a cluster-proportional approach. In this mode, though, it might make sense to adapt the default settings as the CoreDNS workload is likely significantly reduced.</p><p>Overall, you can view the DNS options on a scale. Horizontally scaled DNS provides a small amount of DNS servers. Especially for bigger clusters, a cluster-proportional approach will yield more CoreDNS instances and hence may yield a more balanced DNS solution. By adapting the settings you can further increase the amount of CoreDNS replicas. On the other end of the spectrum, a <a href=/docs/gardener/usage/node-local-dns/>node-local DNS cache</a> provides DNS on every node and allows to reduce the amount of (backend) CoreDNS instances regardless if they are horizontally or cluster-proportionally scaled.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2c8dcee576a95f02378e3b89966d6410>8 - Docker Shim Removal</h1><h1 id=kubernetes-dockershim-removal>Kubernetes dockershim removal</h1><h2 id=whats-happening>What&rsquo;s happening?</h2><p>With Kubernetes v1.20 the built-in dockershim <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#dockershim-deprecation>was deprecated</a> and is scheduled to be <a href=https://github.com/kubernetes/enhancements/issues/2221>removed with v1.24</a>.
Don&rsquo;t Panic! The Kubernetes community has <a href=https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/>published a blogpost</a> and an <a href=https://kubernetes.io/blog/2020/12/02/dockershim-faq/>FAQ</a> with more information.</p><p>Gardener also needs to switch from using the built-in dockershim to <code>containerd</code>.
Gardener will not change running Shoot clusters. But changes to the container runtime will be coupled to the K8s version selected by the Shoot:</p><ul><li>starting with K8s version 1.22 Shoots not explicitly selecting a container runtime will get <code>containerd</code> instead of <code>docker</code>. Shoots can still select <code>docker</code> explicitly if needed.</li><li>starting with K8s version 1.23 <code>docker</code> can no longer be selected.</li></ul><p>At this point in time, we have no plans to support other container runtimes, such as <code>cri-o</code>.</p><h2 id=what-should-i-do>What should I do?</h2><p>As a gardener operator:</p><ul><li>add <code>containerd</code> and <code>docker</code> to <code>.spec.machineImages[].versions[].cri.name</code> in your CloudProfile to allow users selecting a container runtime for their Shoots (see below). <strong>Note:</strong> Please take a look at our detailed information regarding <a href=#container-runtime-support-in-gardener-operating-system-extensions>container runtime support in Gardener Operating System Extensions</a></li><li>update your cloud provider extensions to avoid a node rollout when a Shoot is configured from <code>cri: nil</code> to <code>cri.name: docker</code>. <strong>Note:</strong> Please take a look at our detailed information regarding <a href=#stable-worker-node-hash-support-in-gardener-provider-extensions>stable Worker node hash support in Gardener Provider Extensions</a></li></ul><p>As a shoot owner:</p><ul><li><a href=https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/#find-docker-dependencies>check if you have dependencies to the <code>docker</code> container runtime</a>. <strong>Note:</strong> This is not only about your actual workload, but also concerns ops tooling as well as logging, monitoring and metric agents installed on the nodes</li><li>test with <code>containerd</code>:<ul><li>create a new Shoot or add a Worker Pool to an existing one</li><li><a href=/docs/gardener/api-reference/core/#cri>set <code>.spec.provider.workers[].cri.name: containerd</code></a> for your Shoot</li></ul></li><li>once testing is successful, switch to <code>containerd</code> with your production workload. You don&rsquo;t need to wait for kubernetes v1.22, <code>containerd</code> is considered production ready as of today</li><li>if you find dependencies to <code>docker</code>, set <code>.spec.provider.workers[].cri.name: docker</code> explicitly to avoid defaulting to <code>containerd</code> once you update your Shoot to kubernetes v1.22</li></ul><h2 id=timeline>Timeline</h2><ul><li><strong>2021-08-04:</strong> Kubernetes v1.22 released. Shoots using this version get <code>containerd</code> as default container runtime. Shoots can still select <code>docker</code> explicitly if needed.</li><li><strong>2021-12-07:</strong> Kubernetes v1.23 released. Shoots using this version can no longer select <code>docker</code> as container runtime.</li><li><strong>2022-06-28:</strong> Kubernetes v1.21 goes out of maintenance. This is the last version not affected by these changes. Make sure you have tested thoroughly and set the correct configuration for your Shoots!</li><li><strong>2022-10-28:</strong> Kubernetes v1.22 goes out of maintenance. This is the last version that you can use with <code>docker</code> as container runtime. Make sure you have removed any dependencies to <code>docker</code> as container runtime!</li></ul><p>See <a href=https://kubernetes.io/releases/>the official kubernetes documentation</a> for the exact dates for all releases.</p><h2 id=container-runtime-support-in-gardener-operating-system-extensions>Container Runtime support in Gardener Operating System Extensions</h2><table><thead><tr><th>Operating System</th><th>docker support</th><th>containerd support</th></tr></thead><tbody><tr><td>GardenLinux</td><td>✅</td><td>>= v0.3.0</td></tr><tr><td>Ubuntu</td><td>✅</td><td>>= v1.4.0</td></tr><tr><td>SuSE CHost</td><td>✅</td><td>>= v1.14.0</td></tr><tr><td>CoreOS/FlatCar</td><td>✅</td><td>>= v1.8.0</td></tr></tbody></table><p><strong>Note</strong>: If you&rsquo;re using a different Operating System Extension, start evaluating now if it provides support for <code>containerd</code>. Please refer to <a href=/docs/gardener/extensions/operatingsystemconfig/#cri-support>our documentation of the <code>operatingsystemconfig</code> contract</a> to understand how to support <code>containerd</code> for an Operating System Extension.</p><h2 id=stable-worker-node-hash-support-in-gardener-provider-extensions>Stable Worker node hash support in Gardener Provider Extensions</h2><p>Upgrade to these versions to avoid a node rollout when a Shoot is configured from <code>cri: nil</code> to <code>cri.name: docker</code>.</p><table><thead><tr><th>Provider Extension</th><th>Stable worker hash support</th></tr></thead><tbody><tr><td>Alicloud</td><td>>= 1.26.0</td></tr><tr><td>AWS</td><td>>= 1.27.0</td></tr><tr><td>Azure</td><td>>= 1.21.0</td></tr><tr><td>GCP</td><td>>= 1.18.0</td></tr><tr><td>OpenStack</td><td>>= 1.21.0</td></tr><tr><td>vSphere</td><td>>= 0.11.0</td></tr></tbody></table><p><strong>Note</strong>: If you&rsquo;re using a different Provider Extension, start evaluating now if it keeps the worker hash stable when switching from <code>.spec.provider.workers[].cri: nil</code> to <code>.spec.provider.workers[].cri.name: docker</code>. This doesn&rsquo;t impact functional correctness, however, a node rollout will be triggered when users decide to configure <code>docker</code> for their shoots.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c846de8dc75ecb3d656ca901da206be3>9 - Exposureclasses</h1><h1 id=exposureclasses>ExposureClasses</h1><p>The Gardener API server provides a cluster-scoped <code>ExposureClass</code> resource.
This resource is used to allow exposing the control plane of a Shoot cluster in various network environments like restricted corporate networks, DMZ etc.</p><h2 id=background>Background</h2><p>The <code>ExposureClass</code> resource is based on the concept for the <code>RuntimeClass</code> resource in Kubernetes.</p><p>A <code>RuntimeClass</code> abstracts the installation of a certain container runtime (e.g. gVisor, Kata Containers) on all nodes or a subset of the nodes in a Kubernetes cluster.
See <a href=https://kubernetes.io/docs/concepts/containers/runtime-class/>here</a>.</p><p>In contrast, an <code>ExposureClass</code> abstracts the ability to expose a Shoot clusters control plane in certain network environments (e.g. corporate networks, DMZ, internet) on all Seeds or a subset of the Seeds.</p><p>Example: <code>RuntimeClass</code> and <code>ExposureClass</code></p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: gvisor
handler: gvisorconfig
<span style=color:green># scheduling:</span>
<span style=color:green>#   nodeSelector:</span>
<span style=color:green>#     env: prod</span>
---
kind: ExposureClass
metadata:
  name: internet
handler: internet-config
<span style=color:green># scheduling:</span>
<span style=color:green>#   seedSelector:</span>
<span style=color:green>#     matchLabels:</span>
<span style=color:green>#       network/env: internet</span>
</code></pre></div><p>Similar to <code>RuntimeClasses</code>, <code>ExposureClasses</code> also define a <code>.handler</code> field reflecting the name reference for the corresponding CRI configuration of the <code>RuntimeClass</code> and the control plane exposure configuration for the <code>ExposureClass</code>.</p><p>The CRI handler for <code>RuntimeClasses</code> is usually installed by an administrator (e.g. via a <code>DaemonSet</code> which installs the corresponding container runtime on the nodes).
The control plane exposure configuration for <code>ExposureClasses</code> will be also provided by an administrator.
This exposure configuration is part of the Gardenlet configuration as this component is responsible to configure the control plane accordingly.
See <a href=#Gardenlet-Configuration-ExposureClass-handlers>here</a>.</p><p>The <code>RuntimeClass</code> also supports the selection of a node subset (which have the respective controller runtime binaries installed) for pod scheduling via its <code>.scheduling</code> section.
The <code>ExposureClass</code> also supports the selection of a subset of available Seed clusters whose Gardenlet is capable of applying the exposure configuration for the Shoot control plane accordingly via its <code>.scheduling</code> section.</p><h2 id=usage-by-a-shoot>Usage by a <code>Shoot</code></h2><p>A <code>Shoot</code> can reference an <code>ExposureClass</code> via the <code>.spec.exposureClassName</code> field.</p><p>⚠️ When creating a <code>Shoot</code> resource, the Gardener scheduler will try to assign the <code>Shoot</code> to a <code>Seed</code> which will host its control plane.
The scheduling behaviour can be influenced via the <code>.spec.seedSelectors</code> and/or <code>.spec.tolerations</code> fields in the <code>Shoot</code>.
<code>ExposureClass</code>es can contain also scheduling instructions.
If a <code>Shoot</code> is referencing an <code>ExposureClass</code> then the scheduling instructions of both will be merged into the <code>Shoot</code>.
Those unions of scheduling instructions might lead to a selection of a <code>Seed</code> which is not able to deal with the <code>handler</code> of the <code>ExposureClass</code> and the <code>Shoot</code> creation might end up in an error.
In such case, the <code>Shoot</code> scheduling instructions should be revisited to check that they are not interfere with the ones from the <code>ExposureClass</code>.
If this is not feasible then the combination with the <code>ExposureClass</code> is might not possible and you need to contact your Gardener administrator.</p><details><summary>Example: Shoot and ExposureClass scheduling instructions merge flow</summary><ol><li>Assuming there is the following <code>Shoot</code> which is referencing the <code>ExposureClass</code> below:</li></ol><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: Shoot
metadata:
  name: abc
  namespace: garden-dev
spec:
  exposureClassName: abc
  seedSelectors:
    matchLabels:
      env: prod
---
apiVersion: core.gardener.cloud/v1alpha1
kind: ExposureClass
metadata:
  name: abc
handler: abc
scheduling:
  seedSelector:
    matchLabels:
      network: internal
</code></pre></div><ol start=2><li>Both <code>seedSelectors</code> would be merged into the <code>Shoot</code>. The result would be the following:</li></ol><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1alpha1
kind: Shoot
metadata:
  name: abc
  namespace: garden-dev
spec:
  exposureClassName: abc
  seedSelectors:
    matchLabels:
      env: prod
      network: internal
</code></pre></div><ol start=3><li>Now the Gardener Scheduler would try to find a <code>Seed</code> with those labels.</li></ol><ul><li>If there are <strong>no</strong> Seeds with matching labels for the seed selector then the <code>Shoot</code> will be unschedulable</li><li>If there are Seeds with matching labels for the seed selector then the Shoot will be assigned to the best candidate after the scheduling strategy is applied, see <a href=/docs/gardener/concepts/scheduler/#algorithm-overview>here</a><ul><li>If the <code>Seed</code> is <strong>not</strong> able to serve the <code>ExposureClass</code> handler <code>abc</code> then the Shoot will end up in error state</li><li>If the <code>Seed</code> is able to serve the <code>ExposureClass</code> handler <code>abc</code> then the <code>Shoot</code> will be created</li></ul></li></ul></details><h2 id=gardenlet-configuration-exposureclass-handlers>Gardenlet Configuration <code>ExposureClass</code> handlers</h2><p>The Gardenlet is responsible to realize the control plane exposure strategy defined in the referenced <code>ExposureClass</code> of a <code>Shoot</code>.</p><p>Therefore, the <code>GardenletConfiguration</code> can contain an <code>.exposureClassHandlers</code> list with the respective configuration.</p><p>Example of the <code>GardenletConfiguration</code>:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>exposureClassHandlers:
- name: internet-config
  loadBalancerService:
    annotations:
      loadbalancer/network: internet
- name: internal-config
  loadBalancerService:
    annotations:
      loadbalancer/network: internal
  sni:
    ingress:
      namespace: ingress-internal
      labels:
        network: internal
</code></pre></div><p>Each Gardenlet can define how the handler of a certain <code>ExposureClass</code> needs to be implemented for the Seed(s) where it is responsible for.</p><p>The <code>.name</code> is the name of the handler config and it must match to the <code>.handler</code> in the <code>ExposureClass</code>.</p><p>All control planes on a <code>Seed</code> are exposed via a load balancer.
Either a dedicated one or a central shared one.
The load balancer service needs to be configured in a way that it is reachable from the target network environment.
Therefore, the configuration of load balancer service need to be specified which can be done via the <code>.loadBalancerService</code> section.
The common way to influence load balancer service behaviour is via annotations where the respective cloud-controller-manager will react on and configure the infrastructure load balancer accordingly.</p><p>In case the Gardenlet runs with activated <code>APIServerSNI</code> feature flag (default), the control planes on a <code>Seed</code> will be exposed via a central load balancer and with Envoy via TLS SNI passthrough proxy.
In this case, the Gardenlet will install a dedicated ingress gateway (Envoy + load balancer + respective configuration) for each handler on the <code>Seed</code>.
The configuration of the ingress gateways can be controlled via the <code>.sni</code> section in the same way like for the default ingress gateways.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e25b3c63361fa049e4ac90bb38e50ff1>10 - Istio</h1><h1 id=istio>Istio</h1><p><a href=https://istio.io>Istio</a> offers a service mesh implementation with focus on several important features - traffic, observability, security and policy.</p><h2 id=gardener-managedistio-feature-gate>Gardener <code>ManagedIstio</code> feature gate</h2><p>When enabled in gardenlet the <code>ManagedIstio</code> feature gate can be used to deploy a Gardener-tailored Istio installation in Seed clusters. It&rsquo;s main usage is to enable features such as <a href=/docs/gardener/proposals/08-shoot-apiserver-via-sni/>Shoot API server SNI</a>. This feature should not be enabled on a Seed cluster where Istio is already deployed.</p><h2 id=prerequisites>Prerequisites</h2><ul><li>Third-party JWT is used, therefore each Seed cluster where this feature is enabled must have <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> enabled.</li><li>Kubernetes 1.16+</li></ul><h2 id=differences-with-istios-default-profile>Differences with Istio&rsquo;s default profile</h2><p>The <a href=https://istio.io/docs/setup/additional-setup/config-profiles/>default profile</a> which is recommended for production deployment, is not suitable for the Gardener use case as it offers more functionality than desired. The current installation goes through heavy refactorings due to the <code>IstioOperator</code> and the mixture of Helm values + Kubernetes API specification makes configuring and fine-tuning it very hard. A more simplistic deployment is used by Gardener. The differences are the following:</p><ul><li>Telemetry is not deployed.</li><li><code>istiod</code> is deployed.</li><li><code>istio-ingress-gateway</code> is deployed in a separate <code>istio-ingress</code> namespace.</li><li><code>istio-egress-gateway</code> is not deployed.</li><li>None of the Istio addons are deployed.</li><li>Mixer (deprecated) is not deployed</li><li>Mixer CDRs are not deployed.</li><li>Kubernetes <code>Service</code>, Istio&rsquo;s <code>VirtualService</code> and <code>ServiceEntry</code> are <strong>NOT</strong> advertised in the service mesh. This means that if a <code>Service</code> needs to be accessed directly from the Istio Ingress Gateway, it should have <code>networking.istio.io/exportTo: "*"</code> annotation. <code>VirtualService</code> and <code>ServiceEntry</code> must have <code>.spec.exportTo: ["*"]</code> set on them respectively.</li><li>Istio injector is not enabled.</li><li>mTLS is enabled by default.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c25983cd195ac0d27b90213377bdf169>11 - Logging</h1><h1 id=logging-stack>Logging stack</h1><h3 id=motivation>Motivation</h3><p>Kubernetes uses the underlying container runtime logging, which does not persist logs for stopped and destroyed containers. This makes it difficult to investigate issues in the very common case of not running containers. Gardener provides a solution to this problem for the managed cluster components, by introducing its own logging stack.</p><h3 id=components>Components:</h3><p><img src=/__resources/logging-architecture_c8dc32.png alt></p><ul><li>A Fluent-bit daemonset which works like a log collector and custom custom Golang plugin which spreads log messages to their Loki instances</li><li>One Loki Statefulset in the <code>garden</code> namespace which contains logs for the seed cluster and one per shoot namespace which contains logs for shoot&rsquo;s controlplane.</li><li>One Grafana Deployment in <code>garden</code> namespace and two Deployments per shoot namespace (one exposed to the end users and one for the operators). Grafana is the UI component used in the logging stack.</li></ul><h3 id=how-to-access-the-logs>How to access the logs</h3><p>The first step is to authenticate in front of the Grafana ingress. The secret with the credentials can be found in <code>garden-&lt;project></code> namespace under <code>&lt;shoot-name>.monitoring</code>.
Logs are accessible via Grafana UI. Its URL can be found in the <code>Logging and Monitoring</code> section of a cluster in the Gardener Dashboard.</p><p>There are two methods to explore logs:</p><ul><li><p>The first option is to use the <code>Explore</code> view (available at the left side of the screen). It is used for creating log queries using the predefined filters in Loki. For example:
<code>{pod_name='prometheus-0'}</code>
or with regex:
<code>{pod_name=~'prometheus.+'}</code></p></li><li><p>The other option is to use <code>Dashboards</code> panel. There are custom dashboards for pod logs with one selector field for <code>pod_name</code> and one <code>search</code> field. The <code>search</code> field allows to filter the logs for a particular string. The following dashboards can be used for logs:</p><ul><li>Garden Grafana<ul><li>Pod Logs</li><li>Extensions</li><li>Systemd Logs</li></ul></li><li>User Grafana<ul><li>Kube Apiserver</li><li>Kube Controller Manager</li><li>Kube Scheduler</li><li>Cluster Autoscaler * Operator Grafana</li></ul></li><li>Operator Grafana<ul><li>All user&rsquo;s dashboards</li><li>Kubernetes Pods</li></ul></li></ul></li></ul><h3 id=expose-logs-for-component-to-user-grafana>Expose logs for component to User Grafana</h3><p>Exposing logs for a new component to the User&rsquo;s Grafana is described <a href=/docs/gardener/extensions/logging-and-monitoring/#how-to-expose-logs-to-the-users>here</a></p><h3 id=configuration>Configuration</h3><h4 id=fluent-bit>Fluent-bit</h4><p>The Fluent-bit configurations can be found on <code>charts/seed-bootstrap/charts/fluent-bit/templates/fluent-bit-configmap.yaml</code>
There are five different specifications:</p><ul><li>SERVICE: Defines the location of the server specifications</li><li>INPUT: Defines the location of the input stream of the logs</li><li>OUTPUT: Defines the location of the output source (Loki for example)</li><li>FILTER: Defines filters which match specific keys</li><li>PARSER: Defines parsers which are used by the filters</li></ul><h4 id=loki>Loki</h4><p>The Loki configurations can be found on <code>charts/seed-bootstrap/charts/loki/templates/loki-configmap.yaml</code></p><p>The main specifications there are:</p><ul><li>Index configuration: Currently is used the following one:</li></ul><pre><code>    schema_config:
      configs:
      - from: 2018-04-15
        store: boltdb
        object_store: filesystem
        schema: v11
        index:
          prefix: index_
          period: 24h
</code></pre><ul><li><code>from</code>: is the date from which logs collection is started. Using a date in the past is okay.</li><li><code>store</code>: The DB used for storing the index.</li><li><code>object_store</code>: Where the data is stored</li><li><code>schema</code>: Schema version which should be used (v11 is currently recommended)</li><li><code>index.prefix</code>: The prefix for the index.</li><li><code>index.period</code>: The period for updating the indices</li></ul><p><strong>Adding of new index happens with new config block definition. <code>from</code> field should start from the current day + previous <code>index.period</code> and should not overlap with the current index. The <code>prefix</code> also should be different</strong></p><pre><code>    schema_config:
      configs:
      - from: 2018-04-15
        store: boltdb
        object_store: filesystem
        schema: v11
        index:
          prefix: index_
          period: 24h
      - from: 2020-06-18
        store: boltdb
        object_store: filesystem
        schema: v11
        index:
          prefix: index_new_
          period: 24h
</code></pre><ul><li>chunk_store_config Configuration</li></ul><pre><code>    chunk_store_config: 
      max_look_back_period: 336h
</code></pre><p><strong><code>chunk_store_config.max_look_back_period</code> should be the same as the <code>retention_period</code></strong></p><ul><li>table_manager Configuration</li></ul><pre><code>    table_manager:
      retention_deletes_enabled: true
      retention_period: 336h
</code></pre><p><code>table_manager.retention_period</code> is the living time for each log message. Loki will keep messages for sure for (<code>table_manager.retention_period</code> - <code>index.period</code>) time due to specification in the Loki implementation.</p><h4 id=grafana>Grafana</h4><p>The Grafana configurations can be found on <code>charts/seed-bootstrap/charts/templates/grafana/grafana-datasources-configmap.yaml</code> and
<code>charts/seed-monitoring/charts/grafana/tempates/grafana-datasources-configmap.yaml</code></p><p>This is the Loki configuration that Grafana uses:</p><pre><code>    - name: loki
      type: loki
      access: proxy
      url: http://loki.{{ .Release.Namespace }}.svc:3100
      jsonData:
        maxLines: 5000
</code></pre><ul><li><code>name</code>: is the name of the datasource</li><li><code>type</code>: is the type of the datasource</li><li><code>access</code>: should be set to proxy</li><li><code>url</code>: Loki&rsquo;s url</li><li><code>svc</code>: Loki&rsquo;s port</li><li><code>jsonData.maxLines</code>: The limit of the log messages which Grafana will show to the users.</li></ul><p><strong>Decrease this value if the browser works slowly!</strong></p></div><div class=td-content style=page-break-before:always><h1 id=pg-b03cef45b6b25f644cf6ab64d3c5c66c>12 - Managed See</h1><h1 id=register-shoot-as-seed>Register Shoot as Seed</h1><p>An existing shoot can be registered as a seed by creating a <code>ManagedSeed</code> resource. This resource replaces the <code>use-as-seed</code> annotation that was previously used to create <a href=/docs/gardener/usage/shooted_seed/>shooted seeds</a>, and that is already deprecated. It contains:</p><ul><li>The name of the shoot that should be registered as seed.</li><li>An optional <code>seedTemplate</code> section that contains the <code>Seed</code> spec and parts of its metadata, such as labels and annotations.</li><li>An optional <code>gardenlet</code> section that contains:<ul><li><code>gardenlet</code> deployment parameters, such as the number of replicas, the image, etc.</li><li>The <code>GardenletConfiguration</code> resource that contains controllers configuration, feature gates, and a <code>seedConfig</code> section that contains the <code>Seed</code> spec and parts of its metadata.</li><li>Additional configuration parameters, such as the garden connection bootstrap mechanism (see <a href=/docs/gardener/concepts/gardenlet/#tls-bootstrapping>TLS Bootstrapping</a>), and whether to merge the provided configuration with the configuration of the parent <code>gardenlet</code>.</li></ul></li></ul><p>Either the <code>seedTemplate</code> or the <code>gardenlet</code> section must be specified, but not both:</p><ul><li>If the <code>seedTemplate</code> section is specified, <code>gardenlet</code> is not deployed to the shoot, and a new <code>Seed</code> resource is created based on the template.</li><li>If the <code>gardenlet</code> section is specified, <code>gardenlet</code> is deployed to the shoot, and it registers a new seed upon startup based on the <code>seedConfig</code> section of the <code>GardenletConfiguration</code> resource.</li></ul><p>Note the following important aspects:</p><ul><li>Unlike the <code>Seed</code> resource, the <code>ManagedSeed</code> resource is namespaced. Currently, managed seeds are restricted to the <code>garden</code> namespace.</li><li>The newly created <code>Seed</code> resource always has the same name as the <code>ManagedSeed</code> resource. Attempting to specify a different name in <code>seedTemplate</code> or <code>seedConfig</code> will fail.</li><li>The <code>ManagedSeed</code> resource must always refer to an existing shoot. Attempting to create a <code>ManagedSeed</code> referring to a non-existing shoot will fail.</li><li>A shoot that is being referred to by a <code>ManagedSeed</code> cannot be deleted. Attempting to delete such a shoot will fail.</li><li>You can omit practically everything from the <code>seedTemplate</code> or <code>gardenlet</code> section, including all or most of the <code>Seed</code> spec fields. Proper defaults will be supplied in all cases, based either on the most common use cases or the information already available in the <code>Shoot</code> resource.</li><li>Some <code>Seed</code> spec fields, for example the provider type and region, networking CIDRs for pods, services, and nodes, etc., must be the same as the corresponding <code>Shoot</code> spec fields of the shoot that is being registered as seed. Attempting to use different values (except empty ones, so that they are supplied by the defaulting mechanims) will fail.</li></ul><h2 id=deploying-gardenlet-to-the-shoot>Deploying Gardenlet to the Shoot</h2><p>To register a shoot as a seed and deploy <code>gardenlet</code> to the shoot using a default configuration, create a <code>ManagedSeed</code> resource similar to the following:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: seedmanagement.gardener.cloud/v1alpha1
kind: ManagedSeed
metadata:
  name: my-managed-seed
  namespace: garden
spec:
  shoot:
    name: crazy-botany
  gardenlet: {}
</code></pre></div><p>For an example that uses non-default configuration, see <a href=https://github.com/gardener/gardener/blob/master/example/55-managedseed-gardenlet.yaml>55-managed-seed-gardenlet.yaml</a></p><h2 id=creating-a-seed-from-a-template>Creating a Seed from a Template</h2><p>To register a shoot as a seed from a template without deploying <code>gardenlet</code> to the shoot using a default configuration, create a <code>ManagedSeed</code> resource similar to the following:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: seedmanagement.gardener.cloud/v1alpha1
kind: ManagedSeed
metadata:
  name: my-managed-seed
  namespace: garden
spec:
  shoot:
    name: crazy-botany
  seedTemplate:
    spec:
      dns:
        ingressDomain: <span style=color:#a31515>&#34;&#34;</span>
      networks:
        pods: <span style=color:#a31515>&#34;&#34;</span>
        services: <span style=color:#a31515>&#34;&#34;</span>
      provider:
        type: <span style=color:#a31515>&#34;&#34;</span>
        region: <span style=color:#a31515>&#34;&#34;</span>
</code></pre></div><p>For an example that uses non-default configuration, see <a href=https://github.com/gardener/gardener/blob/master/example/55-managedseed-seedtemplate.yaml>55-managed-seed-seedtemplate.yaml</a></p><h2 id=migrating-from-the-use-as-seed-annotation-to-managedseeds>Migrating from the <code>use-as-seed</code> Annotation to <code>ManagedSeeds</code></h2><p>If you have existing seeds managed via the <code>use-as-seed</code> annotation, you should migrate them to <code>ManagedSeed</code> resources before support for the annotation has been completely removed from Gardener.</p><p>The <em>seed registration controller</em> that is responsible for reconciling the <code>use-as-seed</code> annotation is still functional, However, instead of reconciling the annotation directly as before, it converts it to a <code>ManagedSeed</code> resource and lets the <em>managed seed controller</em> perform the actual reconciliation. Therefore, for every <code>use-as-seed</code> annotation, you already have an equivalent <code>ManagedSeed</code> resource in your cluster. Since it has been created by reconciling an annotation on a shoot, it is also &ldquo;owned&rdquo; by the shoot, that is it contains an <code>ownerReference</code> to the corresponding shoot. This owner reference is used by the seed registration controller to determine that it should continue updating (or deleting) the <code>ManagedSeed</code> as a result of reconciling changes to (or the removal of) the <code>use-as-seed</code> annotation.</p><p>In order to migrate the <code>use-as-seed</code> annotation to a <code>ManagedSeed</code>, you should simply:</p><ul><li>Remove the owner reference to the shoot from the existing <code>ManagedSeed</code> resource.</li><li>Remove the <code>use-as-seed</code> annotation from the <code>Shoot</code> resource.</li><li>From this moment on, update or delete the <code>ManagedSeed</code> directly, instead of indirectly via the <code>use-as-seed</code> annotation.</li></ul><p>If the shoot containing the <code>use-as-seed</code> annotation was created via a yaml file (e.g. via <code>kubectl apply -f</code>), a helm chart, or a script, you should update the corresponding file, template, or script so that it contains or generates the <code>ManagedSeed</code> that you have in your cluster, instead of the <code>use-as-seed</code> annotation. If you use an automated approach, make sure that the owner reference is removed from the existing <code>ManagedSeed</code> before removing the annotation from the <code>Shoot</code>.</p><h3 id=specifying-apiserver-replicas-and-autoscaler-options>Specifying <code>apiServer</code> <code>replicas</code> and <code>autoscaler</code> options</h3><p>A few of <code>use-as-seed</code> configuration options are not supported in a <code>Seed</code> resource, and therefore also not in a <code>ManagedSeed</code>. These options are (from the <a href=/docs/gardener/usage/shooted_seed/>shooted seeds</a> description):</p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>apiServer.autoscaler.minReplicas</code></td><td>Controls the minimum number of <code>kube-apiserver</code> replicas for the shooted seed cluster.</td></tr><tr><td><code>apiServer.autoscaler.maxReplicas</code></td><td>Controls the maximum number of <code>kube-apiserver</code> replicas for the shooted seed cluster.</td></tr><tr><td><code>apiServer.replicas</code></td><td>Controls how many <code>kube-apiserver</code> replicas the shooted seed cluster gets by default.</td></tr></tbody></table><p>For backward compatibility, it is still possible to specify these options via the <code>shoot.gardener.cloud/managed-seed-api-server</code> annotation, using exactly the same syntax as before.</p><p>If you use any of these fields in any or your <code>use-as-seed</code> annotations, instead of removing the annotation completely as mentioned above, simply rename it to <code>managed-seed-api-server</code>, keeping these fields, and removing everything else.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-8d5f560b3aa48903f7142745f2944770>13 - Node Local Dns</h1><h1 id=nodelocaldns-configuration>NodeLocalDNS Configuration</h1><p>This is a short guide describing how to enable DNS caching on the shoot cluster nodes.</p><h2 id=background>Background</h2><p>Currently in Gardener we are using CoreDNS as a deployment that is auto-scaled horizontally to cover for QPS-intensive applications. However, doing so does not seem to be enough to completely circumvent DNS bottlenecks such as:</p><ul><li>Cloud provider limits for DNS lookups.</li><li>Unreliable UDP connections that forces a period of timeout in case packets are dropped.</li><li>Unnecessary node hopping since CoreDNS is not deployed on all nodes, and as a result DNS queries end-up traversing multiple nodes before reaching the destination server.</li><li>Inefficient load-balancing of services (e.g., round-robin might not be enough when using IPTables mode)</li><li>and more &mldr;</li></ul><p>To workaround the issues described above, <code>node-local-dns</code> was introduced. The architecture is described below. The idea is simple:</p><ul><li>For new queries, the connection is upgraded from UDP to TCP and forwarded towards the cluster IP for the original CoreDNS server.</li><li>For previously resolved queries, an immediate response from the same node where the requester workload / pod resides is provided.</li></ul><p><img src=/__resources/node-local-dns_cbcbae.png alt=node-local-dns-architecture></p><h2 id=configuring-nodelocaldns>Configuring NodeLocalDNS</h2><p>All that needs to be done to enable the usage of the <code>node-local-dns</code> feature is to annotate the <code>Shoot</code> resource with the annotation <code>alpha.featuregates.shoot.gardener.cloud/node-local-dns</code> set to <code>"true"</code>:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml> annotations:
   alpha.featuregates.shoot.gardener.cloud/node-local-dns: <span style=color:#a31515>&#34;true&#34;</span>
</code></pre></div><p>It is worth noting that:</p><ul><li>When migrating from IPVS to IPTables, existing pods will continue to leverage the node-local-dns cache.</li><li>When migrating from IPtables to IPVS, only newer pods will be switched to the node-local-dns cache.</li><li>The annotation will take effect during the next shoot reconciliation. This happens automatically once per day in the maintenance period (unless you have disabled it).</li><li>During the reconfiguration of the node-local-dns there might be a short disruption in terms of domain name resolution depending on the setup. Usually, dns requests are repeated for some time as udp is an unreliable protocol, but that strictly depends on the application/way the domain name resolution happens. It is recommended to let the shoot be reconciled during the next maintenance period.</li><li>If a short DNS outage is not a big issue, you can <a href=/docs/gardener/usage/shoot_operations/#immediate-reconciliation>trigger reconciliation</a> directly after setting the annotation.</li><li>Switching node-local-dns off by removing the annotation can be a rather destructive operation that will result in pods without a working dns configuration.</li></ul><p>For more information about <code>node-local-dns</code> please refer to the <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/1024-nodelocal-cache-dns/README.md>KEP</a> or to the <a href=https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/>usage documentation</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-6aba6b44b714cf2748c3009e8eadd0c0>14 - Openidconnect Presets</h1><h1 id=clusteropenidconnectpreset-and-openidconnectpreset>ClusterOpenIDConnectPreset and OpenIDConnectPreset</h1><p>This page provides an overview of ClusterOpenIDConnectPresets and OpenIDConnectPresets, which are objects for injecting <a href=https://openid.net/connect/>OpenIDConnect Configuration</a> into <code>Shoot</code> at creation time. The injected information contains configuration for the Kube API Server and optionally configuration for kubeconfig generation using said configuration.</p><h2 id=openidconnectpreset>OpenIDConnectPreset</h2><p>An OpenIDConnectPreset is an API resource for injecting additional runtime OIDC requirements into a Shoot at creation time. You use label selectors to specify the <code>Shoot</code> to which a given OpenIDConnectPreset applies.</p><p>Using a OpenIDConnectPresets allows project owners to not have to explicitly provide the same OIDC configuration for every <code>Shoot</code> in their <code>Project</code>.</p><p>For more information about the background, see the <a href=https://github.com/gardener/gardener/issues/1161>issue</a> for OpenIDConnectPreset.</p><h3 id=how-openidconnectpreset-works>How OpenIDConnectPreset works</h3><p>Gardener provides an admission controller (OpenIDConnectPreset) which, when enabled, applies OpenIDConnectPresets to incoming <code>Shoot</code> creation requests. When a <code>Shoot</code> creation request occurs, the system does the following:</p><ul><li><p>Retrieve all OpenIDConnectPreset available for use in the <code>Shoot</code> namespace.</p></li><li><p>Check if the shoot label selectors of any OpenIDConnectPreset matches the labels on the Shoot being created.</p></li><li><p>If multiple presets are matched then only one is chosen and results are sorted based on:</p><ol><li><code>.spec.weight</code> value.</li><li>lexicographically ordering their names ( e.g. <code>002preset</code> > <code>001preset</code> )</li></ol></li><li><p>If the <code>Shoot</code> already has a <code>.spec.kubernetes.kubeAPIServer.oidcConfig</code> then no mutation occurs.</p></li></ul><h3 id=simple-openidconnectpreset-example>Simple OpenIDConnectPreset example</h3><p>This is a simple example to show how a <code>Shoot</code> is modified by the OpenIDConnectPreset</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: settings.gardener.cloud/v1alpha1
kind: OpenIDConnectPreset
metadata:
  name:  test-1
  namespace: default
spec:
  shootSelector:
    matchLabels:
      oidc: enabled
  server:
    clientID: test-1
    issuerURL: https://foo.bar
    <span style=color:green># caBundle: |</span>
    <span style=color:green>#   -----BEGIN CERTIFICATE-----</span>
    <span style=color:green>#   Li4u</span>
    <span style=color:green>#   -----END CERTIFICATE-----</span>
    groupsClaim: groups-claim
    groupsPrefix: groups-prefix
    usernameClaim: username-claim
    usernamePrefix: username-prefix
    signingAlgs:
    - RS256
    requiredClaims:
      key: value
  client:
    secret: oidc-client-secret
    extraConfig:
      extra-scopes: <span style=color:#a31515>&#34;email,offline_access,profile&#34;</span>
      foo: bar
  weight: 90
</code></pre></div><p>Create the OpenIDConnectPreset:</p><pre><code class=language-console data-lang=console>kubectl apply -f preset.yaml
</code></pre><p>Examine the created OpenIDConnectPreset:</p><pre><code class=language-console data-lang=console>kubectl get openidconnectpresets
NAME     ISSUER            SHOOT-SELECTOR   AGE
test-1   https://foo.bar   oidc=enabled     1s
</code></pre><p>Simple <code>Shoot</code> example:</p><p>This is a sample of a <code>Shoot</code> with some fields omitted:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: Shoot
metadata:
  name: preset
  namespace: default
  labels:
    oidc: enabled
spec:
  kubernetes:
    allowPrivilegedContainers: <span style=color:#00f>true</span>
    version: 1.20.2
</code></pre></div><p>Create the Shoot:</p><pre><code class=language-console data-lang=console>kubectl apply -f shoot.yaml
</code></pre><p>Examine the created Shoot:</p><pre><code class=language-console data-lang=console>kubectl get shoot preset -o yaml
</code></pre><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: Shoot
metadata:
  name: preset
  namespace: default
  labels:
    oidc: enabled
spec:
  kubernetes:
    kubeAPIServer:
      oidcConfig:
        clientAuthentication:
          extraConfig:
            extra-scopes: email,offline_access,profile
            foo: bar
          secret: oidc-client-secret
        clientID: test-1
        groupsClaim: groups-claim
        groupsPrefix: groups-prefix
        issuerURL: https://foo.bar
        requiredClaims:
          key: value
        signingAlgs:
        - RS256
        usernameClaim: username-claim
        usernamePrefix: username-prefix
    version: 1.20.2
</code></pre></div><h3 id=disable-openidconnectpreset>Disable OpenIDConnectPreset</h3><p>The OpenIDConnectPreset admission control is enabled by default. To disable it use the <code>--disable-admission-plugins</code> flag on the gardener-apiserver.</p><p>For example:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>--disable-admission-plugins=OpenIDConnectPreset
</code></pre></div><h2 id=clusteropenidconnectpreset>ClusterOpenIDConnectPreset</h2><p>A ClusterOpenIDConnectPreset is an API resource for injecting additional runtime OIDC requirements into a Shoot at creation time. In contrast to OpenIDConnect it&rsquo;s a cluster-scoped resource. You use label selectors to specify the <code>Project</code> and <code>Shoot</code> to which a given OpenIDCConnectPreset applies.</p><p>Using a OpenIDConnectPresets allows cluster owners to not have to explicitly provide the same OIDC configuration for every <code>Shoot</code> in specific <code>Project</code>.</p><p>For more information about the background, see the <a href=https://github.com/gardener/gardener/issues/1161>issue</a> for ClusterOpenIDConnectPreset.</p><h3 id=how-clusteropenidconnectpreset-works>How ClusterOpenIDConnectPreset works</h3><p>Gardener provides an admission controller (ClusterOpenIDConnectPreset) which, when enabled, applies ClusterOpenIDConnectPresets to incoming <code>Shoot</code> creation requests. When a <code>Shoot</code> creation request occurs, the system does the following:</p><ul><li><p>Retrieve all ClusterOpenIDConnectPresets available.</p></li><li><p>Check if the project label selector of any ClusterOpenIDConnectPreset matches the labels of the <code>Project</code> in which the <code>Shoot</code> is being created.</p></li><li><p>Check if the shoot label selectors of any ClusterOpenIDConnectPreset matches the labels on the <code>Shoot</code> being created.</p></li><li><p>If multiple presets are matched then only one is chosen and results are sorted based on:</p><ol><li><code>.spec.weight</code> value.</li><li>lexicographically ordering their names ( e.g. <code>002preset</code> > <code>001preset</code> )</li></ol></li><li><p>If the <code>Shoot</code> already has a <code>.spec.kubernetes.kubeAPIServer.oidcConfig</code> then no mutation occurs.</p></li></ul><blockquote><p>Note: Due to the previous requirement if a <code>Shoot</code> is matched by both <code>OpenIDConnectPreset</code> and <code>ClusterOpenIDConnectPreset</code> then <code>OpenIDConnectPreset</code> takes precedence over <code>ClusterOpenIDConnectPreset</code>.</p></blockquote><h3 id=simple-clusteropenidconnectpreset-example>Simple ClusterOpenIDConnectPreset example</h3><p>This is a simple example to show how a <code>Shoot</code> is modified by the ClusterOpenIDConnectPreset</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: settings.gardener.cloud/v1alpha1
kind: ClusterOpenIDConnectPreset
metadata:
  name:  test
spec:
  shootSelector:
    matchLabels:
      oidc: enabled
  projectSelector: {} <span style=color:green># selects all projects.</span>
  server:
    clientID: cluster-preset
    issuerURL: https://foo.bar
    <span style=color:green># caBundle: |</span>
    <span style=color:green>#   -----BEGIN CERTIFICATE-----</span>
    <span style=color:green>#   Li4u</span>
    <span style=color:green>#   -----END CERTIFICATE-----</span>
    groupsClaim: groups-claim
    groupsPrefix: groups-prefix
    usernameClaim: username-claim
    usernamePrefix: username-prefix
    signingAlgs:
    - RS256
    requiredClaims:
      key: value
  client:
    secret: oidc-client-secret
    extraConfig:
      extra-scopes: <span style=color:#a31515>&#34;email,offline_access,profile&#34;</span>
      foo: bar
  weight: 90
</code></pre></div><p>Create the ClusterOpenIDConnectPreset:</p><pre><code class=language-console data-lang=console>kubectl apply -f preset.yaml
</code></pre><p>Examine the created ClusterOpenIDConnectPreset:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl get clusteropenidconnectpresets
NAME     ISSUER            PROJECT-SELECTOR   SHOOT-SELECTOR   AGE
test     https://foo.bar   &lt;none&gt;             oidc=enabled     1s
</code></pre></div><p>This is a sample of a <code>Shoot</code> with some fields omitted:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>kind: Shoot
apiVersion: core.gardener.cloud/v1beta1
metadata:
  name: preset
  namespace: default
  labels:
    oidc: enabled
spec:
  kubernetes:
    allowPrivilegedContainers: <span style=color:#00f>true</span>
    version: 1.20.2
</code></pre></div><p>Create the Shoot:</p><pre><code class=language-console data-lang=console>kubectl apply -f shoot.yaml
</code></pre><p>Examine the created Shoot:</p><pre><code class=language-console data-lang=console>kubectl get shoot preset -o yaml
</code></pre><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: Shoot
metadata:
  name: preset
  namespace: default
  labels:
    oidc: enabled
spec:
  kubernetes:
    kubeAPIServer:
      oidcConfig:
        clientAuthentication:
          extraConfig:
            extra-scopes: email,offline_access,profile
            foo: bar
          secret: oidc-client-secret
        clientID: cluster-preset
        groupsClaim: groups-claim
        groupsPrefix: groups-prefix
        issuerURL: https://foo.bar
        requiredClaims:
          key: value
        signingAlgs:
        - RS256
        usernameClaim: username-claim
        usernamePrefix: username-prefix
    version: 1.20.2
</code></pre></div><h3 id=disable-clusteropenidconnectpreset>Disable ClusterOpenIDConnectPreset</h3><p>The ClusterOpenIDConnectPreset admission control is enabled by default. To disable it use the <code>--disable-admission-plugins</code> flag on the gardener-apiserver.</p><p>For example:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>--disable-admission-plugins=ClusterOpenIDConnectPreset
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-5312b08cd06fa7b827ab36d682e935b0>15 - Projects</h1><h1 id=projects>Projects</h1><p>The Gardener API server supports a cluster-scoped <code>Project</code> resource which is used for data isolation between individual Gardener consumers. For example, each development team has its own project to manage its own shoot clusters.</p><p>Each <code>Project</code> is backed by a Kubernetes <code>Namespace</code> that contains the actual related Kubernetes resources like <code>Secret</code>s or <code>Shoot</code>s.</p><p><strong>Example resource:</strong></p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: Project
metadata:
  name: dev
spec:
  namespace: garden-dev
  description: <span style=color:#a31515>&#34;This is my first project&#34;</span>
  purpose: <span style=color:#a31515>&#34;Experimenting with Gardener&#34;</span>
  owner:
    apiGroup: rbac.authorization.k8s.io
    kind: User
    name: john.doe@example.com
  members:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: alice.doe@example.com
    role: admin
  <span style=color:green># roles:</span>
  <span style=color:green># - viewer </span>
  <span style=color:green># - uam</span>
  <span style=color:green># - extension:foo</span>
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: bob.doe@example.com
    role: viewer
<span style=color:green># tolerations:</span>
<span style=color:green>#   defaults:</span>
<span style=color:green>#   - key: &lt;some-key&gt;</span>
<span style=color:green>#   whitelist:</span>
<span style=color:green>#   - key: &lt;some-key&gt;</span>
</code></pre></div><p>The <code>.spec.namespace</code> field is optional and will be initialized if it&rsquo;s unset.
The name of the resulting namespace will be generated and look like <code>garden-dev-5anj3</code>, i.e., it has a random suffix.
It&rsquo;s also possible to adopt existing namespaces by labeling them <code>gardener.cloud/role=project</code> and <code>project.gardener.cloud/name=dev</code> beforehand (otherwise, they cannot be adopted).</p><p>When deleting a Project resource, the corresponding namespace is also deleted.
To keep a namespace after project deletion, an administrator/operator (not Project members!) can annotate the project-namespace with <code>namespace.gardener.cloud/keep-after-project-deletion</code>.</p><p>The <code>spec.description</code> and <code>.spec.purpose</code> fields can be used to describe to fellow team members and Gardener operators what this project is used for.</p><p>Each project has one dedicated owner, configured in <code>.spec.owner</code> using the <code>rbac.authorization.k8s.io/v1.Subject</code> type.
The owner is the main contact person for Gardener operators.
Please note that the <code>.spec.owner</code> field is deprecated and will be removed in future API versions in favor of the <code>owner</code> role, see below.</p><p>The list of members (again a list in <code>.spec.members[]</code> using the <code>rbac.authorization.k8s.io/v1.Subject</code> type) contains all the people that are associated with the project in any way.
Each project member must have at least one role (currently described in <code>.spec.members[].role</code>, additional roles can be added to <code>.spec.members[].roles[]</code>). The following roles exist:</p><ul><li><code>admin</code>: This allows to fully manage resources inside the project (e.g., secrets, shoots, configmaps, and similar).</li><li><code>uam</code>: This allows to add/modify/remove human users or groups to/from the project member list. Technical users (service accounts) can be managed by all admins.</li><li><code>viewer</code>: This allows to read all resources inside the project except secrets.</li><li><code>owner</code>: This combines the <code>admin</code> and <code>uam</code> roles.</li><li>Extension roles (prefixed with <code>extension:</code>): Please refer to <a href=/docs/gardener/extensions/project-roles/>this document</a>.</li></ul><p>The <a href=/docs/gardener/concepts/controller-manager/#project-controller>project controller</a> inside the Gardener Controller Manager is managing RBAC resources that grant the described privileges to the respective members.</p><p>There are two central <code>ClusterRole</code>s <code>gardener.cloud:system:project-member</code> and <code>gardener.cloud:system:project-viewer</code> that grant the permissions for namespaced resources (e.g., <code>Secret</code>s, <code>Shoot</code>s, etc.).
Via referring <code>RoleBinding</code>s created in the respective namespace the project members get bound to these <code>ClusterRole</code>s and, thus, the needed permissions.
There are also project-specific <code>ClusterRole</code>s granting the permissions for cluster-scoped resources, e.g. the <code>Namespace</code> or <code>Project</code> itself.<br>For each role, the following <code>ClusterRole</code>s, <code>ClusterRoleBinding</code>s, and <code>RoleBinding</code>s are created:</p><table><thead><tr><th>Role</th><th><code>ClusterRole</code></th><th><code>ClusterRoleBinding</code></th><th><code>RoleBinding</code></th></tr></thead><tbody><tr><td><code>admin</code></td><td><code>gardener.cloud:system:project-member:&lt;projectName></code></td><td><code>gardener.cloud:system:project-member:&lt;projectName></code></td><td><code>gardener.cloud:system:project-member</code></td></tr><tr><td><code>uam</code></td><td><code>gardener.cloud:system:project-uam:&lt;projectName></code></td><td><code>gardener.cloud:system:project-uam:&lt;projectName></code></td><td></td></tr><tr><td><code>viewer</code></td><td><code>gardener.cloud:system:project-viewer:&lt;projectName></code></td><td><code>gardener.cloud:system:project-viewer:&lt;projectName></code></td><td><code>gardener.cloud:system:project-viewer</code></td></tr><tr><td><code>owner</code></td><td><code>gardener.cloud:system:project:&lt;projectName></code></td><td><code>gardener.cloud:system:project:&lt;projectName></code></td><td></td></tr><tr><td><code>extension:*</code></td><td><code>gardener.cloud:extension:project:&lt;projectName>:&lt;extensionRoleName></code></td><td></td><td><code>gardener.cloud:extension:project:&lt;projectName>:&lt;extensionRoleName></code></td></tr></tbody></table><h2 id=user-access-management>User Access Management</h2><p>For <code>Project</code>s created before Gardener v1.8 all admins were allowed to manage other members.
Beginning with v1.8 the new <code>uam</code> role is being introduced.
It is backed by the <code>manage-members</code> custom RBAC verb which allows to add/modify/remove human users or groups to/from the project member list.
Human users are subjects with <code>kind=User</code> and <code>name!=system:serviceaccount:*</code>, and groups are subjects with <code>kind=Group</code>.
The management of service account subjects (<code>kind=ServiecAccount</code> or <code>name=system:serviceaccount:*</code>) is not controlled via the <code>uam</code> custom verb but with the standard <code>update</code>/<code>patch</code> verbs for projects.</p><p>All newly created projects will only bind the owner to the <code>uam</code> role.
The owner can still grant the <code>uam</code> role to other members if desired.
For projects created before Gardener v1.8 the Gardener Controller Manager will migrate all projects to also assign the <code>uam</code> role to all <code>admin</code> members (to not break existing use-cases). The corresponding migration logic is present in Gardener Controller Manager from v1.8 to v1.13.
The project owner can gradually remove these roles if desired.</p><h2 id=stale-projects>Stale Projects</h2><p>When a project is not actively used for some period of time the project is marked as &ldquo;stale&rdquo;. This is done by controller called &ldquo;Stale Projects Reconciler&rdquo;. Once the project is marked as stale there is a time frame in which if not used it will be deleted by that controller. More detailed information can be found <a href=/docs/gardener/concepts/controller-manager/#stale-projects-reconciler>here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-51f43840ca475366ea7f9ff5a443fac5>16 - Reversed Vpn Tunnel</h1><h1 id=reversed-vpn-tunnel-setup-and-configuration>Reversed VPN Tunnel Setup and Configuration</h1><p>This is a short guide describing how to enable tunneling traffic from shoot cluster to seed cluster instead of the default &ldquo;seed to shoot&rdquo; direction.</p><h2 id=the-openvpn-default>The OpenVPN Default</h2><p>By default, Gardener makes use of OpenVPN to connect the shoot controlplane running on the seed cluster to the dataplane
running on the shoot worker nodes, usually in isolated networks. This is achieved by having a sidecar to certain control plane components such as the <code>kube-apiserver</code> and <code>prometheus</code>.</p><p>With a sidecar, all traffic directed to the cluster is intercepted by iptables rules and redirected
to the tunnel endpoint in the shoot cluster deployed behind a cloud loadbalancer. This has the following disadvantages:</p><ul><li>Every shoot would require an additional loadbalancer, this accounts for additional overhead in terms of both costs and troubleshooting efforts.</li><li>Private access use-cases would not be possible without having a seed residing in the same private domain as a hard requirement. For example, have a look at <a href=https://github.com/gardener/gardener-extension-provider-gcp/issues/56>this issue</a></li><li>Providing a public endpoint to access components in the shoot poses a security risk.</li></ul><p>This is how it looks like today with the OpenVPN solution:</p><p><code>APIServer | VPN-seed ---> internet ---> LB --> VPN-Shoot (4314) --> Pods | Nodes | Services</code></p><h2 id=reversing-the-tunnel>Reversing the Tunnel</h2><p>To address the above issues, the tunnel can establishment direction can be reverted, i.e. instead of having the client reside in the seed,
we deploy the client in the shoot and initiate the connection from there. This way, there is no need to deploy a special purpose
loadbalancer for the sake of addressing the dataplane, in addition to saving costs, this is considered the more secure alternative.
For more information on how this is achieved, please have a look at the following <a href=/docs/gardener/proposals/14-reversed-cluster-vpn/>GEP</a>.</p><p>How it should look like at the end:</p><p><code>APIServer --> Envoy-Proxy | VPN-Seed-Server &lt;-- Istio/Envoy-Proxy &lt;-- SNI API Server Endpoint &lt;-- LB (one for all clusters of a seed) &lt;--- internet &lt;--- VPN-Shoot-Client --> Pods | Nodes | Services</code></p><h3 id=how-to-configure>How to Configure</h3><p>To enable the usage of the reversed vpn tunnel feature, either the Gardenlet <code>ReversedVPN</code> feature-gate must be set to <code>true</code> as shown below or the shoot must be annotated with <code>"alpha.featuregates.shoot.gardener.cloud/reversed-vpn: true"</code>.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>featureGates:
  ReversedVPN: <span style=color:#00f>true</span>
</code></pre></div><p>Please refer to the examples <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml>here</a> for more information.</p><p>To disable the feature-gate the shoot must be annotated with <code>"alpha.featuregates.shoot.gardener.cloud/reversed-vpn: false"</code></p><p>Once the feature-gate is enabled, a <code>vpn-seed-server</code> deployment will be added to the controlplane. The <code>kube-apiserver</code> will be configured to connect to resources in the dataplane such as pods, services and nodes though the <code>vpn-seed-service</code> via http proxy/connect protocol.
In the dataplane of the cluster, the <code>vpn-shoot</code> will establish the connection to the <code>vpn-seed-server</code> indirectly using the SNI API Server endpoint as a http proxy. After the connection has been established requests from the <code>kube-apiserver</code> will be handled by the tunnel.</p><blockquote><p>Please note this feature is available ONLY for >= 1.18 kubernetes clusters. For clusters with Kubernetes version &lt; 1.18, the default OpenVPN setup will be used by default even if the featuregate is enabled.
Furthermore, this feature is still in Alpha, so you might see instabilities every now and then.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-950da554bf8a0407b2a5fbbc1058ed8e>17 - Secrets Rotation</h1><h1 id=secrets-and-rotation>Secrets and rotation</h1><p>The following sections contain information about <code>Secret</code>s in the Garden cluster that are either provided by users or
generated by Gardener for users.</p><h2 id=list-of-secrets>List of secrets</h2><h3 id=user-provided-secrets>User provided secrets</h3><h4 id=cloudprovider-secret>Cloudprovider Secret</h4><p><em>Example</em>: <a href=https://github.com/gardener/gardener/blob/master/example/70-secret-provider.yaml>https://github.com/gardener/gardener/blob/master/example/70-secret-provider.yaml</a></p><p><em>Usage</em>: Authenticate gardener and kubernetes components for infrastructure operations</p><p><em>Description</em>: Gardener uses the cloudprovider secret to interact with the infrastructure when setting up shoot networks or security groups via the <a href=https://github.com/gardener/terraformer>terraformer</a>. It is also used by the <a href=https://kubernetes.io/docs/concepts/architecture/cloud-controller/>cloud-controller-manager</a> of your Shoot to communicate with the infrastructure for example to create Loadbalancer services, routes or retrieve information about Node objects.
Depending on the cloudprovider the format of the secret differs. Please consult the example above and respective infrastructure extension documentation for the concrete layout.</p><p>To put it in use, a cloudprovider secret is bound to one more namespaces (and therefore projects) using a <a href=https://github.com/gardener/gardener/blob/master/example/80-secretbinding.yaml>SecretBinding</a>. For Shoots created in those projects the secret is synced to the shoot namespace in the seed cluster.</p><p><em>Rotation</em>: Rotating the cloudprovider secret requires multiple steps:</p><ol><li>Update the data keys in the secret.</li><li>⚠️ Wait until all Shoots using the secret are reconciled before you disable the old secret in your infrastructure account! Otherwise the shoots will no longer function.</li><li>After all Shoots using the secret were reconciled you can go ahead and deactivate the old secret in your infrastructure account.</li></ol><h3 id=gardener-generated-secrets>Gardener generated secrets</h3><h4 id=kubeconfig>Kubeconfig</h4><p><em>Name</em>: <code>&lt;shoot-name>.kubeconfig</code></p><p><em>Description</em>: Admin Kubeconfig provided by Gardener for the managed shoot cluster.</p><p>This <code>Secret</code> has multiple keys:</p><ul><li><code>kubeconfig</code>: the completed kubeconfig</li><li><code>token</code>: token for <code>system:cluster-admin</code> user</li><li><code>username</code>/<code>password</code>: basic auth credentials (if enabled via <code>Shoot.spec.kubernetes.kubeAPIServer.enableBasicAuthentication</code>)</li><li><code>ca.crt</code>: the CA bundle for establishing trust to the API server (same as in the <a href=#cluster-certificate-authority-bundle>Cluster CA bundle secret</a>)</li></ul><hr><p><strong>NOTE</strong></p><p>This Kubeconfig contains the highest privileges in the cluster. We strongly discourage distributing or using this Kubeconfig.
Instead, configure dedicated <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/>Service Accounts</a>,
<a href=https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens>OIDC</a> or similar alternatives
to grant role-based and revocable access for a broader audience.</p><hr><p><em>Rotation</em>: Kubeconfig can be rotated by annotating the shoot resource with <code>gardener.cloud/operation: rotate-kubeconfig-credentials</code>.
The substituted Kubeconfig are provided after the initialized reconciliation was performed. Please note, shoot clusters
which were created with Gardener version <code>&lt;= 0.28.0</code> used to have a Kubeconfig based on a client certificate instead of a static token.
These client certificates are not revocable and thus a full credential rotation is not supported.</p><p>You can check the <code>.status.credentials.rotation.kubeconfig</code> field in the <code>Shoot</code> to see when the rotation was last initiated or last completed.</p><h4 id=cluster-certificate-authority-bundle>Cluster Certificate Authority Bundle</h4><p><em>Name</em>: <code>&lt;shoot-name>.ca-cluster</code></p><p><em>Description</em>: Certificate Authority (CA) bundle of the Cluster (<code>Secret</code> key: <code>ca.crt</code>).</p><p>This bundle contains one or multiple CAs which are used for signing serving certificates of the Shoot&rsquo;s API server. Hence, the certificates contained in this <code>Secret</code> can be used to verify the API server&rsquo;s identity when communicating with its public endpoint (e.g. as <code>certificate-authority-data</code> in a Kubeconfig).
This is the same certificate that is also contained in the Kubeconfig&rsquo;s <code>certificate-authority-data</code> field.</p><p><em>Rotation</em>: Not supported yet, but work is in progress. See <a href=https://github.com/gardener/gardener/issues/3292>gardener/gardener#3292</a> and <a href=https://github.com/gardener/gardener/blob/release-v1.42/docs/proposals/18-shoot-CA-rotation.md>GEP-18</a> for more details.</p><h4 id=monitoring>Monitoring</h4><p><em>Name</em>: <code>&lt;shoot-name>.monitoring</code></p><p><em>Description</em>: Username/password for accessing the user Grafana instance of a shoot cluster (<code>Secret</code> keys: <code>username</code>/<code>password</code>).</p><p><em>Rotation</em>: Not supported yet.</p><h4 id=ssh-keypair>SSH-Keypair</h4><p><em>Name</em>: <code>&lt;shoot-name>.ssh-keypair</code></p><p><em>Description</em>: SSH-Keypair that is propagated to the worker nodes of the shoot cluster.
The private key can be used to establish an SSH connection to the workers for troubleshooting purposes (<code>Secret</code> keys: <code>id_rsa</code>/<code>id_rsa.pub</code>).</p><p><em>Rotation</em>: Keypair can be rotated by annotating the shoot resource with <code>gardener.cloud/operation: rotate-ssh-keypair</code>.
Propagating the new keypair to all worker nodes may take longer than the initiated reconciliation of the shoot.
The previous keypair can still be found in the <code>&lt;shoot-name>.ssh-keypair.old</code> secret and is still valid until the next rotation.</p><p>You can check the <code>.status.credentials.rotation.sshKeypair</code> field in the <code>Shoot</code> to see when the rotation was last initiated or last completed.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-42e1bcea9f49e68fd1fe7ce23506a673>18 - Seed Bootstrapping</h1><h1 id=seed-bootstrapping>Seed Bootstrapping</h1><p>Whenever the Gardenlet is responsible for a new <code>Seed</code> resource its &ldquo;seed controller&rdquo; is being activated.
One part of this controller&rsquo;s reconciliation logic is deploying certain components into the <code>garden</code> namespace of the seed cluster itself.
These components are required to spawn and manage control planes for shoot clusters later on.
This document is providing an overview which actions are performed during this bootstrapping phase, and it explains the rationale behind them.</p><h2 id=dependency-watchdog>Dependency Watchdog</h2><p>The dependency watchdog (abbreviation: DWD) is a component developed separately in the <a href=https://github.com/gardener/dependency-watchdog>gardener/dependency-watchdog</a> GitHub repository.
Gardener is using it for two purposes:</p><ol><li>Prevention of melt-down situations when the load balancer used to expose the kube-apiserver of shoot clusters goes down while the kube-apiserver itself is still up and running</li><li>Fast recovery times for crash-looping pods when depending pods are again available</li></ol><p>For the sake of separating these concerns, two instances of the DWD are deployed by the seed controller.</p><h3 id=probe>Probe</h3><p>The <code>dependency-watchdog-probe</code> deployment is responsible for above mentioned first point.</p><p>The <code>kube-apiserver</code> of shoot clusters is exposed via a load balancer, usually with an attached public IP, which serves as the main entry point when it comes to interaction with the shoot cluster (e.g., via <code>kubectl</code>).
While end-users are talking to their clusters via this load balancer, other control plane components like the <code>kube-controller-manager</code> or <code>kube-scheduler</code> run in the same namespace/same cluster, so they can communicate via the in-cluster <code>Service</code> directly instead of using the detour with the load balancer.
However, the worker nodes of shoot clusters run in isolated, distinct networks.
This means that the <code>kubelet</code>s and <code>kube-proxy</code>s also have to talk to the control plane via the load balancer.</p><p>The <code>kube-controller-manager</code> has a special control loop called <a href=https://github.com/kubernetes/kubernetes/tree/master/pkg/controller/nodelifecycle><code>nodelifecycle</code></a> which will set the status of <code>Node</code>s to <code>NotReady</code> in case the kubelet stops to regularly renew its lease/to send its heartbeat.
This will trigger other self-healing capabilities of Kubernetes, for example the eviction of pods from such &ldquo;unready&rdquo; nodes to healthy nodes.
Similarly, the <code>cloud-controller-manager</code> has a control loop that will disconnect load balancers from &ldquo;unready&rdquo; nodes, i.e., such workload would no longer be accessible until moved to a healthy node.</p><p>While these are awesome Kubernetes features on their own, they have a dangerous drawback when applied in the context of Gardener&rsquo;s architecture:
When the <code>kube-apiserver</code> load balancer fails for whatever reason then the <code>kubelet</code>s can&rsquo;t talk to the <code>kube-apiserver</code> to renew their lease anymore.
After a minute or so the <code>kube-controller-manager</code> will get the impression that all nodes have died and will mark them as <code>NotReady</code>.
This will trigger above mentioned eviction as well as detachment of load balancers.
As a result, the customer&rsquo;s workload will go down and become unreachable.</p><p>This is exactly the situation that the DWD prevents:
It regularly tries to talk to the <code>kube-apiserver</code>s of the shoot clusters, once by using their load balancer, and once by talking via the in-cluster <code>Service</code>.
If it detects that the <code>kube-apiserver</code> is reachable internally but not externally it scales down the <code>kube-controller-manager</code> to <code>0</code>.
This will prevent it from marking the shoot worker nodes as &ldquo;unready&rdquo;.
As soon as the <code>kube-apiserver</code> is reachable externally again the <code>kube-controller-manager</code> will be scaled up to <code>1</code> again.</p><h3 id=endpoint>Endpoint</h3><p>The <code>dependency-watchdog-endpoint</code> deployment is responsible for above mentioned second point.</p><p>Kubernetes is restarting failing pods with an exponentially increasing backoff time.
While this is a great strategy to prevent system overloads it has the disadvantage that the delay between restarts is increasing up to multiple minutes very fast.</p><p>In the Gardener context, we are deploying many components that are depending on other components.
For example, the <code>kube-apiserver</code> is depending on a running <code>etcd</code>, or the <code>kube-controller-manager</code> and <code>kube-scheduler</code> are depending on a running <code>kube-apiserver</code>.
In case such a &ldquo;higher-level&rdquo; component fails for whatever reason, the dependent pods will fail and end-up in crash-loops.
As Kubernetes does not know anything about these hierarchies it won&rsquo;t recognize that such pods can be restarted faster as soon as their dependents are up and running again.</p><p>This is exactly the situation in which the DWD will become active:
If it detects that a certain <code>Service</code> is available again (e.g., after the <code>etcd</code> was temporarily down while being moved to another seed node) then DWD will restart all crash-looping dependant pods.
These dependant pods are detected via a pre-configured label selector.</p><p>As of today, the DWD is configured to restart a crash-looping <code>kube-apiserver</code> after <code>etcd</code> became available again, or any pod depending on the <code>kube-apiserver</code> that has a <code>gardener.cloud/role=controlplane</code> label (e.g., <code>kube-controller-manager</code>, <code>kube-scheduler</code>, etc.).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a4713902bcc1223db8a52f790b8cd0ef>19 - Seed Settings</h1><h1 id=settings-for-seeds>Settings for <code>Seed</code>s</h1><p>The <code>Seed</code> resource offers a few settings that are used to control the behaviour of certain Gardener components.
This document provides an overview over the available settings:</p><h2 id=dependency-watchdog>Dependency Watchdog</h2><p>Gardenlet can deploy two instances of the <a href=https://github.com/gardener/dependency-watchdog>dependency-watchdog</a> into the <code>garden</code> namespace of the seed cluster.
One instance only activates the <code>endpoint</code> controller while the second instance only activates the <code>probe</code> controller.</p><h3 id=endpoint-controller>Endpoint Controller</h3><p>The <code>endpoint</code> controller helps to alleviate the delay where control plane components remain unavailable by finding the respective pods in CrashLoopBackoff status and restarting them once their dependants become ready and available again.
For example, if <code>etcd</code> goes down then also <code>kube-apiserver</code> goes down (and into a <code>CrashLoopBackoff</code> state). If <code>etcd</code> comes up again then (without the <code>endpoint</code> controller) it might take some time until <code>kube-apiserver</code> gets restarted as well.</p><p>It can be enabled/disabled via the <code>.spec.settings.dependencyWatchdog.endpoint.enabled</code> field.
It defaults to <code>true</code>.</p><h3 id=probe-controller>Probe Controller</h3><p>The <code>probe</code> controller scales down the <code>kube-controller-manager</code> of shoot clusters in case their respective <code>kube-apiserver</code> is not reachable via its external ingress.
This is in order to avoid melt-down situations since the <code>kube-controller-manager</code> uses in-cluster communication when talking to the <code>kube-apiserver</code>, i.e., it wouldn&rsquo;t be affected if the external access to the <code>kube-apiserver</code> is interrupted for whatever reason.
The <code>kubelet</code>s on the shoot worker nodes, however, would indeed be affected since they typically run in different networks and use the external ingress when talking to the <code>kube-apiserver</code>.
Hence, without scaling down <code>kube-controller-manager</code>, the nodes might be marked as <code>NotReady</code> and eventually replaced (since the <code>kubelet</code>s cannot report their status anymore).
To prevent such unnecessary turbulences, <code>kube-controller-manager</code> is being scaled down until the external ingress becomes available again.</p><p>It can be enabled/disabled via the <code>.spec.settings.dependencyWatchdog.probe.enabled</code> field.
It defaults to <code>true</code>.</p><h2 id=reserve-excess-capacity>Reserve Excess Capacity</h2><p>If the excess capacity reservation is enabled then the Gardenlet will deploy a special <code>Deployment</code> into the <code>garden</code> namespace of the seed cluster.
This <code>Deployment</code>&rsquo;s pod template has only one container, the <code>pause</code> container, which simply runs in an infinite loop.
The priority of the deployment is very low, so any other pod will preempt these <code>pause</code> pods.
This is especially useful if new shoot control planes are created in the seed.
In case the seed cluster runs at its capacity then there is no waiting time required during the scale-up.
Instead, the low-priority <code>pause</code> pods will be preempted and allow newly created shoot control plane pods to be scheduled fast.
In the meantime, the cluster-autoscaler will trigger the scale-up because the preempted <code>pause</code> pods want to run again.
However, this delay doesn&rsquo;t affect the important shoot control plane pods which will improve the user experience.</p><p>It can be enabled/disabled via the <code>.spec.settings.excessCapacityReservation.enabled</code> field.
It defaults to <code>true</code>.</p><h2 id=scheduling>Scheduling</h2><p>By default, the Gardener Scheduler will consider all seed clusters when a new shoot cluster shall be created.
However, administrators/operators might want to exclude some of them from being considered by the scheduler.
Therefore, seed clusters can be marked as &ldquo;invisible&rdquo;.
In this case, the scheduler simply ignores them as if they wouldn&rsquo;t exist.
Shoots can still use the invisible seed but only by explicitly specifying the name in their <code>.spec.seedName</code> field.</p><p>Seed clusters can be marked visible/invisible via the <code>.spec.settings.scheduling.visible</code> field.
It defaults to <code>true</code>.</p><h2 id=shoot-dns>Shoot DNS</h2><p>Generally, the Gardenlet creates a few DNS records during the creation/reconciliation of a shoot cluster (see <a href=/docs/gardener/usage/configuration/>here</a>).
However, some infrastructures don&rsquo;t need/want this behaviour.
Instead, they want to directly use the IP addresses/hostnames of load balancers.
Another use-case is a local development setup where DNS is not needed for simplicity reasons.</p><p>By setting the <code>.spec.settings.shootDNS.enabled</code> field this behavior can be controlled.</p><p>ℹ️ In previous Gardener versions (&lt; 1.5) these settings were controlled via taint keys (<code>seed.gardener.cloud/{disable-capacity-reservation,disable-dns,invisible}</code>).
The taint keys are no longer supported and removed in version 1.12.
The rationale behind it is the implementation of tolerations similar to Kubernetes tolerations.
More information about it can be found in <a href=https://github.com/gardener/gardener/issues/2193>#2193</a>.</p><h2 id=load-balancer-services>Load Balancer Services</h2><p>Gardener creates certain Kubernetes <code>Service</code> objects of type <code>LoadBalancer</code> in the seed cluster.
Most prominently, they are used for exposing the shoot control planes, namely the kube-apiserver of the shoot clusters.
In most cases, the cloud-controller-manager (responsible for managing these load balancers on the respective underlying infrastructure) supports certain customization and settings via annotations.
<a href=https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer>This document</a> provides a good overview and many examples.</p><p>By setting the <code>.spec.settings.loadBalancerServices.annotations</code> field the Gardener administrator can specify a list of annotations which will be injected into the <code>Service</code>s of type <code>LoadBalancer</code>.</p><h2 id=vertical-pod-autoscaler>Vertical Pod Autoscaler</h2><p>Gardener heavily relies on the Kubernetes <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler><code>vertical-pod-autoscaler</code> component</a>.
By default, the seed controller deploys the VPA components into the <code>garden</code> namespace of the respective seed clusters.
In case you want to manage the VPA deployment on your own or have a custom one then you might want to disable the automatic deployment of Gardener.
Otherwise, you might end up with two VPAs which will cause erratic behaviour.
By setting the <code>.spec.settings.verticalPodAutoscaler.enabled=false</code> you can disable the automatic deployment.</p><p>⚠️ In any case, there must be a VPA available for your seed cluster. Using a seed without VPA is not supported.</p><h2 id=owner-checks>Owner Checks</h2><p>When a shoot is scheduled to a seed and actually reconciled, Gardener appoints the seed as the current &ldquo;owner&rdquo; of the shoot by creating a special &ldquo;owner DNS record&rdquo; and checking against it if the seed still owns the shoot in order to guard against &ldquo;split brain scenario&rdquo; during control plane migration, as described in <a href=/docs/gardener/proposals/17-shoot-control-plane-migration-bad-case/>GEP-17 Shoot Control Plane Migration &ldquo;Bad Case&rdquo; Scenario</a>.
This mechanism relies on the DNS resolution of TXT DNS records being possible and highly reliable, since if the owner check fails the shoot will be effectively disabled for the duration of the failure.
In environments where resolving TXT DNS records is either not possible or not considered reliable enough, it may be necessary to disable the owner check mechanism, in order to avoid shoots failing to reconcile or temporary outages due to transient DNS failures.
By setting the <code>.spec.settings.ownerChecks.enabled=false</code> (default is <code>true</code>) the creation and checking of owner DNS records can be disabled for all shoots scheduled on this seed. Note that if owner checks are disabled, migrating shoots scheduled on this seed to other seeds should be considered unsafe, and in the future will be disabled as well.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ac6a66f3733149171ad802e94f5dc7e7>20 - Shoot Auditpolicy</h1><h1 id=audit-a-kubernetes-cluster>Audit a Kubernetes Cluster</h1><p>The shoot cluster is a kubernetes cluster and its <code>kube-apiserver</code> handles the audit events. In order to define which audit events must be logged, a proper audit policy file must be passed to the kubernetes API server. You could find more information about auditing a kubernetes cluster <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/audit/>here</a>.</p><h2 id=default-audit-policy>Default Audit Policy</h2><p>By default, the Gardener will deploy the shoot cluster with audit policy defined in the <a href=https://github.com/gardener/gardener/blob/master/pkg/operation/botanist/component/kubeapiserver/secrets.go>kube-apiserver package</a>.</p><h2 id=custom-audit-policy>Custom Audit Policy</h2><p>If you need specific audit policy for your shoot cluster, then you could deploy the required audit policy in the garden cluster as <code>ConfigMap</code> resource and set up your shoot to refer this <code>ConfigMap</code>. Note, the policy must be stored under the key <code>policy</code> in the data section of the <code>ConfigMap</code>.</p><p>For example, deploy the auditpolicy <code>ConfigMap</code> in the same namespace as your <code>Shoot</code> resource:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl apply -f example/95-configmap-custom-audit-policy.yaml
</code></pre></div><p>then set your shoot to refer that <code>ConfigMap</code> (only related fields are shown):</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:
  kubernetes:
    kubeAPIServer:
      auditConfig:
        auditPolicy:
          configMapRef:
            name: auditpolicy
</code></pre></div><p>The Gardener validate the <code>Shoot</code> resource to refer only existing <code>ConfigMap</code> containing valid audit policy, and rejects the <code>Shoot</code> on failure.
If you want to switch back to the default audit policy, you have to remove the section</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>auditPolicy:
  configMapRef:
    name: &lt;configmap-name&gt;
</code></pre></div><p>from the shoot spec.</p><h2 id=rolling-out-changes-to-the-audit-policy>Rolling Out Changes to the Audit Policy</h2><p>Gardener is not automatically rolling out changes to the Audit Policy to minimize the amount of Shoot reconciliations in order to prevent cloud provider rate limits, etc.
Gardener will pick up the changes on the next reconciliation of Shoots referencing the Audit Policy ConfigMap.
If users want to immediately rollout Audit Policy changes, they can manually trigger a Shoot reconciliation as described in <a href=/docs/gardener/usage/shoot_operations/#immediate-reconciliation>triggering an immediate reconciliation</a>.
This is similar to changes to the cloud provider secret referenced by Shoots.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-fca47cfc40d1ae8a466ee0cce9f8b9e3>21 - Shoot Autoscaling</h1><h1 id=auto-scaling-in-shoot-clusters>Auto-Scaling in Shoot Clusters</h1><p>There are two parts that relate to auto-scaling in Kubernetes clusters in general:</p><ul><li>Horizontal node auto-scaling, i.e., dynamically adding and removing worker nodes</li><li>Vertical pod auto-scaling, i.e., dynamically raising or shrinking the resource requests/limits of pods</li></ul><p>This document provides an overview of both scenarios.</p><h2 id=horizontal-node-auto-scaling>Horizontal Node Auto-Scaling</h2><p>Every shoot cluster that has at least one worker pool with <code>minimum &lt; maximum</code> nodes configuration will get a <code>cluster-autoscaler</code> deployment.
Gardener is leveraging the upstream community Kubernetes <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler><code>cluster-autoscaler</code> component</a>.
We have forked it to <a href=https://github.com/gardener/autoscaler/>gardener/autoscaler</a> so that it supports the way how Gardener manages the worker nodes (leveraging <a href=https://github.com/gardener/machine-controller-manager>gardener/machine-controller-manager</a>).
However, we have not touched the logic how it performs auto-scaling decisions.
Consequently, please refer to the <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#faqdocumentation>offical documentation</a> for this component.</p><p>The <code>Shoot</code> API allows to configure a few flags of the <code>cluster-autoscaler</code>:</p><ul><li><code>.spec.kubernetes.clusterAutoscaler.ScaleDownDelayAfterAdd</code> defines how long after scale up that scale down evaluation resumes (default: <code>1h</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.ScaleDownDelayAfterDelete</code> defines how long after node deletion that scale down evaluation resumes (defaults to <code>ScanInterval</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.ScaleDownDelayAfterFailure</code> defines how long after scale down failure that scale down evaluation resumes (default: <code>3m</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.ScaleDownUnneededTime</code> defines how long a node should be unneeded before it is eligible for scale down (default: <code>30m</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.ScaleDownUtilizationThreshold</code> defines the threshold under which a node is being removed (default: <code>0.5</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.ScanInterval</code> defines how often cluster is reevaluated for scale up or down (default: <code>10s</code>).</li><li><code>.spec.kubernetes.clusterAutoscaler.IgnoreTaints</code> specifies a list of taint keys to ignore in node templates when considering to scale a node group (default: <code>nil</code>).</li></ul><h2 id=vertical-pod-auto-scaling>Vertical Pod Auto-Scaling</h2><p>This form of auto-scaling is not enabled by default and must be explicitly enabled in the <code>Shoot</code> by setting <code>.spec.kubernetes.verticalPodAutoscaler.enabled=true</code>.
The reason is that it was only introduced lately, and some end-users might have already deployed their own VPA into their clusters, i.e., enabling it by default would interfere with such custom deployments and lead to issues, eventually.</p><p>Gardener is also leveraging an upstream community tool, i.e., the Kubernetes <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler><code>vertical-pod-autoscaler</code> component</a>.
If enabled, Gardener will deploy it as part of the control plane into the seed cluster.
It will also be used for the vertical autoscaling of Gardener&rsquo;s system components deployed into the <code>kube-system</code> namespace of shoot clusters, for example, <code>kube-proxy</code> or <code>metrics-server</code>.</p><p>You might want to refer to the <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md>official documentation</a> for this component to get more information how to use it.</p><p>The <code>Shoot</code> API allows to configure a few flags of the <code>vertical-pod-autoscaler</code>:</p><ul><li><code>.spec.kubernetes.verticalPodAutoscaler.evictAfterOOMThreshold</code> defines the threshold that will lead to pod eviction in case it OOMed in less than the given threshold since its start and if it has only one container (default: <code>10m0s</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.evictionRateBurst</code> defines the burst of pods that can be evicted (default: <code>1</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.evictionRateLimit</code> defines the number of pods that can be evicted per second. A rate limit set to 0 or -1 will disable the rate limiter (default: <code>-1</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.evictionTolerance</code> defines the fraction of replica count that can be evicted for update in case more than one pod can be evicted (default: <code>0.5</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.recommendationMarginFraction</code> is the fraction of usage added as the safety margin to the recommended request (default: <code>0.15</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.updaterInterval</code> is the interval how often the updater should run (default: <code>1m0s</code>).</li><li><code>.spec.kubernetes.verticalPodAutoscaler.RecommenderInterval</code> is the interval how often metrics should be fetched (default: <code>1m0s</code>).</li></ul><p>⚠️ Please note that if you disable the VPA again then the related <code>CustomResourceDefinition</code>s will remain in your shoot cluster (although, nobody will act on them).
This will also keep all existing <code>VerticalPodAutoscaler</code> objects in the system, including those that might be created by you. You can delete the <code>CustomResourceDefinition</code>s yourself using <code>kubectl delete crd</code> if you want to get rid of them.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cdc20eb6ababa209934b6e4bb03138d6>22 - Shoot Cleanup</h1><h1 id=cleanup-of-shoot-clusters-in-deletion>Cleanup of Shoot clusters in deletion</h1><p>When a shoot cluster is deleted then Gardener tries to gracefully remove most of the Kubernetes resources inside the cluster.
This is to prevent that any infrastructure or other artefacts remain after the shoot deletion.</p><p>The cleanup is performed in four steps.
Some resources are deleted with a grace period, and all resources are forcefully deleted (by removing blocking finalizers) after some time to not block the cluster deletion entirely.</p><p><strong>Cleanup steps:</strong></p><ol><li>All <code>ValidatingWebhookConfiguration</code>s and <code>MutatingWebhookConfiguration</code>s are deleted with a <code>5m</code> grace period. Forceful finalization happens after <code>5m</code>.</li><li>All <code>APIService</code>s and <code>CustomResourceDefinition</code>s are deleted with a <code>5m</code> grace period. Forceful finalization happens after <code>1h</code>.</li><li>All <code>CronJob</code>s, <code>DaemonSet</code>s, <code>Deployment</code>s, <code>Ingress</code>s, <code>Job</code>s, <code>Pod</code>s, <code>ReplicaSet</code>s, <code>ReplicationController</code>s, <code>Service</code>s, <code>StatefulSet</code>s, <code>PersistentVolumeClaim</code>s are deleted with a <code>5m</code> grace period. Forceful finalization happens after <code>5m</code>.<blockquote><p>If the <code>Shoot</code> is annotated with <code>shoot.gardener.cloud/skip-cleanup=true</code> then only <code>Service</code>s and <code>PersistentVolumeClaim</code>s are considered.</p></blockquote></li><li>All <code>VolumeSnapshot</code>s and <code>VolumeSnapshotContent</code>s are deleted with a <code>5m</code> grace period. Forceful finalization happens after <code>1h</code>.</li><li>All <code>Namespace</code>s are deleted without any grace period. Forceful finalization happens after <code>5m</code>.</li></ol><p>It is possible to override the finalization grace periods via annotations on the <code>Shoot</code>:</p><ul><li><code>shoot.gardener.cloud/cleanup-webhooks-finalize-grace-period-seconds</code> (for the resources handled in step 1)</li><li><code>shoot.gardener.cloud/cleanup-extended-apis-finalize-grace-period-seconds</code> (for the resources handled in step 2)</li><li><code>shoot.gardener.cloud/cleanup-kubernetes-resources-finalize-grace-period-seconds</code> (for the resources handled in step 3)</li><li><code>shoot.gardener.cloud/cleanup-namespaces-finalize-grace-period-seconds</code> (for the resources handled in step 4)</li></ul><p>⚠️ If <code>"0"</code> is provided then all resources are finalized immediately without waiting for any graceful deletion.
Please be aware that this might lead to orphaned infrastructure artefacts.</p><h2 id=infrastructure-cleanup-wait-period>Infrastructure Cleanup Wait Period</h2><p>After all above cleanup steps have been performed and the <code>Infrastructure</code> extension resource has been deleted the gardenlet waits for a certain duration to allow controllers to properly cleanup infrastructure resources.</p><p>By default, this duration is set to <code>5m</code>. Only after this time has passed the shoot deletion flow continues with the entire tear-down of the remaining control plane components (including <code>kube-apiserver</code>s, etc.).</p><p>It is also possible to override this wait period via an annotations on the <code>Shoot</code>:</p><ul><li><code>shoot.gardener.cloud/infrastructure-cleanup-wait-period-seconds</code></li></ul><blockquote><p>ℹ️️ All provided period values larger than the above mentioned defaults are ignored.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-6e28d817b7a970b189d7b89840fa8925>23 - Shoot Info Configmap</h1><h1 id=shoot-info-configmap>Shoot Info <code>ConfigMap</code></h1><h2 id=overview>Overview</h2><p>Gardenlet maintains a <a href=https://kubernetes.io/docs/concepts/configuration/configmap/>ConfigMap</a> inside the Shoot cluster that contains information about the cluster itself. The ConfigMap is named <code>shoot-info</code> and located in the <code>kube-system</code> namespace.</p><h2 id=fields>Fields</h2><p>The following fields are provided:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: ConfigMap
metadata:
  name: shoot-info
  namespace: kube-system
data:
  domain: crazy-botany.core.my-custom-domain.com     <span style=color:green># .spec.dns.domain field from the Shoot resource</span>
  extensions: foobar,foobaz                          <span style=color:green># List of extensions that are enabled</span>
  kubernetesVersion: 1.20.1                          <span style=color:green># .spec.kubernetes.version field from the Shoot resource</span>
  maintenanceBegin: 220000+0100                      <span style=color:green># .spec.maintenance.timeWindow.begin field from the Shoot resource</span>
  maintenanceEnd: 230000+0100                        <span style=color:green># .spec.maintenance.timeWindow.end field from the Shoot resource</span>
  nodeNetwork: 10.250.0.0/16                         <span style=color:green># .spec.networking.nodes field from the Shoot resource</span>
  podNetwork: 100.96.0.0/11                          <span style=color:green># .spec.networking.pods field from the Shoot resource</span>
  projectName: dev                                   <span style=color:green># .metadata.name of the Project</span>
  provider: &lt;some-provider-name&gt;                     <span style=color:green># .spec.provider.type field from the Shoot resource</span>
  region: europe-central-1                           <span style=color:green># .spec.region field from the Shoot resource</span>
  serviceNetwork: 100.64.0.0/13                      <span style=color:green># .spec.networking.services field from the Shoot resource</span>
  shootName: crazy-botany                            <span style=color:green># .metadata.name from the Shoot resource</span>
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-530324ec62b83dd4fb4dbc1c64e21ea4>24 - Shoot Maintenance</h1><h1 id=shoot-maintenance>Shoot Maintenance</h1><p>Shoots configure a maintenance time window in which Gardener performs certain operations that may restart the control plane, roll out the nodes, result in higher network traffic, etc.
This document outlines what happens during a shoot maintenance.</p><h2 id=time-window>Time Window</h2><p>Via the <code>.spec.maintenance.timeWindow</code> field in the shoot specification end-users can configure the time window in which maintenance operations are executed.
Gardener runs one maintenance operation per day in this time window:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:
  maintenance:
    timeWindow:
      begin: 220000+0100
      end: 230000+0100
</code></pre></div><p>The offset (<code>+0100</code>) is considered with respect to UTC time.
The minimum time window is <code>30m</code> and the maximum is <code>6h</code>.</p><p>⚠️ Please note that there is no guarantee that a maintenance operation that e.g. starts a node roll-out will finish <em>within</em> the time window.
Especially for large clusters it may take several hours until a graceful rolling update of the worker nodes succeeds (also depending on the workload and the configured pod disruption budgets/termination grace periods).</p><p>Internally, Gardener is subtracting <code>15m</code> from the end of the time window to (best-effort) try to finish the maintenance until the end is reached, however, it might not work in all cases.</p><p>If you don&rsquo;t specify a time window then Gardener will randomly compute it.
You can change it later, of course.</p><h2 id=automatic-version-updates>Automatic Version Updates</h2><p>The <code>.spec.maintenance.autoUpdate</code> field in the shoot specification allows you to control how/whether automatic updates of Kubernetes patch and machine image versions are performed.
Machine image versions are updated per worker pool.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:
  maintenance:
    autoUpdate:
      kubernetesVersion: <span style=color:#00f>true</span>
      machineImageVersion: <span style=color:#00f>true</span>
</code></pre></div><p>During the daily maintenance, the Gardener Controller Manager updates the Shoot&rsquo;s Kubernetes and machine image version if any of the following criteria applies:</p><ul><li>there is a higher version available and the Shoot opted-in for automatic version updates</li><li>the currently used version is <code>expired</code></li></ul><p>Gardener creates events with type <code>MaintenanceDone</code> on the Shoot describing the action performed during maintenance including the reason why an update has been triggered.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>MaintenanceDone  Updated image of worker-pool &#39;coreos-xy&#39; from &#39;coreos&#39; version &#39;xy&#39; to version &#39;abc&#39;. Reason: AutoUpdate of MachineImage configured.
MaintenanceDone  Updated Kubernetes version &#39;0.0.1&#39; to version &#39;0.0.5&#39;. This is an increase in the patch level. Reason: AutoUpdate of Kubernetes version configured.
MaintenanceDone  Updated Kubernetes version &#39;0.0.5&#39; to version &#39;0.1.5&#39;. This is an increase in the minor level. Reason: Kubernetes version expired - force update required.
</code></pre></div><p>Please refer to <a href=/docs/gardener/usage/shoot_versions/>this document</a> for more information about Kubernetes and machine image versions in Gardener.</p><h2 id=cluster-reconciliation>Cluster Reconciliation</h2><p>Gardener administrators/operators can configure the Gardenlet in a way that it only reconciles shoot clusters during their maintenance time windows.
This behaviour is not controllable by end-users but might make sense for large Gardener installations.
Concretely, your shoot will be reconciled regularly during its maintenance time window.
Outside of the maintenance time window it will only reconcile if you change the specification or if you explicitly trigger it, see also <a href=/docs/gardener/usage/shoot_operations/>this document</a>.</p><h2 id=confine-specification-changesupdates-roll-out>Confine Specification Changes/Updates Roll Out</h2><p>Via the <code>.spec.maintenance.confineSpecUpdateRollout</code> field you can control whether you want to make Gardener roll out changes/updates to your shoot specification only during the maintenance time window.
It is <code>false</code> by default, i.e., any change to your shoot specification triggers a reconciliation (even outside of the maintenance time window).
This is helpful if you want to update your shoot but don&rsquo;t want the changes to be applied immediately. One example use-case would be a Kubernetes version upgrade that you want to roll out during the maintenance time window.
Any update to the specification will not increase the <code>.metadata.generation</code> of the <code>Shoot</code> which is something you should be aware of.
Also, even if Gardener administrators/operators have not enabled the &ldquo;reconciliation in maintenance time window only&rdquo; configuration (as mentioned above) then your shoot will only reconcile in the maintenance time window.
The reason is that Gardener cannot differentiate between create/update/reconcile operations.</p><p>⚠️ If <code>confineSpecUpdateRollout=true</code>, please note that if you change the maintenance time window itself then it will only be effective after the upcoming maintenance.</p><p>⚠️ There is one exceptional change in the shoot specification that triggers an immediate roll out which is changes to the <code>.spec.hibernation.enabled</code> field.
If you hibernate or wake-up your shoot then Gardener gets active right away.</p><h2 id=special-operations-during-maintenance>Special Operations During Maintenance</h2><p>The shoot maintenance controller triggers special operations that are performed as part of the shoot reconciliation.</p><h3 id=infrastructure-and-dnsrecord-reconciliation><code>Infrastructure</code> and <code>DNSRecord</code> Reconciliation</h3><p>The reconciliation of the <code>Infrastructure</code> and <code>DNSRecord</code> extension resources is only demanded during the shoot&rsquo;s maintenance time window.
The rationale behind it is to prevent sending too many requests against the cloud provider APIs, especially on large landscapes or if a user has many shoot clusters in the same cloud provider account.</p><h3 id=restart-control-plane-controllers>Restart Control Plane Controllers</h3><p>Gardener operators can make Gardener restart/delete certain control plane pods during a shoot maintenance.
This feature helps to automatically solve service denials of controllers due to stale caches, dead-locks or starving routines.</p><p>Please note that these are exceptional cases but they are observed from time to time.
Gardener, for example, takes this precautionary measure for <code>kube-controller-manager</code> pods.</p><p>See <a href=/docs/gardener/extensions/shoot-maintenance/>this document</a> to see how extension developers can extend this behaviour.</p><h3 id=restart-some-core-addons>Restart Some Core Addons</h3><p>Gardener operators can make Gardener restart some core addons, at the moment only CoreDNS, during a shoot maintenance.</p><p>CoreDNS benefits from this feature as it automatically solve problems with clients stuck to single replica of the deployment and thus overloading it.
Please note that these are exceptional cases but they are observed from time to time.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-97d7360c220fe009f695e761863dd64c>25 - Shoot Network Policies</h1><h2 id=network-policies-in-the-shoot-cluster>Network policies in the Shoot Cluster</h2><p>In addition to deploying network policies <a href=/docs/gardener/development/seed_network_policies/>into the Seed</a>,
Gardener deploys network policies into the <code>kube-system</code> namespace of the Shoot.
These network policies are used by Shoot system components (that are not part of the control plane).
Other namespaces in the Shoot do not contain network policies deployed by Gardener.</p><p>As best practice, every pod deployed into the <code>kube-system</code> namespace should use appropriate network policies in order to only allow <strong>required</strong> network traffic.
Therefore, pods should have labels matching to the selectors of the available network policies.</p><p>Gardener deploys the following network policies:</p><pre><code>NAME                                       POD-SELECTOR
gardener.cloud--allow-dns                  k8s-app in (kube-dns)
gardener.cloud--allow-from-seed            networking.gardener.cloud/from-seed=allowed
gardener.cloud--allow-to-apiserver         networking.gardener.cloud/to-apiserver=allowed
gardener.cloud--allow-to-dns               networking.gardener.cloud/to-dns=allowed
gardener.cloud--allow-to-from-nginx        app=nginx-ingress
gardener.cloud--allow-to-kubelet           networking.gardener.cloud/to-kubelet=allowed
gardener.cloud--allow-to-public-networks   networking.gardener.cloud/to-public-networks=allowed
gardener.cloud--allow-vpn                  app=vpn-shoot
</code></pre><p>Additionally, there can be network policies deployed by Gardener extensions such as <a href=https://github.com/gardener/gardener-extension-networking-calico>extension-calico</a>.</p><pre><code>NAME                                       POD-SELECTOR
gardener.cloud--allow-from-calico-node     k8s-app=calico-typha
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-ee4728cf7f71a3c0ae90d16a7c9cf79a>26 - Shoot Networking</h1><h1 id=shoot-networking>Shoot Networking</h1><p>This document contains network related information for Shoot clusters.</p><h2 id=pod-network>Pod Network</h2><p>A Pod network is imperative for any kind of cluster communication with Pods not started within the Node&rsquo;s host network.
More information about the Kubernetes network model can be found <a href=https://kubernetes.io/docs/concepts/cluster-administration/networking/>here</a>.</p><p>Gardener allows users to configure the Pod network&rsquo;s CIDR during Shoot creation:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: Shoot
spec:
  networking:
    type: &lt;some-network-extension-name&gt; <span style=color:green># {calico,cilium}</span>
    pods: 100.96.0.0/16
    nodes: ...
    services: ...
</code></pre></div><blockquote><p>⚠️ The <code>networking.pods</code> IP configuration is immutable and cannot be changed afterwards.
Please consider the following paragraph to choose a configuration which will meet your demands.</p></blockquote><p>One of the network plugin&rsquo;s (CNI) tasks is to assign IP addresses to Pods started in the Pod network.
Different network plugins come with different IP address management (IPAM) features, so we can&rsquo;t give any definite advice how IP ranges should be configured.
Nevertheless, we want to outline the standard configuration.</p><p>Information in <code>.spec.networking.pods</code> matches the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/>&ndash;cluster-cidr flag</a> of the Kube-Controller-Manager of your Shoot cluster.
This IP range is divided into smaller subnets, also called <code>podCIDRs</code> (default mask <code>/24</code>) and assigned to Node objects <code>.spec.podCIDR</code>.
Pods get their IP address from this smaller node subnet in a default IPAM setup.
Thus, it must be guaranteed that enough of these subnets can be created for the maximum amount of nodes you expect in the cluster.</p><p><em><strong>Example 1</strong></em></p><pre><code>Pod network: 100.96.0.0/16
nodeCIDRMaskSize: /24
-------------------------

Number of podCIDRs: 256 --&gt; max. Node count 
Number of IPs per podCIDRs: 256
</code></pre><p>With the configuration above a Shoot cluster can at most have <strong>256 nodes</strong> which are ready to run workload in the Pod network.</p><p><em><strong>Example 2</strong></em></p><pre><code>Pod network: 100.96.0.0/20
nodeCIDRMaskSize: /24
-------------------------

Number of podCIDRs: 16 --&gt; max. Node count 
Number of IPs per podCIDRs: 256
</code></pre><p>With the configuration above a Shoot cluster can at most have <strong>16 nodes</strong> which are ready to run workload in the Pod network.</p><p>Beside the configuration in <code>.spec.networking.pods</code>, users can tune the <code>nodeCIDRMaskSize</code> used by Kube-Controller-Manager on shoot creation.
A smaller IP range per node means more <code>podCIDRs</code> and thus the ability to provision more nodes in the cluster, but less available IPs for Pods running on each of the nodes.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: Shoot
spec:
  kubeControllerManager:
    nodeCIDRMaskSize: 24 (default)
</code></pre></div><blockquote><p>⚠️ The <code>nodeCIDRMaskSize</code> configuration is immutable and cannot be changed afterwards.</p></blockquote><p><em><strong>Example 3</strong></em></p><pre><code>Pod network: 100.96.0.0/20
nodeCIDRMaskSize: /25
-------------------------

Number of podCIDRs: 32 --&gt; max. Node count 
Number of IPs per podCIDRs: 128
</code></pre><p>With the configuration above a Shoot cluster can at most have <strong>32 nodes</strong> which are ready to run workload in the Pod network.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-aa95d2c4d5739a992f6e2fe3ce04db73>27 - Shoot Operations</h1><h1 id=trigger-shoot-operations>Trigger shoot operations</h1><p>You can trigger a few explicit operations by annotating the <code>Shoot</code> with an operation annotation.
This might allow you to induct certain behavior without the need to change the <code>Shoot</code> specification.
Some of the operations can also not be caused by changing something in the shoot specification because they can&rsquo;t properly be reflected here.
Note, once the triggered operation is considered by the controllers, the annotation will be automatically removed and you have to add it each time you want to trigger the operation.</p><p>Please note: If <code>.spec.maintenance.confineSpecUpdateRollout=true</code> then the only way to trigger a shoot reconciliation is by setting the <code>reconcile</code> operation, see below.</p><h2 id=immediate-reconciliation>Immediate reconciliation</h2><p>Annotate the shoot with <code>gardener.cloud/operation=reconcile</code> to make the <code>gardenlet</code> start a reconciliation operation without changing the shoot spec and possibly without being in its maintenance time window:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=reconcile
</code></pre></div><h2 id=immediate-maintenance>Immediate maintenance</h2><p>Annotate the shoot with <code>gardener.cloud/operation=maintain</code> to make the <code>gardener-controller-manager</code> start maintaining your shoot immediately (possibly without being in its maintenance time window).
If no reconciliation starts then nothing needed to be maintained:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=maintain
</code></pre></div><h2 id=retry-failed-operation>Retry failed operation</h2><p>Annotate the shoot with <code>gardener.cloud/operation=retry</code> to make the <code>gardenlet</code> start a new reconciliation loop on a failed shoot.
Failed shoots are only reconciled again if a new Gardener version is deployed, the shoot specification is changed or this annotation is set</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=retry
</code></pre></div><h2 id=rotate-kubeconfig-credentials>Rotate kubeconfig credentials</h2><p>Annotate the shoot with <code>gardener.cloud/operation=rotate-kubeconfig-credentials</code> to make the <code>gardenlet</code> exchange the credentials in your shoot cluster&rsquo;s kubeconfig.
This operation is not allowed for shoot clusters that are already in deletion.
Please note that only the token (and basic auth password, if enabled) are exchanged. The cluster CAs remain the same.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation=rotate-kubeconfig-credentials
</code></pre></div><p>You can check the <code>.status.credentials.rotation.kubeconfig</code> field in the <code>Shoot</code> to see when the rotation was last initiated or last completed.</p><h2 id=restart-systemd-services-on-particular-worker-nodes>Restart systemd services on particular worker nodes</h2><p>It is possible to make Gardener restart particular systemd services on your shoot worker nodes if needed.
The annotation is not set on the <code>Shoot</code> resource but directly on the <code>Node</code> object you want to target.
For example, the following will restart both the <code>kubelet</code> and the <code>docker</code> services:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl annotate node &lt;node-name&gt; worker.gardener.cloud/restart-systemd-services=kubelet,docker
</code></pre></div><p>It may take up to a minute until the service is restarted.
The annotation will be removed from the <code>Node</code> object after all specified systemd services have been restarted.
It will also be removed even if the restart of one or more services failed.</p><blockquote><p>ℹ️ In the example mentioned above, you could additionally verify when/whether the kubelet restarted by using <code>kubectl describe node &lt;node-name></code> and looking for such a <code>Starting kubelet</code> event.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-ce24e382e35468e5c98ad64130e0653b>28 - Shoot Purposes</h1><h1 id=shoot-cluster-purpose>Shoot Cluster Purpose</h1><p>The <code>Shoot</code> resource contains a <code>.spec.purpose</code> field indicating how the shoot is used whose allowed values are as follows:</p><ul><li><code>evaluation</code> (default): Indicates that the shoot cluster is for evaluation scenarios.</li><li><code>development</code>: Indicates that the shoot cluster is for development scenarios.</li><li><code>testing</code>: Indicates that the shoot cluster is for testing scenarios.</li><li><code>production</code>: Indicates that the shoot cluster is for production scenarios.</li><li><code>infrastructure</code>: Indicates that the shoot cluster is for infrastructure scenarios (only allowed for shoots in the <code>garden</code> namespace).</li></ul><h2 id=behavioral-differences>Behavioral Differences</h2><p>The following enlists the differences in the way the shoot clusters are set up based on the selected purpose:</p><ul><li><code>testing</code> shoot clusters <strong>do not</strong> get a monitoring or a logging stack as part of their control planes.</li><li><code>production</code> shoot clusters get at least two replicas of the <code>kube-apiserver</code> for their control planes.
Auto-scaling scale down of the main ETCD is disabled for such clusters.</li></ul><p>There are also differences with respect to how <code>testing</code> shoots are scheduled after creation, please consult the <a href=/docs/gardener/concepts/scheduler/>Scheduler documentation</a>.</p><h2 id=future-steps>Future Steps</h2><p>We might introduce more behavioral difference depending on the shoot purpose in the future.
As of today, there are no plans yet.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c9d857194e434604d822b4e8baf96323>29 - Shoot Status</h1><h1 id=shoot-status>Shoot Status</h1><p>This document provides an overview of the <a href=/docs/gardener/api-reference/core/#shootstatus>ShootStatus</a>.</p><h2 id=conditions>Conditions</h2><p>The Shoot status consists of a set of conditions. A <a href=/docs/gardener/api-reference/core/#condition>Condition</a> has the following fields:</p><table><thead><tr><th>Field name</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></td><td>Name of the condition.</td></tr><tr><td><code>status</code></td><td>Indicates whether the condition is applicable, with possible values <code>True</code>, <code>False</code>, <code>Unknown</code>, or <code>Progressing</code>.</td></tr><tr><td><code>lastTransitionTime</code></td><td>Timestamp for when the condition last transitioned from one status to another.</td></tr><tr><td><code>lastUpdateTime</code></td><td>Timestamp for when the condition was updated. Usually changes when <code>reason</code> or <code>message</code> in condition is updated.</td></tr><tr><td><code>reason</code></td><td>Machine-readable, UpperCamelCase text indicating the reason for the condition&rsquo;s last transition.</td></tr><tr><td><code>message</code></td><td>Human-readable message indicating details about the last status transition.</td></tr><tr><td><code>codes</code></td><td>Well-defined error codes in case the condition reports a problem.</td></tr></tbody></table><p>Currently the available Shoot condition types are:</p><ul><li><p><code>APIServerAvailable</code></p><p>This condition type indicates whether the Shoot&rsquo;s kube-apiserver is available or not. In particular, the <code>/healthz</code> endpoint of the kube-apiserver is called, and the expected response code is <code>HTTP 200</code>.</p></li><li><p><code>ControlPlaneHealthy</code></p><p>This condition type indicates whether all the control plane components deployed to the Shoot&rsquo;s namespace in the Seed do exist and are running fine.</p></li><li><p><code>EveryNodeReady</code></p><p>This condition type indicates whether at least the requested minimum number of Nodes is present per each worker pool and whether all Nodes are healthy.</p></li><li><p><code>SystemComponentsHealthy</code></p><p>This condition type indicates whether all system components deployed to the <code>kube-system</code> namespace in the shoot do exist and are running fine. It also reflects whether the tunnel connection between the control plane and the Shoot networks can be established.</p></li></ul><p>The Shoot conditions are maintained by the <a href=https://github.com/gardener/gardener/blob/master/pkg/gardenlet/controller/shoot/shoot_care_control.go>shoot care control</a> of gardenlet.</p><h3 id=sync-period>Sync Period</h3><p>The condition checks are executed periodically at interval which is configurable in the <code>GardenletConfiguration</code> (<code>.controllers.shootCare.syncPeriod</code>, defaults to <code>1m</code>).</p><h3 id=condition-thresholds>Condition Thresholds</h3><p>The <code>GardenletConfiguration</code> also allows configuring condition thresholds (<code>controllers.shootCare.conditionThresholds</code>). Condition threshold is the amount of time to consider condition as <code>Processing</code> on condition status changes.</p><p>Let&rsquo;s check the following example to get better understanding. Let&rsquo;s say that the <code>APIServerAvailable</code> condition of our Shoot is with status <code>True</code>. If the next condition check fails (for example kube-apiserver becomes unreachable), then the condition first goes to <code>Processing</code> state. Only if this state remains for condition threshold amount of time, then the condition finally is updated to <code>False</code>.</p><h3 id=constraints>Constraints</h3><p>Constraints represent conditions of a Shoot’s current state that constraint some operations on it.
Currently there are two constraints:</p><p><strong><code>HibernationPossible</code></strong>:</p><p>This constraint indicates whether a Shoot is allowed to be hibernated.
The rationale behind this constraint is that a Shoot can have <code>ValidatingWebhookConfiguration</code>s or <code>MutatingWebhookConfiguration</code>s acting on resources that are critical for waking up a cluster.
For example, if a webhook has rules for <code>CREATE/UPDATE</code> Pods or Nodes and <code>failurePolicy=Fail</code>, the webhook will block joining <code>Nodes</code> and creating critical system component Pods and thus block the entire wakeup operation, because the server backing the webhook is not running.</p><p>Even if the <code>failurePolicy</code> is set to <code>Ignore</code>, high timeouts (<code>>15s</code>) can lead to blocking requests of control plane components.
That&rsquo;s because most control-plane API calls are made with a client-side timeout of <code>30s</code>, so if a webhook has <code>timeoutSeconds=30</code>
the overall request might still fail as there is overhead in communication with the API server and potential other webhooks.
Generally, it&rsquo;s <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#timeouts>best pratice</a> to specify low timeouts in WebhookConfigs.
Also, it&rsquo;s <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#avoiding-operating-on-the-kube-system-namespace>best practice</a>
to exclude the <code>kube-system</code> namespace from webhooks to avoid blocking critical operations on system components of the cluster.
Shoot owners can do so by adding a <code>namespaceSelector</code> similar to this one to their webhook configurations:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>namespaceSelector:
  matchExpressions:
  - key: gardener.cloud/purpose
    operator: NotIn
    values:
    - kube-system
</code></pre></div><p>If the Shoot still has webhooks with either <code>failurePolicy={Fail,nil}</code> or <code>failurePolicy=Ignore && timeoutSeconds>15</code> that act on <a href=https://github.com/gardener/gardener/blob/master/pkg/operation/botanist/matchers/matcher.go#L60>critical resources</a> in the <code>kube-system</code> namespace, Gardener will set the <code>HibernationPossible</code> to <code>False</code> indicating, that the Shoot can probably not be woken up again after hibernation without manual intervention of the Gardener Operator.
<code>gardener-apiserver</code> will prevent any Shoot with the <code>HibernationPossible</code> constraint set to <code>False</code> from being hibernated, that is via manual hibernation as well as scheduled hibernation.</p><p><strong><code>MaintenancePreconditionsSatisfied</code></strong>:</p><p>This constraint indicates whether all preconditions for a safe maintenance operation are satisfied (see also <a href=/docs/gardener/usage/shoot_maintenance/>this document</a> for more information about what happens during a shoot maintenance).
As of today, the same checks as in the <code>HibernationPossible</code> constraint are being performed (user-deployed webhooks that might interfere with potential rolling updates of shoot worker nodes).
There is no further action being performed on this constraint&rsquo;s status (maintenance is still being performed).
It is meant to make the user aware of potential problems that might occur due to his configurations.</p><h3 id=last-operation>Last Operation</h3><p>The Shoot status holds information about the last operation that is performed on the Shoot. The last operation field reflects overall progress and the tasks that are currently being executed. Allowed operation types are <code>Create</code>, <code>Reconcile</code>, <code>Delete</code>, <code>Migrate</code> and <code>Restore</code>. Allowed operation states are <code>Processing</code>, <code>Succeeded</code>, <code>Error</code>, <code>Failed</code>, <code>Pending</code> and <code>Aborted</code>. An operation in <code>Error</code> state is an operation that will be retried for a configurable amount of time (<code>controllers.shoot.retryDuration</code> field in <code>GardenletConfiguration</code>, defaults to <code>12h</code>). If the operation cannot complete successfully for the configured retry duration, it will be marked as <code>Failed</code>. An operation in <code>Failed</code> state is an operation that won&rsquo;t be retried automatically (to retry such an operation, see <a href=/docs/gardener/usage/shoot_operations/#retry-failed-operation>Retry failed operation</a>).</p><h3 id=last-errors>Last Errors</h3><p>The Shoot status also contains information about the last occurred error(s) (if any) during an operation. A <a href=/docs/gardener/api-reference/core/#lasterror>LastError</a> consists of identifier of the task returned error, human-readable message of the error and error codes (if any) associated with the error.</p><h3 id=error-codes>Error Codes</h3><p>Known error codes are:</p><ul><li><code>ERR_INFRA_UNAUTHENTICATED</code> - indicates that the last error occurred due to the client request not being completed because it lacks valid authentication credentials for the requested resource. It is classified as a non-retryable error code.</li><li><code>ERR_INFRA_UNAUTHORIZED</code> - indicates that the last error occurred due to the server understanding the request but refusing to authorize it. It is classified as a non-retryable error code.</li><li><code>ERR_INFRA_QUOTA_EXCEEDED</code> - indicates that the last error occurred due to infrastructure quota limits. It is classified as a non-retryable error code.</li><li><code>ERR_INFRA_RATE_LIMITS_EXCEEDED</code> - indicates that the last error occurred due to exceeded infrastructure request rate limits.</li><li><code>ERR_INFRA_DEPENDENCIES</code> - indicates that the last error occurred due to dependent objects on the infrastructure level. It is classified as a non-retryable error code.</li><li><code>ERR_RETRYABLE_INFRA_DEPENDENCIES</code> - indicates that the last error occurred due to dependent objects on the infrastructure level, but the operation should be retried.</li><li><code>ERR_INFRA_RESOURCES_DEPLETED</code> - indicates that the last error occurred due to depleted resource in the infrastructure.</li><li><code>ERR_CLEANUP_CLUSTER_RESOURCES</code> - indicates that the last error occurred due to resources in the cluster that are stuck in deletion.</li><li><code>ERR_CONFIGURATION_PROBLEM</code> - indicates that the last error occurred due to a configuration problem. It is classified as a non-retryable error code.</li><li><code>ERR_RETRYABLE_CONFIGURATION_PROBLEM</code> - indicates that the last error occurred due to a retryable configuration problem. &ldquo;Retryable&rdquo; means that the occurred error is likely to be resolved in a ungraceful manner after given period of time.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-442c557a128e8fd2c4f787af32cd900f>30 - Shoot Updates</h1><h1 id=shoot-updates-and-upgrades>Shoot Updates and Upgrades</h1><p>This document describes what happens during shoot updates (changes incorporated in a newly deployed Gardener version) and during shoot upgrades (changes for version controllable by end-users).</p><h2 id=updates>Updates</h2><p>Updates to all aspects of the shoot cluster happen when the gardenlet reconciles the <code>Shoot</code> resource.</p><h3 id=when-are-reconciliations-triggered>When are Reconciliations Triggered</h3><p>Generally, when you change the specification of your <code>Shoot</code> the reconciliation will start immediately, potentially updating your cluster.
Please note that you can also confine the reconciliation triggered due to your specification updates to the cluster&rsquo;s maintenance time window. Please find more information <a href=/docs/gardener/usage/shoot_maintenance/#confine-specification-changesupdates-roll-out>here</a>.</p><p>You can also annotate your shoot with special operation annotations (see <a href=/docs/gardener/usage/shoot_operations/>this document</a>) which will cause the reconciliation to start due to your actions.</p><p>There is also an automatic reconciliation by Gardener.
The period, i.e., how often it is performed, depends on the configuration of the Gardener administrators/operators.
In some Gardener installations the operators might enable &ldquo;reconciliation in maintenance time window only&rdquo; (<a href=/docs/gardener/usage/shoot_maintenance/#cluster-reconciliation>more information</a>) which will result in at least one reconciliation during the time configured in the <code>Shoot</code>&rsquo;s <code>.spec.maintenance.timeWindow</code> field.</p><h3 id=which-updates-are-applied>Which Updates are Applied</h3><p>As end-users can only control the <code>Shoot</code> resource&rsquo;s specification but not the used Gardener version, they don&rsquo;t have any influence on which of the updates are rolled out (other than those settings configurable in the <code>Shoot</code>).
A Gardener operator can deploy a new Gardener version at any point in time.
Any subsequent reconciliation of <code>Shoot</code>s will update them by rolling out the changes incorporated in this new Gardener version.</p><p>Some examples for such shoot updates are:</p><ul><li>Add a new/remove an old component to/from the shoot&rsquo;s control plane running in the seed, or to/from the shoot&rsquo;s system components running on the worker nodes.</li><li>Change the configuration of an existing control plane/system component.</li><li>Restart of existing control plane/system components (this might result in a short unavailability of the Kubernetes API server, e.g., when etcd or a kube-apiserver itself is being restarted)</li></ul><h3 id=behavioural-changes>Behavioural Changes</h3><p>Generally, some of such updates (e.g., configuration changes) could theoretically result in different behaviour of controllers.
If such changes would be backwards-incompatible then we usually follow one of those approaches (depends on the concrete change):</p><ul><li>Only apply the change for new clusters.</li><li>Expose a new field in the <code>Shoot</code> resource that lets users control this changed behaviour to enable it at a convenient point in time.</li><li>Put the change behind an alpha feature gate (disabled by default) in the gardenlet (only controllable by Gardener operators) which will be promoted to beta (enabled by default) in subsequent releases (in this case, end-users have no influence on when the behaviour changes - Gardener operators should inform their end-users and provide clear timelines when they will enable the feature gate).</li></ul><h2 id=upgrades>Upgrades</h2><p>We consider shoot upgrades to change either the</p><ul><li>Kubernetes version (<code>.spec.kubernetes.version</code>)</li><li>Kubernetes version of the worker pool if specified (<code>.spec.provider.workers[].kubernetes.version</code>)</li><li>Machine image version of at least one worker pool (<code>.spec.provider.workers[].machine.image.version</code>)</li></ul><p>Generally, an upgrade is also performed through a reconciliation of the <code>Shoot</code> resource, i.e., the same concepts like for <a href=#updates>shoot updates</a> apply.
If an end-user triggers an upgrade (e.g., by changing the Kubernetes version) after a new Gardener version was deployed but before the shoot was reconciled again, then this upgrade might incorporate the changes delivered with this new Gardener version.</p><h3 id=in-place-vs-rolling-updates>In-Place vs. Rolling Updates</h3><p>If the Kubernetes patch version is changed then the upgrade happens in-place.
This means that the shoot worker nodes remain untouched and only the <code>kubelet</code> process restarts with the new Kubernetes version binary.
The same applies for configuration changes of the kubelet.</p><p>If the Kubernetes minor version is changed then the upgrade is done in a &ldquo;rolling update&rdquo; fashion, similar to how pods in Kubernetes are updated (when backed by a <code>Deployment</code>).
The worker nodes will be terminated one after another and replaced by new machines.
The existing workload is gracefully drained and evicted from the old worker nodes to new worker nodes, respecting the configured <code>PodDisruptionBudget</code>s (see <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb/>Kubernetes documentation</a>).</p><h4 id=customize-rolling-update-behaviour-of-shoot-worker-nodes>Customize Rolling Update Behaviour of Shoot Worker Nodes</h4><p>The <code>.spec.provider.workers[]</code> list exposes two fields that you might configure based on your workload&rsquo;s needs: <code>maxSurge</code> and <code>maxUnavailable</code>.
The same concepts <a href=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment>like in Kubernetes</a> apply.
Additionally, you might customize how the machine-controller-manager (abbrev.: MCM; the component instrumenting this rolling update) is behaving. You can configure the following fields in <code>.spec.provider.worker[].machineControllerManager</code>:</p><ul><li><code>machineDrainTimeout</code>: Timeout (in duration) used while draining of machine before deletion, beyond which MCM forcefully deletes machine (default: <code>10m</code>).</li><li><code>machineHealthTimeout</code>: Timeout (in duration) used while re-joining (in case of temporary health issues) of machine before it is declared as failed (default: <code>10m</code>).</li><li><code>machineCreationTimeout</code>: Timeout (in duration) used while joining (during creation) of machine before it is declared as failed (default: <code>10m</code>).</li><li><code>maxEvictRetries</code>: Maximum number of times evicts would be attempted on a pod before it is forcibly deleted during draining of a machine (default: <code>10</code>).</li><li><code>nodeConditions</code>: List of case-sensitive node-conditions which will change a machine to a <code>Failed</code> state after the <code>machineHealthTimeout</code> duration. It may further be replaced with a new machine if the machine is backed by a machine-set object (defaults: <code>KernelDeadlock</code>, <code>ReadonlyFilesystem</code> , <code>DiskPressure</code>).</li></ul><h4 id=rolling-update-triggers>Rolling Update Triggers</h4><p>Apart from the above mentioned triggers, a rolling update of the shoot worker nodes is also triggered for some changes to your worker pool specification (<code>.spec.provider.workers[]</code>, even if you don&rsquo;t change the Kubernetes or machine image version).
The complete list of fields that trigger a rolling update:</p><ul><li><code>.spec.kubernetes.version</code> (except for patch version changes)</li><li><code>.spec.provider.workers[].machine.image.name</code></li><li><code>.spec.provider.workers[].machine.image.version</code></li><li><code>.spec.provider.workers[].machine.type</code></li><li><code>.spec.provider.workers[].volume.type</code></li><li><code>.spec.provider.workers[].volume.size</code></li><li><code>.spec.provider.workers[].providerConfig</code></li><li><code>.spec.provider.workers[].cri.name</code></li><li><code>.spec.provider.workers[].kubernetes.version</code> (except for patch version changes)</li></ul><p>Generally, the provider extension controllers might have additional constraints for changes leading to rolling updates, so please consult the respective documentation as well.</p><h2 id=related-documentation>Related Documentation</h2><ul><li><a href=/docs/gardener/usage/shoot_operations/>Shoot Operations</a></li><li><a href=/docs/gardener/usage/shoot_maintenance/>Shoot Maintenance</a></li><li><a href=/docs/gardener/usage/shoot_maintenance/#confine-specification-changesupdates-roll-out>Confine Specification Changes/Updates Roll Out To Maintenance Time Window</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-76b0c7e5bbbfbafbbf70acec450a467b>31 - Shoot Versions</h1><h1 id=shoot-kubernetes-and-operating-system-versioning-in-gardener>Shoot Kubernetes and Operating System Versioning in Gardener</h1><h2 id=motivation>Motivation</h2><p>On the one hand-side, Gardener is responsible for managing the Kubernetes and the Operating System (OS) versions of its Shoot clusters.
On the other hand-side, Gardener needs to be configured and updated based on the availability and support of the Kubernetes and Operating System version it provides.
For instance, the Kubernetes community releases <strong>minor</strong> versions roughly every three months and usually maintains <strong>three minor</strong> versions (the current and the last two) with bug fixes and security updates.
Patch releases are done more frequently.</p><p>When using the term <code>Machine image</code> in the following, we refer to the OS version that comes with the machine image of the node/worker pool of a Gardener Shoot cluster.
As such we are not referring to the <code>CloudProvider</code> specific machine image like the <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html><code>AMI</code></a> for AWS.
For more information how Gardener maps machine image versions to <code>CloudProvider</code> specific machine images, take a look at the individual gardener extension providers
such as the <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/usage-as-operator/>provider for AWS</a>.</p><p>Gardener should be configured accordingly to reflect the &ldquo;logical state&rdquo; of a version.
It should be possible to define the Kubernetes or Machine image versions that still receive bug fixes and security patches, and also vice-versa to define the version that are out-of-maintenance and are potentially vulnerable.
Moreover, this allows Gardener to &ldquo;understand&rdquo; the current state of a version and act upon it (more information in the following sections).</p><h2 id=overview>Overview</h2><p><strong>As a Gardener operator</strong>:</p><ul><li>I can classify a version based on it&rsquo;s logical state (<code>preview</code>, <code>supported</code>, <code>deprecated</code> and <code>expired</code> see <a href=#version-classifications>Version Classification</a>).</li><li>I can define which Machine image and Kubernetes versions are eligible for the auto update of clusters during the maintenance time.</li><li>I can disallow the creation of clusters having a certain version (think of severe security issues).</li></ul><p><strong>As an end-user/Shoot owner of Gardener</strong>:</p><ul><li>I can get information about which Kubernetes and Machine image versions exist and their classification.</li><li>I can determine the time when my Shoot clusters Machine image and Kubernetes version will be forcefully updated to the next patch or minor version (in case the cluster is running a deprecated version with an expiration date).</li><li>I can get this information via API from the <code>CloudProfile</code>.</li></ul><h2 id=version-classifications>Version Classifications</h2><p>Administrators can classify versions into four distinct &ldquo;logical states&rdquo;: <code>preview</code>, <code>supported</code>, <code>deprecated</code> and <code>expired</code>.
The version classification serves as a &ldquo;point-of-reference&rdquo; for end-users and also has implications during shoot creation and the maintenance time.</p><p>If a version is unclassified, Gardener cannot make those decision based on the &ldquo;logical state&rdquo;.
Nevertheless, Gardener can operate without version classifications and can be added at any time to the Kubernetes and machine image versions in the <code>CloudProfile</code>.</p><p>As a best practice, versions usually start with the classification <code>preview</code>, then are promoted to <code>supported</code>, eventually <code>deprecated</code> and finally <code>expired</code>.
This information is programmatically available in the <code>CloudProfiles</code> of the Garden cluster.</p><ul><li><p><strong>preview:</strong> A <code>preview</code> version is a new version that has not yet undergone thorough testing, possibly a new release, and needs time to be validated.
Due to its short early age, there is a higher probability of undiscovered issues and is therefore not yet recommended for production usage.
A Shoot does not update (neither <code>auto-update</code> or <code>force-update</code>) to a <code>preview</code> version during the maintenance time.
Also <code>preview</code> versions are not considered for the defaulting to the highest available version when deliberately omitting the patch version during Shoot creation.
Typically, after a fresh release of a new Kubernetes (e.g. v1.23.0) or Machine image version (e.g. coreos-2023.5), the operator tags it as <code>preview</code> until he has gained sufficient experience and regards this version to be reliable.
After the operator gained sufficient trust, the version can be manually promoted to <code>supported</code>.</p></li><li><p><strong>supported:</strong> A <code>supported</code> version is the recommended version for new and existing Shoot clusters. New Shoot clusters should use and existing clusters should update to this version.
Typically for Kubernetes versions, the latest Kubernetes patch versions of the actual (if not still in <code>preview</code>) and the last 3 minor Kubernetes versions are maintained by the community. An operator could define these versions as being <code>supported</code> (e.g. v1.22.1, v1.21.4, v1.20.9 and v1.19.12).</p></li><li><p><strong>deprecated:</strong> A <code>deprecated</code> version is a version that approaches the end of its lifecycle and can contain issues which are probably resolved in a supported version.
New Shoots should not use this version any more.
Existing Shoots will be updated to a newer version if <code>auto-update</code> is enabled (<code>.spec.maintenance.autoUpdate.kubernetesVersion</code> for Kubernetes version <code>auto-update</code>, or <code>.spec.maintenance.autoUpdate.machineImageVersion</code> for machine machine image version <code>auto-update</code>).
Using automatic upgrades, however, does not guarantee that a Shoot runs a non-deprecated version, as the latest version (overall or of the minor version) can be deprecated as well.
Deprecated versions <strong>should</strong> have an expiration date set for eventual expiration.</p></li><li><p><strong>expired:</strong> An <code>expired</code> versions has an expiration date (based on the <a href=https://golang.org/src/time/time.go>Golang time package</a>) in the past.
New clusters with that version cannot be created and existing clusters are forcefully migrated to a higher version during the maintenance time.</p></li></ul><p>Below is an example how the relevant section of the <code>CloudProfile</code> might look like:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: CloudProfile
metadata:
  name: alicloud
spec:
  kubernetes:
    versions:
      - classification: supported
        version: 1.17.1
      - classification: deprecated
        expirationDate: <span style=color:#a31515>&#34;2020-07-24T16:13:26Z&#34;</span>
        version: 1.17.0
      - classification: preview
        version: 1.16.6
      - classification: supported
        version: 1.16.5
      - classification: deprecated
        expirationDate: <span style=color:#a31515>&#34;2020-04-25T09:30:40Z&#34;</span>
        version: 1.16.4
      - classification: supported
        version: 1.15.7
      - classification: deprecated
        expirationDate: <span style=color:#a31515>&#34;2020-06-09T14:01:39Z&#34;</span>
        version: 1.15.6
</code></pre></div><h2 id=version-requirements-kubernetes-and-machine-image>Version Requirements (Kubernetes and Machine image)</h2><p>The Gardener API server enforces the following requirements for versions:</p><h3 id=deletion-of-a-version>Deletion of a version</h3><ul><li>A version that is in use by a Shoot cannot be deleted from the <code>CloudProfile</code>.</li></ul><h3 id=adding-a-version>Adding a version</h3><ul><li>A version must not have an expiration date in the past.</li><li>There can be only one <code>supported</code> version per minor version.</li><li>The latest Kubernetes version cannot have an expiration date.</li><li>The latest version for a machine image can have an expiration date. [*]</li></ul><p><sub>[*] Useful for cases in which support for given machine image needs to be deprecated and removed (for example the machine image reaches end of life).</sub></p><h2 id=forceful-migration-of-expired-versions>Forceful migration of expired versions</h2><p>If a Shoot is running a version after its expiration date has passed, it will be forcefully migrated during its maintenance time.
This happens <strong>even if the owner has opted out of automatic cluster updates!</strong></p><p>For <strong>Machine images</strong>, the Shoots worker pools will be updated to the latest <code>non-preview</code> version of the pools respective image.</p><p>For <strong>Kubernetes versions</strong>, the forceful update picks the latest <code>non-preview</code> patch version of the current minor version.</p><p>If the cluster is already on the latest patch version and the latest patch version is also expired,
it will continue with the latest patch version of the <strong>next consecutive minor Kubernetes version</strong>, so <strong>it will result in an
update of a minor Kubernetes version!</strong></p><p>Please note, that multiple consecutive minor version upgrades are possible.
This can occur if the Shoot is updated to a version that in turn is also <code>expired</code>.
In this case, the version is again upgraded in the <strong>next</strong> maintenance time.</p><p><strong>Depending on the circumstances described above, it can happen that the cluster receives multiple consecutive minor Kubernetes version updates!</strong></p><p>Kubernetes &ldquo;minor version jumps&rdquo; are not allowed - meaning to skip the update to the consecutive minor version and directly update to any version after that.
For instance, the version <code>1.10.x</code> can only update to a version <code>1.11.x</code>, not to <code>1.12.x</code> or any other version.
This is because Kubernetes does not guarantee upgradeability in this case, leading to possibly broken Shoot clusters.
The administrator has to set up the <code>CloudProfile</code> in such a way, that consecutive Kubernetes minor versions are available.
Otherwise, Shoot clusters will fail to upgrade during the maintenance time.</p><p>Consider the <code>CloudProfile</code> below with a Shoot using the Kubernetes version <code>1.10.12</code>.
Even though the version is <code>expired</code>, due to missing <code>1.11.x</code> versions, the Gardener Controller Manager cannot upgrade the Shoot&rsquo;s Kubernetes version.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:
  kubernetes:
    versions:
    - version: 1.12.8
    - version: 1.12.7
    - version: 1.10.12
      expirationDate: <span style=color:#a31515>&#34;&lt;expiration date in the past&gt;&#34;</span>
</code></pre></div><p>The <code>CloudProfile</code> must specify versions <code>1.11.x</code> of the <strong>consecutive</strong> minor version.
Configuring the <code>CloudProfile</code> in such a way, the Shoot&rsquo;s Kubernetes version will be upgraded to version <code>1.11.10</code> in the next maintenance time.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:
  kubernetes:
    versions:
    - version: 1.12.8
    - version: 1.11.10
    - version: 1.11.09
    - version: 1.10.12
      expirationDate: <span style=color:#a31515>&#34;&lt;expiration date in the past&gt;&#34;</span>
</code></pre></div><h2 id=related-documentation>Related Documentation</h2><p>You might want to read about the <a href=/docs/gardener/usage/shoot_updates/>Shoot Updates and Upgrades</a> procedures to get to know the effects of such operations.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cd0360e42e0991fd12773a6ac9995dbe>32 - Shooted See</h1><h1 id=create-shooted-seed-cluster>Create Shooted Seed Cluster</h1><p>Create managed seed (aka &ldquo;shooted seed&rdquo;) cluster with the <code>shoot.gardener.cloud/use-as-seed</code> annotation.</p><p><strong>Note:</strong> Starting with Gardener v1.18, the <code>shoot.gardener.cloud/use-as-seed</code> annotation is deprecated.
It still works as described here, however behind the scenes a <code>ManagedSeed</code> resource is created and reconciled.
It is strongly recommended to use such resources directly to register shoots as seeds, as described in <a href=/docs/gardener/usage/managed_seed/>Register Shoot as Seed</a>. For instructions how to migrate existing seeds managed via the <code>use-as-seed</code> annotation, see <a href=/docs/gardener/usage/managed_seed/#migrating-from-the-use-as-seed-annotation-to-managedseeds>Migrating from the <code>use-as-seed</code> Annotation to <code>ManagedSeeds</code></a>.</p><h2 id=procedure>Procedure</h2><ol><li><p>Add the following project labels to the <code>garden</code> namespace if they don&rsquo;t exist yet:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>labels:
  gardener.cloud/role: project
  project.gardener.cloud/name: garden
</code></pre></div></li><li><p>The annotation works only for shoot clusters created in the <code>garden</code> namespace. Create a project for the <code>garden</code> namespace using <code>kubectl</code> if you don&rsquo;t have one yet.</p><blockquote><p>⚠️<br>Don&rsquo;t use the Gardener Dashboard as it would add a <code>garden</code> prefix for the namespace.</p></blockquote><p>Example: <a href=https://github.com/gardener/gardener/blob/master/example/05-project-dev.yaml>/example/05-project-dev.yaml</a></p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: Project
metadata:
  name: garden
spec:
  owner:
...
  namespace: garden
</code></pre></div></li><li><p>Create the shoot cluster.</p><p>Set the following annotation on the <code>Shoot</code> to mark it as a shooted seed cluster.</p><p>Example (full example: <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>/example/90-shoot.yaml</a>):</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>  annotations:
    shoot.gardener.cloud/use-as-seed: &gt;-<span style=color:#a31515>
</span><span style=color:#a31515>      </span>      <span style=color:#00f>true</span>,shootDefaults.pods=100.96.0.0/11,shootDefaults.services=100.64.0.0/13,disable-capacity-reservation,with-secret-ref
</code></pre></div><blockquote><ul><li>The networks from the seed cluster and its shoot clusters have to be different. To create shoot clusters with the dashboard you have to set a different worker CIDR in the shooted seed cluster (<code>spec.provider.infrastructureConfig</code> and <code>spec.networking.nodes</code>) and set the <code>shootDefaults</code> in the <code>shoot.gardener.cloud/use-as-seed</code> annotation to different CIDRs.</li><li>Optional: The shoot clusters to be created can use the same network as the garden cluster. To use the same network, set different CIDRs for pods and services in the shooted seed cluster (<code>spec.networking.pods</code> and <code>spec.networking.services</code>).</li></ul></blockquote></li></ol><h2 id=configuration-options-for-the-seed-cluster>Configuration Options for the Seed Cluster</h2><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>true</code></td><td>Registers the cluster as a seed cluster. Automatically deploys the gardenlet into the shoot cluster, unless specified otherwise (e.g. setting the <code>no-gardenlet</code> flag).</td></tr><tr><td><code>no-gardenlet</code></td><td>Prevents the automatic deployment of the gardenlet into the shoot cluster. Instead, the <code>Seed</code> object will be created with the assumption that another gardenlet will be responsible for managing it (according to its <code>seedConfig</code> configuration).</td></tr><tr><td><code>disable-capacity-reservation</code></td><td>Set <code>spec.settings.excessCapacity.enabled</code> in the seed cluster to false (see <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml>/example/50-seed.yaml</a>).</td></tr><tr><td><code>invisible</code></td><td>Set <code>spec.settings.scheduling.visible</code> in the seed cluster to false (see <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml>/example/50-seed.yaml</a>)</td></tr><tr><td><code>visible</code></td><td>Set <code>spec.settings.scheduling.visible</code> in the seed cluster to true (see <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml>/example/50-seed.yaml</a>) (<strong>default</strong>).</td></tr><tr><td><code>disable-dns</code></td><td>Set <code>spec.settings.shootDNS.enabled</code> in the seed cluster to false (see <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml>/example/50-seed.yaml</a>).</td></tr><tr><td><code>protected</code></td><td>Only shoot clusters in the <code>garden</code> namespace can use this seed cluster.</td></tr><tr><td><code>unprotected</code></td><td>Shoot clusters from all namespaces can use this seed cluster (<strong>default</strong>).</td></tr><tr><td><code>loadBalancerServices.annotations.*</code></td><td>Set <code>spec.settings.loadBalancerServices.annotations</code> in the seed cluster (see <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml>/example/50-seed.yaml</a>), e.g <code>loadBalancerServices.annotations.service.beta.kubernetes.io/aws-load-balancer-type=nlb</code>.</td></tr><tr><td><code>with-secret-ref</code></td><td>Creates a secret with the <code>kubeconfig</code> of the cluster in the <code>garden</code> namespace in the garden cluster and specifies the <code>.spec.secretRef</code> in the <code>Seed</code> object accordingly.</td></tr><tr><td><code>shootDefaults.pods</code></td><td>Default pod network CIDR for shoot clusters created on this seed cluster.</td></tr><tr><td><code>shootDefaults.services</code></td><td>Default service network CIDR for shoot clusters created on this seed cluster.</td></tr><tr><td><code>minimumVolumeSize</code></td><td>Set <code>spec.volume.minimumSize</code> in the seed cluster (see <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml>/example/50-seed.yaml</a>).</td></tr><tr><td><code>blockCIDRs</code></td><td>Set <code>spec.network.blockCIDRs</code> seperated by <code>;</code> (see <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml>/example/50-seed.yaml</a>).</td></tr><tr><td><code>backup.provider</code></td><td>Set <code>spec.backup.provider</code> in the seed cluster (see <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml>/example/50-seed.yaml</a>).</td></tr><tr><td><code>backup.region</code></td><td>Set <code>spec.backup.region</code> in the seed cluster (see <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml>/example/50-seed.yaml</a>).</td></tr><tr><td><code>backup.secretRef.name</code></td><td>Set <code>spec.backup.secretRef.name</code> in the seed cluster (see <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml>/example/50-seed.yaml</a>).</td></tr><tr><td><code>backup.secretRef.namespace</code></td><td>Set <code>spec.backup.secretRef.namespace</code> in the seed cluster (see <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml>/example/50-seed.yaml</a>).</td></tr><tr><td><code>apiServer.autoscaler.minReplicas</code></td><td>Controls the minimum number of <code>kube-apiserver</code> replicas for the shooted seed cluster.</td></tr><tr><td><code>apiServer.autoscaler.maxReplicas</code></td><td>Controls the maximum number of <code>kube-apiserver</code> replicas for the shooted seed cluster.</td></tr><tr><td><code>apiServer.replicas</code></td><td>Controls how many <code>kube-apiserver</code> replicas the shooted seed cluster gets by default.</td></tr><tr><td><code>use-serviceaccount-bootstrapping</code></td><td>States that the gardenlet registers with the garden cluster using a temporary <code>ServiceAccount</code> instead of a <code>CertificateSigningRequest</code> (<strong>default</strong>)</td></tr><tr><td><code>providerConfig.*</code></td><td>Sets <code>providerConfig</code> configuration parameters of the Seed resource. Each parameter is specified via its path, e.g. <code>providerConfig.param1=foo</code> or <code>providerConfig.sublevel1.sublevel2.param3=bar</code></td></tr><tr><td><code>featureGates.*={true,false}</code></td><td>Overwrites the <code>.featureGates</code> in the gardenlet configuration (only applicable when the <code>no-gardenlet</code> setting is <strong>not</strong> set), e.g. <code>featureGates.APIServerSNI=true</code></td></tr><tr><td><code>resources.capacity.*</code></td><td>Overwrites the <code>resources.capacity</code> field in the gardenlet configuration (only applicable when the <code>no-gardenlet</code> setting is <strong>not</strong> set), e.g. <code>resources.capacity.shoots=250</code></td></tr><tr><td><code>resources.reserved.*</code></td><td>Overwrites the <code>resources.reserved</code> field in the gardenlet configuration (only applicable when the <code>no-gardenlet</code> setting is <strong>not</strong> set), e.g. <code>resources.reserved.foo=42</code></td></tr><tr><td><code>ingress.controller.kind</code></td><td>Activates and specifies the kind of the managed ingress controller in the seed</td></tr><tr><td><code>ingress.controller.providerConfig.*</code></td><td>Sets provider specific configuration parameters for the managed ingress controller. Each parameter is specified via its path, e.g. <code>ingress.controller.providerConfig.param1=foo</code> or <code>ingress.controller.providerConfig.sublevel1.sublevel2.param3=bar</code></td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-ba6ea030a94e576907f8029c24109302>33 - Supported K8s Versions</h1><h1 id=supported-kubernetes-versions>Supported Kubernetes Versions</h1><p>Currently, the Gardener supports the following Kubernetes versions:</p><h2 id=garden-cluster-version>Garden cluster version</h2><p>⚠️ The minimum version of the garden cluster that can be used to run Gardener is <strong><code>1.17.x</code></strong>.</p><h2 id=seed-cluster-versions>Seed cluster versions</h2><p>⚠️ The minimum version of a seed cluster that can be connected to Gardener is <strong><code>1.18.x</code></strong>.
Kubernetes <code>1.18</code> sets the common ground for several Gardener features, e.g. <code>SeedKubeScheduler</code> (<a href=/docs/gardener/deployment/feature_gates/#list-of-feature-gates>ref</a>).
It also enables the Gardener code base to leverage more advanced Kubernetes features, like <a href=https://kubernetes.io/docs/reference/using-api/server-side-apply/>Server-Side Apply</a>, in the future.</p><h2 id=shoot-cluster-versions>Shoot cluster versions</h2><p>Gardener itself is capable of spinning up clusters with Kubernetes versions <strong><code>1.17</code></strong> up to <strong><code>1.23</code></strong>.
However, the concrete versions that can be used for shoot clusters depend on the installed provider extension.
Consequently, please consult the documentation of your provider extension to see which Kubernetes versions are supported for shoot clusters.</p><blockquote><p>👨🏼‍💻 Developers note: <a href=/docs/gardener/development/new-kubernetes-version/>This document</a> explains what needs to be done in order to add support for a new Kubernetes version.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-f92740830de9285764daf47f392d76d6>34 - Tolerations</h1><h1 id=taints-and-tolerations-for-seeds-and-shoots>Taints and Tolerations for <code>Seed</code>s and <code>Shoot</code>s</h1><p>Similar to <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/>taints and tolerations</a> for <code>Node</code>s and <code>Pod</code>s in Kubernetes, the <code>Seed</code> resource supports specifying taints (<code>.spec.taints</code>, see <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml#L48-L55>this example</a>) while the <code>Shoot</code> resource supports specifying tolerations (<code>.spec.tolerations</code>, see <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L268-L269>this example</a>).
The feature is used to control scheduling to seeds as well as decisions whether a shoot can use a certain seed.</p><p>Compared to Kubernetes, Gardener&rsquo;s taints and tolerations are very much down-stripped right now and have some behavioral differences.
Please read the following explanations carefully if you plan to use it.</p><h2 id=scheduling>Scheduling</h2><p>When scheduling a new shoot then the gardener-scheduler will filter all seed candidates whose taints are not tolerated by the shoot.
As Gardener&rsquo;s taints/tolerations don&rsquo;t support <code>effect</code>s yet you can compare this behaviour with using a <code>NoSchedule</code> effect taint in Kubernetes.</p><p>Be reminded that taints/tolerations are no means to define any affinity or selection for seeds - please use <code>.spec.seedSelector</code> in the <code>Shoot</code> to state such desires.</p><p>⚠️ Please note that - unlike how it&rsquo;s implemented in Kubernetes - a certain seed cluster <strong>may</strong> only be used when the shoot tolerates <strong>all</strong> the seed&rsquo;s taints.
This means that specifying <code>.spec.seedName</code> for a seed whose taints are not tolerated will make the gardener-apiserver rejecting the request.</p><p>Consequently, the taints/tolerations feature can be used as means to restrict usage of certain seeds.</p><h2 id=toleration-defaults-and-whitelist>Toleration Defaults and Whitelist</h2><p>The <code>Project</code> resource features a <code>.spec.tolerations</code> object that may carry <code>defaults</code> and a <code>whitelist</code> (see <a href=https://github.com/gardener/gardener/blob/master/example/05-project-dev.yaml#L33-L37>this example</a>).
The corresponding <code>ShootTolerationRestriction</code> admission plugin (cf. Kubernetes' <code>PodTolerationRestriction</code> admission plugin) is responsible for evaluating these settings during creation/update of <code>Shoot</code>s.</p><h3 id=whitelist>Whitelist</h3><p>If a shoot gets created or updated with tolerations then it is validated that only those tolerations may be used which were added to either a) the <code>Project</code>&rsquo;s <code>.spec.tolerations.whitelist</code>, or b) to the global whitelist in the <code>ShootTolerationRestriction</code>&rsquo;s admission config (see <a href=https://github.com/gardener/gardener/blob/master/example/20-admissionconfig.yaml#L7-L14>this example</a>).</p><p>⚠️ Please note that the tolerations whitelist of <code>Project</code>s can only be changed if the user trying to change it is bound to the <code>modify-spec-tolerations-whitelist</code> custom RBAC role, e.g. via the following <code>ClusterRole</code>:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: full-project-modification-access
rules:
- apiGroups:
  - core.gardener.cloud
  resources:
  - projects
  verbs:
  - create
  - patch
  - update
  - modify-spec-tolerations-whitelist
  - delete
</code></pre></div><h3 id=defaults>Defaults</h3><p>If a shoot gets created then the default tolerations specified in both the <code>Project</code>&rsquo;s <code>.spec.tolerations.defaults</code> and global default list in the <code>ShootTolerationRestriction</code> admission plugin&rsquo;s configuration will be added to the <code>.spec.tolerations</code> of the <code>Shoot</code> (unless it already specifies a certain key).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-717af96646de8dc0584626ea13c573c8>35 - Trouble Shooting Guide</h1><h1 id=trouble-shooting-guide>Trouble Shooting Guide</h1><h2 id=are-there-really-issue-that-cannot-be-fixed-o>Are there really issue that cannot be fixed :O?</h2><p>Well, of course not :P. With continuous development of Gardener, over the time its architecture and API might have to be changed to reduce complexity and support more features. In this process developers are bound to keep Gardener version backward compatible with last two releases. But maintaining backward compatibility is quite complex and effortful tasks. So, to save short term complex effort, its common practice in open source community to use work around or hacky solutions sometimes. This results in rare issues which are supposed to be resolved by human interaction across upgrades of Gardener version.</p><p>This guide records the issues that are quite possible across upgrade of Gardener version, root cause and the human action required for graceful resolution of issue. For troubleshooting guide of bugs which are not yet fixed, please refer the associated github issue.</p><p><strong>Note To Maintainers:</strong> Please use only mention the resolution of issues which are by design. For bugs please report the temporary resolution on github issue create for the bug.</p><h3 id=etcd-main-pod-fails-to-come-up-since-backup-restore-sidecar-is-reporting-revisionconsistencycheckerr>Etcd-Main pod fails to come up, since backup-restore sidecar is reporting RevisionConsistencyCheckErr</h3><h4 id=issue>Issue</h4><ul><li>Etcd-main pod goes in <code>CrashLoopBackoff</code>.</li><li>Etcd-backup-restore sidecar reports validation error with RevisionConsistencyCheckErr.</li></ul><h4 id=environment>Environment</h4><ul><li>Gardener version: 0.29.0+</li></ul><h4 id=root-cause>Root Cause</h4><ul><li>From version 0.29.0, Gardener uses shared backup bucket for storing etcd backups, replacing old logic of having single bucket per shoot as per <a href=/docs/gardener/proposals/02-backupinfra/>proposal</a>.</li><li>Since there are very rare chances that etcd data directory will get corrupt, while doing this migration, to avoid etcd down time and implementation effort, we decided to switch directly from old bucket to new shared bucket without migrating old snapshot from old bucket to new bucket.</li><li>In this case just for safety side we added sanity check in etcd-backup-restore sidecar of etcd-main pod, which checks if etcd data revision is greater than the last snapshot revision from old bucket.</li><li>If above check fails mean there is surely some data corruption occurred with etcd, so etcd-backup-restore reports error and then etcd-main pod goes in <code>CrashLoopBackoff</code> creating etcd-main down alerts.</li></ul><h4 id=action>Action</h4><ol><li>Disable the Gardener reconciliation for Shoot by annotating it with <code>shoot.gardener.cloud/ignore=true</code></li><li>Scale down the etcd-main statefulset in seed cluster.</li><li>Find out the latest full snapshot and delta snapshot from old backup bucket. The old backup bucket name is same as the backupInfra resource associated with Shoot in Garden cluster.</li><li>Move them manually to new backup bucket.</li><li>Enable the Gardener reconciliation for shoot by removing annotation <code>shoot.gardener.cloud/ignore=true</code>.</li></ol><h3 id=after-upgradingrestarting-a-local-gardener-setup-the-dnsentries-on-the-seeds-show-the-error--already-busy-for-owner->After upgrading/restarting a local Gardener setup, the DNSEntries on the seeds show the error &ldquo;&mldr; already busy for owner &mldr;&rdquo;</h3><h4 id=issue-1>Issue</h4><ul><li>custom resources DNSEntries on the seeds show the error &ldquo;dns name &ldquo;api.myshoot.mygarden.internal.dev.k8s.ondemand.com&rdquo; already busy for owner &ldquo;seed.gardener.cloud/a1234567-XXXX-XXXX-XXXX-025000000001/aws&rdquo;</li><li>API server is not available via DNS name</li></ul><h4 id=environment-1>Environment</h4><ul><li>Gardener version: 0.20.0+</li></ul><h4 id=root-cause-1>Root Cause</h4><p>DNS records created by Gardener&rsquo;s dns-controller-manager are stored together with meta data, especially
with an owner identifier. In this way the dns-controller-manager knows which records belong to it.
It never changes records which are not owned by it.
The owner identifier is unique for every seed and computed from the Gardener identity and the seed identity.
The Gardener identity is the UUID of the garden namespace of the Gardener cluster.
Especially if you have a local Gardener setup, there are situations where the Kubernetes cluster and therefore the garden namespace have to be recreated.
For example, on updating docker-desktop all containers may have been deleted and are recreated.</p><h4 id=action-1>Action</h4><p>On each seed, you have to tell the dns-controller-manager, that it is also responsible for secondary owner
identifiers. For this purpose create a custom resource <code>DNSOwner</code> and set the attribute <code>ownerId</code> to the old
owner identifier shown in the error message of the DNS entries, e.g.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: dns.gardener.cloud/v1alpha1
kind: DNSOwner
metadata:
  name: old-owner
  namespace: default
spec:
  ownerId: seed.gardener.cloud/a1234567-XXXX-XXXX-XXXX-025000000001/aws
  active: <span style=color:#00f>true</span>
</code></pre></div><p>Currently the dns-controller-manager has to be restarted (i.e. delete its current pod) to make it known of
the secondary owner identifier.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cf607bc71a1ae64925c002dc4829df51>36 - Trusted Tls For Control Planes</h1><h1 id=trusted-tls-certificate-for-shoot-control-planes>Trusted TLS certificate for shoot control planes</h1><p>Shoot clusters are composed of several control plane components deployed by the Gardener and corresponding extensions.</p><p>Some components are exposed via <code>Ingress</code> resources which make them addressable under the HTTPS protocol.</p><p>Examples:</p><ul><li>Alertmanager</li><li>Grafana for operators and end-users</li><li>Prometheus</li></ul><p>Gardener generates the backing TLS certificates which are signed by the shoot cluster&rsquo;s CA by default (self-signed).</p><p>Unlike with a self-contained Kubeconfig file, common internet browsers or operating systems don&rsquo;t trust a shoot&rsquo;s cluster CA and adding it as a trusted root is often undesired in enterprise environments.</p><p>Therefore, Gardener operators can predefine trusted wildcard certificates under which the mentioned endpoints will be served instead.</p><h2 id=register-a-trusted-wildcard-certificate>Register a trusted wildcard certificate</h2><p>Since control plane components are published under the ingress domain (<code>core.gardener.cloud/v1beta1.Seed.spec.dns.ingressDomain</code>) a wildcard certificate is required.</p><p>For example:</p><ul><li>Seed ingress domain: <code>dev.my-seed.example.com</code></li><li><code>CN</code> or <code>SAN</code> for certificate: <code>*.dev.my-seed.example.com</code></li></ul><p>A wildcard certificate matches exactly one seed. It must be deployed as part of your landscape setup as a Kubernetes <code>Secret</code> inside the <code>garden</code> namespace of the corresponding seed cluster.</p><p>Please ensure that the secret has the <code>gardener.cloud/role</code> label shown below.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
data:
  ca.crt: base64-encoded-ca.crt
  tls.crt: base64-encoded-tls.crt
  tls.key: base64-encoded-tls.key
kind: Secret
metadata:
  labels:
    gardener.cloud/role: controlplane-cert
  name: seed-ingress-certificate
  namespace: garden
type: Opaque
</code></pre></div><p>Gardener copies the secret during the reconciliation of shoot clusters to the shoot namespace in the seed. Afterwards, <code>Ingress</code> resources in that namespace for the mentioned components will refer to the wildcard certificate.</p><h2 id=best-practice>Best practice</h2><p>While it is possible to create the wildcard certificates manually and deploy them to seed clusters, it is recommended to let certificate management components do this job. Often, a seed cluster is also a shoot cluster at the same time (shooted seed) and might already provide a certificate service extension.
Otherwise, a Gardener operator may use solutions like <a href=https://github.com/gardener/cert-management>Cert-Management</a> or <a href=https://github.com/jetstack/cert-manager>Cert-Manager</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2a827ed3506304fa29f2a71110704fb0>37 - Worker Pool K8s Versions</h1><h1 id=controlling-the-kubernetes-versions-for-specific-worker-pools>Controlling the Kubernetes versions for specific worker pools</h1><p>Since Gardener <code>v1.36</code>, worker pools can have different Kubernetes versions specified than the control plane.</p><p>It must be enabled by setting the featureGate <code>WorkerPoolKubernetesVersion: true</code> in the gardenlet&rsquo;s component configuration.</p><p>In earlier Gardener versions all worker pools inherited the Kubernetes version of the control plane. Once the Kubernetes version of the control plane was modified, all worker pools have been updated as well (either by rolling the nodes in case of a minor version change, or in-place for patch version changes).</p><p>In order to gracefully perform Kubernetes upgrades (triggering a rolling update of the nodes) with workloads sensitive to restarts (e.g., those dealing with lots of data), it might be required to be able to gradually perform the upgrade process.
In such cases, the Kubernetes version for the worker pools can be pinned (<code>.spec.provider.workers[].kubernetes.version</code>) while the control plane Kubernetes version (<code>.spec.kubernetes.version</code>) is updated.
This results in the nodes being untouched while the control plane is upgraded.
Now a new worker pool (with the version equal to the control plane version) can be added.
Administrators can then reschedule their workloads to the new worekr pool according to their upgrade requirements and processes.</p><h2 id=example-usage-in-a-shoot>Example Usage in a <code>Shoot</code></h2><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:
  kubernetes:
    version: 1.20.1
  provider:
    workers:
    - name: data1
      kubernetes:
        version: 1.19.1
    - name: data2
</code></pre></div><ul><li>If <code>.kubernetes.version</code> is not specified in a worker pool then the Kubernetes version of the kubelet is inherited from the control plane (<code>.spec.kubernetes.version</code>), i.e., in above example the <code>data2</code> pool will use <code>1.20.1</code>.</li><li>If <code>.kubernetes.version</code> is specified in a worker pool then it must meet the following constraints:<ul><li>It must be at most two minor versions lower than the control plane version.</li><li>If it was not specified before then no downgrade is possible (you cannot set it to <code>1.19.1</code> while <code>.spec.kubernetes.version</code> is already <code>1.20.1</code>). The &ldquo;two minor version skew&rdquo; is only possible if the worker pool version is set to control plane version and then the control plane was updated gradually two minor versions.</li><li>If the version is removed from the worker pool, only one minor version difference is allowed to the control plane (you cannot upgrade a pool from version <code>1.18.0</code> to <code>1.20.0</code> in one go).</li></ul></li></ul><p>Automatic updates of Kubernetes versions (see <a href=/docs/gardener/usage/shoot_maintenance/#automatic-version-updates>this document</a>) also apply to worker pool Kubernetes versions.</p></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2022 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js integrity=sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/mermaid@8.9.2/dist/mermaid.min.js integrity=sha384-uQikAXnCAqsMb3ygtdqBYvcwvHUkzGIpjdGyy9owhURXHUxLC5LgTcSxJQH/RzjK crossorigin=anonymous></script><script src=/js/main.min.ef8e0714aff556fd5a9768ed6ecabd2964dd962cd9f89762a373947bb53bc742.js integrity="sha256-744HFK/1Vv1al2jtbsq9KWTdlizZ+Jdio3OUe7U7x0I=" crossorigin=anonymous></script></body></html>