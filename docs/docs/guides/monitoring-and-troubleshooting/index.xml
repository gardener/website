<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Monitor and Troubleshoot on Gardener</title><link>https://gardener.cloud/docs/guides/monitoring-and-troubleshooting/</link><description>Recent content in Monitor and Troubleshoot on Gardener</description><generator>Hugo</generator><language>en-US</language><atom:link href="https://gardener.cloud/docs/guides/monitoring-and-troubleshooting/index.xml" rel="self" type="application/rss+xml"/><item><title>Analyzing Node Removal and Failures</title><link>https://gardener.cloud/docs/guides/monitoring-and-troubleshooting/analysing-node-failures/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/monitoring-and-troubleshooting/analysing-node-failures/</guid><description>&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Sometimes operators want to find out why a certain node got removed. This guide helps to identify possible causes.
There are a few potential reasons why nodes can be removed:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/guides/monitoring-and-troubleshooting/analysing-node-failures/#find-out-whether-the-node-was-unhealthy">broken node&lt;/a>: a node becomes unhealthy and machine-controller-manager terminates it in an attempt to replace the unhealthy node with a new one&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/guides/monitoring-and-troubleshooting/analysing-node-failures/#scale-down">scale-down&lt;/a>: cluster-autoscaler sees that a node is under-utilized and therefore scales down a worker pool&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/guides/monitoring-and-troubleshooting/analysing-node-failures/#node-rolling">node rolling&lt;/a>: configuration changes to a worker pool (or cluster) require all nodes of one or all worker pools to be rolled and thus all nodes to be replaced. Some possible changes are:
&lt;ul>
&lt;li>the K8s/OS version&lt;/li>
&lt;li>changing machine types&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Helpful information can be obtained by using the logging stack. See &lt;a href="https://github.com/gardener/gardener/blob/master/docs/usage/logging.md">Logging Stack&lt;/a> for how to utilize the logging information in Gardener.&lt;/p></description></item><item><title>Get a Shell to a Gardener Shoot Worker Node</title><link>https://gardener.cloud/docs/guides/monitoring-and-troubleshooting/shell-to-node/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/monitoring-and-troubleshooting/shell-to-node/</guid><description>&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>To troubleshoot certain problems in a Kubernetes cluster, operators need access to the host of the Kubernetes node. This can be required if a node misbehaves or fails to join the cluster in the first place.&lt;/p>
&lt;p>With access to the host, it is for instance possible to check the &lt;code>kubelet&lt;/code> logs and interact with common tools such as &lt;code>systemctl&lt;/code> and &lt;code>journalctl&lt;/code>.&lt;/p>
&lt;p>The first section of this guide explores options to get a shell to the node of a Gardener Kubernetes cluster.
The options described in the second section do not rely on Kubernetes capabilities to get shell access to a node and thus can also be used if an instance failed to join the cluster.&lt;/p></description></item><item><title>How to Debug a Pod</title><link>https://gardener.cloud/docs/guides/monitoring-and-troubleshooting/debug-a-pod/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/monitoring-and-troubleshooting/debug-a-pod/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Kubernetes offers powerful options to get more details about startup or runtime failures of pods as e.g. described in
&lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/">Application Introspection and Debugging&lt;/a> or &lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/">Debug Pods and Replication Controllers&lt;/a>.&lt;/p>
&lt;p>In order to identify pods with potential issues, you could, e.g., run &lt;code>kubectl get pods --all-namespaces | grep -iv Running &lt;/code> to filter out the pods which are not in the state &lt;code>Running&lt;/code>. One of frequent error state is &lt;code>CrashLoopBackOff&lt;/code>, which tells that a pod crashes right after the start. Kubernetes then tries to restart the pod again, but often the pod startup fails again.&lt;/p></description></item><item><title>tail -f /var/log/my-application.log</title><link>https://gardener.cloud/docs/guides/monitoring-and-troubleshooting/tail-logfile/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/monitoring-and-troubleshooting/tail-logfile/</guid><description>&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>One thing that always bothered me was that I couldn&amp;rsquo;t get logs of several pods at once with &lt;code>kubectl&lt;/code>. A simple &lt;code>tail -f &amp;lt;path-to-logfile&amp;gt;&lt;/code> isn&amp;rsquo;t possible at all. Certainly, you can use &lt;code>kubectl logs -f &amp;lt;pod-id&amp;gt;&lt;/code>, but it doesn&amp;rsquo;t help if you want to monitor more than one pod at a time.&lt;/p>
&lt;p>This is something you really need a lot, at least if you run several instances of a pod behind a &lt;code>deployment&lt;/code>. This is even more so if you don&amp;rsquo;t have a Kibana or a similar setup.&lt;/p></description></item></channel></rss>