<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.95.0"><link rel=canonical type=text/html href=https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Provider vSphere | Gardener</title><meta name=description content="Gardener extension controller for the vSphere cloud provider"><meta property="og:title" content="Provider vSphere"><meta property="og:description" content="Gardener extension controller for the vSphere cloud provider"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="Provider vSphere"><meta itemprop=description content="Gardener extension controller for the vSphere cloud provider"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="Provider vSphere"><meta name=twitter:description content="Gardener extension controller for the vSphere cloud provider"><link rel=preload href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css as=style><link href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7N3XF5XLGV"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7N3XF5XLGV",{anonymize_ip:!1})}</script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.c28358896a3182d696b00ecdf0e2d5b9.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-vsphere/>Return to the regular view of this page</a>.</p></div><h1 class=title>Provider vSphere</h1><div class=lead>Gardener extension controller for the vSphere cloud provider</div><div class=content><h1 id=gardener-extension-for-vsphere-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for vSphere provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-vsphere-main/jobs/main-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-vsphere-main/jobs/main-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-vsphere><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-vsphere alt="Go Report Card"></a></p><h2 id=overview>Overview</h2><p>The Gardener Extension for vSphere is a <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> provider implementation that allows Gardener to leverage vSphere clusters for machine provisioning.</p><p>vSphere is an undeniable class leader for commercially supported virtual machine orchestration. The Gardener extension for vSphere provider compliments this leadership by allowing Gardener to create Kubernetes nodes within vSphere.</p><p>Like other Gardener provider extensions, the vSphere provider pairs with a provider-specific Machine Controller Manager providing node services to Kubernetes clusters. This extension provides complimentary APIs to Gardener. A Gardener-hosted Kubernetes
cluster does not know anything about it&rsquo;s environment (such as bare metal vs. public cloud or within a hyperscaler vs. standalone), only that the MCM abstraction can manage requests such as cluster autoscaling.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-vsphere/blob/main/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and the architecture details in the GEP-1 proposal.</p><h2 id=use-cases>Use Cases</h2><p>The primary use case for this extension is organizations who wish to deploy a substantial Gardener landscape and use vSphere for data center fleet management. We intentionally sidestep prescribing any particular extension as this is
an intimately local determination and the benefits of different solutions are more than adequately debated in industry literature.</p><p>While we may inadvertently duplicate some documentation in the mainline Gardener documentation, it is only to reduce tedium as new evaluators and developers come up-to-speed with the concepts relevant to successful deployment.
We refer directly to the mainline Gardener documentation for the most up-to-date information.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.26</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.25</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.24</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.23</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.22</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.21</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.20</td><td>untested</td><td>not yet available</td></tr></tbody></table><p>Older versions of the extension <a href=https://github.com/gardener/gardener-extension-provider-vsphere/releases/tag/v0.16.0>(<code>v0.16.0</code> and earlier)</a> are supported prior to current releases.</p><p>Please take a look <a href=/docs/gardener/usage/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=deployment-patterns>Deployment patterns</h2><p>As with any production software, deployment of Gardener and this extension should be considered in the context of both lifecycle and automation. Orgs should aspire to have apply</p><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-vsphere/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md>GEP-4 (New <code>core.gardener.cloud/v1beta1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div></div><div class=td-content style=page-break-before:always><h1 id=pg-35a78d43d0bac7f0de61739ce0338d7f>1 - Deployment</h1><h1 id=deployment-of-the-vsphere-provider-extension>Deployment of the vSphere provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the vSphere provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the vSphere provider extension <a href=https://github.com/gardener/gardener-extension-provider-vsphere>repository</a>.</p><h2 id=gardener-extension-validator-vsphere>gardener-extension-validator-vsphere</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-validator-vsphere</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-e2908e6901d240206d5754a946ddf138>2 - Local Setup</h1><h1 id=deployment>Deployment</h1><h3 id=admission-vsphere>admission-vsphere</h3><p><code>admission-vsphere</code> is an admission webhook server which is responsible for the validation of the cloud provider (vSphere in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/development/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-admission
</span></span></code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-vsphere.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/dev-setup-admission-vsphere.sh
</span></span></code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-vsphere</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-636e168a1b7671fa4ceef9e39ea82a55>3 - Prepare Vsphere</h1><h1 id=vsphere--nsx-t-preparation-for-gardener-extension-vsphere-provider>vSphere / NSX-T Preparation for Gardener Extension &ldquo;vSphere Provider&rdquo;</h1><ul><li><a href=#vsphere--nsx-t-preparation-for-gardener-extension-vsphere-provider>vSphere / NSX-T Preparation for Gardener Extension &ldquo;vSphere Provider&rdquo;</a><ul><li><a href=#vsphere-preparation>vSphere Preparation</a><ul><li><a href=#user-and-role-creation>User and Role Creation</a><ul><li><a href=#vcentervsphere>vCenter/vSphere</a></li><li><a href=#nsx-t>NSX-T</a></li></ul></li><li><a href=#create-folders>Create Folders</a></li><li><a href=#upload-vm-templates-for-worker-nodes>Upload VM Templates for Worker Nodes</a></li><li><a href=#prepare-for-kubernetes-zones-and-regions>Prepare for Kubernetes Zones and Regions</a><ul><li><a href=#create-resource-pools>Create Resource Pool(s)</a></li><li><a href=#tag-regions-and-zones>Tag Regions and Zones</a></li><li><a href=#storage-policies>Storage policies</a><ul><li><a href=#tag-zone-storages>Tag Zone Storages</a></li><li><a href=#create-or-clone-vm-storage-policy>Create or clone VM Storage Policy</a></li></ul></li></ul></li></ul></li><li><a href=#nsx-t-preparation>NSX-T Preparation</a><ul><li><a href=#create-ip-pools>Create IP pools</a><ul><li><a href=#sizing-the-ip-pools>Sizing the IP pools</a></li></ul></li><li><a href=#check-edge-cluster-sizing>Check edge cluster sizing</a></li></ul></li><li><a href=#get-vds-uuids>Get VDS UUIDs</a></li></ul></li></ul><p>Several preparational steps are necessary for VMware vSphere and NSX-T, before this extension can be used
to create Gardener shoot clusters.</p><p>The main version target of this extension is vSphere 7.x together with NSX-T 3.x.
The recommended environment is a system setup with VMware Cloud Foundation (VCF) 4.1.
Older versions like vSphere 6.7U3 with NSX-T 2.5 or 3.0 should still work, but are not tested extensively.</p><h2 id=vsphere-preparation>vSphere Preparation</h2><h3 id=user-and-role-creation>User and Role Creation</h3><p>This extension needs credentials for both the vSphere/vCenter and the NSX-T endpoints.
This section guides through the creation of appropriate roles and users.</p><h4 id=vcentervsphere>vCenter/vSphere</h4><p>The vCenter/vSphere user used for this provider should have been assigned to a role including these permissions
(use vCenter/vSphere Client / Menu Administration / Access Control / Role to define a role and assign it to the user
with <code>Global Permissions</code>)</p><ul><li>Datastore<ul><li>Allocate space</li><li>Browse datastore</li><li>Low level file operations</li><li>Remove file</li><li>Update virtual machine files</li><li>Update virtual machine metadata</li></ul></li><li>Global<ul><li>Cancel task</li><li>Manage custom attributes</li><li>Set custom attribute</li></ul></li><li>Network<ul><li>Assign network</li></ul></li><li>Resource<ul><li>Assign virtual machine to resource pool</li></ul></li><li>Tasks<ul><li>Create task</li><li>Update task</li></ul></li><li>vApp<ul><li>Add virtual machine</li><li>Assign resource pool</li><li>Assign vApp</li><li>Clone</li><li>Power off</li><li>Power on</li><li>View OVF environment</li><li>vApp application configuration</li><li>vApp instance configuration</li><li>vApp managedBy configuration</li><li>vApp resource configuration</li></ul></li><li>Virtual machine<ul><li>Change Configuration<ul><li>Acquire disk lease</li><li>Add existing disk</li><li>Add new disk</li><li>Add or remove device</li><li>Advanced configuration</li><li>Change CPU count</li><li>Change Memory</li><li>Change Settings</li><li>Change Swapfile placement</li><li>Change resource</li><li>Configure Host USB device</li><li>Configure Raw device</li><li>Configure managedBy</li><li>Display connection settings</li><li>Extend virtual disk</li><li>Modify device settings</li><li>Query Fault Tolerance compatibility</li><li>Query unowned files</li><li>Reload from path</li><li>Remove disk</li><li>Rename</li><li>Reset guest information</li><li>Set annotation</li><li>Toggle disk change tracking</li><li>Toggle fork parent</li><li>Upgrade virtual machine compatibility</li></ul></li><li>Edit Inventory<ul><li>Create from existing</li><li>Create new</li><li>Move</li><li>Register</li><li>Remove</li><li>Unregister</li></ul></li><li>Guest operations<ul><li>Guest operation alias modification</li><li>Guest operation alias query</li><li>Guest operation modifications</li><li>Guest operation program execution</li><li>Guest operation queries</li></ul></li><li>Interaction<ul><li>Power off</li><li>Power on</li><li>Reset</li></ul></li><li>Provisioning<ul><li>Allow disk access</li><li>Allow file access</li><li>Allow read-only disk access</li><li>Allow virtual machine files upload</li><li>Clone template</li><li>Clone virtual machine</li><li>Customize guest</li><li>Deploy template</li><li>Mark as virtual machine</li><li>Modify customization specification</li><li>Promote disks</li><li>Read customization specifications</li></ul></li></ul></li></ul><h4 id=nsx-t>NSX-T</h4><p>The NSX-T API is accessed from the infrastructure controller of the vsphere-provider for setting up the network infrastructure resources and the cloud-controller-manager for managing load balancers. Currently, the NSX-T user must have the <code>Enterprise Admin</code> role.</p><h3 id=create-folders>Create Folders</h3><p>Two folders need to be created:
- a folder which will contain the VMs of the shoots (cloud profile <code>spec.providerConfig.folder</code>)
- a folder containing templates (used by cloud profile <code>spec.providerConfig.machineImages[*].versions[*].path</code>)</p><p>In vSphere client:</p><ol><li>From the <em>Menu</em> in the vSphere Client toolbar choose <em>VMs and Templates</em></li><li>Select the vSphere Datacenter of the work load vCenter in the browser</li><li>From the context menu select <em>New Folder</em> > <em>New VM and Template Folder</em>, set folder name to e.g. &ldquo;gardener&rdquo;</li><li>From the context menu of the new folder <em>gardener</em> select <em>New Folder</em>, set folder name to &ldquo;templates&rdquo;</li></ol><h3 id=upload-vm-templates-for-worker-nodes>Upload VM Templates for Worker Nodes</h3><p>Upload <a href=https://github.com/gardenlinux/gardenlinux/releases>gardenlinux OVA</a> or
<a href=https://flatcar-linux.org/releases#stable-release>flatcar OVA</a> templates.</p><ol><li>From the context menu of the folder <code>gardener/templates</code> choose <em>Deploy OVF Template&mldr;</em></li><li>Adjust name if needed</li><li>Select any compute cluster as compute resource</li><li>Select a storage (e.g. VSAN)</li><li>Select any network (not important)</li><li>No need to customize the template</li><li>After deployment is finished select from the context menu of the new deployed VM <em>Template</em> > <em>Convert To Template</em></li></ol><h3 id=prepare-for-kubernetes-zones-and-regions>Prepare for Kubernetes Zones and Regions</h3><p>This step has to be done regardless of whether you actually have more than a single region and zone or not!
Two labels need to be defined in the cloud profile (section <code>spec.providerConfig.failureDomainLabels</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>    failureDomainLabels:
</span></span><span style=display:flex><span>      region: k8s-region
</span></span><span style=display:flex><span>      zone: k8s-zone
</span></span></code></pre></div><p>A Kubernetes zone can either be a vCenter or one of its datacenters</p><p>Zones must be sub-resources of it. If the region is a complete vCenter, the zone must specify datacenter and either compute cluster or resource pool.
Otherwise, i.e. tf the region is a datacenter, the zone must specify either compute cluster or resource pool.</p><p>In the following steps it is assumed:
- the region is specified by a datacenter
- the zone is specified by a compute cluster or one of its resource pools</p><h4 id=create-resource-pools>Create Resource Pool(s)</h4><p>Create a resource pool for every zone:</p><ol><li>From the <em>Menu</em> in the vSphere Client toolbar choose <em>Hosts and Clusters</em></li><li>From the context menu of the compute cluster select <em>New Resource Pool&mldr;</em> and provide the name of the zone. CPU and Memory settings are optional.</li></ol><h4 id=tag-regions-and-zones>Tag Regions and Zones</h4><p>Each zone must be tagged with the category defined by the label defined in the cloud profile (<code>spec.providerConfig.failureDomainLabels.region</code>).
Assuming that the region is a datacenter and the region label is <code>k8s-region</code>:</p><ol><li>From the <em>Menu</em> in the vSphere Client toolbar choose <em>Hosts and Clusters</em></li><li>Select the region&rsquo;s datacenter in the browser</li><li>In the <em>Summary</em> tab there is a sub-window titled <em>Tags</em>. Click the <em>Assign&mldr;</em> link.</li><li>In the <em>Assign Tag</em> dialog select the <em>ADD TAG</em> link above of the table</li><li>In the <em>Create Tag</em> dialog choose the <em>k8s-region</em> category. If it is not defined, click the <em>Create New Category</em> link to create the category.</li><li>Enter the <em>Name</em> of the region.</li><li>Back in the <em>Assign Tag</em> mark the checkbox of the region tag you just have created.</li><li>Click the <em>ASSIGN</em> button</li></ol><p>Assuming that the zones are specified by resource pools and the zone label is <code>k8s-zone</code>:</p><ol><li>From the <em>Menu</em> in the vSphere Client toolbar choose <em>Hosts and Clusters</em></li><li>Select the zone&rsquo;s Compute Cluster in the browser</li><li>In the <em>Summary</em> tab there is a sub-window titled <em>Tags</em>. Click the <em>Assign&mldr;</em> link.</li><li>In the <em>Assign Tag</em> dialog select the <em>ADD TAG</em> link above of the table</li><li>In the <em>Create Tag</em> dialog choose the <em>k8s-zone</em> category. If it is not defined, click the <em>Create New Category</em> link to create the category.</li><li>Enter the <em>Name</em> of the zone.</li><li>Back in the <em>Assign Tag</em> mark the checkbox of the zone tag you just have created.</li><li>Click the <em>ASSIGN</em> button</li></ol><h4 id=storage-policies>Storage policies</h4><p>Each zone can have a separate storage. In this case a storage policy is needed to be compatible with all the zone storages.</p><h5 id=tag-zone-storages>Tag Zone Storages</h5><p>For each zone tag the storage with the corresponding <code>k8s-zone</code> tag for the zone.</p><ol><li>From the <em>Menu</em> in the vSphere Client toolbar choose <em>Storage</em></li><li>Select the zone&rsquo;s storage in the browser</li><li>In the <em>Summary</em> tab there is a sub-window titled <em>Tags</em>. Click the <em>Assign&mldr;</em> link.</li><li>In the <em>Assign Tag</em> dialog select the <em>ADD TAG</em> link above of the table</li><li>In the <em>Create Tag</em> dialog choose the <em>k8s-zone</em> category. If it is not defined, click the <em>Create New Category</em> link to create the category.</li><li>Enter the <em>Name</em> of the zone.</li><li>Back in the <em>Assign Tag</em> mark the checkbox of the zone tag you just have created.</li><li>Click the <em>ASSIGN</em> button</li></ol><h5 id=create-or-clone-vm-storage-policy>Create or clone VM Storage Policy</h5><ol><li><p>From the <em>Menu</em> in the vSphere Client toolbar choose <em>Policies and Profiles</em></p></li><li><p>In the <em>Policies and Profiles</em> list select <em>VM Storage Policies</em></p></li><li><p>Create or clone an existing storage policy</p><p>a) set name, e.g. &ldquo;<region-name> Storage Policy&rdquo; (will be needed for the cloud profile later in <code>spec.providerConfig.defaultClassStoragePolicyName</code>)</p><p>b) On the page <em>Policy structure</em> check only the checkbox <em>Enable tag based placement rules</em></p><p>c) On the page <em>Tage based placement</em> press the <em>ADD TAG RULE</em> button.</p><p>d) For <em>Rule 1</em> select</p><pre tabindex=0><code>*Tag category* =  *k8s-zone*
*Usage option* = *Use storage tagged with*
*Tags* = *all zone tags*.
</code></pre><p>e) Validate the compatible storages on the page <em>Storage compatibility</em></p><p>f) Press <em>FINISH</em> on the <em>Review and finish</em> page</p></li><li><p><strong>IMPORTANT</strong>: Repeat steps 1-3 and create a second StoragePolicy by the name of <code>garden-etcd-fast-main</code>. This will be used by Gardener to provision shoot&rsquo;s etcd PVCs.</p></li></ol><h2 id=nsx-t-preparation>NSX-T Preparation</h2><p>A shared NSX-T is needed for all zones of a region.
External IP address ranges are needed for SNAT and load balancers.
Besides the edge cluster must be sized large enough to deal with the load balancers of all shoots.</p><h3 id=create-ip-pools>Create IP pools</h3><p>Two IP pools are needed for external IP addresses.</p><ol><li>IP pool for <strong>SNAT</strong>
The IP pool name needs to be specified in the cloud profile at <code>spec.providerConfig.regions[*].snatIPPool</code>. Each shoot cluster needs one SNAT IP address for outgoing traffic.</li><li>IP pool(s) for the <strong>load balancers</strong>
The IP pool name(s) need to be specified in the cloud profile at <code>spec.providerConfig.contraints.loadBalancerConfig.classes[*].ipPoolName</code>. An IP address is needed for every port of every Kubernetes service of type <code>LoadBalancer</code>.</li></ol><p>To create them, follow these steps in the NSX-T Manager UI in the web browser:</p><ol><li>From the <em>toolbar</em> at the top of the page choose <em>Networking</em></li><li>From the left side list choose <em>IP Address Pools</em> below the <em>IP Management</em></li><li>Press the <em>ADD IP ADRESS POOL</em> button</li><li>Enter <em>Name</em></li><li>Enter at least one subnet by clicking on <em>Sets</em></li><li>Press the <em>Save</em> button</li></ol><h4 id=sizing-the-ip-pools>Sizing the IP pools</h4><p>Each shoot cluster needs one IP address for SNAT and at least two IP addresses for load balancers VIPs (kube-apiservcer and Gardener shoot-seed VPN). A third IP address may be needed for ingress.
Depending on the payload of a shoot cluster, there may be additional services of type <code>LoadBalancer</code>. An IP address is needed for every port of every Kubernetes service of type <code>LoadBalancer</code>.</p><h3 id=check-edge-cluster-sizing>Check edge cluster sizing</h3><p>For load balancer related configurations limitations of NSX-T, please see the web pages <a href="https://configmax.vmware.com/guest?vmwareproduct=NSX-T%20Data%20Center&release=NSX-T%20Data%20Center%203.1.0&categories=20-0">VMWare Configuration Maximums</a>. The link shows the limitations for NSX-T 3.1, if you have another version, please select the version from the left panel under <em>Select Version</em> and press the <em>VIEW LIMITS</em> button to update the view.</p><p>By default, settings, each shoot cluster has an own T1 gateway and an own LB service (instance) of &ldquo;T-shirt&rdquo; size <code>SMALL</code>.</p><p>Examples for limitations on NSX-T 3.1 using <em>Large Edge Node</em> and <em>SMALL</em> load balancers instances:</p><ol><li><p>There is a limit of 40 small LB instances per egde cluster (for HA 40 per pair of edge nodes)</p><p>=> maximum number of shoot clusters = 40 * (number of edge nodes) / 2</p></li><li><p>For <code>SMALL</code> load balancers, there is a maximum of 20 virtual servers. A virtual server is needed for every port of a service of type <code>LoadBalancer</code></p><p>=> maximum number of services/ports pairs = 20 * (number of edge nodes) / 2</p><p>The load balancer &ldquo;T-shirt&rdquo; size can be set on cloud profile level (<code>spec.providerConfig.contraints.loadBalancerConfig.size</code>) or in the shoot manifest (<code>spec.provider.controlPlaneConfig.loadBalancerSize</code>)</p></li><li><p>The number of pool members is limited to 7,500. For every K8s service port, every worker node is a pool member.</p><p>=> If every shoot cluster has an average number of 15 worker nodes, there can be 500 service/port pairs over all shoot clusters per pair of edge nodes</p></li></ol><h2 id=get-vds-uuids>Get VDS UUIDs</h2><p>This step is only needed, if there are several VDS (virtual distributed switches) for each zone.</p><p>In this case, their UUIDs need to be fetched and set in the cloud profile at <code>spec.providerConfig.regions[*].zones[*].switchUuid</code>.</p><p>Unfortunately, they are not displayed in the vSphere Client.</p><p>Here the command line tool <code>govc</code> is used to look them
up.</p><ol><li>Run <code>govc find / -type DistributedVirtualSwitch</code> to get the full path of all vds/dvs</li><li>For each switch run <code>govc dvs.portgroup.info &lt;switch-path> | grep DvsUuid</code></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-fdbdddf9cd1476016152695500fac711>4 - Tanzu Vsphere</h1><h2 id=create-tanzu-cluster>Create Tanzu Cluster</h2><p>For gardener a Tanzu Kubernetes „guest” cluster is used. Look here for the vSphere documentation <a href=https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-2597788E-2FA4-420E-B9BA-9423F8F7FD9F.html>Provisioning Tanzu Kubernetes Clusters</a></p><h3 id=virtual-machine-classes>Virtual Machine Classes</h3><p>For gardener the minimum Virtual Machine Classes must set to <code>best-effort-large</code>.</p><h3 id=network-settings>Network Settings</h3><p>For the deployment it is possible to provision the cluster with a minimal amount of configuration parameter. It is recommended to set the parameter <code>Default Pod CIDR</code>, <code>Default Services CIDR</code> with values which fit to your enviroment.</p><h3 id=storage-class-settings>Storage Class settings</h3><p>The <code>storageClass</code> Parameter should be defined to avoid problems during deployment.</p><p>Example:</p><pre tabindex=0><code>```yaml
apiVersion: run.tanzu.vmware.com/v1alpha1      #TKG API endpoint
kind: TanzuKubernetesCluster                   #required parameter
metadata:
name: tkg-cluster-1                          #cluster name, user defined
namespace: ns1                               #supervisor namespace
spec:
distribution:
    version: v1.24				 #resolved kubernetes version
topology:
    controlPlane:
    count: 1                                 #number of control plane nodes
    class: best-effort-small                 #vmclass for control plane nodes
    storageClass: vsan-default-storage-policy         #storageclass for control plane
    workers:
    count: 3                                 #number of worker nodes
    class: best-effort-large                 #vmclass for worker nodes
    storageClass: vsan-default-storage-policy         #storageclass for worker nodes
settings:
    network:
    cni:
        name: calico
    services:
        cidrBlocks: [&#34;198.51.100.0/12&#34;]        #Cannot overlap with Supervisor Cluster
    pods:
        cidrBlocks: [&#34;192.0.2.0/16&#34;]           #Cannot overlap with Supervisor Cluster
```
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-cdac9defb035df31c6dfc2cee8cbc281>5 - Usage As End User</h1><h1 id=using-the-vsphere-provider-extension-with-gardener-as-end-user>Using the vSphere provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for VMware vSphere and provide an example <code>Shoot</code> manifest with minimal configuration that you can use to create an vSphere cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=provider-secret-data>Provider secret data</h2><p>Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of your vSphere tenant.
It contains two authentication sets. One for the vSphere host and another for the NSX-T host, which is needed to set up the network infrastructure.
This <code>Secret</code> must look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: core-vsphere
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  vspherePassword: base64(vsphere-password)
</span></span><span style=display:flex><span>  vsphereUsername: base64(vSphere-UserName)
</span></span><span style=display:flex><span>  vsphereInsecureSSL: base64(&#34;true&#34;|&#34;false&#34;)
</span></span><span style=display:flex><span>  nsxtPassword: base64(NSX-T-password)
</span></span><span style=display:flex><span>  nsxtUserName: base64(NSX-T-UserName)
</span></span><span style=display:flex><span>  nsxtInsecureSSL: base64(&#34;true&#34;|&#34;false&#34;)
</span></span></code></pre></div><p>Here <code>base64(...)</code> are only a placeholders for the Base64 encoded values.</p><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration is used for advanced scenarios only.
Nodes on all zones are using IP addresses from the common nodes network as the network is managed by NSX-T.
The infrastructure controller will create several network objects using NSX-T. A network segment is used as the subnet
for the VMs (nodes), a tier-1 gateway, a DHCP server, and a SNAT for the nodes.</p><p>An example <code>InfrastructureConfig</code> for the vSphere extension looks as follows.
You only need to specify it, if you either want to use an existing Tier-1 gateway and load balancer service pair
or if you want to overwrite the automatic selection of the NSX-T version.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>infrastructureConfig:
</span></span><span style=display:flex><span>  apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>  kind: InfrastructureConfig
</span></span><span style=display:flex><span>  <span style=color:green>#overwriteNSXTInfraVersion: &#39;1&#39;</span>
</span></span><span style=display:flex><span>  <span style=color:green>#networks:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#  tier1GatewayPath: /infra/tier-1s/tier1gw-b8213651-9659-4180-8bfd-1e16228e8dcb</span>
</span></span><span style=display:flex><span>  <span style=color:green>#  loadBalancerServicePath: /infra/lb-services/708c5cb1-e5d0-4b16-906f-ec7177a1485d</span>
</span></span></code></pre></div><h3 id=advanced-configuration-settings>Advanced configuration settings</h3><h4 id=section-networks>Section networks</h4><p>By default, the infrastructure controller creates a separate Tier-1 gateway for each shoot cluster
and the cloud controller manager (<code>vsphere-cloud-provider</code>) creates a load balancer service.</p><p>If an existing Tier-1 gateway should be used, you can specify its &lsquo;path&rsquo;. In this case, there
must also be a load balancer service defined for this tier-1 gateway and its &lsquo;path&rsquo; needs to be specified, too.
In the NSX-T manager UI, the path of the tier-1 gateway can be found at <code>Networking / Tier-1 Gateways</code>.
Then select <code>Copy path to clipboard</code> from the context menu of the tier-1 gateway
(click on the three vertical dots on the left of the row). Do the same with the
corresponding load balancer at <code>Networking / Load balancing / Tab Load Balancers</code>
For security reasons the referenced Tier-1 gateway in NSX-T must have a tag with scope <code>authorized-shoots</code> and its
tag value consists of a comma-separated list of the allowed shoot names in the format <code>shoot--&lt;project>--&lt;name></code>
(optionally with wildcard <code>*</code>). Additionally, it must have a tag with scope <code>garden</code> set to the garden ID.</p><p>Example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>infrastructureConfig:
</span></span><span style=display:flex><span>  apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>  kind: InfrastructureConfig
</span></span><span style=display:flex><span>  networks:
</span></span><span style=display:flex><span>    tier1GatewayPath: /infra/tier-1s/tier1gw-b8213651-9659-4180-8bfd-1e16228e8dcb
</span></span><span style=display:flex><span>    loadBalancerServicePath: /infra/lb-services/708c5cb1-e5d0-4b16-906f-ec7177a1485d
</span></span></code></pre></div><p>Please ensure, that the worker nodes cidr (shoot manifest <code>spec.networking.nodes</code>) do not overlap with
other existing segments of the selected tier-1 gateway.</p><h4 id=option-overwritensxtinfraversion>Option overwriteNSXTInfraVersion</h4><p>The option <code>overwriteNSXTInfraVersion</code> can be used to change the network objects created during the initial infrastructure creation.
By default the infra-version is automatically selected according to the NSX-T version. The infra-version <code>'1'</code> is used
for NSX-T 2.5, and infra-version <code>'2'</code> for NSX-T versions >= 3.0. The difference is creation of the the logical DHCP server.
For NSX-T 2.5, only the DHCP server of the &ldquo;Advanced API&rdquo; is usable. For NSX-T >= 3.0 the new DHCP server is default,
but for special purposes infra-version <code>'1'</code> is also allowed.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the vSphere-specific control plane components.
Today, the only component deployed by the vSphere extension is the <code>cloud-controller-manager</code>.</p><p>An example <code>ControlPlaneConfig</code> for the vSphere extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlaneConfig
</span></span><span style=display:flex><span>loadBalancerClasses:
</span></span><span style=display:flex><span>  - name: mypubliclbclass
</span></span><span style=display:flex><span>  - name: myprivatelbclass
</span></span><span style=display:flex><span>    ipPoolName: pool42 <span style=color:green># optional overwrite</span>
</span></span><span style=display:flex><span>loadBalancerSize: SMALL
</span></span><span style=display:flex><span>cloudControllerManager:
</span></span><span style=display:flex><span>  featureGates:
</span></span><span style=display:flex><span>    CustomResourceValidation: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>The <code>loadBalancerClasses</code> optionally defines the load balancer classes to be used.
The specified names must be defined in the constraints section of the cloud profile.
If the list contains a load balancer named &ldquo;default&rdquo;, it is used as the default load balancer.
Otherwise the first one is also the default.
If no classes are specified the default load balancer class is used as defined in the cloud profile constraints section.
If the ipPoolName is overwritten, the corresponding IP pool object in NSX-T must have a tag with scope <code>authorized-shoots</code> and its
tag value consists of a comma-separated list of the allowed shoot names in the format <code>shoot--&lt;project>--&lt;name></code>
(optionally with wildcard <code>*</code>). Additionally, it must have a tag with scope <code>garden</code> set to the garden ID.</p><p>The <code>loadBalancerSize</code> is optional and overwrites the default value specified in the cloud profile config.
It must be one of the values <code>SMALL</code>, <code>MEDIUM</code>, or <code>LARGE</code>. <code>SMALL</code> can manage 10 service ports,
<code>MEDIUM</code> 100, and <code>LARGE</code> 1000.</p><p>The <code>cloudControllerManager.featureGates</code> contains an optional map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><h2 id=example-shoot-manifest-one-availability-zone>Example <code>Shoot</code> manifest (one availability zone)</h2><p>Please find below an example <code>Shoot</code> manifest for one availability zone:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-vsphere
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: vsphere
</span></span><span style=display:flex><span>  region: europe-1
</span></span><span style=display:flex><span>  secretBindingName: core-vsphere
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: vsphere
</span></span><span style=display:flex><span>   
</span></span><span style=display:flex><span>    <span style=color:green>#infrastructureConfig:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#  apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1</span>
</span></span><span style=display:flex><span>    <span style=color:green>#  kind: InfrastructureConfig</span>
</span></span><span style=display:flex><span>    <span style=color:green>#  overwriteNSXTInfraVersion: &#39;1&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    <span style=color:green>#  loadBalancerClasses:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#  - name: mylbclass</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: std-04
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - europe-1a
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-vsphere@v0.12</code>.</p><h2 id=shoot-ca-certificate-and-serviceaccount-signing-key-rotation>Shoot CA Certificate and <code>ServiceAccount</code> Signing Key Rotation</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>ShootCARotation</code> feature gate since <code>gardener-extension-provider-vsphere@v0.13</code> and <code>ShootSARotation</code> feature gate since <code>gardener-extension-provider-vsphere@v0.14</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0e203669ce71e2ac15660f0895763756>6 - Usage As Operator</h1><h1 id=using-the-vsphere-provider-extension-with-gardener-as-operator>Using the vSphere provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for VMware vSphere and provide an example <code>CloudProfile</code> manifest with minimal configuration that you can use to allow creating vSphere shoot clusters.</p><h2 id=cloudprofileconfig><code>CloudProfileConfig</code></h2><p>The cloud profile configuration contains information about the real machine image paths in the vSphere environment (image names).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the vSphere extension knows the image ID for every version you want to offer.</p><p>It also contains optional default values for DNS servers that shall be used for shoots.
In the <code>dnsServers[]</code> list you can specify IP addresses that are used as DNS configuration for created shoot subnets.</p><p>The <code>dhcpOptions</code> list allows to specify DHCP options. See <a href=https://www.iana.org/assignments/bootp-dhcp-parameters/bootp-dhcp-parameters.xhtml>BOOTP Vendor Extensions and DHCP Options</a>
for valid codes (tags) and details about values. The code <code>15</code> (domain name) is only allowed for
when using NSX-T 2.5. For NSX-T >= 3.0 use <code>119</code> (search domain).</p><p>The <code>dockerDaemonOptions</code> allow to adjust the docker daemon configuration.</p><ul><li>with <code>dockerDaemonOptions.httpProxyConf</code> the content of the proxy configuration file can be set.
See <a href=https://docs.docker.com/config/daemon/systemd/#httphttps-proxy>Docker HTTP/HTTPS proxy</a> for more details</li><li>with <code>dockerDaemonOptions.insecureRegistries</code> insecure registries can be specified. This
should only be used for development or evaluation purposes.</li></ul><p>Also, you have to specify several name of NSX-T objects in the constraints.</p><p>An example <code>CloudProfileConfig</code> for the vSphere extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: CloudProfileConfig
</span></span><span style=display:flex><span>namePrefix: my_gardener
</span></span><span style=display:flex><span>defaultClassStoragePolicyName: <span style=color:#a31515>&#34;vSAN Default Storage Policy&#34;</span>
</span></span><span style=display:flex><span>folder: my-vsphere-vm-folder
</span></span><span style=display:flex><span>regions:
</span></span><span style=display:flex><span>- name: region1
</span></span><span style=display:flex><span>  vsphereHost: my.vsphere.host
</span></span><span style=display:flex><span>  vsphereInsecureSSL: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  nsxtHost: my.vsphere.host
</span></span><span style=display:flex><span>  nsxtInsecureSSL: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  transportZone: <span style=color:#a31515>&#34;my-tz&#34;</span>
</span></span><span style=display:flex><span>  logicalTier0Router: <span style=color:#a31515>&#34;my-tier0router&#34;</span>
</span></span><span style=display:flex><span>  edgeCluster: <span style=color:#a31515>&#34;my-edgecluster&#34;</span>
</span></span><span style=display:flex><span>  snatIpPool: <span style=color:#a31515>&#34;my-snat-ip-pool&#34;</span>
</span></span><span style=display:flex><span>  datacenter: my-vsphere-dc
</span></span><span style=display:flex><span>  zones:
</span></span><span style=display:flex><span>  - name: zone1
</span></span><span style=display:flex><span>    computeCluster: my-vsphere-computecluster1
</span></span><span style=display:flex><span>    <span style=color:green># resourcePool: my-resource-pool1 # provide either computeCluster or resourcePool or hostSystem</span>
</span></span><span style=display:flex><span>    <span style=color:green># hostSystem: my-host1 # provide either computeCluster or resourcePool or hostSystem</span>
</span></span><span style=display:flex><span>    datastore: my-vsphere-datastore1
</span></span><span style=display:flex><span>    <span style=color:green>#datastoreCluster: my-vsphere-datastore-cluster # provide either datastore or datastoreCluster</span>
</span></span><span style=display:flex><span>  - name: zone2
</span></span><span style=display:flex><span>    computeCluster: my-vsphere-computecluster2
</span></span><span style=display:flex><span>    <span style=color:green># resourcePool: my-resource-pool2 # provide either computeCluster or resourcePool or hostSystem</span>
</span></span><span style=display:flex><span>    <span style=color:green># hostSystem: my-host2 # provide either computeCluster or resourcePool or hostSystem</span>
</span></span><span style=display:flex><span>    datastore: my-vsphere-datastore2
</span></span><span style=display:flex><span>    <span style=color:green>#datastoreCluster: my-vsphere-datastore-cluster # provide either datastore or datastoreCluster</span>
</span></span><span style=display:flex><span>constraints:
</span></span><span style=display:flex><span>  loadBalancerConfig:
</span></span><span style=display:flex><span>    size: MEDIUM
</span></span><span style=display:flex><span>    classes:
</span></span><span style=display:flex><span>    - name: default
</span></span><span style=display:flex><span>      ipPoolName: gardener_lb_vip
</span></span><span style=display:flex><span><span style=color:green># optional DHCP options like 119 (search domain), 42 (NTP), 15 (domain name (only NSX-T 2.5))</span>
</span></span><span style=display:flex><span><span style=color:green>#dhcpOptions:</span>
</span></span><span style=display:flex><span><span style=color:green>#- code: 15</span>
</span></span><span style=display:flex><span><span style=color:green>#  values:</span>
</span></span><span style=display:flex><span><span style=color:green>#  - foo.bar.com</span>
</span></span><span style=display:flex><span><span style=color:green>#- code: 42</span>
</span></span><span style=display:flex><span><span style=color:green>#  values:</span>
</span></span><span style=display:flex><span><span style=color:green>#  - 136.243.202.118</span>
</span></span><span style=display:flex><span><span style=color:green>#  - 80.240.29.124</span>
</span></span><span style=display:flex><span><span style=color:green>#  - 78.46.53.8</span>
</span></span><span style=display:flex><span><span style=color:green>#  - 162.159.200.123</span>
</span></span><span style=display:flex><span>dnsServers:
</span></span><span style=display:flex><span>- 10.10.10.11
</span></span><span style=display:flex><span>- 10.10.10.12
</span></span><span style=display:flex><span>machineImages:
</span></span><span style=display:flex><span>- name: flatcar
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 3139.2.3
</span></span><span style=display:flex><span>    path: gardener/templates/flatcar-3139.2.3
</span></span><span style=display:flex><span>    guestId: other4xLinux64Guest
</span></span><span style=display:flex><span><span style=color:green>#dockerDaemonOptions:</span>
</span></span><span style=display:flex><span><span style=color:green>#  httpProxyConf: |</span>
</span></span><span style=display:flex><span><span style=color:green>#    [Service]</span>
</span></span><span style=display:flex><span><span style=color:green>#    Environment=&#34;HTTPS_PROXY=https://proxy.example.com:443&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#  insecureRegistries:</span>
</span></span><span style=display:flex><span><span style=color:green>#  - myregistrydomain.com:5000</span>
</span></span><span style=display:flex><span><span style=color:green>#  - blabla.mycompany.local</span>
</span></span></code></pre></div><h2 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h2><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: vsphere
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: vsphere
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: CloudProfileConfig
</span></span><span style=display:flex><span>    namePrefix: my_gardener
</span></span><span style=display:flex><span>    defaultClassStoragePolicyName: <span style=color:#a31515>&#34;vSAN Default Storage Policy&#34;</span>
</span></span><span style=display:flex><span>    folder: my-vsphere-vm-folder
</span></span><span style=display:flex><span>    regions:
</span></span><span style=display:flex><span>    - name: region1
</span></span><span style=display:flex><span>      vsphereHost: my.vsphere.host
</span></span><span style=display:flex><span>      vsphereInsecureSSL: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      nsxtHost: my.vsphere.host
</span></span><span style=display:flex><span>      nsxtInsecureSSL: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      transportZone: <span style=color:#a31515>&#34;my-tz&#34;</span>
</span></span><span style=display:flex><span>      logicalTier0Router: <span style=color:#a31515>&#34;my-tier0router&#34;</span>
</span></span><span style=display:flex><span>      edgeCluster: <span style=color:#a31515>&#34;my-edgecluster&#34;</span>
</span></span><span style=display:flex><span>      snatIpPool: <span style=color:#a31515>&#34;my-snat-ip-pool&#34;</span>
</span></span><span style=display:flex><span>      datacenter: my-vsphere-dc
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - name: zone1
</span></span><span style=display:flex><span>        computeCluster: my-vsphere-computecluster1
</span></span><span style=display:flex><span>        <span style=color:green># resourcePool: my-resource-pool1 # provide either computeCluster or resourcePool or hostSystem</span>
</span></span><span style=display:flex><span>        <span style=color:green># hostSystem: my-host1 # provide either computeCluster or resourcePool or hostSystem</span>
</span></span><span style=display:flex><span>        datastore: my-vsphere-datastore1
</span></span><span style=display:flex><span>        <span style=color:green>#datastoreCluster: my-vsphere-datastore-cluster # provide either datastore or datastoreCluster</span>
</span></span><span style=display:flex><span>      - name: zone2
</span></span><span style=display:flex><span>        computeCluster: my-vsphere-computecluster2
</span></span><span style=display:flex><span>        <span style=color:green># resourcePool: my-resource-pool2 # provide either computeCluster or resourcePool or hostSystem</span>
</span></span><span style=display:flex><span>        <span style=color:green># hostSystem: my-host2 # provide either computeCluster or resourcePool or hostSystem</span>
</span></span><span style=display:flex><span>        datastore: my-vsphere-datastore2
</span></span><span style=display:flex><span>        <span style=color:green>#datastoreCluster: my-vsphere-datastore-cluster # provide either datastore or datastoreCluster</span>
</span></span><span style=display:flex><span>    constraints:
</span></span><span style=display:flex><span>      loadBalancerConfig:
</span></span><span style=display:flex><span>        size: MEDIUM
</span></span><span style=display:flex><span>        classes:
</span></span><span style=display:flex><span>        - name: default
</span></span><span style=display:flex><span>          ipPoolName: gardener_lb_vip
</span></span><span style=display:flex><span>    dnsServers:
</span></span><span style=display:flex><span>    - 10.10.10.11
</span></span><span style=display:flex><span>    - 10.10.10.12
</span></span><span style=display:flex><span>    machineImages:
</span></span><span style=display:flex><span>    - name: coreos
</span></span><span style=display:flex><span>      versions:
</span></span><span style=display:flex><span>      - version: 3139.2.3
</span></span><span style=display:flex><span>        path: gardener/templates/flatcar-3139.2.3
</span></span><span style=display:flex><span>        guestId: other4xLinux64Guest
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.23.4
</span></span><span style=display:flex><span>    - version: 1.24.0
</span></span><span style=display:flex><span>    - version: 1.24.1
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: flatcar
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 3139.2.3
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: std-02
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 8Gi
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  - name: std-04
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;4&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 16Gi
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  - name: std-08
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;8&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 32Gi
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - name: region1
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - name: zone1
</span></span><span style=display:flex><span>    - name: zone2
</span></span></code></pre></div><h2 id=which-versions-of-kubernetesvsphere-are-supported>Which versions of Kubernetes/vSphere are supported</h2><p>This extension targets Kubernetes >= <code>v1.20</code> and vSphere <code>6.7 U3</code> or later.</p><ul><li>vSphere CSI driver needs vSphere <code>6.7 U3</code> or later,
and Kubernetes >= <code>v1.16</code>
(see <a href=https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/index.html>VMware vSphere Container Storage Plug-in</a> for more details)</li><li>vSpere CPI driver needs vSphere <code>6.7 U3</code> or later,
and Kubernetes >= <code>v1.11</code>
(see <a href=https://github.com/kubernetes/cloud-provider-vsphere/blob/master/docs/book/cloud_provider_interface.md#which-versions-of-kubernetesvsphere-support-it>cloud-provider-vsphere CPI - Cloud Provider Interface</a>)</li></ul><h2 id=supported-vm-images>Supported VM images</h2><p>Currently, only Gardenlinux and Flatcar (CoreOS fork) are supported.
Virtual Machine Hardware must be version 15 or higher, but images are upgraded
automatically if their hardware has an older version.</p></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2023 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid@8.13.4/dist/mermaid.min.js integrity="sha512-JERecFUBbsm75UpkVheAuDOE8NdHjQBrPACfEQYPwvPG+fjgCpHAz1Jw2ci9EXmd3DdfiWth3O3CQvcfEg8gsA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.7b24c0fb082ffb2de6cb14d6c95e9f8053053709ffcf8c761ef8e9ad2f8021e4.js integrity="sha256-eyTA+wgv+y3myxTWyV6fgFMFNwn/z4x2HvjprS+AIeQ=" crossorigin=anonymous></script></body></html>