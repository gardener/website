<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener – Advanced</title><link>https://gardener.cloud/docs/gardener/advanced/</link><description>Recent content in Advanced on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://gardener.cloud/docs/gardener/advanced/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Cleanup of Shoot Clusters in Deletion</title><link>https://gardener.cloud/docs/gardener/advanced/shoot_cleanup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/advanced/shoot_cleanup/</guid><description>
&lt;h1 id="cleanup-of-shoot-clusters-in-deletion">Cleanup of Shoot Clusters in Deletion&lt;/h1>
&lt;p>When a shoot cluster is deleted then Gardener tries to gracefully remove most of the Kubernetes resources inside the cluster.
This is to prevent that any infrastructure or other artifacts remain after the shoot deletion.&lt;/p>
&lt;p>The cleanup is performed in four steps.
Some resources are deleted with a grace period, and all resources are forcefully deleted (by removing blocking finalizers) after some time to not block the cluster deletion entirely.&lt;/p>
&lt;p>&lt;strong>Cleanup steps:&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>All &lt;code>ValidatingWebhookConfiguration&lt;/code>s and &lt;code>MutatingWebhookConfiguration&lt;/code>s are deleted with a &lt;code>5m&lt;/code> grace period. Forceful finalization happens after &lt;code>5m&lt;/code>.&lt;/li>
&lt;li>All &lt;code>APIService&lt;/code>s and &lt;code>CustomResourceDefinition&lt;/code>s are deleted with a &lt;code>5m&lt;/code> grace period. Forceful finalization happens after &lt;code>1h&lt;/code>.&lt;/li>
&lt;li>All &lt;code>CronJob&lt;/code>s, &lt;code>DaemonSet&lt;/code>s, &lt;code>Deployment&lt;/code>s, &lt;code>Ingress&lt;/code>s, &lt;code>Job&lt;/code>s, &lt;code>Pod&lt;/code>s, &lt;code>ReplicaSet&lt;/code>s, &lt;code>ReplicationController&lt;/code>s, &lt;code>Service&lt;/code>s, &lt;code>StatefulSet&lt;/code>s, &lt;code>PersistentVolumeClaim&lt;/code>s are deleted with a &lt;code>5m&lt;/code> grace period. Forceful finalization happens after &lt;code>5m&lt;/code>.
&lt;blockquote>
&lt;p>If the &lt;code>Shoot&lt;/code> is annotated with &lt;code>shoot.gardener.cloud/skip-cleanup=true&lt;/code>, then only &lt;code>Service&lt;/code>s and &lt;code>PersistentVolumeClaim&lt;/code>s are considered.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>All &lt;code>VolumeSnapshot&lt;/code>s and &lt;code>VolumeSnapshotContent&lt;/code>s are deleted with a &lt;code>5m&lt;/code> grace period. Forceful finalization happens after &lt;code>1h&lt;/code>.&lt;/li>
&lt;/ol>
&lt;p>It is possible to override the finalization grace periods via annotations on the &lt;code>Shoot&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>shoot.gardener.cloud/cleanup-webhooks-finalize-grace-period-seconds&lt;/code> (for the resources handled in step 1)&lt;/li>
&lt;li>&lt;code>shoot.gardener.cloud/cleanup-extended-apis-finalize-grace-period-seconds&lt;/code> (for the resources handled in step 2)&lt;/li>
&lt;li>&lt;code>shoot.gardener.cloud/cleanup-kubernetes-resources-finalize-grace-period-seconds&lt;/code> (for the resources handled in step 3)&lt;/li>
&lt;/ul>
&lt;p>⚠️ If &lt;code>&amp;quot;0&amp;quot;&lt;/code> is provided, then all resources are finalized immediately without waiting for any graceful deletion.
Please be aware that this might lead to orphaned infrastructure artifacts.&lt;/p></description></item><item><title>Docs: containerd Registry Configuration</title><link>https://gardener.cloud/docs/gardener/advanced/containerd-registry-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/advanced/containerd-registry-configuration/</guid><description>
&lt;h1 id="containerd-registry-configuration">&lt;code>containerd&lt;/code> Registry Configuration&lt;/h1>
&lt;p>containerd supports configuring registries and mirrors. Using this native containerd feature, Shoot owners can configure containerd to use public or private mirrors for a given upstream registry. More details about the registry configuration can be found in the &lt;a href="https://github.com/containerd/containerd/blob/main/docs/hosts.md">corresponding upstream documentation&lt;/a>.&lt;/p>
&lt;h3 id="containerd-registry-configuration-patterns">&lt;code>containerd&lt;/code> Registry Configuration Patterns&lt;/h3>
&lt;p>At the time of writing this document, containerd support two patterns for configuring registries/mirrors.&lt;/p>
&lt;blockquote>
&lt;p>Note: Trying to use both of the patterns at the same time is not supported by containerd. Only one of the configuration patterns has to be followed strictly.&lt;/p>
&lt;/blockquote>
&lt;h5 id="old-and-deprecated-pattern">Old and Deprecated Pattern&lt;/h5>
&lt;p>The old and deprecated pattern is specifying &lt;code>registry.mirrors&lt;/code> and &lt;code>registry.configs&lt;/code> in the containerd&amp;rsquo;s config.toml file. See the &lt;a href="https://github.com/containerd/containerd/blob/main/docs/cri/registry.md">upstream documentation&lt;/a>.
Example of the old and deprecated pattern:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>version = 2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[plugins.&lt;span style="color:#a31515">&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span>.registry]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [plugins.&lt;span style="color:#a31515">&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span>.registry.mirrors]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [plugins.&lt;span style="color:#a31515">&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span>.registry.mirrors.&lt;span style="color:#a31515">&amp;#34;docker.io&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> endpoint = [&lt;span style="color:#a31515">&amp;#34;https://public-mirror.example.com&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In the above example, containerd is configured to first try to pull &lt;code>docker.io&lt;/code> images from a configured endpoint (&lt;code>https://public-mirror.example.com&lt;/code>). If the image is not available in &lt;code>https://public-mirror.example.com&lt;/code>, then containerd will fall back to the upstream registry (&lt;code>docker.io&lt;/code>) and will pull the image from there.&lt;/p>
&lt;h5 id="hosts-directory-pattern">Hosts Directory Pattern&lt;/h5>
&lt;p>The hosts directory pattern is the new and recommended pattern for configuring registries. It is available starting &lt;code>containerd@v1.5.0&lt;/code>. See the &lt;a href="https://github.com/containerd/containerd/blob/main/docs/hosts.md">upstream documentation&lt;/a>.
The above example in the hosts directory pattern looks as follows.
The &lt;code>/etc/containerd/config.toml&lt;/code> file has the following section:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>version = 2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[plugins.&lt;span style="color:#a31515">&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span>.registry]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config_path = &lt;span style="color:#a31515">&amp;#34;/etc/containerd/certs.d&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The following hosts directory structure has to be created:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ tree /etc/containerd/certs.d
/etc/containerd/certs.d
└── docker.io
└── hosts.toml
&lt;/code>&lt;/pre>&lt;p>Finally, for the &lt;code>docker.io&lt;/code> upstream registry, we configure a &lt;code>hosts.toml&lt;/code> file as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>server = &lt;span style="color:#a31515">&amp;#34;https://registry-1.docker.io&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[host.&lt;span style="color:#a31515">&amp;#34;http://public-mirror.example.com&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> capabilities = [&lt;span style="color:#a31515">&amp;#34;pull&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;resolve&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="configuring-containerd-registries-for-a-shoot">Configuring &lt;code>containerd&lt;/code> Registries for a Shoot&lt;/h3>
&lt;p>Gardener supports configuring &lt;code>containerd&lt;/code> registries on a Shoot using the new &lt;a href="https://github.com/containerd/containerd/blob/main/docs/hosts.md">hosts directory pattern&lt;/a>. For each Shoot Node, Gardener creates the &lt;code>/etc/containerd/certs.d&lt;/code> directory and adds the following section to the containerd&amp;rsquo;s &lt;code>/etc/containerd/config.toml&lt;/code> file:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>[plugins.&lt;span style="color:#a31515">&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span>.registry] &lt;span style="color:#008000"># gardener-managed&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config_path = &lt;span style="color:#a31515">&amp;#34;/etc/containerd/certs.d&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This allows Shoot owners to use the &lt;a href="https://github.com/containerd/containerd/blob/main/docs/hosts.md">hosts directory pattern&lt;/a> to configure registries for containerd. To do this, the Shoot owners need to create a directory under &lt;code>/etc/containerd/certs.d&lt;/code> that is named with the upstream registry host name. In the newly created directory, a &lt;code>hosts.toml&lt;/code> file needs to be created. For more details, see the &lt;a href="https://gardener.cloud/docs/gardener/advanced/containerd-registry-configuration/#hosts-directory-pattern">hosts directory pattern section&lt;/a> and the &lt;a href="https://github.com/containerd/containerd/blob/main/docs/hosts.md">upstream documentation&lt;/a>.&lt;/p>
&lt;h3 id="the-registry-cache-extension">The registry-cache Extension&lt;/h3>
&lt;p>There is a Gardener-native extension named &lt;a href="https://github.com/gardener/gardener-extension-registry-cache">registry-cache&lt;/a> that supports:&lt;/p>
&lt;ul>
&lt;li>Configuring containerd registry mirrors based on the above-described contract. The feature is added in &lt;a href="https://github.com/gardener/gardener-extension-registry-cache/releases/tag/v0.6.0">registry-cache@v0.6.0&lt;/a>.&lt;/li>
&lt;li>Running pull through cache(s) in the Shoot.&lt;/li>
&lt;/ul>
&lt;p>For more details, see the &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-registry-cache/">registry-cache documentation&lt;/a>.&lt;/p></description></item><item><title>Docs: Control Plane Endpoints And Ports</title><link>https://gardener.cloud/docs/gardener/advanced/control-plane-endpoints-and-ports/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/advanced/control-plane-endpoints-and-ports/</guid><description>
&lt;h1 id="endpoints-and-ports-of-a-shoot-control-plane">Endpoints and Ports of a Shoot Control-Plane&lt;/h1>
&lt;p>With the &lt;a href="https://gardener.cloud/docs/gardener/reversed-vpn-tunnel/">reversed VPN&lt;/a> tunnel, there are no endpoints with open ports in the shoot cluster required by Gardener.
In order to allow communication to the shoots control-plane in the seed cluster, there are endpoints shared by multiple shoots of a seed cluster.
Depending on the configured zones or &lt;a href="https://gardener.cloud/docs/gardener/networking/exposureclasses/">exposure classes&lt;/a>, there are different endpoints in a seed cluster. The IP address(es) can be determined by a DNS query for the API Server URL.
The main entry-point into the seed cluster is the load balancer of the Istio ingress-gateway service. Depending on the infrastructure provider, there can be one IP address per zone.&lt;/p>
&lt;p>The load balancer of the Istio ingress-gateway service exposes the following TCP ports:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>443&lt;/strong> for requests to the shoot API Server. The request is dispatched according to the set TLS SNI extension.&lt;/li>
&lt;li>&lt;strong>8443&lt;/strong> for requests to the shoot API Server via &lt;code>api-server-proxy&lt;/code>, dispatched based on the proxy protocol target, which is the IP address of &lt;code>kubernetes.default.svc.cluster.local&lt;/code> in the shoot.&lt;/li>
&lt;li>&lt;strong>8132&lt;/strong> to establish the reversed VPN connection. It&amp;rsquo;s dispatched according to an HTTP header value.&lt;/li>
&lt;/ul>
&lt;h2 id="kube-apiserver-via-sni">&lt;code>kube-apiserver&lt;/code> via SNI&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/api-server-sni_b9424a.png" alt="kube-apiserver via SNI">&lt;/p>
&lt;p>DNS entries for &lt;code>api.&amp;lt;external-domain&amp;gt;&lt;/code> and &lt;code>api.&amp;lt;shoot&amp;gt;.&amp;lt;project&amp;gt;.&amp;lt;internal-domain&amp;gt;&lt;/code> point to the load balancer of an Istio ingress-gateway service.
The Kubernetes client sets the server name to &lt;code>api.&amp;lt;external-domain&amp;gt;&lt;/code> or &lt;code>api.&amp;lt;shoot&amp;gt;.&amp;lt;project&amp;gt;.&amp;lt;internal-domain&amp;gt;&lt;/code>.
Based on SNI, the connection is forwarded to the respective API Server at TCP layer. There is no TLS termination at the Istio ingress-gateway.
TLS termination happens on the shoots API Server. Traffic is end-to-end encrypted between the client and the API Server. The certificate authority and authentication are defined in the corresponding &lt;code>kubeconfig&lt;/code>.
Details can be found in &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md">GEP-08&lt;/a>.&lt;/p>
&lt;h2 id="kube-apiserver-via-apiserver-proxy">&lt;code>kube-apiserver&lt;/code> via &lt;code>apiserver-proxy&lt;/code>&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/api-server-proxy_5b1751.png" alt="apiserver-proxy">&lt;/p>
&lt;p>Inside the shoot cluster, the API Server can also be reached by the cluster internal name &lt;code>kubernetes.default.svc.cluster.local&lt;/code>.
The pods &lt;code>apiserver-proxy&lt;/code> are deployed in the host network as daemonset and intercept connections to the Kubernetes service IP address.
The destination address is changed to the cluster IP address of the service &lt;code>kube-apiserver.&amp;lt;shoot-namespace&amp;gt;.svc.cluster.local&lt;/code> in the seed cluster.
The connections are forwarded via the &lt;a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/listeners/listener_filters/proxy_protocol">HaProxy Proxy Protocol&lt;/a> to the Istio ingress-gateway in the seed cluster.
The Istio ingress-gateway forwards the connection to the respective shoot API Server by it&amp;rsquo;s cluster IP address.
As TLS termination happens at the API Server, the traffic is end-to-end encrypted the same way as with SNI.&lt;/p>
&lt;p>Details can be found in &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/11-apiserver-network-proxy.md">GEP-11&lt;/a>.&lt;/p>
&lt;h2 id="reversed-vpn-tunnel">Reversed VPN Tunnel&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/reversed-vpn_ba3dfe.png" alt="Reversed VPN">&lt;/p>
&lt;p>As the API Server has to be able to connect to endpoints in the shoot cluster, a VPN connection is established.
This VPN connection is initiated from a VPN client in the shoot cluster.
The VPN client connects to the Istio ingress-gateway and is forwarded to the VPN server in the control-plane namespace of the shoot.
Once the VPN tunnel between the VPN client in the shoot and the VPN server in the seed cluster is established, the API Server can connect to nodes, services and pods in the shoot cluster.&lt;/p>
&lt;p>More details can be found in the &lt;a href="https://gardener.cloud/docs/gardener/reversed-vpn-tunnel/">usage document&lt;/a> and &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/14-reversed-cluster-vpn.md">GEP-14&lt;/a>.&lt;/p></description></item><item><title>Docs: Custom containerd Configuration</title><link>https://gardener.cloud/docs/gardener/advanced/custom-containerd-config/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/advanced/custom-containerd-config/</guid><description>
&lt;h1 id="custom-containerd-configuration">Custom &lt;code>containerd&lt;/code> Configuration&lt;/h1>
&lt;p>In case a &lt;code>Shoot&lt;/code> cluster uses &lt;code>containerd&lt;/code>, it is possible to make the &lt;code>containerd&lt;/code> process load custom configuration files.
Gardener initializes &lt;code>containerd&lt;/code> with the following statement:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>imports = [&lt;span style="color:#a31515">&amp;#34;/etc/containerd/conf.d/*.toml&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This means that all &lt;code>*.toml&lt;/code> files in the &lt;code>/etc/containerd/conf.d&lt;/code> directory will be imported and merged with the default configuration.
To prevent unintended configuration overwrites, please be aware that containerd merges config sections, not individual keys (see &lt;a href="https://github.com/containerd/containerd/issues/5837#issuecomment-894840240">here&lt;/a> and &lt;a href="https://github.com/gardener/gardener/pull/7316">here&lt;/a>).
Please consult the &lt;a href="https://github.com/containerd/containerd/blob/main/docs/man/containerd-config.toml.5.md#format">upstream &lt;code>containerd&lt;/code> documentation&lt;/a> for more information.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ Note that this only applies to nodes which were newly created after &lt;code>gardener/gardener@v1.51&lt;/code> was deployed. Existing nodes are not affected.&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: Necessary Labeling for Custom CSI Components</title><link>https://gardener.cloud/docs/gardener/advanced/csi_components/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/advanced/csi_components/</guid><description>
&lt;h1 id="necessary-labeling-for-custom-csi-components">Necessary Labeling for Custom CSI Components&lt;/h1>
&lt;p>Some provider extensions for Gardener are using CSI components to manage persistent volumes in the shoot clusters.
Additionally, most of the provider extensions are deploying controllers for taking volume snapshots (CSI snapshotter).&lt;/p>
&lt;p>End-users can deploy their own CSI components and controllers into shoot clusters.
In such situations, there are multiple controllers acting on the &lt;code>VolumeSnapshot&lt;/code> custom resources (each responsible for those instances associated with their respective driver provisioner types).&lt;/p>
&lt;p>However, this might lead to operational conflicts that cannot be overcome by Gardener alone.
Concretely, Gardener cannot know which custom CSI components were installed by end-users which can lead to issues, especially during shoot cluster deletion.
You can add a label to your custom CSI components indicating that Gardener should not try to remove them during shoot cluster deletion. This means you have to take care of the lifecycle for these components yourself!&lt;/p>
&lt;h2 id="recommendations">Recommendations&lt;/h2>
&lt;p>Custom CSI components are typically regular &lt;code>Deployment&lt;/code>s running in the shoot clusters.&lt;/p>
&lt;p>&lt;strong>Please label them with the &lt;code>shoot.gardener.cloud/no-cleanup=true&lt;/code> label.&lt;/strong>&lt;/p>
&lt;h2 id="background-information">Background Information&lt;/h2>
&lt;p>When a shoot cluster is deleted, Gardener deletes most Kubernetes resources (&lt;code>Deployment&lt;/code>s, &lt;code>DaemonSet&lt;/code>s, &lt;code>StatefulSet&lt;/code>s, etc.). Gardener will also try to delete CSI components if they are not marked with the above mentioned label.&lt;/p>
&lt;p>This can result in &lt;code>VolumeSnapshot&lt;/code> resources still having finalizers that will never be cleaned up.
Consequently, manual intervention is required to clean them up before the cluster deletion can continue.&lt;/p></description></item><item><title>Docs: Readiness of Shoot Worker Nodes</title><link>https://gardener.cloud/docs/gardener/advanced/node-readiness/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/advanced/node-readiness/</guid><description>
&lt;h1 id="readiness-of-shoot-worker-nodes">Readiness of Shoot Worker Nodes&lt;/h1>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>When registering new &lt;code>Nodes&lt;/code>, kubelet adds the &lt;code>node.kubernetes.io/not-ready&lt;/code> taint to prevent scheduling workload Pods to the &lt;code>Node&lt;/code> until the &lt;code>Ready&lt;/code> condition gets &lt;code>True&lt;/code>.
However, the kubelet does not consider the readiness of node-critical Pods.
Hence, the &lt;code>Ready&lt;/code> condition might get &lt;code>True&lt;/code> and the &lt;code>node.kubernetes.io/not-ready&lt;/code> taint might get removed, for example, before the CNI daemon Pod (e.g., &lt;code>calico-node&lt;/code>) has successfully placed the CNI binaries on the machine.&lt;/p>
&lt;p>This problem has been discussed extensively in kubernetes, e.g., in &lt;a href="https://github.com/kubernetes/kubernetes/issues/75890">kubernetes/kubernetes#75890&lt;/a>.
However, several proposals have been rejected because the problem can be solved by using the &lt;code>--register-with-taints&lt;/code> kubelet flag and dedicated controllers (&lt;a href="https://github.com/kubernetes/enhancements/pull/1003#issuecomment-619087019">ref&lt;/a>).&lt;/p>
&lt;h2 id="implementation-in-gardener">Implementation in Gardener&lt;/h2>
&lt;p>Gardener makes sure that workload Pods are only scheduled to &lt;code>Nodes&lt;/code> where all node-critical components required for running workload Pods are ready.
For this, Gardener follows the proposed solution by the Kubernetes community and registers new &lt;code>Node&lt;/code> objects with the &lt;code>node.gardener.cloud/critical-components-not-ready&lt;/code> taint (effect &lt;code>NoSchedule&lt;/code>).
gardener-resource-manager&amp;rsquo;s &lt;a href="https://gardener.cloud/docs/gardener/concepts/resource-manager/#node-controller">&lt;code>Node&lt;/code> controller&lt;/a> reacts on newly created &lt;code>Node&lt;/code> objects that have this taint.
The controller removes the taint once all node-critical Pods are ready (determined by checking the Pods&amp;rsquo; &lt;code>Ready&lt;/code> conditions).&lt;/p>
&lt;p>The &lt;code>Node&lt;/code> controller considers all &lt;code>DaemonSets&lt;/code> and &lt;code>Pods&lt;/code> with the label &lt;code>node.gardener.cloud/critical-component=true&lt;/code> as node-critical.
If there are &lt;code>DaemonSets&lt;/code> that contain the &lt;code>node.gardener.cloud/critical-component=true&lt;/code> label in their metadata and in their Pod template, the &lt;code>Node&lt;/code> controller waits for corresponding daemon Pods to be scheduled and to get ready before removing the taint.&lt;/p>
&lt;p>Additionally, the &lt;code>Node&lt;/code> controller checks for the readiness of &lt;code>csi-driver-node&lt;/code> components if a respective Pod indicates that it uses such a driver.
This is achieved through a well-defined annotation prefix (&lt;code>node.gardener.cloud/wait-for-csi-node-&lt;/code>).
For example, the &lt;code>csi-driver-node&lt;/code> Pod for Openstack Cinder is annotated with &lt;code>node.gardener.cloud/wait-for-csi-node-cinder=cinder.csi.openstack.org&lt;/code>.
A key prefix is used instead of a &amp;ldquo;regular&amp;rdquo; annotation to allow for multiple CSI drivers being registered by one &lt;code>csi-driver-node&lt;/code> Pod.
The annotation key&amp;rsquo;s suffix can be chosen arbitrarily (in this case &lt;code>cinder&lt;/code>) and the annotation value needs to match the actual driver name as specified in the &lt;code>CSINode&lt;/code> object.
The &lt;code>Node&lt;/code> controller will verify that the used driver is properly registered in this object before removing the &lt;code>node.gardener.cloud/critical-components-not-ready&lt;/code> taint.
Note that the &lt;code>csi-driver-node&lt;/code> Pod still needs to be labelled and tolerate the taint as described above to be considered in this additional check.&lt;/p>
&lt;h2 id="marking-node-critical-components">Marking Node-Critical Components&lt;/h2>
&lt;p>To make use of this feature, node-critical DaemonSets and Pods need to:&lt;/p>
&lt;ul>
&lt;li>Tolerate the &lt;code>node.gardener.cloud/critical-components-not-ready&lt;/code> &lt;code>NoSchedule&lt;/code> taint.&lt;/li>
&lt;li>Be labelled with &lt;code>node.gardener.cloud/critical-component=true&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>&lt;code>csi-driver-node&lt;/code> Pods additionally need to:&lt;/p>
&lt;ul>
&lt;li>Be annotated with &lt;code>node.gardener.cloud/wait-for-csi-node-&amp;lt;name&amp;gt;=&amp;lt;full-driver-name&amp;gt;&lt;/code>.
It&amp;rsquo;s required that these Pods fulfill the above criteria (label and toleration) as well.&lt;/li>
&lt;/ul>
&lt;p>Gardener already marks components like kube-proxy, apiserver-proxy and node-local-dns as node-critical.
Provider extensions mark components like csi-driver-node as node-critical and add the &lt;code>wait-for-csi-node&lt;/code> annotation.
Network extensions mark components responsible for setting up CNI on worker Nodes (e.g., &lt;code>calico-node&lt;/code>) as node-critical.
If shoot owners manage any additional node-critical components, they can make use of this feature as well.&lt;/p></description></item><item><title>Docs: Taints and Tolerations for Seeds and Shoots</title><link>https://gardener.cloud/docs/gardener/advanced/tolerations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/advanced/tolerations/</guid><description>
&lt;h1 id="taints-and-tolerations-for-seeds-and-shoots">Taints and Tolerations for &lt;code>Seed&lt;/code>s and &lt;code>Shoot&lt;/code>s&lt;/h1>
&lt;p>Similar to &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">taints and tolerations&lt;/a> for &lt;code>Node&lt;/code>s and &lt;code>Pod&lt;/code>s in Kubernetes, the &lt;code>Seed&lt;/code> resource supports specifying taints (&lt;code>.spec.taints&lt;/code>, see &lt;a href="https://github.com/gardener/gardener/blob/master/example/50-seed.yaml#L48-L55">this example&lt;/a>) while the &lt;code>Shoot&lt;/code> resource supports specifying tolerations (&lt;code>.spec.tolerations&lt;/code>, see &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L268-L269">this example&lt;/a>).
The feature is used to control scheduling to seeds as well as decisions whether a shoot can use a certain seed.&lt;/p>
&lt;p>Compared to Kubernetes, Gardener&amp;rsquo;s taints and tolerations are very much down-stripped right now and have some behavioral differences.
Please read the following explanations carefully if you plan to use them.&lt;/p>
&lt;h2 id="scheduling">Scheduling&lt;/h2>
&lt;p>When scheduling a new shoot, the gardener-scheduler will filter all seed candidates whose taints are not tolerated by the shoot.
As Gardener&amp;rsquo;s taints/tolerations don&amp;rsquo;t support &lt;code>effect&lt;/code>s yet, you can compare this behaviour with using a &lt;code>NoSchedule&lt;/code> effect taint in Kubernetes.&lt;/p>
&lt;p>Be reminded that taints/tolerations are no means to define any affinity or selection for seeds - please use &lt;code>.spec.seedSelector&lt;/code> in the &lt;code>Shoot&lt;/code> to state such desires.&lt;/p>
&lt;p>⚠️ Please note that - unlike how it&amp;rsquo;s implemented in Kubernetes - a certain seed cluster &lt;strong>may&lt;/strong> only be used when the shoot tolerates &lt;strong>all&lt;/strong> the seed&amp;rsquo;s taints.
This means that specifying &lt;code>.spec.seedName&lt;/code> for a seed whose taints are not tolerated will make the gardener-apiserver reject the request.&lt;/p>
&lt;p>Consequently, the taints/tolerations feature can be used as means to restrict usage of certain seeds.&lt;/p>
&lt;h2 id="toleration-defaults-and-whitelist">Toleration Defaults and Whitelist&lt;/h2>
&lt;p>The &lt;code>Project&lt;/code> resource features a &lt;code>.spec.tolerations&lt;/code> object that may carry &lt;code>defaults&lt;/code> and a &lt;code>whitelist&lt;/code> (see &lt;a href="https://github.com/gardener/gardener/blob/master/example/05-project-dev.yaml#L33-L37">this example&lt;/a>).
The corresponding &lt;code>ShootTolerationRestriction&lt;/code> admission plugin (cf. Kubernetes&amp;rsquo; &lt;code>PodTolerationRestriction&lt;/code> admission plugin) is responsible for evaluating these settings during creation/update of &lt;code>Shoot&lt;/code>s.&lt;/p>
&lt;h3 id="whitelist">Whitelist&lt;/h3>
&lt;p>If a shoot gets created or updated with tolerations, then it is validated that only those tolerations may be used that were added to either a) the &lt;code>Project&lt;/code>&amp;rsquo;s &lt;code>.spec.tolerations.whitelist&lt;/code>, or b) to the global whitelist in the &lt;code>ShootTolerationRestriction&lt;/code>&amp;rsquo;s admission config (see &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-admissionconfig.yaml#L7-L14">this example&lt;/a>).&lt;/p>
&lt;p>⚠️ Please note that the tolerations whitelist of &lt;code>Project&lt;/code>s can only be changed if the user trying to change it is bound to the &lt;code>modify-spec-tolerations-whitelist&lt;/code> custom RBAC role, e.g., via the following &lt;code>ClusterRole&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: rbac.authorization.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ClusterRole
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: full-project-modification-access
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- apiGroups:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - core.gardener.cloud
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - projects
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> verbs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - create
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - patch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - update
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - modify-spec-tolerations-whitelist
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - delete
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="defaults">Defaults&lt;/h3>
&lt;p>If a shoot gets created, then the default tolerations specified in both the &lt;code>Project&lt;/code>&amp;rsquo;s &lt;code>.spec.tolerations.defaults&lt;/code> and the global default list in the &lt;code>ShootTolerationRestriction&lt;/code> admission plugin&amp;rsquo;s configuration will be added to the &lt;code>.spec.tolerations&lt;/code> of the &lt;code>Shoot&lt;/code> (unless it already specifies a certain key).&lt;/p></description></item></channel></rss>