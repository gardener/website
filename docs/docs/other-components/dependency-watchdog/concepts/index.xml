<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Concepts on Gardener</title><link>https://gardener.cloud/docs/other-components/dependency-watchdog/concepts/</link><description>Recent content in Concepts on Gardener</description><generator>Hugo</generator><language>en-US</language><atom:link href="https://gardener.cloud/docs/other-components/dependency-watchdog/concepts/index.xml" rel="self" type="application/rss+xml"/><item><title>Prober</title><link>https://gardener.cloud/docs/other-components/dependency-watchdog/concepts/prober/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/dependency-watchdog/concepts/prober/</guid><description>&lt;h1 id="prober">Prober&lt;/h1>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Prober starts asynchronous and periodic probes for every shoot cluster. The first probe is the api-server probe which checks the reachability of the API Server from the control plane. The second probe is the lease probe which is done after the api server probe is successful and checks if the number of expired node leases is below a certain threshold.
If the lease probe fails, it will scale down the dependent kubernetes resources. Once the connectivity to &lt;code>kube-apiserver&lt;/code> is reestablished and the number of expired node leases are within the accepted threshold, the prober will then proactively scale up the dependent kubernetes resources it had scaled down earlier. The failure threshold fraction for lease probe
and dependent kubernetes resources are defined in &lt;a href="https://github.com/gardener/dependency-watchdog/blob/master/example/01-dwd-prober-configmap.yaml">configuration&lt;/a> that is passed to the prober.&lt;/p></description></item><item><title>Weeder</title><link>https://gardener.cloud/docs/other-components/dependency-watchdog/concepts/weeder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/dependency-watchdog/concepts/weeder/</guid><description>&lt;h1 id="weeder">Weeder&lt;/h1>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Weeder watches for an update to service endpoints and on receiving such an event it will create a time-bound watch for all configured dependent pods that need to be actively recovered in case they have not yet recovered from &lt;code>CrashLoopBackoff&lt;/code> state. In a nutshell it accelerates recovery of pods when an upstream service recovers.&lt;/p>
&lt;p>An interference in automatic recovery for dependent pods is required because kubernetes pod restarts a container with an exponential backoff when the pod is in &lt;code>CrashLoopBackOff&lt;/code> state. This backoff could become quite large if the service stays down for long. Presence of weeder would not let that happen as it&amp;rsquo;ll restart the pod.&lt;/p></description></item></channel></rss>