<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener – Usage</title><link>https://gardener.cloud/docs/gardener/usage/</link><description>Recent content in Usage on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Thu, 19 Nov 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://gardener.cloud/docs/gardener/usage/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Hibernate a Cluster</title><link>https://gardener.cloud/docs/gardener/usage/shoot_hibernate/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_hibernate/</guid><description>
&lt;h1 id="hibernate-a-cluster">Hibernate a Cluster&lt;/h1>
&lt;p>Clusters are only needed 24 hours a day if they run productive workload. So whenever you do development in a cluster, or just use it for tests or demo purposes, you can save much money if you scale-down your Kubernetes resources whenever you don&amp;rsquo;t need them. However, scaling them down manually can become time-consuming the more resources you have.&lt;/p>
&lt;p>Gardener offers a clever way to automatically scale-down all resources to zero: cluster hibernation. You can either hibernate a cluster by pushing a button or by defining a hibernation schedule.&lt;/p>
&lt;blockquote>
&lt;p>To save costs, it&amp;rsquo;s recommended to define a hibernation schedule before the creation of a cluster. You can hibernate your cluster or wake up your cluster manually even if there&amp;rsquo;s a schedule for its hibernation.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;a href="#what-is-hibernated">What is hibernated?&lt;/a>&lt;/li>
&lt;li>&lt;a href="#what-isnt-affected-by-the-hibernation">What isn’t affected by the hibernation?&lt;/a>&lt;/li>
&lt;li>&lt;a href="#hibernate-your-cluster-manually">Hibernate your cluster manually&lt;/a>&lt;/li>
&lt;li>&lt;a href="#wake-up-your-cluster-manually">Wake up your cluster manually&lt;/a>&lt;/li>
&lt;li>&lt;a href="#create-a-schedule-to-hibernate-your-cluster">Create a schedule to hibernate your cluster&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="what-is-hibernated">What is hibernated?&lt;/h2>
&lt;p>When a cluster is hibernated, Gardener scales down worker nodes and the cluster&amp;rsquo;s control plane to free resources at the IaaS provider. This affects:&lt;/p>
&lt;ul>
&lt;li>Your workload, for example, pods, deployments, custom resources.&lt;/li>
&lt;li>The virtual machines running your workload.&lt;/li>
&lt;li>The resources of the control plane of your cluster.&lt;/li>
&lt;/ul>
&lt;h2 id="what-isnt-affected-by-the-hibernation">What isn’t affected by the hibernation?&lt;/h2>
&lt;p>To scale up everything where it was before hibernation, Gardener doesn’t delete state-related information, that is, information stored in persistent volumes. The cluster state as persistent in &lt;code>etcd&lt;/code> is also preserved.&lt;/p>
&lt;h2 id="hibernate-your-cluster-manually">Hibernate your cluster manually&lt;/h2>
&lt;p>The &lt;code>.spec.hibernation.enabled&lt;/code> field specifies whether the cluster needs to be hibernated or not. If the field is set to &lt;code>true&lt;/code>, the cluster&amp;rsquo;s desired state is to be hibernated. If it is set to &lt;code>false&lt;/code> or not specified at all, the cluster&amp;rsquo;s desired state is to be awakened.&lt;/p>
&lt;p>To hibernate your cluster you can run the following &lt;code>kubectl&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl patch shoot -n $NAMESPACE $SHOOT_NAME -p &amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;hibernation&amp;#34;:{&amp;#34;enabled&amp;#34;: true}}}&amp;#39;
&lt;/code>&lt;/pre>&lt;h2 id="wake-up-your-cluster-manually">Wake up your cluster manually&lt;/h2>
&lt;p>To wake up your cluster you can run the following &lt;code>kubectl&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl patch shoot -n $NAMESPACE $SHOOT_NAME -p &amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;hibernation&amp;#34;:{&amp;#34;enabled&amp;#34;: false}}}&amp;#39;
&lt;/code>&lt;/pre>&lt;h2 id="create-a-schedule-to-hibernate-your-cluster">Create a schedule to hibernate your cluster&lt;/h2>
&lt;p>You can specify a hibernation schedule to automatically hibernate/wake up a cluster.&lt;/p>
&lt;p>Let&amp;rsquo;s have a look into the following example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span> hibernation:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">false&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> schedules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - start: &lt;span style="color:#a31515">&amp;#34;0 20 * * *&amp;#34;&lt;/span> &lt;span style="color:#008000"># Start hibernation every day at 8PM&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> end: &lt;span style="color:#a31515">&amp;#34;0 6 * * *&amp;#34;&lt;/span> &lt;span style="color:#008000"># Stop hibernation every day at 6AM&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> location: &lt;span style="color:#a31515">&amp;#34;America/Los_Angeles&amp;#34;&lt;/span> &lt;span style="color:#008000"># Specify a location for the cron to run in&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The above section configures a hibernation schedule that hibernates the cluster every day at 08:00 PM and wakes it up at 06:00 AM. The &lt;code>start&lt;/code> or &lt;code>end&lt;/code> fields can be omitted, though at least one of them has to be specified. Hence, it is possible to configure a hibernation schedule that only hibernates or wakes up a cluster. The &lt;code>location&lt;/code> field is the time location used to evaluate the cron expressions.&lt;/p></description></item><item><title>Docs: APIServer SNI Injection</title><link>https://gardener.cloud/docs/gardener/usage/apiserver-sni-injection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/apiserver-sni-injection/</guid><description>
&lt;h1 id="apiserversni-environment-variable-injection">APIServerSNI environment variable injection&lt;/h1>
&lt;p>If the Gardener administrator has enabled &lt;code>APIServerSNI&lt;/code> feature gate for a particular Seed cluster, then in each Shoot cluster&amp;rsquo;s &lt;code>kube-system&lt;/code> namespace a &lt;code>DaemonSet&lt;/code> called &lt;code>apiserver-proxy&lt;/code> is deployed. It routes traffic to the upstream Shoot Kube APIServer. See the &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md">APIServer SNI GEP&lt;/a> for more details.&lt;/p>
&lt;p>To skip this extra network hop, a &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook">mutating webhook&lt;/a> called &lt;code>apiserver-proxy.networking.gardener.cloud&lt;/code> is deployed next to the API server in the Seed. It adds &lt;code>KUBERNETES_SERVICE_HOST&lt;/code> environment variable to each container and init container that do not specify it. See the webhook &lt;a href="https://github.com/gardener/apiserver-proxy/">repository&lt;/a> for more information.&lt;/p>
&lt;h2 id="opt-out-of-pod-injection">Opt-out of pod injection&lt;/h2>
&lt;p>In some cases it&amp;rsquo;s desirable to opt-out of Pod injection:&lt;/p>
&lt;ul>
&lt;li>DNS is disabled on that individual Pod, but it still needs to talk to the kube-apiserver.&lt;/li>
&lt;li>Want to test the &lt;code>kube-proxy&lt;/code> and &lt;code>kubelet&lt;/code> in-cluster discovery.&lt;/li>
&lt;/ul>
&lt;h3 id="opt-out-of-pod-injection-for-specific-pods">Opt-out of pod injection for specific pods&lt;/h3>
&lt;p>To opt out of the injection, the Pod should be labeled with &lt;code>apiserver-proxy.networking.gardener.cloud/inject: disable&lt;/code> e.g.:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Deployment
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> replicas: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> selector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiserver-proxy.networking.gardener.cloud/inject: disable
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> image: nginx:1.14.2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ports:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - containerPort: 80
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="opt-out-of-pod-injection-on-namespace-level">Opt-out of pod injection on namespace level&lt;/h3>
&lt;p>To opt out of the injection of &lt;strong>all&lt;/strong> Pods in a namespace, you should label your namespace with &lt;code>apiserver-proxy.networking.gardener.cloud/inject: disable&lt;/code> e.g.:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiserver-proxy.networking.gardener.cloud/inject: disable
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-namespace
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>or via &lt;code>kubectl&lt;/code> for existing namespace:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl label namespace my-namespace apiserver-proxy.networking.gardener.cloud/inject=disable
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>NOTE: Please be aware that it&amp;rsquo;s not possible to disable injection on namespace level and enable it for individual pods in it.&lt;/p>
&lt;/blockquote>
&lt;h3 id="opt-out-of-pod-injection-for-the-entire-cluster">Opt-out of pod injection for the entire cluster&lt;/h3>
&lt;p>If the injection is causing problems for different workloads and ignoring individual pods or namespaces is not possible, then the feature could be disabled for the entire cluster with the &lt;code>alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector&lt;/code> annotation with value &lt;code>disable&lt;/code> on the &lt;code>Shoot&lt;/code> resource itself:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector: &lt;span style="color:#a31515">&amp;#39;disable&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-cluster
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>or via &lt;code>kubectl&lt;/code> for existing shoot cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl label shoot my-cluster alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector=disable
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>NOTE: Please be aware that it&amp;rsquo;s not possible to disable injection on cluster level and enable it for individual pods in it.&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: Configuration</title><link>https://gardener.cloud/docs/gardener/usage/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/configuration/</guid><description>
&lt;h1 id="gardener-configuration-and-usage">Gardener Configuration and Usage&lt;/h1>
&lt;p>Gardener automates the full lifecycle of Kubernetes clusters as a service.
Additionally, it has several extension points allowing external controllers to plug-in to the lifecycle.
As a consequence, there are several configuration options for the various custom resources that are partially required.&lt;/p>
&lt;p>This document describes the&lt;/p>
&lt;ol>
&lt;li>&lt;a href="#configuration-and-usage-of-gardener-as-operatoradministrator">configuration and usage of Gardener as operator/administrator&lt;/a>.&lt;/li>
&lt;li>&lt;a href="#configuration-and-usage-of-gardener-as-end-userstakeholdercustomer">configuration and usage of Gardener as end-user/stakeholder/customer&lt;/a>.&lt;/li>
&lt;/ol>
&lt;h2 id="configuration-and-usage-of-gardener-as-operatoradministrator">Configuration and Usage of Gardener as Operator/Administrator&lt;/h2>
&lt;p>When we use the terms &amp;ldquo;operator/administrator&amp;rdquo; we refer to both the people deploying and operating Gardener.
Gardener consists of the following components:&lt;/p>
&lt;ol>
&lt;li>&lt;code>gardener-apiserver&lt;/code>, a Kubernetes-native API extension that serves custom resources in the Kubernetes-style (like &lt;code>Seed&lt;/code>s and &lt;code>Shoot&lt;/code>s), and a component that contains multiple admission plugins.&lt;/li>
&lt;li>&lt;code>gardener-admission-controller&lt;/code>, an HTTP(S) server with several handlers to be used in a &lt;a href="https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/charts/application/templates/validatingwebhook-admission-controller.yaml">ValidatingWebhookConfiguration&lt;/a>.&lt;/li>
&lt;li>&lt;code>gardener-controller-manager&lt;/code>, a component consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for maintaining &lt;code>Shoot&lt;/code>s, reconciling &lt;code>Project&lt;/code>s, etc.).&lt;/li>
&lt;li>&lt;code>gardener-scheduler&lt;/code>, a component that assigns newly created &lt;code>Shoot&lt;/code> clusters to appropriate &lt;code>Seed&lt;/code> clusters.&lt;/li>
&lt;li>&lt;code>gardenlet&lt;/code>, a component running in seed clusters and consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for reconciliation and deletion of &lt;code>Shoot&lt;/code>s).&lt;/li>
&lt;/ol>
&lt;p>Each of these components have various configuration options.
The &lt;code>gardener-apiserver&lt;/code> uses the standard API server library maintained by the Kubernetes community, and as such it mainly supports command line flags.
Other components use so-called componentconfig files that describe their configuration in a Kubernetes-style versioned object.&lt;/p>
&lt;h3 id="configuration-file-for-gardener-admission-controller">Configuration file for Gardener admission controller&lt;/h3>
&lt;p>The Gardener admission controller does only support one command line flag which should be a path to a valid admission-controller configuration file.
Please take a look at &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-admission-controller.yaml">this&lt;/a> example configuration.&lt;/p>
&lt;h3 id="configuration-file-for-gardener-controller-manager">Configuration file for Gardener controller manager&lt;/h3>
&lt;p>The Gardener controller manager does only support one command line flag which should be a path to a valid controller-manager configuration file.
Please take a look at &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-controller-manager.yaml">this&lt;/a> example configuration.&lt;/p>
&lt;h3 id="configuration-file-for-gardener-scheduler">Configuration file for Gardener scheduler&lt;/h3>
&lt;p>The Gardener scheduler also only supports one command line flag which should be a path to a valid scheduler configuration file.
Please take a look at &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-scheduler.yaml">this&lt;/a> example configuration.
Information about the concepts of the Gardener scheduler can be found &lt;a href="https://gardener.cloud/docs/gardener/concepts/scheduler/">here&lt;/a>&lt;/p>
&lt;h3 id="configuration-file-for-gardenlet">Configuration file for Gardenlet&lt;/h3>
&lt;p>The Gardenlet also only supports one command line flag which should be a path to a valid gardenlet configuration file.
Please take a look at &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml">this&lt;/a> example configuration.
Information about the concepts of the Gardenlet can be found &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/">here&lt;/a>&lt;/p>
&lt;h3 id="system-configuration">System configuration&lt;/h3>
&lt;p>After successful deployment of the four components you need to setup the system.
Let&amp;rsquo;s first focus on some &amp;ldquo;static&amp;rdquo; configuration.
When the &lt;code>gardenlet&lt;/code> starts it scans the &lt;code>garden&lt;/code> namespace of the garden cluster for &lt;code>Secret&lt;/code>s that have influence on its reconciliation loops, mainly the &lt;code>Shoot&lt;/code> reconciliation:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Internal domain secret&lt;/strong>, contains the DNS provider credentials (having appropriate privileges) which will be used to create/delete so-called &amp;ldquo;internal&amp;rdquo; DNS records for the Shoot clusters, please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/10-secret-internal-domain.yaml">this&lt;/a> for an example.&lt;/p>
&lt;ul>
&lt;li>This secret is used in order to establish a stable endpoint for shoot clusters which is used internally by all control plane components.&lt;/li>
&lt;li>The DNS records are normal DNS records but called &amp;ldquo;internal&amp;rdquo; in our scenario because only the kubeconfigs for the control plane components use this endpoint when talking to the shoot clusters.&lt;/li>
&lt;li>It is forbidden to change the internal domain secret if there are existing shoot clusters.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Default domain secrets&lt;/strong> (optional), contain the DNS provider credentials (having appropriate privileges) which will be used to create/delete DNS records for a default domain for shoots (e.g., &lt;code>example.com&lt;/code>), please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/10-secret-default-domain.yaml">this&lt;/a> for an example.&lt;/p>
&lt;ul>
&lt;li>Not every end-user/stakeholder/customer has its own domain, however, Gardener needs to create a DNS record for every shoot cluster.&lt;/li>
&lt;li>As landscape operator you might want to define a default domain owned and controlled by you that is used for all shoot clusters that don&amp;rsquo;t specify their own domain.&lt;/li>
&lt;li>If you have multiple default domain secrets defined you can add a priority as an annotation (&lt;code>dns.gardener.cloud/domain-default-priority&lt;/code>) to select which domain should be used for new shoots while creation. The domain with the highest priority is selected while shoot creation. If there is no annotation defined the default priority is &lt;code>0&lt;/code>, also all non integer values are considered as priority &lt;code>0&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Alerting secrets&lt;/strong> (optional), contain the alerting configuration and credentials for the &lt;a href="https://prometheus.io/docs/alerting/alertmanager/">AlertManager&lt;/a> to send email alerts. It is also possible to configure the monitoring stack to send alerts to an AlertManager not deployed by Gardener to handle alerting. Please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/10-secret-alerting.yaml">this&lt;/a> for an example.&lt;/p>
&lt;ul>
&lt;li>If email alerting is configured:
&lt;ul>
&lt;li>An AlertManager is deployed into each seed cluster that handles the alerting for all shoots on the seed cluster.&lt;/li>
&lt;li>Gardener will inject the SMTP credentials into the configuration of the AlertManager.&lt;/li>
&lt;li>The AlertManager will send emails to the configured email address in case any alerts are firing.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>If an external AlertManager is configured:
&lt;ul>
&lt;li>Each shoot has a &lt;a href="https://prometheus.io/docs/introduction/overview/">Prometheus&lt;/a> responsible for monitoring components and sending out alerts. The alerts will be sent to a URL configured in the alerting secret.&lt;/li>
&lt;li>This external AlertManager is not managed by Gardener and can be configured however the operator sees fit.&lt;/li>
&lt;li>Supported authentication types are no authentication, basic, or mutual TLS.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>OpenVPN Diffie-Hellmann Key secret&lt;/strong> (optional), contains the self-generated Diffie-Hellmann key used by OpenVPN in your landscape, please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/10-secret-openvpn-diffie-hellman.yaml">this&lt;/a> for an example.&lt;/p>
&lt;ul>
&lt;li>If you don&amp;rsquo;t specify a custom key then a default key is used, but for productive landscapes it&amp;rsquo;s recommend to create a landscape-specific key and define it.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Global monitoring secrets&lt;/strong> (optional), contains basic authentication credentials for the Prometheus aggregating metrics for all clusters.&lt;/p>
&lt;ul>
&lt;li>These secrets are synced to each seed cluster and used to gain access to the aggregate monitoring components.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Apart from this &amp;ldquo;static&amp;rdquo; configuration there are several custom resources extending the Kubernetes API and used by Gardener.
As an operator/administrator you have to configure some of them to make the system work.&lt;/p>
&lt;h3 id="configuration-and-usage-of-gardener-as-end-userstakeholdercustomer">Configuration and Usage of Gardener as End-User/Stakeholder/Customer&lt;/h3>
&lt;p>As an end-user/stakeholder/customer you are using a Gardener landscape that has been setup for you by another team.
You don&amp;rsquo;t need to care about how Gardener itself has to be configured or how it has to be deployed.
Take a look at &lt;a href="https://gardener.cloud/docs/gardener/concepts/apiserver/">this document&lt;/a> - it describes which resources are offered by Gardener.
You may want to have a more detailed look for &lt;code>Project&lt;/code>s, &lt;code>SecretBinding&lt;/code>s, &lt;code>Shoot&lt;/code>s, and &lt;code>(Cluster)OpenIDConnectPreset&lt;/code>s.&lt;/p></description></item><item><title>Docs: Control Plane Endpoints And Ports</title><link>https://gardener.cloud/docs/gardener/usage/control-plane-endpoints-and-ports/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/control-plane-endpoints-and-ports/</guid><description>
&lt;h1 id="endpoints-and-ports-of-a-shoot-control-plane">Endpoints and Ports of a Shoot Control-Plane&lt;/h1>
&lt;p>With the &lt;a href="https://gardener.cloud/docs/gardener/usage/reversed-vpn-tunnel/">reversed VPN&lt;/a> tunnel, there are no endpoints with open ports in the shoot cluster required by gardener.
In order to allow communication to the shoots control-plane in the seed cluster, there are endpoints shared by multiple shoots of a seed cluster.
Depending on the configured zones or &lt;a href="https://gardener.cloud/docs/gardener/usage/exposureclasses/">exposure classes&lt;/a>, there are different endpoints in a seed cluster. The IP address(es) can be determined by a DNS query for the API Server URL.
The main entry-point into the seed cluster is the load balancer of the Istio ingress-gateway service. Depending on the infrastructure provider, there can be one IP address per zone.&lt;/p>
&lt;p>The load balancer of the Istio ingress-gateway service exposes the following TCP ports:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>443&lt;/strong> for requests to the shoot API Server. The request is dispatched according to the set TLS SNI extension.&lt;/li>
&lt;li>&lt;strong>8443&lt;/strong> for requests to the shoot API Server via &lt;code>api-server-proxy&lt;/code>, dispatched based on the proxy protocol target, which is the IP address of &lt;code>kubernetes.default.svc.cluster.local&lt;/code> in the shoot.&lt;/li>
&lt;li>&lt;strong>8132&lt;/strong> to establish the reversed VPN connection. It&amp;rsquo;s dispatched according to an HTTP header value.&lt;/li>
&lt;/ul>
&lt;h2 id="kube-apiserver-via-sni">&lt;code>kube-apiserver&lt;/code> via SNI&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/api-server-sni_feb16f.png" alt="kube-apiserver via SNI">&lt;/p>
&lt;p>DNS entries for &lt;code>api.&amp;lt;external-domain&amp;gt;&lt;/code> and &lt;code>api.&amp;lt;shoot&amp;gt;.&amp;lt;project&amp;gt;.&amp;lt;internal-domain&amp;gt;&lt;/code> point to the load balancer of an Istio ingress-gateway service.
The Kubernetes client sets the server name to &lt;code>api.&amp;lt;external-domain&amp;gt;&lt;/code> or &lt;code>api.&amp;lt;shoot&amp;gt;.&amp;lt;project&amp;gt;.&amp;lt;internal-domain&amp;gt;&lt;/code>.
Based on SNI, the connection is forwarded to the respective API Server at TCP layer. There is no TLS termination at the Istio ingress-gateway.
TLS termination happens on the shoots API Server. Traffic is end-to-end encrypted between the client and the API Server. The certificate authority and authentication are defined in the corresponding &lt;code>kubeconfig&lt;/code>.
Details can be found in &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md">GEP-08&lt;/a>.&lt;/p>
&lt;h2 id="kube-apiserver-via-apiserver-proxy">&lt;code>kube-apiserver&lt;/code> via &lt;code>apiserver-proxy&lt;/code>&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/api-server-proxy_b419fc.png" alt="apiserver-proxy">&lt;/p>
&lt;p>Inside the shoot cluster, the API Server can also be reached by the cluster internal name &lt;code>kubernetes.default.svc.cluster.local&lt;/code>.
The pods &lt;code>apiserver-proxy&lt;/code> are deployed in the host network as daemonset and intercept connections to the Kubernetes service IP address.
The destination address is changed to the cluster IP address of the service &lt;code>kube-apiserver.&amp;lt;shoot-namespace&amp;gt;.svc.cluster.local&lt;/code> in the seed cluster.
The connections are forwarded via the &lt;a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/listeners/listener_filters/proxy_protocol">HaProxy Proxy Protocol&lt;/a> to the Istio ingress-gateway in the seed cluster.
The Istio ingress-gateway forwards the connection to the respective shoot API Server by it&amp;rsquo;s cluster IP address.
As TLS termination happens at the API Server, the traffic is end-to-end encrypted the same way as with SNI.&lt;/p>
&lt;p>Details can be found in &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/11-apiserver-network-proxy.md">GEP-11&lt;/a>.&lt;/p>
&lt;h2 id="reversed-vpn-tunnel">Reversed VPN tunnel&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/reversed-vpn_e89ad6.png" alt="Reversed VPN">&lt;/p>
&lt;p>As the API Server has to be able to connect to endpoints in the shoot cluster, a VPN connection is established.
This VPN connection is initiated from a VPN client in the shoot cluster.
The VPN client connects to the Istio ingress-gateway and is forwarded to the VPN server in the control-plane namespace of the shoot.
Once the VPN tunnel between the VPN client in the shoot and the VPN server in the seed cluster is established, the API Server can connect to nodes,
services and pods in the shoot cluster.&lt;/p>
&lt;p>More details can be found in the &lt;a href="https://gardener.cloud/docs/gardener/usage/reversed-vpn-tunnel/">usage document&lt;/a> and &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/14-reversed-cluster-vpn.md">GEP-14&lt;/a>.&lt;/p></description></item><item><title>Docs: Control Plane Migration</title><link>https://gardener.cloud/docs/gardener/usage/control_plane_migration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/control_plane_migration/</guid><description>
&lt;h1 id="control-plane-migration">Control Plane Migration&lt;/h1>
&lt;h2 id="preconditions">Preconditions&lt;/h2>
&lt;p>To be able to use this feature, the &lt;code>SeedChange&lt;/code> feature gate has to be enabled on your &lt;code>gardener-apiserver&lt;/code>.&lt;/p>
&lt;p>Also, the involved Seeds need to have enabled BackupBuckets.&lt;/p>
&lt;h2 id="shootstate">ShootState&lt;/h2>
&lt;p>&lt;code>ShootState&lt;/code> is an API resource which stores non-reconstructible state and data required to completely recreate a &lt;code>Shoot&lt;/code>&amp;rsquo;s control plane on a new &lt;code>Seed&lt;/code>. The &lt;code>ShootState&lt;/code> resource is created on &lt;code>Shoot&lt;/code> creation in its &lt;code>Project&lt;/code> namespace and the required state/data is persisted during &lt;code>Shoot&lt;/code> creation or reconciliation.&lt;/p>
&lt;h2 id="shoot-control-plane-migration">Shoot Control Plane Migration&lt;/h2>
&lt;p>Triggering the migration is done by changing the &lt;code>Shoot&lt;/code>&amp;rsquo;s &lt;code>.spec.seedName&lt;/code> to a &lt;code>Seed&lt;/code> that differs from the &lt;code>.status.seedName&lt;/code>, we call this &lt;code>Seed&lt;/code> &lt;code>&amp;quot;Destination Seed&amp;quot;&lt;/code>. This action can only be performed by an operator with necessary RBAC. If the Destination &lt;code>Seed&lt;/code> does not have a backup and restore configuration, the change to &lt;code>spec.seedName&lt;/code> is rejected. Additionally, this Seed must not be set for deletion and must be healthy.&lt;/p>
&lt;p>If the &lt;code>Shoot&lt;/code> has different &lt;code>.spec.seedName&lt;/code> and &lt;code>.status.seedName&lt;/code> a process is started to prepare the Control Plane for migration:&lt;/p>
&lt;ol>
&lt;li>&lt;code>.status.lastOperation&lt;/code> is changed to &lt;code>Migrate&lt;/code>.&lt;/li>
&lt;li>Kubernetes API Server is stopped and the extension resources are annotated with &lt;code>gardener.cloud/operation=migrate&lt;/code>.&lt;/li>
&lt;li>Full snapshot of the ETCD is created and terminating of the Control Plane in the &lt;code>Source Seed&lt;/code> is initiated.&lt;/li>
&lt;/ol>
&lt;p>If the process is successful, we update the status of the &lt;code>Shoot&lt;/code> by setting the &lt;code>.status.seedName&lt;/code> to the null value. That way, a restoration is triggered in the &lt;code>Destination Seed&lt;/code> and &lt;code>.status.lastOperation&lt;/code> is changed to &lt;code>Restore&lt;/code>. The control plane migration is completed when the &lt;code>Restore&lt;/code> operation has completed successfully.&lt;/p>
&lt;p>When the &lt;code>CopyEtcdBackupsDuringControlPlaneMigration&lt;/code> feature gate is enabled on the &lt;code>gardenlet&lt;/code>, the etcd backups will be copied over to the &lt;code>BackupBucket&lt;/code> of the &lt;code>Destination Seed&lt;/code> during control plane migration and any future backups will be uploaded there. Otherwise, backups will continue to be uploaded to the &lt;code>BackupBucket&lt;/code> of the &lt;code>Source Seed&lt;/code>,&lt;/p>
&lt;h2 id="triggering-the-migration">Triggering the migration&lt;/h2>
&lt;p>For controlplane migration, operators with necessary RBAC can use the &lt;a href="https://gardener.cloud/docs/gardener/concepts/scheduler/#shootsbinding-subresource">&lt;code>shoots/binding&lt;/code>&lt;/a> subresource to change the &lt;code>.spec.seedName&lt;/code>, with the following commands:&lt;/p>
&lt;pre tabindex="0">&lt;code>export NAMESPACE=my-namespace
export SHOOT_NAME=my-shoot
kubectl get --raw /apis/core.gardener.cloud/v1beta1/namespaces/${NAMESPACE}/shoots/${SHOOT_NAME} | jq -c &amp;#39;.spec.seedName = &amp;#34;&amp;lt;destination-seed&amp;gt;&amp;#34;&amp;#39; | kubectl replace --raw /apis/core.gardener.cloud/v1beta1/namespaces/${NAMESPACE}/shoots/${SHOOT_NAME}/binding -f - | jq -r &amp;#39;.spec.seedName&amp;#39;
&lt;/code>&lt;/pre></description></item><item><title>Docs: CSI Components</title><link>https://gardener.cloud/docs/gardener/usage/csi_components/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/csi_components/</guid><description>
&lt;h1 id="custom-csi-components">(Custom) CSI Components&lt;/h1>
&lt;p>Some provider extensions for Gardener are using CSI components to manage persistent volumes in the shoot clusters.
Additionally, most of the provider extensions are deploying controllers for taking volume snapshots (CSI snapshotter).&lt;/p>
&lt;p>End-users can deploy their own CSI components and controllers into shoot clusters.
In such situations, there are multiple controllers acting on the &lt;code>VolumeSnapshot&lt;/code> custom resources (each responsible for those instances associated with their respective driver provisioner types).&lt;/p>
&lt;p>However, this might lead to operational conflicts that cannot be overcome by Gardener alone.
Concretely, Gardener cannot know which custom CSI components were installed by end-users which can lead to issues, especially during shoot cluster deletion.
You can add a label to your custom CSI components indicating that Gardener should not try to remove them during shoot cluster deletion. This means you have to take care of the lifecycle for these components yourself!&lt;/p>
&lt;h2 id="recommendations">Recommendations&lt;/h2>
&lt;p>Custom CSI components are typically regular &lt;code>Deployment&lt;/code>s running in the shoot clusters.&lt;/p>
&lt;p>&lt;strong>Please label them with the &lt;code>shoot.gardener.cloud/no-cleanup=true&lt;/code> label.&lt;/strong>&lt;/p>
&lt;h2 id="background-information">Background Information&lt;/h2>
&lt;p>When a shoot cluster is deleted, Gardener deletes most Kubernetes resources (&lt;code>Deployment&lt;/code>s, &lt;code>DaemonSet&lt;/code>s, &lt;code>StatefulSet&lt;/code>s, etc.). Gardener will also try to delete CSI components if they are not marked with the above mentioned label.&lt;/p>
&lt;p>This can result in &lt;code>VolumeSnapshot&lt;/code> resources still having finalizers that will never be cleaned up.
Consequently, manual intervention is required to clean them up before the cluster deletion can continue.&lt;/p></description></item><item><title>Docs: Custom containerd Configuration</title><link>https://gardener.cloud/docs/gardener/usage/custom-containerd-config/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/custom-containerd-config/</guid><description>
&lt;h1 id="custom-containerd-configuration">Custom &lt;code>containerd&lt;/code> Configuration&lt;/h1>
&lt;p>In case a &lt;code>Shoot&lt;/code> cluster uses &lt;code>containerd&lt;/code> (see &lt;a href="https://gardener.cloud/docs/gardener/usage/docker-shim-removal/">this document&lt;/a>) for more information), it is possible to make the &lt;code>containerd&lt;/code> process load custom configuration files.
Gardener initializes &lt;code>contaienerd&lt;/code> with the following statement:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>imports = [&lt;span style="color:#a31515">&amp;#34;/etc/containerd/conf.d/*.toml&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This means that all &lt;code>*.toml&lt;/code> files in the &lt;code>/etc/containerd/conf.d&lt;/code> directory will be imported and merged with the default configuration.
To prevent unintended configuration overwrites, please be aware that containerd merges config sections, not individual keys (see &lt;a href="https://github.com/containerd/containerd/issues/5837#issuecomment-894840240">here&lt;/a> and &lt;a href="https://github.com/gardener/gardener/pull/7316">here&lt;/a>).
Please consult the &lt;a href="https://github.com/containerd/containerd/blob/main/docs/man/containerd-config.toml.5.md#format">upstream &lt;code>containerd&lt;/code> documentation&lt;/a> for more information.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ Note that this only applies to nodes which were newly created after &lt;code>gardener/gardener@v1.51&lt;/code> was deployed. Existing nodes are not affected.&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: Custom DNS Configuration</title><link>https://gardener.cloud/docs/gardener/usage/custom-dns-config/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/custom-dns-config/</guid><description>
&lt;h1 id="custom-dns-configuration">Custom DNS Configuration&lt;/h1>
&lt;p>Gardener provides Kubernetes-Clusters-As-A-Service where all the system components (e.g., kube-proxy, networking, dns, &amp;hellip;) are managed.
As a result, Gardener needs to ensure and auto-correct additional configuration to those system components to avoid unnecessary down-time.&lt;/p>
&lt;p>In some cases, auto-correcting system components can prevent users from deploying applications on top of the cluster that requires bits of customization, DNS configuration can be a good example.&lt;/p>
&lt;p>To allow for customizations for DNS configuration (that could potentially lead to downtime) while having the option to &amp;ldquo;undo&amp;rdquo;, we utilize the &lt;code>import&lt;/code> plugin from CoreDNS [1].
which enables in-line configuration changes.&lt;/p>
&lt;h2 id="how-to-use">How to use&lt;/h2>
&lt;p>To customize your CoreDNS cluster config, you can simply edit a &lt;code>ConfigMap&lt;/code> named &lt;code>coredns-custom&lt;/code> in the &lt;code>kube-system&lt;/code> namespace.
By editing, this &lt;code>ConfigMap&lt;/code>, you are modifying CoreDNS configuration, therefore care is advised.&lt;/p>
&lt;p>For example, to apply new config to CoreDNS that would point all &lt;code>.global&lt;/code> DNS requests to another DNS pod, simply edit the configuration as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ConfigMap
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: coredns-custom
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: kube-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> istio.server: |&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> global:8053 {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> errors
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> cache 30
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> forward . 1.2.3.4
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> }&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> corefile.override: |&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> # &amp;lt;some-plugin&amp;gt; &amp;lt;some-plugin-config&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> debug
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> whoami&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It is important to have the &lt;code>ConfigMap&lt;/code> keys ending with &lt;code>*.server&lt;/code> (if you would like to add a new server) or &lt;code>*.override&lt;/code>
if you want to customize the current server configuration (it is optional setting both).&lt;/p>
&lt;h2 id="optional-reload-coredns">[Optional] Reload CoreDNS&lt;/h2>
&lt;p>As Gardener is configuring the &lt;code>reload&lt;/code> &lt;a href="https://coredns.io/plugins/reload/">plugin&lt;/a> of CoreDNS a restart of the CoreDNS components is typically not necessary to propagate &lt;code>ConfigMap&lt;/code> changes. However, if you don&amp;rsquo;t want to wait for the default (30s) to kick in, you can roll-out your CoreDNS deployment using:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n kube-system rollout restart deploy coredns
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will reload the config into CoreDNS.&lt;/p>
&lt;p>The approach we follow here was inspired by AKS&amp;rsquo;s approach [2].&lt;/p>
&lt;h2 id="anti-pattern">Anti-Pattern&lt;/h2>
&lt;p>Applying a configuration that is in-compatible with the running version of CoreDNS is an anti-pattern (sometimes plugin configuration changes,
simply applying a configuration can break DNS).&lt;/p>
&lt;p>If incompatible changes are applied by mistake, simply delete the content of the &lt;code>ConfigMap&lt;/code> and re-apply.
This should bring the cluster DNS back to functioning state.&lt;/p>
&lt;h2 id="node-local-dns">Node Local DNS&lt;/h2>
&lt;p>Custom DNS configuration] may not work as expected in conjunction with &lt;code>NodeLocalDNS&lt;/code>.
With &lt;code>NodeLocalDNS&lt;/code>, ordinary DNS queries targeted at the upstream DNS servers, i.e. non-kubernetes domains,
will not end up at CoreDNS, but will instead be directly sent to the upstream DNS server. Therefore, configuration
applying to non-kubernetes entities, e.g. the &lt;code>istio.server&lt;/code> block in the
&lt;a href="https://gardener.cloud/docs/gardener/usage/custom-dns-config/">custom DNS configuration&lt;/a> example, may not have any effect with &lt;code>NodeLocalDNS&lt;/code> enabled.
If this kind of custom configuration is required, forwarding to upstream DNS has to be disabled.
This can be done by setting the option (&lt;code>spec.systemComponents.nodeLocalDNS.disableForwardToUpstreamDNS&lt;/code>) in the &lt;code>Shoot&lt;/code> resource to &lt;code>true&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> systemComponents:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeLocalDNS:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> disableForwardToUpstreamDNS: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="references">References&lt;/h2>
&lt;p>[1] &lt;a href="https://github.com/coredns/coredns/tree/master/plugin/import">Import plugin&lt;/a>
[2] &lt;a href="https://docs.microsoft.com/en-us/azure/aks/coredns-custom">AKS Custom DNS&lt;/a>&lt;/p></description></item><item><title>Docs: Default Seccomp Profile</title><link>https://gardener.cloud/docs/gardener/usage/default_seccomp_profile/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/default_seccomp_profile/</guid><description>
&lt;h1 id="default-seccomp-profile-and-configuration">Default Seccomp Profile and Configuration&lt;/h1>
&lt;p>This is a short guide describing how to enable the defaulting of seccomp profiles for Gardener managed workloads in the seed.&lt;/p>
&lt;h2 id="default-kubernetes-behavior">Default Kubernetes Behavior&lt;/h2>
&lt;p>The state of Kubernetes in versions &amp;lt; 1.25 is such that all workloads by default run in &lt;code>Unconfined&lt;/code> (seccomp disabled) mode. This is undesirable since this is the least restrictive profile. Also mind that any privileged container will always run as &lt;code>Unconfined&lt;/code>. More information about seccomp can be found in this &lt;a href="https://kubernetes.io/docs/tutorials/security/seccomp/">Kubernetes tutorial&lt;/a>.&lt;/p>
&lt;h2 id="setting-the-seccomp-profile-to-runtimedefault-for-seed-clusters">Setting the Seccomp Profile to RuntimeDefault for seed clusters&lt;/h2>
&lt;p>To address the above issue, Gardener provides a webhook that is capable of mutating pods in the seed clusters, explicitly providing them with a seccomp profile type of &lt;code>RuntimeDefault&lt;/code>. This profile is defined by the container runtime and represents a set of default syscalls that are allowed or not.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> securityContext:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> seccompProfile:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: RuntimeDefault
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>A &lt;code>Pod&lt;/code> is mutated when all of the following preconditions are fulfilled:&lt;/p>
&lt;ol>
&lt;li>The &lt;code>Pod&lt;/code> is created in Gardener managed namespace.&lt;/li>
&lt;li>The &lt;code>Pod&lt;/code> is NOT labeled with &lt;code>seccompprofile.resources.gardener.cloud/skip&lt;/code>.&lt;/li>
&lt;li>The &lt;code>Pod&lt;/code> does NOT explicitly specify &lt;code>.spec.securityContext.seccompProfile.type&lt;/code>.&lt;/li>
&lt;/ol>
&lt;h3 id="how-to-configure">How to Configure&lt;/h3>
&lt;p>To enable this feature, the Gardenlet &lt;code>DefaultSeccompProfile&lt;/code> feature gate must be set to &lt;code>true&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>featureGates:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> DefaultSeccompProfile: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Please refer to the examples &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml">here&lt;/a> for more information.&lt;/p>
&lt;p>Once the feature gate is enabled, the webhook will be registered and configured for the seed cluster. Newly created pods will be mutated to have their seccomp profile set to &lt;code>RuntimeDefault&lt;/code>.&lt;/p>
&lt;blockquote>
&lt;p>Please note that this feature is still in Alpha, so you might see instabilities every now and then.&lt;/p>
&lt;/blockquote>
&lt;h2 id="setting-the-seccomp-profile-to-runtimedefault-for-shoot-clusters">Setting the Seccomp Profile to RuntimeDefault for shoot clusters&lt;/h2>
&lt;p>For kubernetes shoot versions &amp;gt;= 1.25 you can enable the use of &lt;code>RuntimeDefault&lt;/code> as the default seccomp profile for all workloads. If enabled, the kubelet will use the &lt;code>RuntimeDefault&lt;/code> seccomp profile by default, which is defined by the container runtime, instead of using the &lt;code>Unconfined&lt;/code> mode. More information for this feature can be found in the &lt;a href="https://kubernetes.io/docs/tutorials/security/seccomp/#enable-the-use-of-runtimedefault-as-the-default-seccomp-profile-for-all-workloads">Kubernetes documentation&lt;/a>&lt;/p>
&lt;p>To use seccomp profile defaulting, you must run the kubelet with the &lt;code>SeccompDefault&lt;/code> feature gate enabled (this is the default for k8s versions &amp;gt;= 1.25).&lt;/p>
&lt;h3 id="how-to-configure-1">How to Configure&lt;/h3>
&lt;p>To enable this feature, the kubelet &lt;code>seccompDefault&lt;/code> configuration parameter must be set to &lt;code>true&lt;/code> in the shoot&amp;rsquo;s spec.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.25.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubelet:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> seccompDefault: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Please refer to the examples &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml">here&lt;/a> for more information.&lt;/p></description></item><item><title>Docs: DNS Autoscaling</title><link>https://gardener.cloud/docs/gardener/usage/dns-autoscaling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/dns-autoscaling/</guid><description>
&lt;h1 id="dns-autoscaling">DNS Autoscaling&lt;/h1>
&lt;p>This is a short guide describing different options how to automatically scale CoreDNS in the shoot cluster.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Currently, Gardener uses CoreDNS as DNS server. Per default, it is installed as a deployment into the shoot cluster that is auto-scaled horizontally to cover for QPS-intensive applications. However, doing so does not seem to be enough to completely circumvent DNS bottlenecks such as:&lt;/p>
&lt;ul>
&lt;li>Cloud provider limits for DNS lookups.&lt;/li>
&lt;li>Unreliable UDP connections that forces a period of timeout in case packets are dropped.&lt;/li>
&lt;li>Unnecessary node hopping since CoreDNS is not deployed on all nodes, and as a result DNS queries end-up traversing multiple nodes before reaching the destination server.&lt;/li>
&lt;li>Inefficient load-balancing of services (e.g., round-robin might not be enough when using IPTables mode).&lt;/li>
&lt;li>Overload of the CoreDNS replicas as the maximum amount of replicas is fixed.&lt;/li>
&lt;li>and more &amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>As an alternative with extended configuration options, Gardener provides cluster-proportional autoscaling of CoreDNS. This guide focuses on the configuration of cluster-proportional autoscaling of CoreDNS and its advantages/disadvantages compared to the horizontal
autoscaling.
Please note that there is also the option to use a &lt;a href="https://gardener.cloud/docs/gardener/usage/node-local-dns/">node-local DNS cache&lt;/a>, which helps mitigate potential DNS bottlenecks (see &lt;a href="#trade-offs-in-conjunction-with-nodelocaldns">Trade-offs in conjunction with NodeLocalDNS&lt;/a> for considerations regarding using NodeLocalDNS together with one of the CoreDNS autoscaling approaches).&lt;/p>
&lt;h2 id="configuring-cluster-proportional-dns-autoscaling">Configuring cluster-proportional DNS Autoscaling&lt;/h2>
&lt;p>All that needs to be done to enable the usage of cluster-proportional autoscaling of CoreDNS is to set the corresponding option (&lt;code>spec.systemComponents.coreDNS.autoscaling.mode&lt;/code>) in the &lt;code>Shoot&lt;/code> resource to &lt;code>cluster-proportional&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> systemComponents:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> coreDNS:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> autoscaling:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mode: cluster-proportional
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To switch back to horizontal DNS autoscaling you can set the &lt;code>spec.systemComponents.coreDNS.autoscaling.mode&lt;/code> to &lt;code>horizontal&lt;/code> (or remove the &lt;code>coreDNS&lt;/code> section).&lt;/p>
&lt;p>Once the cluster-proportional autoscaling of CoreDNS has been enabled and the Shoot cluster has been reconciled afterwards, a ConfigMap called &lt;code>coredns-autoscaler&lt;/code> will be created in the &lt;code>kube-system&lt;/code> namespace with the default settings. The content will be similar to the following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>linear: &lt;span style="color:#a31515">&amp;#39;{&amp;#34;coresPerReplica&amp;#34;:256,&amp;#34;min&amp;#34;:2,&amp;#34;nodesPerReplica&amp;#34;:16}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It is possible to adapt the ConfigMap according to your needs in case the defaults do not work as desired. The number of CoreDNS replicas is calculated according to the following formula:&lt;/p>
&lt;pre tabindex="0">&lt;code>replicas = max( ceil( cores × 1 / coresPerReplica ) , ceil( nodes × 1 / nodesPerReplica ) )
&lt;/code>&lt;/pre>&lt;p>Depending on your needs, you can adjust &lt;code>coresPerReplica&lt;/code> or &lt;code>nodesPerReplica&lt;/code>, but it is also possible to override &lt;code>min&lt;/code> if required.&lt;/p>
&lt;h2 id="trade-offs-of-horizontal-and-cluster-proportional-dns-autoscaling">Trade-offs of horizontal and cluster-proportional DNS Autoscaling&lt;/h2>
&lt;p>The horizontal autoscaling of CoreDNS as implemented by Gardener is fully managed, i.e. you do not need to perform any configuration changes. It scales according to the CPU usage of CoreDNS replicas meaning that it will create new replicas if the existing ones are under heavy load. This approach scales between 2 and 5 instances, which is sufficient for most workloads. In case this is not enough, the cluster-proportional autoscaling approach can be used instead with its more flexible configuration options.&lt;/p>
&lt;p>The cluster-proportional autoscaling of CoreDNS as implemented by Gardener is fully managed, but allows more configuration options to adjust the default settings to your individual needs. It scales according to the cluster size, i.e. if your cluster grows in terms of cores/nodes so will the amount of CoreDNS replicas. However, it does not take the actual workload, e.g. CPU consumption, into account.&lt;/p>
&lt;p>Experience shows that the horizontal autoscaling of CoreDNS works for a variety of workloads. It does reach its limits if a cluster has a high amount of DNS requests, though. The cluster-proportional autoscaling approach allows to fine-tune the amount of CoreDNS replicas. It helps to scale in clusters of changing size. However, please keep in mind that you need to cater for the maximum amount of DNS requests as the replicas will not be adapted according to the workload, but only according to the cluster size (cores/nodes).&lt;/p>
&lt;h2 id="trade-offs-in-conjunction-with-nodelocaldns">Trade-offs in conjunction with NodeLocalDNS&lt;/h2>
&lt;p>Using a &lt;a href="https://gardener.cloud/docs/gardener/usage/node-local-dns/">node-local DNS cache&lt;/a> can mitigate a lot of the potential DNS related problems. It works fine with a DNS workload that can be handle through the cache and reduces the inter-node DNS communication. As &lt;a href="https://gardener.cloud/docs/gardener/usage/node-local-dns/">node-local DNS cache&lt;/a> reduces the amount of traffic being sent to the cluster&amp;rsquo;s CoreDNS replicas, it usually works fine with horizontally scaled CoreDNS. Nevertheless, it also works with CoreDNS scaled in a cluster-proportional approach. In this mode, though, it might make sense to adapt the default settings as the CoreDNS workload is likely significantly reduced.&lt;/p>
&lt;p>Overall, you can view the DNS options on a scale. Horizontally scaled DNS provides a small amount of DNS servers. Especially for bigger clusters, a cluster-proportional approach will yield more CoreDNS instances and hence may yield a more balanced DNS solution. By adapting the settings you can further increase the amount of CoreDNS replicas. On the other end of the spectrum, a &lt;a href="https://gardener.cloud/docs/gardener/usage/node-local-dns/">node-local DNS cache&lt;/a> provides DNS on every node and allows to reduce the amount of (backend) CoreDNS instances regardless if they are horizontally or cluster-proportionally scaled.&lt;/p></description></item><item><title>Docs: DNS Search Path Optimization</title><link>https://gardener.cloud/docs/gardener/usage/dns-search-path-optimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/dns-search-path-optimization/</guid><description>
&lt;h1 id="dns-search-path-optimization">DNS Search Path Optimization&lt;/h1>
&lt;h2 id="dns-search-path">DNS Search Path&lt;/h2>
&lt;p>Using fully qualified names has some downsides, e.g. it may become harder to move deployments from one landscape to the
next. It is far easier and simple to rely on short/local names, which may have different meaning depending on the context
they are used in.&lt;/p>
&lt;p>The DNS search path allows the usage of short/local names. It is an ordered list of DNS suffixes to append to short/local
names to create a fully qualified name.&lt;/p>
&lt;p>If a short/local name should be resolved each entry is appended to it one by one to check whether it can be resolved. The
process stops when either the name could be resolved or the DNS search path ends. As the last step after trying the search
path, the short/local name is attempted to be resolved on it own.&lt;/p>
&lt;h2 id="dns-option-ndots">DNS Option &lt;code>ndots&lt;/code>&lt;/h2>
&lt;p>As explained in the &lt;a href="#dns-search-path">section above&lt;/a>, the DNS search path is used for short/local names to create fully
qualified names. The DNS option &lt;code>ndots&lt;/code> specifies how many dots (&lt;code>.&lt;/code>) a name needs to have to be considered fully qualified.
For names with less than &lt;code>ndots&lt;/code> dots (&lt;code>.&lt;/code>), the &lt;a href="#dns-search-path">DNS search path&lt;/a> will be applied.&lt;/p>
&lt;h2 id="dns-search-path-ndots-and-kubernetes">DNS Search Path, &lt;code>ndots&lt;/code> and Kubernetes&lt;/h2>
&lt;p>Kubernetes tries to make it easy/convenient for developers to use name resolution. It provides several means to address a
service, most notably by its name directly, using the namespace as suffix, utilizing &lt;code>&amp;lt;namespace&amp;gt;.svc&lt;/code> as suffix or as a
fully qualified name as &lt;code>&amp;lt;service&amp;gt;.&amp;lt;namespace&amp;gt;.svc.cluster.local&lt;/code> (assuming &lt;code>cluster.local&lt;/code> to be the cluster domain).&lt;/p>
&lt;p>This is why the DNS search path is fairly long in Kubernetes, usually consisting of &lt;code>&amp;lt;namespace&amp;gt;.svc.cluster.local&lt;/code>,
&lt;code>svc.cluster.local&lt;/code>, &lt;code>cluster.local&lt;/code> and potentially some additional entries coming from the local network of the cluster.
For various reasons, the default &lt;code>ndots&lt;/code> value in the context of Kubernetes is with &lt;code>5&lt;/code> also fairly large. See
&lt;a href="https://github.com/kubernetes/kubernetes/issues/33554#issuecomment-266251056">this comment&lt;/a> for a more detailed description.&lt;/p>
&lt;h2 id="dns-search-pathndots-problem-in-kubernetes">DNS Search Path/&lt;code>ndots&lt;/code> Problem in Kubernetes&lt;/h2>
&lt;p>As the DNS search path is long and &lt;code>ndots&lt;/code> is large, a lot of DNS queries might traverse the DNS search path. This results
in an explosion of DNS requests.&lt;/p>
&lt;p>For example, consider the name resolution of the default kubernetes service &lt;code>kubernetes.default.svc.cluster.local&lt;/code>. As this
name has only four dots it is not considered a fully qualified name according to the default &lt;code>ndots=5&lt;/code> setting. Therefore,
the DNS search path is applied resulting in the following queries being created&lt;/p>
&lt;ul>
&lt;li>&lt;code>kubernetes.default.svc.cluster.local.some-namespace.svc.cluster.local&lt;/code>&lt;/li>
&lt;li>&lt;code>kubernetes.default.svc.cluster.local.svc.cluster.local&lt;/code>&lt;/li>
&lt;li>&lt;code>kubernetes.default.svc.cluster.local.cluster.local&lt;/code>&lt;/li>
&lt;li>&lt;code>kubernetes.default.svc.cluster.local.network-domain&lt;/code>&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>In IPv4/IPv6 dual stack systems, the amount of DNS requests may even double as each name is resolved for IPv4 and IPv6.&lt;/p>
&lt;h2 id="general-workaroundsmitigations">General Workarounds/Mitigations&lt;/h2>
&lt;p>Kubernetes provides the capability to set the DNS options for each pod (see
&lt;a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-dns-config">Pod DNS config&lt;/a> for details).
However, this has to be applied for every pod (doing name resolution) to resolve the problem. A mutating webhook may be
useful in this regard. Unfortunately, the DNS requirements may be different depending on the workload. Therefore, a general
solution may difficult to impossible.&lt;/p>
&lt;p>Another approach is to use always fully qualified names and append a dot (&lt;code>.&lt;/code>) to the name to prevent the name resolution
system from using the DNS search path. This might be somewhat counterintuitive as most developers are not used to the
trailing dot (&lt;code>.&lt;/code>). Furthermore, it makes moving to different landscapes more difficult/error-prone.&lt;/p>
&lt;h2 id="gardener-specific-workaroundsmitigations">Gardener specific Workarounds/Mitigations&lt;/h2>
&lt;p>Gardener allows users to &lt;a href="https://gardener.cloud/docs/gardener/usage/custom-dns-config/">customize their DNS configuration&lt;/a>. CoreDNS allows several approaches to deal with
the requests generated by the DNS search path. &lt;a href="https://coredns.io/plugins/cache/">Caching&lt;/a> is possible as well as
&lt;a href="https://coredns.io/plugins/rewrite/">query rewriting&lt;/a>. There are also several other &lt;a href="https://coredns.io/plugins/">plugins&lt;/a>
available, which may mitigate the situation.&lt;/p>
&lt;h2 id="gardener-dns-query-rewriting">Gardener DNS Query Rewriting&lt;/h2>
&lt;p>As explained &lt;a href="#dns-search-path-ndots-and-kubernetes">above&lt;/a>, the application of the DNS search path may lead to the undesired
creation of DNS requests. Especially with the default setting of &lt;code>ndots=5&lt;/code>, seemingly fully qualified names pointing to
services in the cluster may trigger the DNS search path application.&lt;/p>
&lt;p>Gardener allows to automatically rewrite some obviously incorrect DNS names, which stem from application of the DNS search
path, to the most likely desired name. The feature can be enabled by setting the Gardenlet feature gate &lt;code>CoreDNSQueryRewriting&lt;/code> to &lt;code>true&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>featureGates:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> CoreDNSQueryRewriting: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In case the feature is enabled in the Gardenlet it can be disabled per shoot cluster by setting the annotation
&lt;code>alpha.featuregates.shoot.gardener.cloud/core-dns-rewriting-disabled&lt;/code> to any value.&lt;/p>
&lt;p>This will automatically rewrite requests like &lt;code>service.namespace.svc.cluster.local.svc.cluster.local&lt;/code> to
&lt;code>service.namespace.svc.cluster.local&lt;/code>.&lt;/p>
&lt;p>In case applications also target services for name resolution, which are outside of the cluster and have less than &lt;code>ndots&lt;/code> dots,
it might be helpful to prevent search path application for them as well. One way to achieve it is by adding them to the
&lt;code>commonSuffixes&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> systemComponents:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> coreDNS:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rewriting:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> commonSuffixes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - gardener.cloud
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>DNS requests containing a common suffix and ending in &lt;code>.svc.cluster.local&lt;/code> are assumed to be incorrect application of the DNS
search path. Therefore, they are rewritten to everything ending in the common suffix. For example, &lt;code>www.gardener.cloud.svc.cluster.local&lt;/code>
would be rewritten to &lt;code>www.gardener.cloud&lt;/code>.&lt;/p>
&lt;p>Please note that the common suffixes should be long enough and include enough dots (&lt;code>.&lt;/code>) to prevent random overlap with
other DNS queries. For example, it would be a bad idea to simply put &lt;code>com&lt;/code> on the list of common suffixes as there may be
services/namespaces, which have &lt;code>com&lt;/code> as part of their name. The effect would be seemingly random DNS requests. Gardener
requires that common suffixes contain at least one dot (.) and adds a second dot at the beginning. For instance, a common
suffix of &lt;code>example.com&lt;/code> in the configuration would match &lt;code>*.example.com&lt;/code>.&lt;/p>
&lt;p>Since some clients verify the host in the response of a DNS query, the host must also be rewritten.
For that reason we can&amp;rsquo;t rewrite a query for &lt;code>service.dst-namespace.svc.cluster.local.src-namespace.svc.cluster.local&lt;/code> or
&lt;code>www.example.com.src-namespace.svc.cluster.local&lt;/code> as for an answer rewrite &lt;code>src-namespace&lt;/code> would not be known.&lt;/p></description></item><item><title>Docs: Docker Shim Removal</title><link>https://gardener.cloud/docs/gardener/usage/docker-shim-removal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/docker-shim-removal/</guid><description>
&lt;h1 id="kubernetes-dockershim-removal">Kubernetes dockershim removal&lt;/h1>
&lt;h2 id="whats-happening">What&amp;rsquo;s happening?&lt;/h2>
&lt;p>With Kubernetes v1.20 the built-in dockershim &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#dockershim-deprecation">was deprecated&lt;/a> and is scheduled to be &lt;a href="https://github.com/kubernetes/enhancements/issues/2221">removed with v1.24&lt;/a>.
Don&amp;rsquo;t Panic! The Kubernetes community has &lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">published a blogpost&lt;/a> and an &lt;a href="https://kubernetes.io/blog/2022/02/17/dockershim-faq/">FAQ&lt;/a> with more information.&lt;/p>
&lt;p>Gardener also needs to switch from using the built-in dockershim to &lt;code>containerd&lt;/code>.
Gardener will not change running Shoot clusters. But changes to the container runtime will be coupled to the K8s version selected by the Shoot:&lt;/p>
&lt;ul>
&lt;li>starting with K8s version 1.22 Shoots not explicitly selecting a container runtime will get &lt;code>containerd&lt;/code> instead of &lt;code>docker&lt;/code>. Shoots can still select &lt;code>docker&lt;/code> explicitly if needed.&lt;/li>
&lt;li>starting with K8s version 1.23 &lt;code>docker&lt;/code> can no longer be selected.&lt;/li>
&lt;/ul>
&lt;p>At this point in time, we have no plans to support other container runtimes, such as &lt;code>cri-o&lt;/code>.&lt;/p>
&lt;h2 id="what-should-i-do">What should I do?&lt;/h2>
&lt;p>As a gardener operator:&lt;/p>
&lt;ul>
&lt;li>add &lt;code>containerd&lt;/code> and &lt;code>docker&lt;/code> to &lt;code>.spec.machineImages[].versions[].cri.name&lt;/code> in your CloudProfile to allow users selecting a container runtime for their Shoots (see below). &lt;strong>Note:&lt;/strong> Please take a look at our detailed information regarding &lt;a href="#container-runtime-support-in-gardener-operating-system-extensions">container runtime support in Gardener Operating System Extensions&lt;/a>&lt;/li>
&lt;li>update your cloud provider extensions to avoid a node rollout when a Shoot is configured from &lt;code>cri: nil&lt;/code> to &lt;code>cri.name: docker&lt;/code>. &lt;strong>Note:&lt;/strong> Please take a look at our detailed information regarding &lt;a href="#stable-worker-node-hash-support-in-gardener-provider-extensions">stable Worker node hash support in Gardener Provider Extensions&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>As a shoot owner:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-deprecation-affects-you/#find-docker-dependencies">check if you have dependencies to the &lt;code>docker&lt;/code> container runtime&lt;/a>. &lt;strong>Note:&lt;/strong> This is not only about your actual workload, but also concerns ops tooling as well as logging, monitoring and metric agents installed on the nodes&lt;/li>
&lt;li>test with &lt;code>containerd&lt;/code>:
&lt;ul>
&lt;li>create a new Shoot or add a Worker Pool to an existing one&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#cri">set &lt;code>.spec.provider.workers[].cri.name: containerd&lt;/code>&lt;/a> for your Shoot&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>once testing is successful, switch to &lt;code>containerd&lt;/code> with your production workload. You don&amp;rsquo;t need to wait for kubernetes v1.22, &lt;code>containerd&lt;/code> is considered production ready as of today&lt;/li>
&lt;li>if you find dependencies to &lt;code>docker&lt;/code>, set &lt;code>.spec.provider.workers[].cri.name: docker&lt;/code> explicitly to avoid defaulting to &lt;code>containerd&lt;/code> once you update your Shoot to kubernetes v1.22&lt;/li>
&lt;/ul>
&lt;h2 id="timeline">Timeline&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>2021-08-04:&lt;/strong> Kubernetes v1.22 released. Shoots using this version get &lt;code>containerd&lt;/code> as default container runtime. Shoots can still select &lt;code>docker&lt;/code> explicitly if needed.&lt;/li>
&lt;li>&lt;strong>2021-12-07:&lt;/strong> Kubernetes v1.23 released. Shoots using this version can no longer select &lt;code>docker&lt;/code> as container runtime.&lt;/li>
&lt;li>&lt;strong>2022-06-28:&lt;/strong> Kubernetes v1.21 goes out of maintenance. This is the last version not affected by these changes. Make sure you have tested thoroughly and set the correct configuration for your Shoots!&lt;/li>
&lt;li>&lt;strong>2022-10-28:&lt;/strong> Kubernetes v1.22 goes out of maintenance. This is the last version that you can use with &lt;code>docker&lt;/code> as container runtime. Make sure you have removed any dependencies to &lt;code>docker&lt;/code> as container runtime!&lt;/li>
&lt;/ul>
&lt;p>See &lt;a href="https://kubernetes.io/releases/">the official kubernetes documentation&lt;/a> for the exact dates for all releases.&lt;/p>
&lt;h2 id="container-runtime-support-in-gardener-operating-system-extensions">Container Runtime support in Gardener Operating System Extensions&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Operating System&lt;/th>
&lt;th>docker support&lt;/th>
&lt;th>containerd support&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>GardenLinux&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>&amp;gt;= v0.3.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ubuntu&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>&amp;gt;= v1.4.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SuSE CHost&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>&amp;gt;= v1.14.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CoreOS/FlatCar&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>&amp;gt;= v1.8.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Note&lt;/strong>: If you&amp;rsquo;re using a different Operating System Extension, start evaluating now if it provides support for &lt;code>containerd&lt;/code>. Please refer to &lt;a href="https://gardener.cloud/docs/gardener/extensions/operatingsystemconfig/#cri-support">our documentation of the &lt;code>operatingsystemconfig&lt;/code> contract&lt;/a> to understand how to support &lt;code>containerd&lt;/code> for an Operating System Extension.&lt;/p>
&lt;h2 id="stable-worker-node-hash-support-in-gardener-provider-extensions">Stable Worker node hash support in Gardener Provider Extensions&lt;/h2>
&lt;p>Upgrade to these versions to avoid a node rollout when a Shoot is configured from &lt;code>cri: nil&lt;/code> to &lt;code>cri.name: docker&lt;/code>.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Provider Extension&lt;/th>
&lt;th>Stable worker hash support&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Alicloud&lt;/td>
&lt;td>&amp;gt;= 1.26.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>AWS&lt;/td>
&lt;td>&amp;gt;= 1.27.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Azure&lt;/td>
&lt;td>&amp;gt;= 1.21.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GCP&lt;/td>
&lt;td>&amp;gt;= 1.18.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenStack&lt;/td>
&lt;td>&amp;gt;= 1.21.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>vSphere&lt;/td>
&lt;td>&amp;gt;= 0.11.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Note&lt;/strong>: If you&amp;rsquo;re using a different Provider Extension, start evaluating now if it keeps the worker hash stable when switching from &lt;code>.spec.provider.workers[].cri: nil&lt;/code> to &lt;code>.spec.provider.workers[].cri.name: docker&lt;/code>. This doesn&amp;rsquo;t impact functional correctness, however, a node rollout will be triggered when users decide to configure &lt;code>docker&lt;/code> for their shoots.&lt;/p></description></item><item><title>Docs: ExposureClasses</title><link>https://gardener.cloud/docs/gardener/usage/exposureclasses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/exposureclasses/</guid><description>
&lt;h1 id="exposureclasses">ExposureClasses&lt;/h1>
&lt;p>The Gardener API server provides a cluster-scoped &lt;code>ExposureClass&lt;/code> resource.
This resource is used to allow exposing the control plane of a Shoot cluster in various network environments like restricted corporate networks, DMZ etc.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>The &lt;code>ExposureClass&lt;/code> resource is based on the concept for the &lt;code>RuntimeClass&lt;/code> resource in Kubernetes.&lt;/p>
&lt;p>A &lt;code>RuntimeClass&lt;/code> abstracts the installation of a certain container runtime (e.g. gVisor, Kata Containers) on all nodes or a subset of the nodes in a Kubernetes cluster.
See &lt;a href="https://kubernetes.io/docs/concepts/containers/runtime-class/">here&lt;/a>.&lt;/p>
&lt;p>In contrast, an &lt;code>ExposureClass&lt;/code> abstracts the ability to expose a Shoot clusters control plane in certain network environments (e.g. corporate networks, DMZ, internet) on all Seeds or a subset of the Seeds.&lt;/p>
&lt;p>Example: &lt;code>RuntimeClass&lt;/code> and &lt;code>ExposureClass&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: node.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: RuntimeClass
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gvisor
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>handler: gvisorconfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># scheduling:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># nodeSelector:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># env: prod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ExposureClass
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: internet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>handler: internet-config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># scheduling:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># seedSelector:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># matchLabels:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># network/env: internet&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Similar to &lt;code>RuntimeClasses&lt;/code>, &lt;code>ExposureClasses&lt;/code> also define a &lt;code>.handler&lt;/code> field reflecting the name reference for the corresponding CRI configuration of the &lt;code>RuntimeClass&lt;/code> and the control plane exposure configuration for the &lt;code>ExposureClass&lt;/code>.&lt;/p>
&lt;p>The CRI handler for &lt;code>RuntimeClasses&lt;/code> is usually installed by an administrator (e.g. via a &lt;code>DaemonSet&lt;/code> which installs the corresponding container runtime on the nodes).
The control plane exposure configuration for &lt;code>ExposureClasses&lt;/code> will be also provided by an administrator.
This exposure configuration is part of the Gardenlet configuration as this component is responsible to configure the control plane accordingly.
See &lt;a href="#Gardenlet-Configuration-ExposureClass-handlers">here&lt;/a>.&lt;/p>
&lt;p>The &lt;code>RuntimeClass&lt;/code> also supports the selection of a node subset (which have the respective controller runtime binaries installed) for pod scheduling via its &lt;code>.scheduling&lt;/code> section.
The &lt;code>ExposureClass&lt;/code> also supports the selection of a subset of available Seed clusters whose Gardenlet is capable of applying the exposure configuration for the Shoot control plane accordingly via its &lt;code>.scheduling&lt;/code> section.&lt;/p>
&lt;h2 id="usage-by-a-shoot">Usage by a &lt;code>Shoot&lt;/code>&lt;/h2>
&lt;p>A &lt;code>Shoot&lt;/code> can reference an &lt;code>ExposureClass&lt;/code> via the &lt;code>.spec.exposureClassName&lt;/code> field.&lt;/p>
&lt;p>⚠️ When creating a &lt;code>Shoot&lt;/code> resource, the Gardener scheduler will try to assign the &lt;code>Shoot&lt;/code> to a &lt;code>Seed&lt;/code> which will host its control plane.
The scheduling behaviour can be influenced via the &lt;code>.spec.seedSelectors&lt;/code> and/or &lt;code>.spec.tolerations&lt;/code> fields in the &lt;code>Shoot&lt;/code>.
&lt;code>ExposureClass&lt;/code>es can contain also scheduling instructions.
If a &lt;code>Shoot&lt;/code> is referencing an &lt;code>ExposureClass&lt;/code> then the scheduling instructions of both will be merged into the &lt;code>Shoot&lt;/code>.
Those unions of scheduling instructions might lead to a selection of a &lt;code>Seed&lt;/code> which is not able to deal with the &lt;code>handler&lt;/code> of the &lt;code>ExposureClass&lt;/code> and the &lt;code>Shoot&lt;/code> creation might end up in an error.
In such case, the &lt;code>Shoot&lt;/code> scheduling instructions should be revisited to check that they are not interfere with the ones from the &lt;code>ExposureClass&lt;/code>.
If this is not feasible then the combination with the &lt;code>ExposureClass&lt;/code> is might not possible and you need to contact your Gardener administrator.&lt;/p>
&lt;details>
&lt;summary>Example: Shoot and ExposureClass scheduling instructions merge flow&lt;/summary>
&lt;ol>
&lt;li>Assuming there is the following &lt;code>Shoot&lt;/code> which is referencing the &lt;code>ExposureClass&lt;/code> below:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: abc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden-dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> exposureClassName: abc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> seedSelectors:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> env: prod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ExposureClass
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: abc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>handler: abc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>scheduling:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> seedSelector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> network: internal
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>Both &lt;code>seedSelectors&lt;/code> would be merged into the &lt;code>Shoot&lt;/code>. The result would be the following:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: abc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden-dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> exposureClassName: abc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> seedSelectors:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> env: prod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> network: internal
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>Now the Gardener Scheduler would try to find a &lt;code>Seed&lt;/code> with those labels.&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>If there are &lt;strong>no&lt;/strong> Seeds with matching labels for the seed selector then the &lt;code>Shoot&lt;/code> will be unschedulable&lt;/li>
&lt;li>If there are Seeds with matching labels for the seed selector then the Shoot will be assigned to the best candidate after the scheduling strategy is applied, see &lt;a href="https://gardener.cloud/docs/gardener/concepts/scheduler/#algorithm-overview">here&lt;/a>
&lt;ul>
&lt;li>If the &lt;code>Seed&lt;/code> is &lt;strong>not&lt;/strong> able to serve the &lt;code>ExposureClass&lt;/code> handler &lt;code>abc&lt;/code> then the Shoot will end up in error state&lt;/li>
&lt;li>If the &lt;code>Seed&lt;/code> is able to serve the &lt;code>ExposureClass&lt;/code> handler &lt;code>abc&lt;/code> then the &lt;code>Shoot&lt;/code> will be created&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/details>
&lt;h2 id="gardenlet-configuration-exposureclass-handlers">Gardenlet Configuration &lt;code>ExposureClass&lt;/code> handlers&lt;/h2>
&lt;p>The Gardenlet is responsible to realize the control plane exposure strategy defined in the referenced &lt;code>ExposureClass&lt;/code> of a &lt;code>Shoot&lt;/code>.&lt;/p>
&lt;p>Therefore, the &lt;code>GardenletConfiguration&lt;/code> can contain an &lt;code>.exposureClassHandlers&lt;/code> list with the respective configuration.&lt;/p>
&lt;p>Example of the &lt;code>GardenletConfiguration&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>exposureClassHandlers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: internet-config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> loadBalancerService:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> loadbalancer/network: internet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: internal-config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> loadBalancerService:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> loadbalancer/network: internal
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sni:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ingress:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: ingress-internal
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> network: internal
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Each Gardenlet can define how the handler of a certain &lt;code>ExposureClass&lt;/code> needs to be implemented for the Seed(s) where it is responsible for.&lt;/p>
&lt;p>The &lt;code>.name&lt;/code> is the name of the handler config and it must match to the &lt;code>.handler&lt;/code> in the &lt;code>ExposureClass&lt;/code>.&lt;/p>
&lt;p>All control planes on a &lt;code>Seed&lt;/code> are exposed via a load balancer.
Either a dedicated one or a central shared one.
The load balancer service needs to be configured in a way that it is reachable from the target network environment.
Therefore, the configuration of load balancer service need to be specified which can be done via the &lt;code>.loadBalancerService&lt;/code> section.
The common way to influence load balancer service behaviour is via annotations where the respective cloud-controller-manager will react on and configure the infrastructure load balancer accordingly.&lt;/p>
&lt;p>In case the Gardenlet runs with activated &lt;code>APIServerSNI&lt;/code> feature flag (default), the control planes on a &lt;code>Seed&lt;/code> will be exposed via a central load balancer and with Envoy via TLS SNI passthrough proxy.
In this case, the Gardenlet will install a dedicated ingress gateway (Envoy + load balancer + respective configuration) for each handler on the &lt;code>Seed&lt;/code>.
The configuration of the ingress gateways can be controlled via the &lt;code>.sni&lt;/code> section in the same way like for the default ingress gateways.&lt;/p></description></item><item><title>Docs: Ipv6</title><link>https://gardener.cloud/docs/gardener/usage/ipv6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/ipv6/</guid><description>
&lt;h1 id="ipv6-in-gardener-clusters">IPv6 in Gardener Clusters&lt;/h1>
&lt;blockquote>
&lt;p>🚧 IPv6 networking is currently under development.&lt;/p>
&lt;/blockquote>
&lt;h2 id="ipv6-single-stack-networking">IPv6 Single-Stack Networking&lt;/h2>
&lt;p>&lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/21-ipv6-singlestack-local.md">GEP-21&lt;/a> proposes IPv6 Single-Stack Support in the local Gardener environment.
This documentation will be enhanced while implementing GEP-21, see &lt;a href="https://github.com/gardener/gardener/issues/7051">gardener/gardener#7051&lt;/a>.&lt;/p>
&lt;p>To use IPv6 single-stack networking, the &lt;a href="https://gardener.cloud/docs/gardener/deployment/feature_gates/">feature gate&lt;/a> &lt;code>IPv6SingleStack&lt;/code> must be enabled on gardener-apiserver.&lt;/p>
&lt;h2 id="development-setup">Development Setup&lt;/h2>
&lt;p>Developing or testing IPv6-related features requires a Linux machine (docker only supports IPv6 on Linux) and native IPv6 connectivity to the internet.
If you&amp;rsquo;re on a different OS or don&amp;rsquo;t have IPv6 connectivity in your office environment or via your home ISP, make sure to check out &lt;a href="https://github.com/gardener-community/dev-box-gcp">gardener-community/dev-box-gcp&lt;/a>, which allows you to circumvent these limitations.&lt;/p>
&lt;h2 id="container-images">Container Images&lt;/h2>
&lt;p>If you plan on using custom images, make sure your registry supports IPv6 access.
The &lt;code>docker.io&lt;/code> registry doesn&amp;rsquo;t support pulling images over IPv6 (see &lt;a href="https://www.docker.com/blog/beta-ipv6-support-on-docker-hub-registry/">Beta IPv6 Support on Docker Hub Registry&lt;/a>).
Use the &lt;a href="https://cloud.google.com/container-registry/docs/pulling-cached-images">Google Mirror&lt;/a> of Docker Hub instead which supports dual-stack network access.&lt;/p></description></item><item><title>Docs: Istio</title><link>https://gardener.cloud/docs/gardener/usage/istio/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/istio/</guid><description>
&lt;h1 id="istio">Istio&lt;/h1>
&lt;p>&lt;a href="https://istio.io">Istio&lt;/a> offers a service mesh implementation with focus on several important features - traffic, observability, security and policy.&lt;/p>
&lt;h2 id="gardener-managedistio-feature-gate">Gardener &lt;code>ManagedIstio&lt;/code> feature gate&lt;/h2>
&lt;p>When enabled in gardenlet the &lt;code>ManagedIstio&lt;/code> feature gate can be used to deploy a Gardener-tailored Istio installation in Seed clusters. It&amp;rsquo;s main usage is to enable features such as &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md">Shoot API server SNI&lt;/a>. This feature should not be enabled on a Seed cluster where Istio is already deployed.&lt;/p>
&lt;p>However, this feature gate is deprecated, turned on by default and will be removed in a future version of Gardener.
This means that Gardener will unconditionally deploy Istio with its desired configuration to seed clusters.
Consequently, existing/bring-your-own Istio deployments will no longer be supported.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;ul>
&lt;li>Third-party JWT is used, therefore each Seed cluster where this feature is enabled must have &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection">Service Account Token Volume Projection&lt;/a> enabled.&lt;/li>
&lt;li>Kubernetes 1.16+&lt;/li>
&lt;/ul>
&lt;h2 id="differences-with-istios-default-profile">Differences with Istio&amp;rsquo;s default profile&lt;/h2>
&lt;p>The &lt;a href="https://istio.io/docs/setup/additional-setup/config-profiles/">default profile&lt;/a> which is recommended for production deployment, is not suitable for the Gardener use case as it offers more functionality than desired. The current installation goes through heavy refactorings due to the &lt;code>IstioOperator&lt;/code> and the mixture of Helm values + Kubernetes API specification makes configuring and fine-tuning it very hard. A more simplistic deployment is used by Gardener. The differences are the following:&lt;/p>
&lt;ul>
&lt;li>Telemetry is not deployed.&lt;/li>
&lt;li>&lt;code>istiod&lt;/code> is deployed.&lt;/li>
&lt;li>&lt;code>istio-ingress-gateway&lt;/code> is deployed in a separate &lt;code>istio-ingress&lt;/code> namespace.&lt;/li>
&lt;li>&lt;code>istio-egress-gateway&lt;/code> is not deployed.&lt;/li>
&lt;li>None of the Istio addons are deployed.&lt;/li>
&lt;li>Mixer (deprecated) is not deployed&lt;/li>
&lt;li>Mixer CDRs are not deployed.&lt;/li>
&lt;li>Kubernetes &lt;code>Service&lt;/code>, Istio&amp;rsquo;s &lt;code>VirtualService&lt;/code> and &lt;code>ServiceEntry&lt;/code> are &lt;strong>NOT&lt;/strong> advertised in the service mesh. This means that if a &lt;code>Service&lt;/code> needs to be accessed directly from the Istio Ingress Gateway, it should have &lt;code>networking.istio.io/exportTo: &amp;quot;*&amp;quot;&lt;/code> annotation. &lt;code>VirtualService&lt;/code> and &lt;code>ServiceEntry&lt;/code> must have &lt;code>.spec.exportTo: [&amp;quot;*&amp;quot;]&lt;/code> set on them respectively.&lt;/li>
&lt;li>Istio injector is not enabled.&lt;/li>
&lt;li>mTLS is enabled by default.&lt;/li>
&lt;/ul>
&lt;h2 id="handling-multiple-availability-zones-with-istio">Handling multiple availability zones with Istio&lt;/h2>
&lt;p>For various reasons, e.g. improved resiliency to certain failures, it may be beneficial to use multiple availability zones in a seed cluster. While availability zones have advantages in being able to cover some failure domains, they also come with some additional challenges. Most notably, the latency across availability zone boundaries is higher than within an availability zone. Furthermore, there might be additional cost implied by network traffic crossing an availability zone boundary. Therefore, it may be useful to try to keep traffic within an availability zone if possible. The istio deployment as part of Gardener has been adapted to allow this.&lt;/p>
&lt;p>A seed cluster spanning multiple availability zones may be used for &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_high_availability/">highly-available shoot control planes&lt;/a>. Those control planes may use a single or multiple availability zones. In addition to that, ordinary none-highly-available shoot control planes may be scheduled to such a seed cluster as well. The result is that the seed cluster may have control planes spanning multiple availability zones and control planes that are pinned to exactly one availability zone. These two types need to be handled differently when trying to prevent unnecessary cross-zonal traffic.&lt;/p>
&lt;p>The goal is achieved by using multiple istio ingress gateways. The default istio ingress gateway spans all availability zones. It is used for multi-zonal shoot control planes. For each availability zone, there is an additional istio ingress gateway, which is utilized only for single-zone shoot control planes pinned to this availability zone. This is illustrated in the following diagram.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/multi-zonal-istio_bf47f6.png" alt="Multi Availability Zone Handling in Istio">&lt;/p>
&lt;p>Please note that operators may need to perform additional tuning to prevent cross-zonal traffic completely. The &lt;a href="https://gardener.cloud/docs/gardener/usage/seed_settings/#load-balancer-services">loadbalancer settings in the seed specification&lt;/a> offer various options, e.g. by setting the external traffic policy to &lt;code>local&lt;/code> or using infrastructure specific loadbalancer annotations.&lt;/p>
&lt;p>Furthermore, note that this approach is also taken in case &lt;a href="https://gardener.cloud/docs/gardener/usage/exposureclasses/">&lt;code>ExposureClass&lt;/code>es&lt;/a> are used. For each exposure class, additional zonal istio ingress gateways may be deployed to cover for single-zone shoot control planes using the exposure class.&lt;/p></description></item><item><title>Docs: Logging</title><link>https://gardener.cloud/docs/gardener/usage/logging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/logging/</guid><description>
&lt;h1 id="logging-stack">Logging stack&lt;/h1>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;p>Kubernetes uses the underlying container runtime logging, which does not persist logs for stopped and destroyed containers. This makes it difficult to investigate issues in the very common case of not running containers. Gardener provides a solution to this problem for the managed cluster components, by introducing its own logging stack.&lt;/p>
&lt;h3 id="components">Components:&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/logging-architecture_c8dc32.png" alt="">&lt;/p>
&lt;ul>
&lt;li>A Fluent-bit daemonset which works like a log collector and custom Golang plugin which spreads log messages to their Loki instances&lt;/li>
&lt;li>One Loki Statefulset in the &lt;code>garden&lt;/code> namespace which contains logs for the seed cluster and one per shoot namespace which contains logs for shoot&amp;rsquo;s controlplane.&lt;/li>
&lt;li>One Grafana Deployment in &lt;code>garden&lt;/code> namespace and two Deployments per shoot namespace (one exposed to the end users and one for the operators). Grafana is the UI component used in the logging stack.&lt;/li>
&lt;/ul>
&lt;h3 id="container-logs-rotation-and-retention">Container Logs rotation and retention&lt;/h3>
&lt;p>Container &lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/#log-rotation">log rotation&lt;/a> in Kubernetes describes a subtile but important implementation detail depending on the type of the used high-level container runtime. When the used container runtime is not CRI compliant (such as &lt;code>dockershim&lt;/code>) then the &lt;code>kubelet&lt;/code> does not provide any rotation or retention implementations, hence leaving those aspects to the downstream components. When the used container runtime is CRI compliant (such as &lt;code>containerd&lt;/code>) then the &lt;code>kubelet&lt;/code> provides the necessary implementation with two configuration options:&lt;/p>
&lt;ul>
&lt;li>&lt;code>ContainerLogMaxSize&lt;/code> for rotation&lt;/li>
&lt;li>&lt;code>ContainerLogMaxFiles&lt;/code> for retention.&lt;/li>
&lt;/ul>
&lt;h4 id="docker-container-runtime">Docker container runtime&lt;/h4>
&lt;p>In this case, the log rotation and retention is implemented by a &lt;code>logrotate&lt;/code> service provisioned by Gardener which rotates logs once &lt;code>100M&lt;/code> size is reached. Logs are compressed on daily basis and retained for a maximum period of &lt;code>14d&lt;/code>.&lt;/p>
&lt;h4 id="containerd-runtime">ContainerD runtime&lt;/h4>
&lt;p>In this case, it is possible to configure the &lt;code>containerLogMaxSize&lt;/code> and &lt;code>containerLogMaxFiles&lt;/code> fields in the Shoot specification. Both fields are optional and if nothing is specified then the &lt;code>kubelet&lt;/code> rotates on the same size &lt;code>100M&lt;/code> as in the &lt;code>docker&lt;/code> container runtime. Those fields are part of provider&amp;rsquo;s workers definition. Here is an example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> workers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - cri:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: containerd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubelet:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># accepted values are of resource.Quantity&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containerLogMaxSize: 150Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containerLogMaxFiles: 10
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The values of the &lt;code>containerLogMaxSize&lt;/code> and &lt;code>containerLogMaxFiles&lt;/code> fields need to be considered with care since container log files claim disk space from the host. On the opposite side, log rotations on too small sizes may result in frequent rotations which can be missed by other components (log shippers) observing these rotations.&lt;/p>
&lt;p>In the majority of the cases, the defaults shall do just. Custom configuration might be of use under rare conditions.&lt;/p>
&lt;h3 id="extension-of-the-logging-stack">Extension of the logging stack&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/shoot-node-logging-architecture_23c018.png" alt="">
The logging stack is extended to scrape logs from the systemd services of each shoots&amp;rsquo; nodes and from all Gardener components in the shoot &lt;code>kube-system&lt;/code> namespace. These logs are exposed only to the Gardener operators.&lt;/p>
&lt;p>Also, in the shoot control plane an &lt;code>event-logger&lt;/code> pod is deployed which scrapes events from the shoot &lt;code>kube-system&lt;/code> namespace and shoot &lt;code>control-plane&lt;/code> namespace in the seed. The &lt;code>event-logger&lt;/code> logs the events to the standard output. Then the &lt;code>fluent-bit&lt;/code> gets these events as container logs and sends them to the Loki in the shoot control plane (similar to how it works for any other control plane component).&lt;/p>
&lt;h3 id="how-to-access-the-logs">How to access the logs&lt;/h3>
&lt;p>The logs are accessible via Grafana. To access them:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Authenticate via basic auth to gain access to Grafana.
The Grafana URL can be found in the &lt;code>Logging and Monitoring&lt;/code> section of a cluster in the Gardener Dashboard alongside the credentials.
The secret containing the credentials is stored in the project namespace following the naming pattern &lt;code>&amp;lt;shoot-name&amp;gt;.monitoring&lt;/code>.
For Gardener operators, the credentials are also stored in the control-plane (&lt;code>shoot--&amp;lt;project-name&amp;gt;--&amp;lt;shoot-name&amp;gt;&lt;/code>) namespace in the &lt;code>observability-ingress-users-&amp;lt;hash&amp;gt;&lt;/code> secret in the seed.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Grafana contains several dashboards that aim to facilitate the work of operators and users.
From the &lt;code>Explore&lt;/code> tab, users and operators have unlimited abilities to extract and manipulate logs.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>&lt;em>&lt;strong>NOTE:&lt;/strong>&lt;/em> Gardener Operators are people part of the Gardener team with operator permissions, not operators of the end-user cluster!&lt;/p>
&lt;/blockquote>
&lt;h4 id="how-to-use-explore-tab">How to use &lt;code>Explore&lt;/code> tab&lt;/h4>
&lt;p>If you click on the &lt;code>Log browser &amp;gt;&lt;/code> button you will see all of the available labels.
Clicking on the label you can see all of its available values for the given period of time you have specified.
If you are searching for logs for the past one hour do not expect to see labels or values for which there were no logs for that period of time.
By clicking on a value, Grafana automatically eliminates all other label and/or values with which no valid log stream can be made.
After choosing the right labels and their values, click on &lt;code>Show logs&lt;/code> button.
This will build &lt;code>Log query&lt;/code> and execute it.
This approach is convenient when you don&amp;rsquo;t know the labels names or they values.
&lt;img src="https://gardener.cloud/__resources/explore-button-usage_0dfdca.png" alt="">&lt;/p>
&lt;p>Once you felt comfortable, you can start to use the &lt;a href="https://grafana.com/docs/loki/latest/logql/log_queries/">LogQL&lt;/a> language to search for logs.
Next to the &lt;code>Log browser &amp;gt;&lt;/code> button is the place where you can type log queries.&lt;/p>
&lt;p>Examples:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>If you want to get logs for &lt;code>calico-node-&amp;lt;hash&amp;gt;&lt;/code> pod in the cluster &lt;code>kube-system&lt;/code>.
The name of the node on which &lt;code>calico-node&lt;/code> was running is known but not the hash suffix of the &lt;code>calico-node&lt;/code> pod.
Also we want to search for errors in the logs.&lt;/p>
&lt;p>&lt;code>{pod_name=~&amp;quot;calico-node-.+&amp;quot;, nodename=&amp;quot;ip-10-222-31-182.eu-central-1.compute.internal&amp;quot;} |~ &amp;quot;error&amp;quot;&lt;/code>&lt;/p>
&lt;p>Here, you will get as much help as possible from the Grafana by giving you suggestions and auto-completion.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If you want to get the logs from &lt;code>kubelet&lt;/code> systemd service of a given node and search for a pod name in the logs.&lt;/p>
&lt;p>&lt;code>{unit=&amp;quot;kubelet.service&amp;quot;, nodename=&amp;quot;ip-10-222-31-182.eu-central-1.compute.internal&amp;quot;} |~ &amp;quot;pod name&amp;quot;&lt;/code>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>&lt;em>&lt;strong>NOTE:&lt;/strong>&lt;/em> Under &lt;code>unit&lt;/code> label there is only the &lt;code>docker&lt;/code>, &lt;code>containerd&lt;/code>, &lt;code>kubelet&lt;/code> and &lt;code>kernel&lt;/code> logs.&lt;/p>
&lt;/blockquote>
&lt;ol start="3">
&lt;li>
&lt;p>If you want to get the logs from &lt;code>cloud-config-downloader&lt;/code> systemd service of a given node and search for a string in the logs.&lt;/p>
&lt;p>&lt;code>{job=&amp;quot;systemd-combine-journal&amp;quot;,nodename=&amp;quot;ip-10-222-31-182.eu-central-1.compute.internal&amp;quot;} | unpack | unit=&amp;quot;cloud-config-downloader.service&amp;quot; |~ &amp;quot;last execution was&amp;quot;&lt;/code>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>&lt;em>&lt;strong>NOTE:&lt;/strong>&lt;/em> &lt;code>{job=&amp;quot;systemd-combine-journal&amp;quot;,nodename=&amp;quot;&amp;lt;node name&amp;gt;&amp;quot;}&lt;/code> stream &lt;a href="https://grafana.com/docs/loki/latest/clients/promtail/stages/pack/">pack&lt;/a> all logs from systemd services except &lt;code>docker&lt;/code>, &lt;code>containerd&lt;/code>, &lt;code>kubelet&lt;/code> and &lt;code>kernel&lt;/code>. To filter those log by unit you have to &lt;a href="https://grafana.com/docs/loki/latest/logql/log_queries/#unpack">unpack&lt;/a> them first.&lt;/p>
&lt;/blockquote>
&lt;ol start="4">
&lt;li>Retrieving events:&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;p>If you want to get the events from the shoot &lt;code>kube-system&lt;/code> namespace generated by &lt;code>kubelet&lt;/code> and related to the &lt;code>node-problem-detector&lt;/code>:&lt;/p>
&lt;p>&lt;code>{job=&amp;quot;event-logging&amp;quot;} | unpack | origin_extracted=&amp;quot;shoot&amp;quot;,source=&amp;quot;kubelet&amp;quot;,object=~&amp;quot;.*node-problem-detector.*&amp;quot;&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If you want to get the events generated by MCM in the shoot control plane in the seed:&lt;/p>
&lt;p>&lt;code>{job=&amp;quot;event-logging&amp;quot;} | unpack | origin_extracted=&amp;quot;seed&amp;quot;,source=~&amp;quot;.*machine-controller-manager.*&amp;quot;&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>&lt;em>&lt;strong>NOTE:&lt;/strong>&lt;/em> In order to group events by origin one has to specify &lt;code>origin_extracted&lt;/code> because &lt;code>origin&lt;/code> label is reserved for all of the logs from the seed and the &lt;code>event-logger&lt;/code> resides in the seed, so all of its logs are coming as they are only from the seed. The actual origin is embedded in the unpacked event. When unpacked the embedded &lt;code>origin&lt;/code> becomes &lt;code>origin_extracted&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;h3 id="expose-logs-for-component-to-user-grafana">Expose logs for component to User Grafana&lt;/h3>
&lt;p>Exposing logs for a new component to the User&amp;rsquo;s Grafana is described &lt;a href="https://gardener.cloud/docs/gardener/extensions/logging-and-monitoring/#how-to-expose-logs-to-the-users">here&lt;/a>&lt;/p>
&lt;h3 id="configuration">Configuration&lt;/h3>
&lt;h4 id="fluent-bit">Fluent-bit&lt;/h4>
&lt;p>The Fluent-bit configurations can be found on &lt;code>charts/seed-bootstrap/charts/fluent-bit/templates/fluent-bit-configmap.yaml&lt;/code>
There are five different specifications:&lt;/p>
&lt;ul>
&lt;li>SERVICE: Defines the location of the server specifications&lt;/li>
&lt;li>INPUT: Defines the location of the input stream of the logs&lt;/li>
&lt;li>OUTPUT: Defines the location of the output source (Loki for example)&lt;/li>
&lt;li>FILTER: Defines filters which match specific keys&lt;/li>
&lt;li>PARSER: Defines parsers which are used by the filters&lt;/li>
&lt;/ul>
&lt;h4 id="loki">Loki&lt;/h4>
&lt;p>The Loki configurations can be found on &lt;code>charts/seed-bootstrap/charts/loki/templates/loki-configmap.yaml&lt;/code>&lt;/p>
&lt;p>The main specifications there are:&lt;/p>
&lt;ul>
&lt;li>Index configuration: Currently is used the following one:&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code> schema_config:
configs:
- from: 2018-04-15
store: boltdb
object_store: filesystem
schema: v11
index:
prefix: index_
period: 24h
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>&lt;code>from&lt;/code>: is the date from which logs collection is started. Using a date in the past is okay.&lt;/li>
&lt;li>&lt;code>store&lt;/code>: The DB used for storing the index.&lt;/li>
&lt;li>&lt;code>object_store&lt;/code>: Where the data is stored&lt;/li>
&lt;li>&lt;code>schema&lt;/code>: Schema version which should be used (v11 is currently recommended)&lt;/li>
&lt;li>&lt;code>index.prefix&lt;/code>: The prefix for the index.&lt;/li>
&lt;li>&lt;code>index.period&lt;/code>: The period for updating the indices&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Adding of new index happens with new config block definition. &lt;code>from&lt;/code> field should start from the current day + previous &lt;code>index.period&lt;/code> and should not overlap with the current index. The &lt;code>prefix&lt;/code> also should be different&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code> schema_config:
configs:
- from: 2018-04-15
store: boltdb
object_store: filesystem
schema: v11
index:
prefix: index_
period: 24h
- from: 2020-06-18
store: boltdb
object_store: filesystem
schema: v11
index:
prefix: index_new_
period: 24h
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>chunk_store_config Configuration&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code> chunk_store_config:
max_look_back_period: 336h
&lt;/code>&lt;/pre>&lt;p>&lt;strong>&lt;code>chunk_store_config.max_look_back_period&lt;/code> should be the same as the &lt;code>retention_period&lt;/code>&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>table_manager Configuration&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code> table_manager:
retention_deletes_enabled: true
retention_period: 336h
&lt;/code>&lt;/pre>&lt;p>&lt;code>table_manager.retention_period&lt;/code> is the living time for each log message. Loki will keep messages for sure for (&lt;code>table_manager.retention_period&lt;/code> - &lt;code>index.period&lt;/code>) time due to specification in the Loki implementation.&lt;/p>
&lt;h4 id="grafana">Grafana&lt;/h4>
&lt;p>The Grafana configurations can be found on &lt;code>charts/seed-bootstrap/charts/templates/grafana/grafana-datasources-configmap.yaml&lt;/code> and
&lt;code>charts/seed-monitoring/charts/grafana/tempates/grafana-datasources-configmap.yaml&lt;/code>&lt;/p>
&lt;p>This is the Loki configuration that Grafana uses:&lt;/p>
&lt;pre tabindex="0">&lt;code> - name: loki
type: loki
access: proxy
url: http://loki.{{ .Release.Namespace }}.svc:3100
jsonData:
maxLines: 5000
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>&lt;code>name&lt;/code>: is the name of the datasource&lt;/li>
&lt;li>&lt;code>type&lt;/code>: is the type of the datasource&lt;/li>
&lt;li>&lt;code>access&lt;/code>: should be set to proxy&lt;/li>
&lt;li>&lt;code>url&lt;/code>: Loki&amp;rsquo;s url&lt;/li>
&lt;li>&lt;code>svc&lt;/code>: Loki&amp;rsquo;s port&lt;/li>
&lt;li>&lt;code>jsonData.maxLines&lt;/code>: The limit of the log messages which Grafana will show to the users.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Decrease this value if the browser works slowly!&lt;/strong>&lt;/p></description></item><item><title>Docs: Managed Seed</title><link>https://gardener.cloud/docs/gardener/usage/managed_seed/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/managed_seed/</guid><description>
&lt;h1 id="register-shoot-as-seed">Register Shoot as Seed&lt;/h1>
&lt;p>An existing shoot can be registered as a seed by creating a &lt;code>ManagedSeed&lt;/code> resource. This resource contains:&lt;/p>
&lt;ul>
&lt;li>The name of the shoot that should be registered as seed.&lt;/li>
&lt;li>An &lt;code>gardenlet&lt;/code> section that contains:
&lt;ul>
&lt;li>&lt;code>gardenlet&lt;/code> deployment parameters, such as the number of replicas, the image, etc.&lt;/li>
&lt;li>The &lt;code>GardenletConfiguration&lt;/code> resource that contains controllers configuration, feature gates, and a &lt;code>seedConfig&lt;/code> section that contains the &lt;code>Seed&lt;/code> spec and parts of its metadata.&lt;/li>
&lt;li>Additional configuration parameters, such as the garden connection bootstrap mechanism (see &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/#tls-bootstrapping">TLS Bootstrapping&lt;/a>), and whether to merge the provided configuration with the configuration of the parent &lt;code>gardenlet&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;code>gardenlet&lt;/code> is deployed to the shoot, and it registers a new seed upon startup based on the &lt;code>seedConfig&lt;/code> section.&lt;/p>
&lt;blockquote>
&lt;p>Note: Earlier Gardener allowed specifying a &lt;code>seedTemplate&lt;/code> directly in the &lt;code>ManagedSeed&lt;/code> resource. This feature is discontinued, any seed configuration must be via the &lt;code>GardenletConfiguration&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>Note the following important aspects:&lt;/p>
&lt;ul>
&lt;li>Unlike the &lt;code>Seed&lt;/code> resource, the &lt;code>ManagedSeed&lt;/code> resource is namespaced. Currently, managed seeds are restricted to the &lt;code>garden&lt;/code> namespace.&lt;/li>
&lt;li>The newly created &lt;code>Seed&lt;/code> resource always has the same name as the &lt;code>ManagedSeed&lt;/code> resource. Attempting to specify a different name in &lt;code>seedConfig&lt;/code> will fail.&lt;/li>
&lt;li>The &lt;code>ManagedSeed&lt;/code> resource must always refer to an existing shoot. Attempting to create a &lt;code>ManagedSeed&lt;/code> referring to a non-existing shoot will fail.&lt;/li>
&lt;li>A shoot that is being referred to by a &lt;code>ManagedSeed&lt;/code> cannot be deleted. Attempting to delete such a shoot will fail.&lt;/li>
&lt;li>You can omit practically everything from the &lt;code>gardenlet&lt;/code> section, including all or most of the &lt;code>Seed&lt;/code> spec fields. Proper defaults will be supplied in all cases, based either on the most common use cases or the information already available in the &lt;code>Shoot&lt;/code> resource.&lt;/li>
&lt;li>Also, if your seed is configured to host HA shoot control planes, then &lt;code>gardenlet&lt;/code> will be deployed with multiple replicas across nodes or availability zones by default.&lt;/li>
&lt;li>Some &lt;code>Seed&lt;/code> spec fields, for example the provider type and region, networking CIDRs for pods, services, and nodes, etc., must be the same as the corresponding &lt;code>Shoot&lt;/code> spec fields of the shoot that is being registered as seed. Attempting to use different values (except empty ones, so that they are supplied by the defaulting mechanims) will fail.&lt;/li>
&lt;/ul>
&lt;h2 id="deploying-gardenlet-to-the-shoot">Deploying Gardenlet to the Shoot&lt;/h2>
&lt;p>To register a shoot as a seed and deploy &lt;code>gardenlet&lt;/code> to the shoot using a default configuration, create a &lt;code>ManagedSeed&lt;/code> resource similar to the following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: seedmanagement.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ManagedSeed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-managed-seed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> shoot:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: crazy-botany
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> gardenlet: {}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For an example that uses non-default configuration, see &lt;a href="https://github.com/gardener/gardener/blob/master/example/55-managedseed-gardenlet.yaml">55-managed-seed-gardenlet.yaml&lt;/a>&lt;/p>
&lt;h3 id="renewing-the-gardenlet-kubeconfig-secret">Renewing the Gardenlet Kubeconfig Secret&lt;/h3>
&lt;p>In order to making the &lt;code>ManagedSeed&lt;/code> controller renew the gardenlet&amp;rsquo;s kubeconfig secret, annotate the &lt;code>ManagedSeed&lt;/code> with &lt;code>gardener.cloud/operation=renew-kubeconfig&lt;/code>. This will trigger a reconciliation during which the kubeconfig secret is deleted and the bootstrapping is performed again (during which gardenlet obtains a new client certificate).&lt;/p>
&lt;p>It is also possible to trigger the renewal on the secret directly, see &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/#rotate-certificates-using-bootstrap-kubeconfig">here&lt;/a>.&lt;/p>
&lt;h3 id="specifying-apiserver-replicas-and-autoscaler-options">Specifying &lt;code>apiServer&lt;/code> &lt;code>replicas&lt;/code> and &lt;code>autoscaler&lt;/code> options&lt;/h3>
&lt;p>There are few configuration options that are not supported in a &lt;code>Shoot&lt;/code> resource but due to backward compatibility reasons it is possible to specify them for a &lt;code>Shoot&lt;/code> that is referred by a &lt;code>ManagedSeed&lt;/code>. These options are:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Option&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>apiServer.autoscaler.minReplicas&lt;/code>&lt;/td>
&lt;td>Controls the minimum number of &lt;code>kube-apiserver&lt;/code> replicas for the shoot registered as seed cluster.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>apiServer.autoscaler.maxReplicas&lt;/code>&lt;/td>
&lt;td>Controls the maximum number of &lt;code>kube-apiserver&lt;/code> replicas for the shoot registered as seed cluster.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>apiServer.replicas&lt;/code>&lt;/td>
&lt;td>Controls how many &lt;code>kube-apiserver&lt;/code> replicas the shoot registered as seed cluster gets by default.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>It is possible to specify these options via the &lt;code>shoot.gardener.cloud/managed-seed-api-server&lt;/code> annotation on the Shoot resource. Example configuration:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> shoot.gardener.cloud/managed-seed-api-server: &lt;span style="color:#a31515">&amp;#34;apiServer.replicas=3,apiServer.autoscaler.minReplicas=3,apiServer.autoscaler.maxReplicas=6&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="enforced-configuration-options">Enforced configuration options&lt;/h3>
&lt;p>The following configuration options are enforced by Gardener API server for the ManagedSeed resources:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The vertical pod autoscaler should be enabled from the Shoot specification.&lt;/p>
&lt;p>The vertical pod autoscaler is a prerequisite for a Seed cluster. It is possible to enable the VPA feature for a Seed &lt;a href="https://gardener.cloud/docs/gardener/usage/seed_settings/#vertical-pod-autoscaler">(using the Seed spec)&lt;/a> and for a Shoot &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_autoscaling/#vertical-pod-auto-scaling">(using the Shoot spec)&lt;/a>. In context of &lt;code>ManagedSeed&lt;/code>s, enabling the VPA in the Seed spec (instead of the Shoot spec) offers less flexibility and increases the network transfer and cost. Due to these reasons, the Gardener API server enforces the vertical pod autoscaler to be enabled from the Shoot specification.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The nginx-ingress addon should not be enabled for a Shoot referred by a ManagedSeed.&lt;/p>
&lt;p>An Ingress controller is also a prerequisite for a Seed cluster. For a Seed cluster it is possible to enable Gardener managed Ingress controller or to deploy self-managed Ingress controller. There is also the nginx-ingress addon that can be enabled for a Shoot (using the Shoot spec). However, the Shoot nginx-ingress addon is in deprecated mode and it is not recommended for production clusters. Due to these reasons, the Gardener API server does not allow the Shoot nginx-ingress addon to be enabled for ManagedSeeds.&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>Docs: Node Readiness</title><link>https://gardener.cloud/docs/gardener/usage/node-readiness/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/node-readiness/</guid><description>
&lt;h1 id="readiness-of-shoot-worker-nodes">Readiness of Shoot Worker Nodes&lt;/h1>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>When registering new &lt;code>Nodes&lt;/code>, kubelet adds the &lt;code>node.kubernetes.io/not-ready&lt;/code> taint to prevent scheduling workload Pods to the &lt;code>Node&lt;/code> until the &lt;code>Ready&lt;/code> condition gets &lt;code>True&lt;/code>.
However, the kubelet does not consider the readiness of node-critical Pods.
Hence, the &lt;code>Ready&lt;/code> condition might get &lt;code>True&lt;/code> and the &lt;code>node.kubernetes.io/not-ready&lt;/code> taint might get removed, for example, before the CNI daemon Pod (e.g. &lt;code>calico-node&lt;/code>) has successfully placed the CNI binaries on the machine.&lt;/p>
&lt;p>This problem has been discussed extensively in kubernetes, e.g., in &lt;a href="https://github.com/kubernetes/kubernetes/issues/75890">kubernetes/kubernetes#75890&lt;/a>.
However, several proposals have been rejected because the problem can be solved by using the &lt;code>--register-with-taints&lt;/code> kubelet flag and dedicated controllers (&lt;a href="https://github.com/kubernetes/enhancements/pull/1003#issuecomment-619087019">ref&lt;/a>).&lt;/p>
&lt;h2 id="implementation-in-gardener">Implementation in Gardener&lt;/h2>
&lt;p>Gardener makes sure that workload Pods are only scheduled to &lt;code>Nodes&lt;/code> where all node-critical components required for running workload Pods are ready.
For this, Gardener follows the proposed solution by the Kubernetes community and registers new &lt;code>Node&lt;/code> objects with the &lt;code>node.gardener.cloud/critical-components-not-ready&lt;/code> taint (effect &lt;code>NoSchedule&lt;/code>).
gardener-resource-manager&amp;rsquo;s &lt;a href="https://gardener.cloud/docs/gardener/concepts/resource-manager/#node-controller">&lt;code>Node&lt;/code> controller&lt;/a> reacts on newly created &lt;code>Node&lt;/code> objects that have this taint.
The controller removes the taint once all node-critical Pods are ready (determined by checking the Pods&amp;rsquo; &lt;code>Ready&lt;/code> conditions).&lt;/p>
&lt;p>The &lt;code>Node&lt;/code> controller considers all &lt;code>DaemonSets&lt;/code> and &lt;code>Pods&lt;/code> with the label &lt;code>node.gardener.cloud/critical-component=true&lt;/code> as node-critical.
If there are &lt;code>DaemonSets&lt;/code> that contain the &lt;code>node.gardener.cloud/critical-component=true&lt;/code> label in their metadata and in their Pod template, the &lt;code>Node&lt;/code> controller waits for corresponding daemon Pods to be scheduled and to get ready before removing the taint.&lt;/p>
&lt;h2 id="marking-node-critical-components">Marking Node-Critical Components&lt;/h2>
&lt;p>To make use of this feature, node-critical DaemonSets and Pods need to&lt;/p>
&lt;ul>
&lt;li>tolerate the &lt;code>node.gardener.cloud/critical-components-not-ready&lt;/code> &lt;code>NoSchedule&lt;/code> taint and&lt;/li>
&lt;li>be labelled with &lt;code>node.gardener.cloud/critical-component=true&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>Gardener already marks components like kube-proxy, apiserver-proxy and node-local-dns as node-critical.
Provider extensions mark components like csi-driver-node as node-critical.
Network extensions mark components responsible for setting up CNI on worker Nodes (e.g. &lt;code>calico-node&lt;/code>) as node-critical.
If shoot owners manage any additional node-critical components, they can make use of this feature as well.&lt;/p></description></item><item><title>Docs: NodeLocalDNS Configuration</title><link>https://gardener.cloud/docs/gardener/usage/node-local-dns/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/node-local-dns/</guid><description>
&lt;h1 id="nodelocaldns-configuration">NodeLocalDNS Configuration&lt;/h1>
&lt;p>This is a short guide describing how to enable DNS caching on the shoot cluster nodes.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Currently in Gardener we are using CoreDNS as a deployment that is auto-scaled horizontally to cover for QPS-intensive applications. However, doing so does not seem to be enough to completely circumvent DNS bottlenecks such as:&lt;/p>
&lt;ul>
&lt;li>Cloud provider limits for DNS lookups.&lt;/li>
&lt;li>Unreliable UDP connections that forces a period of timeout in case packets are dropped.&lt;/li>
&lt;li>Unnecessary node hopping since CoreDNS is not deployed on all nodes, and as a result DNS queries end-up traversing multiple nodes before reaching the destination server.&lt;/li>
&lt;li>Inefficient load-balancing of services (e.g., round-robin might not be enough when using IPTables mode)&lt;/li>
&lt;li>and more &amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>To workaround the issues described above, &lt;code>node-local-dns&lt;/code> was introduced. The architecture is described below. The idea is simple:&lt;/p>
&lt;ul>
&lt;li>For new queries, the connection is upgraded from UDP to TCP and forwarded towards the cluster IP for the original CoreDNS server.&lt;/li>
&lt;li>For previously resolved queries, an immediate response from the same node where the requester workload / pod resides is provided.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://gardener.cloud/__resources/node-local-dns_6d452a.png" alt="node-local-dns-architecture">&lt;/p>
&lt;h2 id="configuring-nodelocaldns">Configuring NodeLocalDNS&lt;/h2>
&lt;p>All that needs to be done to enable the usage of the &lt;code>node-local-dns&lt;/code> feature is to set the corresponding option (&lt;code>spec.systemComponents.nodeLocalDNS.enabled&lt;/code>) in the &lt;code>Shoot&lt;/code> resource to &lt;code>true&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> systemComponents:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeLocalDNS:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It is worth noting that:&lt;/p>
&lt;ul>
&lt;li>When migrating from IPVS to IPTables, existing pods will continue to leverage the node-local-dns cache.&lt;/li>
&lt;li>When migrating from IPtables to IPVS, only newer pods will be switched to the node-local-dns cache.&lt;/li>
&lt;li>The annotation will take effect during the next shoot reconciliation. This happens automatically once per day in the maintenance period (unless you have disabled it).&lt;/li>
&lt;li>During the reconfiguration of the node-local-dns there might be a short disruption in terms of domain name resolution depending on the setup. Usually, dns requests are repeated for some time as udp is an unreliable protocol, but that strictly depends on the application/way the domain name resolution happens. It is recommended to let the shoot be reconciled during the next maintenance period.&lt;/li>
&lt;li>If a short DNS outage is not a big issue, you can &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_operations/#immediate-reconciliation">trigger reconciliation&lt;/a> directly after setting the annotation.&lt;/li>
&lt;li>Switching node-local-dns off by removing the annotation can be a rather destructive operation that will result in pods without a working dns configuration.&lt;/li>
&lt;/ul>
&lt;p>For more information about &lt;code>node-local-dns&lt;/code> please refer to the &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/1024-nodelocal-cache-dns/README.md">KEP&lt;/a> or to the &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/">usage documentation&lt;/a>.&lt;/p>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;p>Custom DNS configuration may not work as expected in conjunction with &lt;code>NodeLocalDNS&lt;/code>.
Please refer to &lt;a href="https://gardener.cloud/docs/gardener/usage/custom-dns-config/#node-local-dns">Custom DNS Configuration&lt;/a>.&lt;/p></description></item><item><title>Docs: OpenIDConnect Presets</title><link>https://gardener.cloud/docs/gardener/usage/openidconnect-presets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/openidconnect-presets/</guid><description>
&lt;h1 id="clusteropenidconnectpreset-and-openidconnectpreset">ClusterOpenIDConnectPreset and OpenIDConnectPreset&lt;/h1>
&lt;p>This page provides an overview of ClusterOpenIDConnectPresets and OpenIDConnectPresets, which are objects for injecting &lt;a href="https://openid.net/connect/">OpenIDConnect Configuration&lt;/a> into &lt;code>Shoot&lt;/code> at creation time. The injected information contains configuration for the Kube API Server and optionally configuration for kubeconfig generation using said configuration.&lt;/p>
&lt;h2 id="openidconnectpreset">OpenIDConnectPreset&lt;/h2>
&lt;p>An OpenIDConnectPreset is an API resource for injecting additional runtime OIDC requirements into a Shoot at creation time. You use label selectors to specify the &lt;code>Shoot&lt;/code> to which a given OpenIDConnectPreset applies.&lt;/p>
&lt;p>Using a OpenIDConnectPresets allows project owners to not have to explicitly provide the same OIDC configuration for every &lt;code>Shoot&lt;/code> in their &lt;code>Project&lt;/code>.&lt;/p>
&lt;p>For more information about the background, see the &lt;a href="https://github.com/gardener/gardener/issues/1161">issue&lt;/a> for OpenIDConnectPreset.&lt;/p>
&lt;h3 id="how-openidconnectpreset-works">How OpenIDConnectPreset works&lt;/h3>
&lt;p>Gardener provides an admission controller (OpenIDConnectPreset) which, when enabled, applies OpenIDConnectPresets to incoming &lt;code>Shoot&lt;/code> creation requests. When a &lt;code>Shoot&lt;/code> creation request occurs, the system does the following:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Retrieve all OpenIDConnectPreset available for use in the &lt;code>Shoot&lt;/code> namespace.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Check if the shoot label selectors of any OpenIDConnectPreset matches the labels on the Shoot being created.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If multiple presets are matched then only one is chosen and results are sorted based on:&lt;/p>
&lt;ol>
&lt;li>&lt;code>.spec.weight&lt;/code> value.&lt;/li>
&lt;li>lexicographically ordering their names ( e.g. &lt;code>002preset&lt;/code> &amp;gt; &lt;code>001preset&lt;/code> )&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>If the &lt;code>Shoot&lt;/code> already has a &lt;code>.spec.kubernetes.kubeAPIServer.oidcConfig&lt;/code> then no mutation occurs.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="simple-openidconnectpreset-example">Simple OpenIDConnectPreset example&lt;/h3>
&lt;p>This is a simple example to show how a &lt;code>Shoot&lt;/code> is modified by the OpenIDConnectPreset&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: settings.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: OpenIDConnectPreset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: test-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> shootSelector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidc: enabled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clientID: test-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerURL: https://foo.bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># caBundle: |&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># -----BEGIN CERTIFICATE-----&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Li4u&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># -----END CERTIFICATE-----&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> groupsClaim: groups-claim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> groupsPrefix: groups-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernameClaim: username-claim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernamePrefix: username-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> signingAlgs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - RS256
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requiredClaims:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> key: value
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> client:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secret: oidc-client-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extraConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extra-scopes: &lt;span style="color:#a31515">&amp;#34;email,offline_access,profile&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> foo: bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> weight: 90
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create the OpenIDConnectPreset:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl apply -f preset.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Examine the created OpenIDConnectPreset:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl get openidconnectpresets
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME ISSUER SHOOT-SELECTOR AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>test-1 https://foo.bar oidc=enabled 1s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Simple &lt;code>Shoot&lt;/code> example:&lt;/p>
&lt;p>This is a sample of a &lt;code>Shoot&lt;/code> with some fields omitted:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: preset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidc: enabled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> allowPrivilegedContainers: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.20.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create the Shoot:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl apply -f shoot.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Examine the created Shoot:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl get shoot preset -o yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: preset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidc: enabled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeAPIServer:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidcConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clientAuthentication:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extraConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extra-scopes: email,offline_access,profile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> foo: bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secret: oidc-client-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clientID: test-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> groupsClaim: groups-claim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> groupsPrefix: groups-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerURL: https://foo.bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requiredClaims:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> key: value
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> signingAlgs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - RS256
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernameClaim: username-claim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernamePrefix: username-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.20.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="disable-openidconnectpreset">Disable OpenIDConnectPreset&lt;/h3>
&lt;p>The OpenIDConnectPreset admission control is enabled by default. To disable it use the &lt;code>--disable-admission-plugins&lt;/code> flag on the gardener-apiserver.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>--disable-admission-plugins=OpenIDConnectPreset
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="clusteropenidconnectpreset">ClusterOpenIDConnectPreset&lt;/h2>
&lt;p>A ClusterOpenIDConnectPreset is an API resource for injecting additional runtime OIDC requirements into a Shoot at creation time. In contrast to OpenIDConnect it&amp;rsquo;s a cluster-scoped resource. You use label selectors to specify the &lt;code>Project&lt;/code> and &lt;code>Shoot&lt;/code> to which a given OpenIDCConnectPreset applies.&lt;/p>
&lt;p>Using a OpenIDConnectPresets allows cluster owners to not have to explicitly provide the same OIDC configuration for every &lt;code>Shoot&lt;/code> in specific &lt;code>Project&lt;/code>.&lt;/p>
&lt;p>For more information about the background, see the &lt;a href="https://github.com/gardener/gardener/issues/1161">issue&lt;/a> for ClusterOpenIDConnectPreset.&lt;/p>
&lt;h3 id="how-clusteropenidconnectpreset-works">How ClusterOpenIDConnectPreset works&lt;/h3>
&lt;p>Gardener provides an admission controller (ClusterOpenIDConnectPreset) which, when enabled, applies ClusterOpenIDConnectPresets to incoming &lt;code>Shoot&lt;/code> creation requests. When a &lt;code>Shoot&lt;/code> creation request occurs, the system does the following:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Retrieve all ClusterOpenIDConnectPresets available.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Check if the project label selector of any ClusterOpenIDConnectPreset matches the labels of the &lt;code>Project&lt;/code> in which the &lt;code>Shoot&lt;/code> is being created.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Check if the shoot label selectors of any ClusterOpenIDConnectPreset matches the labels on the &lt;code>Shoot&lt;/code> being created.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If multiple presets are matched then only one is chosen and results are sorted based on:&lt;/p>
&lt;ol>
&lt;li>&lt;code>.spec.weight&lt;/code> value.&lt;/li>
&lt;li>lexicographically ordering their names ( e.g. &lt;code>002preset&lt;/code> &amp;gt; &lt;code>001preset&lt;/code> )&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>If the &lt;code>Shoot&lt;/code> already has a &lt;code>.spec.kubernetes.kubeAPIServer.oidcConfig&lt;/code> then no mutation occurs.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Note: Due to the previous requirement if a &lt;code>Shoot&lt;/code> is matched by both &lt;code>OpenIDConnectPreset&lt;/code> and &lt;code>ClusterOpenIDConnectPreset&lt;/code> then &lt;code>OpenIDConnectPreset&lt;/code> takes precedence over &lt;code>ClusterOpenIDConnectPreset&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;h3 id="simple-clusteropenidconnectpreset-example">Simple ClusterOpenIDConnectPreset example&lt;/h3>
&lt;p>This is a simple example to show how a &lt;code>Shoot&lt;/code> is modified by the ClusterOpenIDConnectPreset&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: settings.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ClusterOpenIDConnectPreset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: test
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> shootSelector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidc: enabled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> projectSelector: {} &lt;span style="color:#008000"># selects all projects.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clientID: cluster-preset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerURL: https://foo.bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># caBundle: |&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># -----BEGIN CERTIFICATE-----&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Li4u&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># -----END CERTIFICATE-----&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> groupsClaim: groups-claim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> groupsPrefix: groups-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernameClaim: username-claim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernamePrefix: username-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> signingAlgs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - RS256
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requiredClaims:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> key: value
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> client:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secret: oidc-client-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extraConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extra-scopes: &lt;span style="color:#a31515">&amp;#34;email,offline_access,profile&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> foo: bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> weight: 90
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create the ClusterOpenIDConnectPreset:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl apply -f preset.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Examine the created ClusterOpenIDConnectPreset:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl get clusteropenidconnectpresets
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME ISSUER PROJECT-SELECTOR SHOOT-SELECTOR AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>test https://foo.bar &amp;lt;none&amp;gt; oidc=enabled 1s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This is a sample of a &lt;code>Shoot&lt;/code> with some fields omitted:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: preset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidc: enabled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> allowPrivilegedContainers: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.20.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create the Shoot:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl apply -f shoot.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Examine the created Shoot:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl get shoot preset -o yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: preset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidc: enabled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeAPIServer:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidcConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clientAuthentication:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extraConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extra-scopes: email,offline_access,profile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> foo: bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secret: oidc-client-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clientID: cluster-preset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> groupsClaim: groups-claim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> groupsPrefix: groups-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerURL: https://foo.bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requiredClaims:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> key: value
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> signingAlgs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - RS256
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernameClaim: username-claim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernamePrefix: username-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.20.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="disable-clusteropenidconnectpreset">Disable ClusterOpenIDConnectPreset&lt;/h3>
&lt;p>The ClusterOpenIDConnectPreset admission control is enabled by default. To disable it use the &lt;code>--disable-admission-plugins&lt;/code> flag on the gardener-apiserver.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>--disable-admission-plugins=ClusterOpenIDConnectPreset
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Pod Security</title><link>https://gardener.cloud/docs/gardener/usage/pod-security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/pod-security/</guid><description>
&lt;h1 id="migrating-from-podsecuritypolicys-to-podsecurity-admission-controller">Migrating From &lt;code>PodSecurityPolicy&lt;/code>s To PodSecurity Admission Controller&lt;/h1>
&lt;p>Kubernetes has deprecated the &lt;code>PodSecurityPolicy&lt;/code> API in &lt;code>v1.21&lt;/code> and it will be removed in &lt;code>v1.25&lt;/code>. With &lt;code>v1.23&lt;/code>, a new feature called &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">&lt;code>PodSecurity&lt;/code>&lt;/a> was promoted to beta. From &lt;code>v1.25&lt;/code> onwards, there will be no API serving &lt;code>PodSecurityPolicy&lt;/code>s, so you have to cleanup all the existing PSPs before upgrading your cluster. Detailed migration steps are described &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/">here&lt;/a>.&lt;/p>
&lt;p>After migration, you should disable the &lt;code>PodSecurityPolicy&lt;/code> admission plugin. To do so, you have to add:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>admissionPlugins:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: PodSecurityPolicy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> disabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>in &lt;code>spec.kubernetes.kubeAPIServer.admissionPlugins&lt;/code> field in the &lt;code>Shoot&lt;/code> resource. Please refer the example &lt;code>Shoot&lt;/code> manifest &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml">here&lt;/a>.&lt;/p>
&lt;p>Only if the &lt;code>PodSecurityPolicy&lt;/code> admission plugin is disabled the cluster can be upgraded to &lt;code>v1.25&lt;/code>.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ You should disable the admission plugin and wait until Gardener finish at least one &lt;code>Shoot&lt;/code> reconciliation before upgrading to &lt;code>v1.25&lt;/code>. This is to make sure all the &lt;code>PodSecurityPolicy&lt;/code> related resources deployed by Gardener are cleaned up.&lt;/p>
&lt;/blockquote>
&lt;h2 id="admission-configuration-for-the-podsecurity-admission-plugin">Admission Configuration For The &lt;code>PodSecurity&lt;/code> Admission Plugin&lt;/h2>
&lt;p>If you wish to add your custom configuration for the &lt;code>PodSecurity&lt;/code> plugin and your cluster version is &lt;code>v1.23+&lt;/code>, you can do so in the Shoot spec under &lt;code>.spec.kubernetes.kubeAPIServer.admissionPlugins&lt;/code> by adding:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>admissionPlugins:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: PodSecurity
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: pod-security.admission.config.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: PodSecurityConfiguration
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Defaults applied when a mode label is not set.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Level label values must be one of:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - &amp;#34;privileged&amp;#34; (default)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - &amp;#34;baseline&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - &amp;#34;restricted&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Version label values must be one of:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - &amp;#34;latest&amp;#34; (default) &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - specific version like &amp;#34;v1.25&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> defaults:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enforce: &lt;span style="color:#a31515">&amp;#34;privileged&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enforce-version: &lt;span style="color:#a31515">&amp;#34;latest&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> audit: &lt;span style="color:#a31515">&amp;#34;privileged&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> audit-version: &lt;span style="color:#a31515">&amp;#34;latest&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> warn: &lt;span style="color:#a31515">&amp;#34;privileged&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> warn-version: &lt;span style="color:#a31515">&amp;#34;latest&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> exemptions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Array of authenticated usernames to exempt.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernames: []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Array of runtime class names to exempt.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> runtimeClasses: []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Array of namespaces to exempt.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespaces: []
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>⚠️ Note that &lt;code>pod-security.admission.config.k8s.io/v1&lt;/code> configuration requires &lt;code>v1.25&lt;/code>+. For &lt;code>v1.23&lt;/code> and &lt;code>v1.24&lt;/code>, use &lt;code>pod-security.admission.config.k8s.io/v1beta1&lt;/code>. For &lt;code>v1.22&lt;/code>, use &lt;code>pod-security.admission.config.k8s.io/v1alpha1&lt;/code>.&lt;/p>
&lt;p>Also note that in &lt;code>v1.22&lt;/code> the feature gate &lt;code>PodSecurity&lt;/code> is not enabled by default. You have to add:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>featureGates:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> PodSecurity: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>under &lt;code>.spec.kubernetes.kubeAPIServer&lt;/code>.&lt;/p>
&lt;p>For proper functioning of Gardener, &lt;code>kube-system&lt;/code> namespace will also be automatically added to the &lt;code>exemptions.namespaces&lt;/code> list.&lt;/p>
&lt;h2 id="speckubernetesallowprivilegedcontainers-in-the-shoot-spec">&lt;code>.spec.kubernetes.allowPrivilegedContainers&lt;/code> in the Shoot spec&lt;/h2>
&lt;p>If this field is set to &lt;code>true&lt;/code> then all authenticated users can use the &amp;ldquo;gardener.privileged&amp;rdquo; &lt;code>PodSecurityPolicy&lt;/code>, allowing full unrestricted access to Pod features. However, the &lt;code>PodSecurityPolicy&lt;/code> admission plugin is removed in Kubernetes &lt;code>v1.25&lt;/code> and &lt;code>PodSecurity&lt;/code> has taken its place as its successor. Therefore, this field doesn&amp;rsquo;t have any relevance in versions &lt;code>&amp;gt;= v1.25&lt;/code> anymore. If you need to set a default pod admission level for your cluster, follow &lt;a href="#admission-configuration-for-the-podsecurity-admission-plugin">this documentation&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>Note: You should remove this field from the &lt;code>Shoot&lt;/code> spec for &lt;code>v1.24&lt;/code> clusters after migrating to the new &lt;code>PodSecurity&lt;/code> admission controller, before upgrading your cluster to &lt;code>v1.25&lt;/code>.&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: Projects</title><link>https://gardener.cloud/docs/gardener/usage/projects/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/projects/</guid><description>
&lt;h1 id="projects">Projects&lt;/h1>
&lt;p>The Gardener API server supports a cluster-scoped &lt;code>Project&lt;/code> resource which is used for data isolation between individual Gardener consumers. For example, each development team has its own project to manage its own shoot clusters.&lt;/p>
&lt;p>Each &lt;code>Project&lt;/code> is backed by a Kubernetes &lt;code>Namespace&lt;/code> that contains the actual related Kubernetes resources like &lt;code>Secret&lt;/code>s or &lt;code>Shoot&lt;/code>s.&lt;/p>
&lt;p>&lt;strong>Example resource:&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Project
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden-dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> description: &lt;span style="color:#a31515">&amp;#34;This is my first project&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> purpose: &lt;span style="color:#a31515">&amp;#34;Experimenting with Gardener&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> owner:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: User
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: john.doe@example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> members:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: User
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: alice.doe@example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> role: admin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># roles:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - viewer &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - uam&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - serviceaccountmanager&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - extension:foo&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: User
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: bob.doe@example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> role: viewer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># tolerations:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># defaults:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># - key: &amp;lt;some-key&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># whitelist:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># - key: &amp;lt;some-key&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>.spec.namespace&lt;/code> field is optional and is initialized if unset.
The name of the resulting namespace will be determined based on the &lt;code>Project&lt;/code> name and UID, e.g. &lt;code>garden-dev-5aef3&lt;/code>.
It&amp;rsquo;s also possible to adopt existing namespaces by labeling them &lt;code>gardener.cloud/role=project&lt;/code> and &lt;code>project.gardener.cloud/name=dev&lt;/code> beforehand (otherwise, they cannot be adopted).&lt;/p>
&lt;p>When deleting a Project resource, the corresponding namespace is also deleted.
To keep a namespace after project deletion, an administrator/operator (not Project members!) can annotate the project-namespace with &lt;code>namespace.gardener.cloud/keep-after-project-deletion&lt;/code>.&lt;/p>
&lt;p>The &lt;code>spec.description&lt;/code> and &lt;code>.spec.purpose&lt;/code> fields can be used to describe to fellow team members and Gardener operators what this project is used for.&lt;/p>
&lt;p>Each project has one dedicated owner, configured in &lt;code>.spec.owner&lt;/code> using the &lt;code>rbac.authorization.k8s.io/v1.Subject&lt;/code> type.
The owner is the main contact person for Gardener operators.
Please note that the &lt;code>.spec.owner&lt;/code> field is deprecated and will be removed in future API versions in favor of the &lt;code>owner&lt;/code> role, see below.&lt;/p>
&lt;p>The list of members (again a list in &lt;code>.spec.members[]&lt;/code> using the &lt;code>rbac.authorization.k8s.io/v1.Subject&lt;/code> type) contains all the people that are associated with the project in any way.
Each project member must have at least one role (currently described in &lt;code>.spec.members[].role&lt;/code>, additional roles can be added to &lt;code>.spec.members[].roles[]&lt;/code>). The following roles exist:&lt;/p>
&lt;ul>
&lt;li>&lt;code>admin&lt;/code>: This allows to fully manage resources inside the project (e.g., secrets, shoots, configmaps, and similar). Mind that the &lt;code>admin&lt;/code> role has readonly access to service accounts.&lt;/li>
&lt;li>&lt;code>serviceaccountmanager&lt;/code>: This allows to fully manage service accounts inside the project namespace and request tokens for them. The permissions of the created service accounts are instead managed by the &lt;code>admin&lt;/code> role. Please refer to &lt;a href="https://gardener.cloud/docs/gardener/usage/service-account-manager/">Service Account Manager&lt;/a>.&lt;/li>
&lt;li>&lt;code>uam&lt;/code>: This allows to add/modify/remove human users or groups to/from the project member list.&lt;/li>
&lt;li>&lt;code>viewer&lt;/code>: This allows to read all resources inside the project except secrets.&lt;/li>
&lt;li>&lt;code>owner&lt;/code>: This combines the &lt;code>admin&lt;/code>, &lt;code>uam&lt;/code> and &lt;code>serviceaccountmanager&lt;/code> roles.&lt;/li>
&lt;li>Extension roles (prefixed with &lt;code>extension:&lt;/code>): Please refer to &lt;a href="https://gardener.cloud/docs/gardener/extensions/project-roles/">Extending Project Roles&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>The &lt;a href="https://gardener.cloud/docs/gardener/concepts/controller-manager/#project-controller">project controller&lt;/a> inside the Gardener Controller Manager is managing RBAC resources that grant the described privileges to the respective members.&lt;/p>
&lt;p>There are three central &lt;code>ClusterRole&lt;/code>s &lt;code>gardener.cloud:system:project-member&lt;/code>, &lt;code>gardener.cloud:system:project-viewer&lt;/code> and &lt;code>gardener.cloud:system:project-serviceaccountmanager&lt;/code> that grant the permissions for namespaced resources (e.g., &lt;code>Secret&lt;/code>s, &lt;code>Shoot&lt;/code>s, &lt;code>ServiceAccount&lt;/code>s, etc.).
Via referring &lt;code>RoleBinding&lt;/code>s created in the respective namespace the project members get bound to these &lt;code>ClusterRole&lt;/code>s and, thus, the needed permissions.
There are also project-specific &lt;code>ClusterRole&lt;/code>s granting the permissions for cluster-scoped resources, e.g. the &lt;code>Namespace&lt;/code> or &lt;code>Project&lt;/code> itself.&lt;br>
For each role, the following &lt;code>ClusterRole&lt;/code>s, &lt;code>ClusterRoleBinding&lt;/code>s, and &lt;code>RoleBinding&lt;/code>s are created:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Role&lt;/th>
&lt;th>&lt;code>ClusterRole&lt;/code>&lt;/th>
&lt;th>&lt;code>ClusterRoleBinding&lt;/code>&lt;/th>
&lt;th>&lt;code>RoleBinding&lt;/code>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>admin&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-member:&amp;lt;projectName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-member:&amp;lt;projectName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-member&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>serviceaccountmanager&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-serviceaccountmanager&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>uam&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-uam:&amp;lt;projectName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-uam:&amp;lt;projectName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>viewer&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-viewer:&amp;lt;projectName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-viewer:&amp;lt;projectName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-viewer&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>owner&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project:&amp;lt;projectName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project:&amp;lt;projectName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>extension:*&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:extension:project:&amp;lt;projectName&amp;gt;:&amp;lt;extensionRoleName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;code>gardener.cloud:extension:project:&amp;lt;projectName&amp;gt;:&amp;lt;extensionRoleName&amp;gt;&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="user-access-management">User Access Management&lt;/h2>
&lt;p>For &lt;code>Project&lt;/code>s created before Gardener v1.8 all admins were allowed to manage other members.
Beginning with v1.8 the new &lt;code>uam&lt;/code> role is being introduced.
It is backed by the &lt;code>manage-members&lt;/code> custom RBAC verb which allows to add/modify/remove human users or groups to/from the project member list.
Human users are subjects with &lt;code>kind=User&lt;/code> and &lt;code>name!=system:serviceaccount:*&lt;/code>, and groups are subjects with &lt;code>kind=Group&lt;/code>.
The management of service account subjects (&lt;code>kind=ServiceAccount&lt;/code> or &lt;code>name=system:serviceaccount:*&lt;/code>) is not controlled via the &lt;code>uam&lt;/code> custom verb but with the standard &lt;code>update&lt;/code>/&lt;code>patch&lt;/code> verbs for projects.&lt;/p>
&lt;p>All newly created projects will only bind the owner to the &lt;code>uam&lt;/code> role.
The owner can still grant the &lt;code>uam&lt;/code> role to other members if desired.
For projects created before Gardener v1.8 the Gardener Controller Manager will migrate all projects to also assign the &lt;code>uam&lt;/code> role to all &lt;code>admin&lt;/code> members (to not break existing use-cases). The corresponding migration logic is present in Gardener Controller Manager from v1.8 to v1.13.
The project owner can gradually remove these roles if desired.&lt;/p>
&lt;h2 id="stale-projects">Stale Projects&lt;/h2>
&lt;p>When a project is not actively used for some period of time, it is marked as &amp;ldquo;stale&amp;rdquo;. This is done by a controller called &lt;a href="https://gardener.cloud/docs/gardener/concepts/controller-manager/#stale-projects-reconciler">&amp;ldquo;Stale Projects Reconciler&amp;rdquo;&lt;/a>. Once the project is marked as stale there is a time frame in which if not used it will be deleted by that controller.&lt;/p></description></item><item><title>Docs: Reversed VPN Tunnel</title><link>https://gardener.cloud/docs/gardener/usage/reversed-vpn-tunnel/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/reversed-vpn-tunnel/</guid><description>
&lt;h1 id="reversed-vpn-tunnel-setup-and-configuration">Reversed VPN Tunnel Setup and Configuration&lt;/h1>
&lt;p>The Reversed VPN Tunnel is enabled by default.
A highly available VPN connection is automatically deployed in all shoots that configure an HA control-plane.&lt;/p>
&lt;h2 id="reversed-vpn-tunnel">Reversed VPN Tunnel&lt;/h2>
&lt;p>In the first VPN solution, connection establishment was initiated by a VPN client in the seed cluster.
Due to several issues with this solution, the tunnel establishment direction has been reverted.
The client is deployed in the shoot and initiates the connection from there. This way, there is no need to deploy a special purpose
loadbalancer for the sake of addressing the data-plane, in addition to saving costs, this is considered the more secure alternative.
For more information on how this is achieved, please have a look at the following &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/14-reversed-cluster-vpn.md">GEP&lt;/a>.&lt;/p>
&lt;p>Connection establishment with a reversed tunnel:&lt;/p>
&lt;p>&lt;code>APIServer --&amp;gt; Envoy-Proxy | VPN-Seed-Server &amp;lt;-- Istio/Envoy-Proxy &amp;lt;-- SNI API Server Endpoint &amp;lt;-- LB (one for all clusters of a seed) &amp;lt;--- internet &amp;lt;--- VPN-Shoot-Client --&amp;gt; Pods | Nodes | Services&lt;/code>&lt;/p>
&lt;p>The reversed VPN tunnel is always deployed.
The feature gate &lt;code>ReversedVPN&lt;/code> is GA and will be removed in a future release.&lt;/p>
&lt;h2 id="high-availability-for-reversed-vpn-tunnel">High Availability for Reversed VPN Tunnel&lt;/h2>
&lt;p>Shoots which define &lt;code>spec.controlPlane.highAvailability.failureTolerance: {node, zone}&lt;/code> get an HA control-plane including a
highly available VPN connection by deploying redundant VPN servers and clients.&lt;/p>
&lt;p>Please note that it is not possible to move an open connection to another VPN tunnel. Especially long-running
commands like &lt;code>kubectl exec -it ...&lt;/code> or &lt;code>kubectl logs -f ...&lt;/code> will still break if the routing path must be switched
because either VPN server or client are not reachable anymore. New request should be possible within seconds.&lt;/p>
&lt;h3 id="ha-architecture-for-vpn">HA Architecture for VPN&lt;/h3>
&lt;p>Establishing a connection from the VPN client on the shoot to the server in the control plane works nearly the same
way as in the non-HA case. The only difference is that the VPN client targets one of two VPN servers, represented by two services
&lt;code>vpn-seed-server-0&lt;/code> and &lt;code>vpn-seed-server-1&lt;/code> with endpoints in pods with the same name.
The VPN tunnel is used by a &lt;code>kube-apiserver&lt;/code> to reach nodes, services, or pods in the shoot cluster.
In the non-HA case, a kube-apiserver uses an HTTP proxy running as a side-car in the VPN server to address
the shoot networks via the VPN tunnel and the &lt;code>vpn-shoot&lt;/code> acts as a router.
In the HA case, the setup is more complicated. Instead of an HTTP proxy in the VPN server, the kube-apiserver has
additional side-cars. One side-car for each VPN client to connect to the corresponding VPN server.
On the shoot side, there are now two &lt;code>vpn-shoot&lt;/code> pods, each with two VPN clients for each VPN server.
With this setup, there would be four possible routes, but only one can be used. Switching the route kills all
open connections. Therefore, another layer is introduced: link aggregation, also named &lt;a href="https://www.kernel.org/doc/Documentation/networking/bonding.txt">bonding&lt;/a>.
In Linux, you can create a network link by using several other links as slaves. Bonding is here used with
active-backup mode. This means the traffic only goes through the active sublink and is only changed if the active one
becomes unavailable. Switching happens in the bonding network driver without changing any routes. So with this layer,
vpn-seed-server pods can be rolled without disrupting open connections.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/vpn-ha-architecture_2f71aa.png" alt="VPN HA Architecture">&lt;/p>
&lt;p>With bonding, there are 2 possible routing paths, ensuring that there is at least one routing path intact even if
one &lt;code>vpn-seed-server&lt;/code> pod and one &lt;code>vpn-shoot&lt;/code> pod are unavailable at the same time.&lt;/p>
&lt;p>As it is not possible to use multi-path routing, one routing path must be configured explicitly.
For this purpose, the &lt;code>path-controller&lt;/code> script is running in another side-car of the kube-apiserver pod.
It pings all shoot-side VPN clients regularly every few seconds. If the active routing path is not responsive anymore,
the routing is switched to the other responsive routing path.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/vpn-ha-routing-paths_aa0d8d.png" alt="Four possible routing paths">&lt;/p>
&lt;p>For general information about HA control-plane see &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/20-ha-control-planes.md">GEP-20&lt;/a>.&lt;/p></description></item><item><title>Docs: Seed Bootstrapping</title><link>https://gardener.cloud/docs/gardener/usage/seed_bootstrapping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/seed_bootstrapping/</guid><description>
&lt;h1 id="seed-bootstrapping">Seed Bootstrapping&lt;/h1>
&lt;p>Whenever the Gardenlet is responsible for a new &lt;code>Seed&lt;/code> resource its &amp;ldquo;seed controller&amp;rdquo; is being activated.
One part of this controller&amp;rsquo;s reconciliation logic is deploying certain components into the &lt;code>garden&lt;/code> namespace of the seed cluster itself.
These components are required to spawn and manage control planes for shoot clusters later on.
This document is providing an overview which actions are performed during this bootstrapping phase, and it explains the rationale behind them.&lt;/p>
&lt;h2 id="dependency-watchdog">Dependency Watchdog&lt;/h2>
&lt;p>The dependency watchdog (abbreviation: DWD) is a component developed separately in the &lt;a href="https://github.com/gardener/dependency-watchdog">gardener/dependency-watchdog&lt;/a> GitHub repository.
Gardener is using it for two purposes:&lt;/p>
&lt;ol>
&lt;li>Prevention of melt-down situations when the load balancer used to expose the kube-apiserver of shoot clusters goes down while the kube-apiserver itself is still up and running&lt;/li>
&lt;li>Fast recovery times for crash-looping pods when depending pods are again available&lt;/li>
&lt;/ol>
&lt;p>For the sake of separating these concerns, two instances of the DWD are deployed by the seed controller.&lt;/p>
&lt;h3 id="probe">Probe&lt;/h3>
&lt;p>The &lt;code>dependency-watchdog-probe&lt;/code> deployment is responsible for above mentioned first point.&lt;/p>
&lt;p>The &lt;code>kube-apiserver&lt;/code> of shoot clusters is exposed via a load balancer, usually with an attached public IP, which serves as the main entry point when it comes to interaction with the shoot cluster (e.g., via &lt;code>kubectl&lt;/code>).
While end-users are talking to their clusters via this load balancer, other control plane components like the &lt;code>kube-controller-manager&lt;/code> or &lt;code>kube-scheduler&lt;/code> run in the same namespace/same cluster, so they can communicate via the in-cluster &lt;code>Service&lt;/code> directly instead of using the detour with the load balancer.
However, the worker nodes of shoot clusters run in isolated, distinct networks.
This means that the &lt;code>kubelet&lt;/code>s and &lt;code>kube-proxy&lt;/code>s also have to talk to the control plane via the load balancer.&lt;/p>
&lt;p>The &lt;code>kube-controller-manager&lt;/code> has a special control loop called &lt;a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/controller/nodelifecycle">&lt;code>nodelifecycle&lt;/code>&lt;/a> which will set the status of &lt;code>Node&lt;/code>s to &lt;code>NotReady&lt;/code> in case the kubelet stops to regularly renew its lease/to send its heartbeat.
This will trigger other self-healing capabilities of Kubernetes, for example the eviction of pods from such &amp;ldquo;unready&amp;rdquo; nodes to healthy nodes.
Similarly, the &lt;code>cloud-controller-manager&lt;/code> has a control loop that will disconnect load balancers from &amp;ldquo;unready&amp;rdquo; nodes, i.e., such workload would no longer be accessible until moved to a healthy node.&lt;/p>
&lt;p>While these are awesome Kubernetes features on their own, they have a dangerous drawback when applied in the context of Gardener&amp;rsquo;s architecture:
When the &lt;code>kube-apiserver&lt;/code> load balancer fails for whatever reason then the &lt;code>kubelet&lt;/code>s can&amp;rsquo;t talk to the &lt;code>kube-apiserver&lt;/code> to renew their lease anymore.
After a minute or so the &lt;code>kube-controller-manager&lt;/code> will get the impression that all nodes have died and will mark them as &lt;code>NotReady&lt;/code>.
This will trigger above mentioned eviction as well as detachment of load balancers.
As a result, the customer&amp;rsquo;s workload will go down and become unreachable.&lt;/p>
&lt;p>This is exactly the situation that the DWD prevents:
It regularly tries to talk to the &lt;code>kube-apiserver&lt;/code>s of the shoot clusters, once by using their load balancer, and once by talking via the in-cluster &lt;code>Service&lt;/code>.
If it detects that the &lt;code>kube-apiserver&lt;/code> is reachable internally but not externally it scales down the &lt;code>kube-controller-manager&lt;/code> to &lt;code>0&lt;/code>.
This will prevent it from marking the shoot worker nodes as &amp;ldquo;unready&amp;rdquo;.
As soon as the &lt;code>kube-apiserver&lt;/code> is reachable externally again the &lt;code>kube-controller-manager&lt;/code> will be scaled up to &lt;code>1&lt;/code> again.&lt;/p>
&lt;h3 id="endpoint">Endpoint&lt;/h3>
&lt;p>The &lt;code>dependency-watchdog-endpoint&lt;/code> deployment is responsible for above mentioned second point.&lt;/p>
&lt;p>Kubernetes is restarting failing pods with an exponentially increasing backoff time.
While this is a great strategy to prevent system overloads it has the disadvantage that the delay between restarts is increasing up to multiple minutes very fast.&lt;/p>
&lt;p>In the Gardener context, we are deploying many components that are depending on other components.
For example, the &lt;code>kube-apiserver&lt;/code> is depending on a running &lt;code>etcd&lt;/code>, or the &lt;code>kube-controller-manager&lt;/code> and &lt;code>kube-scheduler&lt;/code> are depending on a running &lt;code>kube-apiserver&lt;/code>.
In case such a &amp;ldquo;higher-level&amp;rdquo; component fails for whatever reason, the dependent pods will fail and end-up in crash-loops.
As Kubernetes does not know anything about these hierarchies it won&amp;rsquo;t recognize that such pods can be restarted faster as soon as their dependents are up and running again.&lt;/p>
&lt;p>This is exactly the situation in which the DWD will become active:
If it detects that a certain &lt;code>Service&lt;/code> is available again (e.g., after the &lt;code>etcd&lt;/code> was temporarily down while being moved to another seed node) then DWD will restart all crash-looping dependant pods.
These dependant pods are detected via a pre-configured label selector.&lt;/p>
&lt;p>As of today, the DWD is configured to restart a crash-looping &lt;code>kube-apiserver&lt;/code> after &lt;code>etcd&lt;/code> became available again, or any pod depending on the &lt;code>kube-apiserver&lt;/code> that has a &lt;code>gardener.cloud/role=controlplane&lt;/code> label (e.g., &lt;code>kube-controller-manager&lt;/code>, &lt;code>kube-scheduler&lt;/code>, etc.).&lt;/p></description></item><item><title>Docs: Seed Settings</title><link>https://gardener.cloud/docs/gardener/usage/seed_settings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/seed_settings/</guid><description>
&lt;h1 id="settings-for-seeds">Settings for &lt;code>Seed&lt;/code>s&lt;/h1>
&lt;p>The &lt;code>Seed&lt;/code> resource offers a few settings that are used to control the behaviour of certain Gardener components.
This document provides an overview over the available settings:&lt;/p>
&lt;h2 id="dependency-watchdog">Dependency Watchdog&lt;/h2>
&lt;p>Gardenlet can deploy two instances of the &lt;a href="https://github.com/gardener/dependency-watchdog">dependency-watchdog&lt;/a> into the &lt;code>garden&lt;/code> namespace of the seed cluster.
One instance only activates the &lt;code>endpoint&lt;/code> controller while the second instance only activates the &lt;code>probe&lt;/code> controller.&lt;/p>
&lt;h3 id="endpoint-controller">Endpoint Controller&lt;/h3>
&lt;p>The &lt;code>endpoint&lt;/code> controller helps to alleviate the delay where control plane components remain unavailable by finding the respective pods in CrashLoopBackoff status and restarting them once their dependants become ready and available again.
For example, if &lt;code>etcd&lt;/code> goes down then also &lt;code>kube-apiserver&lt;/code> goes down (and into a &lt;code>CrashLoopBackoff&lt;/code> state). If &lt;code>etcd&lt;/code> comes up again then (without the &lt;code>endpoint&lt;/code> controller) it might take some time until &lt;code>kube-apiserver&lt;/code> gets restarted as well.&lt;/p>
&lt;p>It can be enabled/disabled via the &lt;code>.spec.settings.dependencyWatchdog.endpoint.enabled&lt;/code> field.
It defaults to &lt;code>true&lt;/code>.&lt;/p>
&lt;h3 id="probe-controller">Probe Controller&lt;/h3>
&lt;p>The &lt;code>probe&lt;/code> controller scales down the &lt;code>kube-controller-manager&lt;/code> of shoot clusters in case their respective &lt;code>kube-apiserver&lt;/code> is not reachable via its external ingress.
This is in order to avoid melt-down situations since the &lt;code>kube-controller-manager&lt;/code> uses in-cluster communication when talking to the &lt;code>kube-apiserver&lt;/code>, i.e., it wouldn&amp;rsquo;t be affected if the external access to the &lt;code>kube-apiserver&lt;/code> is interrupted for whatever reason.
The &lt;code>kubelet&lt;/code>s on the shoot worker nodes, however, would indeed be affected since they typically run in different networks and use the external ingress when talking to the &lt;code>kube-apiserver&lt;/code>.
Hence, without scaling down &lt;code>kube-controller-manager&lt;/code>, the nodes might be marked as &lt;code>NotReady&lt;/code> and eventually replaced (since the &lt;code>kubelet&lt;/code>s cannot report their status anymore).
To prevent such unnecessary turbulences, &lt;code>kube-controller-manager&lt;/code> is being scaled down until the external ingress becomes available again.&lt;/p>
&lt;p>It can be enabled/disabled via the &lt;code>.spec.settings.dependencyWatchdog.probe.enabled&lt;/code> field.
It defaults to &lt;code>true&lt;/code>.&lt;/p>
&lt;h2 id="reserve-excess-capacity">Reserve Excess Capacity&lt;/h2>
&lt;p>If the excess capacity reservation is enabled then the Gardenlet will deploy a special &lt;code>Deployment&lt;/code> into the &lt;code>garden&lt;/code> namespace of the seed cluster.
This &lt;code>Deployment&lt;/code>&amp;rsquo;s pod template has only one container, the &lt;code>pause&lt;/code> container, which simply runs in an infinite loop.
The priority of the deployment is very low, so any other pod will preempt these &lt;code>pause&lt;/code> pods.
This is especially useful if new shoot control planes are created in the seed.
In case the seed cluster runs at its capacity then there is no waiting time required during the scale-up.
Instead, the low-priority &lt;code>pause&lt;/code> pods will be preempted and allow newly created shoot control plane pods to be scheduled fast.
In the meantime, the cluster-autoscaler will trigger the scale-up because the preempted &lt;code>pause&lt;/code> pods want to run again.
However, this delay doesn&amp;rsquo;t affect the important shoot control plane pods which will improve the user experience.&lt;/p>
&lt;p>It can be enabled/disabled via the &lt;code>.spec.settings.excessCapacityReservation.enabled&lt;/code> field.
It defaults to &lt;code>true&lt;/code>.&lt;/p>
&lt;h2 id="scheduling">Scheduling&lt;/h2>
&lt;p>By default, the Gardener Scheduler will consider all seed clusters when a new shoot cluster shall be created.
However, administrators/operators might want to exclude some of them from being considered by the scheduler.
Therefore, seed clusters can be marked as &amp;ldquo;invisible&amp;rdquo;.
In this case, the scheduler simply ignores them as if they wouldn&amp;rsquo;t exist.
Shoots can still use the invisible seed but only by explicitly specifying the name in their &lt;code>.spec.seedName&lt;/code> field.&lt;/p>
&lt;p>Seed clusters can be marked visible/invisible via the &lt;code>.spec.settings.scheduling.visible&lt;/code> field.
It defaults to &lt;code>true&lt;/code>.&lt;/p>
&lt;p>ℹ️ In previous Gardener versions (&amp;lt; 1.5) these settings were controlled via taint keys (&lt;code>seed.gardener.cloud/{disable-capacity-reservation,invisible}&lt;/code>).
The taint keys are no longer supported and removed in version 1.12.
The rationale behind it is the implementation of tolerations similar to Kubernetes tolerations.
More information about it can be found in &lt;a href="https://github.com/gardener/gardener/issues/2193">#2193&lt;/a>.&lt;/p>
&lt;h2 id="load-balancer-services">Load Balancer Services&lt;/h2>
&lt;p>Gardener creates certain Kubernetes &lt;code>Service&lt;/code> objects of type &lt;code>LoadBalancer&lt;/code> in the seed cluster.
Most prominently, they are used for exposing the shoot control planes, namely the kube-apiserver of the shoot clusters.
In most cases, the cloud-controller-manager (responsible for managing these load balancers on the respective underlying infrastructure) supports certain customization and settings via annotations.
&lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer">This document&lt;/a> provides a good overview and many examples.&lt;/p>
&lt;p>By setting the &lt;code>.spec.settings.loadBalancerServices.annotations&lt;/code> field the Gardener administrator can specify a list of annotations which will be injected into the &lt;code>Service&lt;/code>s of type &lt;code>LoadBalancer&lt;/code>.&lt;/p>
&lt;h3 id="external-traffic-policy">External Traffic Policy&lt;/h3>
&lt;p>Setting the &lt;a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip">external traffic policy&lt;/a> to &lt;code>Local&lt;/code> can be beneficial as it
preserves the source IP address of client requests. In addition to that, it removes one hop in the data path and hence reduces request latency. On some cloud infrastructures, it can furthermore be
used in conjunction with &lt;code>Service&lt;/code> annotations as described above to prevent cross-zonal traffic from the load balancer to the backend pod.&lt;/p>
&lt;p>The default external traffic policy is &lt;code>Cluster&lt;/code> meaning that all traffic from the load balancer will be sent to any cluster node, which then itself will redirect the traffic to the actual receiving
pod. This approach adds a node to the data path, may cross the zone boundaries twice and replaces the source IP with one of the cluster nodes.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/external-traffic-policy-cluster_350bbe.png" alt="External Traffic Policy Cluster">&lt;/p>
&lt;p>Using external traffic policy &lt;code>Local&lt;/code> drops the additional node, i.e. only cluster nodes with corresponding backend pods will be in the list of backends of the load balancer. However, this has
multiple implications. The health check port in this scenario is exposed by &lt;code>kube-proxy&lt;/code> , i.e. if &lt;code>kube-proxy&lt;/code> is not working on a node a corresponding pod on the node will not receive traffic from
the load balancer as the load balancer will see a failing health check. (This is quite different from ordinary service routing where &lt;code>kube-proxy&lt;/code> is only responsible for setup, but does not need to
run for its operation.) Furthermore, load balancing may become imbalanced if multiple pods run on the same node because load balancers will split the load equally among the nodes and not among the
pods. This is mitigated by corresponding node anti affinities.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/external-traffic-policy-local_b36f63.png" alt="External Traffic Policy Local">&lt;/p>
&lt;p>Operators need to take these implications into account when considering switching external traffic policy to &lt;code>Local&lt;/code>.&lt;/p>
&lt;h3 id="zone-specific-settings">Zone-specific Settings&lt;/h3>
&lt;p>In case a seed cluster is configured to use multiple zones via &lt;code>.spec.provider.zones&lt;/code>, it may be necessary to configure the load balancers in individual zones in different way, e.g. by utilizing
different annotations. One reason may be to reduce cross-zonal traffic and have zone-specific load balancers in place. Zone-specific load balancers may then be bound to zone-specific subnets or
availability zones in the cloud infrastructure.&lt;/p>
&lt;p>Besides the load balancer annotations, it is also possible to set the &lt;a href="#external-traffic-policy">external traffic policy&lt;/a> for each zone-specific load balancer individually.&lt;/p>
&lt;h2 id="vertical-pod-autoscaler">Vertical Pod Autoscaler&lt;/h2>
&lt;p>Gardener heavily relies on the Kubernetes &lt;a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">&lt;code>vertical-pod-autoscaler&lt;/code> component&lt;/a>.
By default, the seed controller deploys the VPA components into the &lt;code>garden&lt;/code> namespace of the respective seed clusters.
In case you want to manage the VPA deployment on your own or have a custom one then you might want to disable the automatic deployment of Gardener.
Otherwise, you might end up with two VPAs which will cause erratic behaviour.
By setting the &lt;code>.spec.settings.verticalPodAutoscaler.enabled=false&lt;/code> you can disable the automatic deployment.&lt;/p>
&lt;p>⚠️ In any case, there must be a VPA available for your seed cluster. Using a seed without VPA is not supported.&lt;/p>
&lt;h2 id="owner-checks">Owner Checks&lt;/h2>
&lt;p>When a shoot is scheduled to a seed and actually reconciled, Gardener appoints the seed as the current &amp;ldquo;owner&amp;rdquo; of the shoot by creating a special &amp;ldquo;owner DNS record&amp;rdquo; and checking against it if the seed still owns the shoot in order to guard against &amp;ldquo;split brain scenario&amp;rdquo; during control plane migration, as described in &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/17-shoot-control-plane-migration-bad-case.md">GEP-17 Shoot Control Plane Migration &amp;ldquo;Bad Case&amp;rdquo; Scenario&lt;/a>.
This mechanism relies on the DNS resolution of TXT DNS records being possible and highly reliable, since if the owner check fails the shoot will be effectively disabled for the duration of the failure.
In environments where resolving TXT DNS records is either not possible or not considered reliable enough, it may be necessary to disable the owner check mechanism, in order to avoid shoots failing to reconcile or temporary outages due to transient DNS failures.
By setting the &lt;code>.spec.settings.ownerChecks.enabled=false&lt;/code> (default is &lt;code>true&lt;/code>) the creation and checking of owner DNS records can be disabled for all shoots scheduled on this seed. Note that if owner checks are disabled, migrating shoots scheduled on this seed to other seeds should be considered unsafe, and in the future will be disabled as well.&lt;/p></description></item><item><title>Docs: Service Account Manager</title><link>https://gardener.cloud/docs/gardener/usage/service-account-manager/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/service-account-manager/</guid><description>
&lt;h1 id="service-account-manager">Service Account Manager&lt;/h1>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>With Gardener &lt;code>v1.47&lt;/code> a new role called &lt;code>serviceaccountmanager&lt;/code> was introduced. This role allows to fully manage &lt;code>ServiceAccount&lt;/code>&amp;rsquo;s in the project namespace and request tokens for them. This is the preferred way of managing the access to a project namespace as it aims to replace the usage of the default &lt;code>ServiceAccount&lt;/code> secrets that will no longer be generated automatically with Kubernetes &lt;code>v1.24+&lt;/code>.&lt;/p>
&lt;h2 id="actions">Actions&lt;/h2>
&lt;p>Once assigned the &lt;code>serviceaccountmanager&lt;/code> role, a user can create/update/delete &lt;code>ServiceAccount&lt;/code>s in the project namespace.&lt;/p>
&lt;h3 id="create-a-service-account">Create a Service Account&lt;/h3>
&lt;p>In order to create a &lt;code>ServiceAccount&lt;/code> named &amp;ldquo;robot-user&amp;rdquo;, run the following &lt;code>kubectl&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-code" data-lang="code">kubectl -n project-abc create sa robot-user
&lt;/code>&lt;/pre>&lt;h3 id="request-a-token-for-a-service-account">Request a Token for a Service Account&lt;/h3>
&lt;p>A token for the &amp;ldquo;robot-user&amp;rdquo; &lt;code>ServiceAccount&lt;/code> can be requested via the &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-request-v1/">TokenRequest API&lt;/a> in several ways:&lt;/p>
&lt;ul>
&lt;li>using &lt;code>kubectl&lt;/code> &amp;gt;= v1.24&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n project-abc create token robot-user --duration=3600s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>using &lt;code>kubectl&lt;/code> &amp;lt; v1.24&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat &lt;span style="color:#a31515">&amp;lt;&amp;lt;EOF | kubectl create -f - --raw /api/v1/namespaces/project-abc/serviceaccounts/robot-user/token
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">{
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;apiVersion&amp;#34;: &amp;#34;authentication.k8s.io/v1&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;kind&amp;#34;: &amp;#34;TokenRequest&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;spec&amp;#34;: {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;expirationSeconds&amp;#34;: 3600
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>directly calling the Kubernetes HTTP API&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>curl -X POST https://api.gardener/api/v1/namespaces/project-abc/serviceaccounts/robot-user/token &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> -H &lt;span style="color:#a31515">&amp;#34;Authorization: Bearer &amp;lt;auth-token&amp;gt;&amp;#34;&lt;/span> &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> -H &lt;span style="color:#a31515">&amp;#34;Content-Type: application/json&amp;#34;&lt;/span> &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> -d &lt;span style="color:#a31515">&amp;#39;{
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;apiVersion&amp;#34;: &amp;#34;authentication.k8s.io/v1&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;kind&amp;#34;: &amp;#34;TokenRequest&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;spec&amp;#34;: {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;expirationSeconds&amp;#34;: 3600
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> }&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Mind that the returned token is not stored within the Kubernetes cluster, will be valid for &lt;code>3600&lt;/code> seconds, and will be invalidated if the &amp;ldquo;robot-user&amp;rdquo; &lt;code>ServiceAccount&lt;/code> is deleted. Although &lt;code>expirationSeconds&lt;/code> can be modified depending on the needs, the returned token&amp;rsquo;s validity will not exceed the configured &lt;code>service-account-max-token-expiration&lt;/code> duration for the garden cluster. It is advised that the actual &lt;code>expirationTimestamp&lt;/code> is verified so that expectations are met. This can be done by asserting the &lt;code>expirationTimestamp&lt;/code> in the &lt;code>TokenRequestStatus&lt;/code> or the &lt;code>exp&lt;/code> claim in the token itself.&lt;/p>
&lt;h3 id="delete-a-service-account">Delete a Service Account&lt;/h3>
&lt;p>In order to delete the &lt;code>ServiceAccount&lt;/code> named &amp;ldquo;robot-user&amp;rdquo;, run the following &lt;code>kubectl&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-code" data-lang="code">kubectl -n project-abc delete sa robot-user
&lt;/code>&lt;/pre>&lt;p>This will invalidate all existing tokens for the &amp;ldquo;robot-user&amp;rdquo; &lt;code>ServiceAccount&lt;/code>.&lt;/p></description></item><item><title>Docs: Shoot Access</title><link>https://gardener.cloud/docs/gardener/usage/shoot_access/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_access/</guid><description>
&lt;h1 id="accessing-shoot-clusters">Accessing Shoot Clusters&lt;/h1>
&lt;p>After creation of a shoot cluster, end-users require a &lt;code>kubeconfig&lt;/code> to access it. There are several options available to get to such &lt;code>kubeconfig&lt;/code>.&lt;/p>
&lt;h2 id="static-token-kubeconfig">Static Token Kubeconfig&lt;/h2>
&lt;p>This &lt;code>kubeconfig&lt;/code> contains a &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#static-token-file">static token&lt;/a> and provides &lt;code>cluster-admin&lt;/code> privileges.
It is created by default and persisted in the &lt;code>&amp;lt;shoot-name&amp;gt;.kubeconfig&lt;/code> secret in the project namespace in the garden cluster.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enableStaticTokenKubeconfig: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It is &lt;strong>not&lt;/strong> the recommended method to access the shoot cluster as the static token &lt;code>kubeconfig&lt;/code> has some security flaws associated with it:&lt;/p>
&lt;ul>
&lt;li>The static token in the &lt;code>kubeconfig&lt;/code> doesn&amp;rsquo;t have any expiration date. Read &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_credentials_rotation/#kubeconfig">this document&lt;/a> to learn how to rotate the static token.&lt;/li>
&lt;li>The static token doesn&amp;rsquo;t have any user identity associated with it. The user in that token will always be &lt;code>system:cluster-admin&lt;/code> irrespective of the person accessing the cluster. Hence, it is impossible to audit the events in cluster.&lt;/li>
&lt;/ul>
&lt;p>When &lt;code>enableStaticTokenKubeconfig&lt;/code> field is not explicitly set in the Shoot spec:&lt;/p>
&lt;ul>
&lt;li>for Shoot clusters using Kubernetes version &amp;lt; 1.26 the field is defaulted to &lt;code>true&lt;/code>.&lt;/li>
&lt;li>for Shoot clusters using Kubernetes version &amp;gt;= 1.26 the field is defaulted to &lt;code>false&lt;/code>.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Note: Starting with Kubernetes 1.27, the &lt;code>enableStaticTokenKubeconfig&lt;/code> field will be locked to &lt;code>false&lt;/code>. The &lt;a href="#shootsadminkubeconfig-subresource">&lt;code>shoots/adminkubeconfig&lt;/code> subresource&lt;/a> should be used instead.&lt;/p>
&lt;/blockquote>
&lt;h2 id="shootsadminkubeconfig-subresource">&lt;code>shoots/adminkubeconfig&lt;/code> subresource&lt;/h2>
&lt;p>The &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/16-adminkubeconfig-subresource.md">&lt;code>shoots/adminkubeconfig&lt;/code>&lt;/a> subresource allows users to dynamically generate temporary &lt;code>kubeconfig&lt;/code>s that can be used to access shoot cluster with &lt;code>cluster-admin&lt;/code> privileges. The credentials associated with this &lt;code>kubeconfig&lt;/code> are client certificates which have a very short validity and must be renewed before they expire (by calling the subresource endpoint again).&lt;/p>
&lt;p>The username associated with such &lt;code>kubeconfig&lt;/code> will be the same which is used for authenticating to the Gardener API. Apart from this advantage, the created &lt;code>kubeconfig&lt;/code> will not be persisted anywhere.&lt;/p>
&lt;p>In order to request such a &lt;code>kubeconfig&lt;/code>, you can run the following commands:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>export NAMESPACE=my-namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export SHOOT_NAME=my-shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl create &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> -f &amp;lt;path&amp;gt;/&amp;lt;to&amp;gt;/kubeconfig-request.json &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --raw /apis/core.gardener.cloud/v1beta1/namespaces/&lt;span style="color:#a31515">${&lt;/span>NAMESPACE&lt;span style="color:#a31515">}&lt;/span>/shoots/&lt;span style="color:#a31515">${&lt;/span>SHOOT_NAME&lt;span style="color:#a31515">}&lt;/span>/adminkubeconfig | jq -r &lt;span style="color:#a31515">&amp;#34;.status.kubeconfig&amp;#34;&lt;/span> | base64 -d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here, the &lt;code>kubeconfig-request.json&lt;/code> has the following content:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;apiVersion&amp;#34;: &lt;span style="color:#a31515">&amp;#34;authentication.gardener.cloud/v1alpha1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;kind&amp;#34;: &lt;span style="color:#a31515">&amp;#34;AdminKubeconfigRequest&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;spec&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;expirationSeconds&amp;#34;: 1000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>The &lt;a href="https://github.com/gardener/gardenctl-v2/">&lt;code>gardenctl-v2&lt;/code>&lt;/a> tool makes it easy to target shoot clusters and automatically renews such &lt;code>kubeconfig&lt;/code> when required.&lt;/p>
&lt;/blockquote>
&lt;h2 id="openid-connect">OpenID Connect&lt;/h2>
&lt;p>The &lt;code>kube-apiserver&lt;/code> of shoot clusters can be provided with &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens">OpenID Connect configuration&lt;/a> via the Shoot spec:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidcConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It is the end-user&amp;rsquo;s responsibility to incorporate the OpenID Connect configurations in &lt;code>kubeconfig&lt;/code> for accessing the cluster (i.e., Gardener will not automatically generate &lt;code>kubeconfig&lt;/code> based on these OIDC settings).
The recommended way is using the &lt;code>kubectl&lt;/code> plugin called &lt;a href="https://github.com/int128/kubelogin">&lt;code>kubectl oidc-login&lt;/code>&lt;/a> for OIDC authentication.&lt;/p>
&lt;p>If you want to use the same OIDC configuration for all your shoots by default then you can use the &lt;code>ClusterOpenIDConnectPreset&lt;/code> and &lt;code>OpenIDConnectPreset&lt;/code> API resources. They allow defaulting the &lt;code>.spec.kubernetes.kubeAPIServer.oidcConfig&lt;/code> fields for newly created &lt;code>Shoot&lt;/code>s such that you don&amp;rsquo;t have to repeat yourself every time (similar to &lt;code>PodPreset&lt;/code> resources in Kubernetes).
&lt;code>ClusterOpenIDConnectPreset&lt;/code> specified OIDC configuration applies to &lt;code>Projects&lt;/code> and &lt;code>Shoots&lt;/code> cluster-wide (hence, only available to Gardener operators) while &lt;code>OpenIDConnectPreset&lt;/code> is &lt;code>Project&lt;/code>-scoped.
Shoots have to &amp;ldquo;opt-in&amp;rdquo; for such defaulting by using the &lt;code>oidc=enable&lt;/code> label.&lt;/p>
&lt;p>For further information on &lt;code>(Cluster)OpenIDConnectPreset&lt;/code>, refer to &lt;a href="https://gardener.cloud/docs/gardener/usage/openidconnect-presets/">this document&lt;/a>.&lt;/p></description></item><item><title>Docs: Shoot Auditpolicy</title><link>https://gardener.cloud/docs/gardener/usage/shoot_auditpolicy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_auditpolicy/</guid><description>
&lt;h1 id="audit-a-kubernetes-cluster">Audit a Kubernetes Cluster&lt;/h1>
&lt;p>The shoot cluster is a kubernetes cluster and its &lt;code>kube-apiserver&lt;/code> handles the audit events. In order to define which audit events must be logged, a proper audit policy file must be passed to the kubernetes API server. You could find more information about auditing a kubernetes cluster &lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">here&lt;/a>.&lt;/p>
&lt;h2 id="default-audit-policy">Default Audit Policy&lt;/h2>
&lt;p>By default, the Gardener will deploy the shoot cluster with audit policy defined in the &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/operation/botanist/component/kubeapiserver/secrets.go">kube-apiserver package&lt;/a>.&lt;/p>
&lt;h2 id="custom-audit-policy">Custom Audit Policy&lt;/h2>
&lt;p>If you need specific audit policy for your shoot cluster, then you could deploy the required audit policy in the garden cluster as &lt;code>ConfigMap&lt;/code> resource and set up your shoot to refer this &lt;code>ConfigMap&lt;/code>. Note, the policy must be stored under the key &lt;code>policy&lt;/code> in the data section of the &lt;code>ConfigMap&lt;/code>.&lt;/p>
&lt;p>For example, deploy the auditpolicy &lt;code>ConfigMap&lt;/code> in the same namespace as your &lt;code>Shoot&lt;/code> resource:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f example/95-configmap-custom-audit-policy.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>then set your shoot to refer that &lt;code>ConfigMap&lt;/code> (only related fields are shown):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeAPIServer:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> auditConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> auditPolicy:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> configMapRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: auditpolicy
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The Gardener validate the &lt;code>Shoot&lt;/code> resource to refer only existing &lt;code>ConfigMap&lt;/code> containing valid audit policy, and rejects the &lt;code>Shoot&lt;/code> on failure.
If you want to switch back to the default audit policy, you have to remove the section&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>auditPolicy:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> configMapRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: &amp;lt;configmap-name&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>from the shoot spec.&lt;/p>
&lt;h2 id="rolling-out-changes-to-the-audit-policy">Rolling Out Changes to the Audit Policy&lt;/h2>
&lt;p>Gardener is not automatically rolling out changes to the Audit Policy to minimize the amount of Shoot reconciliations in order to prevent cloud provider rate limits, etc.
Gardener will pick up the changes on the next reconciliation of Shoots referencing the Audit Policy ConfigMap.
If users want to immediately rollout Audit Policy changes, they can manually trigger a Shoot reconciliation as described in &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_operations/#immediate-reconciliation">triggering an immediate reconciliation&lt;/a>.
This is similar to changes to the cloud provider secret referenced by Shoots.&lt;/p></description></item><item><title>Docs: Shoot Autoscaling</title><link>https://gardener.cloud/docs/gardener/usage/shoot_autoscaling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_autoscaling/</guid><description>
&lt;h1 id="auto-scaling-in-shoot-clusters">Auto-Scaling in Shoot Clusters&lt;/h1>
&lt;p>There are two parts that relate to auto-scaling in Kubernetes clusters in general:&lt;/p>
&lt;ul>
&lt;li>Horizontal node auto-scaling, i.e., dynamically adding and removing worker nodes&lt;/li>
&lt;li>Vertical pod auto-scaling, i.e., dynamically raising or shrinking the resource requests/limits of pods&lt;/li>
&lt;/ul>
&lt;p>This document provides an overview of both scenarios.&lt;/p>
&lt;h2 id="horizontal-node-auto-scaling">Horizontal Node Auto-Scaling&lt;/h2>
&lt;p>Every shoot cluster that has at least one worker pool with &lt;code>minimum &amp;lt; maximum&lt;/code> nodes configuration will get a &lt;code>cluster-autoscaler&lt;/code> deployment.
Gardener is leveraging the upstream community Kubernetes &lt;a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">&lt;code>cluster-autoscaler&lt;/code> component&lt;/a>.
We have forked it to &lt;a href="https://github.com/gardener/autoscaler/">gardener/autoscaler&lt;/a> so that it supports the way how Gardener manages the worker nodes (leveraging &lt;a href="https://github.com/gardener/machine-controller-manager">gardener/machine-controller-manager&lt;/a>).
However, we have not touched the logic how it performs auto-scaling decisions.
Consequently, please refer to the &lt;a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#faqdocumentation">offical documentation&lt;/a> for this component.&lt;/p>
&lt;p>The &lt;code>Shoot&lt;/code> API allows to configure a few flags of the &lt;code>cluster-autoscaler&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>.spec.kubernetes.clusterAutoscaler.ScaleDownDelayAfterAdd&lt;/code> defines how long after scale up that scale down evaluation resumes (default: &lt;code>1h&lt;/code>).&lt;/li>
&lt;li>&lt;code>.spec.kubernetes.clusterAutoscaler.ScaleDownDelayAfterDelete&lt;/code> defines how long after node deletion that scale down evaluation resumes (defaults to &lt;code>ScanInterval&lt;/code>).&lt;/li>
&lt;li>&lt;code>.spec.kubernetes.clusterAutoscaler.ScaleDownDelayAfterFailure&lt;/code> defines how long after scale down failure that scale down evaluation resumes (default: &lt;code>3m&lt;/code>).&lt;/li>
&lt;li>&lt;code>.spec.kubernetes.clusterAutoscaler.ScaleDownUnneededTime&lt;/code> defines how long a node should be unneeded before it is eligible for scale down (default: &lt;code>30m&lt;/code>).&lt;/li>
&lt;li>&lt;code>.spec.kubernetes.clusterAutoscaler.ScaleDownUtilizationThreshold&lt;/code> defines the threshold under which a node is being removed (default: &lt;code>0.5&lt;/code>).&lt;/li>
&lt;li>&lt;code>.spec.kubernetes.clusterAutoscaler.ScanInterval&lt;/code> defines how often cluster is reevaluated for scale up or down (default: &lt;code>10s&lt;/code>).&lt;/li>
&lt;li>&lt;code>.spec.kubernetes.clusterAutoscaler.IgnoreTaints&lt;/code> specifies a list of taint keys to ignore in node templates when considering to scale a node group (default: &lt;code>nil&lt;/code>).&lt;/li>
&lt;/ul>
&lt;h2 id="vertical-pod-auto-scaling">Vertical Pod Auto-Scaling&lt;/h2>
&lt;p>This form of auto-scaling is not enabled by default and must be explicitly enabled in the &lt;code>Shoot&lt;/code> by setting &lt;code>.spec.kubernetes.verticalPodAutoscaler.enabled=true&lt;/code>.
The reason is that it was only introduced lately, and some end-users might have already deployed their own VPA into their clusters, i.e., enabling it by default would interfere with such custom deployments and lead to issues, eventually.&lt;/p>
&lt;p>Gardener is also leveraging an upstream community tool, i.e., the Kubernetes &lt;a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">&lt;code>vertical-pod-autoscaler&lt;/code> component&lt;/a>.
If enabled, Gardener will deploy it as part of the control plane into the seed cluster.
It will also be used for the vertical autoscaling of Gardener&amp;rsquo;s system components deployed into the &lt;code>kube-system&lt;/code> namespace of shoot clusters, for example, &lt;code>kube-proxy&lt;/code> or &lt;code>metrics-server&lt;/code>.&lt;/p>
&lt;p>You might want to refer to the &lt;a href="https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md">official documentation&lt;/a> for this component to get more information how to use it.&lt;/p>
&lt;p>The &lt;code>Shoot&lt;/code> API allows to configure a few flags of the &lt;code>vertical-pod-autoscaler&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>.spec.kubernetes.verticalPodAutoscaler.evictAfterOOMThreshold&lt;/code> defines the threshold that will lead to pod eviction in case it OOMed in less than the given threshold since its start and if it has only one container (default: &lt;code>10m0s&lt;/code>).&lt;/li>
&lt;li>&lt;code>.spec.kubernetes.verticalPodAutoscaler.evictionRateBurst&lt;/code> defines the burst of pods that can be evicted (default: &lt;code>1&lt;/code>).&lt;/li>
&lt;li>&lt;code>.spec.kubernetes.verticalPodAutoscaler.evictionRateLimit&lt;/code> defines the number of pods that can be evicted per second. A rate limit set to 0 or -1 will disable the rate limiter (default: &lt;code>-1&lt;/code>).&lt;/li>
&lt;li>&lt;code>.spec.kubernetes.verticalPodAutoscaler.evictionTolerance&lt;/code> defines the fraction of replica count that can be evicted for update in case more than one pod can be evicted (default: &lt;code>0.5&lt;/code>).&lt;/li>
&lt;li>&lt;code>.spec.kubernetes.verticalPodAutoscaler.recommendationMarginFraction&lt;/code> is the fraction of usage added as the safety margin to the recommended request (default: &lt;code>0.15&lt;/code>).&lt;/li>
&lt;li>&lt;code>.spec.kubernetes.verticalPodAutoscaler.updaterInterval&lt;/code> is the interval how often the updater should run (default: &lt;code>1m0s&lt;/code>).&lt;/li>
&lt;li>&lt;code>.spec.kubernetes.verticalPodAutoscaler.recommenderInterval&lt;/code> is the interval how often metrics should be fetched (default: &lt;code>1m0s&lt;/code>).&lt;/li>
&lt;/ul>
&lt;p>⚠️ Please note that if you disable the VPA again then the related &lt;code>CustomResourceDefinition&lt;/code>s will remain in your shoot cluster (although, nobody will act on them).
This will also keep all existing &lt;code>VerticalPodAutoscaler&lt;/code> objects in the system, including those that might be created by you. You can delete the &lt;code>CustomResourceDefinition&lt;/code>s yourself using &lt;code>kubectl delete crd&lt;/code> if you want to get rid of them.&lt;/p></description></item><item><title>Docs: Shoot Cleanup</title><link>https://gardener.cloud/docs/gardener/usage/shoot_cleanup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_cleanup/</guid><description>
&lt;h1 id="cleanup-of-shoot-clusters-in-deletion">Cleanup of Shoot clusters in deletion&lt;/h1>
&lt;p>When a shoot cluster is deleted then Gardener tries to gracefully remove most of the Kubernetes resources inside the cluster.
This is to prevent that any infrastructure or other artefacts remain after the shoot deletion.&lt;/p>
&lt;p>The cleanup is performed in four steps.
Some resources are deleted with a grace period, and all resources are forcefully deleted (by removing blocking finalizers) after some time to not block the cluster deletion entirely.&lt;/p>
&lt;p>&lt;strong>Cleanup steps:&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>All &lt;code>ValidatingWebhookConfiguration&lt;/code>s and &lt;code>MutatingWebhookConfiguration&lt;/code>s are deleted with a &lt;code>5m&lt;/code> grace period. Forceful finalization happens after &lt;code>5m&lt;/code>.&lt;/li>
&lt;li>All &lt;code>APIService&lt;/code>s and &lt;code>CustomResourceDefinition&lt;/code>s are deleted with a &lt;code>5m&lt;/code> grace period. Forceful finalization happens after &lt;code>1h&lt;/code>.&lt;/li>
&lt;li>All &lt;code>CronJob&lt;/code>s, &lt;code>DaemonSet&lt;/code>s, &lt;code>Deployment&lt;/code>s, &lt;code>Ingress&lt;/code>s, &lt;code>Job&lt;/code>s, &lt;code>Pod&lt;/code>s, &lt;code>ReplicaSet&lt;/code>s, &lt;code>ReplicationController&lt;/code>s, &lt;code>Service&lt;/code>s, &lt;code>StatefulSet&lt;/code>s, &lt;code>PersistentVolumeClaim&lt;/code>s are deleted with a &lt;code>5m&lt;/code> grace period. Forceful finalization happens after &lt;code>5m&lt;/code>.
&lt;blockquote>
&lt;p>If the &lt;code>Shoot&lt;/code> is annotated with &lt;code>shoot.gardener.cloud/skip-cleanup=true&lt;/code> then only &lt;code>Service&lt;/code>s and &lt;code>PersistentVolumeClaim&lt;/code>s are considered.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>All &lt;code>VolumeSnapshot&lt;/code>s and &lt;code>VolumeSnapshotContent&lt;/code>s are deleted with a &lt;code>5m&lt;/code> grace period. Forceful finalization happens after &lt;code>1h&lt;/code>.&lt;/li>
&lt;li>All &lt;code>Namespace&lt;/code>s are deleted without any grace period. Forceful finalization happens after &lt;code>5m&lt;/code>.&lt;/li>
&lt;/ol>
&lt;p>It is possible to override the finalization grace periods via annotations on the &lt;code>Shoot&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>shoot.gardener.cloud/cleanup-webhooks-finalize-grace-period-seconds&lt;/code> (for the resources handled in step 1)&lt;/li>
&lt;li>&lt;code>shoot.gardener.cloud/cleanup-extended-apis-finalize-grace-period-seconds&lt;/code> (for the resources handled in step 2)&lt;/li>
&lt;li>&lt;code>shoot.gardener.cloud/cleanup-kubernetes-resources-finalize-grace-period-seconds&lt;/code> (for the resources handled in step 3)&lt;/li>
&lt;li>&lt;code>shoot.gardener.cloud/cleanup-namespaces-finalize-grace-period-seconds&lt;/code> (for the resources handled in step 4)&lt;/li>
&lt;/ul>
&lt;p>⚠️ If &lt;code>&amp;quot;0&amp;quot;&lt;/code> is provided then all resources are finalized immediately without waiting for any graceful deletion.
Please be aware that this might lead to orphaned infrastructure artefacts.&lt;/p>
&lt;h2 id="infrastructure-cleanup-wait-period">Infrastructure Cleanup Wait Period&lt;/h2>
&lt;p>After all above cleanup steps have been performed and the &lt;code>Infrastructure&lt;/code> extension resource has been deleted the gardenlet waits for a certain duration to allow controllers to properly cleanup infrastructure resources.&lt;/p>
&lt;p>By default, this duration is set to &lt;code>5m&lt;/code>. Only after this time has passed the shoot deletion flow continues with the entire tear-down of the remaining control plane components (including &lt;code>kube-apiserver&lt;/code>s, etc.).&lt;/p>
&lt;p>It is also possible to override this wait period via an annotations on the &lt;code>Shoot&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>shoot.gardener.cloud/infrastructure-cleanup-wait-period-seconds&lt;/code>&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>ℹ️️ All provided period values larger than the above mentioned defaults are ignored.&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: Shoot Credentials Rotation</title><link>https://gardener.cloud/docs/gardener/usage/shoot_credentials_rotation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_credentials_rotation/</guid><description>
&lt;h1 id="credentials-rotation-for-shoot-clusters">Credentials Rotation For Shoot Clusters&lt;/h1>
&lt;p>There are a lot of different credentials for &lt;code>Shoot&lt;/code>s to make sure that the various components can communicate with each other, and to make sure it is usable and operable.&lt;/p>
&lt;p>This page explains how the varieties of credentials can be rotated so that the cluster can be considered secure.&lt;/p>
&lt;h2 id="user-provided-credentials">User-Provided Credentials&lt;/h2>
&lt;h3 id="cloud-provider-keys">Cloud Provider Keys&lt;/h3>
&lt;p>End-users must provide credentials such that Gardener and Kubernetes controllers can communicate with the respective cloud provider APIs in order to perform infrastructure operations.
For example, Gardener uses them to setup and maintain the networks, security groups, subnets, etc., while the &lt;a href="https://kubernetes.io/docs/concepts/architecture/cloud-controller/">cloud-controller-manager&lt;/a> uses them to reconcile load balancers and routes, and the &lt;a href="https://kubernetes-csi.github.io/docs/">CSI controller&lt;/a> uses them to reconcile volumes and disks.&lt;/p>
&lt;p>Depending on the cloud provider, the required &lt;a href="https://github.com/gardener/gardener/blob/master/example/70-secret-provider.yaml">data keys of the &lt;code>Secret&lt;/code> differ&lt;/a>.
Please consult the documentation of the respective provider extension documentation to get to know the concrete data keys (e.g., &lt;a href="https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/usage-as-end-user/#provider-secret-data">this document for AWS&lt;/a>).&lt;/p>
&lt;p>&lt;strong>It is the responsibility of the end-user to regularly rotate those credentials.&lt;/strong>
The following steps are required to perform the rotation:&lt;/p>
&lt;ol>
&lt;li>Update the data in the &lt;code>Secret&lt;/code> with new credentials.&lt;/li>
&lt;li>⚠️ Wait until all &lt;code>Shoot&lt;/code>s using the &lt;code>Secret&lt;/code> are reconciled before you disable the old credentials in your cloud provider account! Otherwise, the &lt;code>Shoot&lt;/code>s will no longer work as expected. Check out &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_operations/#immediate-reconciliation">this document&lt;/a> to learn how to trigger a reconciliation of your &lt;code>Shoot&lt;/code>s.&lt;/li>
&lt;li>After all &lt;code>Shoot&lt;/code>s using the &lt;code>Secret&lt;/code> were reconciled, you can go ahead and deactivate the old credentials in your provider account.&lt;/li>
&lt;/ol>
&lt;h2 id="gardener-provided-credentials">Gardener-Provided Credentials&lt;/h2>
&lt;p>Below credentials are generated by Gardener when shoot clusters are being created.
Those include&lt;/p>
&lt;ul>
&lt;li>kubeconfig (if enabled)&lt;/li>
&lt;li>certificate authorities (and related server and client certificates)&lt;/li>
&lt;li>observability passwords for Grafana&lt;/li>
&lt;li>SSH key pair for worker nodes&lt;/li>
&lt;li>ETCD encryption key&lt;/li>
&lt;li>&lt;code>ServiceAccount&lt;/code> token signing key&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>🚨 There is no auto-rotation of those credentials, and it is the responsibility of the end-user to regularly rotate them.&lt;/strong>&lt;/p>
&lt;p>While it is possible to rotate them one by one, there is also a convenient method to combine the rotation of all of those credentials.
The rotation happens in two phases since it might be required to update some API clients (e.g., when CAs are rotated).
In order to start the rotation (first phase), you have to annotate the shoot with the &lt;code>rotate-credentials-start&lt;/code> operation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n &amp;lt;shoot-namespace&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=rotate-credentials-start
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>You can check the &lt;code>.status.credentials.rotation&lt;/code> field in the &lt;code>Shoot&lt;/code> to see when the rotation was last initiated and last completed.&lt;/p>
&lt;/blockquote>
&lt;p>Kindly consider the detailed descriptions below to learn how the rotation is performed and what your responsibilities are.
Please note that all respective individual actions apply for this combined rotation as well (e.g., worker nodes are rolled out in the first phase).&lt;/p>
&lt;p>You can complete the rotation (second phase) by annotating the shoot with the &lt;code>rotate-credentials-complete&lt;/code> operation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n &amp;lt;shoot-namespace&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=rotate-credentials-complete
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="kubeconfig">Kubeconfig&lt;/h3>
&lt;p>If the &lt;code>.spec.kubernetes.enableStaticTokenKubeconfig&lt;/code> field is set to &lt;code>true&lt;/code> (default) then Gardener generates a &lt;code>kubeconfig&lt;/code> with &lt;code>cluster-admin&lt;/code> privileges for the &lt;code>Shoot&lt;/code>s containing credentials for communication with the &lt;code>kube-apiserver&lt;/code> (see &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_access/#static-token-kubeconfig">this document&lt;/a> for more information).&lt;/p>
&lt;p>This &lt;code>Secret&lt;/code> is stored with name &lt;code>&amp;lt;shoot-name&amp;gt;.kubeconfig&lt;/code> in the project namespace in the garden cluster and has multiple data keys:&lt;/p>
&lt;ul>
&lt;li>&lt;code>kubeconfig&lt;/code>: the completed kubeconfig&lt;/li>
&lt;li>&lt;code>ca.crt&lt;/code>: the CA bundle for establishing trust to the API server (same as in the &lt;a href="#cluster-certificate-authority-bundle">Cluster CA bundle secret&lt;/a>)&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>&lt;code>Shoots&lt;/code> created with Gardener &amp;lt;= 0.28 used to have a &lt;code>kubeconfig&lt;/code> based on a client certificate instead of a static token. With the first kubeconfig rotation, such clusters will get a static token as well.&lt;/p>
&lt;p>⚠️ This does not invalidate the old client certificate. In order to do this, you should perform a rotation of the CAs (see section below).&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>It is the responsibility of the end-user to regularly rotate those credentials (or disable this &lt;code>kubeconfig&lt;/code> entirely).&lt;/strong>
In order to rotate the &lt;code>token&lt;/code> in this &lt;code>kubeconfig&lt;/code>, annotate the &lt;code>Shoot&lt;/code> with &lt;code>gardener.cloud/operation=rotate-kubeconfig-credentials&lt;/code>.
This operation is not allowed for &lt;code>Shoot&lt;/code>s that are already marked for deletion.
Please note that only the token (and basic auth password, if enabled) are exchanged.
The CA certificate remains the same (see section below for information about the rotation).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n &amp;lt;shoot-namespace&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=rotate-kubeconfig-credentials
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>You can check the &lt;code>.status.credentials.rotation.kubeconfig&lt;/code> field in the &lt;code>Shoot&lt;/code> to see when the rotation was last initiated and last completed.&lt;/p>
&lt;/blockquote>
&lt;h3 id="certificate-authorities">Certificate Authorities&lt;/h3>
&lt;p>Gardener generates several certificate authorities (CAs) to ensure secured communication between the various components and actors.
Most of those CAs are used for internal communication (e.g., &lt;code>kube-apiserver&lt;/code> talks to etcd, &lt;code>vpn-shoot&lt;/code> talks to the &lt;code>vpn-seed-server&lt;/code>, &lt;code>kubelet&lt;/code> talks to &lt;code>kube-apiserver&lt;/code> etc.).
However, there is also the &amp;ldquo;cluster CA&amp;rdquo; which is part of all &lt;code>kubeconfig&lt;/code>s and used to sign the server certificate exposed by the &lt;code>kube-apiserver&lt;/code>.&lt;/p>
&lt;p>Gardener populates a &lt;code>Secret&lt;/code> with name &lt;code>&amp;lt;shoot-name&amp;gt;.ca-cluster&lt;/code> in the project namespace in the garden cluster which contains the following data keys:&lt;/p>
&lt;ul>
&lt;li>&lt;code>ca.crt&lt;/code>: the CA bundle of the cluster&lt;/li>
&lt;/ul>
&lt;p>This bundle contains one or multiple CAs which are used for signing serving certificates of the &lt;code>Shoot&lt;/code>&amp;rsquo;s API server.
Hence, the certificates contained in this &lt;code>Secret&lt;/code> can be used to verify the API server&amp;rsquo;s identity when communicating with its public endpoint (e.g. as &lt;code>certificate-authority-data&lt;/code> in a &lt;code>kubeconfig&lt;/code>).
This is the same certificate that is also contained in the &lt;code>kubeconfig&lt;/code>&amp;rsquo;s &lt;code>certificate-authority-data&lt;/code> field.&lt;/p>
&lt;blockquote>
&lt;p>&lt;code>Shoot&lt;/code>s created with Gardener &amp;gt;= v1.45 have a dedicated client CA which verifies the legitimacy of client certificates. For older &lt;code>Shoot&lt;/code>s, the client CA is equal to the cluster CA. With the first CA rotation, such clusters will get a dedicated client CA as well.&lt;/p>
&lt;/blockquote>
&lt;p>All of the certificates are valid for 10 years.
Since it requires adaptation for the consumers of the &lt;code>Shoot&lt;/code>, there is no automatic rotation and &lt;strong>it is the responsibility of the end-user to regularly rotate the CA certificates.&lt;/strong>&lt;/p>
&lt;p>The rotation happens in three stages (see also &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/18-shoot-CA-rotation.md">GEP-18&lt;/a> for the full details):&lt;/p>
&lt;ul>
&lt;li>In stage one, new CAs are created and added to the bundle (together with the old CAs). Client certificates are re-issued immediately.&lt;/li>
&lt;li>In stage two, end-users update all cluster API clients that communicate with the control plane.&lt;/li>
&lt;li>In stage three, the old CAs are dropped from the bundle and server certificate are re-issued.&lt;/li>
&lt;/ul>
&lt;p>Technically, the &lt;code>Preparing&lt;/code> phase indicates stage one.
Once it is completed, the &lt;code>Prepared&lt;/code> phase indicates readiness for stage two.
The &lt;code>Completing&lt;/code> phase indicates stage three, and the &lt;code>Completed&lt;/code> phase states that the rotation process has finished.&lt;/p>
&lt;blockquote>
&lt;p>You can check the &lt;code>.status.credentials.rotation.certificateAuthorities&lt;/code> field in the &lt;code>Shoot&lt;/code> to see when the rotation was last initiated, last completed, and in which phase it currently is.&lt;/p>
&lt;/blockquote>
&lt;p>In order to start the rotation (stage one), you have to annotate the shoot with the &lt;code>rotate-ca-start&lt;/code> operation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n &amp;lt;shoot-namespace&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=rotate-ca-start
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will trigger a &lt;code>Shoot&lt;/code> reconciliation and performs stage one.
After it is completed, the &lt;code>.status.credentials.rotation.certificateAuthorities.phase&lt;/code> is set to &lt;code>Prepared&lt;/code>.&lt;/p>
&lt;p>Now you must update all API clients outside the cluster (such as the &lt;code>kubeconfig&lt;/code>s on developer machines) to use the newly issued CA bundle in the &lt;code>&amp;lt;shoot-name&amp;gt;.ca-cluster&lt;/code> &lt;code>Secret&lt;/code>.
Please also note that client certificates must be re-issued now.&lt;/p>
&lt;p>After updating all API clients, you can complete the rotation by annotating the shoot with the &lt;code>rotate-ca-complete&lt;/code> operation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n &amp;lt;shoot-namespace&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=rotate-ca-complete
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will trigger another &lt;code>Shoot&lt;/code> reconciliation and performs stage three.
After it is completed, the &lt;code>.status.credentials.rotation.certificateAuthorities.phase&lt;/code> is set to &lt;code>Completed&lt;/code>.
You could update your API clients again and drop the old CA from their bundle.&lt;/p>
&lt;blockquote>
&lt;p>Note that the CA rotation also rotates all internal CAs and signed certificates.
Hence, most of the components need to be restarted (including etcd and &lt;code>kube-apiserver&lt;/code>).&lt;/p>
&lt;p>⚠️ In stage one, all worker nodes of the &lt;code>Shoot&lt;/code> will be rolled out to ensure that the &lt;code>Pod&lt;/code>s as well as the &lt;code>kubelet&lt;/code>s get the updated credentials as well.&lt;/p>
&lt;/blockquote>
&lt;h3 id="observability-passwords-for-grafana-and-prometheus">Observability Password(s) For Grafana and Prometheus&lt;/h3>
&lt;p>For &lt;code>Shoot&lt;/code>s with &lt;code>.spec.purpose!=testing&lt;/code>, Gardener deploys an observability stack with Prometheus for monitoring, Alertmanager for alerting (optional), Loki for logging, and Grafana for visualization.
The Grafana instance is exposed via &lt;code>Ingress&lt;/code> and accessible for end-users via basic authentication credentials generated and managed by Gardener.&lt;/p>
&lt;p>Those credentials are stored in a &lt;code>Secret&lt;/code> with name &lt;code>&amp;lt;shoot-name&amp;gt;.monitoring&lt;/code> in the project namespace in the garden cluster and has multiple data keys:&lt;/p>
&lt;ul>
&lt;li>&lt;code>username&lt;/code>: the user name&lt;/li>
&lt;li>&lt;code>password&lt;/code>: the password&lt;/li>
&lt;li>&lt;code>auth&lt;/code>: the user name with SHA-1 representation of the password&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>It is the responsibility of the end-user to regularly rotate those credentials.&lt;/strong>
In order to rotate the &lt;code>password&lt;/code>, annotate the &lt;code>Shoot&lt;/code> with &lt;code>gardener.cloud/operation=rotate-observability-credentials&lt;/code>.
This operation is not allowed for &lt;code>Shoot&lt;/code>s that are already marked for deletion.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n &amp;lt;shoot-namespace&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=rotate-observability-credentials
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>You can check the &lt;code>.status.credentials.rotation.observability&lt;/code> field in the &lt;code>Shoot&lt;/code> to see when the rotation was last initiated and last completed.&lt;/p>
&lt;/blockquote>
&lt;h3 id="ssh-key-pair-for-worker-nodes">SSH Key Pair For Worker Nodes&lt;/h3>
&lt;p>Gardener generates an SSH key pair whose public key is propagated to all worker nodes of the &lt;code>Shoot&lt;/code>.
The private key can be used to establish an SSH connection to the workers for troubleshooting purposes.
It is recommended to use &lt;a href="https://github.com/gardener/gardenctl-v2/">&lt;code>gardenctl-v2&lt;/code>&lt;/a> and its &lt;code>gardenctl ssh&lt;/code> command since it is required to first open up the security groups and create a bastion VM (no direct SSH access to the worker nodes is possible).&lt;/p>
&lt;p>The private key is stored in a &lt;code>Secret&lt;/code> with name &lt;code>&amp;lt;shoot-name&amp;gt;.ssh-keypair&lt;/code> in the project namespace in the garden cluster and has multiple data keys:&lt;/p>
&lt;ul>
&lt;li>&lt;code>id_rsa&lt;/code>: the private key&lt;/li>
&lt;li>&lt;code>id_rsa.pub&lt;/code>: the public key for SSH&lt;/li>
&lt;/ul>
&lt;p>In order to rotate the keys, annotate the &lt;code>Shoot&lt;/code> with &lt;code>gardener.cloud/operation=rotate-ssh-keypair&lt;/code>.
This will propagate a new key to all worker nodes while keeping the old key active and valid as well (it will only be invalidated/removed with the next rotation).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n &amp;lt;shoot-namespace&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=rotate-ssh-keypair
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>You can check the &lt;code>.status.credentials.rotation.sshKeypair&lt;/code> field in the &lt;code>Shoot&lt;/code> to see when the rotation was last initiated or last completed.&lt;/p>
&lt;/blockquote>
&lt;p>The old key is stored in a &lt;code>Secret&lt;/code> with name &lt;code>&amp;lt;shoot-name&amp;gt;.ssh-keypair.old&lt;/code> in the project namespace in the garden cluster and has the same data keys as the regular &lt;code>Secret&lt;/code>.&lt;/p>
&lt;h3 id="etcd-encryption-key">ETCD Encryption Key&lt;/h3>
&lt;p>This key is used to encrypt the data of &lt;code>Secret&lt;/code> resources inside etcd (see &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/">upstream Kubernetes documentation&lt;/a>).&lt;/p>
&lt;p>The encryption key has no expiration date.
There is no automatic rotation and &lt;strong>it is the responsibility of the end-user to regularly rotate the encryption key.&lt;/strong>&lt;/p>
&lt;p>The rotation happens in three stages:&lt;/p>
&lt;ul>
&lt;li>In stage one, a new encryption key is created and added to the bundle (together with the old encryption key).&lt;/li>
&lt;li>In stage two, all &lt;code>Secret&lt;/code>s in the cluster are rewritten by the &lt;code>kube-apiserver&lt;/code> so that they become encrypted with the new encryption key.&lt;/li>
&lt;li>In stage three, the old encryption is dropped from the bundle.&lt;/li>
&lt;/ul>
&lt;p>Technically, the &lt;code>Preparing&lt;/code> phase indicates the stages one and two.
Once it is completed, the &lt;code>Prepared&lt;/code> phase indicates readiness for stage three.
The &lt;code>Completing&lt;/code> phase indicates stage three, and the &lt;code>Completed&lt;/code> phase states that the rotation process has finished.&lt;/p>
&lt;blockquote>
&lt;p>You can check the &lt;code>.status.credentials.rotation.etcdEncryptionKey&lt;/code> field in the &lt;code>Shoot&lt;/code> to see when the rotation was last initiated, last completed, and in which phase it currently is.&lt;/p>
&lt;/blockquote>
&lt;p>In order to start the rotation (stage one), you have to annotate the shoot with the &lt;code>rotate-etcd-encryption-key-start&lt;/code> operation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n &amp;lt;shoot-namespace&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=rotate-etcd-encryption-key-start
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will trigger a &lt;code>Shoot&lt;/code> reconciliation and performs the stages one and two.
After it is completed, the &lt;code>.status.credentials.rotation.etcdEncryptionKey.phase&lt;/code> is set to &lt;code>Prepared&lt;/code>.
Now you can complete the rotation by annotating the shoot with the &lt;code>rotate-etcd-encryption-key-complete&lt;/code> operation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n &amp;lt;shoot-namespace&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=rotate-etcd-encryption-key-complete
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will trigger another &lt;code>Shoot&lt;/code> reconciliation and performs stage three.
After it is completed, the &lt;code>.status.credentials.rotation.etcdEncryptionKey.phase&lt;/code> is set to &lt;code>Completed&lt;/code>.&lt;/p>
&lt;h3 id="serviceaccount-token-signing-key">&lt;code>ServiceAccount&lt;/code> Token Signing Key&lt;/h3>
&lt;p>Gardener generates a key which is used to sign the tokens for &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">&lt;code>ServiceAccount&lt;/code>s&lt;/a>.
Those tokens are typically used by workload &lt;code>Pod&lt;/code>s running inside the cluster in order to authenticate themselves with the &lt;code>kube-apiserver&lt;/code>.
This also includes system components running in the &lt;code>kube-system&lt;/code> namespace.&lt;/p>
&lt;p>The token signing key has no expiration date.
Since it might require adaptation for the consumers of the &lt;code>Shoot&lt;/code>, there is no automatic rotation and &lt;strong>it is the responsibility of the end-user to regularly rotate the signing key.&lt;/strong>&lt;/p>
&lt;p>The rotation happens in three stages, similar to how the &lt;a href="#certificate-authorities">CA certificates&lt;/a> are rotated:&lt;/p>
&lt;ul>
&lt;li>In stage one, a new signing key is created and added to the bundle (together with the old signing key).&lt;/li>
&lt;li>In stage two, end-users update all out-of-cluster API clients that communicate with the control plane via &lt;code>ServiceAccount&lt;/code> tokens.&lt;/li>
&lt;li>In stage three, the old signing key is dropped from the bundle.&lt;/li>
&lt;/ul>
&lt;p>Technically, the &lt;code>Preparing&lt;/code> phase indicates stage one.
Once it is completed, the &lt;code>Prepared&lt;/code> phase indicates readiness for stage two.
The &lt;code>Completing&lt;/code> phase indicates stage three, and the &lt;code>Completed&lt;/code> phase states that the rotation process has finished.&lt;/p>
&lt;blockquote>
&lt;p>You can check the &lt;code>.status.credentials.rotation.serviceAccountKey&lt;/code> field in the &lt;code>Shoot&lt;/code> to see when the rotation was last initiated, last completed, and in which phase it currently is.&lt;/p>
&lt;/blockquote>
&lt;p>In order to start the rotation (stage one), you have to annotate the shoot with the &lt;code>rotate-serviceaccount-key-start&lt;/code> operation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n &amp;lt;shoot-namespace&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=rotate-serviceaccount-key-start
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will trigger a &lt;code>Shoot&lt;/code> reconciliation and performs stage one.
After it is completed, the &lt;code>.status.credentials.rotation.serviceAccountKey.phase&lt;/code> is set to &lt;code>Prepared&lt;/code>.&lt;/p>
&lt;p>Now you must update all API clients outside the cluster using a &lt;code>ServiceAccount&lt;/code> token (such as the &lt;code>kubeconfig&lt;/code>s on developer machines) to use a token issued by the new signing key.
Gardener already generates new static token secrets for all &lt;code>ServiceAccount&lt;/code>s in the cluster.
However, if you need to create it manually, you can check out &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#manually-create-a-service-account-api-token">this document&lt;/a> for instructions.&lt;/p>
&lt;p>After updating all API clients, you can complete the rotation by annotating the shoot with the &lt;code>rotate-serviceaccount-key-complete&lt;/code> operation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n &amp;lt;shoot-namespace&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=rotate-serviceaccount-key-complete
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will trigger another &lt;code>Shoot&lt;/code> reconciliation and performs stage three.
After it is completed, the &lt;code>.status.credentials.rotation.serviceAccountKey.phase&lt;/code> is set to &lt;code>Completed&lt;/code>.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ In stage one, all worker nodes of the &lt;code>Shoot&lt;/code> will be rolled out to ensure that the &lt;code>Pod&lt;/code>s use a new token.&lt;/p>
&lt;/blockquote>
&lt;h3 id="openvpn-tls-auth-keys">OpenVPN TLS Auth Keys&lt;/h3>
&lt;p>This key is used to ensure encrypted communication for the VPN connection between the control plane in the seed cluster and the shoot cluster.
It is currently &lt;strong>not&lt;/strong> rotated automatically and there is no way to trigger it manually.&lt;/p></description></item><item><title>Docs: Shoot High Availability</title><link>https://gardener.cloud/docs/gardener/usage/shoot_high_availability/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_high_availability/</guid><description>
&lt;h1 id="highly-available-shoot-control-plane">Highly Available Shoot Control Plane&lt;/h1>
&lt;p>Shoot resource offers a way to request for a highly available control plane.&lt;/p>
&lt;h2 id="failure-tolerance-types">Failure Tolerance Types&lt;/h2>
&lt;p>A highly available shoot control plane can be setup with either a failure tolerance of &lt;code>zone&lt;/code> or &lt;code>node&lt;/code>.&lt;/p>
&lt;h3 id="node-failure-tolerance">&lt;code>Node&lt;/code> Failure Tolerance&lt;/h3>
&lt;p>Failure tolerance of &lt;code>node&lt;/code> will have the following characteristics:&lt;/p>
&lt;ul>
&lt;li>Control plane components will be spread across different nodes within a single availability zone. There will not be
more than one replica per node for each control plane component which has more than one replica.&lt;/li>
&lt;li>&lt;code>Worker pool&lt;/code> should have a minimum of 3 nodes.&lt;/li>
&lt;li>A multi-node etcd (quorum size of 3) will be provisioned offering zero-downtime capabilities with each member in a
different node within a single availability zone.&lt;/li>
&lt;/ul>
&lt;h3 id="zone-failure-tolerance">&lt;code>Zone&lt;/code> Failure Tolerance&lt;/h3>
&lt;p>Failure tolerance of &lt;code>zone&lt;/code> will have the following characteristics:&lt;/p>
&lt;ul>
&lt;li>Control plane components will be spread across different availability zones. There will be at least
one replica per zone for each control plane component which has more than one replica.&lt;/li>
&lt;li>Gardener scheduler will automatically select a &lt;code>seed&lt;/code> which has a minimum of 3 zones to host the shoot control plane.&lt;/li>
&lt;li>A multi-node etcd (quorum size of 3) will be provisioned offering zero-downtime capabilities with each member in a
different zone.&lt;/li>
&lt;/ul>
&lt;h2 id="shoot-spec">Shoot Spec&lt;/h2>
&lt;p>To request for a highly available shoot control plane gardener provides the following configuration in the shoot spec.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> controlPlane:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> highAvailability:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> failureTolerance:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: &amp;lt;node | zone&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Allowed Transitions&lt;/strong>&lt;/p>
&lt;p>If you already have a shoot cluster with non-HA control plane then following upgrades are possible:&lt;/p>
&lt;ul>
&lt;li>Upgrade of non-HA shoot control plane to HA shoot control plane with &lt;code>node&lt;/code> failure tolerance.&lt;/li>
&lt;li>Upgrade of non-HA shoot control plane to HA shoot control plane with &lt;code>zone&lt;/code> failure tolerance. However, it is essential that the &lt;code>seed&lt;/code> which is currently hosting the shoot control plane should be &lt;code>multi-zonal&lt;/code>. If it is not then request to upgrade will be rejected.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>NOTE: There will be a small downtime during the upgrade especially for etcd which will transition from a single node etcd cluster to a multi-node etcd cluster.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Disallowed Transitions&lt;/strong>&lt;/p>
&lt;p>If you have already set-up an HA shoot control plane with &lt;code>node&lt;/code> failure tolerance, then an upgrade to &lt;code>zone&lt;/code> failure tolerance is currently not supported, mainly because already existing volumes are bound to the zone they were created in originally.&lt;/p>
&lt;h2 id="zone-outage-situation">Zone Outage Situation&lt;/h2>
&lt;p>An availability zone outage might lead to the requirement to change your cluster setup temporarily and on short notice, in order to compensate failures and shortages resulting from the outage.
For instance, if the shoot cluster has worker nodes across three zones where one zone goes down, the computing power from these nodes is also gone during that time.
Changing the worker pool (&lt;code>shoot.spec.provider.workers[]&lt;/code>) and infrastructure (&lt;code>shoot.spec.provider.infrastructureConfig&lt;/code>) configuration can eliminate this disbalance, having enough machines in healthy availability zones that can cope with the requests of your applications.&lt;/p>
&lt;p>Gardener relies on a sophisticated reconciliation flow with several dependencies for which various flow steps wait for the &lt;em>readiness&lt;/em> of prior ones.
During a zone outage, this can block the entire flow, e.g. because all three &lt;code>etcd&lt;/code> replicas can never be ready when a zone is down, and required changes mentioned above can never be accomplished.
For this, a special one-off annotation &lt;code>shoot.gardener.cloud/skip-readiness&lt;/code> helps to skip any readiness checks in the flow.&lt;/p>
&lt;blockquote>
&lt;p>The &lt;code>shoot.gardener.cloud/skip-readiness&lt;/code> annotation serves as a last resort if reconciliation is stuck because of important changes during an AZ outage. Use it with caution, only in exceptional cases and after a case-by-case evaluation with your Gardener landscape administrator. If used together with other operations like Kubernetes version upgrades or credential rotation, the annotation may lead to a severe outage of your shoot control plane.&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: Shoot Info Configmap</title><link>https://gardener.cloud/docs/gardener/usage/shoot_info_configmap/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_info_configmap/</guid><description>
&lt;h1 id="shoot-info-configmap">Shoot Info &lt;code>ConfigMap&lt;/code>&lt;/h1>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Gardenlet maintains a &lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/">ConfigMap&lt;/a> inside the Shoot cluster that contains information about the cluster itself. The ConfigMap is named &lt;code>shoot-info&lt;/code> and located in the &lt;code>kube-system&lt;/code> namespace.&lt;/p>
&lt;h2 id="fields">Fields&lt;/h2>
&lt;p>The following fields are provided:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ConfigMap
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: shoot-info
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: kube-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> domain: crazy-botany.core.my-custom-domain.com &lt;span style="color:#008000"># .spec.dns.domain field from the Shoot resource&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extensions: foobar,foobaz &lt;span style="color:#008000"># List of extensions that are enabled&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetesVersion: 1.20.1 &lt;span style="color:#008000"># .spec.kubernetes.version field from the Shoot resource&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maintenanceBegin: 220000+0100 &lt;span style="color:#008000"># .spec.maintenance.timeWindow.begin field from the Shoot resource&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maintenanceEnd: 230000+0100 &lt;span style="color:#008000"># .spec.maintenance.timeWindow.end field from the Shoot resource&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeNetwork: 10.250.0.0/16 &lt;span style="color:#008000"># .spec.networking.nodes field from the Shoot resource&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> podNetwork: 100.96.0.0/11 &lt;span style="color:#008000"># .spec.networking.pods field from the Shoot resource&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> projectName: dev &lt;span style="color:#008000"># .metadata.name of the Project&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider: &amp;lt;some-provider-name&amp;gt; &lt;span style="color:#008000"># .spec.provider.type field from the Shoot resource&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> region: europe-central-1 &lt;span style="color:#008000"># .spec.region field from the Shoot resource&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> serviceNetwork: 100.64.0.0/13 &lt;span style="color:#008000"># .spec.networking.services field from the Shoot resource&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> shootName: crazy-botany &lt;span style="color:#008000"># .metadata.name from the Shoot resource&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Shoot Maintenance</title><link>https://gardener.cloud/docs/gardener/usage/shoot_maintenance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_maintenance/</guid><description>
&lt;h1 id="shoot-maintenance">Shoot Maintenance&lt;/h1>
&lt;p>Shoots configure a maintenance time window in which Gardener performs certain operations that may restart the control plane, roll out the nodes, result in higher network traffic, etc. A summary of what was changed in the last maintenance time window in shoot specification is kept in shoot status &lt;code>.status.lastMaintenance&lt;/code> field.&lt;/p>
&lt;p>This document outlines what happens during a shoot maintenance.&lt;/p>
&lt;h2 id="time-window">Time Window&lt;/h2>
&lt;p>Via the &lt;code>.spec.maintenance.timeWindow&lt;/code> field in the shoot specification end-users can configure the time window in which maintenance operations are executed.
Gardener runs one maintenance operation per day in this time window:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maintenance:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> timeWindow:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> begin: 220000+0100
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> end: 230000+0100
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The offset (&lt;code>+0100&lt;/code>) is considered with respect to UTC time.
The minimum time window is &lt;code>30m&lt;/code> and the maximum is &lt;code>6h&lt;/code>.&lt;/p>
&lt;p>⚠️ Please note that there is no guarantee that a maintenance operation that e.g. starts a node roll-out will finish &lt;em>within&lt;/em> the time window.
Especially for large clusters it may take several hours until a graceful rolling update of the worker nodes succeeds (also depending on the workload and the configured pod disruption budgets/termination grace periods).&lt;/p>
&lt;p>Internally, Gardener is subtracting &lt;code>15m&lt;/code> from the end of the time window to (best-effort) try to finish the maintenance until the end is reached, however, it might not work in all cases.&lt;/p>
&lt;p>If you don&amp;rsquo;t specify a time window then Gardener will randomly compute it.
You can change it later, of course.&lt;/p>
&lt;h2 id="automatic-version-updates">Automatic Version Updates&lt;/h2>
&lt;p>The &lt;code>.spec.maintenance.autoUpdate&lt;/code> field in the shoot specification allows you to control how/whether automatic updates of Kubernetes patch and machine image versions are performed.
Machine image versions are updated per worker pool.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maintenance:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> autoUpdate:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetesVersion: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineImageVersion: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>During the daily maintenance, the Gardener Controller Manager updates the Shoot&amp;rsquo;s Kubernetes and machine image version if any of the following criteria applies:&lt;/p>
&lt;ul>
&lt;li>there is a higher version available and the Shoot opted-in for automatic version updates&lt;/li>
&lt;li>the currently used version is &lt;code>expired&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Gardener creates events with type &lt;code>MaintenanceDone&lt;/code> on the Shoot describing the action performed during maintenance including the reason why an update has been triggered.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>MaintenanceDone Updated image of worker-pool &amp;#39;coreos-xy&amp;#39; from &amp;#39;coreos&amp;#39; version &amp;#39;xy&amp;#39; to version &amp;#39;abc&amp;#39;. Reason: AutoUpdate of MachineImage configured.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>MaintenanceDone Updated Kubernetes version &amp;#39;0.0.1&amp;#39; to version &amp;#39;0.0.5&amp;#39;. This is an increase in the patch level. Reason: AutoUpdate of Kubernetes version configured.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>MaintenanceDone Updated Kubernetes version &amp;#39;0.0.5&amp;#39; to version &amp;#39;0.1.5&amp;#39;. This is an increase in the minor level. Reason: Kubernetes version expired - force update required.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Please refer to &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_versions/">this document&lt;/a> for more information about Kubernetes and machine image versions in Gardener.&lt;/p>
&lt;h2 id="cluster-reconciliation">Cluster Reconciliation&lt;/h2>
&lt;p>Gardener administrators/operators can configure the Gardenlet in a way that it only reconciles shoot clusters during their maintenance time windows.
This behaviour is not controllable by end-users but might make sense for large Gardener installations.
Concretely, your shoot will be reconciled regularly during its maintenance time window.
Outside of the maintenance time window it will only reconcile if you change the specification or if you explicitly trigger it, see also &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_operations/">this document&lt;/a>.&lt;/p>
&lt;h2 id="confine-specification-changesupdates-roll-out">Confine Specification Changes/Updates Roll Out&lt;/h2>
&lt;p>Via the &lt;code>.spec.maintenance.confineSpecUpdateRollout&lt;/code> field you can control whether you want to make Gardener roll out changes/updates to your shoot specification only during the maintenance time window.
It is &lt;code>false&lt;/code> by default, i.e., any change to your shoot specification triggers a reconciliation (even outside of the maintenance time window).
This is helpful if you want to update your shoot but don&amp;rsquo;t want the changes to be applied immediately. One example use-case would be a Kubernetes version upgrade that you want to roll out during the maintenance time window.
Any update to the specification will not increase the &lt;code>.metadata.generation&lt;/code> of the &lt;code>Shoot&lt;/code> which is something you should be aware of.
Also, even if Gardener administrators/operators have not enabled the &amp;ldquo;reconciliation in maintenance time window only&amp;rdquo; configuration (as mentioned above) then your shoot will only reconcile in the maintenance time window.
The reason is that Gardener cannot differentiate between create/update/reconcile operations.&lt;/p>
&lt;p>⚠️ If &lt;code>confineSpecUpdateRollout=true&lt;/code>, please note that if you change the maintenance time window itself then it will only be effective after the upcoming maintenance.&lt;/p>
&lt;p>⚠️ As exceptions to above rules, &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_operations/#immediate-reconciliation">manually triggered reconciliations&lt;/a> and changes to the &lt;code>.spec.hibernation.enabled&lt;/code> field trigger immediate rollouts.
I.e., if you hibernate or wake-up your shoot, or you explicitly tell Gardener to reconcile your shoot, then Gardener gets active right away.&lt;/p>
&lt;h2 id="shoot-operations">Shoot Operations&lt;/h2>
&lt;p>In case you would like to perform a &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_operations/#credentials-rotation-operations">shoot credential rotation&lt;/a> or a &lt;code>reconcile&lt;/code> operation during your maintenance time window, you can annotate the &lt;code>Shoot&lt;/code> with&lt;/p>
&lt;pre tabindex="0">&lt;code>maintenance.gardener.cloud/operation=&amp;lt;operation&amp;gt;
&lt;/code>&lt;/pre>&lt;p>This will execute the specified &lt;code>&amp;lt;operation&amp;gt;&lt;/code> during the next maintenance reconciliation.
Note that Gardener will remove this annotation after it has been performed in the maintenance reconciliation.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ This is skipped when the &lt;code>Shoot&lt;/code>&amp;rsquo;s &lt;code>.status.lastOperation.state=Failed&lt;/code>. Make sure to &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_operations/#retry-failed-reconciliation">retry&lt;/a> your shoot reconciliation beforehand.&lt;/p>
&lt;/blockquote>
&lt;h2 id="special-operations-during-maintenance">Special Operations During Maintenance&lt;/h2>
&lt;p>The shoot maintenance controller triggers special operations that are performed as part of the shoot reconciliation.&lt;/p>
&lt;h3 id="infrastructure-and-dnsrecord-reconciliation">&lt;code>Infrastructure&lt;/code> and &lt;code>DNSRecord&lt;/code> Reconciliation&lt;/h3>
&lt;p>The reconciliation of the &lt;code>Infrastructure&lt;/code> and &lt;code>DNSRecord&lt;/code> extension resources is only demanded during the shoot&amp;rsquo;s maintenance time window.
The rationale behind it is to prevent sending too many requests against the cloud provider APIs, especially on large landscapes or if a user has many shoot clusters in the same cloud provider account.&lt;/p>
&lt;h3 id="restart-control-plane-controllers">Restart Control Plane Controllers&lt;/h3>
&lt;p>Gardener operators can make Gardener restart/delete certain control plane pods during a shoot maintenance.
This feature helps to automatically solve service denials of controllers due to stale caches, dead-locks or starving routines.&lt;/p>
&lt;p>Please note that these are exceptional cases but they are observed from time to time.
Gardener, for example, takes this precautionary measure for &lt;code>kube-controller-manager&lt;/code> pods.&lt;/p>
&lt;p>See &lt;a href="https://gardener.cloud/docs/gardener/extensions/shoot-maintenance/">this document&lt;/a> to see how extension developers can extend this behaviour.&lt;/p>
&lt;h3 id="restart-some-core-addons">Restart Some Core Addons&lt;/h3>
&lt;p>Gardener operators can make Gardener restart some core addons, at the moment only CoreDNS, during a shoot maintenance.&lt;/p>
&lt;p>CoreDNS benefits from this feature as it automatically solve problems with clients stuck to single replica of the deployment and thus overloading it.
Please note that these are exceptional cases but they are observed from time to time.&lt;/p></description></item><item><title>Docs: Shoot Network Policies</title><link>https://gardener.cloud/docs/gardener/usage/shoot_network_policies/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_network_policies/</guid><description>
&lt;h2 id="network-policies-in-the-shoot-cluster">Network policies in the Shoot Cluster&lt;/h2>
&lt;p>In addition to deploying network policies &lt;a href="https://gardener.cloud/docs/gardener/development/seed_network_policies/">into the Seed&lt;/a>,
Gardener deploys network policies into the &lt;code>kube-system&lt;/code> namespace of the Shoot.
These network policies are used by Shoot system components (that are not part of the control plane).
Other namespaces in the Shoot do not contain network policies deployed by Gardener.&lt;/p>
&lt;p>As best practice, every pod deployed into the &lt;code>kube-system&lt;/code> namespace should use appropriate network policies in order to only allow &lt;strong>required&lt;/strong> network traffic.
Therefore, pods should have labels matching to the selectors of the available network policies.&lt;/p>
&lt;p>Gardener deploys the following network policies:&lt;/p>
&lt;pre tabindex="0">&lt;code>NAME POD-SELECTOR
gardener.cloud--allow-dns k8s-app in (kube-dns)
gardener.cloud--allow-from-seed networking.gardener.cloud/from-seed=allowed
gardener.cloud--allow-to-apiserver networking.gardener.cloud/to-apiserver=allowed
gardener.cloud--allow-to-dns networking.gardener.cloud/to-dns=allowed
gardener.cloud--allow-to-from-nginx app=nginx-ingress
gardener.cloud--allow-to-kubelet networking.gardener.cloud/to-kubelet=allowed
gardener.cloud--allow-to-public-networks networking.gardener.cloud/to-public-networks=allowed
gardener.cloud--allow-vpn app=vpn-shoot
&lt;/code>&lt;/pre>&lt;p>Additionally, there can be network policies deployed by Gardener extensions such as &lt;a href="https://github.com/gardener/gardener-extension-networking-calico">extension-calico&lt;/a>.&lt;/p>
&lt;pre tabindex="0">&lt;code>NAME POD-SELECTOR
gardener.cloud--allow-from-calico-node k8s-app=calico-typha
&lt;/code>&lt;/pre></description></item><item><title>Docs: Shoot Networking</title><link>https://gardener.cloud/docs/gardener/usage/shoot_networking/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_networking/</guid><description>
&lt;h1 id="shoot-networking">Shoot Networking&lt;/h1>
&lt;p>This document contains network related information for Shoot clusters.&lt;/p>
&lt;h2 id="pod-network">Pod Network&lt;/h2>
&lt;p>A Pod network is imperative for any kind of cluster communication with Pods not started within the Node&amp;rsquo;s host network.
More information about the Kubernetes network model can be found &lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">here&lt;/a>.&lt;/p>
&lt;p>Gardener allows users to configure the Pod network&amp;rsquo;s CIDR during Shoot creation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networking:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: &amp;lt;some-network-extension-name&amp;gt; &lt;span style="color:#008000"># {calico,cilium}&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pods: 100.96.0.0/16
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodes: ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> services: ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>⚠️ The &lt;code>networking.pods&lt;/code> IP configuration is immutable and cannot be changed afterwards.
Please consider the following paragraph to choose a configuration which will meet your demands.&lt;/p>
&lt;/blockquote>
&lt;p>One of the network plugin&amp;rsquo;s (CNI) tasks is to assign IP addresses to Pods started in the Pod network.
Different network plugins come with different IP address management (IPAM) features, so we can&amp;rsquo;t give any definite advice how IP ranges should be configured.
Nevertheless, we want to outline the standard configuration.&lt;/p>
&lt;p>Information in &lt;code>.spec.networking.pods&lt;/code> matches the &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/">&amp;ndash;cluster-cidr flag&lt;/a> of the Kube-Controller-Manager of your Shoot cluster.
This IP range is divided into smaller subnets, also called &lt;code>podCIDRs&lt;/code> (default mask &lt;code>/24&lt;/code>) and assigned to Node objects &lt;code>.spec.podCIDR&lt;/code>.
Pods get their IP address from this smaller node subnet in a default IPAM setup.
Thus, it must be guaranteed that enough of these subnets can be created for the maximum amount of nodes you expect in the cluster.&lt;/p>
&lt;p>&lt;em>&lt;strong>Example 1&lt;/strong>&lt;/em>&lt;/p>
&lt;pre tabindex="0">&lt;code>Pod network: 100.96.0.0/16
nodeCIDRMaskSize: /24
-------------------------
Number of podCIDRs: 256 --&amp;gt; max. Node count
Number of IPs per podCIDRs: 256
&lt;/code>&lt;/pre>&lt;p>With the configuration above a Shoot cluster can at most have &lt;strong>256 nodes&lt;/strong> which are ready to run workload in the Pod network.&lt;/p>
&lt;p>&lt;em>&lt;strong>Example 2&lt;/strong>&lt;/em>&lt;/p>
&lt;pre tabindex="0">&lt;code>Pod network: 100.96.0.0/20
nodeCIDRMaskSize: /24
-------------------------
Number of podCIDRs: 16 --&amp;gt; max. Node count
Number of IPs per podCIDRs: 256
&lt;/code>&lt;/pre>&lt;p>With the configuration above a Shoot cluster can at most have &lt;strong>16 nodes&lt;/strong> which are ready to run workload in the Pod network.&lt;/p>
&lt;p>Beside the configuration in &lt;code>.spec.networking.pods&lt;/code>, users can tune the &lt;code>nodeCIDRMaskSize&lt;/code> used by Kube-Controller-Manager on shoot creation.
A smaller IP range per node means more &lt;code>podCIDRs&lt;/code> and thus the ability to provision more nodes in the cluster, but less available IPs for Pods running on each of the nodes.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeControllerManager:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeCIDRMaskSize: 24 (default)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>⚠️ The &lt;code>nodeCIDRMaskSize&lt;/code> configuration is immutable and cannot be changed afterwards.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;em>&lt;strong>Example 3&lt;/strong>&lt;/em>&lt;/p>
&lt;pre tabindex="0">&lt;code>Pod network: 100.96.0.0/20
nodeCIDRMaskSize: /25
-------------------------
Number of podCIDRs: 32 --&amp;gt; max. Node count
Number of IPs per podCIDRs: 128
&lt;/code>&lt;/pre>&lt;p>With the configuration above a Shoot cluster can at most have &lt;strong>32 nodes&lt;/strong> which are ready to run workload in the Pod network.&lt;/p></description></item><item><title>Docs: Shoot Operations</title><link>https://gardener.cloud/docs/gardener/usage/shoot_operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_operations/</guid><description>
&lt;h1 id="trigger-shoot-operations">Trigger Shoot Operations&lt;/h1>
&lt;p>You can trigger a few explicit operations by annotating the &lt;code>Shoot&lt;/code> with an operation annotation.
This might allow you to induct certain behavior without the need to change the &lt;code>Shoot&lt;/code> specification.
Some of the operations can also not be caused by changing something in the shoot specification because they can&amp;rsquo;t properly be reflected here.
Note, once the triggered operation is considered by the controllers, the annotation will be automatically removed and you have to add it each time you want to trigger the operation.&lt;/p>
&lt;p>Please note: If &lt;code>.spec.maintenance.confineSpecUpdateRollout=true&lt;/code> then the only way to trigger a shoot reconciliation is by setting the &lt;code>reconcile&lt;/code> operation, see below.&lt;/p>
&lt;h2 id="immediate-reconciliation">Immediate Reconciliation&lt;/h2>
&lt;p>Annotate the shoot with &lt;code>gardener.cloud/operation=reconcile&lt;/code> to make the &lt;code>gardenlet&lt;/code> start a reconciliation operation without changing the shoot spec and possibly without being in its maintenance time window:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n garden-&amp;lt;project-name&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=reconcile
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="immediate-maintenance">Immediate Maintenance&lt;/h2>
&lt;p>Annotate the shoot with &lt;code>gardener.cloud/operation=maintain&lt;/code> to make the &lt;code>gardener-controller-manager&lt;/code> start maintaining your shoot immediately (possibly without being in its maintenance time window).
If no reconciliation starts then nothing needed to be maintained:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n garden-&amp;lt;project-name&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=maintain
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="retry-failed-reconciliation">Retry Failed Reconciliation&lt;/h2>
&lt;p>Annotate the shoot with &lt;code>gardener.cloud/operation=retry&lt;/code> to make the &lt;code>gardenlet&lt;/code> start a new reconciliation loop on a failed shoot.
Failed shoots are only reconciled again if a new Gardener version is deployed, the shoot specification is changed or this annotation is set&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n garden-&amp;lt;project-name&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=retry
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="credentials-rotation-operations">Credentials Rotation Operations&lt;/h2>
&lt;p>Please consult &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_credentials_rotation/">this document&lt;/a> for more information.&lt;/p>
&lt;h2 id="restart-systemd-services-on-particular-worker-nodes">Restart &lt;code>systemd&lt;/code> Services On Particular Worker Nodes&lt;/h2>
&lt;p>It is possible to make Gardener restart particular systemd services on your shoot worker nodes if needed.
The annotation is not set on the &lt;code>Shoot&lt;/code> resource but directly on the &lt;code>Node&lt;/code> object you want to target.
For example, the following will restart both the &lt;code>kubelet&lt;/code> and the &lt;code>docker&lt;/code> services:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl annotate node &amp;lt;node-name&amp;gt; worker.gardener.cloud/restart-systemd-services=kubelet,docker
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It may take up to a minute until the service is restarted.
The annotation will be removed from the &lt;code>Node&lt;/code> object after all specified systemd services have been restarted.
It will also be removed even if the restart of one or more services failed.&lt;/p>
&lt;blockquote>
&lt;p>ℹ️ In the example mentioned above, you could additionally verify when/whether the kubelet restarted by using &lt;code>kubectl describe node &amp;lt;node-name&amp;gt;&lt;/code> and looking for such a &lt;code>Starting kubelet&lt;/code> event.&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: Shoot Purposes</title><link>https://gardener.cloud/docs/gardener/usage/shoot_purposes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_purposes/</guid><description>
&lt;h1 id="shoot-cluster-purpose">Shoot Cluster Purpose&lt;/h1>
&lt;p>The &lt;code>Shoot&lt;/code> resource contains a &lt;code>.spec.purpose&lt;/code> field indicating how the shoot is used whose allowed values are as follows:&lt;/p>
&lt;ul>
&lt;li>&lt;code>evaluation&lt;/code> (default): Indicates that the shoot cluster is for evaluation scenarios.&lt;/li>
&lt;li>&lt;code>development&lt;/code>: Indicates that the shoot cluster is for development scenarios.&lt;/li>
&lt;li>&lt;code>testing&lt;/code>: Indicates that the shoot cluster is for testing scenarios.&lt;/li>
&lt;li>&lt;code>production&lt;/code>: Indicates that the shoot cluster is for production scenarios.&lt;/li>
&lt;li>&lt;code>infrastructure&lt;/code>: Indicates that the shoot cluster is for infrastructure scenarios (only allowed for shoots in the &lt;code>garden&lt;/code> namespace).&lt;/li>
&lt;/ul>
&lt;h2 id="behavioral-differences">Behavioral Differences&lt;/h2>
&lt;p>The following enlists the differences in the way the shoot clusters are set up based on the selected purpose:&lt;/p>
&lt;ul>
&lt;li>&lt;code>testing&lt;/code> shoot clusters &lt;strong>do not&lt;/strong> get a monitoring or a logging stack as part of their control planes.&lt;/li>
&lt;li>&lt;code>production&lt;/code> shoot clusters get at least two replicas of the &lt;code>kube-apiserver&lt;/code> for their control planes.
Auto-scaling scale down of the main ETCD is disabled for such clusters.&lt;/li>
&lt;/ul>
&lt;p>There are also differences with respect to how &lt;code>testing&lt;/code> shoots are scheduled after creation, please consult the &lt;a href="https://gardener.cloud/docs/gardener/concepts/scheduler/">Scheduler documentation&lt;/a>.&lt;/p>
&lt;h2 id="future-steps">Future Steps&lt;/h2>
&lt;p>We might introduce more behavioral difference depending on the shoot purpose in the future.
As of today, there are no plans yet.&lt;/p></description></item><item><title>Docs: Shoot Scheduling Profiles</title><link>https://gardener.cloud/docs/gardener/usage/shoot_scheduling_profiles/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_scheduling_profiles/</guid><description>
&lt;h1 id="shoot-scheduling-profiles">Shoot Scheduling Profiles&lt;/h1>
&lt;p>This guide describes the available scheduling profiles and how they can be configured in the Shoot cluster. It also clarifies how a custom scheduling profile can be configured.&lt;/p>
&lt;h2 id="scheduling-profiles">Scheduling profiles&lt;/h2>
&lt;p>The scheduling process in the &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/">kube-scheduler&lt;/a> happens in series of stages. A &lt;a href="https://kubernetes.io/docs/reference/scheduling/config/#profiles">scheduling profile&lt;/a> allows configuring the different stages of the scheduling.&lt;/p>
&lt;p>As of today, Gardener supports two predefined scheduling profiles:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>balanced&lt;/code> (default)&lt;/p>
&lt;p>&lt;strong>Overview&lt;/strong>&lt;/p>
&lt;p>The &lt;code>balanced&lt;/code> profile attempts to spread Pods evenly across Nodes to obtain a more balanced resource usage. This profile provides the default kube-scheduler behavior.&lt;/p>
&lt;p>&lt;strong>How it works?&lt;/strong>&lt;/p>
&lt;p>The kube-scheduler is started without any profiles. In such case, by default, one profile with the scheduler name &lt;code>default-scheduler&lt;/code> is created. This profile includes the default plugins. If a Pod doesn&amp;rsquo;t specify the &lt;code>.spec.schedulerName&lt;/code> field, kube-apiserver sets it to &lt;code>default-scheduler&lt;/code>. Then, the Pod gets scheduled by the &lt;code>default-scheduler&lt;/code> accordingly.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>bin-packing&lt;/code> (alpha)&lt;/p>
&lt;p>&lt;strong>Overview&lt;/strong>&lt;/p>
&lt;p>The &lt;code>bin-packing&lt;/code> profile scores Nodes based on the allocation of resources. It prioritizes Nodes with most allocated resources. By favoring the Nodes with most allocation some of the other Nodes become under-utilized over time (because new Pods keep being scheduled to the most allocated Nodes). Then, the cluster-autoscaler identifies such under-utilized Nodes and removes them from the cluster. In this way, this profile provides a greater overall resource utilization (compared to the &lt;code>balanced&lt;/code> profile).&lt;/p>
&lt;blockquote>
&lt;p>Note: The decision of when to remove a Node is a trade-off between optimizing for utilization or the availability of resources. Removing under-utilized Nodes improves cluster utilization, but new workloads might have to wait for resources to be provisioned again before they can run.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Note: The &lt;code>bin-packing&lt;/code> profile is considered as alpha feature. Use it only for evaluation purposes.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>How it works?&lt;/strong>&lt;/p>
&lt;p>The kube-scheduler is configured with the following bin packing profile:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: kubescheduler.config.k8s.io/v1beta3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: KubeSchedulerConfiguration
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>profiles:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- schedulerName: bin-packing-scheduler
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pluginConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: NodeResourcesFit
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> args:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scoringStrategy:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: MostAllocated
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> plugins:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> score:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> disabled:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: NodeResourcesBalancedAllocation
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To impose the new profile, a &lt;code>MutatingWebhookConfiguration&lt;/code> is deployed in the Shoot cluster. The &lt;code>MutatingWebhookConfiguration&lt;/code> intercepts &lt;code>CREATE&lt;/code> operations for Pods and sets the &lt;code>.spec.schedulerName&lt;/code> field to &lt;code>bin-packing-scheduler&lt;/code>. Then, the Pod gets scheduled by the &lt;code>bin-packing-scheduler&lt;/code> accordingly. Pods that specify a custom scheduler (i.e., having &lt;code>.spec.schedulerName&lt;/code> different from &lt;code>default-scheduler&lt;/code> and &lt;code>bin-packing-scheduler&lt;/code>) are not affected.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="configuring-the-scheduling-profile">Configuring the scheduling profile&lt;/h2>
&lt;p>The scheduling profile can be configured via the &lt;code>.spec.kubernetes.kubeScheduler.profile&lt;/code> field in the Shoot:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeScheduler:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> profile: &lt;span style="color:#a31515">&amp;#34;balanced&amp;#34;&lt;/span> &lt;span style="color:#008000"># or &amp;#34;bin-packing&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="custom-scheduling-profiles">Custom scheduling profiles&lt;/h2>
&lt;p>The kube-scheduler&amp;rsquo;s component configs allows configuring custom scheduling profiles to match the cluster needs. As of today, Gardener supports only two predefined scheduling profiles. The profile configuration in the component config is quite expressive and it is not possible to easily define profiles that would match the needs of every cluster. Because of these reasons, there are no plans to add support for new predefined scheduling profiles. If a cluster owner wants to use a custom scheduling profile then they have to deploy (and maintain) a dedicated kube-scheduler deployment in the cluster itself.&lt;/p></description></item><item><title>Docs: Shoot Serviceaccounts</title><link>https://gardener.cloud/docs/gardener/usage/shoot_serviceaccounts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_serviceaccounts/</guid><description>
&lt;h1 id="serviceaccount-configurations-for-shoot-clusters">&lt;code>ServiceAccount&lt;/code> Configurations For Shoot Clusters&lt;/h1>
&lt;p>The &lt;code>Shoot&lt;/code> specification allows to configure some of the settings for the handling of &lt;code>ServiceAccount&lt;/code>s:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeAPIServer:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> serviceAccountConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuer: foo
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> acceptedIssuers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - foo1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - foo2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extendTokenExpiration: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maxTokenExpiration: 45d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="issuer-and-accepted-issuers">Issuer And Accepted Issuers&lt;/h2>
&lt;p>The &lt;code>.spec.kubernetes.kubeAPIServer.serviceAccountConfig.{issuer,acceptedIssuers}&lt;/code> field are translated to the &lt;code>--service-account-issuer&lt;/code> flag for the &lt;code>kube-apiserver&lt;/code>.
The issuer will assert its identifier in the &lt;code>iss&lt;/code> claim of issued tokens.
According to the &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">upstream specification&lt;/a>, values need to meet the following requirements:&lt;/p>
&lt;blockquote>
&lt;p>This value is a string or URI. If this option is not a valid URI per the OpenID Discovery 1.0 spec, the ServiceAccountIssuerDiscovery feature will remain disabled, even if the feature gate is set to true. It is highly recommended that this value comply with the OpenID spec: &lt;a href="https://openid.net/specs/openid-connect-discovery-1_0.html">https://openid.net/specs/openid-connect-discovery-1_0.html&lt;/a>. In practice, this means that service-account-issuer must be an https URL. It is also highly recommended that this URL be capable of serving OpenID discovery documents at {service-account-issuer}/.well-known/openid-configuration.&lt;/p>
&lt;/blockquote>
&lt;p>By default, Gardener uses the internal cluster domain as issuer (e.g., &lt;code>https://api.foo.bar.example.com&lt;/code>).
If you specify the &lt;code>issuer&lt;/code> then this default issuer will always be part of the list of accepted issuers (you don&amp;rsquo;t need to specify it yourself).&lt;/p>
&lt;p>⚠️ Caution: If you change from the default issuer to a custom &lt;code>issuer&lt;/code> then all previously issued tokens are still valid/accepted.
However, if you change from a custom &lt;code>issuer&lt;/code> &lt;code>A&lt;/code> to another &lt;code>issuer&lt;/code> &lt;code>B&lt;/code> (custom or default) then you have to add &lt;code>A&lt;/code> to the &lt;code>acceptedIssuers&lt;/code> so that previously issued tokens are not invalidated.
Otherwise, the control plane components as well as system components and your workload pods might fail.
You can remove &lt;code>A&lt;/code> from the &lt;code>acceptedIssuers&lt;/code> when all active tokens were issued by &lt;code>B&lt;/code>.
This can be ensured by using &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection">projected token volumes&lt;/a> with a short validity, or by rolling out all pods.
Additionally, all &lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/#service-account-token-secrets">&lt;code>ServiceAccount&lt;/code> token secrets&lt;/a> should be recreated.
Apart from this you should wait for at least &lt;code>12h&lt;/code> to make sure the control plane and system components receive a new token from Gardener.&lt;/p>
&lt;h2 id="token-expirations">Token Expirations&lt;/h2>
&lt;p>The &lt;code>.spec.kubernetes.kubeAPIServer.serviceAccountConfig.extendTokenExpiration&lt;/code> configures the &lt;code>--service-account-extend-token-expiration&lt;/code> flag of the &lt;code>kube-apiserver&lt;/code>.
It is enabled by default and has the following specification:&lt;/p>
&lt;blockquote>
&lt;p>Turns on projected service account expiration extension during token generation, which helps safe transition from legacy token to bound service account token feature. If this flag is enabled, admission injected tokens would be extended up to 1 year to prevent unexpected failure during transition, ignoring value of service-account-max-token-expiration.&lt;/p>
&lt;/blockquote>
&lt;p>The &lt;code>.spec.kubernetes.kubeAPIServer.serviceAccountConfig.maxTokenExpiration&lt;/code> configures the &lt;code>--service-account-max-token-expiration&lt;/code> flag of the &lt;code>kube-apiserver&lt;/code>.
It has the following specification:&lt;/p>
&lt;blockquote>
&lt;p>The maximum validity duration of a token created by the service account token issuer. If an otherwise valid TokenRequest with a validity duration larger than this value is requested, a token will be issued with a validity duration of this value.&lt;/p>
&lt;/blockquote>
&lt;p>⚠️ Note that the value for this field must be in the &lt;code>[30d,90d]&lt;/code> range.
The background for this limitation is that all Gardener components to rely on the &lt;code>TokenRequest&lt;/code> API and the Kubernetes service account token projection feature with short-lived, auto-rotating tokens.
Any values lower than &lt;code>30d&lt;/code> risk impacting the SLO for shoot clusters, and any values above &lt;code>90d&lt;/code> violate security best practices with respect to maximum validity of credentials before they must be rotated.
Given that the field just specifies the upper bound, end-users can still use lower values for their individual workload by specifying the &lt;code>.spec.volumes[].projected.sources[].serviceAccountToken.expirationSeconds&lt;/code> in the &lt;code>PodSpec&lt;/code>s.&lt;/p></description></item><item><title>Docs: Shoot Status</title><link>https://gardener.cloud/docs/gardener/usage/shoot_status/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_status/</guid><description>
&lt;h1 id="shoot-status">Shoot Status&lt;/h1>
&lt;p>This document provides an overview of the &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#shootstatus">ShootStatus&lt;/a>.&lt;/p>
&lt;h2 id="conditions">Conditions&lt;/h2>
&lt;p>The Shoot status consists of a set of conditions. A &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#condition">Condition&lt;/a> has the following fields:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field name&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>type&lt;/code>&lt;/td>
&lt;td>Name of the condition.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>status&lt;/code>&lt;/td>
&lt;td>Indicates whether the condition is applicable, with possible values &lt;code>True&lt;/code>, &lt;code>False&lt;/code>, &lt;code>Unknown&lt;/code>, or &lt;code>Progressing&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>lastTransitionTime&lt;/code>&lt;/td>
&lt;td>Timestamp for when the condition last transitioned from one status to another.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>lastUpdateTime&lt;/code>&lt;/td>
&lt;td>Timestamp for when the condition was updated. Usually changes when &lt;code>reason&lt;/code> or &lt;code>message&lt;/code> in condition is updated.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>reason&lt;/code>&lt;/td>
&lt;td>Machine-readable, UpperCamelCase text indicating the reason for the condition&amp;rsquo;s last transition.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>message&lt;/code>&lt;/td>
&lt;td>Human-readable message indicating details about the last status transition.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>codes&lt;/code>&lt;/td>
&lt;td>Well-defined error codes in case the condition reports a problem.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Currently the available Shoot condition types are:&lt;/p>
&lt;ul>
&lt;li>&lt;code>APIServerAvailable&lt;/code>&lt;/li>
&lt;li>&lt;code>ControlPlaneHealthy&lt;/code>&lt;/li>
&lt;li>&lt;code>EveryNodeReady&lt;/code>&lt;/li>
&lt;li>&lt;code>ObservabilityComponentsHealthy&lt;/code>&lt;/li>
&lt;li>&lt;code>SystemComponentsHealthy&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>The Shoot conditions are maintained by the &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/gardenlet/controller/shoot/care/reconciler.go">shoot care reconciler&lt;/a> of gardenlet.
Find more information in &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/#shoot-controller">this document&lt;/a>.&lt;/p>
&lt;h3 id="sync-period">Sync Period&lt;/h3>
&lt;p>The condition checks are executed periodically at interval which is configurable in the &lt;code>GardenletConfiguration&lt;/code> (&lt;code>.controllers.shootCare.syncPeriod&lt;/code>, defaults to &lt;code>1m&lt;/code>).&lt;/p>
&lt;h3 id="condition-thresholds">Condition Thresholds&lt;/h3>
&lt;p>The &lt;code>GardenletConfiguration&lt;/code> also allows configuring condition thresholds (&lt;code>controllers.shootCare.conditionThresholds&lt;/code>). Condition threshold is the amount of time to consider condition as &lt;code>Processing&lt;/code> on condition status changes.&lt;/p>
&lt;p>Let&amp;rsquo;s check the following example to get better understanding. Let&amp;rsquo;s say that the &lt;code>APIServerAvailable&lt;/code> condition of our Shoot is with status &lt;code>True&lt;/code>. If the next condition check fails (for example kube-apiserver becomes unreachable), then the condition first goes to &lt;code>Processing&lt;/code> state. Only if this state remains for condition threshold amount of time, then the condition finally is updated to &lt;code>False&lt;/code>.&lt;/p>
&lt;h3 id="constraints">Constraints&lt;/h3>
&lt;p>Constraints represent conditions of a Shoot’s current state that constraint some operations on it.
The current constraints are:&lt;/p>
&lt;p>&lt;strong>&lt;code>HibernationPossible&lt;/code>&lt;/strong>:&lt;/p>
&lt;p>This constraint indicates whether a Shoot is allowed to be hibernated.
The rationale behind this constraint is that a Shoot can have &lt;code>ValidatingWebhookConfiguration&lt;/code>s or &lt;code>MutatingWebhookConfiguration&lt;/code>s acting on resources that are critical for waking up a cluster.
For example, if a webhook has rules for &lt;code>CREATE/UPDATE&lt;/code> Pods or Nodes and &lt;code>failurePolicy=Fail&lt;/code>, the webhook will block joining &lt;code>Nodes&lt;/code> and creating critical system component Pods and thus block the entire wakeup operation, because the server backing the webhook is not running.&lt;/p>
&lt;p>Even if the &lt;code>failurePolicy&lt;/code> is set to &lt;code>Ignore&lt;/code>, high timeouts (&lt;code>&amp;gt;15s&lt;/code>) can lead to blocking requests of control plane components.
That&amp;rsquo;s because most control-plane API calls are made with a client-side timeout of &lt;code>30s&lt;/code>, so if a webhook has &lt;code>timeoutSeconds=30&lt;/code>
the overall request might still fail as there is overhead in communication with the API server and potential other webhooks.
Generally, it&amp;rsquo;s &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#timeouts">best pratice&lt;/a> to specify low timeouts in WebhookConfigs.
Also, it&amp;rsquo;s &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#avoiding-operating-on-the-kube-system-namespace">best practice&lt;/a>
to exclude the &lt;code>kube-system&lt;/code> namespace from webhooks to avoid blocking critical operations on system components of the cluster.
Shoot owners can do so by adding a &lt;code>namespaceSelector&lt;/code> similar to this one to their webhook configurations:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>namespaceSelector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchExpressions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: gardener.cloud/purpose
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> operator: NotIn
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> values:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - kube-system
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If the Shoot still has webhooks with either &lt;code>failurePolicy={Fail,nil}&lt;/code> or &lt;code>failurePolicy=Ignore &amp;amp;&amp;amp; timeoutSeconds&amp;gt;15&lt;/code> that act on &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/operation/botanist/matchers/matcher.go#L60">critical resources&lt;/a> in the &lt;code>kube-system&lt;/code> namespace, Gardener will set the &lt;code>HibernationPossible&lt;/code> to &lt;code>False&lt;/code> indicating, that the Shoot can probably not be woken up again after hibernation without manual intervention of the Gardener Operator.
&lt;code>gardener-apiserver&lt;/code> will prevent any Shoot with the &lt;code>HibernationPossible&lt;/code> constraint set to &lt;code>False&lt;/code> from being hibernated, that is via manual hibernation as well as scheduled hibernation.&lt;/p>
&lt;blockquote>
&lt;p>By setting &lt;code>.controllers.shootCare.webhookRemediatorEnabled=true&lt;/code> in the gardenlet configuration, the auto-remediation of webhooks not following the best practices can be turned on in the shoot clusters.
Concretely, missing &lt;code>namespaceSelector&lt;/code>s or &lt;code>objectSelector&lt;/code>s will be added and too high &lt;code>timeoutSeconds&lt;/code> will be lowered.
In some cases, the &lt;code>failurePolicy&lt;/code> will be set from &lt;code>Fail&lt;/code> to &lt;code>Ignore&lt;/code>.
Gardenlet will also add an annotation to make it visible to end-users that their webhook configurations were mutated and should be fixed by them in the first place.
Note that all of this is no perfect solution and just done on a best effort basis.
Only the owner of the webhook can know whether it indeed is problematic and configured correctly.&lt;/p>
&lt;p>Webhooks labeled with &lt;code>remediation.webhook.shoot.gardener.cloud/exclude=true&lt;/code> will be excluded from auto-remediation.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>&lt;code>MaintenancePreconditionsSatisfied&lt;/code>&lt;/strong>:&lt;/p>
&lt;p>This constraint indicates whether all preconditions for a safe maintenance operation are satisfied (see also &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_maintenance/">this document&lt;/a> for more information about what happens during a shoot maintenance).
As of today, the same checks as in the &lt;code>HibernationPossible&lt;/code> constraint are being performed (user-deployed webhooks that might interfere with potential rolling updates of shoot worker nodes).
There is no further action being performed on this constraint&amp;rsquo;s status (maintenance is still being performed).
It is meant to make the user aware of potential problems that might occur due to his configurations.&lt;/p>
&lt;p>&lt;strong>&lt;code>CACertificateValiditiesAcceptable&lt;/code>&lt;/strong>:&lt;/p>
&lt;p>This constraints indicates that there is at least one CA certificate which expires in less than &lt;code>1y&lt;/code>.
It will not be added to the &lt;code>.status.constraints&lt;/code> if there is no such CA certificate.
However, if it&amp;rsquo;s visible, then a &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_credentials_rotation/#certificate-authorities">credentials rotation operation&lt;/a> should be considered.&lt;/p>
&lt;h3 id="last-operation">Last Operation&lt;/h3>
&lt;p>The Shoot status holds information about the last operation that is performed on the Shoot. The last operation field reflects overall progress and the tasks that are currently being executed. Allowed operation types are &lt;code>Create&lt;/code>, &lt;code>Reconcile&lt;/code>, &lt;code>Delete&lt;/code>, &lt;code>Migrate&lt;/code> and &lt;code>Restore&lt;/code>. Allowed operation states are &lt;code>Processing&lt;/code>, &lt;code>Succeeded&lt;/code>, &lt;code>Error&lt;/code>, &lt;code>Failed&lt;/code>, &lt;code>Pending&lt;/code> and &lt;code>Aborted&lt;/code>. An operation in &lt;code>Error&lt;/code> state is an operation that will be retried for a configurable amount of time (&lt;code>controllers.shoot.retryDuration&lt;/code> field in &lt;code>GardenletConfiguration&lt;/code>, defaults to &lt;code>12h&lt;/code>). If the operation cannot complete successfully for the configured retry duration, it will be marked as &lt;code>Failed&lt;/code>. An operation in &lt;code>Failed&lt;/code> state is an operation that won&amp;rsquo;t be retried automatically (to retry such an operation, see &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_operations/#retry-failed-operation">Retry failed operation&lt;/a>).&lt;/p>
&lt;h3 id="last-errors">Last Errors&lt;/h3>
&lt;p>The Shoot status also contains information about the last occurred error(s) (if any) during an operation. A &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#lasterror">LastError&lt;/a> consists of identifier of the task returned error, human-readable message of the error and error codes (if any) associated with the error.&lt;/p>
&lt;h3 id="error-codes">Error Codes&lt;/h3>
&lt;p>Known error codes are:&lt;/p>
&lt;ul>
&lt;li>&lt;code>ERR_INFRA_UNAUTHENTICATED&lt;/code> - indicates that the last error occurred due to the client request not being completed because it lacks valid authentication credentials for the requested resource. It is classified as a non-retryable error code.&lt;/li>
&lt;li>&lt;code>ERR_INFRA_UNAUTHORIZED&lt;/code> - indicates that the last error occurred due to the server understanding the request but refusing to authorize it. It is classified as a non-retryable error code.&lt;/li>
&lt;li>&lt;code>ERR_INFRA_QUOTA_EXCEEDED&lt;/code> - indicates that the last error occurred due to infrastructure quota limits. It is classified as a non-retryable error code.&lt;/li>
&lt;li>&lt;code>ERR_INFRA_RATE_LIMITS_EXCEEDED&lt;/code> - indicates that the last error occurred due to exceeded infrastructure request rate limits.&lt;/li>
&lt;li>&lt;code>ERR_INFRA_DEPENDENCIES&lt;/code> - indicates that the last error occurred due to dependent objects on the infrastructure level. It is classified as a non-retryable error code.&lt;/li>
&lt;li>&lt;code>ERR_RETRYABLE_INFRA_DEPENDENCIES&lt;/code> - indicates that the last error occurred due to dependent objects on the infrastructure level, but the operation should be retried.&lt;/li>
&lt;li>&lt;code>ERR_INFRA_RESOURCES_DEPLETED&lt;/code> - indicates that the last error occurred due to depleted resource in the infrastructure.&lt;/li>
&lt;li>&lt;code>ERR_CLEANUP_CLUSTER_RESOURCES&lt;/code> - indicates that the last error occurred due to resources in the cluster that are stuck in deletion.&lt;/li>
&lt;li>&lt;code>ERR_CONFIGURATION_PROBLEM&lt;/code> - indicates that the last error occurred due to a configuration problem. It is classified as a non-retryable error code.&lt;/li>
&lt;li>&lt;code>ERR_RETRYABLE_CONFIGURATION_PROBLEM&lt;/code> - indicates that the last error occurred due to a retryable configuration problem. &amp;ldquo;Retryable&amp;rdquo; means that the occurred error is likely to be resolved in a ungraceful manner after given period of time.&lt;/li>
&lt;li>&lt;code>ERR_PROBLEMATIC_WEBHOOK&lt;/code> - indicates that the last error occurred due to a webhook not following the Kubernetes best practices (&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#best-practices-and-warnings">https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#best-practices-and-warnings&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="status-label">Status Label&lt;/h3>
&lt;p>Shoots will be automatically labeled with the &lt;code>shoot.gardener.cloud/status&lt;/code> label.
Its value might either be &lt;code>healthy&lt;/code>, &lt;code>progressing&lt;/code>, &lt;code>unhealthy&lt;/code> or &lt;code>unknown&lt;/code> depending on the &lt;code>.status.conditions&lt;/code>, &lt;code>.status.lastOperation&lt;/code> and &lt;code>status.lastErrors&lt;/code> of the &lt;code>Shoot&lt;/code>.
This can be used as an easy filter method to find shoots based on their &amp;ldquo;health&amp;rdquo; status.&lt;/p></description></item><item><title>Docs: Shoot Supported Architectures</title><link>https://gardener.cloud/docs/gardener/usage/shoot_supported_architectures/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_supported_architectures/</guid><description>
&lt;h1 id="supported-cpu-architectures-for-shoot-worker-nodes">Supported CPU Architectures for Shoot Worker Nodes&lt;/h1>
&lt;p>Users can create shoot clusters with worker groups having virtual machines of different architectures. CPU architecture of each worker pool can be specified in the &lt;code>Shoot&lt;/code> specification as follows:&lt;/p>
&lt;h2 id="example-usage-in-a-shoot">Example Usage in a &lt;code>Shoot&lt;/code>&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> workers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: cpu-worker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machine:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> architecture: &amp;lt;some-cpu-architecture&amp;gt; &lt;span style="color:#008000"># optional&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If no value is specified for the architecture field, it defaults to &lt;code>amd64&lt;/code>. For a valid shoot object, a machine type should be present in the respective &lt;code>CloudProfile&lt;/code> with the same CPU architecture as specified in the &lt;code>Shoot&lt;/code> yaml. Also, a valid machine image should be present in the &lt;code>CloudProfile&lt;/code> that supports the required architecture specified in the &lt;code>Shoot&lt;/code> worker pool.&lt;/p>
&lt;h2 id="example-usage-in-a-cloudprofile">Example Usage in a &lt;code>CloudProfile&lt;/code>&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineImages:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: test-image
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> versions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - architectures: &lt;span style="color:#008000"># optional&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &amp;lt;architecture-1&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &amp;lt;architecture-2&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.2.3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineTypes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - architecture: &amp;lt;some-cpu-architecture&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cpu: &lt;span style="color:#a31515">&amp;#34;2&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> gpu: &lt;span style="color:#a31515">&amp;#34;0&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memory: 8Gi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: test-machine
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Currently, Gardener supports two most widely used CPU architectures:&lt;/p>
&lt;ul>
&lt;li>&lt;code>amd64&lt;/code>&lt;/li>
&lt;li>&lt;code>arm64&lt;/code>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Shoot Updates</title><link>https://gardener.cloud/docs/gardener/usage/shoot_updates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_updates/</guid><description>
&lt;h1 id="shoot-updates-and-upgrades">Shoot Updates and Upgrades&lt;/h1>
&lt;p>This document describes what happens during shoot updates (changes incorporated in a newly deployed Gardener version) and during shoot upgrades (changes for version controllable by end-users).&lt;/p>
&lt;h2 id="updates">Updates&lt;/h2>
&lt;p>Updates to all aspects of the shoot cluster happen when the gardenlet reconciles the &lt;code>Shoot&lt;/code> resource.&lt;/p>
&lt;h3 id="when-are-reconciliations-triggered">When are Reconciliations Triggered&lt;/h3>
&lt;p>Generally, when you change the specification of your &lt;code>Shoot&lt;/code> the reconciliation will start immediately, potentially updating your cluster.
Please note that you can also confine the reconciliation triggered due to your specification updates to the cluster&amp;rsquo;s maintenance time window. Please find more information &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_maintenance/#confine-specification-changesupdates-roll-out">here&lt;/a>.&lt;/p>
&lt;p>You can also annotate your shoot with special operation annotations (see &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_operations/">this document&lt;/a>) which will cause the reconciliation to start due to your actions.&lt;/p>
&lt;p>There is also an automatic reconciliation by Gardener.
The period, i.e., how often it is performed, depends on the configuration of the Gardener administrators/operators.
In some Gardener installations the operators might enable &amp;ldquo;reconciliation in maintenance time window only&amp;rdquo; (&lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_maintenance/#cluster-reconciliation">more information&lt;/a>) which will result in at least one reconciliation during the time configured in the &lt;code>Shoot&lt;/code>&amp;rsquo;s &lt;code>.spec.maintenance.timeWindow&lt;/code> field.&lt;/p>
&lt;h3 id="which-updates-are-applied">Which Updates are Applied&lt;/h3>
&lt;p>As end-users can only control the &lt;code>Shoot&lt;/code> resource&amp;rsquo;s specification but not the used Gardener version, they don&amp;rsquo;t have any influence on which of the updates are rolled out (other than those settings configurable in the &lt;code>Shoot&lt;/code>).
A Gardener operator can deploy a new Gardener version at any point in time.
Any subsequent reconciliation of &lt;code>Shoot&lt;/code>s will update them by rolling out the changes incorporated in this new Gardener version.&lt;/p>
&lt;p>Some examples for such shoot updates are:&lt;/p>
&lt;ul>
&lt;li>Add a new/remove an old component to/from the shoot&amp;rsquo;s control plane running in the seed, or to/from the shoot&amp;rsquo;s system components running on the worker nodes.&lt;/li>
&lt;li>Change the configuration of an existing control plane/system component.&lt;/li>
&lt;li>Restart of existing control plane/system components (this might result in a short unavailability of the Kubernetes API server, e.g., when etcd or a kube-apiserver itself is being restarted)&lt;/li>
&lt;/ul>
&lt;h3 id="behavioural-changes">Behavioural Changes&lt;/h3>
&lt;p>Generally, some of such updates (e.g., configuration changes) could theoretically result in different behaviour of controllers.
If such changes would be backwards-incompatible then we usually follow one of those approaches (depends on the concrete change):&lt;/p>
&lt;ul>
&lt;li>Only apply the change for new clusters.&lt;/li>
&lt;li>Expose a new field in the &lt;code>Shoot&lt;/code> resource that lets users control this changed behaviour to enable it at a convenient point in time.&lt;/li>
&lt;li>Put the change behind an alpha feature gate (disabled by default) in the gardenlet (only controllable by Gardener operators) which will be promoted to beta (enabled by default) in subsequent releases (in this case, end-users have no influence on when the behaviour changes - Gardener operators should inform their end-users and provide clear timelines when they will enable the feature gate).&lt;/li>
&lt;/ul>
&lt;h2 id="upgrades">Upgrades&lt;/h2>
&lt;p>We consider shoot upgrades to change either the&lt;/p>
&lt;ul>
&lt;li>Kubernetes version (&lt;code>.spec.kubernetes.version&lt;/code>)&lt;/li>
&lt;li>Kubernetes version of the worker pool if specified (&lt;code>.spec.provider.workers[].kubernetes.version&lt;/code>)&lt;/li>
&lt;li>Machine image version of at least one worker pool (&lt;code>.spec.provider.workers[].machine.image.version&lt;/code>)&lt;/li>
&lt;/ul>
&lt;p>Generally, an upgrade is also performed through a reconciliation of the &lt;code>Shoot&lt;/code> resource, i.e., the same concepts like for &lt;a href="#updates">shoot updates&lt;/a> apply.
If an end-user triggers an upgrade (e.g., by changing the Kubernetes version) after a new Gardener version was deployed but before the shoot was reconciled again, then this upgrade might incorporate the changes delivered with this new Gardener version.&lt;/p>
&lt;h3 id="in-place-vs-rolling-updates">In-Place vs. Rolling Updates&lt;/h3>
&lt;p>If the Kubernetes patch version is changed then the upgrade happens in-place.
This means that the shoot worker nodes remain untouched and only the &lt;code>kubelet&lt;/code> process restarts with the new Kubernetes version binary.
The same applies for configuration changes of the kubelet.&lt;/p>
&lt;p>If the Kubernetes minor version is changed then the upgrade is done in a &amp;ldquo;rolling update&amp;rdquo; fashion, similar to how pods in Kubernetes are updated (when backed by a &lt;code>Deployment&lt;/code>).
The worker nodes will be terminated one after another and replaced by new machines.
The existing workload is gracefully drained and evicted from the old worker nodes to new worker nodes, respecting the configured &lt;code>PodDisruptionBudget&lt;/code>s (see &lt;a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">Kubernetes documentation&lt;/a>).&lt;/p>
&lt;h4 id="customize-rolling-update-behaviour-of-shoot-worker-nodes">Customize Rolling Update Behaviour of Shoot Worker Nodes&lt;/h4>
&lt;p>The &lt;code>.spec.provider.workers[]&lt;/code> list exposes two fields that you might configure based on your workload&amp;rsquo;s needs: &lt;code>maxSurge&lt;/code> and &lt;code>maxUnavailable&lt;/code>.
The same concepts &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment">like in Kubernetes&lt;/a> apply.
Additionally, you might customize how the machine-controller-manager (abbrev.: MCM; the component instrumenting this rolling update) is behaving. You can configure the following fields in &lt;code>.spec.provider.worker[].machineControllerManager&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>machineDrainTimeout&lt;/code>: Timeout (in duration) used while draining of machine before deletion, beyond which MCM forcefully deletes machine (default: &lt;code>10m&lt;/code>).&lt;/li>
&lt;li>&lt;code>machineHealthTimeout&lt;/code>: Timeout (in duration) used while re-joining (in case of temporary health issues) of machine before it is declared as failed (default: &lt;code>10m&lt;/code>).&lt;/li>
&lt;li>&lt;code>machineCreationTimeout&lt;/code>: Timeout (in duration) used while joining (during creation) of machine before it is declared as failed (default: &lt;code>10m&lt;/code>).&lt;/li>
&lt;li>&lt;code>maxEvictRetries&lt;/code>: Maximum number of times evicts would be attempted on a pod before it is forcibly deleted during draining of a machine (default: &lt;code>10&lt;/code>).&lt;/li>
&lt;li>&lt;code>nodeConditions&lt;/code>: List of case-sensitive node-conditions which will change a machine to a &lt;code>Failed&lt;/code> state after the &lt;code>machineHealthTimeout&lt;/code> duration. It may further be replaced with a new machine if the machine is backed by a machine-set object (defaults: &lt;code>KernelDeadlock&lt;/code>, &lt;code>ReadonlyFilesystem&lt;/code> , &lt;code>DiskPressure&lt;/code>).&lt;/li>
&lt;/ul>
&lt;h4 id="rolling-update-triggers">Rolling Update Triggers&lt;/h4>
&lt;p>Apart from the above mentioned triggers, a rolling update of the shoot worker nodes is also triggered for some changes to your worker pool specification (&lt;code>.spec.provider.workers[]&lt;/code>, even if you don&amp;rsquo;t change the Kubernetes or machine image version).
The complete list of fields that trigger a rolling update:&lt;/p>
&lt;ul>
&lt;li>&lt;code>.spec.kubernetes.version&lt;/code> (except for patch version changes)&lt;/li>
&lt;li>&lt;code>.spec.provider.workers[].machine.image.name&lt;/code>&lt;/li>
&lt;li>&lt;code>.spec.provider.workers[].machine.image.version&lt;/code>&lt;/li>
&lt;li>&lt;code>.spec.provider.workers[].machine.type&lt;/code>&lt;/li>
&lt;li>&lt;code>.spec.provider.workers[].volume.type&lt;/code>&lt;/li>
&lt;li>&lt;code>.spec.provider.workers[].volume.size&lt;/code>&lt;/li>
&lt;li>&lt;code>.spec.provider.workers[].providerConfig&lt;/code>&lt;/li>
&lt;li>&lt;code>.spec.provider.workers[].cri.name&lt;/code>&lt;/li>
&lt;li>&lt;code>.spec.provider.workers[].kubernetes.version&lt;/code> (except for patch version changes)&lt;/li>
&lt;li>&lt;code>.status.credentials.rotation.certificateAuthorities.lastInitiationTime&lt;/code> (changed by gardener when a shoot CA rotation is initiated)&lt;/li>
&lt;li>&lt;code>.status.credentials.rotation.serviceAccountKey.lastInitiationTime&lt;/code> (changed by gardener when a shoot service account signing key rotation is initiated)&lt;/li>
&lt;/ul>
&lt;p>Generally, the provider extension controllers might have additional constraints for changes leading to rolling updates, so please consult the respective documentation as well.&lt;/p>
&lt;h2 id="related-documentation">Related Documentation&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_operations/">Shoot Operations&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_maintenance/">Shoot Maintenance&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_maintenance/#confine-specification-changesupdates-roll-out">Confine Specification Changes/Updates Roll Out To Maintenance Time Window&lt;/a>.&lt;/li>
&lt;/ul></description></item><item><title>Docs: Shoot Versions</title><link>https://gardener.cloud/docs/gardener/usage/shoot_versions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_versions/</guid><description>
&lt;h1 id="shoot-kubernetes-and-operating-system-versioning-in-gardener">Shoot Kubernetes and Operating System Versioning in Gardener&lt;/h1>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>On the one hand-side, Gardener is responsible for managing the Kubernetes and the Operating System (OS) versions of its Shoot clusters.
On the other hand-side, Gardener needs to be configured and updated based on the availability and support of the Kubernetes and Operating System version it provides.
For instance, the Kubernetes community releases &lt;strong>minor&lt;/strong> versions roughly every three months and usually maintains &lt;strong>three minor&lt;/strong> versions (the current and the last two) with bug fixes and security updates.
Patch releases are done more frequently.&lt;/p>
&lt;p>When using the term &lt;code>Machine image&lt;/code> in the following, we refer to the OS version that comes with the machine image of the node/worker pool of a Gardener Shoot cluster.
As such we are not referring to the &lt;code>CloudProvider&lt;/code> specific machine image like the &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html">&lt;code>AMI&lt;/code>&lt;/a> for AWS.
For more information how Gardener maps machine image versions to &lt;code>CloudProvider&lt;/code> specific machine images, take a look at the individual gardener extension providers
such as the &lt;a href="https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/usage-as-operator/">provider for AWS&lt;/a>.&lt;/p>
&lt;p>Gardener should be configured accordingly to reflect the &amp;ldquo;logical state&amp;rdquo; of a version.
It should be possible to define the Kubernetes or Machine image versions that still receive bug fixes and security patches, and also vice-versa to define the version that are out-of-maintenance and are potentially vulnerable.
Moreover, this allows Gardener to &amp;ldquo;understand&amp;rdquo; the current state of a version and act upon it (more information in the following sections).&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>&lt;strong>As a Gardener operator&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>I can classify a version based on it&amp;rsquo;s logical state (&lt;code>preview&lt;/code>, &lt;code>supported&lt;/code>, &lt;code>deprecated&lt;/code> and &lt;code>expired&lt;/code> see &lt;a href="#version-classifications">Version Classification&lt;/a>).&lt;/li>
&lt;li>I can define which Machine image and Kubernetes versions are eligible for the auto update of clusters during the maintenance time.&lt;/li>
&lt;li>I can disallow the creation of clusters having a certain version (think of severe security issues).&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>As an end-user/Shoot owner of Gardener&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>I can get information about which Kubernetes and Machine image versions exist and their classification.&lt;/li>
&lt;li>I can determine the time when my Shoot clusters Machine image and Kubernetes version will be forcefully updated to the next patch or minor version (in case the cluster is running a deprecated version with an expiration date).&lt;/li>
&lt;li>I can get this information via API from the &lt;code>CloudProfile&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="version-classifications">Version Classifications&lt;/h2>
&lt;p>Administrators can classify versions into four distinct &amp;ldquo;logical states&amp;rdquo;: &lt;code>preview&lt;/code>, &lt;code>supported&lt;/code>, &lt;code>deprecated&lt;/code> and &lt;code>expired&lt;/code>.
The version classification serves as a &amp;ldquo;point-of-reference&amp;rdquo; for end-users and also has implications during shoot creation and the maintenance time.&lt;/p>
&lt;p>If a version is unclassified, Gardener cannot make those decision based on the &amp;ldquo;logical state&amp;rdquo;.
Nevertheless, Gardener can operate without version classifications and can be added at any time to the Kubernetes and machine image versions in the &lt;code>CloudProfile&lt;/code>.&lt;/p>
&lt;p>As a best practice, versions usually start with the classification &lt;code>preview&lt;/code>, then are promoted to &lt;code>supported&lt;/code>, eventually &lt;code>deprecated&lt;/code> and finally &lt;code>expired&lt;/code>.
This information is programmatically available in the &lt;code>CloudProfiles&lt;/code> of the Garden cluster.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>preview:&lt;/strong> A &lt;code>preview&lt;/code> version is a new version that has not yet undergone thorough testing, possibly a new release, and needs time to be validated.
Due to its short early age, there is a higher probability of undiscovered issues and is therefore not yet recommended for production usage.
A Shoot does not update (neither &lt;code>auto-update&lt;/code> or &lt;code>force-update&lt;/code>) to a &lt;code>preview&lt;/code> version during the maintenance time.
Also &lt;code>preview&lt;/code> versions are not considered for the defaulting to the highest available version when deliberately omitting the patch version during Shoot creation.
Typically, after a fresh release of a new Kubernetes (e.g. v1.25.0) or Machine image version (e.g. suse-chost 15.4.20220818), the operator tags it as &lt;code>preview&lt;/code> until he has gained sufficient experience and regards this version to be reliable.
After the operator gained sufficient trust, the version can be manually promoted to &lt;code>supported&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>supported:&lt;/strong> A &lt;code>supported&lt;/code> version is the recommended version for new and existing Shoot clusters. New Shoot clusters should use and existing clusters should update to this version.
Typically for Kubernetes versions, the latest Kubernetes patch versions of the actual (if not still in &lt;code>preview&lt;/code>) and the last 3 minor Kubernetes versions are maintained by the community. An operator could define these versions as being &lt;code>supported&lt;/code> (e.g. v1.24.6, v1.23.12 and v1.22.15).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>deprecated:&lt;/strong> A &lt;code>deprecated&lt;/code> version is a version that approaches the end of its lifecycle and can contain issues which are probably resolved in a supported version.
New Shoots should not use this version any more.
Existing Shoots will be updated to a newer version if &lt;code>auto-update&lt;/code> is enabled (&lt;code>.spec.maintenance.autoUpdate.kubernetesVersion&lt;/code> for Kubernetes version &lt;code>auto-update&lt;/code>, or &lt;code>.spec.maintenance.autoUpdate.machineImageVersion&lt;/code> for machine image version &lt;code>auto-update&lt;/code>).
Using automatic upgrades, however, does not guarantee that a Shoot runs a non-deprecated version, as the latest version (overall or of the minor version) can be deprecated as well.
Deprecated versions &lt;strong>should&lt;/strong> have an expiration date set for eventual expiration.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>expired:&lt;/strong> An &lt;code>expired&lt;/code> versions has an expiration date (based on the &lt;a href="https://golang.org/src/time/time.go">Golang time package&lt;/a>) in the past.
New clusters with that version cannot be created and existing clusters are forcefully migrated to a higher version during the maintenance time.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Below is an example how the relevant section of the &lt;code>CloudProfile&lt;/code> might look like:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: CloudProfile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: alicloud
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> versions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - classification: preview
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.25.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - classification: supported
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.24.6
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - classification: deprecated
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> expirationDate: &lt;span style="color:#a31515">&amp;#34;2022-11-30T23:59:59Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.24.5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - classification: supported
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.23.12
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - classification: deprecated
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> expirationDate: &lt;span style="color:#a31515">&amp;#34;2023-01-31T23:59:59Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.23.11
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - classification: supported
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.22.15
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - classification: deprecated
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.21.14
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="version-requirements-kubernetes-and-machine-image">Version Requirements (Kubernetes and Machine image)&lt;/h2>
&lt;p>The Gardener API server enforces the following requirements for versions:&lt;/p>
&lt;h3 id="deletion-of-a-version">Deletion of a version&lt;/h3>
&lt;ul>
&lt;li>A version that is in use by a Shoot cannot be deleted from the &lt;code>CloudProfile&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h3 id="adding-a-version">Adding a version&lt;/h3>
&lt;ul>
&lt;li>A version must not have an expiration date in the past.&lt;/li>
&lt;li>There can be only one &lt;code>supported&lt;/code> version per minor version.&lt;/li>
&lt;li>The latest Kubernetes version cannot have an expiration date.&lt;/li>
&lt;li>The latest version for a machine image can have an expiration date. [*]&lt;/li>
&lt;/ul>
&lt;p>&lt;sub>[*] Useful for cases in which support for given machine image needs to be deprecated and removed (for example the machine image reaches end of life).&lt;/sub>&lt;/p>
&lt;h2 id="forceful-migration-of-expired-versions">Forceful migration of expired versions&lt;/h2>
&lt;p>If a Shoot is running a version after its expiration date has passed, it will be forcefully migrated during its maintenance time.
This happens &lt;strong>even if the owner has opted out of automatic cluster updates!&lt;/strong>&lt;/p>
&lt;p>For &lt;strong>Machine images&lt;/strong>, the Shoots worker pools will be updated to the latest &lt;code>non-preview&lt;/code> version of the pools respective image.&lt;/p>
&lt;p>For &lt;strong>Kubernetes versions&lt;/strong>, the forceful update picks the latest &lt;code>non-preview&lt;/code> patch version of the current minor version.&lt;/p>
&lt;p>If the cluster is already on the latest patch version and the latest patch version is also expired,
it will continue with the latest patch version of the &lt;strong>next consecutive minor Kubernetes version&lt;/strong>, so &lt;strong>it will result in an
update of a minor Kubernetes version!&lt;/strong>&lt;/p>
&lt;p>Please note, that multiple consecutive minor version upgrades are possible.
This can occur if the Shoot is updated to a version that in turn is also &lt;code>expired&lt;/code>.
In this case, the version is again upgraded in the &lt;strong>next&lt;/strong> maintenance time.&lt;/p>
&lt;p>&lt;strong>Depending on the circumstances described above, it can happen that the cluster receives multiple consecutive minor Kubernetes version updates!&lt;/strong>&lt;/p>
&lt;p>Kubernetes &amp;ldquo;minor version jumps&amp;rdquo; are not allowed - meaning to skip the update to the consecutive minor version and directly update to any version after that.
For instance, the version &lt;code>1.20.x&lt;/code> can only update to a version &lt;code>1.21.x&lt;/code>, not to &lt;code>1.22.x&lt;/code> or any other version.
This is because Kubernetes does not guarantee upgradeability in this case, leading to possibly broken Shoot clusters.
The administrator has to set up the &lt;code>CloudProfile&lt;/code> in such a way, that consecutive Kubernetes minor versions are available.
Otherwise, Shoot clusters will fail to upgrade during the maintenance time.&lt;/p>
&lt;p>Consider the &lt;code>CloudProfile&lt;/code> below with a Shoot using the Kubernetes version &lt;code>1.20.12&lt;/code>.
Even though the version is &lt;code>expired&lt;/code>, due to missing &lt;code>1.21.x&lt;/code> versions, the Gardener Controller Manager cannot upgrade the Shoot&amp;rsquo;s Kubernetes version.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> versions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.22.8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.22.7
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.20.12
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> expirationDate: &lt;span style="color:#a31515">&amp;#34;&amp;lt;expiration date in the past&amp;gt;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>CloudProfile&lt;/code> must specify versions &lt;code>1.21.x&lt;/code> of the &lt;strong>consecutive&lt;/strong> minor version.
Configuring the &lt;code>CloudProfile&lt;/code> in such a way, the Shoot&amp;rsquo;s Kubernetes version will be upgraded to version &lt;code>1.21.10&lt;/code> in the next maintenance time.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> versions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.22.8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.21.10
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.21.09
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.20.12
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> expirationDate: &lt;span style="color:#a31515">&amp;#34;&amp;lt;expiration date in the past&amp;gt;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="related-documentation">Related Documentation&lt;/h2>
&lt;p>You might want to read about the &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_updates/">Shoot Updates and Upgrades&lt;/a> procedures to get to know the effects of such operations.&lt;/p></description></item><item><title>Docs: Shoot Workers Settings</title><link>https://gardener.cloud/docs/gardener/usage/shoot_workers_settings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/shoot_workers_settings/</guid><description>
&lt;h1 id="shoot-worker-nodes-settings">Shoot Worker Nodes Settings&lt;/h1>
&lt;p>Users can configure settings affecting all worker nodes via &lt;code>.spec.provider.workersSettings&lt;/code> in the &lt;code>Shoot&lt;/code> resource.&lt;/p>
&lt;h2 id="ssh-access">SSH Access&lt;/h2>
&lt;p>&lt;code>SSHAccess&lt;/code> indicates whether the &lt;code>sshd.service&lt;/code> should be running on the worker nodes. This is ensured by a systemd service called &lt;code>sshd-ensurer.service&lt;/code> which runs every 15 seconds on each worker node. When set to &lt;code>true&lt;/code>, the systemd service ensures that the &lt;code>sshd.service&lt;/code> is enabled and running. If it is set to &lt;code>false&lt;/code>, the systemd service ensures that &lt;code>sshd.service&lt;/code> is stopped and disabled. This also terminates all established SSH connections. In addition, when this value is set to &lt;code>false&lt;/code>, existing &lt;code>Bastion&lt;/code> resources are deleted during &lt;code>Shoot&lt;/code> reconciliation and new ones are prevented from being created, SSH keypairs are not created/rotated, SSH keypair secrets are deleted from the Garden cluster, and the &lt;code>gardener-user.service&lt;/code> is not deployed to the worker nodes.&lt;/p>
&lt;p>&lt;code>sshAccess.enabled&lt;/code> is set to &lt;code>true&lt;/code> by default.&lt;/p>
&lt;h3 id="example-usage-in-a-shoot">Example Usage in a &lt;code>Shoot&lt;/code>&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> workersSettings:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sshAccess:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">false&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Supported K8s Versions</title><link>https://gardener.cloud/docs/gardener/usage/supported_k8s_versions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/supported_k8s_versions/</guid><description>
&lt;h1 id="supported-kubernetes-versions">Supported Kubernetes Versions&lt;/h1>
&lt;p>Currently, Gardener supports the following Kubernetes versions:&lt;/p>
&lt;h2 id="garden-clusters">Garden Clusters&lt;/h2>
&lt;p>The minimum version of a garden cluster that can be used to run Gardener is &lt;strong>&lt;code>1.20.x&lt;/code>&lt;/strong>.&lt;/p>
&lt;h2 id="seed-clusters">Seed Clusters&lt;/h2>
&lt;p>The minimum version of a seed cluster that can be connected to Gardener is &lt;strong>&lt;code>1.20.x&lt;/code>&lt;/strong>.
Please note that Gardener does not support seeds with version &lt;strong>&amp;gt;= &lt;code>1.25&lt;/code>&lt;/strong> yet when the &lt;code>HVPA&lt;/code> or &lt;code>HVPAForShootedSeed&lt;/code> feature gate is enabled. For more details, see &lt;a href="https://github.com/gardener/gardener/issues/6893">https://github.com/gardener/gardener/issues/6893&lt;/a>.&lt;/p>
&lt;h2 id="shoot-clusters">Shoot Clusters&lt;/h2>
&lt;p>Gardener itself is capable of spinning up clusters with Kubernetes versions &lt;strong>&lt;code>1.20&lt;/code>&lt;/strong> up to &lt;strong>&lt;code>1.26&lt;/code>&lt;/strong>.
However, the concrete versions that can be used for shoot clusters depend on the installed provider extension.
Consequently, please consult the documentation of your provider extension to see which Kubernetes versions are supported for shoot clusters.&lt;/p>
&lt;blockquote>
&lt;p>👨🏼‍💻 Developers note: &lt;a href="https://gardener.cloud/docs/gardener/development/new-kubernetes-version/">This document&lt;/a> explains what needs to be done in order to add support for a new Kubernetes version.&lt;/p>
&lt;/blockquote>
&lt;h2 id="support-timeline">Support Timeline&lt;/h2>
&lt;p>The Kubernetes project maintains the most recent three minor releases and releases a new minor version every 4 months.
This means that a release has patch support for approximately 1 year.
See &lt;a href="https://kubernetes.io/releases/">this document&lt;/a> for the official upstream information.&lt;/p>
&lt;p>In the past, the Gardener project did not have a policy regarding the number of supported Kubernetes versions at the same time.
Beginning with 2023, a new policy has been introduced:&lt;/p>
&lt;blockquote>
&lt;p>The Gardener project supports the last four Kubernetes minor versions and drops support for the oldest minor version as soon as support for a new minor version has been introduced.&lt;/p>
&lt;/blockquote>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Kubernetes Version&lt;/th>
&lt;th>End of Life&lt;/th>
&lt;th>Supported Since&lt;/th>
&lt;th>Support Dropped After&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1.20&lt;/td>
&lt;td>2022-02-28&lt;/td>
&lt;td>v1.15.0&lt;/td>
&lt;td>2023-01-31&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.21&lt;/td>
&lt;td>2022-06-28&lt;/td>
&lt;td>v1.21.0&lt;/td>
&lt;td>2023-02-28&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.22&lt;/td>
&lt;td>2022-10-28&lt;/td>
&lt;td>v1.31.0&lt;/td>
&lt;td>2023-04-30&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.23&lt;/td>
&lt;td>2023-02-28&lt;/td>
&lt;td>v1.39.0&lt;/td>
&lt;td>1.27 is supported (&amp;gt; 2023-04)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.24&lt;/td>
&lt;td>2023-07-28&lt;/td>
&lt;td>v1.48.0&lt;/td>
&lt;td>1.28 is supported (&amp;gt; 2023-08)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.25&lt;/td>
&lt;td>2023-10-28&lt;/td>
&lt;td>v1.56.0&lt;/td>
&lt;td>1.29 is supported (&amp;gt; 2023-12)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.26&lt;/td>
&lt;td>2024-02-28&lt;/td>
&lt;td>v1.63.0&lt;/td>
&lt;td>1.30 is supported (&amp;gt; 2024-04)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The three versions 1.20, 1.21, 1.22 (which all are officially out of maintenance already) are handled specially to allow users to adapt to this new policy.
Beginning with 1.23, the support of the oldest version is dropped after the support of a new version was introduced.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ Note that this guideline only concerns the code of &lt;code>gardener/gardener&lt;/code> and is not related to the versions offered in &lt;code>CloudProfile&lt;/code>s.
It is recommended to always only offer the last three minor versions with &lt;code>supported&lt;/code> classification in &lt;code>CloudProfile&lt;/code>s and deprecate the oldest version with an expiration date before a new minor Kubernetes version is released.&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: Tolerations</title><link>https://gardener.cloud/docs/gardener/usage/tolerations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/tolerations/</guid><description>
&lt;h1 id="taints-and-tolerations-for-seeds-and-shoots">Taints and Tolerations for &lt;code>Seed&lt;/code>s and &lt;code>Shoot&lt;/code>s&lt;/h1>
&lt;p>Similar to &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">taints and tolerations&lt;/a> for &lt;code>Node&lt;/code>s and &lt;code>Pod&lt;/code>s in Kubernetes, the &lt;code>Seed&lt;/code> resource supports specifying taints (&lt;code>.spec.taints&lt;/code>, see &lt;a href="https://github.com/gardener/gardener/blob/master/example/50-seed.yaml#L48-L55">this example&lt;/a>) while the &lt;code>Shoot&lt;/code> resource supports specifying tolerations (&lt;code>.spec.tolerations&lt;/code>, see &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L268-L269">this example&lt;/a>).
The feature is used to control scheduling to seeds as well as decisions whether a shoot can use a certain seed.&lt;/p>
&lt;p>Compared to Kubernetes, Gardener&amp;rsquo;s taints and tolerations are very much down-stripped right now and have some behavioral differences.
Please read the following explanations carefully if you plan to use it.&lt;/p>
&lt;h2 id="scheduling">Scheduling&lt;/h2>
&lt;p>When scheduling a new shoot then the gardener-scheduler will filter all seed candidates whose taints are not tolerated by the shoot.
As Gardener&amp;rsquo;s taints/tolerations don&amp;rsquo;t support &lt;code>effect&lt;/code>s yet you can compare this behaviour with using a &lt;code>NoSchedule&lt;/code> effect taint in Kubernetes.&lt;/p>
&lt;p>Be reminded that taints/tolerations are no means to define any affinity or selection for seeds - please use &lt;code>.spec.seedSelector&lt;/code> in the &lt;code>Shoot&lt;/code> to state such desires.&lt;/p>
&lt;p>⚠️ Please note that - unlike how it&amp;rsquo;s implemented in Kubernetes - a certain seed cluster &lt;strong>may&lt;/strong> only be used when the shoot tolerates &lt;strong>all&lt;/strong> the seed&amp;rsquo;s taints.
This means that specifying &lt;code>.spec.seedName&lt;/code> for a seed whose taints are not tolerated will make the gardener-apiserver rejecting the request.&lt;/p>
&lt;p>Consequently, the taints/tolerations feature can be used as means to restrict usage of certain seeds.&lt;/p>
&lt;h2 id="toleration-defaults-and-whitelist">Toleration Defaults and Whitelist&lt;/h2>
&lt;p>The &lt;code>Project&lt;/code> resource features a &lt;code>.spec.tolerations&lt;/code> object that may carry &lt;code>defaults&lt;/code> and a &lt;code>whitelist&lt;/code> (see &lt;a href="https://github.com/gardener/gardener/blob/master/example/05-project-dev.yaml#L33-L37">this example&lt;/a>).
The corresponding &lt;code>ShootTolerationRestriction&lt;/code> admission plugin (cf. Kubernetes&amp;rsquo; &lt;code>PodTolerationRestriction&lt;/code> admission plugin) is responsible for evaluating these settings during creation/update of &lt;code>Shoot&lt;/code>s.&lt;/p>
&lt;h3 id="whitelist">Whitelist&lt;/h3>
&lt;p>If a shoot gets created or updated with tolerations then it is validated that only those tolerations may be used which were added to either a) the &lt;code>Project&lt;/code>&amp;rsquo;s &lt;code>.spec.tolerations.whitelist&lt;/code>, or b) to the global whitelist in the &lt;code>ShootTolerationRestriction&lt;/code>&amp;rsquo;s admission config (see &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-admissionconfig.yaml#L7-L14">this example&lt;/a>).&lt;/p>
&lt;p>⚠️ Please note that the tolerations whitelist of &lt;code>Project&lt;/code>s can only be changed if the user trying to change it is bound to the &lt;code>modify-spec-tolerations-whitelist&lt;/code> custom RBAC role, e.g. via the following &lt;code>ClusterRole&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: rbac.authorization.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ClusterRole
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: full-project-modification-access
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- apiGroups:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - core.gardener.cloud
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - projects
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> verbs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - create
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - patch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - update
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - modify-spec-tolerations-whitelist
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - delete
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="defaults">Defaults&lt;/h3>
&lt;p>If a shoot gets created then the default tolerations specified in both the &lt;code>Project&lt;/code>&amp;rsquo;s &lt;code>.spec.tolerations.defaults&lt;/code> and global default list in the &lt;code>ShootTolerationRestriction&lt;/code> admission plugin&amp;rsquo;s configuration will be added to the &lt;code>.spec.tolerations&lt;/code> of the &lt;code>Shoot&lt;/code> (unless it already specifies a certain key).&lt;/p></description></item><item><title>Docs: Trouble Shooting Guide</title><link>https://gardener.cloud/docs/gardener/usage/trouble_shooting_guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/trouble_shooting_guide/</guid><description>
&lt;h1 id="trouble-shooting-guide">Trouble Shooting Guide&lt;/h1>
&lt;h2 id="are-there-really-issue-that-cannot-be-fixed-o">Are there really issue that cannot be fixed :O?&lt;/h2>
&lt;p>Well, of course not :P. With continuous development of Gardener, over the time its architecture and API might have to be changed to reduce complexity and support more features. In this process developers are bound to keep Gardener version backward compatible with last two releases. But maintaining backward compatibility is quite complex and effortful tasks. So, to save short term complex effort, its common practice in open source community to use work around or hacky solutions sometimes. This results in rare issues which are supposed to be resolved by human interaction across upgrades of Gardener version.&lt;/p>
&lt;p>This guide records the issues that are quite possible across upgrade of Gardener version, root cause and the human action required for graceful resolution of issue. For troubleshooting guide of bugs which are not yet fixed, please refer the associated github issue.&lt;/p>
&lt;p>&lt;strong>Note To Maintainers:&lt;/strong> Please use only mention the resolution of issues which are by design. For bugs please report the temporary resolution on github issue create for the bug.&lt;/p>
&lt;h3 id="etcd-main-pod-fails-to-come-up-since-backup-restore-sidecar-is-reporting-revisionconsistencycheckerr">Etcd-Main pod fails to come up, since backup-restore sidecar is reporting RevisionConsistencyCheckErr&lt;/h3>
&lt;h4 id="issue">Issue&lt;/h4>
&lt;ul>
&lt;li>Etcd-main pod goes in &lt;code>CrashLoopBackoff&lt;/code>.&lt;/li>
&lt;li>Etcd-backup-restore sidecar reports validation error with RevisionConsistencyCheckErr.&lt;/li>
&lt;/ul>
&lt;h4 id="environment">Environment&lt;/h4>
&lt;ul>
&lt;li>Gardener version: 0.29.0+&lt;/li>
&lt;/ul>
&lt;h4 id="root-cause">Root Cause&lt;/h4>
&lt;ul>
&lt;li>From version 0.29.0, Gardener uses shared backup bucket for storing etcd backups, replacing old logic of having single bucket per shoot as per &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md">proposal&lt;/a>.&lt;/li>
&lt;li>Since there are very rare chances that etcd data directory will get corrupt, while doing this migration, to avoid etcd down time and implementation effort, we decided to switch directly from old bucket to new shared bucket without migrating old snapshot from old bucket to new bucket.&lt;/li>
&lt;li>In this case just for safety side we added sanity check in etcd-backup-restore sidecar of etcd-main pod, which checks if etcd data revision is greater than the last snapshot revision from old bucket.&lt;/li>
&lt;li>If above check fails mean there is surely some data corruption occurred with etcd, so etcd-backup-restore reports error and then etcd-main pod goes in &lt;code>CrashLoopBackoff&lt;/code> creating etcd-main down alerts.&lt;/li>
&lt;/ul>
&lt;h4 id="action">Action&lt;/h4>
&lt;ol>
&lt;li>Disable the Gardener reconciliation for Shoot by annotating it with &lt;code>shoot.gardener.cloud/ignore=true&lt;/code>&lt;/li>
&lt;li>Scale down the etcd-main statefulset in seed cluster.&lt;/li>
&lt;li>Find out the latest full snapshot and delta snapshot from old backup bucket. The old backup bucket name is same as the backupInfra resource associated with Shoot in Garden cluster.&lt;/li>
&lt;li>Move them manually to new backup bucket.&lt;/li>
&lt;li>Enable the Gardener reconciliation for shoot by removing annotation &lt;code>shoot.gardener.cloud/ignore=true&lt;/code>.&lt;/li>
&lt;/ol></description></item><item><title>Docs: Trusted Tls For Control Planes</title><link>https://gardener.cloud/docs/gardener/usage/trusted-tls-for-control-planes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/trusted-tls-for-control-planes/</guid><description>
&lt;h1 id="trusted-tls-certificate-for-shoot-control-planes">Trusted TLS certificate for shoot control planes&lt;/h1>
&lt;p>Shoot clusters are composed of several control plane components deployed by the Gardener and corresponding extensions.&lt;/p>
&lt;p>Some components are exposed via &lt;code>Ingress&lt;/code> resources which make them addressable under the HTTPS protocol.&lt;/p>
&lt;p>Examples:&lt;/p>
&lt;ul>
&lt;li>Alertmanager&lt;/li>
&lt;li>Grafana&lt;/li>
&lt;li>Prometheus&lt;/li>
&lt;/ul>
&lt;p>Gardener generates the backing TLS certificates which are signed by the shoot cluster&amp;rsquo;s CA by default (self-signed).&lt;/p>
&lt;p>Unlike with a self-contained Kubeconfig file, common internet browsers or operating systems don&amp;rsquo;t trust a shoot&amp;rsquo;s cluster CA and adding it as a trusted root is often undesired in enterprise environments.&lt;/p>
&lt;p>Therefore, Gardener operators can predefine trusted wildcard certificates under which the mentioned endpoints will be served instead.&lt;/p>
&lt;h2 id="register-a-trusted-wildcard-certificate">Register a trusted wildcard certificate&lt;/h2>
&lt;p>Since control plane components are published under the ingress domain (&lt;code>core.gardener.cloud/v1beta1.Seed.spec.dns.ingressDomain&lt;/code>) a wildcard certificate is required.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;ul>
&lt;li>Seed ingress domain: &lt;code>dev.my-seed.example.com&lt;/code>&lt;/li>
&lt;li>&lt;code>CN&lt;/code> or &lt;code>SAN&lt;/code> for certificate: &lt;code>*.dev.my-seed.example.com&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>A wildcard certificate matches exactly one seed. It must be deployed as part of your landscape setup as a Kubernetes &lt;code>Secret&lt;/code> inside the &lt;code>garden&lt;/code> namespace of the corresponding seed cluster.&lt;/p>
&lt;p>Please ensure that the secret has the &lt;code>gardener.cloud/role&lt;/code> label shown below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ca.crt: base64-encoded-ca.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tls.crt: base64-encoded-tls.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tls.key: base64-encoded-tls.key
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> gardener.cloud/role: controlplane-cert
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: seed-ingress-certificate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type: Opaque
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Gardener copies the secret during the reconciliation of shoot clusters to the shoot namespace in the seed. Afterwards, &lt;code>Ingress&lt;/code> resources in that namespace for the mentioned components will refer to the wildcard certificate.&lt;/p>
&lt;h2 id="best-practice">Best practice&lt;/h2>
&lt;p>While it is possible to create the wildcard certificates manually and deploy them to seed clusters, it is recommended to let certificate management components do this job. Often, a seed cluster is also a shoot cluster at the same time (ManagedSeed) and might already provide a certificate service extension.
Otherwise, a Gardener operator may use solutions like &lt;a href="https://github.com/gardener/cert-management">Cert-Management&lt;/a> or &lt;a href="https://github.com/jetstack/cert-manager">Cert-Manager&lt;/a>.&lt;/p></description></item><item><title>Docs: Worker Pool K8s Versions</title><link>https://gardener.cloud/docs/gardener/usage/worker_pool_k8s_versions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/worker_pool_k8s_versions/</guid><description>
&lt;h1 id="controlling-the-kubernetes-versions-for-specific-worker-pools">Controlling the Kubernetes versions for specific worker pools&lt;/h1>
&lt;p>Since Gardener &lt;code>v1.36&lt;/code>, worker pools can have different Kubernetes versions specified than the control plane.&lt;/p>
&lt;p>In earlier Gardener versions all worker pools inherited the Kubernetes version of the control plane. Once the Kubernetes version of the control plane was modified, all worker pools have been updated as well (either by rolling the nodes in case of a minor version change, or in-place for patch version changes).&lt;/p>
&lt;p>In order to gracefully perform Kubernetes upgrades (triggering a rolling update of the nodes) with workloads sensitive to restarts (e.g., those dealing with lots of data), it might be required to be able to gradually perform the upgrade process.
In such cases, the Kubernetes version for the worker pools can be pinned (&lt;code>.spec.provider.workers[].kubernetes.version&lt;/code>) while the control plane Kubernetes version (&lt;code>.spec.kubernetes.version&lt;/code>) is updated.
This results in the nodes being untouched while the control plane is upgraded.
Now a new worker pool (with the version equal to the control plane version) can be added.
Administrators can then reschedule their workloads to the new worker pool according to their upgrade requirements and processes.&lt;/p>
&lt;h2 id="example-usage-in-a-shoot">Example Usage in a &lt;code>Shoot&lt;/code>&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.24.6
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> workers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: data1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.23.13
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: data2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>If &lt;code>.kubernetes.version&lt;/code> is not specified in a worker pool, then the Kubernetes version of the kubelet is inherited from the control plane (&lt;code>.spec.kubernetes.version&lt;/code>), i.e., in the above example, the &lt;code>data2&lt;/code> pool will use &lt;code>1.24.6&lt;/code>.&lt;/li>
&lt;li>If &lt;code>.kubernetes.version&lt;/code> is specified in a worker pool then it must meet the following constraints:
&lt;ul>
&lt;li>It must be at most two minor versions lower than the control plane version.&lt;/li>
&lt;li>If it was not specified before, then no downgrade is possible (you cannot set it to &lt;code>1.23.13&lt;/code> while &lt;code>.spec.kubernetes.version&lt;/code> is already &lt;code>1.24.6&lt;/code>). The &amp;ldquo;two minor version skew&amp;rdquo; is only possible if the worker pool version is set to control plane version and then the control plane was updated gradually two minor versions.&lt;/li>
&lt;li>If the version is removed from the worker pool, only one minor version difference is allowed to the control plane (you cannot upgrade a pool from version &lt;code>1.22.0&lt;/code> to &lt;code>1.24.0&lt;/code> in one go).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Automatic updates of Kubernetes versions (see &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_maintenance/#automatic-version-updates">Shoot Maintenance&lt;/a>) also apply to worker pool Kubernetes versions.&lt;/p></description></item></channel></rss>