<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.95.0"><link rel=canonical type=text/html href=https://gardener.cloud/docs/guides/applications/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/guides/applications/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Applications | Gardener</title><meta name=description content><meta property="og:title" content="Applications"><meta property="og:description" content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/guides/applications/"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="Applications"><meta itemprop=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="Applications"><meta name=twitter:description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><link rel=preload href=/scss/main.min.f409ec5d094ff6c252963bb8c757cd438f6656aad339c5a27da149e85021b16f.css as=style><link href=/scss/main.min.f409ec5d094ff6c252963bb8c757cd438f6656aad339c5a27da149e85021b16f.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://demo.gardener.cloud target=_blank><span>Demo</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><div class=dropdown><a href=/docs class=nav-link>Documentation</a><div class=dropdown-content><a class=taxonomy-term href=/docs>Users</a>
<a class=taxonomy-term href=/docs>Operators</a>
<a class=taxonomy-term href=/docs>Developers</a>
<a class=taxonomy-term href=/docs>All</a></div></div></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.f4251909c27f618377a383b67a7d7726.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/guides/applications/>Return to the regular view of this page</a>.</p></div><h1 class=title>Applications</h1><div class=content></div></div><div class=td-content><h1 id=pg-446f31cac2d2296305c751ed96396a92>1 - Specifying a Disruption Budget for Kubernetes Controllers</h1><h2 id=introduction-of-disruptions>Introduction of Disruptions</h2><p>We need to understand that some kind of voluntary disruptions can happen to pods.
For example, they can be caused by cluster administrators who want to perform automated cluster actions, like upgrading and autoscaling clusters.
Typical application owner actions include:</p><ul><li>deleting the deployment or other controller that manages the pod</li><li>updating a deployment&rsquo;s pod template causing a restart</li><li>directly deleting a pod (e.g., by accident)</li></ul><h2 id=setup-pod-disruption-budgets>Setup Pod Disruption Budgets</h2><p>Kubernetes offers a feature called PodDisruptionBudget (PDB) for each application.
A PDB limits the number of pods of a replicated application that are down simultaneously from voluntary disruptions.</p><p>The most common use case is when you want to protect an application specified by one of the built-in Kubernetes controllers:</p><ul><li>Deployment</li><li>ReplicationController</li><li>ReplicaSet</li><li>StatefulSet</li></ul><p>A PodDisruptionBudget has three fields:</p><ul><li>A label selector <code>.spec.selector</code> to specify the set of pods to which it applies.</li><li><code>.spec.minAvailable</code> which is a description of the number of pods from that set that must still be available after the eviction, even in the absence of the evicted pod. minAvailable can be either an absolute number or a percentage.</li><li><code>.spec.maxUnavailable</code> which is a description of the number of pods from that set that can be unavailable after the eviction. It can be either an absolute number or a percentage.</li></ul><h2 id=cluster-upgrade-or-node-deletion-failed-due-to-pdb-violation>Cluster Upgrade or Node Deletion Failed due to PDB Violation</h2><p>Misconfiguration of the PDB could block the cluster upgrade or node deletion processes. There are two main cases that can cause a misconfiguration.</p><h3 id=case-1-the-replica-of-kubernetes-controllers-is-1>Case 1: The replica of Kubernetes controllers is 1</h3><ul><li><p>Only 1 replica is running: there is no <code>replicaCount</code> setup or <code>replicaCount</code> for the Kubernetes controllers is set to 1</p></li><li><p>PDB configuration</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  spec:
</span></span><span style=display:flex><span>    minAvailable: 1
</span></span></code></pre></div></li><li><p>To fix this PDB misconfiguration, you need to change the value of <code>replicaCount</code> for the Kubernetes controllers to a number greater than 1</p></li></ul><h3 id=case-2-hpa-configuration-violates-pdb>Case 2: HPA configuration violates PDB</h3><p>In Kubernetes, a HorizontalPodAutoscaler automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of automatically scaling the workload to match demand. The HorizontalPodAutoscaler manages the replicas field of the Kubernetes controllers.</p><ul><li><p>There is no <code>replicaCount</code> setup or <code>replicaCount</code> for the Kubernetes controllers is set to 1</p></li><li><p>PDB configuration</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  spec:
</span></span><span style=display:flex><span>    minAvailable: 1
</span></span></code></pre></div></li><li><p>HPA configuration</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  spec:
</span></span><span style=display:flex><span>    minReplicas: 1
</span></span></code></pre></div></li><li><p>To fix this PDB misconfiguration, you need to change the value of HPA <code>minReplicas</code> to be greater than 1</p></li></ul><h2 id=related-links>Related Links</h2><ul><li><a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb/>Specifying a Disruption Budget for Your Application</a></li><li><a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/>Horizontal Pod Autoscaling</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0832066a6c13985f26b5da6fa31d85e2>2 - Access a Port of a Pod Locally</h1><h2 id=question>Question</h2><p>You have deployed an application with a web UI or an internal endpoint in your Kubernetes (K8s) cluster. How to access this endpoint <strong>without an external load balancer</strong> (e.g., Ingress)?</p><p>This tutorial presents two options:</p><ul><li>Using Kubernetes port forward</li><li>Using Kubernetes apiserver proxy</li></ul><p>Please note that the options described here are mostly for quick testing or troubleshooting your application. For enabling access to your application for productive environment, please refer to the <a href=https://kubernetes.io/docs/concepts/services-networking/service/>official Kubernetes documentation</a>.</p><h2 id=solution-1-using-kubernetes-port-forward>Solution 1: Using Kubernetes Port Forward</h2><p>You could use the port forwarding functionality of <code>kubectl</code> to access the pods from your local host <strong>without involving a service</strong>.</p><p>To access any pod follow these steps:</p><ol><li>Run <code>kubectl get pods</code></li><li>Note down the name of the pod in question as <code>&lt;your-pod-name></code></li><li>Run <code>kubectl port-forward &lt;your-pod-name> &lt;local-port>:&lt;your-app-port></code></li><li>Run a web browser or curl locally and enter the URL: <code>http(s)://localhost:&lt;local-port></code></li></ol><p>In addition, <code>kubectl port-forward</code> allows using a resource name, such as a deployment name or service name, to select a matching pod to port forward.
More details can be found in the <a href=https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/>Kubernetes documentation</a>.</p><p>The main drawback of this approach is that the pod&rsquo;s name changes as soon as it is restarted. Moreover, you need to have a web browser on your client and you need to make sure that the local port is not already used by an application running on your system. Finally, sometimes the port forwarding is canceled due to nonobvious reasons. This leads to a kind of shaky approach. A more stable possibility is based on accessing the app via the kube-proxy, which accesses the corresponding service.</p><p><img src=/__resources/howto-port-forward_e507fa.svg alt=port-forward></p><h2 id=solution-2-using-the-apiserver-proxy-of-your-kubernetes-cluster>Solution 2: Using the apiserver Proxy of Your Kubernetes Cluster</h2><p>There are <a href=https://kubernetes.io/docs/concepts/cluster-administration/proxies/>several different proxies</a> in Kubernetes. In this tutorial we will be using <em>apiserver proxy</em> to enable the access to the services in your cluster without Ingress. <strong>Unlike the first solution, here a service is required.</strong></p><p>Use the following format to compose a URL for accessing your service through an existing proxy on the Kubernetes cluster:</p><p><code>https://&lt;your-cluster-master>/api/v1/namespace/&lt;your-namespace>/services/&lt;your-service>:&lt;your-service-port>/proxy/&lt;service-endpoint></code></p><p><strong>Example:</strong></p><table><thead><tr><th>your-main-cluster</th><th style=text-align:center>your-namespace</th><th style=text-align:right>your-service</th><th style=text-align:right>your-service-port</th><th style=text-align:right>your-service-endpoint</th><th style=text-align:right>url to access service</th></tr></thead><tbody><tr><td>api.testclstr.cpet.k8s.sapcloud.io</td><td style=text-align:center>default</td><td style=text-align:right>nginx-svc</td><td style=text-align:right>80</td><td style=text-align:right>/</td><td style=text-align:right><code>http://api.testclstr.cpet.k8s.sapcloud.io/api/v1/namespaces/default/services/nginx-svc:80/proxy/</code></td></tr><tr><td>api.testclstr.cpet.k8s.sapcloud.io</td><td style=text-align:center>default</td><td style=text-align:right>docker-nodejs-svc</td><td style=text-align:right>4500</td><td style=text-align:right>/cpu?baseNumber=4</td><td style=text-align:right><code>https://api.testclstr.cpet.k8s.sapcloud.io/api/v1/namespaces/default/services/docker-nodejs-svc:4500/proxy/cpu?baseNumber=4</code></td></tr></tbody></table><p>For more details on the format, please refer to the <a href=https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services>official Kubernetes documentation</a>.</p><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>There are applications which do not support relative URLs yet, e.g. <a href=https://github.com/prometheus/prometheus/issues/1583>Prometheus</a> (as of November, 2022).
This typically leads to missing JavaScript objects, which could be investigated with your browser&rsquo;s development tools. If such an issue occurs, please use the <code>port-forward</code> approach <a href=/docs/guides/applications/access-pod-from-local/#solution-1-using-kubernetes-port-forward>described above</a>.</div></div><div class=td-content style=page-break-before:always><h1 id=pg-50c78851e66f83193c75c3f74fa93dce>3 - Auditing Kubernetes for Secure Setup</h1><div class=lead>A few insecure configurations in Kubernetes</div><p><img src=/__resources/teaser_f209ec.svg alt=teaser></p><h2 id=increasing-the-security-of-all-gardener-stakeholders>Increasing the Security of All Gardener Stakeholders</h2><p>In summer 2018, the <a href=https://github.com/gardener/gardener>Gardener project team</a> asked <a href=https://kinvolk.io/>Kinvolk</a> to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work was to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a <a href=https://kubernetes.io/blog/2018/05/17/gardener/#kubernetes-control-plane>Control-Plane-as-a-Service</a> with a <a href=https://kubernetes.io/blog/2018/05/17/gardener/#network-air-gap>network air gap</a>.</p><p>Along the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.</p><h2 id=major-findings>Major Findings</h2><p>From this experience, we’d like to share a few examples of security issues that could happen on a Kubernetes installation and how to fix them.</p><p>Alban Crequy (<a href=https://kinvolk.io/>Kinvolk</a>) and Dirk Marwinski (<a href=https://www.sap.com>SAP SE</a>) gave a presentation entitled <a href=https://kccncchina2018english.sched.com/event/H2Hd/hardening-multi-cloud-kubernetes-clusters-as-a-service-dirk-marwinski-sap-se-alban-crequy-kinvolk-gmbh>Hardening Multi-Cloud Kubernetes Clusters as a Service</a> at KubeCon 2018 in Shanghai presenting some of the findings.</p><p>Here is a summary of the findings:</p><ul><li><p>Privilege escalation due to insecure configuration of the Kubernetes API server</p><ul><li>Root cause: Same certificate authority (CA) is used for both the API server and the proxy that allows accessing the API server.</li><li>Risk: Users can get access to the API server.</li><li>Recommendation: Always use different CAs.</li></ul></li><li><p>Exploration of the control plane network with malicious HTTP-redirects</p><ul><li>Root cause: See detailed description below.</li><li>Risk: Provoked error message contains full HTTP payload from anexisting endpoint which can be exploited. The contents of the payload depends on your setup, but can potentially be user data, configuration data, and credentials.<ul><li>Recommendation:<ul><li>Use the latest version of Gardener</li><li>Ensure the seed cluster&rsquo;s container network supports network policies. Clusters that have been created with <a href=https://github.com/gardener/kubify>Kubify</a> are not protected as Flannel is used there which doesn&rsquo;t support network policies.</li></ul></li></ul></li></ul></li><li><p>Reading private AWS metadata via Grafana</p><ul><li>Root cause: It is possible to configuring a new custom data source in Grafana, we could send HTTP requests to target the control</li><li>Risk: Users can get the &ldquo;user-data&rdquo; for the seed cluster from the metadata service and retrieve a kubeconfig for that Kubernetes cluster</li><li>Recommendation: Lockdown Grafana features to only what&rsquo;s necessary in this setup, block all unnecessary outgoing traffic, move Grafana to a different network, lockdown unauthenticated endpoints</li></ul></li></ul><h2 id=scenario-1-privilege-escalation-with-insecure-api-server>Scenario 1: Privilege Escalation with Insecure API Server</h2><p>In most configurations, different components connect directly to the Kubernetes API server, often using a <code>kubeconfig</code> with a client
certificate. The API server is started with the flag:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>/hyperkube apiserver --client-ca-file=/srv/kubernetes/ca/ca.crt ...
</span></span></code></pre></div><p>The API server will check whether the client certificate presented by kubectl, kubelet, scheduler or another component is really signed by the configured certificate authority for clients.</p><p><img src=/__resources/image3_6adf12.png alt><em>The API server can have many clients of various kinds</em><br><br><br></p><p>However, it is possible to configure the API server differently for use with an intermediate authenticating proxy. The proxy will authenticate the client with its own custom method and then issue HTTP requests to the API server with additional HTTP headers specifying the user name and group name. The API server should only accept HTTP requests with HTTP headers from a legitimate proxy. To allow the API server to check incoming requests, you need pass on a list of certificate authorities (CAs) to it. Requests coming from a proxy are only accepted if they use a client certificate that is signed by one of the CAs of that list.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>--requestheader-client-ca-file=/srv/kubernetes/ca/ca-proxy.crt
</span></span><span style=display:flex><span>--requestheader-username-headers=X-Remote-User
</span></span><span style=display:flex><span>--requestheader-group-headers=X-Remote-Group
</span></span></code></pre></div><p><img src=/__resources/image2_dc8a05.png alt><em>API server clients can reach the API server through an authenticating proxy</em><br><br><br></p><p>So far, so good. But what happens if the malicious user “Mallory” tries to connect directly to the API server and reuses the HTTP headers to pretend to be someone else?</p><p><img src=/__resources/image8_f31fde.png alt><em>What happens when a client bypasses the proxy, connecting directly to the API server?</em><br><br><br></p><p>With a correct configuration, Mallory’s kubeconfig will have a certificate signed by the API server certificate authority but not signed by the proxy certificate authority. So the API server will not accept the extra HTTP header “X-Remote-Group: system:masters”.</p><p>You only run into an issue when the same certificate authority is used for both the API server and the proxy. Then, any Kubernetes client certificate can be used to take the role of different user or group as the API server will accept the user header and group header.</p><p>The <code>kubectl</code> tool does not normally add those HTTP headers but it’s pretty easy to generate the corresponding HTTP requests manually.</p><p>We worked on <a href=https://github.com/kubernetes/website/pull/10093>improving the Kubernetes documentation</a> to make clearer that this configuration should be avoided.</p><h2 id=scenario-2-exploration-of-the-control-plane-network-with-malicious-http-redirects>Scenario 2: Exploration of the Control Plane Network with Malicious HTTP-Redirects</h2><p>The API server is a central component of Kubernetes and many components initiate connections to it, including the kubelet running on worker nodes. Most of the requests from those clients will end up updating Kubernetes objects (pods, services, deployments, and so on) in the etcd database but the API server usually does not need to initiate TCP connections itself.</p><p><img src=/__resources/image7_260265.png alt><em>The API server is mostly a component that receives requests</em><br><br><br></p><p>However, there are exceptions. Some <code>kubectl</code> commands will trigger the API server to open a new connection to the kubelet. <code>kubectl exec</code> is one of those commands. In order to get the standard I/Os from the pod, the API server will start an HTTP connection to the kubelet on the worker node where the pod is running. Depending on the container runtime used, it can be done in different ways, but one way to do it is for the kubelet to reply with a HTTP-302 redirection to the <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md>Container Runtime Interface (CRI)</a>. Basically, the kubelet is telling the API server to get the streams from CRI itself directly instead of forwarding. The redirection from the kubelet will only change the port and path from the URL; the IP address will not be changed because the kubelet and the CRI component run on the same worker node.</p><p><img src=/__resources/image1_8021c1.png alt><em>But the API server also initiates some connections, for example, to worker nodes</em><br><br><br></p><p>It’s often quite easy for users of a Kubernetes cluster to get access to worker nodes and tamper with the kubelet. They could be given explicit SSH access or they could be given a kubeconfig with enough privileges to create privileged pods or even just pods with “host” volumes.</p><p>In contrast, users (even those with “system:masters” permissions or “root” rights) are often not given access to the control plane.
On setups like, for example, GKE or Gardener, the control plane is running on separate nodes, with a different administrative access. It could be hosted on a different cloud provider account. So users are not free to explore the internal networking the control plane.</p><p>What would happen if a user was tampering with the kubelet to make it maliciously redirect <code>kubectl exec</code> requests to a different random endpoint? Most likely the given endpoint would not speak to the streaming server protocol, so there would be an error. However, the full HTTP payload from the endpoint is included in the error message printed by kubectl exec.</p><p><img src=/__resources/image6_1ae9dd.png alt><em>The API server is tricked to connect to other components</em><br><br><br></p><p>The impact of this issue depends on the specific setup. But in many configurations, we could find a metadata service (such as the <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html>AWS metadata service</a>) containing user data, configurations and credentials. The setup we explored had a different AWS account and a different <a href=https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html>EC2 instance profile</a> for the worker nodes and the control plane. This issue allowed users to get access to the AWS metadata service in the context of the control plane, which they should not have access to.</p><p>We have reported this issue to the <a href=https://kubernetes.io/docs/reference/issues-security/security/>Kubernetes Security mailing list</a> and the public pull request that addresses the issue has been merged <a href=https://github.com/kubernetes/kubernetes/pull/66516>PR#66516</a>.
It provides a way to enforce HTTP redirect validation (disabled by default).</p><p>But there are several other ways that users could trigger the API server to generate HTTP requests and get the reply payload back, so it is advised to isolate the API server and other components from the network as additional precautious measures. Depending on where the API server runs, it could be with <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/>Kubernetes Network Policies</a>, <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html>EC2 Security Groups</a> or just iptables directly. Following the <a href=https://en.wikipedia.org/wiki/Defense_in_depth_(computing)>defense in depth principle</a>, it is a good idea to apply the API server HTTP redirect validation when it is available as well as firewall rules.</p><p>In Gardener, this has been fixed with Kubernetes network policies along with changes to ensure the API server does not need to contact the metadata service. You can see more details in the <a href=https://groups.google.com/forum/#!forum/gardener>announcements on the Gardener mailing list</a>.
This is tracked in <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-2475">CVE-2018-2475</a>.</p><p><em>To be protected from this issue, stakeholders should:</em></p><ul><li><em>Use the latest version of Gardener</em></li><li><em>Ensure the seed cluster’s container network supports network policies. Clusters that have been created with <a href=https://github.com/gardener/kubify>Kubify</a> are not protected as Flannel is used there which doesn’t support network policies.</em></li></ul><h2 id=scenario-3-reading-private-aws-metadata-via-grafana>Scenario 3: Reading Private AWS Metadata via Grafana</h2><p>For our tests, we had access to a Kubernetes setup where users are not only given access to the API server in the control plane, but also to a Grafana instance that is used to gather data from their Kubernetes clusters via Prometheus. The control plane is managed and users don’t have access to the nodes that it runs. They can only access the API server and Grafana via a load balancer. The internal network of the control plane is therefore hidden to users.</p><p><img src=/__resources/image5_0e9a4e.png alt><em>Prometheus and Grafana can be used to monitor worker nodes</em><br><br><br></p><p>Unfortunately, that setup was not protecting the control plane network from nosy users. By configuring a new custom data source in Grafana, we could send HTTP requests to target the control plane network, for example the AWS metadata service. The reply payload is not displayed on the Grafana Web UI but it is possible to access it from the debugging console of the Chrome browser.</p><p><img src=/__resources/image9_53e7b4.png alt><em>Credentials can be retrieved from the debugging console of Chrome</em><br><br><br></p><p><img src=/__resources/image4_657bb4.png alt><em>Adding a Grafana data source is a way to issue HTTP requests to arbitrary targets</em><br><br><br></p><p>In that installation, users could get the “user-data” for the seed cluster from the metadata service and retrieve a kubeconfig for that Kubernetes cluster.</p><p>There are many possible measures to avoid this situation: lockdown Grafana features to only what’s necessary in this setup, block all unnecessary outgoing traffic, move Grafana to a different network, or lockdown unauthenticated endpoints, among others.</p><h2 id=conclusion>Conclusion</h2><p>The three scenarios above show pitfalls with a Kubernetes setup. A lot of them were specific to the Kubernetes installation: different cloud providers or different configurations will show different weaknesses. Users should no longer be given access to Grafana.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-dfc897213f903e12a8b113f65b6f7b16>4 - Container Image Not Pulled</h1><div class=lead>Wrong Container Image or Invalid Registry Permissions</div><h2 id=problem>Problem</h2><p>Two of the most common causes of this problems are specifying the wrong container image or trying to use private images without providing registry credentials.</p><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>There is no observable difference in pod status between a missing image and incorrect registry permissions. In either case, Kubernetes will report an <code>ErrImagePull</code> status for the pods. For this reason, this article deals with both scenarios.</div><h2 id=example>Example</h2><p>Let&rsquo;s see an example. We&rsquo;ll create a pod named <em>fail</em>, referencing a non-existent Docker image:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl run -i --tty fail --image=tutum/curl:1.123456
</span></span></code></pre></div><p>The command doesn&rsquo;t return and you can terminate the process with <code>Ctrl+C</code>.</p><h2 id=error-analysis>Error Analysis</h2><p>We can then inspect our pods and see that we have one pod with a status of <strong>ErrImagePull</strong> or <strong>ImagePullBackOff</strong>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ (minikube) kubectl get pods
</span></span><span style=display:flex><span>NAME                      READY     STATUS         RESTARTS   AGE
</span></span><span style=display:flex><span>client-5b65b6c866-cs4ch   1/1       Running        1          1m
</span></span><span style=display:flex><span>fail-6667d7685d-7v6w8     0/1       ErrImagePull   0          &lt;invalid&gt;
</span></span><span style=display:flex><span>vuejs-578574b75f-5x98z    1/1       Running        0          1d
</span></span><span style=display:flex><span>$ (minikube) 
</span></span></code></pre></div><p>For some additional information, we can <code>describe</code> the failing pod.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl describe pod fail-6667d7685d-7v6w8
</span></span></code></pre></div><p>As you can see in the events section, your image can&rsquo;t be pulled:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>Name:   fail-6667d7685d-7v6w8
</span></span><span style=display:flex><span>Namespace: default
</span></span><span style=display:flex><span>Node:   minikube/192.168.64.10
</span></span><span style=display:flex><span>Start Time: Wed, 22 Nov 2017 10:01:59 +0100
</span></span><span style=display:flex><span>Labels:   pod-template-hash=2223832418
</span></span><span style=display:flex><span>    run=fail
</span></span><span style=display:flex><span>Annotations: kubernetes.io/created-by={<span style=color:#a31515>&#34;kind&#34;</span>:<span style=color:#a31515>&#34;SerializedReference&#34;</span>,<span style=color:#a31515>&#34;apiVersion&#34;</span>:<span style=color:#a31515>&#34;v1&#34;</span>,<span style=color:#a31515>&#34;reference&#34;</span>:{<span style=color:#a31515>&#34;kind&#34;</span>:<span style=color:#a31515>&#34;ReplicaSet&#34;</span>,<span style=color:#a31515>&#34;namespace&#34;</span>:<span style=color:#a31515>&#34;default&#34;</span>,<span style=color:#a31515>&#34;name&#34;</span>:<span style=color:#a31515>&#34;fail-6667d7685d&#34;</span>,<span style=color:#a31515>&#34;uid&#34;</span>:<span style=color:#a31515>&#34;cc4ccb3f-cf63-11e7-afca-4a7a1fa05b3f&#34;</span>,<span style=color:#a31515>&#34;a...
</span></span></span><span style=display:flex><span><span style=color:#a31515>.
</span></span></span><span style=display:flex><span><span style=color:#a31515>.
</span></span></span><span style=display:flex><span><span style=color:#a31515>.
</span></span></span><span style=display:flex><span><span style=color:#a31515>.
</span></span></span><span style=display:flex><span><span style=color:#a31515>Events:
</span></span></span><span style=display:flex><span><span style=color:#a31515>  FirstSeen	LastSeen	Count	From			SubObjectPath		Type		Reason			Message
</span></span></span><span style=display:flex><span><span style=color:#a31515>  ---------	--------	-----	----			-------------		--------	------			-------
</span></span></span><span style=display:flex><span><span style=color:#a31515>  1m		1m		1	default-scheduler				Normal		Scheduled		Successfully assigned fail-6667d7685d-7v6w8 to minikube
</span></span></span><span style=display:flex><span><span style=color:#a31515>  1m		1m		1	kubelet, minikube				Normal		SuccessfulMountVolume	MountVolume.SetUp succeeded for volume &#34;</span>default-token-9fr6r<span style=color:#a31515>&#34; 
</span></span></span><span style=display:flex><span><span style=color:#a31515>  1m		6s		4	kubelet, minikube	spec.containers{fail}	Normal		Pulling			pulling image &#34;</span>tutum/curl:1.123456<span style=color:#a31515>&#34;
</span></span></span><span style=display:flex><span><span style=color:#a31515>  1m		5s		4	kubelet, minikube	spec.containers{fail}	Warning		Failed			Failed to pull image &#34;</span>tutum/curl:1.123456<span style=color:#a31515>&#34;: rpc error: code = Unknown desc = Error response from daemon: manifest for tutum/curl:1.123456 not found
</span></span></span><span style=display:flex><span><span style=color:#a31515>  1m		&lt;invalid&gt;	10	kubelet, minikube				Warning		FailedSync		Error syncing pod
</span></span></span><span style=display:flex><span><span style=color:#a31515>  1m		&lt;invalid&gt;	6	kubelet, minikube	spec.containers{fail}	Normal		BackOff			Back-off pulling image &#34;</span>tutum/curl:1.123456<span style=color:#a31515>&#34;
</span></span></span></code></pre></div><p><strong>Why couldn&rsquo;t Kubernetes pull the image?</strong>
There are three primary candidates besides network connectivity issues:</p><ul><li>The image tag is incorrect</li><li>The image doesn&rsquo;t exist</li><li>Kubernetes doesn&rsquo;t have permissions to pull that image</li></ul><p>If you don&rsquo;t notice a typo in your image tag, then it&rsquo;s time to test using your local machine. I usually start by
running <strong>docker pull on my local development machine</strong> with the exact same image tag. In this case, I would
run <code>docker pull tutum/curl:1.123456</code>.</p><p>If this succeeds, then it probably means that Kubernetes doesn&rsquo;t have the correct permissions to pull that image.</p><p>Add the docker registry user/pwd to your cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl create secret docker-registry dockersecret --docker-server=https://index.docker.io/v1/ --docker-username=&lt;username&gt; --docker-password=&lt;password&gt; --docker-email=&lt;email&gt;
</span></span></code></pre></div><p>If the exact image tag fails, then I will test without an explicit image tag:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>docker pull tutum/curl
</span></span></code></pre></div><p>This command will attempt to pull the latest tag. If this succeeds, then that means the originally specified tag doesn&rsquo;t exist. Go to the Docker registry and check which tags are available for this image.</p><p>If <code>docker pull tutum/curl</code> (without an exact tag) fails, then we have a bigger problem - that image does not exist at all in our image registry.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7c3aec8af16c81adcc2c298b27666c54>5 - Container Image Not Updating</h1><div class=lead>Updating images in your cluster during development</div><h2 id=introduction>Introduction</h2><p>A container image should use a fixed tag or the SHA of the image. It should not use the tags <strong>latest</strong>, <strong>head</strong>, <strong>canary</strong>, or other tags that are designed to be <em>floating</em>.</p><h2 id=problem>Problem</h2><p>If you have encountered this issue, you have probably done something along the lines of:</p><ul><li>Deploy anything using an image tag (e.g., <code>cp-enablement/awesomeapp:1.0</code>)</li><li>Fix a bug in awesomeapp</li><li>Build a new image and push it with the <strong>same tag</strong> (<code>cp-enablement/awesomeapp:1.0</code>)</li><li>Update the deployment</li><li>Realize that the bug is still present</li><li>Repeat steps 3-5 without any improvement</li></ul><p>The problem relates to how Kubernetes decides whether to do a <em>docker pull</em> when starting a container.
Since we tagged our image as <em>:1.0</em>, the default pull policy is <strong>IfNotPresent</strong>. The Kubelet already has a local
copy of <code>cp-enablement/awesomeapp:1.0</code>, so it doesn&rsquo;t attempt to do a docker pull. When the new Pods come up,
they&rsquo;re still using the old broken Docker image.</p><p>There are a couple of ways to resolve this, with the recommended one being to <strong>use unique tags</strong>.</p><h2 id=solution>Solution</h2><p>In order to fix the problem, you can use the following bash script that runs anytime the deployment is updated to create a new tag and push it to the registry.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#00f>#!/usr/bin/env bash
</span></span></span><span style=display:flex><span><span style=color:#00f></span>
</span></span><span style=display:flex><span><span style=color:green># Set the docker image name and the corresponding repository</span>
</span></span><span style=display:flex><span><span style=color:green># Ensure that you change them in the deployment.yml as well.</span>
</span></span><span style=display:flex><span><span style=color:green># You must be logged in with docker login.</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span><span style=color:green># CHANGE THIS TO YOUR Docker.io SETTINGS</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>PROJECT=awesomeapp
</span></span><span style=display:flex><span>REPOSITORY=cp-enablement
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># causes the shell to exit if any subcommand or pipeline returns a non-zero status.</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>set -e
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># set debug mode</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>set -x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># build my nodeJS app</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>npm run build
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># get the latest version ID from the Docker.io registry and increment them</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>VERSION=<span style=color:#00f>$(</span>curl https://registry.hub.docker.com/v1/repositories/$REPOSITORY/$PROJECT/tags  | sed -e <span style=color:#a31515>&#39;s/[][]//g&#39;</span> -e <span style=color:#a31515>&#39;s/&#34;//g&#39;</span> -e <span style=color:#a31515>&#39;s/ //g&#39;</span> | tr <span style=color:#a31515>&#39;}&#39;</span> <span style=color:#a31515>&#39;\n&#39;</span>  | awk -F: <span style=color:#a31515>&#39;{print $3}&#39;</span> | grep v| tail -n 1<span style=color:#00f>)</span>
</span></span><span style=display:flex><span>VERSION=<span style=color:#a31515>${</span>VERSION:1<span style=color:#a31515>}</span>
</span></span><span style=display:flex><span>((VERSION++))
</span></span><span style=display:flex><span>VERSION=<span style=color:#a31515>&#34;v</span>$VERSION<span style=color:#a31515>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># build the new docker image</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>echo <span style=color:#a31515>&#39;&gt;&gt;&gt; Building new image&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#a31515>&#39;&gt;&gt;&gt; Push new image&#39;</span>
</span></span><span style=display:flex><span>docker push $REPOSITORY/$PROJECT:$VERSION
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-d1c34c5916bb9019db676213877f1ddf>6 - Custom Seccomp Profile</h1><h2 id=overview>Overview</h2><p><a href=https://en.wikipedia.org/wiki/Seccomp>Seccomp</a> (secure computing mode) is a security facility in the Linux kernel for restricting the set of system calls applications can make.</p><p>Starting from Kubernetes v1.3.0, the Seccomp feature is in <code>Alpha</code>. To configure it on a <code>Pod</code>, the following annotations can be used:</p><ul><li><code>seccomp.security.alpha.kubernetes.io/pod: &lt;seccomp-profile></code> where <code>&lt;seccomp-profile></code> is the seccomp profile to apply to all containers in a <code>Pod</code>.</li><li><code>container.seccomp.security.alpha.kubernetes.io/&lt;container-name>: &lt;seccomp-profile></code> where <code>&lt;seccomp-profile></code> is the seccomp profile to apply to <code>&lt;container-name></code> in a <code>Pod</code>.</li></ul><p>More details can be found in the <code>PodSecurityPolicy</code> <a href=https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp>documentation</a>.</p><h2 id=installation-of-a-custom-profile>Installation of a Custom Profile</h2><p>By default, kubelet loads custom Seccomp profiles from <code>/var/lib/kubelet/seccomp/</code>. There are two ways in which Seccomp profiles can be added to a <code>Node</code>:</p><ul><li>to be baked in the machine image</li><li>to be added at runtime</li></ul><p>This guide focuses on creating those profiles via a <code>DaemonSet</code>.</p><p>Create a file called <code>seccomp-profile.yaml</code> with the following content:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: seccomp-profile
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  my-profile.json: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    {
</span></span></span><span style=display:flex><span><span style=color:#a31515>      &#34;defaultAction&#34;: &#34;SCMP_ACT_ALLOW&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>      &#34;syscalls&#34;: [
</span></span></span><span style=display:flex><span><span style=color:#a31515>        {
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;name&#34;: &#34;chmod&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;action&#34;: &#34;SCMP_ACT_ERRNO&#34;
</span></span></span><span style=display:flex><span><span style=color:#a31515>        }
</span></span></span><span style=display:flex><span><span style=color:#a31515>      ]
</span></span></span><span style=display:flex><span><span style=color:#a31515>    }</span>    
</span></span></code></pre></div><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>The policy above is a very simple one and not suitable for complex applications. The <a href=https://github.com/moby/moby/blob/v17.05.0-ce/profiles/seccomp/default.json>default docker profile</a> can be used a reference. Feel free to modify it to your needs.</div><p>Apply the <code>ConfigMap</code> in your cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>$</span> kubectl apply -f seccomp-profile.yaml
</span></span><span style=display:flex><span>configmap/seccomp-profile created
</span></span></code></pre></div><p>The next steps is to create the <code>DaemonSet</code> Seccomp installer. It&rsquo;s going to copy the policy from above in <code>/var/lib/kubelet/seccomp/my-profile.json</code>.</p><p>Create a file called <code>seccomp-installer.yaml</code> with the following content:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: DaemonSet
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: seccomp
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    security: seccomp
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      security: seccomp
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        security: seccomp
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      initContainers:
</span></span><span style=display:flex><span>      - name: installer
</span></span><span style=display:flex><span>        image: alpine:3.10.0
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>, <span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;cp -r -L /seccomp/*.json /host/seccomp/&#34;</span>]
</span></span><span style=display:flex><span>        volumeMounts:
</span></span><span style=display:flex><span>        - name: profiles
</span></span><span style=display:flex><span>          mountPath: /seccomp
</span></span><span style=display:flex><span>        - name: hostseccomp
</span></span><span style=display:flex><span>          mountPath: /host/seccomp
</span></span><span style=display:flex><span>          readOnly: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: pause
</span></span><span style=display:flex><span>        image: k8s.gcr.io/pause:3.1
</span></span><span style=display:flex><span>      terminationGracePeriodSeconds: 5
</span></span><span style=display:flex><span>      volumes:
</span></span><span style=display:flex><span>      - name: hostseccomp
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /var/lib/kubelet/seccomp
</span></span><span style=display:flex><span>      - name: profiles
</span></span><span style=display:flex><span>        configMap:
</span></span><span style=display:flex><span>          name: seccomp-profile
</span></span></code></pre></div><p>Create the installer and wait until it&rsquo;s ready on all <code>Nodes</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>$</span> kubectl apply -f seccomp-installer.yaml
</span></span><span style=display:flex><span>daemonset.apps/seccomp-installer created
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=font-weight:700>$</span> kubectl -n kube-system get pods -l security=seccomp
</span></span><span style=display:flex><span>NAME                      READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>seccomp-installer-wjbxq   1/1     Running   0          21s
</span></span></code></pre></div><h2 id=create-a-pod-using-a-custom-seccomp-profile>Create a Pod Using a Custom Seccomp Profile</h2><p>Finally, we want to create a profile which uses our new Seccomp profile <code>my-profile.json</code>.</p><p>Create a file called <code>my-seccomp-pod.yaml</code> with the following content:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: seccomp-app
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    seccomp.security.alpha.kubernetes.io/pod: <span style=color:#a31515>&#34;localhost/my-profile.json&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green># you can specify seccomp profile per container. If you add another profile you can configure</span>
</span></span><span style=display:flex><span>    <span style=color:green># it for a specific container - &#39;pause&#39; in this case.</span>
</span></span><span style=display:flex><span>    <span style=color:green># container.seccomp.security.alpha.kubernetes.io/pause: &#34;localhost/some-other-profile.json&#34;</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - name: pause
</span></span><span style=display:flex><span>    image: k8s.gcr.io/pause:3.1
</span></span></code></pre></div><p>Create the <code>Pod</code> and see that it&rsquo;s running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>$</span> kubectl apply -f my-seccomp-pod.yaml
</span></span><span style=display:flex><span>pod/seccomp-app created
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=font-weight:700>$</span> kubectl get pod seccomp-app
</span></span><span style=display:flex><span>NAME         READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>seccomp-app  1/1     Running   0          42s
</span></span></code></pre></div><h2 id=throubleshooting>Throubleshooting</h2><p>If an invalid or a non-existing profile is used, then the <code>Pod</code> will be stuck in <code>ContainerCreating</code> phase:</p><p><code>broken-seccomp-pod.yaml</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: broken-seccomp
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    seccomp.security.alpha.kubernetes.io/pod: <span style=color:#a31515>&#34;localhost/not-existing-profile.json&#34;</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - name: pause
</span></span><span style=display:flex><span>    image: k8s.gcr.io/pause:3.1
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>$</span> kubectl apply -f broken-seccomp-pod.yaml
</span></span><span style=display:flex><span>pod/broken-seccomp created
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=font-weight:700>$</span> kubectl get pod broken-seccomp
</span></span><span style=display:flex><span>NAME            READY   STATUS              RESTARTS   AGE
</span></span><span style=display:flex><span>broken-seccomp  1/1     ContainerCreating   0          2m
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=font-weight:700>$</span> kubectl describe pod broken-seccomp
</span></span><span style=display:flex><span>Name:               broken-seccomp
</span></span><span style=display:flex><span>Namespace:          default
</span></span><span style=display:flex><span>....
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  Type     Reason                  Age               From                     Message
</span></span><span style=display:flex><span>  ----     ------                  ----              ----                     -------
</span></span><span style=display:flex><span>  Normal   Scheduled               18s               default-scheduler        Successfully assigned kube-system/broken-seccomp to docker-desktop
</span></span><span style=display:flex><span>  Warning  FailedCreatePodSandBox  4s (x2 over 18s)  kubelet, docker-desktop  Failed create pod sandbox: rpc error: code = Unknown desc = failed to make sandbox docker config for pod &#34;broken-seccomp&#34;: failed to generate sandbox security options
</span></span><span style=display:flex><span>for sandbox &#34;broken-seccomp&#34;: failed to generate seccomp security options for container: cannot load seccomp profile &#34;/var/lib/kubelet/seccomp/not-existing-profile.json&#34;: open /var/lib/kubelet/seccomp/not-existing-profile.json: no such file or directory
</span></span></code></pre></div><h2 id=related-links>Related Links</h2><ul><li><a href=https://en.wikipedia.org/wiki/Seccomp>Seccomp</a></li><li><a href=https://lwn.net/Articles/656307/>A Seccomp Overview</a></li><li><a href=https://docs.docker.com/engine/security/seccomp>Seccomp Security Profiles for Docker</a></li><li><a href=https://man7.org/conf/lpc2015/limiting_kernel_attack_surface_with_seccomp-LPC_2015-Kerrisk.pdf>Using Seccomp to Limit the Kernel Attack Surface</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-24fc31737eb6e0bca85431844d9ef0eb>7 - Dockerfile Pitfalls</h1><div class=lead>Common Dockerfile pitfalls</div><h2 id=using-the-latest-tag-for-an-image>Using the <code>latest</code> Tag for an Image</h2><p>Many Dockerfiles use the <code>FROM package:latest</code> pattern at the top of their Dockerfiles to pull the latest image from a Docker registry.</p><h3 id=bad-dockerfile>Bad Dockerfile</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Dockerfile data-lang=Dockerfile><span style=display:flex><span><span style=color:#00f>FROM</span><span style=color:#a31515> alpine</span><span>
</span></span></span></code></pre></div><p>While simple, using the latest tag for an image means that your build can suddenly break if that image gets updated. This can lead to problems where everything builds fine locally (because your local cache thinks it is the latest), while a build server may fail, because some pipelines make a clean pull on every build. Additionally, troubleshooting can prove to be difficult, since the maintainer of the Dockerfile didn&rsquo;t actually make any changes.</p><h3 id=good-dockerfile>Good Dockerfile</h3><p>A digest takes the place of the tag when pulling an image. This will ensure that your Dockerfile remains immutable.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Dockerfile data-lang=Dockerfile><span style=display:flex><span><span style=color:#00f>FROM</span><span style=color:#a31515> alpine@sha256:7043076348bf5040220df6ad703798fd8593a0918d06d3ce30c6c93be117e430</span><span>
</span></span></span></code></pre></div><h2 id=running-aptapkyum-update>Running apt/apk/yum update</h2><p>Running <code>apt-get install</code> is one of those things virtually every Debian-based Dockerfile will have to do in order to satiate some external package requirements your code needs to run. However, using <code>apt-get</code> as an example, this comes with its own problems.</p><p><strong>apt-get upgrade</strong></p><p>This will update all your packages to their latests versions, which can be bad because it prevents your Dockerfile from creating consistent, immutable builds.</p><p><strong>apt-get update (in a different line than the one running your apt-get install command)</strong></p><p>Running <code>apt-get update</code> as a single line entry will get cached by the build and won&rsquo;t actually run every time you need to run <code>apt-get install</code>. Instead, make sure you run <code>apt-get update</code> in the same line with all the packages to ensure that all are updated correctly.</p><h2 id=avoid-big-container-images>Avoid Big Container Images</h2><p>Building a small container image will reduce the time needed to start or restart pods. An image based on the popular <a href=http://alpinelinux.org/>Alpine Linux project</a> is much smaller than most distribution based images (~5MB). For most popular languages and products, there is usually an official Alpine Linux image, e.g., <a href=https://hub.docker.com/_/golang/>golang</a>, <a href=https://hub.docker.com/_/node/>nodejs</a>, and <a href=https://hub.docker.com/_/postgres/>postgres</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$  docker images
</span></span><span style=display:flex><span>REPOSITORY                                                      TAG                     IMAGE ID            CREATED             SIZE
</span></span><span style=display:flex><span>postgres                                                        9.6.9-alpine            6583932564f8        13 days ago         39.26 MB
</span></span><span style=display:flex><span>postgres                                                        9.6                     d92dad241eff        13 days ago         235.4 MB
</span></span><span style=display:flex><span>postgres                                                        10.4-alpine             93797b0f31f4        13 days ago         39.56 MB
</span></span></code></pre></div><p>In addition, for compiled languages such as Go or C++ that do not require build time tooling during runtime, it is recommended to avoid build time tooling in the final images. With Docker&rsquo;s support for <a href=https://docs.docker.com/engine/userguide/eng-image/multistage-build/>multi-stages builds</a>, this can be easily achieved with minimal effort. Such an example can be found at <a href=https://docs.docker.com/develop/develop-images/multistage-build/#name-your-build-stages>Multi-stage builds</a>.</p><p>Google&rsquo;s <a href=https://github.com/GoogleContainerTools/distroless>distroless</a> image is also a good base image.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b5f2d8b3929a230e1b3d8e12f6f6b43f>8 - Dynamic Volume Provisioning</h1><div class=lead>Running a Postgres database on Kubernetes</div><h2 id=overview>Overview</h2><p>The example shows how to run a Postgres database on Kubernetes and how to dynamically provision and mount the storage volumes needed by the database</p><h2 id=run-postgres-database>Run Postgres Database</h2><p>Define the following Kubernetes resources in a yaml file:</p><ul><li>PersistentVolumeClaim (PVC)</li><li>Deployment</li></ul><h3 id=persistentvolumeclaim>PersistentVolumeClaim</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: PersistentVolumeClaim
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: postgresdb-pvc
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  accessModes:
</span></span><span style=display:flex><span>    - ReadWriteOnce
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>    requests:
</span></span><span style=display:flex><span>      storage: 9Gi
</span></span><span style=display:flex><span>  storageClassName: <span style=color:#a31515>&#39;default&#39;</span>
</span></span></code></pre></div><p>This defines a PVC using the storage class <code>default</code>. Storage classes abstract from the underlying storage provider as well as other parameters, like disk-type (e.g., solid-state vs standard disks).</p><p>The default storage class has the annotation <strong>{&ldquo;storageclass.kubernetes.io/is-default-class&rdquo;:&ldquo;true&rdquo;}</strong>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl describe sc default
</span></span><span style=display:flex><span>Name:            default
</span></span><span style=display:flex><span>IsDefaultClass:  Yes
</span></span><span style=display:flex><span>Annotations:     kubectl.kubernetes.io/last-applied-configuration={<span style=color:#a31515>&#34;apiVersion&#34;</span>:<span style=color:#a31515>&#34;storage.k8s.io/v1beta1&#34;</span>,<span style=color:#a31515>&#34;kind&#34;</span>:<span style=color:#a31515>&#34;StorageClass&#34;</span>,<span style=color:#a31515>&#34;metadata&#34;</span>:{<span style=color:#a31515>&#34;annotations&#34;</span>:{<span style=color:#a31515>&#34;storageclass.kubernetes.io/is-default-class&#34;</span>:<span style=color:#a31515>&#34;true&#34;</span>},<span style=color:#a31515>&#34;labels&#34;</span>:{<span style=color:#a31515>&#34;addonmanager.kubernetes.io/mode&#34;</span>:<span style=color:#a31515>&#34;Exists&#34;</span>},<span style=color:#a31515>&#34;name&#34;</span>:<span style=color:#a31515>&#34;default&#34;</span>,<span style=color:#a31515>&#34;namespace&#34;</span>:<span style=color:#a31515>&#34;&#34;</span>},<span style=color:#a31515>&#34;parameters&#34;</span>:{<span style=color:#a31515>&#34;type&#34;</span>:<span style=color:#a31515>&#34;gp2&#34;</span>},<span style=color:#a31515>&#34;provisioner&#34;</span>:<span style=color:#a31515>&#34;kubernetes.io/aws-ebs&#34;</span>}
</span></span><span style=display:flex><span>,storageclass.kubernetes.io/is-default-class=true
</span></span><span style=display:flex><span>Provisioner:           kubernetes.io/aws-ebs
</span></span><span style=display:flex><span>Parameters:            type=gp2
</span></span><span style=display:flex><span>AllowVolumeExpansion:  &lt;unset&gt;
</span></span><span style=display:flex><span>MountOptions:          &lt;none&gt;
</span></span><span style=display:flex><span>ReclaimPolicy:         Delete
</span></span><span style=display:flex><span>VolumeBindingMode:     Immediate
</span></span><span style=display:flex><span>Events:                &lt;none&gt;
</span></span></code></pre></div><p>A Persistent Volume is automatically created when it is dynamically provisioned. In the following example, the PVC is defined
as &ldquo;postgresdb-pvc&rdquo;, and a corresponding PV &ldquo;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb&rdquo; is created and associated with the PVC automatically.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl create -f .<span style=color:#a31515>\p</span>ostgres_deployment.yaml
</span></span><span style=display:flex><span>persistentvolumeclaim <span style=color:#a31515>&#34;postgresdb-pvc&#34;</span> created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl get pv
</span></span><span style=display:flex><span>NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                    STORAGECLASS   REASON    AGE
</span></span><span style=display:flex><span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            Delete           Bound     default/postgresdb-pvc   default                  3s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl get pvc
</span></span><span style=display:flex><span>NAME             STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
</span></span><span style=display:flex><span>postgresdb-pvc   Bound     pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            default        8s
</span></span></code></pre></div><p>Notice that the <strong>RECLAIM POLICY</strong> is <strong>Delete</strong> (default value), which is one of the two reclaim policies, the other
one is <strong>Retain</strong>. (A third policy <strong>Recycle</strong> has been deprecated). In the case of <strong>Delete</strong>, the PV is deleted automatically when the PVC is removed, and the data on the PVC will also be lost.</p><p>On the other hand, a PV with <strong>Retain</strong> policy will not be deleted when the PVC is removed, and moved to <strong>Release</strong> status, so that data can be recovered by Administrators later.</p><p>You can use the <code>kubectl patch</code> command to change the reclaim policy as described in <a href=https://kubernetes.io/docs/tasks/administer-cluster/change-pv-reclaim-policy/>Change the Reclaim Policy of a PersistentVolume</a>
or use <code>kubectl edit pv &lt;pv-name></code> to edit it online as shown below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get pv
</span></span><span style=display:flex><span>NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                    STORAGECLASS   REASON    AGE
</span></span><span style=display:flex><span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            Delete           Bound     default/postgresdb-pvc   default                  44m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># change the reclaim policy from &#34;Delete&#34; to &#34;Retain&#34;</span>
</span></span><span style=display:flex><span>$ kubectl edit pv pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb
</span></span><span style=display:flex><span>persistentvolume <span style=color:#a31515>&#34;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb&#34;</span> edited
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># check the reclaim policy afterwards</span>
</span></span><span style=display:flex><span>$ kubectl get pv
</span></span><span style=display:flex><span>NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                    STORAGECLASS   REASON    AGE
</span></span><span style=display:flex><span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            Retain           Bound     default/postgresdb-pvc   default                  45m
</span></span></code></pre></div><h3 id=deployment>Deployment</h3><p>Once a PVC is created, you can use it in your container via <code>volumes.persistentVolumeClaim.claimName</code>. In the below example, the PVC <strong>postgresdb-pvc</strong> is mounted as readable and writable, and in <code>volumeMounts</code> two paths in the container are mounted to subfolders in the volume.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: postgres
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: postgres
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    deployment.kubernetes.io/revision: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  strategy:
</span></span><span style=display:flex><span>    type: RollingUpdate
</span></span><span style=display:flex><span>    rollingUpdate:
</span></span><span style=display:flex><span>      maxUnavailable: 1
</span></span><span style=display:flex><span>      maxSurge: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: postgres
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      name: postgres
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: postgres
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>        - name: postgres
</span></span><span style=display:flex><span>          image: <span style=color:#a31515>&#34;cpettech.docker.repositories.sap.ondemand.com/jtrack_postgres:howto&#34;</span>
</span></span><span style=display:flex><span>          env:
</span></span><span style=display:flex><span>            - name: POSTGRES_USER
</span></span><span style=display:flex><span>              value: postgres
</span></span><span style=display:flex><span>            - name: POSTGRES_PASSWORD
</span></span><span style=display:flex><span>              value: p5FVqfuJFrM42cVX9muQXxrC3r8S9yn0zqWnFR6xCoPqxqVQ
</span></span><span style=display:flex><span>            - name: POSTGRES_INITDB_XLOGDIR
</span></span><span style=display:flex><span>              value: <span style=color:#a31515>&#34;/var/log/postgresql/logs&#34;</span>
</span></span><span style=display:flex><span>          ports:
</span></span><span style=display:flex><span>            - containerPort: 5432
</span></span><span style=display:flex><span>          volumeMounts:
</span></span><span style=display:flex><span>            - mountPath: /var/lib/postgresql/data
</span></span><span style=display:flex><span>              name: postgre-db
</span></span><span style=display:flex><span>              subPath: data     <span style=color:green># https://github.com/kubernetes/website/pull/2292.  Solve the issue of crashing initdb due to non-empty directory (i.e. lost+found)</span>
</span></span><span style=display:flex><span>            - mountPath: /var/log/postgresql/logs
</span></span><span style=display:flex><span>              name: postgre-db
</span></span><span style=display:flex><span>              subPath: logs
</span></span><span style=display:flex><span>      volumes:
</span></span><span style=display:flex><span>        - name: postgre-db
</span></span><span style=display:flex><span>          persistentVolumeClaim:
</span></span><span style=display:flex><span>            claimName: postgresdb-pvc
</span></span><span style=display:flex><span>            readOnly: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      imagePullSecrets:
</span></span><span style=display:flex><span>      - name: cpettechregistry
</span></span></code></pre></div><p>To check the mount points in the container:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get po
</span></span><span style=display:flex><span>NAME                        READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>postgres-7f485fd768-c5jf9   1/1       Running   0          32m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl exec -it postgres-7f485fd768-c5jf9 bash
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>root@postgres-7f485fd768-c5jf9:/# ls /var/lib/postgresql/data/
</span></span><span style=display:flex><span>base    pg_clog       pg_dynshmem  pg_ident.conf  pg_multixact  pg_replslot  pg_snapshots  pg_stat_tmp  pg_tblspc    PG_VERSION  postgresql.auto.conf  postmaster.opts
</span></span><span style=display:flex><span>global  pg_commit_ts  pg_hba.conf  pg_logical     pg_notify     pg_serial    pg_stat       pg_subtrans  pg_twophase  pg_xlog     postgresql.conf       postmaster.pid
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>root@postgres-7f485fd768-c5jf9:/# ls /var/log/postgresql/logs/
</span></span><span style=display:flex><span>000000010000000000000001  archive_status
</span></span></code></pre></div><h2 id=deleting-a-persistentvolumeclaim>Deleting a PersistentVolumeClaim</h2><p>In case of a <strong>Delete</strong> policy, deleting a PVC will also delete its associated PV. If <strong>Retain</strong> is the reclaim policy, the PV will change status from <strong>Bound</strong> to <strong>Released</strong> when the PVC is deleted.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># Check pvc and pv before deletion</span>
</span></span><span style=display:flex><span>$ kubectl get pvc
</span></span><span style=display:flex><span>NAME             STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
</span></span><span style=display:flex><span>postgresdb-pvc   Bound     pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            default        50m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl get pv
</span></span><span style=display:flex><span>NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                    STORAGECLASS   REASON    AGE
</span></span><span style=display:flex><span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            Retain           Bound     default/postgresdb-pvc   default                  50m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># delete pvc</span>
</span></span><span style=display:flex><span>$ kubectl delete pvc postgresdb-pvc
</span></span><span style=display:flex><span>persistentvolumeclaim <span style=color:#a31515>&#34;postgresdb-pvc&#34;</span> deleted
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># pv changed to status &#34;Released&#34;</span>
</span></span><span style=display:flex><span>$ kubectl get pv
</span></span><span style=display:flex><span>NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                    STORAGECLASS   REASON    AGE
</span></span><span style=display:flex><span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            Retain           Released   default/postgresdb-pvc   default                  51m
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-c92b3f016e0373ce1873b3ea8d48df59>9 - Install Knative in Gardener Clusters</h1><div class=lead>A walkthrough the steps for installing Knative in Gardener shoot clusters.</div><h2 id=overview>Overview</h2><p>This guide walks you through the installation of the latest version of Knative using pre-built images on a <a href=https://gardener.cloud>Gardener</a> created cluster environment. To set up your own Gardener, see the <a href=/docs/gardener/>documentation</a> or have a look at the <a href=https://github.com/gardener/landscape-setup-template>landscape-setup-template</a> project. To learn more about this open source project, read the <a href=https://kubernetes.io/blog/2018/05/17/gardener/>blog on kubernetes.io</a>.</p><h2 id=prerequisites>Prerequisites</h2><p>Knative requires a Kubernetes cluster v1.15 or newer.</p><h2 id=steps>Steps</h2><h3 id=install-and-configure-kubectl>Install and Configure kubectl</h3><ol><li><p>If you already have <code>kubectl</code> CLI, run <code>kubectl version --short</code> to check the version. You need v1.10 or newer. If your <code>kubectl</code> is older, follow the next step to install a newer version.</p></li><li><p><a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>Install the kubectl CLI</a>.</p></li></ol><h3 id=access-gardener>Access Gardener</h3><ol><li><p>Create a project in the Gardener dashboard. This will essentially create a
Kubernetes namespace with the name <code>garden-&lt;my-project></code>.</p></li><li><p><a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/#configure-kubectl>Configure access to your Gardener project</a> using a kubeconfig.</p><p>If you are not the Gardener Administrator already, you can create a technical user in the Gardener dashboard. Go to the &ldquo;Members&rdquo; section and add a service account. You can then download the kubeconfig for your project. You can skip this step if you create your cluster using the user interface; it is only needed for programmatic access, make sure you set <code>export KUBECONFIG=garden-my-project.yaml</code> in your shell.</p><p><img src=/__resources/gardener_service_account_2a7657.png alt="Download kubeconfig for Gardener"></p></li></ol><h3 id=creating-a-kubernetes-cluster>Creating a Kubernetes Cluster</h3><p>You can create your cluster using <code>kubectl</code> CLI by providing a cluster specification yaml file. You can find an example for GCP in the <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>gardener/gardener repository</a>. Make sure the namespace matches that of your project. Then just apply the prepared so-called &ldquo;shoot&rdquo; cluster CRD with kubectl:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl apply --filename my-cluster.yaml
</span></span></code></pre></div><p>The easier alternative is to create the cluster following the cluster creation wizard in the Gardener dashboard:
<img src=/__resources/gardener_shoot_creation_709dcf.png alt="shoot creation" title="shoot creation via the dashboard"></p><h3 id=configure-kubectl-for-your-cluster>Configure kubectl for Your Cluster</h3><p>You can now download the kubeconfig for your freshly created cluster in the Gardener dashboard or via the CLI as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl --namespace shoot--my-project--my-cluster get secret kubecfg --output jsonpath={.data.kubeconfig} | base64 --decode &gt; my-cluster.yaml
</span></span></code></pre></div><p>This kubeconfig file has full administrators access to you cluster. For the rest of this guide, be sure you have <code>export KUBECONFIG=my-cluster.yaml</code> set.</p><h2 id=installing-istio>Installing Istio</h2><p>Knative depends on Istio. If your cloud platform offers a managed Istio installation, we recommend installing Istio that way, unless you need the ability to customize your installation.</p><p>Otherwise, see the <a href=https://knative.dev/docs/install/installing-istio/>Installing Istio for Knative guide</a> to install Istio.</p><p>You must install Istio on your Kubernetes cluster before continuing with these instructions to install Knative.</p><h2 id=installing-cluster-local-gateway-for-serving-cluster-internal-traffic>Installing <code>cluster-local-gateway</code> for Serving Cluster-Internal Traffic</h2><p>If you installed Istio, you can install a <code>cluster-local-gateway</code> within your Knative cluster so that you can serve cluster-internal traffic. If you want to configure your revisions to use routes that are visible only within your cluster, <a href=https://knative.dev/docs/admin/install/knative-offerings/>install and use the <code>cluster-local-gateway</code></a>.</p><h2 id=installing-knative>Installing Knative</h2><p>The following commands install all available Knative components as well as the standard set of observability plugins. Knative&rsquo;s installation guide - <a href=https://knative.dev/docs/admin/install/>Installing Knative</a>.</p><ol><li><p>If you are upgrading from Knative 0.3.x: Update your domain and static IP address to be associated with the LoadBalancer <code>istio-ingressgateway</code> instead of <code>knative-ingressgateway</code>. Then run the following to clean up leftover resources:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete svc knative-ingressgateway -n istio-system
</span></span><span style=display:flex><span>kubectl delete deploy knative-ingressgateway -n istio-system
</span></span></code></pre></div><p>If you have the Knative Eventing Sources component installed, you will also need to delete the following resource before upgrading:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete statefulset/controller-manager -n knative-sources
</span></span></code></pre></div><p>While the deletion of this resource during the upgrade process will not prevent modifications to Eventing Source resources, those changes will not be completed until the upgrade process finishes.</p></li><li><p>To install Knative, first install the CRDs by running the <code>kubectl apply</code> command once with the <code>-l knative.dev/crd-install=true</code> flag. This prevents race conditions during the install, which cause intermittent errors:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply --selector knative.dev/crd-install=true <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml
</span></span></code></pre></div></li><li><p>To complete the installation of Knative and its dependencies, run the <code>kubectl apply</code> command again, this time without the <code>--selector</code> flag:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply --filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml
</span></span></code></pre></div></li><li><p>Monitor the Knative components until all of the components show a <code>STATUS</code> of <code>Running</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods --namespace knative-serving
</span></span><span style=display:flex><span>kubectl get pods --namespace knative-eventing
</span></span><span style=display:flex><span>kubectl get pods --namespace knative-monitoring
</span></span></code></pre></div></li></ol><h2 id=set-your-custom-domain>Set Your Custom Domain</h2><ol><li>Fetch the external IP or CNAME of the knative-ingressgateway:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl --namespace istio-system get service knative-ingressgateway
</span></span><span style=display:flex><span>NAME                     TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                                      AGE
</span></span><span style=display:flex><span>knative-ingressgateway   LoadBalancer   100.70.219.81   35.233.41.212   80:32380/TCP,443:32390/TCP,32400:32400/TCP   4d
</span></span></code></pre></div><ol><li>Create a wildcard DNS entry in your custom domain to point to the above IP or CNAME:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>*.knative.&lt;my domain&gt; == A 35.233.41.212
</span></span><span style=display:flex><span><span style=color:green># or CNAME if you are on AWS</span>
</span></span><span style=display:flex><span>*.knative.&lt;my domain&gt; == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com
</span></span></code></pre></div><ol><li>Adapt your Knative config-domain (set your domain in the data field):</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl --namespace knative-serving get configmaps config-domain --output yaml
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  knative.&lt;my domain&gt;: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>  name: config-domain
</span></span><span style=display:flex><span>  namespace: knative-serving
</span></span></code></pre></div><h2 id=whats-next>What&rsquo;s Next</h2><p>Now that your cluster has Knative installed, you can see what Knative has to offer.</p><p>Deploy your first app with the <a href=https://knative.dev/docs/serving/getting-started-knative-app/>Getting Started with Knative App Deployment</a> guide.</p><p>Get started with Knative Eventing by walking through one of the <a href=https://knative.dev/docs/eventing/samples/>Eventing Samples</a>.</p><p><a href=https://knative.dev/docs/serving/installing-cert-manager/>Install Cert-Manager</a> if you want to use the <a href=https://knative.dev/docs/serving/encryption/enabling-automatic-tls-certificate-provisioning/>automatic TLS cert provisioning feature</a>.</p><h2 id=cleaning-up>Cleaning Up</h2><p>Use the Gardener dashboard to delete your cluster, or execute the following with kubectl pointing to your <code>garden-my-project.yaml</code> kubeconfig:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project annotate shoot my-cluster confirmation.gardener.cloud/deletion=true
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project delete shoot my-cluster
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-f42467bee108207022ce49c040e6f7a7>10 - Integrity and Immutability</h1><div class=lead>Ensure that you always get the right image</div><h2 id=introduction>Introduction</h2><p>When transferring data among networked systems, <strong>trust is a central concern</strong>. In particular, when communicating over an untrusted medium such as the internet, it is critical to ensure the <strong>integrity and immutability</strong> of all the data a system operates on. Especially if you use Docker Engine to push and pull images (data) to a <strong>public registry</strong>.</p><p>This immutability offers you a guarantee that any and all containers that you instantiate will be absolutely identical at inception. Surprise surprise, deterministic operations.</p><h2 id=a-lesson-in-deterministic-ops>A Lesson in Deterministic Ops</h2><p>Docker Tags are about as reliable and disposable as this guy down here.</p><p><img src=/__resources/howto-content-trust_d582d8.svg alt=docker-labels></p><p>Seems simple enough. You have probably already deployed hundreds of YAML&rsquo;s or started endless counts of Docker containers.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run --name mynginx1 -P -d nginx:1.13.9
</span></span></code></pre></div><p>or</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: rss-site
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: web
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: web
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>        - name: front-end
</span></span><span style=display:flex><span>          image: nginx:1.13.9
</span></span><span style=display:flex><span>          ports:
</span></span><span style=display:flex><span>            - containerPort: 80
</span></span></code></pre></div><p><strong>But Tags are mutable and humans are prone to error. Not a good combination.</strong> Here, we’ll dig into why the use of tags can be dangerous and how to deploy your containers across a pipeline and across environments with <strong>determinism in mind</strong>.</p><p>Let&rsquo;s say that you want to ensure that whether it’s today or 5 years from now, that specific deployment uses the very same image that you have defined. Any updates or newer versions of an image should be executed as a new deployment. <strong>The solution: digest</strong></p><p>A digest takes the place of the tag when pulling an image. For example, to pull the above image by digest, run the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run --name mynginx1 -P -d nginx@sha256:4771d09578c7c6a65299e110b3ee1c0a2592f5ea2618d23e4ffe7a4cab1ce5de
</span></span></code></pre></div><p>You can now make sure that the same image is always loaded at every deployment. It doesn&rsquo;t matter if the TAG of the image has been changed or not. <strong>This solves the problem of repeatability.</strong></p><h2 id=content-trust>Content Trust</h2><p>However, there’s an additionally hidden danger. It is possible for an attacker to replace a server image with another
one infected with malware.</p><p><img src=/__resources/howto-content-trust-replace_dd8c41.svg alt=docker-content-trust></p><p><a href=https://docs.docker.com/engine/security/trust/content_trust/>Docker Content trust</a> gives you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel.</p><p>Prior to version 1.8, Docker didn’t have a way to verify the authenticity of a server image. But in v1.8, a new feature called <strong>Docker Content Trust</strong> was introduced to automatically sign and verify the signature of a publisher.</p><p>So, as soon as a server image is downloaded, it is cross-checked with the signature of the publisher to see if someone tampered with it in any way. <strong>This solves the problem of trust.</strong></p><p>In addition, you should scan all images for known vulnerabilities.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-235d807e19f979fc08e642acd04ce659>11 - Kubernetes Antipatterns</h1><div class=lead>Common antipatterns for Kubernetes and Docker</div><p><img src=/__resources/howto-antipattern_572177.png alt=antipattern></p><p>This HowTo covers common Kubernetes antipatterns that we have seen over the past months.</p><h2 id=running-as-root-user>Running as Root User</h2><p>Whenever possible, do not run containers as root user. One could be tempted to say that Kubernetes pods and nodes are well separated. Host and containers running on it share the same kernel. If a container is compromised, the root user in the container has full control over the underlying node.</p><p>Watch the very good presentation by Liz Rice at the KubeCon 2018</p><iframe width=560 height=315 src=https://www.youtube.com/embed/ltrV-Qmh3oY frameborder=0 allow="autoplay; encrypted-media" allowfullscreen></iframe><p>Use <code>RUN groupadd -r anygroup && useradd -r -g anygroup myuser</code> to create a group and add a user to it. Use the <code>USER</code> command to switch to this user. Note that you may also consider to provide <a href=https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#user>an explicit UID/GID</a> if required.</p><p>For example:</p><pre tabindex=0><code>ARG GF_UID=&#34;500&#34;
ARG GF_GID=&#34;500&#34;

# add group &amp; user
RUN groupadd -r -g $GF_GID appgroup &amp;&amp; \
   useradd appuser -r -u $GF_UID -g appgroup

USER appuser
</code></pre><h2 id=store-data-or-logs-in-containers>Store Data or Logs in Containers</h2><p>Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the
container, as they are lost when the container is closed. Use persistence volumes instead to persist data outside
of containers. Using an <a href=https://www.elastic.co/de/what-is/elk-stack>ELK stack</a> is another good option for storing and processing logs.</p><h2 id=using-pod-ip-addresses>Using Pod IP Addresses</h2><p>Each pod is assigned an IP address. It is necessary for pods to communicate with each other to build an application, e.g. an application must communicate with a database. Existing pods are terminated and new pods are constantly started. If you would rely on the IP address of a pod or container, you would need to update the application configuration constantly. This makes the application fragile.</p><p>Create services instead. They provide a logical name that can be assigned independently of the varying number and IP addresses of containers. Services are the basic concept for load balancing within Kubernetes.</p><h2 id=more-than-one-process-in-a-container>More Than One Process in a Container</h2><p>A docker file provides a <code>CMD</code> and <code>ENTRYPOINT</code> to start the image. <code>CMD</code> is often used around a script that makes a configuration and then starts the container. Do not try to start multiple processes with this script. It is important to consider the separation of concerns when creating docker images. Running multiple processes in a single pod makes managing your containers, collecting logs and updating each process more difficult.</p><p>You can split the image into multiple containers and manage them independently - even in one pod. Bear in mind that Kubernetes only monitors the process with <code>PID=1</code>. If more than one process is started within a container, then these no longer fall under the control of Kubernetes.</p><h2 id=creating-images-in-a-running-container>Creating Images in a Running Container</h2><p>A new image can be created with the <code>docker commit</code> command. This is useful if changes have been made to the container and you want to persist them for later error analysis. However, images created like this are not reproducible and completely worthless for a CI/CD environment. Furthermore, another developer cannot recognize which components the image contains. Instead, always make changes to the docker file, close existing containers and start a new container with the updated image.</p><h2 id=saving-passwords-in-a-docker-image-->Saving Passwords in a docker Image 💀</h2><p><strong>Do not save passwords in a Docker file!</strong> They are in plain text and are checked into a repository. That makes them completely vulnerable even if you are using a private repository like the Artifactory.</p><p>Always use <a href=https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure>Secrets or ConfigMaps</a> to provision passwords or inject them by mounting a persistent volume.</p><h2 id=using-the-latest-tag>Using the &rsquo;latest&rsquo; Tag</h2><p>Starting an image with <em>tomcat</em> is tempting. If no tags are specified, a container is started with the <code>tomcat:latest</code> image. This image may no longer be up to date and refer to an older version instead. Running a production application requires complete control of the environment with exact versions of the image.</p><p>Make sure you always use a tag or even better the <strong>sha256 hash</strong> of the image, e.g., <code>tomcat@sha256:c34ce3c1fcc0c7431e1392cc3abd0dfe2192ffea1898d5250f199d3ac8d8720f</code>.</p><h3 id=why-use-the-sha256-hash>Why Use the sha256 Hash?</h3><p>Tags are not immutable and can be overwritten by a developer at any time. In this case you don&rsquo;t have complete control over your image - which is bad.</p><h2 id=different-images-per-environment>Different Images per Environment</h2><p>Don&rsquo;t create different images for development, testing, staging and production environments. The image should be the <strong>source of truth</strong> and should only be created once and pushed to the repository. This <code>image:tag</code> should be used for different environments in the future.</p><h2 id=depend-on-start-order-of-pods>Depend on Start Order of Pods</h2><p>Applications often depend on containers being started in a certain order. For example, a database container must be up and running before an application can connect to it. The application should be resilient to such changes, as the db pod can be unreachable or restarted at any time. The application container should be able to handle such situations without terminating or crashing.</p><h2 id=additional-anti-patterns-and-patterns>Additional Anti-Patterns and Patterns</h2><p>In the community, vast experience has been collected to improve the stability and usability of Docker and Kubernetes.</p><p>Refer to <a href=https://github.com/gravitational/workshop/blob/master/k8sprod.md>Kubernetes Production Patterns</a> for more information.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7e3d5e1b3bf12bae9f3ee2bd9e99f831>12 - Namespace Isolation</h1><div class=lead>Deny all traffic from other namespaces</div><h2 id=overview>Overview</h2><p>You can configure a <strong>NetworkPolicy</strong> to deny all the traffic from other namespaces while allowing all the traffic coming from the same namespace the pod was deployed into.</p><img src=/__resources/howto-namespaceisolation_77a6bb.png alt=howto-namespaceisolation width=100%><p><strong>There are many reasons why you may chose to employ Kubernetes network policies:</strong></p><ul><li>Isolate multi-tenant deployments</li><li>Regulatory compliance</li><li>Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each other</li></ul><p>Kubernetes <strong>network policies</strong> are application centric compared to infrastructure/network centric standard firewalls.
<strong>There are no explicit CIDRs or IP addresses used</strong> for matching source or destination IP’s.
<strong>Network policies build up on labels and selectors</strong> which are key concepts of Kubernetes that are used to organize (for example, all DB tier pods of an app) and select subsets of objects.</p><h2 id=example>Example</h2><p>We create two nginx HTTP-Servers in two namespaces and block all traffic between the two namespaces. E.g. you are
unable to get content from <em>namespace1</em> if you are sitting in <em>namespace2</em>.</p><h2 id=setup-the-namespaces>Setup the Namespaces</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># create two namespaces for test purpose</span>
</span></span><span style=display:flex><span>kubectl create ns customer1
</span></span><span style=display:flex><span>kubectl create ns customer2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># create a standard HTTP web server</span>
</span></span><span style=display:flex><span>kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer1
</span></span><span style=display:flex><span>kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># expose the port 80 for external access</span>
</span></span><span style=display:flex><span>kubectl expose deployment nginx --port=80 --type=NodePort -n=customer1
</span></span><span style=display:flex><span>kubectl expose deployment nginx --port=80 --type=NodePort -n=customer2
</span></span></code></pre></div><hr><h2 id=test-without-np>Test Without NP</h2><img src=/__resources/howto-namespaceisolation-without_2dd6b0.png alt=howto-namespaceisolation-without width=80%><p>Create a pod with <em>curl</em> preinstalled inside the namespace <em>customer1</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># create a &#34;bash&#34; pod in one namespace</span>
</span></span><span style=display:flex><span>kubectl run -i --tty client --image=tutum/curl -n=customer1
</span></span></code></pre></div><p>Try to <em>curl</em> the exposed nginx server to get the default index.html page. <strong>Execute this in the bash prompt of the pod created above.</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># get the index.html from the nginx of the namespace &#34;customer1&#34; =&gt; success</span>
</span></span><span style=display:flex><span>curl http://nginx.customer1
</span></span><span style=display:flex><span><span style=color:green># get the index.html from the nginx of the namespace &#34;customer2&#34; =&gt; success</span>
</span></span><span style=display:flex><span>curl http://nginx.customer2
</span></span></code></pre></div><p>Both calls are done in a pod within the namespace <em>customer1</em> and both nginx servers are always reachable, no matter in what namespace.</p><hr><h2 id=test-with-np>Test with NP</h2><img src=/__resources/howto-namespaceisolation-with_9a7cc8.png alt=howto-namespaceisolation-with width=80%><p>Install the <strong>NetworkPolicy</strong> from your shell:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: NetworkPolicy
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: deny-from-other-namespaces
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  podSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>  ingress:
</span></span><span style=display:flex><span>  - from:
</span></span><span style=display:flex><span>    - podSelector: {}
</span></span></code></pre></div><ul><li>it applies the policy to ALL pods in the named namespace as the <code>spec.podSelector.matchLabels</code> is empty and therefore selects all pods.</li><li>it allows traffic from ALL pods in the named namespace, as <code>spec.ingress.from.podSelector</code> is empty and therefore selects all pods.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f ./network-policy.yaml -n=customer1
</span></span><span style=display:flex><span>kubectl apply -f ./network-policy.yaml -n=customer2
</span></span></code></pre></div><p>After this, <code>curl http://nginx.customer2</code> shouldn&rsquo;t work anymore if you are a service inside the namespace <em>customer1</em> and
vice versa<div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>This policy, once applied, will also disable all external traffic to these pods. For example, you can create a service of type <code>LoadBalancer</code> in namespace <code>customer1</code> that match the nginx pod. When you request the service by its <code>&lt;EXTERNAL_IP>:&lt;PORT></code>, then the network policy that will deny the ingress traffic from the service and the request will time out.</div></p><h2 id=related-links>Related Links</h2><p>You can get more information on how to configure the <strong>NetworkPolicies</strong> at:</p><ul><li><a href=https://docs.projectcalico.org/v3.0/getting-started/kubernetes/tutorials/advanced-policy>Calico WebSite</a></li><li><a href=https://github.com/ahmetb/kubernetes-network-policy-recipes>Kubernetes NP Recipes</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-93caf142ce14ddc5cba9918a162423c5>13 - Orchestration of Container Startup</h1><div class=lead>How to orchestrate a startup sequence of multiple containers</div><h2 id=disclaimer>Disclaimer</h2><p>If an application depends on other services deployed separately, do not rely on a certain start sequence of containers. Instead, ensure that the application can cope with unavailability of the services it depends on.</p><h2 id=introduction>Introduction</h2><p>Kubernetes offers a feature called <a href=https://kubernetes.io/docs/concepts/workloads/pods/init-containers/>InitContainers</a> to perform some tasks during a pod&rsquo;s initialization.
In this tutorial, we demonstrate how to use <code>InitContainers</code> in order to orchestrate a starting sequence of multiple containers. The tutorial uses the example app <a href=https://medium.com/@xcoulon/deploying-your-first-web-app-on-minikube-6e98d2884b3a>url-shortener</a>, which consists of two components:</p><ul><li>postgresql database</li><li>webapp which depends on the postgresql database and provides two endpoints: <em>create a short url from a given location</em> and <em>redirect from a given short URL to the corresponding target location</em></li></ul><p>This app represents the minimal example where an application relies on another service or database. In this example, if the application starts before the database is ready, the application will fail as shown below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl logs webapp-958cf5567-h247n
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2018-06-12T11:02:42Z&#34;</span> level=info msg=<span style=color:#a31515>&#34;Connecting to Postgres database using: host=`postgres:5432` dbname=`url_shortener_db` username=`user`\n&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2018-06-12T11:02:42Z&#34;</span> level=fatal msg=<span style=color:#a31515>&#34;failed to start: failed to open connection to database: dial tcp: lookup postgres on 100.64.0.10:53: no such host\n&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl get po -w
</span></span><span style=display:flex><span>NAME                                READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Pending   0         0s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Pending   0         0s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       ContainerCreating   0         0s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       ContainerCreating   0         1s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Error     0         2s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Error     1         3s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       CrashLoopBackOff   1         4s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Error     2         18s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       CrashLoopBackOff   2         29s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Error     3         43s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       CrashLoopBackOff   3         56s
</span></span></code></pre></div><p>If the <code>restartPolicy</code> is set to <code>Always</code> (default) in the yaml file, the application will continue to restart the pod with an exponential back-off delay in case of failure.</p><h2 id=using-initcontaniner>Using InitContaniner</h2><p>To avoid such a situation, <code>InitContainers</code> can be defined, which are executed prior to the application container. If one of the <code>InitContainers</code> fails, the application container won&rsquo;t be triggered.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: webapp
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: webapp
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: webapp
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      initContainers:  <span style=color:green># check if DB is ready, and only continue when true</span>
</span></span><span style=display:flex><span>      - name: check-db-ready
</span></span><span style=display:flex><span>        image: postgres:9.6.5
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#39;sh&#39;</span>, <span style=color:#a31515>&#39;-c&#39;</span>,  <span style=color:#a31515>&#39;until pg_isready -h postgres -p 5432;  do echo waiting for database; sleep 2; done;&#39;</span>]
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - image: xcoulon/go-url-shortener:0.1.0
</span></span><span style=display:flex><span>        name: go-url-shortener
</span></span><span style=display:flex><span>        env:
</span></span><span style=display:flex><span>        - name: POSTGRES_HOST
</span></span><span style=display:flex><span>          value: postgres
</span></span><span style=display:flex><span>        - name: POSTGRES_PORT
</span></span><span style=display:flex><span>          value: <span style=color:#a31515>&#34;5432&#34;</span>
</span></span><span style=display:flex><span>        - name: POSTGRES_DATABASE
</span></span><span style=display:flex><span>          value: url_shortener_db
</span></span><span style=display:flex><span>        - name: POSTGRES_USER
</span></span><span style=display:flex><span>          value: user
</span></span><span style=display:flex><span>        - name: POSTGRES_PASSWORD
</span></span><span style=display:flex><span>          value: mysecretpassword
</span></span><span style=display:flex><span>        ports:
</span></span><span style=display:flex><span>        - containerPort: 8080
</span></span></code></pre></div><p>In the above example, the <code>InitContainers</code> use the docker image <code>postgres:9.6.5</code>, which is different from the application container.</p><p>This also brings the advantage of not having to include unnecessary tools (e.g., pg_isready) in the application container.</p><p>With introduction of <code>InitContainers</code>, in case the database is not available yet, the pod startup will look like similarly to:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get po -w
</span></span><span style=display:flex><span>NAME                                READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>nginx-deployment-5cc79d6bfd-t9n8h   1/1       Running   0          5d
</span></span><span style=display:flex><span>privileged-pod                      1/1       Running   0          4d
</span></span><span style=display:flex><span>webapp-fdcb49cbc-4gs4n   0/1       Pending   0         0s
</span></span><span style=display:flex><span>webapp-fdcb49cbc-4gs4n   0/1       Pending   0         0s
</span></span><span style=display:flex><span>webapp-fdcb49cbc-4gs4n   0/1       Init:0/1   0         0s
</span></span><span style=display:flex><span>webapp-fdcb49cbc-4gs4n   0/1       Init:0/1   0         1s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl  logs webapp-fdcb49cbc-4gs4n
</span></span><span style=display:flex><span>Error from server (BadRequest): container <span style=color:#a31515>&#34;go-url-shortener&#34;</span> in pod <span style=color:#a31515>&#34;webapp-fdcb49cbc-4gs4n&#34;</span> is waiting to start: PodInitializing
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-2e1edb9ad9287519ffefdaad371ad887>14 - Out-Dated HTML and JS Files Delivered</h1><div class=lead>Why is my application always outdated?</div><h2 id=problem>Problem</h2><p><strong>After updating your HTML and JavaScript sources in your web application, the Kubernetes cluster delivers outdated versions - why?</strong></p><h2 id=overview>Overview</h2><p>By default, Kubernetes service pods are not accessible from the external network, but only from other pods within the same Kubernetes cluster.</p><p>The Gardener cluster has a built-in configuration for HTTP load balancing called <strong>Ingress</strong>, defining rules for external connectivity to Kubernetes services. Users who want external access to their Kubernetes services create an ingress resource that defines rules, including the URI path, backing service name, and other information. The Ingress controller can then automatically program a frontend load balancer to enable Ingress configuration.</p><p><img src=/__resources/howto-nginx_c8fb27.svg alt=nginx></p><h2 id=example-ingress-configuration>Example Ingress Configuration</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1beta1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: vuejs-ingress
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: test.ingress.&lt;GARDENER-CLUSTER&gt;.&lt;GARDENER-PROJECT&gt;.shoot.canary.k8s-hana.ondemand.com
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - backend:
</span></span><span style=display:flex><span>          serviceName: vuejs-svc
</span></span><span style=display:flex><span>          servicePort: 8080
</span></span></code></pre></div><p>where:</p><ul><li><strong>&lt;GARDENER-CLUSTER></strong>: The cluster name in the Gardener</li><li><strong>&lt;GARDENER-PROJECT></strong>: You project name in the Gardener</li></ul><h2 id=diagnosing-the-problem>Diagnosing the Problem</h2><p>The ingress controller we are using is <strong>NGINX</strong>. NGINX is a software load balancer, web server, and <strong>content cache</strong> built on top of open source NGINX.</p><p><strong>NGINX caches the content as specified in the HTTP header.</strong> If the HTTP header is missing, it is assumed that the cache is <strong>forever</strong> and NGINX never updates the content in the stupidest case.</p><h2 id=solution>Solution</h2><p>In general, you can avoid this pitfall with one of the solutions below:</p><ul><li>Use a cache buster + HTTP-Cache-Control (prefered)</li><li>Use HTTP-Cache-Control with a lower retention period</li><li>Disable the caching in the ingress (just for dev purposes)</li></ul><p>Learning how to set the HTTP header or setup a cache buster is left to you, as an exercise for your web framework (e.g., Express/NodeJS, SpringBoot, &mldr;)</p><p>Here is an example on how to disable the cache control for your ingress, done with an annotation in your
ingress YAML (during development).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: networking.k8s.io/v1beta1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    ingress.kubernetes.io/cache-enable: <span style=color:#a31515>&#34;false&#34;</span>
</span></span><span style=display:flex><span>  name: vuejs-ingress
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: test.ingress.&lt;GARDENER-CLUSTER&gt;.&lt;GARDENER-PROJECT&gt;.shoot.canary.k8s-hana.ondemand.com
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - backend:
</span></span><span style=display:flex><span>          serviceName: vuejs-svc
</span></span><span style=display:flex><span>          servicePort: 8080
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-096b92593d33f1e8fa2b3742550b1534>15 - Remove Committed Secrets in Github 💀</h1><div class=lead>Never ever commit a kubeconfig.yaml into github</div><h2 id=overview>Overview</h2><p>If you commit sensitive data, such as a <code>kubeconfig.yaml</code> or <code>SSH key</code> into a Git repository, you can remove it from
the history. To entirely remove unwanted files from a repository&rsquo;s history you can use the git <code>filter-branch</code> command.</p><p>The git <code>filter-branch</code> command rewrites your repository&rsquo;s history, which changes the SHAs for existing commits that you alter and any dependent commits. Changed commit SHAs may affect open pull requests in your repository. <strong>Merging or closing all open pull requests before removing files from your repository is recommended.</strong></p><div class="alert alert-warning" role=alert><h4 class=alert-heading>Warning</h4>If someone has already checked out the repository, then of course they have the secret on their computer. So ALWAYS revoke the OAuthToken/Password or whatever it was immediately.</div><h2 id=purging-a-file-from-your-repositorys-history>Purging a File from Your Repository&rsquo;s History</h2><div class="alert alert-warning" role=alert><h4 class=alert-heading>Warning</h4>If you run <code>git filter-branch</code> after stashing changes, you won&rsquo;t be able to retrieve your changes with other stash commands. Before running <code>git filter-branch</code>, we recommend unstashing any changes you&rsquo;ve made. To unstash the last set of changes you&rsquo;ve stashed, run <code>git stash show -p | git apply -R</code>. For more information, see <a href=https://git-scm.com/book/en/v2/Git-Tools-Stashing-and-Cleaning>Git Tools - Stashing and Cleaning</a>.</div><p>To illustrate how <code>git filter-branch</code> works, we&rsquo;ll show you how to remove your file with sensitive data from the history of your repository and add it to .gitignore to ensure that it is not accidentally re-committed.</p><p><strong>1. Navigate into the repository&rsquo;s working directory:</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd YOUR-REPOSITORY
</span></span></code></pre></div><p><strong>2. Run the following command, replacing <code>PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA</code> with the path to the file you want to remove, not just its filename.</strong></p><p>These arguments will:</p><ul><li>Force Git to process, but not check out, the entire history of every branch and tag</li><li>Remove the specified file, as well as any empty commits generated as a result</li><li>Overwrite your existing tags</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git filter-branch --force --index-filter <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span><span style=color:#a31515>&#39;git rm --cached --ignore-unmatch PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA&#39;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--prune-empty --tag-name-filter cat -- --all
</span></span></code></pre></div><p><strong>3. Add your file with sensitive data to <code>.gitignore</code> to ensure that you don&rsquo;t accidentally commit it again:</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span> echo <span style=color:#a31515>&#34;YOUR-FILE-WITH-SENSITIVE-DATA&#34;</span> &gt;&gt; .gitignore
</span></span></code></pre></div><p>Double-check that you&rsquo;ve removed everything you wanted to from your repository&rsquo;s history, and that all of your branches are checked out. Once you&rsquo;re happy with the state of your repository, continue to the next step.</p><p><strong>4. Force-push your local changes to overwrite your GitHub repository, as well as all the branches you&rsquo;ve pushed up:</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git push origin --force --all
</span></span></code></pre></div><p><strong>4. In order to remove the sensitive file from your tagged releases, you&rsquo;ll also need to force-push against your Git tags:</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git push origin --force --tags
</span></span></code></pre></div><div class="alert alert-warning" role=alert><h4 class=alert-heading>Warning</h4>Tell your collaborators to <strong>rebase, not merge</strong>, any branches they created off of your old (tainted) repository history. One merge commit could reintroduce some or all of the tainted history that you just went to the trouble of purging.</div><h2 id=related-links>Related Links</h2><ul><li><a href=https://help.github.com/articles/removing-sensitive-data-from-a-repository/>Removing Sensitive Data from a Repository</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ee35bcc542b67fe16881e25789669bcb>16 - Shoot Pod Autoscaling Best Practices</h1><h1 id=introduction>Introduction</h1><p>There are two types of pod autoscaling in Kubernetes: Horizontal Pod Autoscaling (HPA) and Vertical Pod Autoscaling (VPA). HPA (implemented as part of the kube-controller-manager) scales the number of pod replicas, while VPA (implemented as independent community project) adjusts the CPU and memory requests for the pods. Both types of autoscaling aim to optimize resource usage/costs and maintain the performance and (high) availability of applications running on Kubernetes.</p><h2 id=horizontal-pod-autoscaling-hpahttpskubernetesiodocstasksrun-applicationhorizontal-pod-autoscale><a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale>Horizontal Pod Autoscaling (HPA)</a></h2><p>Horizontal Pod Autoscaling involves increasing or decreasing the number of pod replicas in a deployment, replica set, stateful set, or <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-autoscaling/853-configurable-hpa-scale-velocity/README.md#summary>anything really with a scale subresource that manages pods</a>. HPA adjusts the number of replicas based on specified metrics, such as CPU or memory average utilization (usage divided by requests; most common) or average value (usage; less common). When the demand on your application increases, HPA automatically scales out the number of pods to meet the demand. Conversely, when the demand decreases, it scales in the number of pods to reduce resource usage.</p><p>HPA targets (mostly stateless) applications where adding more instances of the application can linearly increase the ability to handle additional load. It is very useful for applications that experience variable traffic patterns, as it allows for real-time scaling without the need for manual intervention.</p><blockquote><p>[!NOTE]
HPA continuously monitors the metrics of the targeted pods and adjusts the number of replicas based on the observed metrics. It operates solely on the current metrics when it calculates the averages across all pods, meaning it reacts to the immediate resource usage without considering past trends or patterns. Also, all pods are treated equally based on the average metrics. This could potentially lead to situations where some pods are under high load while others are underutilized. Therefore, particular care must be applied to (fair) load-balancing (connection vs. request vs. actual resource load balancing are crucial).</p></blockquote><h3 id=a-few-words-on-the-cluster-proportional-horizontal-autoscaler-cpahttpsgithubcomkubernetes-sigscluster-proportional-autoscaler-and-the-cluster-proportional-vertical-autoscaler-cpvahttpsgithubcomkubernetes-sigscluster-proportional-vertical-autoscaler>A Few Words on the <a href=https://github.com/kubernetes-sigs/cluster-proportional-autoscaler>Cluster-Proportional (Horizontal) Autoscaler (CPA)</a> and the <a href=https://github.com/kubernetes-sigs/cluster-proportional-vertical-autoscaler>Cluster-Proportional Vertical Autoscaler (CPVA)</a></h3><p>Besides HPA and VPA, CPA and CPVA are further options for scaling horizontally or vertically (neither is deployed by Gardener and must be deployed by the user). Unlike HPA and VPA, CPA and CPVA do not monitor the actual pod metrics, but scale solely on the number of nodes or CPU cores in the cluster. While this approach may be helpful and sufficient in a few rare cases, it is often a risky and crude scaling scheme that we do not recommend. More often than not, cluster-proportional scaling results in either under- or over-reserving your resources.</p><h2 id=vertical-pod-autoscaling-vpahttpsgithubcomkubernetesautoscalertreemastervertical-pod-autoscalerreadme><a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#readme>Vertical Pod Autoscaling (VPA)</a></h2><p>Vertical Pod Autoscaling, on the other hand, focuses on adjusting the CPU and memory resources allocated to the pods themselves. Instead of changing the number of replicas, VPA tweaks the resource requests (and limits, but only <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md#keeping-limit-proportional-to-request>proportionally</a>, if configured) for the pods in a deployment, replica set, stateful set, daemon set, or <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/FAQ.md#how-can-i-apply-vpa-to-my-custom-resource>anything really with a scale subresource that manages pods</a>. This means that each pod can be given more, or fewer resources as needed.</p><p>VPA is very useful for optimizing the resource requests of pods that have dynamic resource needs over time. It does so by mutating pod requests (unfortunately, <a href=https://github.com/kubernetes/design-proposals-archive/blob/main/autoscaling/vertical-pod-autoscaler.md#in-place-updates>not in-place</a>). Therefore, in order to apply new recommendations, pods that are &ldquo;out of bounds&rdquo; (i.e. below a configured/computed lower or above a configured/computed upper recommendation percentile) will be evicted proactively, but also pods that are &ldquo;within bounds&rdquo; may be evicted after a grace period. The corresponding higher-level replication controller will then recreate a new pod that VPA will then mutate to set the currently recommended requests (and proportional limits, if configured).</p><blockquote><p>[!NOTE]
VPA continuously monitors all targeted pods and calculates recommendations based on their usage (one recommendation for the entire target). This calculation is influenced by configurable percentiles, with a greater emphasis on recent usage data and a gradual decrease (=decay) in the relevance of older data. However, this means, that VPA doesn&rsquo;t take into account individual needs of single pods - eventually, all pods will receive the same recommendation, which may lead to considerable resource waste. Ideally, VPA would update pods <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/1287-in-place-update-pod-resources/README.md>in-place</a> depending on their individual needs, but that&rsquo;s (individual recommendations) not in its design, even if <a href=https://github.com/kubernetes/design-proposals-archive/blob/main/autoscaling/vertical-pod-autoscaler.md#in-place-updates>in-place updates</a> get implemented, which may be years away for VPA based on current activity on the component.</p></blockquote><h2 id=selecting-the-appropriate-autoscaler>Selecting the Appropriate Autoscaler</h2><p>Before deciding on an autoscaling strategy, it&rsquo;s important to understand the characteristics of your application:</p><ul><li><strong>Interruptibility:</strong> Most importantly, if the clients of your workload are too sensitive to disruptions/cannot cope well with terminating pods, then maybe neither HPA nor VPA is an option (both, HPA and VPA cause pods and connections to be terminated, though VPA even more frequently). Clients must retry on disruptions, which is a reasonable ask in a highly dynamic (and self-healing) environment such as Kubernetes, but this is often not respected (or expected) by your clients (they may not know or care you run the workload in a Kubernetes cluster and have different expectations to the stability of the workload unless you communicated those through <a href=https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos>SLIs/SLOs/SLAs</a>).</li><li><strong>Statelessness:</strong> Is your application stateless or stateful? Stateless applications are typically better candidates for HPA as they can be easily scaled out by adding more replicas without worrying about maintaining state.</li><li><strong>Traffic Patterns:</strong> Does your application experience variable traffic? If so, HPA can help manage these fluctuations by adjusting the number of replicas to handle the load.</li><li><strong>Resource Usage:</strong> Does your application&rsquo;s resource usage change over time? VPA can adjust the CPU and memory reservations dynamically, which is beneficial for applications with non-uniform resource requirements.</li><li><strong>Scalability:</strong> Can your application handle increased load by scaling vertically (more resources per pod) or does it require horizontal scaling (more pod instances)?</li></ul><p>HPA is the right choice if:</p><ul><li>Your application is stateless and can handle increased load by adding more instances.</li><li>You experience short-term fluctuations in traffic that require quick scaling responses.</li><li>You want to maintain a specific performance metric, such as requests per second per pod.</li></ul><p>VPA is the right choice if:</p><ul><li>Your application&rsquo;s resource requirements change over time, and you want to optimize resource usage without manual intervention.</li><li>You want to avoid the complexity of managing resource requests for each pod, especially when they run code where it&rsquo;s impossible for you to suggest static requests.</li></ul><p>In essence:</p><ul><li>For applications that can handle increased load by simply adding more replicas, HPA should be used to handle short-term fluctuations in load by scaling the number of replicas.</li><li>For applications that require more resources per pod to handle additional work, VPA should be used to adjust the resource allocation for longer-term trends in resource usage.</li></ul><p>Consequently, if both cases apply (VPA often applies), HPA and VPA can also be combined. However, combining both, especially on the same metrics (CPU and memory), requires understanding and care to avoid conflicts and ensure that the autoscaling actions do not interfere with and rather complement each other. For more details, see <a href=/docs/guides/applications/shoot-pod-autoscaling-best-practices/#combining-hpa-and-vpa>Combining HPA and VPA</a>.</p><h1 id=horizontal-pod-autoscaler-hpa>Horizontal Pod Autoscaler (HPA)</h1><p>HPA operates by monitoring resource metrics for all pods in a target. It computes the desired number of replicas from the current average metrics and the desired user-defined metrics <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details>as follows</a>:</p><p><code>desiredReplicas = ceil[currentReplicas * (currentMetricValue / desiredMetricValue)]</code></p><p>HPA checks the metrics at regular intervals, which can be configured by the user. <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-metrics-apis>Several types of metrics</a> are supported (classical resource metrics like CPU and memory, but also custom and external metrics like requests per second or queue length can be configured, if available). If a scaling event is necessary, HPA adjusts the replica count for the targeted resource.</p><h2 id=defining-an-hpa-resource>Defining an HPA Resource</h2><p>To configure HPA, you need to create an HPA resource in your cluster. This resource specifies the target to scale, the metrics to be used for scaling decisions, and the desired thresholds. Here&rsquo;s an example of an HPA configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: autoscaling/v2
</span></span><span style=display:flex><span>kind: HorizontalPodAutoscaler
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: foo-hpa
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  scaleTargetRef:
</span></span><span style=display:flex><span>    apiVersion: apps/v1
</span></span><span style=display:flex><span>    kind: Deployment
</span></span><span style=display:flex><span>    name: foo-deployment
</span></span><span style=display:flex><span>  minReplicas: 1
</span></span><span style=display:flex><span>  maxReplicas: 10
</span></span><span style=display:flex><span>  metrics:
</span></span><span style=display:flex><span>  - type: Resource
</span></span><span style=display:flex><span>    resource:
</span></span><span style=display:flex><span>      name: cpu
</span></span><span style=display:flex><span>      target:
</span></span><span style=display:flex><span>        type: AverageValue
</span></span><span style=display:flex><span>        averageValue: 2
</span></span><span style=display:flex><span>  - type: Resource
</span></span><span style=display:flex><span>    resource:
</span></span><span style=display:flex><span>      name: memory
</span></span><span style=display:flex><span>      target:
</span></span><span style=display:flex><span>        type: AverageValue
</span></span><span style=display:flex><span>        averageValue: 8G
</span></span><span style=display:flex><span>  behavior:
</span></span><span style=display:flex><span>    scaleUp:
</span></span><span style=display:flex><span>      stabilizationWindowSeconds: 30
</span></span><span style=display:flex><span>      policies:
</span></span><span style=display:flex><span>      - type: Percent
</span></span><span style=display:flex><span>        value: 100
</span></span><span style=display:flex><span>        periodSeconds: 60
</span></span><span style=display:flex><span>    scaleDown:
</span></span><span style=display:flex><span>      stabilizationWindowSeconds: 1800
</span></span><span style=display:flex><span>      policies:
</span></span><span style=display:flex><span>      - type: Pods
</span></span><span style=display:flex><span>        value: 1
</span></span><span style=display:flex><span>        periodSeconds: 300
</span></span></code></pre></div><p>In this example, HPA is configured to scale <code>foo-deployment</code> based on pod average CPU and memory usage. It will maintain an average CPU and memory usage (not utilization, which is usage divided by requests!) across all replicas of 2 CPUs and 8G or lower with as few replicas as possible. The number of replicas will be scaled between a minimum of 1 and a maximum of 10 based on this target.</p><p>Since a while, you can also <a href=https://kubernetes.io/blog/2023/05/02/hpa-container-resource-metric>configure the autoscaling based on the resource usage of individual containers</a>, not only on the resource usage of the entire pod. All you need to do is to switch the <code>type</code> from <code>Resource</code> to <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#container-resource-metrics><code>ContainerResource</code> and specify the container name</a>.</p><p>In the official documentation (<a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale>[1]</a> and <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough>[2]</a>) you will find examples with average utilization (<code>averageUtilization</code>), not average usage (<code>averageValue</code>), but this is not particularly helpful, especially if you plan to combine HPA together with VPA on the same metrics (generally discouraged in the documentation). If you want to safely combine both on the same metrics, you should scale on average usage (<code>averageValue</code>) as shown above. For more details, see <a href=/docs/guides/applications/shoot-pod-autoscaling-best-practices/#combining-hpa-and-vpa>Combining HPA and VPA</a>.</p><p>Finally, the behavior section influences how fast you scale up and down. Most of the time (depends on your workload), you like to scale out faster than you scale in. In this example, the configuration will trigger a scale-out only after observing the need to scale out for 30s (<code>stabilizationWindowSeconds</code>) and will then only scale out at most 100% (<code>value</code> + <code>type</code>) of the current number of replicas every 60s (<code>periodSeconds</code>). The configuration will trigger a scale-in only after observing the need to scale in for 1800s (<code>stabilizationWindowSeconds</code>) and will then only scale in at most 1 pod (<code>value</code> + <code>type</code>) every 300s (<code>periodSeconds</code>). As you can see, scale-out happens quicker than scale-in in this example.</p><h2 id=hpa-actually-kcm-options>HPA (actually KCM) Options</h2><p>HPA is a function of the kube-controller-manager (KCM).</p><p>You can read up the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager>full KCM options</a> online and set most of them conveniently in your <a href=https://github.com/gardener/gardener/blob/957e4c7/example/90-shoot.yaml#L221-L226>Gardener shoot cluster spec</a>:</p><ul><li><code>downscaleStabilization</code> (default 5m): HPA will scale out whenever the formula (in accordance with the behavior section, if present in the HPA resource) yields a higher replica count, but it won&rsquo;t scale in just as eagerly. This option lets you define a trailing time window that HPA must check and only if the recommended replica count is consistently lower throughout the entire time window, HPA will scale in (in accordance with the behavior section, if present in the HPA resource). If at any point in time in that trailing time window the recommended replica count isn&rsquo;t lower, scale-in won&rsquo;t happen. This setting is just a default, if nothing is defined in the behavior section of an HPA resource. The default for the upscale stabilization is 0s and it cannot be set via a KCM option (<a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-autoscaling/853-configurable-hpa-scale-velocity/README.md>downscale stabilization was historically more important than upscale stabilization and when later the behavior sections were added to the HPA resources, upscale stabilization remained missing from the KCM options</a>).</li><li><code>tolerance</code> (default +/-10%): HPA will not scale out or in if the desired replica count is (mathematically as a float) near the actual replica count (see <a href=https://github.com/kubernetes/kubernetes/blob/f3f5dd99ac7bdc61c61c3d587575090c3473ab5a/pkg/controller/podautoscaler/replica_calculator.go#L97-L103>source code</a> for details), which is a form of hysteresis to avoid replica flapping around a threshold.</li></ul><p>There are a few more configurable options of lesser interest:</p><ul><li><p><code>syncPeriod</code> (default 15s): How often HPA retrieves the pods and metrics respectively how often it recomputes and sets the desired replica count.</p></li><li><p><code>cpuInitializationPeriod</code> (default 30s) and <code>initialReadinessDelay</code> (default 5m): Both settings only affect whether or not CPU metrics are considered for scaling decisions. They can be easily misinterpreted as the <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details>official docs</a> are <a href=https://github.com/kubernetes/website/issues/12657>somewhat hard to read</a> (see <a href=https://github.com/kubernetes/kubernetes/blob/f3f5dd99ac7bdc61c61c3d587575090c3473ab5a/pkg/controller/podautoscaler/replica_calculator.go#L399-L418>source code</a> for details, which is more readable, if you ignore the comments). Normally, you have little reason to modify them, but here is what they do:</p><ul><li><code>cpuInitializationPeriod</code>: Defines a grace period after a pod starts during which HPA won&rsquo;t consider CPU metrics of the pod for scaling if the pod is either not ready <strong>or</strong> it is ready, but a given CPU metric is older than the last state transition (to ready). This is to ignore CPU metrics that predate the current readiness while still in initialization to not make scaling decisions based on potentially misleading data. If the pod is ready and a CPU metric was collected after it became ready, it is considered also within this grace period.</li><li><code>initialReadinessDelay</code>: Defines another grace period after a pod starts during which HPA won&rsquo;t consider CPU metrics of the pod for scaling if the pod is not ready <strong>and</strong> it became not ready within this grace period (the docs/comments want to check whether the pod was ever ready, but the <a href=https://github.com/kubernetes/kubernetes/blob/f3f5dd99ac7bdc61c61c3d587575090c3473ab5a/pkg/controller/podautoscaler/replica_calculator.go#L411>code only checks whether the pod condition last transition time to not ready happened within that grace period</a> which it could have <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions>from being ready or simply unknown before</a>). This is to ignore not (ever have been) ready pods while still in initialization to not make scaling decisions based on potentially misleading data. If the pod is ready, it is considered also within this grace period.</li></ul><p>So, regardless of the values of these settings, if a pod is reporting ready and it has a CPU metric from the time after it became ready, that pod and its metric will be considered. This holds true even if the pod becomes ready very early into its initialization. These settings cannot be used to &ldquo;black-out&rdquo; pods for a certain duration before being considered for scaling decisions. Instead, if it is your goal to ignore a potentially resource-intensive initialization phase that could wrongly lead to further scale-out, you would need to configure your pods to not report as ready until that resource-intensive initialization phase is over.</p></li></ul><h2 id=considerations-when-using-hpa>Considerations When Using HPA</h2><ul><li><strong>Selection of metrics:</strong> Besides CPU and memory, HPA can also target custom or external metrics. Pick those (in addition or exclusively), if you guarantee certain SLOs in your SLAs.</li><li><strong>Targeting usage or utilization:</strong> HPA supports usage (absolute) and utilization (relative). Utilization is often preferred in simple examples, but usage is more precise and versatile.</li><li><strong>Compatibility with VPA:</strong> Care must be taken when using HPA in conjunction with VPA, as they can potentially interfere with each other&rsquo;s scaling decisions.</li></ul><h1 id=vertical-pod-autoscaler-vpa>Vertical Pod Autoscaler (VPA)</h1><p>VPA operates by monitoring resource metrics for all pods in a target. It computes a resource requests recommendation from the historic and current resource metrics. VPA checks the metrics at regular intervals, which can be configured by the user. Only CPU and memory are supported. If VPA detects that a pod&rsquo;s resource allocation is too high or too low, it may evict pods (if within the permitted disruption budget), which will trigger the creation of a new pod by the corresponding higher-level replication controller, which will then be mutated by VPA to match resource requests recommendation. This happens in three different components that work together:</p><ul><li><strong>VPA Recommender:</strong> The Recommender observes the historic and current resource metrics of pods and generates recommendations based on this data.</li><li><strong>VPA Updater:</strong> The Updater component checks the recommendations from the Recommender and decides whether any pod&rsquo;s resource requests need to be updated. If an update is needed, the Updater will evict the pod.</li><li><strong>VPA Admission Controller:</strong> When a pod is (re-)created, the Admission Controller modifies the pod&rsquo;s resource requests based on the recommendations from the Recommender. This ensures that the pod starts with the optimal amount of resources.</li></ul><p>Since VPA doesn&rsquo;t support in-place updates, pods will be evicted. You will want to control voluntary evictions by means of <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb>Pod Disruption Budgets (PDBs)</a>. Please make yourself familiar with those and use them.</p><blockquote><p>[!NOTE]
PDBs will not always work as expected and can also get in your way, e.g. if the PDB is violated or would be violated, it may possibly block evictions that would actually help your workload, e.g. to get a pod out of an <code>OOMKilled</code> <code>CrashLoopBackoff</code> (if the PDB is or would be violated, not even unhealthy pods would be evicted as they could theoretically become healthy again, which VPA doesn&rsquo;t know). In order to overcome this issue, it is now possible (alpha since Kubernetes <code>v1.26</code> in combination with the feature gate <code>PDBUnhealthyPodEvictionPolicy</code> on the API server, beta and enabled by default since Kubernetes <code>v1.27</code>) to configure the so-called <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy>unhealthy pod eviction policy</a>. The default is still <code>IfHealthyBudget</code> as a change in default would have changed the behavior (as described above), but you can now also set <code>AlwaysAllow</code> at the PDB (<code>spec.unhealthyPodEvictionPolicy</code>). For more information, please check out <a href=https://github.com/kubernetes/kubernetes/issues/72320>this discussion</a>, <a href=https://github.com/kubernetes/kubernetes/pull/105296>the PR</a> and <a href="https://groups.google.com/g/kubernetes-sig-apps/c/_joO4swogKY?pli=1">this document</a> and balance the pros and cons for yourself. In short, the new <code>AlwaysAllow</code> option is probably the better choice in most of the cases while <code>IfHealthyBudget</code> is useful only if you have frequent temporary transitions or for special cases where you have already implemented controllers that depend on the old behavior.</p></blockquote><h2 id=defining-a-vpa-resource>Defining a VPA Resource</h2><p>To configure VPA, you need to create a VPA resource in your cluster. This resource specifies the target to scale, the metrics to be used for scaling decisions, and the policies for resource updates. Here&rsquo;s an example of an VPA configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: autoscaling.k8s.io/v1
</span></span><span style=display:flex><span>kind: VerticalPodAutoscaler
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: foo-vpa
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  targetRef:
</span></span><span style=display:flex><span>    apiVersion: <span style=color:#a31515>&#34;apps/v1&#34;</span>
</span></span><span style=display:flex><span>    kind:       Deployment
</span></span><span style=display:flex><span>    name:       foo-deployment
</span></span><span style=display:flex><span>  updatePolicy:
</span></span><span style=display:flex><span>    updateMode: <span style=color:#a31515>&#34;Auto&#34;</span>
</span></span><span style=display:flex><span>  resourcePolicy:
</span></span><span style=display:flex><span>    containerPolicies:
</span></span><span style=display:flex><span>    - containerName: foo-container
</span></span><span style=display:flex><span>      controlledValues: RequestsOnly
</span></span><span style=display:flex><span>      minAllowed:
</span></span><span style=display:flex><span>        cpu: 50m
</span></span><span style=display:flex><span>        memory: 200M
</span></span><span style=display:flex><span>      maxAllowed:
</span></span><span style=display:flex><span>        cpu: 4
</span></span><span style=display:flex><span>        memory: 16G
</span></span></code></pre></div><p>In this example, VPA is configured to scale <code>foo-deployment</code> requests (<code>RequestsOnly</code>) from 50m cores (<code>minAllowed</code>) up to 4 cores (<code>maxAllowed</code>) and 200M memory (<code>minAllowed</code>) up to 16G memory (<code>maxAllowed</code>) automatically (<code>updateMode</code>). VPA doesn&rsquo;t support in-place updates, so in <code>updateMode</code> <code>Auto</code> it will evict pods under certain conditions and then mutate the requests (and possibly limits if you omit <code>controlledValues</code> or set it to <code>RequestsAndLimits</code>, which is the default) of upcoming new pods.</p><p><a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md#quick-start>Multiple update modes exist</a>. They influence eviction and mutation. The most important ones are:</p><ul><li><code>Off</code>: In this mode, recommendations are computed, but never applied. This mode is useful, if you want to learn more about your workload or if you have a custom controller that depends on VPA&rsquo;s recommendations but shall act instead of VPA.</li><li><code>Initial</code>: In this mode, recommendations are computed and applied, but pods are never proactively evicted to enforce new recommendations over time. This mode is useful, if you want to control pod evictions yourself (similar to the <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies><code>StatefulSet</code> <code>updateStrategy</code> <code>OnDelete</code></a>) or your workload is sensitive to evictions, e.g. some brownfield singleton application or a daemon set pod that is critical for the node.</li><li><code>Auto</code> (default): In this mode, recommendations are computed, applied, and pods are even proactively evicted to enforce new recommendations over time. This applies recommendations continuously without you having to worry too much.</li></ul><p>As mentioned, <code>controlledValues</code> influences whether only requests or requests and limits are scaled:</p><ul><li><code>RequestsOnly</code>: Updates only requests and doesn&rsquo;t change limits. Useful if you have defined absolute limits (unrelated to the requests).</li><li><code>RequestsAndLimits</code> (default): Updates requests and proportionally scales limits along with the requests. Useful if you have defined relative limits (related to the requests). In this case, the gap between requests and limits should be either zero for QoS <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#guaranteed><code>Guaranteed</code></a> or small for QoS <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#burstable><code>Burstable</code></a> to avoid useless (way beyond the threshold of unhealthy behavior) or absurd (larger than node capacity) values.</li></ul><p>VPA doesn&rsquo;t offer many more settings that can be tuned per VPA resource than you see above (different than HPA&rsquo;s <code>behavior</code> section). However, there is one more that isn&rsquo;t shown above, which allows to <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md#controlling-eviction-behavior-based-on-scaling-direction-and-resource>scale only up or only down (<code>evictionRequirements[].changeRequirement</code>)</a>, in case you need that, e.g. to provide resources when needed, but avoid disruptions otherwise.</p><h2 id=vpa-options>VPA Options</h2><p>VPA is an independent community project that <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md#components-of-vpa>consists of</a> a recommender (computing target recommendations and bounds), an updater (evicting pods that are out of recommendation bounds), and an admission controller (mutating webhook applying the target recommendation to newly created pods). As such, they have independent options.</p><h3 id=vpa-recommender-options>VPA Recommender Options</h3><p>You can read up the <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/FAQ.md#what-are-the-parameters-to-vpa-recommender>full VPA recommender options</a> online and set some of them conveniently in your <a href=https://github.com/gardener/gardener/blob/957e4c7/example/90-shoot.yaml#L298-L307>Gardener shoot cluster spec</a>:</p><ul><li><code>recommendationMarginFraction</code> (default 15%): Safety margin that will be added to the recommended requests.</li><li><code>targetCPUPercentile</code> (default 90%): CPU usage percentile that will be targeted with the CPU recommendation (i.e. recommendation will &ldquo;fit&rdquo; e.g. 90% of the observed CPU usages). This setting is relevant for balancing your requests reservations vs. your costs. If you want to reduce costs, you can reduce this value (higher risk because of potential under-reservation, but lower costs), because CPU is compressible, but then VPA may lack the necessary signals for scale-up as throttling on an otherwise fully utilized node will go unnoticed by VPA. If you want to err on the safe side, you can increase this value, but you will then target more and more a worst case scenario, quickly (maybe even exponentially) increasing the costs.</li><li><code>targetMemoryPercentile</code> (default 90%): Memory usage percentile that will be targeted with the memory recommendation (i.e. recommendation will &ldquo;fit&rdquo; e.g. 90% of the observed memory usages). This setting is relevant for balancing your requests reservations vs. your costs. If you want to reduce costs, you can reduce this value (higher risk because of potential under-reservation, but lower costs), because OOMs will trigger bump-ups, but those will disrupt the workload. If you want to err on the safe side, you can increase this value, but you will then target more and more a worst case scenario, quickly (maybe even exponentially) increasing the costs.</li></ul><p>There are a few more configurable options of lesser interest:</p><ul><li><code>recommenderInterval</code> (default 1m): How often VPA retrieves the pods and metrics respectively how often it recomputes the recommendations and bounds.</li></ul><p>There are many more options that you can only configure if you deploy your own VPA and which we will not discuss here, but you can check them out <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/FAQ.md#what-are-the-parameters-to-vpa-recommender>here</a>.</p><blockquote><p>[!NOTE]
Due to an implementation detail (smallest bucket size), VPA cannot create recommendations below <a href=https://github.com/kubernetes/autoscaler/blob/1f89ff92cf87dd3700f74f9b387ae4846aa51846/vertical-pod-autoscaler/pkg/recommender/model/aggregations_config.go#L89-L99>10m cores</a> and <a href=https://github.com/kubernetes/autoscaler/blob/1f89ff92cf87dd3700f74f9b387ae4846aa51846/vertical-pod-autoscaler/pkg/recommender/model/aggregations_config.go#L101-L111>10M memory</a> even if <code>minAllowed</code> is lower.</p></blockquote><h3 id=vpa-updater-options>VPA Updater Options</h3><p>You can read up the <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/FAQ.md#what-are-the-parameters-to-vpa-updater>full VPA updater options</a> online and set some of them conveniently in your <a href=https://github.com/gardener/gardener/blob/957e4c7/example/90-shoot.yaml#L298-L307>Gardener shoot cluster spec</a>:</p><ul><li><code>evictAfterOOMThreshold</code> (default 10m): Pods where at least one container OOMs within this time period since its start will be actively evicted, which will implicitly apply the new target recommendation that will have been <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md#custom-memory-bump-up-after-oomkill>bumped up after <code>OOMKill</code></a>. Please note, the kubelet may evict pods even before an OOM, but only if <code>kube-reserved</code> is underrun, i.e. <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#pod-selection-for-kubelet-eviction>node-level resources are running low</a>. In these cases, eviction will happen first by pod priority and second by how much the usage overruns the requests.</li><li><code>evictionTolerance</code> (default 50%): Defines a threshold below which no further eligible pod will be evited anymore, i.e. limits how many eligible pods may be in eviction in parallel (but at least 1). The <a href=https://github.com/kubernetes/autoscaler/blob/4d0511363eeeff657119797dd8d26e851dcc3459/vertical-pod-autoscaler/pkg/updater/eviction/pods_eviction_restriction.go#L108-L117>threshold is computed as follows</a>: <code>running - evicted > replicas - tolerance</code>. Example: 10 replicas, 9 running, 8 eligible for eviction, 20% tolerance with 10 replicas which amounts to 2 pods, and no pod evicted in this round yet, then <code>9 - 0 > 10 - 2</code> is true and a pod would be evicted, but the next one would be in violation as <code>9 - 1 = 10 - 2</code> and no further pod would be evicted anymore in this round.</li><li><code>evictionRateBurst</code> (default 1): Defines how many eligible pods may be evicted in one go.</li><li><code>evictionRateLimit</code> (default disabled): Defines how many eligible pods may be evicted per second (a value of 0 or -1 disables the rate limiting).</li></ul><p>In general, avoid modifying these eviction settings unless you have good reasons and try to rely on <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb>Pod Disruption Budgets (PDBs)</a> instead. However, <a href=https://github.com/kubernetes/kubernetes/issues/108124>PDBs are not available for daemon sets</a>.</p><p>There are a few more configurable options of lesser interest:</p><ul><li><code>updaterInterval</code> (default 1m): How often VPA evicts the pods.</li></ul><p>There are many more options that you can only configure if you deploy your own VPA and which we will not discuss here, but you can check them out <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/FAQ.md#what-are-the-parameters-to-vpa-updater>here</a>.</p><h2 id=considerations-when-using-vpa>Considerations When Using VPA</h2><ul><li><strong>Initial Resource Estimates:</strong> VPA requires historical resource usage data to base its recommendations on. Until they kick in, your initial resource requests apply and should be sensible.</li><li><strong>Pod Disruption:</strong> When VPA adjusts the resources for a pod, it may need to &ldquo;recreate&rdquo; the pod, which can cause temporary disruptions. This should be taken into account.</li><li><strong>Compatibility with HPA:</strong> Care must be taken when using VPA in conjunction with HPA, as they can potentially interfere with each other&rsquo;s scaling decisions.</li></ul><h1 id=combining-hpa-and-vpa>Combining HPA and VPA</h1><p>HPA and VPA serve different purposes and operate on different axes of scaling. HPA increases or decreases the number of pod replicas based on metrics like CPU or memory usage, effectively scaling the application out or in. VPA, on the other hand, adjusts the CPU and memory reservations of individual pods, scaling the application up or down.</p><p>When used together, these autoscalers can provide both horizontal and vertical scaling. However, they can also conflict with each other if used on the same metrics (e.g. both on CPU or both on memory). In particular, if VPA adjusts the requests, the utilization, i.e. the ratio between usage and requests, will approach 100% (for various reasons not exactly right, but for this consideration, close enough), which may trigger HPA to scale out, if it&rsquo;s configured to scale on utilization below 100% (often seen in simple examples), which will spread the load across more pods, which may trigger VPA again to adjust the requests to match the new pod usages.</p><p>This is a feedback loop and it stems from <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details>HPA&rsquo;s method of calculating the desired number of replicas</a>, which is:</p><p><code>desiredReplicas = ceil[currentReplicas * (currentMetricValue / desiredMetricValue)]</code></p><p>If <code>desiredMetricValue</code> is utilization and VPA adjusts the requests, which changes the utilization, this may inadvertently trigger HPA and create said feedback loop. On the other hand, if <code>desiredMetricValue</code> is usage and VPA adjusts the requests now, this will have no impact on HPA anymore (HPA will always influence VPA, but we can control whether VPA influences HPA).</p><p>Therefore, to safely combine HPA and VPA, consider the following strategies:</p><ul><li><strong>Configure HPA and VPA on different metrics:</strong> One way to avoid conflicts is to use HPA and VPA based on different metrics. For instance, you could configure HPA to scale based on requests per seconds (or another representative custom/external metric) and VPA to adjust CPU and memory requests. This way, each autoscaler operates independently based on its specific metric(s).</li><li><strong>Configure HPA to scale on usage, not utilization, when used with VPA:</strong> Another way to avoid conflicts is to use HPA not on average utilization (<code>averageUtilization</code>), but instead on average usage (<code>averageValue</code>) as replicas driver, which is an absolute metric (requests don&rsquo;t affect usage). This way, you can combine both autoscalers even on the same metrics.</li></ul><h1 id=pod-autoscaling-and-cluster-autoscaler>Pod Autoscaling and Cluster Autoscaler</h1><p>Autoscaling within Kubernetes can be implemented at different levels: pod autoscaling (HPA and VPA) and cluster autoscaling (CA). While pod autoscaling adjusts the number of pod replicas or their resource reservations, cluster autoscaling focuses on the number of nodes in the cluster, so that your pods can be hosted. If your workload isn&rsquo;t static and especially if you make use of pod autoscaling, it only works if you have sufficient node capacity available. The most effective way to do that, without running a worst-case number of nodes, is to configure burstable worker pools in your shoot spec, i.e. define a true minimum node count and a worst-case maximum node count and leave the node autoscaling to Gardener that internally uses the Cluster Autoscaler to provision and deprovision nodes as needed.</p><p>Cluster Autoscaler automatically adjusts the number of nodes by adding or removing nodes based on the demands of the workloads and the available resources. It interacts with the cloud provider&rsquo;s APIs to provision or deprovision nodes as needed. Cluster Autoscaler monitors the utilization of nodes and the scheduling of pods. If it detects that pods cannot be scheduled due to a lack of resources, it will trigger the addition of new nodes to the cluster. Conversely, if nodes are underutilized for some time and their pods can be placed on other nodes, it will remove those nodes to reduce costs and improve resource efficiency.</p><p>Best Practices:</p><ul><li><strong>Resource Buffering:</strong> Maintain a buffer of resources to accommodate temporary spikes in demand without waiting for node provisioning. This can be done by <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-can-i-configure-overprovisioning-with-cluster-autoscaler>deploying pods with low priority that can be preempted when real workloads require resources</a>. This helps in faster pod scheduling and avoids delays in scaling out or up.</li><li><strong>Pod Disruption Budgets (PDBs):</strong> Use <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb>PDBs</a> to ensure that during scale-down events, the availability of applications is maintained as the Cluster Autoscaler will not voluntarily evict a pod if a PDB would be violated.</li></ul><h2 id=interesting-ca-options>Interesting CA Options</h2><p>CA can be configured in your <a href=https://github.com/gardener/gardener/blob/957e4c7/example/90-shoot.yaml#L281-L297>Gardener shoot cluster spec globally</a> and also in parts <a href=https://github.com/gardener/gardener/blob/957e4c7/example/90-shoot.yaml#L48-L53>per worker pool</a>:</p><ul><li>Can only be configured globally:<ul><li><code>expander</code> (default least-waste): Defines the &ldquo;expander&rdquo; algorithm to use during scale-up, see <a href=https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders>FAQ</a>.</li><li><code>scaleDownDelayAfterAdd</code> (default 1h): Defines how long after scaling up a node, a node may be scaled down.</li><li><code>scaleDownDelayAfterFailure</code> (default 3m): Defines how long after scaling down a node failed, scaling down will be resumed.</li><li><code>scaleDownDelayAfterDelete</code> (default 0s): Defines how long after scaling down a node, another node may be scaled down.</li></ul></li><li>Can be configured globally and also overwritten individually per worker pool:<ul><li><code>scaleDownUtilizationThreshold</code> (default 50%): Defines the threshold below which a node becomes eligible for scaling down.</li><li><code>scaleDownUnneededTime</code> (default 30m): Defines the trailing time window the node must be consistently below a certain utilization threshold before it can finally be scaled down.</li></ul></li></ul><p>There are many more options that you can only configure if you deploy your own CA and which we will not discuss here, but you can check them out <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-are-the-parameters-to-ca>here</a>.</p><h1 id=importance-of-monitoring>Importance of Monitoring</h1><p>Monitoring is a critical component of autoscaling for several reasons:</p><ul><li><strong>Performance Insights:</strong> It provides insights into how well your autoscaling strategy is meeting the performance requirements of your applications.</li><li><strong>Resource Utilization:</strong> It helps you understand resource utilization patterns, enabling you to optimize resource allocation and reduce waste.</li><li><strong>Cost Management:</strong> It allows you to track the cost implications of scaling actions, helping you to maintain control over your cloud spending.</li><li><strong>Troubleshooting:</strong> It enables you to quickly identify and address issues with autoscaling, such as unexpected scaling behavior or resource bottlenecks.</li></ul><p>To effectively monitor autoscaling, you should leverage the following tools and metrics:</p><ul><li><strong><a href=https://sigs.k8s.io/metrics-server>Kubernetes Metrics Server</a>:</strong> Collects resource metrics from kubelets and provides them to HPA and VPA for autoscaling decisions (automatically provided by Gardener).</li><li><strong>Prometheus:</strong> An open-source monitoring system that can collect and store custom metrics, providing a rich dataset for autoscaling decisions.</li><li><strong>Grafana/Plutono:</strong> A visualization tool that integrates with Prometheus to create dashboards for monitoring autoscaling metrics and events.</li><li><strong>Cloud Provider Tools:</strong> Most cloud providers offer native monitoring solutions that can be used to track the performance and costs associated with autoscaling.</li></ul><p>Key metrics to monitor include:</p><ul><li><strong>CPU and Memory Utilization:</strong> Track the resource utilization of your pods and nodes to understand how they correlate with scaling events.</li><li><strong>Pod Count:</strong> Monitor the number of pod replicas over time to see how HPA is responding to changes in load.</li><li><strong>Scaling Events:</strong> Keep an eye on scaling events triggered by HPA and VPA to ensure they align with expected behavior.</li><li><strong>Application Performance Metrics:</strong> Track application-specific metrics such as response times, error rates, and throughput.</li></ul><p>Based on the insights gained from monitoring, you may need to adjust your autoscaling configurations:</p><ul><li><strong>Refine Thresholds:</strong> If you notice frequent scaling actions or periods of underutilization or overutilization, adjust the thresholds used by HPA and VPA to better match the workload patterns.</li><li><strong>Update Policies:</strong> Modify VPA update policies if you observe that the current settings are causing too much or too little pod disruption.</li><li><strong>Custom Metrics:</strong> If using custom metrics, ensure they accurately reflect the load on your application and adjust them if they do not.</li><li><strong>Scaling Limits:</strong> Review and adjust the minimum and maximum scaling limits to prevent over-scaling or under-scaling based on the capacity of your cluster and the criticality of your applications.</li></ul><h1 id=quality-of-service-qos>Quality of Service (QoS)</h1><p>A few words on the <a href=https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod>quality of service for pods</a>. Basically, there are 3 classes of QoS and they influence the eviction of pods when <code>kube-reserved</code> is underrun, i.e. <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#pod-selection-for-kubelet-eviction>node-level resources are running low</a>:</p><ul><li><a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#besteffort><code>BestEffort</code></a>, i.e. pods where no container has CPU or memory requests or limits: <strong>Avoid them</strong> unless you have really good reasons. The kube-scheduler will place them just anywhere according to its policy, e.g. <code>balanced</code> or <code>bin-packing</code>, but whatever resources these pods consume, may bring other pods into trouble or even the kubelet and the container runtime itself, if it happens very suddenly.</li><li><a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#burstable><code>Burstable</code></a>, i.e. pods where at least one container has CPU or memory requests and at least one has no limits or limits that don&rsquo;t match the requests: <strong>Prefer them</strong> unless you have really good reasons for the other QoS classes. Always specify proper requests or use VPA to recommend those. <a href=https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-requests-are-scheduled>This helps the kube-scheduler to make the right scheduling decisions</a>. Not having limits will additionally provide upward resource flexibility, if the node is not under pressure.</li><li><a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#guaranteed><code>Guaranteed</code></a>, i.e. pods where all containers have CPU and memory requests and equal limits: <strong>Avoid them</strong> unless you really know the limits or throttling/killing is intended. While &ldquo;Guaranteed&rdquo; sounds like something &ldquo;positive&rdquo; in the English language, this class comes with the downside, that pods will be actively CPU-throttled and will actively go OOM, even if the node is not under pressure and has excess capacity left. Worse, if containers in the pod are under VPA, their CPU requests/limits will often not be scaled up as CPU throttling will go unnoticed by VPA.</li></ul><h1 id=summary>Summary</h1><ul><li>As a rule of thumb, always set CPU and memory requests (or let VPA do that) and always avoid CPU and memory limits.<ul><li>CPU limits aren&rsquo;t helpful on an under-utilized node (=may result in needless outages) and even suppress the signals for VPA to act. On a nearly or fully utilized node, CPU limits are practically irrelevant as only the requests matter, which are translated into CPU shares that provide a fair use of the CPU anyway (see <a href=https://docs.kernel.org/scheduler/sched-design-CFS.html>CFS</a>).<br>Therefore, if you do not know the healthy range, do not set CPU limits. If you as author of the source code know its healthy range, set them to the upper threshold of that healthy range (everything above, from your knowledge of that code, is definitely an unbound busy loop or similar, which is the main reason for CPU limits, besides batch jobs where throttling is acceptable or even desired).</li><li>Memory limits may be more useful, but suffer a similar, though not as negative downside. As with CPU limits, memory limits aren&rsquo;t helpful on an under-utilized node (=may result in needless outages), but different than CPU limits, they result in an OOM, which triggers VPA to provide more memory suddenly (modifies the currently computed recommendations by a configurable factor, defaulting to +20%, see <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md#custom-memory-bump-up-after-oomkill>docs</a>).<br>Therefore, if you do not know the healthy range, do not set memory limits. If you as author of the source code know its healthy range, set them to the upper threshold of that healthy range (everything above, from your knowledge of that code, is definitely an unbound memory leak or similar, which is the main reason for memory limits)</li></ul></li><li>Horizontal Pod Autoscaling (HPA): Use for pods that support horizontal scaling. Prefer scaling on usage, not utilization, as this is more predictable (not dependent on a second variable, namely the current requests) and conflict-free with vertical pod autoscaling (VPA).</li><li>As a rule of thumb, set the initial replicas to the 5th percentile of the actually observed replica count in production. Since HPA reacts fast, this is not as critical, but may help reduce initial load on the control plane early after deployment. However, be cautious when you update the higher-level resource not to inadvertently reset the current HPA-controlled replica count (very easy to make mistake that can lead to catastrophic loss of pods). HPA modifies the replica count directly in the spec and you do not want to overwrite that. Even if it reacts fast, it is not instant (not via a mutating webhook as VPA operates) and the damage may already be done.</li><li>As for minimum and maximum, let your high availability requirements determine the minimum and your theoretical maximum load determine the maximum, flanked with alerts to detect erroneous run-away out-scaling or the actual nearing of your practical maximum load, so that you can intervene.</li><li>Vertical Pod Autoscaling (VPA): Use for containers that have a significant usage (e.g. any container above 50m CPU or 100M memory) and a significant usage spread over time (by more than 2x), i.e. ignore small (e.g. side-cars) or static (e.g. Java statically allocated heap) containers, but otherwise use it to provide the resources needed on the one hand and keep the costs in check on the other hand.</li><li>As a rule of thumb, set the initial requests to the 5th percentile of the actually observed CPU resp. memory usage in production. Since VPA may need some time at first to respond and evict pods, this is especially critical early after deployment. The lower bound, below which pods will be immediately evicted, converges much faster than the upper bound, above which pods will be immediately evicted, but it isn&rsquo;t instant, e.g. after 5 minutes the lower bound is just at 60% of the computed lower bound; after 12 hours the upper bound is still at 300% of the computed upper bound (see <a href=https://github.com/kubernetes/autoscaler/blob/b3a501cbe11e46bea1f8879d39c8436ef03e7139/vertical-pod-autoscaler/pkg/recommender/logic/recommender.go#L118-L143>code</a>). Unlike with HPA, you don&rsquo;t need to be as cautious when updating the higher-level resource in the case of VPA. As long as VPA&rsquo;s mutating webhook (VPA Admission Controller) is operational (which also the VPA Updater checks before evicting pods), it&rsquo;s generally safe to update the higher-level resource. However, if it&rsquo;s not up and running, any new pods that are spawned (e.g. as a consequence of a rolling update of the higher-level resource or for any other reason) will not be mutated. Instead, they will receive whatever requests are currently configured at the higher-level resource, which can lead to catastrophic resource under-reservation. Gardener deploys the VPA Admission Controller in HA - if unhealthy, it is reported under the <code>ControlPlaneHealthy</code> shoot status condition.</li><li>If you have defined absolute limits (unrelated to the requests), configure VPA to only scale the requests or else it will proportionally scale the limits as well, which can easily become useless (way beyond the threshold of unhealthy behavior) or absurd (larger than node capacity):<pre tabindex=0><code>spec:
  resourcePolicy:
    containerPolicies:
    - controlledValues: RequestsOnly
      ...
</code></pre>If you have defined relative limits (related to the requests), the default policy to scale the limits proportionally with the requests is fine, but the gap between requests and limits must be zero for QoS <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#guaranteed><code>Guaranteed</code></a> and should best be small for QoS <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#burstable><code>Burstable</code></a> to avoid useless or absurd limits either, e.g. prefer limits being 5 to at most 20% larger than requests as opposed to being 100% larger or more.</li><li>As a rule of thumb, set <code>minAllowed</code> to the highest observed VPA recommendation (usually during the initialization phase or during any periodical activity) for an otherwise practically idle container, so that you avoid needless trashing (e.g. resource usage calms down over time and recommendations drop consecutively until eviction, which will then lead again to initialization or later periodical activity and higher recommendations and new evictions).<br>⚠️ You may want to provide higher <code>minAllowed</code> values, if you observe that up-scaling takes too long for CPU or memory for a too large percentile of your workload. This will get you out of the danger zone of too few resources for too many pods at the expense of providing too many resources for a few pods. Memory may react faster than CPU, because CPU throttling is not visible and memory gets aided by OOM bump-up incidents, but still, if you observe that up-scaling takes too long, you may want to increase <code>minAllowed</code> accordingly.</li><li>As a rule of thumb, set <code>maxAllowed</code> to your theoretical maximum load, flanked with alerts to detect erroneous run-away usage or the actual nearing of your practical maximum load, so that you can intervene. However, VPA can easily recommend requests larger than what is allocatable on a node, so you must either ensure large enough nodes (Gardener can scale up from zero, in case you like to <a href=https://gardener.cloud/docs/getting-started/features/cluster-autoscaler/#scaling-by-priority>define a low-priority worker pool with more resources</a> for very large pods) and/or cap VPA&rsquo;s target recommendations using <code>maxAllowed</code> at the node allocatable remainder (after daemon set pods) of the largest eligible machine type (may result in under-provisioning resources for a pod). Use your monitoring and check maximum pod usage to decide about the maximum machine type.</li></ul><h2 id=recommendations-in-a-box>Recommendations in a Box</h2><table><thead><tr><th>Container</th><th>When to use</th><th>Value</th></tr></thead><tbody><tr><td>Requests</td><td>- <strong>Set them (recommended)</strong> unless:<br>- Do not set requests for QoS <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#besteffort><code>BestEffort</code></a>; useful only if pod can be evicted as often as needed <strong>and</strong> pod can pick up where it left off without any penalty</td><td>Set <code>requests</code> to <strong>95th percentile (w/o VPA)</strong> of the actually observed CPU resp. memory usage in production resp. <strong>5th percentile (w/ VPA)</strong> (see below)</td></tr><tr><td>Limits</td><td>- <strong>Avoid them (recommended)</strong> unless:<br>- Set limits for QoS <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#guaranteed><code>Guaranteed</code></a>; useful only if pod has strictly static resource requirements<br>- Set CPU limits if you want to throttle CPU usage for containers that can be throttled w/o any other disadvantage than processing time (never do that when time-critical operations like leases are involved)<br>- Set limits if you know the healthy range and want to shield against unbound busy loops, unbound memory leaks, or similar</td><td>If you really can (otherwise not), set <code>limits</code> to healthy theoretical max load</td></tr></tbody></table><table><thead><tr><th>Scaler</th><th>When to use</th><th>Initial</th><th>Minimum</th><th>Maximum</th></tr></thead><tbody><tr><td>HPA</td><td><strong>Use for pods that support horizontal scaling</strong></td><td>Set initial <code>replicas</code> to 5th percentile of the actually observed replica count in production (prefer scaling on usage, not utilization) and make sure to never overwrite it later when controlled by HPA</td><td>Set <code>minReplicas</code> to 0 (requires <a href=https://github.com/kubernetes/kubernetes/blob/4c7960a67c29b7954cccc6c7d77a62133af3484f/pkg/features/kube_features.go#L266-L267>feature gate</a> and <a href=https://github.com/kubernetes/kubernetes/pull/74526>custom/external metrics</a>), to 1 (regular HPA minimum), or whatever the high availability requirements of the workload demand</td><td>Set <code>maxReplicas</code> to healthy theoretical max load</td></tr><tr><td>VPA</td><td><strong>Use for containers that have a significant usage</strong> (>50m/100M) <strong>and a significant usage spread over time</strong> (>2x)</td><td>Set initial <code>requests</code> to 5th percentile of the actually observed CPU resp. memory usage in production</td><td>Set <code>minAllowed</code> to highest observed VPA recommendation (includes start-up phase) for an otherwise practically idle container (avoids pod trashing when pod gets evicted after idling)</td><td>Set <code>maxAllowed</code> to fresh node allocatable remainder after daemonset pods (avoids pending pods when requests exeed fresh node allocatable remainder) or, if you really can (otherwise not), to healthy theoretical max load (less disruptive than limits as no throttling or OOM happens on under-utilized nodes)</td></tr><tr><td>CA</td><td><strong>Use for dynamic workloads</strong>, definitely if you use HPA and/or VPA</td><td>N/A</td><td>Set <code>minimum</code> to 0 or number of nodes required right after cluster creation or wake-up</td><td>Set <code>maximum</code> to healthy theoretical max load</td></tr></tbody></table><blockquote><p>[!NOTE]
Theoretical max load may be very difficult to ascertain, especially with modern software that consists of building blocks you do not own or know in detail. If you have comprehensive monitoring in place, you may be tempted to pick the observed maximum and add a safety margin or even factor on top (2x, 4x, or any other number), but this is not to be confused with &ldquo;theoretical max load&rdquo; (solely depending on the code, not observations from the outside). At any point in time, your numbers may change, e.g. because you updated a software component or your usage increased. If you decide to use numbers that are set based only on observations, make sure to flank those numbers with monitoring alerts, so that you have sufficient time to investigate, revise, and readjust if necessary.</p></blockquote><h1 id=conclusion>Conclusion</h1><p>Pod autoscaling is a dynamic and complex aspect of Kubernetes, but it is also one of the most powerful tools at your disposal for maintaining efficient, reliable, and cost-effective applications. By carefully selecting the appropriate autoscaler, setting well-considered thresholds, and continuously monitoring and adjusting your strategies, you can ensure that your Kubernetes deployments are well-equipped to handle your resource demands while not over-paying for the provided resources at the same time.</p><p>As Kubernetes continues to evolve (e.g. <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/1287-in-place-update-pod-resources/README.md>in-place updates</a>) and as new patterns and practices emerge, the approaches to autoscaling may also change. However, the principles discussed above will remain foundational to creating scalable and resilient Kubernetes workloads. Whether you&rsquo;re a developer or operations engineer, a solid understanding of pod autoscaling will be instrumental in the successful deployment and management of containerized applications.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e90d19652253417e1b6dfc323d5a446d>17 - Using Prometheus and Grafana to Monitor K8s</h1><div class=lead>How to deploy and configure Prometheus and Grafana to collect and monitor kubelet container metrics</div><h2 id=disclaimer>Disclaimer</h2><p>This post is meant to give a basic end-to-end description for deploying and using Prometheus and Grafana. Both applications offer a wide range of flexibility, which needs to be considered in case you have specific requirements. Such advanced details are not in the scope of this topic.</p><h2 id=introduction>Introduction</h2><p><a href=https://prometheus.io/>Prometheus</a> is an open-source systems monitoring and alerting toolkit for recording numeric time series. It fits both machine-centric monitoring as well as monitoring of highly dynamic service-oriented architectures. In a world of microservices, its support for multi-dimensional data collection and querying is a particular strength.</p><p>Prometheus is the second hosted project to <a href=https://prometheus.io/blog/2018/08/09/prometheus-graduates-within-cncf/>graduate within CNCF</a>.</p><p>The following characteristics make Prometheus a good match for monitoring Kubernetes clusters:</p><ul><li><p>Pull-based Monitoring
Prometheus is a <a href=https://prometheus.io/blog/2016/07/23/pull-does-not-scale-or-does-it/>pull-based</a> monitoring system, which means that the Prometheus server dynamically discovers and pulls metrics from your services running in Kubernetes.</p></li><li><p>Labels
Prometheus and Kubernetes share the same label (key-value) concept that can be used to select objects in the system.<br>Labels are used to identify time series and sets of label matchers can be used in the query language (<a href=https://prometheus.io/docs/prometheus/latest/querying/basics/>PromQL</a>) to select the time series to be aggregated.</p></li><li><p>Exporters<br>There are many <a href=https://prometheus.io/docs/instrumenting/exporters/>exporters</a> available, which enable integration of databases or even other monitoring systems not already providing a way to export metrics to Prometheus. One prominent exporter is the so called <a href=https://github.com/prometheus/node_exporter>node-exporter</a>, which allows to monitor hardware and OS related metrics of Unix systems.</p></li><li><p>Powerful Query Language
The Prometheus query language <a href=https://prometheus.io/docs/prometheus/latest/querying/basics/>PromQL</a> lets the user select and aggregate time series data in real time. Results can either be shown as a graph, viewed as tabular data in the Prometheus expression browser, or consumed by external systems via the <a href=https://prometheus.io/docs/prometheus/latest/querying/api/>HTTP API</a>.</p></li></ul><p>Find query examples on <a href=https://github.com/infinityworks/prometheus-example-queries/blob/master/README.md>Prometheus Query Examples</a>.</p><p>One very popular open-source visualization tool not only for Prometheus is <a href=https://grafana.com>Grafana</a>. Grafana is a metric analytics and visualization suite. It is popular for visualizing time series data for infrastructure and application analytics but many use it in other domains including industrial sensors, home automation, weather, and process control. For more information, see the <a href=http://docs.grafana.org/>Grafana Documentation</a>.</p><p>Grafana accesses data via <a href=https://grafana.com/docs/grafana/latest/basics/>Data Sources</a>. The continuously growing list of supported backends includes Prometheus.</p><p>Dashboards are created by combining panels, e.g., <a href=http://docs.grafana.org/reference/graph/>Graph</a> and <a href=http://docs.grafana.org/reference/dashlist/>Dashlist</a>.</p><p>In this example, we describe an End-To-End scenario including the deployment of Prometheus and a basic monitoring configuration as the one provided for Kubernetes clusters created by Gardener.</p><p>If you miss elements on the Prometheus web page when accessing it via its service URL <code>https://&lt;your K8s FQN>/api/v1/namespaces/&lt;your-prometheus-namespace>/services/prometheus-prometheus-server:80/proxy</code>, this is probably caused by a Prometheus issue - <a href=https://github.com/prometheus/prometheus/issues/1583>#1583</a>. To workaround this issue, set up a port forward <code>kubectl port-forward -n &lt;your-prometheus-namespace> &lt;prometheus-pod> 9090:9090</code> on your client and access the Prometheus UI from there with your locally installed web browser. This issue is not relevant in case you use the service type <code>LoadBalancer</code>.</p><h2 id=preparation>Preparation</h2><p>The deployment of <a href=https://github.com/kubernetes/charts/tree/master/stable/prometheus>Prometheus</a> and <a href=https://github.com/kubernetes/charts/tree/master/stable/grafana>Grafana</a> is based on Helm charts.<br>Make sure to implement the Helm settings before deploying the Helm charts.</p><p>The Kubernetes clusters provided by <a href=https://github.com/gardener>Gardener</a> use role based access control (<a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac/>RBAC</a>). To authorize the Prometheus node-exporter to access hardware and OS relevant metrics of your cluster&rsquo;s worker nodes, specific artifacts need to be deployed.</p><p>Bind the Prometheus service account to the <code>garden.sapcloud.io:monitoring:prometheus</code> cluster role by running the command
<code>kubectl apply -f crbinding.yaml</code>.</p><p>Content of <code>crbinding.yaml</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: rbac.authorization.k8s.io/v1beta1
</span></span><span style=display:flex><span>kind: ClusterRoleBinding
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: &lt;your-prometheus-name&gt;-server
</span></span><span style=display:flex><span>roleRef:
</span></span><span style=display:flex><span>  apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>  kind: ClusterRole
</span></span><span style=display:flex><span>  name: garden.sapcloud.io:monitoring:prometheus
</span></span><span style=display:flex><span>subjects:
</span></span><span style=display:flex><span>- kind: ServiceAccount
</span></span><span style=display:flex><span>  name: &lt;your-prometheus-name&gt;-server
</span></span><span style=display:flex><span>  namespace: &lt;your-prometheus-namespace&gt;
</span></span></code></pre></div><h2 id=deployment-of-prometheus-and-grafana>Deployment of Prometheus and Grafana</h2><p>Only minor changes are needed to deploy <a href=https://github.com/kubernetes/charts/tree/master/stable/prometheus>Prometheus</a> and <a href=https://github.com/kubernetes/charts/tree/master/stable/grafana>Grafana</a> based on Helm charts.</p><p>Copy the following configuration into a file called <code>values.yaml</code> and deploy Prometheus:
<code>helm install &lt;your-prometheus-name> --namespace &lt;your-prometheus-namespace> stable/prometheus -f values.yaml</code></p><p>Typically, Prometheus and Grafana are deployed into the same namespace. There is no technical reason behind this, so feel free to choose different namespaces.</p><p>Content of <code>values.yaml</code> for Prometheus:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>rbac:
</span></span><span style=display:flex><span>  create: <span style=color:#00f>false</span> <span style=color:green># Already created in Preparation step</span>
</span></span><span style=display:flex><span>nodeExporter:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>false</span> <span style=color:green># The node-exporter is already deployed by default</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>server:
</span></span><span style=display:flex><span>  global:
</span></span><span style=display:flex><span>    scrape_interval: 30s
</span></span><span style=display:flex><span>    scrape_timeout: 30s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>serverFiles:
</span></span><span style=display:flex><span>  prometheus.yml:
</span></span><span style=display:flex><span>    rule_files:
</span></span><span style=display:flex><span>      - /etc/config/rules
</span></span><span style=display:flex><span>      - /etc/config/alerts      
</span></span><span style=display:flex><span>    scrape_configs:
</span></span><span style=display:flex><span>    - job_name: <span style=color:#a31515>&#39;kube-kubelet&#39;</span>
</span></span><span style=display:flex><span>      honor_labels: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      scheme: https
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      tls_config:
</span></span><span style=display:flex><span>      <span style=color:green># This is needed because the kubelets&#39; certificates are not generated</span>
</span></span><span style=display:flex><span>      <span style=color:green># for a specific pod IP</span>
</span></span><span style=display:flex><span>        insecure_skip_verify: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      kubernetes_sd_configs:
</span></span><span style=display:flex><span>      - role: node
</span></span><span style=display:flex><span>      relabel_configs:
</span></span><span style=display:flex><span>      - target_label: __metrics_path__
</span></span><span style=display:flex><span>        replacement: /metrics
</span></span><span style=display:flex><span>      - source_labels: [__meta_kubernetes_node_address_InternalIP]
</span></span><span style=display:flex><span>        target_label: instance
</span></span><span style=display:flex><span>      - action: labelmap
</span></span><span style=display:flex><span>        regex: __meta_kubernetes_node_label_(.+)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    - job_name: <span style=color:#a31515>&#39;kube-kubelet-cadvisor&#39;</span>
</span></span><span style=display:flex><span>      honor_labels: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      scheme: https
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      tls_config:
</span></span><span style=display:flex><span>      <span style=color:green># This is needed because the kubelets&#39; certificates are not generated</span>
</span></span><span style=display:flex><span>      <span style=color:green># for a specific pod IP</span>
</span></span><span style=display:flex><span>        insecure_skip_verify: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      kubernetes_sd_configs:
</span></span><span style=display:flex><span>      - role: node
</span></span><span style=display:flex><span>      relabel_configs:
</span></span><span style=display:flex><span>      - target_label: __metrics_path__
</span></span><span style=display:flex><span>        replacement: /metrics/cadvisor
</span></span><span style=display:flex><span>      - source_labels: [__meta_kubernetes_node_address_InternalIP]
</span></span><span style=display:flex><span>        target_label: instance
</span></span><span style=display:flex><span>      - action: labelmap
</span></span><span style=display:flex><span>        regex: __meta_kubernetes_node_label_(.+)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:green># Example scrape config for probing services via the Blackbox Exporter.</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># Relabelling allows to configure the actual service scrape endpoint using the following annotations:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/probe`: Only probe services that have a value of `true`</span>
</span></span><span style=display:flex><span>    - job_name: <span style=color:#a31515>&#39;kubernetes-services&#39;</span>
</span></span><span style=display:flex><span>      metrics_path: /probe
</span></span><span style=display:flex><span>      params:
</span></span><span style=display:flex><span>        module: [http_2xx]
</span></span><span style=display:flex><span>      kubernetes_sd_configs:
</span></span><span style=display:flex><span>        - role: service
</span></span><span style=display:flex><span>      relabel_configs:
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
</span></span><span style=display:flex><span>          action: keep
</span></span><span style=display:flex><span>          regex: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        - source_labels: [__address__]
</span></span><span style=display:flex><span>          target_label: __param_target
</span></span><span style=display:flex><span>        - target_label: __address__
</span></span><span style=display:flex><span>          replacement: blackbox
</span></span><span style=display:flex><span>        - source_labels: [__param_target]
</span></span><span style=display:flex><span>          target_label: instance
</span></span><span style=display:flex><span>        - action: labelmap
</span></span><span style=display:flex><span>          regex: __meta_kubernetes_service_label_(.+)
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_namespace]
</span></span><span style=display:flex><span>          target_label: kubernetes_namespace
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_name]
</span></span><span style=display:flex><span>          target_label: kubernetes_name
</span></span><span style=display:flex><span>    <span style=color:green># Example scrape config for pods</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># Relabelling allows to configure the actual service scrape endpoint using the following annotations:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/scrape`: Only scrape pods that have a value of `true`</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/path`: If the metrics path is not `/metrics` override this.</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.</span>
</span></span><span style=display:flex><span>    - job_name: <span style=color:#a31515>&#39;kubernetes-pods&#39;</span>
</span></span><span style=display:flex><span>      kubernetes_sd_configs:
</span></span><span style=display:flex><span>        - role: pod
</span></span><span style=display:flex><span>      relabel_configs:
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
</span></span><span style=display:flex><span>          action: keep
</span></span><span style=display:flex><span>          regex: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: __metrics_path__
</span></span><span style=display:flex><span>          regex: (.+)
</span></span><span style=display:flex><span>        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          regex: (.+):(?:\d+);(\d+)
</span></span><span style=display:flex><span>          replacement: ${1}:${2}
</span></span><span style=display:flex><span>          target_label: __address__
</span></span><span style=display:flex><span>        - action: labelmap
</span></span><span style=display:flex><span>          regex: __meta_kubernetes_pod_label_(.+)
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_namespace]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: kubernetes_namespace
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_pod_name]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: kubernetes_pod_name
</span></span><span style=display:flex><span>    <span style=color:green># Scrape config for service endpoints.</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># The relabeling allows the actual service scrape endpoint to be configured</span>
</span></span><span style=display:flex><span>    <span style=color:green># via the following annotations:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/scrape`: Only scrape services that have a value of `true`</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need</span>
</span></span><span style=display:flex><span>    <span style=color:green># to set this to `https` &amp; most likely set the `tls_config` of the scrape config.</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/path`: If the metrics path is not `/metrics` override this.</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/port`: If the metrics are exposed on a different port to the</span>
</span></span><span style=display:flex><span>    <span style=color:green># service then set this appropriately.</span>
</span></span><span style=display:flex><span>    - job_name: <span style=color:#a31515>&#39;kubernetes-service-endpoints&#39;</span>
</span></span><span style=display:flex><span>      kubernetes_sd_configs:
</span></span><span style=display:flex><span>        - role: endpoints
</span></span><span style=display:flex><span>      relabel_configs:
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
</span></span><span style=display:flex><span>          action: keep
</span></span><span style=display:flex><span>          regex: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: __scheme__
</span></span><span style=display:flex><span>          regex: (https?)
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: __metrics_path__
</span></span><span style=display:flex><span>          regex: (.+)
</span></span><span style=display:flex><span>        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: __address__
</span></span><span style=display:flex><span>          regex: (.+)(?::\d+);(\d+)
</span></span><span style=display:flex><span>          replacement: $1:$2
</span></span><span style=display:flex><span>        - action: labelmap
</span></span><span style=display:flex><span>          regex: __meta_kubernetes_service_label_(.+)
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_namespace]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: kubernetes_namespace
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_name]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: kubernetes_name <span style=color:green># Add your additional configuration here...</span>
</span></span></code></pre></div><p>Next, deploy Grafana. Since the deployment in this post is based on the Helm default values, the settings below are set explicitly in case the default changed.</p><p>Deploy Grafana via <code>helm install grafana --namespace &lt;your-prometheus-namespace> stable/grafana -f values.yaml</code>. Here, the same namespace is chosen for Prometheus and for Grafana.</p><p>Content of <code>values.yaml</code> for Grafana:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>server:
</span></span><span style=display:flex><span>  ingress:
</span></span><span style=display:flex><span>    enabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>  service:
</span></span><span style=display:flex><span>    type: ClusterIP
</span></span></code></pre></div><p>Check the running state of the pods on the Kubernetes Dashboard or by running <code>kubectl get pods -n &lt;your-prometheus-namespace></code>. In case of errors, check the log files of the pod(s) in question.</p><p>The text output of Helm after the deployment of Prometheus and Grafana contains very useful information, e.g., the user and password of the Grafana Admin user. The credentials are stored as secrets in the namespace <code>&lt;your-prometheus-namespace></code> and could be decoded via <code>kubectl get secret --namespace &lt;my-grafana-namespace> grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo</code>.</p><h2 id=basic-functional-tests>Basic Functional Tests</h2><p>To access the web UI of both applications, use port forwarding of port 9090.</p><p>Setup port forwarding for port 9090:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl port-forward -n &lt;your-prometheus-namespace&gt; &lt;your-prometheus-server-pod&gt; 9090:9090
</span></span></code></pre></div><p>Open <code>http://localhost:9090</code> in your web browser. Select Graph from the top tab and enter the following expressing to show the overall CPU usage for a server (see <a href=https://github.com/infinityworks/prometheus-example-queries/blob/master/README.md>Prometheus Query Examples</a>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-promql data-lang=promql><span style=display:flex><span>100 * (1 - <span style=color:#00f>avg</span> <span style=color:#00f>by</span>(instance)(<span style=color:#00f>irate</span>(node_cpu{mode=&#39;<span style=color:#a31515>idle</span>&#39;}[<span style=color:#a31515>5m</span>])))
</span></span></code></pre></div><p>This should show some data in a graph.</p><p>To show the same data in Grafana setup port forwarding for port 3000 for the Grafana pod and open the Grafana Web UI by opening <code>http://localhost:3000</code> in a browser. Enter the credentials of the admin user.</p><p>Next, you need to enter the server name of your Prometheus deployment. This name is shown directly after the installation via helm.</p><p>Run</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>helm status &lt;your-prometheus-name&gt;
</span></span></code></pre></div><p>to find this name. Below, this server name is referenced by <code>&lt;your-prometheus-server-name></code>.</p><p>First, you need to add your Prometheus server as data source:</p><ol><li>Navigate to <em>Dashboards → Data Sources</em></li><li>Choose <em>Add data source</em></li><li>Enter:<br><em>Name</em>: <code>&lt;your-prometheus-datasource-name></code><br><em>Type</em>: Prometheus<br><em>URL</em>: <code>http://&lt;your-prometheus-server-name></code><br><em>Access</em>: <code>proxy</code></li><li>Choose <em>Save & Test</em></li></ol><p>In case of failure, check the Prometheus URL in the Kubernetes Dashboard.</p><p>To add a Graph follow these steps:</p><ol><li>In the left corner, select <em>Dashboards → New</em> to create a new dashboard</li><li>Select <em>Graph</em> to create a new graph</li><li>Next, select the <em>Panel Title → Edit</em></li><li>Select your Prometheus Data Source in the drop down list</li><li>Enter the expression <code>100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m])))</code> in the entry field A</li><li>Select the floppy disk symbol (Save) on top</li></ol><p>Now you should have a very basic Prometheus and Grafana setup for your Kubernetes cluster.</p><p>As a next step you can implement monitoring for your applications by implementing the <a href=https://prometheus.io/docs/instrumenting/clientlibs/>Prometheus client API</a>.</p><h2 id=related-links>Related Links</h2><ul><li><a href=https://prometheus.io/>Prometheus</a></li><li><a href=https://github.com/kubernetes/charts/tree/master/stable/prometheus>Prometheus Helm Chart</a></li><li><a href=https://grafana.com>Grafana</a></li><li><a href=https://github.com/kubernetes/charts/tree/master/stable/grafana>Grafana Helm Chart</a></li></ul></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2023 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid@8.13.4/dist/mermaid.min.js integrity="sha512-JERecFUBbsm75UpkVheAuDOE8NdHjQBrPACfEQYPwvPG+fjgCpHAz1Jw2ci9EXmd3DdfiWth3O3CQvcfEg8gsA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.11dbee029dba1a98021fb7be4d7405a7392afb38ff5640a21ff4f4c4c5057b2f.js integrity="sha256-EdvuAp26GpgCH7e+TXQFpzkq+zj/VkCiH/T0xMUFey8=" crossorigin=anonymous></script></body></html>