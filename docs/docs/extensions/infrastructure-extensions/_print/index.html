<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.83.1"><link rel=canonical type=text/html href=https://gardener.cloud/docs/extensions/infrastructure-extensions/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/extensions/infrastructure-extensions/index.xml><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=icon type=image/x-icon href=https://gardener.cloud/images/favicon.ico><link rel=icon type=image/png href=https://gardener.cloud/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://gardener.cloud/images/favicon-16x16.png sizes=16x16><title>Infrastructure Extensions | Gardener</title><meta name=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:title" content="Infrastructure Extensions"><meta property="og:description" content="Gardener extension controllers for the different infrastructures"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/extensions/infrastructure-extensions/"><meta itemprop=name content="Infrastructure Extensions"><meta itemprop=description content="Gardener extension controllers for the different infrastructures"><meta name=twitter:card content="summary"><meta name=twitter:title content="Infrastructure Extensions"><meta name=twitter:description content="Gardener extension controllers for the different infrastructures"><link rel=preload href=/scss/main.min.ca2e9ddee7809848b536632b41e4e4df665800778ffe11b75edde5bdd6c78963.css as=style><link href=/scss/main.min.ca2e9ddee7809848b536632b41e4e4df665800778ffe11b75edde5bdd6c78963.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://unpkg.com/lunr@2.3.8/lunr.min.js integrity=sha384-vRQ9bDyE0Wnu+lMfm57BlYLO0/XauFuKpVsZPs7KEDwYKktWi5+Kz3MP8++DFlRY crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.2035d9813dafac83fe2a48b18d50f237.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/extensions/infrastructure-extensions/>Return to the regular view of this page</a>.</p></div><h1 class=title>Infrastructure Extensions</h1><div class=lead>Gardener extension controllers for the different infrastructures</div><div class=content></div></div><div class=td-content><h1 id=pg-936f45ed7bca2e441d2b1f9f2ad32c57>1 - Provider Alicloud</h1><div class=lead>Gardener extension controller for the Alibaba cloud provider</div><h1 id=gardener-extension-for-alicloud-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Alicloud provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-alicloud-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-alicloud-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-alicloud><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-alicloud alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=/docs/gardener/proposals/01-extensibility/>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the Alicloud provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=/docs/gardener/proposals/01-extensibility/>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.23</td><td>1.23.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.22</td><td>1.22.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.22%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.22%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.22 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.21</td><td>1.21.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.21%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.21%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.21 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.20</td><td>1.20.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.20%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.20%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.20 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.19</td><td>1.19.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.19%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.19%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.19 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.18</td><td>1.18.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.18%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.18%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.18 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.17</td><td>1.17.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.17%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.17%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.17 Conformance Tests"></a></td></tr></tbody></table><p>Please take a look <a href=/docs/gardener/usage/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-alicloud/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=/docs/gardener/proposals/01-extensibility/>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=/docs/gardener/proposals/04-new-core-gardener-cloud-apis/>GEP-4 (New <code>core.gardener.cloud/v1alpha1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6725fdfc61db3ef032d903c3c4029514>1.1 - Deployment</h1><h1 id=deployment-of-the-alicloud-provider-extension>Deployment of the AliCloud provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the AliCloud provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the AliCloud provider extension <a href=https://github.com/gardener/gardener-extension-provider-alicloud>repository</a>.</p><h2 id=gardener-extension-admission-alicloud>gardener-extension-admission-alicloud</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-admission-alicloud</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: &lt;CA-DATA&gt;
    server: https://default.kubernetes.svc.cluster.local
  name: garden
contexts:
- context:
    cluster: garden
    user: garden
  name: garden
current-context: garden
users:
- name: garden
  user:
    tokenFile: /var/run/secrets/projected/serviceaccount/token
</code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: &lt;CA-DATA&gt;
    server: https://virtual-garden.api
  name: virtual-garden
contexts:
- context:
    cluster: virtual-garden
    user: virtual-garden
  name: virtual-garden
current-context: virtual-garden
users:
- name: virtual-garden
  user:
    tokenFile: /var/run/secrets/projected/serviceaccount/token
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-07e59d41316d9bf3e092041d57e8dcfc>1.2 - Local Setup</h1><h3 id=admission-alicloud>admission-alicloud</h3><p><code>admission-alicloud</code> is an admission webhook server which is responsible for the validation of the cloud provider (Alicloud in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/development/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>make start-admission
</code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-alicloud.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>./hack/dev-setup-admission-alicloud.sh
</code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-alicloud</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bb31b8fd625e8e321607849b37068ed6>1.3 - Usage As End User</h1><h1 id=using-the-alicloud-provider-extension-with-gardener-as-end-user>Using the Alicloud provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>This document describes the configurable options for Alicloud and provides an example <code>Shoot</code> manifest with minimal configuration that can be used to create an Alicloud cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=alicloud-provider-credentials>Alicloud Provider Credentials</h2><p>In order for Gardener to create a Kubernetes cluster using Alicloud infrastructure components, a Shoot has to provide credentials with sufficient permissions to the desired Alicloud project.
Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of the Alicloud project.</p><p>This <code>Secret</code> must look as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Secret
metadata:
  name: core-alicloud
  namespace: garden-dev
type: Opaque
data:
  accessKeyID: base64(access-key-id)
  accessKeySecret: base64(access-key-secret)
</code></pre></div><p>The <code>SecretBinding</code> is configurable in the <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>Shoot cluster</a> with the field <code>secretBindingName</code>.</p><p>The required credentials for the Alicloud project are an <a href=https://www.alibabacloud.com/help/doc-detail/29009.htm>AccessKey Pair</a> associated with a <a href=https://www.alibabacloud.com/help/doc-detail/28627.htm>Resource Access Management (RAM) User</a>.
A RAM user is a special account that can be used by services and applications to interact with Alicloud Cloud Platform APIs.
Applications can use AccessKey pair to authorize themselves to a set of APIs and perform actions within the permissions granted to the RAM user.</p><p>Make sure to <a href=https://www.alibabacloud.com/help/doc-detail/93720.htm>create a Resource Access Management User</a>, and <a href=https://partners-intl.aliyun.com/help/doc-detail/116401.htm>create an AccessKey Pair</a> that shall be used for the Shoot cluster.</p><h3 id=permissions>Permissions</h3><p>Please make sure the provided credentials have the correct privileges. You can use the following Alicloud RAM policy document and attach it to the RAM user backed by the credentials you provided.</p><details><summary>Click to expand the Alicloud RAM policy document!</summary><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
    &#34;Statement&#34;: [
        {
            &#34;Action&#34;: [
                <span style=color:#a31515>&#34;vpc:*&#34;</span>
            ],
            &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
            &#34;Resource&#34;: [
                <span style=color:#a31515>&#34;*&#34;</span>
            ]
        },
        {
            &#34;Action&#34;: [
                <span style=color:#a31515>&#34;ecs:*&#34;</span>
            ],
            &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
            &#34;Resource&#34;: [
                <span style=color:#a31515>&#34;*&#34;</span>
            ]
        },
        {
            &#34;Action&#34;: [
                <span style=color:#a31515>&#34;slb:*&#34;</span>
            ],
            &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
            &#34;Resource&#34;: [
                <span style=color:#a31515>&#34;*&#34;</span>
            ]
        },
        {
            &#34;Action&#34;: [
                <span style=color:#a31515>&#34;ram:GetRole&#34;</span>,
                <span style=color:#a31515>&#34;ram:CreateRole&#34;</span>,
                <span style=color:#a31515>&#34;ram:CreateServiceLinkedRole&#34;</span>
            ],
            &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
            &#34;Resource&#34;: [
                <span style=color:#a31515>&#34;*&#34;</span>
            ]
        },
        {
            &#34;Action&#34;: [
                <span style=color:#a31515>&#34;ros:*&#34;</span>
            ],
            &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
            &#34;Resource&#34;: [
                <span style=color:#a31515>&#34;*&#34;</span>
            ]
        }
    ],
    &#34;Version&#34;: <span style=color:#a31515>&#34;1&#34;</span>
}
</code></pre></div></details><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.</p><p>An example <code>InfrastructureConfig</code> for the Alicloud extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
kind: InfrastructureConfig
networks:
  vpc: <span style=color:green># specify either &#39;id&#39; or &#39;cidr&#39;</span>
  <span style=color:green># id: my-vpc</span>
    cidr: 10.250.0.0/16
  <span style=color:green># gardenerManagedNATGateway: true</span>
  zones:
  - name: eu-central-1a
    workers: 10.250.1.0/24
  <span style=color:green># natGateway:</span>
    <span style=color:green># eipAllocationID: eip-ufxsdg122elmszcg</span>
</code></pre></div><p>The <code>networks.vpc</code> section describes whether you want to create the shoot cluster in an already existing VPC or whether to create a new one:</p><ul><li>If <code>networks.vpc.id</code> is given then you have to specify the VPC ID of the existing VPC that was created by other means (manually, other tooling, &mldr;).</li><li>If <code>networks.vpc.cidr</code> is given then you have to specify the VPC CIDR of a new VPC that will be created during shoot creation.
You can freely choose a private CIDR range.</li><li>Either <code>networks.vpc.id</code> or <code>networks.vpc.cidr</code> must be present, but not both at the same time.</li><li>When <code>networks.vpc.id</code> is present, in addition, you can also choose to set <code>networks.vpc.gardenerManagedNATGateway</code>. It is by default <code>false</code>. When it is set to <code>true</code>,
Gardener will create an Enhanced NATGateway in the VPC and associate it with a VSwitch created in the first zone in the <code>networks.zones</code>.</li><li>Please note that when <code>networks.vpc.id</code> is present, and <code>networks.vpc.gardenerManagedNATGateway</code> is <code>false</code> or not set, you have to <strong>manually</strong> create an Enhance NATGateway
and associate it with a VSwitch that you <strong>manually</strong> created. In this case, make sure the worker CIDRs in <code>networks.zones</code> do not overlap with the one you created.
If a NATGateway is created manually and a shoot is created in the same VPC with <code>networks.vpc.gardenerManagedNATGateway</code> set <code>true</code>, you need to manually adjust the route rule accordingly.
You may refer to <a href=https://www.alibabacloud.com/help/en/doc-detail/121139.html>here</a>.</li></ul><p>The <code>networks.zones</code> section describes which subnets you want to create in availability zones.
For every zone, the Alicloud extension creates one subnet:</p><ul><li>The <code>workers</code> subnet is used for all shoot worker nodes, i.e., VMs which later run your applications.</li></ul><p>For every subnet, you have to specify a CIDR range contained in the VPC CIDR specified above, or the VPC CIDR of your already existing VPC.
You can freely choose these CIDR and it is your responsibility to properly design the network layout to suit your needs.</p><p>If you want to use multiple availability zones then add a second, third, &mldr; entry to the <code>networks.zones[]</code> list and properly specify the AZ name in <code>networks.zones[].name</code>.</p><p>Apart from the VPC and the subnets the Alicloud extension will also create a NAT gateway (only if a new VPC is created), a key pair, elastic IPs, VSwitches, a SNAT table entry, and security groups.</p><p>By default, the Alicloud extension will create a corresponding Elastic IP that it attaches to this NAT gateway and which is used for egress traffic.
The <code>networks.zones[].natGateway.eipAllocationID</code> field allows you to specify the Elastic IP Allocation ID of an existing Elastic IP allocation in case you want to bring your own.
If provided, no new Elastic IP will be created and, instead, the Elastic IP specified by you will be used.</p><p>⚠️ If you change this field for an already existing infrastructure then it will disrupt egress traffic while Alicloud applies this change, because the NAT gateway must be recreated with the new Elastic IP association.
Also, please note that the existing Elastic IP will be permanently deleted if it was earlier created by the Alicloud extension.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the Alicloud-specific control plane components.
Today, the Alicloud extension deploys the <code>cloud-controller-manager</code> and the CSI controllers.</p><p>An example <code>ControlPlaneConfig</code> for the Alicloud extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
kind: ControlPlaneConfig
csi:
  enableADController: <span style=color:#00f>true</span>
cloudControllerManager:
  featureGates:
    CustomResourceValidation: <span style=color:#00f>true</span>
</code></pre></div><p>The <code>csi.enableADController</code> is used as the value of environment <a href=https://github.com/kubernetes-sigs/alibaba-cloud-csi-driver/blob/cd0788a0a440926d504d8f8fb7f6e738fe96f3ae/pkg/disk/nodeserver.go#L80>DISK_AD_CONTROLLER</a>, which is used for AliCloud csi-disk-plugin. This field is optional. When a new shoot is creatd, this field is automatically set true. For an existing shoot created in previous versions, it remains unchanged. If there are persistent volumes created before year 2021, please be cautious to set this field <em>true</em> because they may fail to mount to nodes.</p><p>The <code>cloudControllerManager.featureGates</code> contains a map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><h2 id=workerconfig><code>WorkerConfig</code></h2><p>The Alicloud extension does not support a specific <code>WorkerConfig</code>. However, it supports additional data volumes (plus encryption) per machine.
By default (if not stated otherwise), all the disks are unencrypted.
For each data volume, you have to specify a name.
It also supports encrypted system disk.
However, only <a href="https://www.alibabacloud.com/help/doc-detail/172789.htm?spm=a2c63.l28256.b99.244.5da67453bNBrCt">Customized image</a> is currently supported to be used as a basic image for encrypted system disk.
Please be noted that the change of system disk encryption flag will cause reconciliation of a shoot, and it will result in nodes rolling update within the worker group.</p><p>The following YAML is a snippet of a <code>Shoot</code> resource:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:
  provider:
    workers:
    - name: cpu-worker
      ...
      volume:
        type: cloud_efficiency
        size: 20Gi
        encrypted: <span style=color:#00f>true</span>
      dataVolumes:
      - name: kubelet-dir
        type: cloud_efficiency
        size: 25Gi
        encrypted: <span style=color:#00f>true</span>
</code></pre></div><h2 id=example-shoot-manifest-one-availability-zone>Example <code>Shoot</code> manifest (one availability zone)</h2><p>Please find below an example <code>Shoot</code> manifest for one availability zone:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1alpha1
kind: Shoot
metadata:
  name: johndoe-alicloud
  namespace: garden-dev
spec:
  cloudProfileName: alicloud
  region: eu-central-1
  secretBindingName: core-alicloud
  provider:
    type: alicloud
    infrastructureConfig:
      apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
      kind: InfrastructureConfig
      networks:
        vpc:
          cidr: 10.250.0.0/16
        zones:
        - name: eu-central-1a
          workers: 10.250.0.0/19
    controlPlaneConfig:
      apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
      kind: ControlPlaneConfig
    workers:
    - name: worker-xoluy
      machine:
        type: ecs.sn2ne.large
      minimum: 2
      maximum: 2
      volume:
        size: 50Gi
        type: cloud_efficiency
      zones:
      - eu-central-1a
  networking:
    nodes: 10.250.0.0/16
    type: calico
  kubernetes:
    version: 1.16.1
  maintenance:
    autoUpdate:
      kubernetesVersion: <span style=color:#00f>true</span>
      machineImageVersion: <span style=color:#00f>true</span>
  addons:
    kubernetes-dashboard:
      enabled: <span style=color:#00f>true</span>
    nginx-ingress:
      enabled: <span style=color:#00f>true</span>
</code></pre></div><h2 id=example-shoot-manifest-two-availability-zones>Example <code>Shoot</code> manifest (two availability zones)</h2><p>Please find below an example <code>Shoot</code> manifest for two availability zones:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1alpha1
kind: Shoot
metadata:
  name: johndoe-alicloud
  namespace: garden-dev
spec:
  cloudProfileName: alicloud
  region: eu-central-1
  secretBindingName: core-alicloud
  provider:
    type: alicloud
    infrastructureConfig:
      apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
      kind: InfrastructureConfig
      networks:
        vpc:
          cidr: 10.250.0.0/16
        zones:
        - name: eu-central-1a
          workers: 10.250.0.0/26
        - name: eu-central-1b
          workers: 10.250.0.64/26
    controlPlaneConfig:
      apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
      kind: ControlPlaneConfig
    workers:
    - name: worker-xoluy
      machine:
        type: ecs.sn2ne.large
      minimum: 2
      maximum: 4
      volume:
        size: 50Gi
        type: cloud_efficiency
        <span style=color:green># NOTE: Below comment is for the case when encrypted field of an existing shoot is updated from false to true. </span>
        <span style=color:green># It will cause affected nodes to be rolling updated. Users must trigger a MAINTAIN operation of the shoot. </span>
        <span style=color:green># Otherwise, the shoot will fail to reconcile.</span>
        <span style=color:green># You could do it either via Dashboard or annotating the shoot with gardener.cloud/operation=maintain</span>
        encrypted: <span style=color:#00f>true</span>
      zones:
      - eu-central-1a
      - eu-central-1b
  networking:
    nodes: 10.250.0.0/16
    type: calico
  kubernetes:
    version: 1.16.1
  maintenance:
    autoUpdate:
      kubernetesVersion: <span style=color:#00f>true</span>
      machineImageVersion: <span style=color:#00f>true</span>
  addons:
    kubernetes-dashboard:
      enabled: <span style=color:#00f>true</span>
    nginx-ingress:
      enabled: <span style=color:#00f>true</span>
</code></pre></div><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-alicloud@v1.33</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-eb56b2718d72c1dc655334ee1654389a>1.4 - Usage As Operator</h1><h1 id=using-the-alicloud-provider-extension-with-gardener-as-operator>Using the Alicloud provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.
The <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml><code>core.gardener.cloud/v1beta1.Seed</code> resource</a> is structured similarly.
Additionally, it allows configuring settings for the backups of the main etcds' data of shoot clusters control planes running in this seed cluster.</p><p>This document explains the necessary configuration for this provider extension. In addition, this document also describes how to enable the use of customized machine images for Alicloud.</p><h2 id=cloudprofile-resource><code>CloudProfile</code> resource</h2><p>This section describes, how the configuration for <code>CloudProfile</code> looks like for Alicloud by providing an example <code>CloudProfile</code> manifest with minimal configuration that can be used to allow the creation of Alicloud shoot clusters.</p><h3 id=cloudprofileconfig><code>CloudProfileConfig</code></h3><p>The cloud profile configuration contains information about the real machine image IDs in the Alicloud environment (AMIs).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the Alicloud extension knows the AMI for every version you want to offer.</p><p>An example <code>CloudProfileConfig</code> for the Alicloud extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
kind: CloudProfileConfig
machineImages:
- name: coreos
  versions:
  - version: 2023.4.0
    regions:
    - name: eu-central-1
      id: coreos_2023_4_0_64_30G_alibase_20190319.vhd
</code></pre></div><h3 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h3><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: CloudProfile
metadata:
  name: alicloud
spec:
  type: alicloud
  kubernetes:
    versions:
    - version: 1.16.1
    - version: 1.16.0
      expirationDate: <span style=color:#a31515>&#34;2020-04-05T01:02:03Z&#34;</span>
  machineImages:
  - name: coreos
    versions:
    - version: 2023.4.0
  machineTypes:
  - name: ecs.sn2ne.large
    cpu: <span style=color:#a31515>&#34;2&#34;</span>
    gpu: <span style=color:#a31515>&#34;0&#34;</span>
    memory: 8Gi
  volumeTypes:
  - name: cloud_efficiency
    class: standard
  - name: cloud_ssd
    class: premium
  regions:
  - name: eu-central-1
    zones:
    - name: eu-central-1a
    - name: eu-central-1b
  providerConfig:
    apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
    kind: CloudProfileConfig
    machineImages:
    - name: coreos
      versions:
      - version: 2023.4.0
        regions:
        - name: eu-central-1
          id: coreos_2023_4_0_64_30G_alibase_20190319.vhd
</code></pre></div><h2 id=enable-customized-machine-images-for-the-alicloud-extension>Enable customized machine images for the Alicloud extension</h2><p>Customized machine images can be created for an Alicloud account and shared with other Alicloud accounts.
The same customized machine image has different image ID in different regions on Alicloud.
If you need to enable <code>encrypted system disk</code>, you must provide customized machine images.
Administrators/Operators need to explicitly declare them per imageID per region as below:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>machineImages:
- name: customized_coreos
  regions:
  - imageID: &lt;image_id_in_eu_central_1&gt;
    region: eu-central-1
  - imageID: &lt;image_id_in_cn_shanghai&gt;
    region: cn-shanghai
  ...
  version: 2191.4.1
...
</code></pre></div><p>End-users have to have the permission to use the customized image from its creator Alicloud account. To enable end-users to use customized images, the images are shared from Alicloud account of Seed operator with end-users' Alicloud accounts. Administrators/Operators need to explicitly provide Seed operator&rsquo;s Alicloud account access credentials (base64 encoded) as below:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>machineImageOwnerSecret:
  name: machine-image-owner
  accessKeyID: &lt;base64_encoded_access_key_id&gt;
  accessKeySecret: &lt;base64_encoded_access_key_secret&gt;
</code></pre></div><p>As a result, a Secret named <code>machine-image-owner</code> by default will be created in namespace of Alicloud provider extension.</p><p>Operators should also maintain custom image IDs which are to be shared with end-users as below:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>toBeSharedImageIDs:
- &lt;image_id_1&gt;
- &lt;image_id_2&gt;
- &lt;image_id_3&gt;
</code></pre></div><h3 id=example-controllerdeployment-manifest-for-enabling-customized-machine-images>Example <code>ControllerDeployment</code> manifest for enabling customized machine images</h3><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: ControllerDeployment
metadata:
  name: extension-provider-alicloud
spec:
  type: helm
   providerConfig:
    chart: |<span style=color:#a31515>
</span><span style=color:#a31515>      </span>      H4sIFAAAAAAA/yk...
    values:
      config:
        machineImageOwnerSecret:
          accessKeyID: &lt;base64_encoded_access_key_id&gt;
          accessKeySecret: &lt;base64_encoded_access_key_secret&gt;
        toBeSharedImageIDs:
        - &lt;image_id_1&gt;
        - &lt;image_id_2&gt;
        ...
        machineImages:
        - name: customized_coreos
          regions:
          - imageID: &lt;image_id_in_eu_central_1&gt;
            region: eu-central-1
          - imageID: &lt;image_id_in_cn_shanghai&gt;
            region: cn-shanghai
          ...
          version: 2191.4.1
        ...
        csi:
          enableADController: <span style=color:#00f>true</span>
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          memory: 128Mi
</code></pre></div><h2 id=seed-resource><code>Seed</code> resource</h2><p>This provider extension does not support any provider configuration for the <code>Seed</code>&rsquo;s <code>.spec.provider.providerConfig</code> field.
However, it supports to managing of backup infrastructure, i.e., you can specify a configuration for the <code>.spec.backup</code> field.</p><h3 id=backup-configuration>Backup configuration</h3><p>A Seed of type <code>alicloud</code> can be configured to perform backups for the main etcds' of the shoot clusters control planes using Alicloud <a href=https://www.alibabacloud.com/help/doc-detail/31817.htm>Object Storage Service</a>.</p><p>The location/region where the backups will be stored defaults to the region of the Seed (<code>spec.provider.region</code>).</p><p>Please find below an example <code>Seed</code> manifest (partly) that configures backups using Alicloud Object Storage Service.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>---
apiVersion: core.gardener.cloud/v1beta1
kind: Seed
metadata:
  name: my-seed
spec:
  provider:
    type: alicloud
    region: cn-shanghai
  backup:
    provider: alicloud
    secretRef:
      name: backup-credentials
      namespace: garden
  ...
</code></pre></div><p>An example of the referenced secret containing the credentials for the Alicloud Object Storage Service can be found in the <a href=https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-etcd-backup-secret.yaml>example folder</a>.</p><h4 id=permissions-for-alicloud-object-storage-service>Permissions for Alicloud Object Storage Service</h4><p>Please make sure the RAM user associated with the provided AccessKey pair has the following permission.</p><ul><li>AliyunOSSFullAccess</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2fdc592b220f6647eac2d46161fce5c7>2 - Provider AWS</h1><div class=lead>Gardener extension controller for the AWS cloud provider</div><h1 id=gardener-extension-for-aws-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for AWS provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-aws-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-aws-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-aws><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-aws alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=/docs/gardener/proposals/01-extensibility/>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the AWS provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=/docs/gardener/proposals/01-extensibility/>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.23</td><td>1.23.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.22</td><td>1.22.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.22%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.22%20AWS/tests_status?style=svg" alt="Gardener v1.22 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.21</td><td>1.21.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.21%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.21%20AWS/tests_status?style=svg" alt="Gardener v1.21 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.20</td><td>1.20.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.20%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.20%20AWS/tests_status?style=svg" alt="Gardener v1.20 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.19</td><td>1.19.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.19%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.19%20AWS/tests_status?style=svg" alt="Gardener v1.19 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.18</td><td>1.18.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.18%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.18%20AWS/tests_status?style=svg" alt="Gardener v1.18 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.17</td><td>1.17.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.17%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.17%20AWS/tests_status?style=svg" alt="Gardener v1.17 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.16</td><td>1.16.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.16%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.16%20AWS/tests_status?style=svg" alt="Gardener v1.16 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.15</td><td>1.15.0+</td><td>[1]</td></tr></tbody></table><p>[1] Conformance tests are still executed and validated, unfortunately <a href=https://github.com/kubernetes/test-infra/pull/18509#issuecomment-668204180>no longer shown in TestGrid</a>.</p><p>Please take a look <a href=/docs/gardener/usage/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><h2 id=compatibility>Compatibility</h2><p>The following lists known compatibility issues of this extension controller with other Gardener components.</p><table><thead><tr><th>AWS Extension</th><th>Gardener</th><th>Action</th><th>Notes</th></tr></thead><tbody><tr><td><code>&lt;= v1.15.0</code></td><td><code>>v1.10.0</code></td><td>Please update the provider version to <code>> v1.15.0</code> or disable the feature gate <code>MountHostCADirectories</code> in the Gardenlet.</td><td>Applies if feature flag <code>MountHostCADirectories</code> in the Gardenlet is enabled. Shoots with CSI enabled (Kubernetes version >= 1.18) miss a mount to the directory <code>/etc/ssl</code> in the Shoot API Server. This can lead to not trusting external Root CAs when the API Server makes requests via webhooks or OIDC.</td></tr></tbody></table><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-aws/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=/docs/gardener/proposals/01-extensibility/>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=/docs/gardener/proposals/04-new-core-gardener-cloud-apis/>GEP-4 (New <code>core.gardener.cloud/v1alpha1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-06631c97e1303d2dca2d0507f0a44b1b>2.1 - Deployment</h1><h1 id=deployment-of-the-aws-provider-extension>Deployment of the AWS provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the AWS provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the AWS provider extension <a href=https://github.com/gardener/gardener-extension-provider-aws>repository</a>.</p><h2 id=gardener-extension-admission-aws>gardener-extension-admission-aws</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-admission-aws</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: &lt;CA-DATA&gt;
    server: https://default.kubernetes.svc.cluster.local
  name: garden
contexts:
- context:
    cluster: garden
    user: garden
  name: garden
current-context: garden
users:
- name: garden
  user:
    tokenFile: /var/run/secrets/projected/serviceaccount/token
</code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: &lt;CA-DATA&gt;
    server: https://virtual-garden.api
  name: virtual-garden
contexts:
- context:
    cluster: virtual-garden
    user: virtual-garden
  name: virtual-garden
current-context: virtual-garden
users:
- name: virtual-garden
  user:
    tokenFile: /var/run/secrets/projected/serviceaccount/token
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-2a8b19ff0eb4b808c7bdd23b8033043b>2.2 - Local Setup</h1><h3 id=admission-aws>admission-aws</h3><p><code>admission-aws</code> is an admission webhook server which is responsible for the validation of the cloud provider (AWS in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/development/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>make start-admission
</code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-aws.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>./hack/dev-setup-admission-aws.sh
</code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-aws</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0bff1d7b2879983d062a0220314509f4>2.3 - Usage As End User</h1><h1 id=using-the-aws-provider-extension-with-gardener-as-end-user>Using the AWS provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for AWS and provide an example <code>Shoot</code> manifest with minimal configuration that you can use to create an AWS cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=provider-secret-data>Provider Secret Data</h2><p>Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of your AWS account.
This <code>Secret</code> must look as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Secret
metadata:
  name: core-aws
  namespace: garden-dev
type: Opaque
data:
  accessKeyID: base64(access-key-id)
  secretAccessKey: base64(secret-access-key)
</code></pre></div><p>The <a href=https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>AWS documentation</a> explains the necessary steps to enable programmatic access, i.e. create <strong>access key ID</strong> and <strong>access key</strong>, for the user of your choice.</p><p>⚠️ For security reasons, we recommend creating a <strong>dedicated user with programmatic access only</strong>. Please avoid re-using a IAM user which has access to the AWS console (human user).</p><p>⚠️ Depending on your AWS API usage it can be problematic to reuse the same AWS Account for different Shoot clusters in the same region due to rate limits. Please consider spreading your Shoots over multiple AWS Accounts if you are hitting those limits.</p><h3 id=permissions>Permissions</h3><p>Please make sure that the provided credentials have the correct privileges. You can use the following AWS IAM policy document and attach it to the IAM user backed by the credentials you provided (please check the <a href=http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html>official AWS documentation</a> as well):</p><details><summary>Click to expand the AWS IAM policy document!</summary><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  &#34;Version&#34;: <span style=color:#a31515>&#34;2012-10-17&#34;</span>,
  &#34;Statement&#34;: [
    {
      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
      &#34;Action&#34;: <span style=color:#a31515>&#34;autoscaling:*&#34;</span>,
      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
    },
    {
      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
      &#34;Action&#34;: <span style=color:#a31515>&#34;ec2:*&#34;</span>,
      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
    },
    {
      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
      &#34;Action&#34;: <span style=color:#a31515>&#34;elasticloadbalancing:*&#34;</span>,
      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
    },
    {
      &#34;Action&#34;: [
        <span style=color:#a31515>&#34;iam:GetInstanceProfile&#34;</span>,
        <span style=color:#a31515>&#34;iam:GetPolicy&#34;</span>,
        <span style=color:#a31515>&#34;iam:GetPolicyVersion&#34;</span>,
        <span style=color:#a31515>&#34;iam:GetRole&#34;</span>,
        <span style=color:#a31515>&#34;iam:GetRolePolicy&#34;</span>,
        <span style=color:#a31515>&#34;iam:ListPolicyVersions&#34;</span>,
        <span style=color:#a31515>&#34;iam:ListRolePolicies&#34;</span>,
        <span style=color:#a31515>&#34;iam:ListAttachedRolePolicies&#34;</span>,
        <span style=color:#a31515>&#34;iam:ListInstanceProfilesForRole&#34;</span>,
        <span style=color:#a31515>&#34;iam:CreateInstanceProfile&#34;</span>,
        <span style=color:#a31515>&#34;iam:CreatePolicy&#34;</span>,
        <span style=color:#a31515>&#34;iam:CreatePolicyVersion&#34;</span>,
        <span style=color:#a31515>&#34;iam:CreateRole&#34;</span>,
        <span style=color:#a31515>&#34;iam:CreateServiceLinkedRole&#34;</span>,
        <span style=color:#a31515>&#34;iam:AddRoleToInstanceProfile&#34;</span>,
        <span style=color:#a31515>&#34;iam:AttachRolePolicy&#34;</span>,
        <span style=color:#a31515>&#34;iam:DetachRolePolicy&#34;</span>,
        <span style=color:#a31515>&#34;iam:RemoveRoleFromInstanceProfile&#34;</span>,
        <span style=color:#a31515>&#34;iam:DeletePolicy&#34;</span>,
        <span style=color:#a31515>&#34;iam:DeletePolicyVersion&#34;</span>,
        <span style=color:#a31515>&#34;iam:DeleteRole&#34;</span>,
        <span style=color:#a31515>&#34;iam:DeleteRolePolicy&#34;</span>,
        <span style=color:#a31515>&#34;iam:DeleteInstanceProfile&#34;</span>,
        <span style=color:#a31515>&#34;iam:PutRolePolicy&#34;</span>,
        <span style=color:#a31515>&#34;iam:PassRole&#34;</span>,
        <span style=color:#a31515>&#34;iam:UpdateAssumeRolePolicy&#34;</span>
      ],
      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
    }
  ]
}
</code></pre></div></details><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.</p><p>An example <code>InfrastructureConfig</code> for the AWS extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
kind: InfrastructureConfig
enableECRAccess: <span style=color:#00f>true</span>
networks:
  vpc: <span style=color:green># specify either &#39;id&#39; or &#39;cidr&#39;</span>
  <span style=color:green># id: vpc-123456</span>
    cidr: 10.250.0.0/16
  <span style=color:green># gatewayEndpoints:</span>
  <span style=color:green># - s3</span>
  zones:
  - name: eu-west-1a
    internal: 10.250.112.0/22
    public: 10.250.96.0/22
    workers: 10.250.0.0/19
  <span style=color:green># elasticIPAllocationID: eipalloc-123456</span>
ignoreTags:
  keys: <span style=color:green># individual ignored tag keys</span>
  - SomeCustomKey
  - AnotherCustomKey
  keyPrefixes: <span style=color:green># ignored tag key prefixes</span>
  - user.specific/prefix/
</code></pre></div><p>The <code>enableECRAccess</code> flag specifies whether the AWS IAM role policy attached to all worker nodes of the cluster shall contain permissions to access the Elastic Container Registry of the respective AWS account.
If the flag is not provided it is defaulted to <code>true</code>.
Please note that if the <code>iamInstanceProfile</code> is set for a worker pool in the <code>WorkerConfig</code> (see below) then <code>enableECRAccess</code> does not have any effect.
It only applies for those worker pools whose <code>iamInstanceProfile</code> is not set.</p><details><summary>Click to expand the default AWS IAM policy document used for the instance profiles!</summary><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  &#34;Version&#34;: <span style=color:#a31515>&#34;2012-10-17&#34;</span>,
  &#34;Statement&#34;: [
    {
      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
      &#34;Action&#34;: [
        <span style=color:#a31515>&#34;ec2:DescribeInstances&#34;</span>
      ],
      &#34;Resource&#34;: [
        <span style=color:#a31515>&#34;*&#34;</span>
      ]
    },
    <span>//</span> <span>Only</span> <span>if</span> <span>`.enableECRAccess`</span> <span>is</span> <span>`</span><span style=color:#00f>true</span><span>`.</span>
    {
      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
      &#34;Action&#34;: [
        <span style=color:#a31515>&#34;ecr:GetAuthorizationToken&#34;</span>,
        <span style=color:#a31515>&#34;ecr:BatchCheckLayerAvailability&#34;</span>,
        <span style=color:#a31515>&#34;ecr:GetDownloadUrlForLayer&#34;</span>,
        <span style=color:#a31515>&#34;ecr:GetRepositoryPolicy&#34;</span>,
        <span style=color:#a31515>&#34;ecr:DescribeRepositories&#34;</span>,
        <span style=color:#a31515>&#34;ecr:ListImages&#34;</span>,
        <span style=color:#a31515>&#34;ecr:BatchGetImage&#34;</span>
      ],
      &#34;Resource&#34;: [
        <span style=color:#a31515>&#34;*&#34;</span>
      ]
    }
  ]
}
</code></pre></div></details><p>The <code>networks.vpc</code> section describes whether you want to create the shoot cluster in an already existing VPC or whether to create a new one:</p><ul><li>If <code>networks.vpc.id</code> is given then you have to specify the VPC ID of the existing VPC that was created by other means (manually, other tooling, &mldr;).
Please make sure that the VPC has attached an internet gateway - the AWS controller won&rsquo;t create one automatically for existing VPCs. To make sure the nodes are able to join and operate in your cluster properly, please make sure that your VPC has enabled <a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html>DNS Support</a>, explicitly the attributes <code>enableDnsHostnames</code> and <code>enableDnsSupport</code> must be set to <code>true</code>.</li><li>If <code>networks.vpc.cidr</code> is given then you have to specify the VPC CIDR of a new VPC that will be created during shoot creation.
You can freely choose a private CIDR range.</li><li>Either <code>networks.vpc.id</code> or <code>networks.vpc.cidr</code> must be present, but not both at the same time.</li><li><code>networks.vpc.gatewayEndpoints</code> is optional. If specified then each item is used as service name in a corresponding Gateway VPC Endpoint.</li></ul><p>The <code>networks.zones</code> section contains configuration for resources you want to create or use in availability zones.
For every zone, the AWS extension creates three subnets:</p><ul><li>The <code>internal</code> subnet is used for <a href=https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internal-load-balancers.html>internal AWS load balancers</a>.</li><li>The <code>public</code> subnet is used for <a href=https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html>public AWS load balancers</a>.</li><li>The <code>workers</code> subnet is used for all shoot worker nodes, i.e., VMs which later run your applications.</li></ul><p>For every subnet, you have to specify a CIDR range contained in the VPC CIDR specified above, or the VPC CIDR of your already existing VPC.
You can freely choose these CIDRs and it is your responsibility to properly design the network layout to suit your needs.</p><p>Also, the AWS extension creates a dedicated NAT gateway for each zone.
By default, it also creates a corresponding Elastic IP that it attaches to this NAT gateway and which is used for egress traffic.
The <code>elasticIPAllocationID</code> field allows you to specify the ID of an existing Elastic IP allocation in case you want to bring your own.
If provided, no new Elastic IP will be created and, instead, the Elastic IP specified by you will be used.</p><p>⚠️ If you change this field for an already existing infrastructure then it will disrupt egress traffic while AWS applies this change.
The reason is that the NAT gateway must be recreated with the new Elastic IP association.
Also, please note that the existing Elastic IP will be permanently deleted if it was earlier created by the AWS extension.</p><p>You can configure <a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html>Gateway VPC Endpoints</a> by adding items in the optional list <code>networks.vpc.gatewayEndpoints</code>. Each item in the list is used as a service name and a corresponding endpoint is created for it. All created endpoints point to the service within the cluster&rsquo;s region. For example, consider this (partial) shoot config:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:
  region: eu-central-1
  provider:
    type: aws
    infrastructureConfig:
      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
      kind: InfrastructureConfig
      networks:
        vpc:
          gatewayEndpoints:
          - s3
</code></pre></div><p>The service name of the S3 Gateway VPC Endpoint in this example is <code>com.amazonaws.eu-central-1.s3</code>.</p><p>If you want to use multiple availability zones then add a second, third, &mldr; entry to the <code>networks.zones[]</code> list and properly specify the AZ name in <code>networks.zones[].name</code>.</p><p>Apart from the VPC and the subnets the AWS extension will also create DHCP options and an internet gateway (only if a new VPC is created), routing tables, security groups, elastic IPs, NAT gateways, EC2 key pairs, IAM roles, and IAM instance profiles.</p><p>The <code>ignoreTags</code> section allows to configure which resource tags on AWS resources managed by Gardener should be ignored during
infrastructure reconciliation. By default, all tags that are added outside of Gardener&rsquo;s
reconciliation will be removed during the next reconciliation. This field allows users and automation to add
custom tags on AWS resources created and managed by Gardener without loosing them on the next reconciliation.
Tags can ignored either by specifying exact key values (<code>ignoreTags.keys</code>) or key prefixes (<code>ignoreTags.keyPrefixes</code>).
In both cases it is forbidden to ignore the <code>Name</code> tag or any tag starting with <code>kubernetes.io</code> or <code>gardener.cloud</code>.<br>Please note though, that the tags are only ignored on resources created on behalf of the <code>Infrastructure</code> CR (i.e. VPC,
subnets, security groups, keypair, etc.), while tags on machines, volumes, etc. are not in the scope of this controller.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the AWS-specific control plane components.
Today, the only component deployed by the AWS extension is the <code>cloud-controller-manager</code>.</p><p>An example <code>ControlPlaneConfig</code> for the AWS extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
kind: ControlPlaneConfig
cloudControllerManager:
  featureGates:
    CustomResourceValidation: <span style=color:#00f>true</span>
storage:
  managedDefaultClass: <span style=color:#00f>false</span>
</code></pre></div><p>The <code>cloudControllerManager.featureGates</code> contains a map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><p>The <code>storage.managedDefaultClass</code> controls if the <code>default</code> storage / volume snapshot classes are marked as default by Gardener. Set it to <code>false</code> to <a href=https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/>mark another storage / volume snapshot class as default</a> without Gardener overwriting this change. If unset, this field defaults to <code>true</code>.</p><h2 id=workerconfig><code>WorkerConfig</code></h2><p>The AWS extension supports encryption for volumes plus support for additional data volumes per machine.
For each data volume, you have to specify a name.
By default (if not stated otherwise), all the disks (root & data volumes) are encrypted.
Please make sure that your <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html>instance-type supports encryption</a>.
If your instance-type doesn&rsquo;t support encryption, you will have to disable encryption (which is enabled by default) by setting <code>volume.encrpyted</code> to <code>false</code> (refer below shown YAML snippet).</p><p>The following YAML is a snippet of a <code>Shoot</code> resource:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:
  provider:
    workers:
    - name: cpu-worker
      ...
      volume:
        type: gp2
        size: 20Gi
        encrypted: <span style=color:#00f>false</span>
      dataVolumes:
      - name: kubelet-dir
        type: gp2
        size: 25Gi
        encrypted: <span style=color:#00f>true</span>
</code></pre></div><blockquote><p>Note: The AWS extension does not support EBS volume (root & data volumes) encryption with <a href=https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk>customer managed CMK</a>. Support for <a href=https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk>customer managed CMK</a> is out of scope for now. Only <a href=https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk>AWS managed CMK</a> is supported.</p></blockquote><p>Additionally, it is possible to provide further AWS-specific values for configuring the worker pools.
It can be provided in <code>.spec.provider.workers[].providerConfig</code> and is evaluated by the AWS worker controller when it reconciles the shoot machines.</p><p>An example <code>WorkerConfig</code> for the AWS extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
kind: WorkerConfig
volume:
  iops: 10000
dataVolumes:
- name: kubelet-dir
  iops: 12345
  snapshotID: snap-1234
iamInstanceProfile: <span style=color:green># (specify either ARN or name)</span>
  name: my-profile
<span style=color:green># arn: my-instance-profile-arn</span>
nodeTemplate: <span style=color:green># (to be specified only if the node capacity would be different from cloudprofile info during runtime)</span>
  capacity:
    cpu: 2
    gpu: 0
    memory: 50Gi
</code></pre></div><p>The <code>.volume.iops</code> is the number of I/O operations per second (IOPS) that the volume supports.
For <code>io1</code> volume type, this represents the number of IOPS that are provisioned for the volume.
For <code>gp2</code> volume type, this represents the baseline performance of the volume and the rate at which the volume accumulates I/O credits for bursting. For more information about General Purpose SSD baseline performance, I/O credits, and bursting, see Amazon EBS Volume Types (<a href=http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html>http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html</a>) in the Amazon Elastic Compute Cloud User Guide.<br>Constraint: Range is 100-20000 IOPS for <code>io1</code> volumes and 100-10000 IOPS for <code>gp2</code> volumes.</p><p>The <code>.dataVolumes</code> can optionally contain configurations for the data volumes stated in the <code>Shoot</code> specification in the <code>.spec.provider.workers[].dataVolumes</code> list.
The <code>.name</code> must match to the name of the data volume in the shoot.
Apart from the <code>.iops</code> (which, again, is only valid for <code>io1</code> or <code>gp2</code> volumes), it is also possible to provide a snapshot ID.
It allows to <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume.html>restore the data volume from an existing snapshot</a>.</p><p>The <code>iamInstanceProfile</code> section allows to specify the IAM instance profile name xor ARN that should be used for this worker pool.
If not specified, a dedicated IAM instance profile created by the infrastructure controller is used (see above).</p><h2 id=example-shoot-manifest-one-availability-zone>Example <code>Shoot</code> manifest (one availability zone)</h2><p>Please find below an example <code>Shoot</code> manifest for one availability zone:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1alpha1
kind: Shoot
metadata:
  name: johndoe-aws
  namespace: garden-dev
spec:
  cloudProfileName: aws
  region: eu-central-1
  secretBindingName: core-aws
  provider:
    type: aws
    infrastructureConfig:
      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
      kind: InfrastructureConfig
      networks:
        vpc:
          cidr: 10.250.0.0/16
        zones:
        - name: eu-central-1a
          internal: 10.250.112.0/22
          public: 10.250.96.0/22
          workers: 10.250.0.0/19
    controlPlaneConfig:
      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
      kind: ControlPlaneConfig
    workers:
    - name: worker-xoluy
      machine:
        type: m5.large
      minimum: 2
      maximum: 2
      volume:
        size: 50Gi
        type: gp2
    <span style=color:green># The following provider config is only valid if the volume type is `io1`.</span>
    <span style=color:green># providerConfig:</span>
    <span style=color:green>#   apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1</span>
    <span style=color:green>#   kind: WorkerConfig</span>
    <span style=color:green>#   volume:</span>
    <span style=color:green>#     iops: 10000</span>
      zones:
      - eu-central-1a
  networking:
    nodes: 10.250.0.0/16
    type: calico
  kubernetes:
    version: 1.16.1
  maintenance:
    autoUpdate:
      kubernetesVersion: <span style=color:#00f>true</span>
      machineImageVersion: <span style=color:#00f>true</span>
  addons:
    kubernetes-dashboard:
      enabled: <span style=color:#00f>true</span>
    nginx-ingress:
      enabled: <span style=color:#00f>true</span>
</code></pre></div><h2 id=example-shoot-manifest-three-availability-zones>Example <code>Shoot</code> manifest (three availability zones)</h2><p>Please find below an example <code>Shoot</code> manifest for three availability zones:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1alpha1
kind: Shoot
metadata:
  name: johndoe-aws
  namespace: garden-dev
spec:
  cloudProfileName: aws
  region: eu-central-1
  secretBindingName: core-aws
  provider:
    type: aws
    infrastructureConfig:
      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
      kind: InfrastructureConfig
      networks:
        vpc:
          cidr: 10.250.0.0/16
        zones:
        - name: eu-central-1a
          workers: 10.250.0.0/26
          public: 10.250.96.0/26
          internal: 10.250.112.0/26
        - name: eu-central-1b
          workers: 10.250.0.64/26
          public: 10.250.96.64/26
          internal: 10.250.112.64/26
        - name: eu-central-1c
          workers: 10.250.0.128/26
          public: 10.250.96.128/26
          internal: 10.250.112.128/26
    controlPlaneConfig:
      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
      kind: ControlPlaneConfig
    workers:
    - name: worker-xoluy
      machine:
        type: m5.large
      minimum: 3
      maximum: 9
      volume:
        size: 50Gi
        type: gp2
      zones:
      - eu-central-1a
      - eu-central-1b
      - eu-central-1c
  networking:
    nodes: 10.250.0.0/16
    type: calico
  kubernetes:
    version: 1.16.1
  maintenance:
    autoUpdate:
      kubernetesVersion: <span style=color:#00f>true</span>
      machineImageVersion: <span style=color:#00f>true</span>
  addons:
    kubernetes-dashboard:
      enabled: <span style=color:#00f>true</span>
    nginx-ingress:
      enabled: <span style=color:#00f>true</span>
</code></pre></div><h2 id=csi-volume-provisioners>CSI volume provisioners</h2><p>Every AWS shoot cluster that has at least Kubernetes v1.18 will be deployed with the AWS EBS CSI driver.
It is compatible with the legacy in-tree volume provisioner that was deprecated by the Kubernetes community and will be removed in future versions of Kubernetes.
End-users might want to update their custom <code>StorageClass</code>es to the new <code>ebs.csi.aws.com</code> provisioner.
Shoot clusters with Kubernetes v1.17 or less will use the in-tree <code>kubernetes.io/aws-ebs</code> volume provisioner in the kube-controller-manager and the kubelet.</p><h3 id=node-specific-volume-limits>Node-specific Volume Limits</h3><p>The Kubernetes scheduler allows configurable limit for the number of volumes that can be attached to a node. See <a href=https://k8s.io/docs/concepts/storage/storage-limits/#custom-limits>https://k8s.io/docs/concepts/storage/storage-limits/#custom-limits</a>.</p><p>CSI drivers usually have a different procedure for configuring this custom limit. By default, the EBS CSI driver parses the machine type name and then decides the volume limit. However, this is only a rough approximation and not good enough in most cases. Specifying the volume attach limit via command line flag (<code>--volume-attach-limit</code>) is currently the alternative until a more sophisticated solution presents itself (dynamically discovering the maximum number of attachable volume per EC2 machine type, see also <a href=https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/347)>https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/347)</a>. The AWS extension allows the <code>--volume-attach-limit</code> flag of the EBS CSI driver to be configurable via <code>aws.provider.extensions.gardener.cloud/volume-attach-limit</code> annotation on the <code>Shoot</code> resource. If the annotation is added to an existing <code>Shoot</code>, then reconciliation needs to be triggered manually (see <a href=/docs/gardener/usage/shoot_operations/#immediate-reconciliation>Immediate reconciliation</a>), as in general adding annotation to resource is not a change that leads to <code>.metadata.generation</code> increase in general.</p><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-aws@v1.34</code>.
Note that this feature is only usable for <code>Shoot</code>s whose <code>.spec.kubernetes.version</code> is greater or equal than the CSI migration version (<code>1.18</code>).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-1816fb764f847c52625eabfbeefd9827>2.4 - Usage As Operator</h1><h1 id=using-the-aws-provider-extension-with-gardener-as-operator>Using the AWS provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.
Similarly, the <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml><code>core.gardener.cloud/v1beta1.Seed</code> resource</a> is structured.
Additionally, it allows to configure settings for the backups of the main etcds' data of shoot clusters control planes running in this seed cluster.</p><p>This document explains what is necessary to configure for this provider extension.</p><h2 id=cloudprofile-resource><code>CloudProfile</code> resource</h2><p>In this section we are describing how the configuration for <code>CloudProfile</code>s looks like for AWS and provide an example <code>CloudProfile</code> manifest with minimal configuration that you can use to allow creating AWS shoot clusters.</p><h3 id=cloudprofileconfig><code>CloudProfileConfig</code></h3><p>The cloud profile configuration contains information about the real machine image IDs in the AWS environment (AMIs).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the AWS extension knows the AMI for every version you want to offer.</p><p>An example <code>CloudProfileConfig</code> for the AWS extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
kind: CloudProfileConfig
machineImages:
- name: coreos
  versions:
  - version: 2135.6.0
    regions:
    - name: eu-central-1
      ami: ami-034fd8c3f4026eb39
</code></pre></div><h3 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h3><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: CloudProfile
metadata:
  name: aws
spec:
  type: aws
  kubernetes:
    versions:
    - version: 1.16.1
    - version: 1.16.0
      expirationDate: <span style=color:#a31515>&#34;2020-04-05T01:02:03Z&#34;</span>
  machineImages:
  - name: coreos
    versions:
    - version: 2135.6.0
  machineTypes:
  - name: m5.large
    cpu: <span style=color:#a31515>&#34;2&#34;</span>
    gpu: <span style=color:#a31515>&#34;0&#34;</span>
    memory: 8Gi
    usable: <span style=color:#00f>true</span>
  volumeTypes:
  - name: gp2
    class: standard
    usable: <span style=color:#00f>true</span>
  - name: io1
    class: premium
    usable: <span style=color:#00f>true</span>
  regions:
  - name: eu-central-1
    zones:
    - name: eu-central-1a
    - name: eu-central-1b
    - name: eu-central-1c
  providerConfig:
    apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
    kind: CloudProfileConfig
    machineImages:
    - name: coreos
      versions:
      - version: 2135.6.0
        regions:
        - name: eu-central-1
          ami: ami-034fd8c3f4026eb39
</code></pre></div><h2 id=seed-resource><code>Seed</code> resource</h2><p>This provider extension does not support any provider configuration for the <code>Seed</code>&rsquo;s <code>.spec.provider.providerConfig</code> field.
However, it supports to manage backup infrastructure, i.e., you can specify configuration for the <code>.spec.backup</code> field.</p><h3 id=backup-configuration>Backup configuration</h3><p>Please find below an example <code>Seed</code> manifest (partly) that configures backups.
As you can see, the location/region where the backups will be stored can be different to the region where the seed cluster is running.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Secret
metadata:
  name: backup-credentials
  namespace: garden
type: Opaque
data:
  accessKeyID: base64(access-key-id)
  secretAccessKey: base64(secret-access-key)
---
apiVersion: core.gardener.cloud/v1beta1
kind: Seed
metadata:
  name: my-seed
spec:
  provider:
    type: aws
    region: eu-west-1
  backup:
    provider: aws
    region: eu-central-1
    secretRef:
      name: backup-credentials
      namespace: garden
  ...
</code></pre></div><p>Please look up <a href=https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys</a> as well.</p><h4 id=permissions-for-aws-iam-user>Permissions for AWS IAM user</h4><p>Please make sure that the provided credentials have the correct privileges. You can use the following AWS IAM policy document and attach it to the IAM user backed by the credentials you provided (please check the <a href=http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html>official AWS documentation</a> as well):</p><details><summary>Click to expand the AWS IAM policy document!</summary><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  &#34;Version&#34;: <span style=color:#a31515>&#34;2012-10-17&#34;</span>,
  &#34;Statement&#34;: [
    {
      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
      &#34;Action&#34;: <span style=color:#a31515>&#34;s3:*&#34;</span>,
      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
    }
  ]
}
</code></pre></div></details></div><div class=td-content style=page-break-before:always><h1 id=pg-327ad4a2eb9d221f7e8f6cc268e1fee3>3 - Provider Azure</h1><div class=lead>Gardener extension controller for the Azure cloud provider</div><h1 id=gardener-extension-for-azure-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Azure provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-azure-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-azure-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-azure><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-azure alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=/docs/gardener/proposals/01-extensibility/>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the Azure provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=/docs/gardener/proposals/01-extensibility/>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.23</td><td>1.23.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.22</td><td>1.22.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.22%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.22%20Azure/tests_status?style=svg" alt="Gardener v1.22 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.21</td><td>1.21.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.21%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.21%20Azure/tests_status?style=svg" alt="Gardener v1.21 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.20</td><td>1.20.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.20%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.20%20Azure/tests_status?style=svg" alt="Gardener v1.20 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.19</td><td>1.19.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.19%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.19%20Azure/tests_status?style=svg" alt="Gardener v1.19 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.18</td><td>1.18.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.18%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.18%20Azure/tests_status?style=svg" alt="Gardener v1.18 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.17</td><td>1.17.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.17%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.17%20Azure/tests_status?style=svg" alt="Gardener v1.17 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.16</td><td>1.16.0+, except 1.16.2</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.16%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.16%20Azure/tests_status?style=svg" alt="Gardener v1.16 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.15</td><td>1.15.0+, except 1.15.5</td><td>[1]</td></tr></tbody></table><p>[1] Conformance tests are still executed and validated, unfortunately <a href=https://github.com/kubernetes/test-infra/pull/18509#issuecomment-668204180>no longer shown in TestGrid</a>.</p><p>Please take a look <a href=/docs/gardener/usage/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-azure/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=/docs/gardener/proposals/01-extensibility/>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=/docs/gardener/proposals/04-new-core-gardener-cloud-apis/>GEP-4 (New <code>core.gardener.cloud/v1alpha1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-badcb60646a7f42794a59288364229e8>3.1 - Deployment</h1><h1 id=deployment-of-the-azure-provider-extension>Deployment of the Azure provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the Azure provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the Azure provider extension <a href=https://github.com/gardener/gardener-extension-provider-azure>repository</a>.</p><h2 id=gardener-extension-admission-azure>gardener-extension-admission-azure</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-admission-azure</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: &lt;CA-DATA&gt;
    server: https://default.kubernetes.svc.cluster.local
  name: garden
contexts:
- context:
    cluster: garden
    user: garden
  name: garden
current-context: garden
users:
- name: garden
  user:
    tokenFile: /var/run/secrets/projected/serviceaccount/token
</code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: &lt;CA-DATA&gt;
    server: https://virtual-garden.api
  name: virtual-garden
contexts:
- context:
    cluster: virtual-garden
    user: virtual-garden
  name: virtual-garden
current-context: virtual-garden
users:
- name: virtual-garden
  user:
    tokenFile: /var/run/secrets/projected/serviceaccount/token
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-244fdac6e5820831afb7c9945e0b580c>3.2 - Local Setup</h1><h3 id=admission-azure>admission-azure</h3><p><code>admission-azure</code> is an admission webhook server which is responsible for the validation of the cloud provider (Azure in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/development/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>make start-admission
</code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-azure.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>./hack/dev-setup-admission-azure.sh
</code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-azure</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-46c4e745ca368d82e869f0ac23fd4e7d>3.3 - Migrate Loadbalancer</h1><h1 id=migrate-azure-shoot-load-balancer-from-basic-to-standard-sku>Migrate Azure Shoot Load Balancer from basic to standard SKU</h1><p>This guide descibes how to migrate the Load Balancer of an Azure Shoot cluster from the basic SKU to the standard SKU.<br><strong>Be aware:</strong> You need to delete and recreate all services of type Load Balancer, which means that the public ip addresses of your service endpoints will change.<br>Please do this only if the Stakeholder really needs to migrate this Shoot to use standard Load Balancers. All new Shoot clusters will automatically use Azure Standard Load Balancers.</p><ol><li>Disable temporarily Gardeners reconciliation.<br>The Gardener Controller Manager need to be configured to allow ignoring Shoot clusters.
This can be configured in its the <code>ControllerManagerConfiguration</code> via the field <code>.controllers.shoot.respectSyncPeriodOverwrite="true"</code>.</li></ol><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:green># In the Garden cluster.</span>
kubectl annotate shoot &lt;shoot-name&gt; shoot.garden.sapcloud.io/ignore=<span style=color:#a31515>&#34;true&#34;</span>

<span style=color:green># In the Seed cluster.</span>
kubectl -n &lt;shoot-namespace&gt; scale deployment gardener-resource-manager --replicas=0
</code></pre></div><ol start=2><li>Backup all Kubernetes services of type Load Balancer.</li></ol><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:green># In the Shoot cluster.</span>
<span style=color:green># Determine all Load Balancer services.</span>
kubectl get service --all-namespaces | grep LoadBalancer

<span style=color:green># Backup each Load Balancer service.</span>
echo <span style=color:#a31515>&#34;---&#34;</span> &gt;&gt; service-backup.yaml &amp;&amp; kubectl -n &lt;namespace&gt; get service &lt;service-name&gt; -o yaml &gt;&gt; service-backup.yaml
</code></pre></div><ol start=3><li>Delete all Load Balancer services.</li></ol><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:green># In the Shoot cluster.</span>
kubectl -n &lt;namespace&gt; delete service &lt;service-name&gt;
</code></pre></div><ol start=4><li>Wait until until Load Balancer is deleted.
Wait until all services of type Load Balancer are deleted and the Azure Load Balancer resource is also deleted.
Check via the Azure Portal if the Load Balancer within the Shoot Resource Group has been deleted.
This should happen automatically after all Kubernetes Load Balancer service are gone within a few minutes.</li></ol><p>Alternatively the Azure cli can be used to check the Load Balancer in the Shoot Resource Group.
The credentials to configure the cli are available on the Seed cluster in the Shoot namespace.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:green># In the Seed cluster.</span>
<span style=color:green># Fetch the credentials from cloudprovider secret.</span>
kubectl -n &lt;shoot-namespace&gt; get secret cloudprovider -o yaml

<span style=color:green># Configure the Azure cli, with the base64 decoded values of the cloudprovider secret.</span>
az login --service-principal --username &lt;clientID&gt; --password &lt;clientSecret&gt; --tenant &lt;tenantID&gt;
az account set -s &lt;subscriptionID&gt;

<span style=color:green># Fetch the constantly the Shoot Load Balancer in the Shoot Resource Group. Wait until the resource is gone.</span>
watch <span style=color:#a31515>&#39;az network lb show -g shoot--&lt;project-name&gt;--&lt;shoot-name&gt; -n shoot--&lt;project-name&gt;--&lt;shoot-name&gt;&#39;</span>

<span style=color:green># Logout.</span>
az logout
</code></pre></div><ol start=5><li>Modify the <code>cloud-povider-config</code> configmap in the Seed namespace of the Shoot.<br>The key <code>cloudprovider.conf</code> contains the Kubernetes cloud-provider configuration.
The value is a multiline string. Please change the value of the field <code>loadBalancerSku</code> from <code>basic</code> to <code>standard</code>.
Iff the field does not exists then append <code>loadBalancerSku: \"standard\"\n</code> to the value/string.</li></ol><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:green># In the Seed cluster.</span>
kubectl -n &lt;shoot-namespace&gt; edit cm cloud-provider-config
</code></pre></div><ol start=6><li>Enable Gardeners reconcilation and trigger a reconciliation.</li></ol><pre><code># In the Garden cluster
# Enable reconcilation
kubectl annotate shoot &lt;shoot-name&gt; shoot.garden.sapcloud.io/ignore-

# Trigger reconcilation
kubectl annotate shoot &lt;shoot-name&gt; shoot.garden.sapcloud.io/operation=&quot;reconcile&quot;
</code></pre><p>Wait until the cluster has been reconciled.</p><ol start=6><li>Recreate the services from the backup file.<br>Probably you need to remove some fields from the service defintions e.g. <code>.spec.clusterIP</code>, <code>.metadata.uid</code> or <code>.status</code> etc.</li></ol><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubectl apply -f service-backup.yaml
</code></pre></div><ol start=7><li>If successful remove backup file.</li></ol><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:green># Delete the backup file.</span>
rm -f service-backup.yaml
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-aaec8948c232e5161bb808b6485480de>3.4 - Usage As End User</h1><h1 id=using-the-azure-provider-extension-with-gardener-as-end-user>Using the Azure provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>This document describes the configurable options for Azure and provides an example <code>Shoot</code> manifest with minimal configuration that can be used to create an Azure cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=azure-provider-credentials>Azure Provider Credentials</h2><p>In order for Gardener to create a Kubernetes cluster using Azure infrastructure components, a Shoot has to provide credentials with sufficient permissions to the desired Azure subscription.
Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of the Azure subscription.
The <code>SecretBinding</code> is configurable in the <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>Shoot cluster</a> with the field <code>secretBindingName</code>.</p><p>Create an <a href=https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal>Azure Application and Service Principle</a> and obtain its credentials.
Please make sure the Azure application has the following IAM roles.</p><ul><li><a href=https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#contributor>Contributor</a></li></ul><p>The example below demonstrates how the secret containing the client credentials of the Azure Application has to look like:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Secret
metadata:
  name: core-azure
  namespace: garden-dev
type: Opaque
data:
  clientID: base64(client-id)
  clientSecret: base64(client-secret)
  subscriptionID: base64(subscription-id)
  tenantID: base64(tenant-id)
</code></pre></div><p>⚠️ Depending on your API usage it can be problematic to reuse the same Service Principal for different Shoot clusters due to rate limits.
Please consider spreading your Shoots over Service Principals from different Azure subscriptions if you are hitting those limits.</p><h3 id=managed-service-principals>Managed Service Principals</h3><p>The operators of the Gardener Azure extension can provide managed service principals.
This eliminates the need for users to provide an own service principal for a Shoot.</p><p>To make use of a managed service principal, the Azure secret of a Shoot cluster must contain only a <code>subscriptionID</code> and a <code>tenantID</code> field, but no <code>clientID</code> and <code>clientSecret</code>.
Removing those fields from the secret of an existing Shoot will also let it adopt the managed service principal.</p><p>Based on the <code>tenantID</code> field, the Gardener extension will try to assign the managed service principal to the Shoot.
If no managed service principal can be assigned then the next operation on the Shoot will fail.</p><p>⚠️ The managed service principal need to be assigned to the users Azure subscription with proper permissions before using it.</p><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.</p><p>An example <code>InfrastructureConfig</code> for the Azure extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
kind: InfrastructureConfig
networks:
  vnet: <span style=color:green># specify either &#39;name&#39; and &#39;resourceGroup&#39; or &#39;cidr&#39;</span>
    <span style=color:green># name: my-vnet</span>
    <span style=color:green># resourceGroup: my-vnet-resource-group</span>
    cidr: 10.250.0.0/16
  workers: 10.250.0.0/19
  <span style=color:green># natGateway:</span>
  <span style=color:green>#   enabled: false</span>
  <span style=color:green>#   idleConnectionTimeoutMinutes: 4</span>
  <span style=color:green>#   zone: 1</span>
  <span style=color:green>#   ipAddresses:</span>
  <span style=color:green>#   - name: my-public-ip-name</span>
  <span style=color:green>#     resourceGroup: my-public-ip-resource-group</span>
  <span style=color:green>#     zone: 1</span>
  <span style=color:green># serviceEndpoints:</span>
  <span style=color:green># - Microsoft.Test</span>
  <span style=color:green># zones:</span>
  <span style=color:green># - name: 1</span>
  <span style=color:green>#   cidr: &#34;10.250.0.0/24</span>
  <span style=color:green># - name: 2</span>
  <span style=color:green>#   cidr: &#34;10.250.0.0/24&#34;</span>
  <span style=color:green>#   natGateway:</span>
  <span style=color:green>#     enabled: false</span>
zoned: <span style=color:#00f>false</span>
<span style=color:green># resourceGroup:</span>
<span style=color:green>#   name: mygroup</span>
<span style=color:green>#identity:</span>
<span style=color:green>#  name: my-identity-name</span>
<span style=color:green>#  resourceGroup: my-identity-resource-group</span>
<span style=color:green>#  acrAccess: true</span>
</code></pre></div><p>Currently, it&rsquo;s not yet possible to deploy into existing resource groups, but in the future it will.
The <code>.resourceGroup.name</code> field will allow specifying the name of an already existing resource group that the shoot cluster and all infrastructure resources will be deployed to.</p><p>Via the <code>.zoned</code> boolean you can tell whether you want to use Azure availability zones or not.
If you don&rsquo;t use zones then an availability set will be created and only basic load balancers will be used.
Zoned clusters use standard load balancers.</p><p>The <code>networks.vnet</code> section describes whether you want to create the shoot cluster in an already existing VNet or whether to create a new one:</p><ul><li>If <code>networks.vnet.name</code> and <code>networks.vnet.resourceGroup</code> are given then you have to specify the VNet name and VNet resource group name of the existing VNet that was created by other means (manually, other tooling, &mldr;).</li><li>If <code>networks.vnet.cidr</code> is given then you have to specify the VNet CIDR of a new VNet that will be created during shoot creation.
You can freely choose a private CIDR range.</li><li>Either <code>networks.vnet.name</code> and <code>neworks.vnet.resourceGroup</code> or <code>networks.vnet.cidr</code> must be present, but not both at the same time.</li></ul><p>The <code>networks.workers</code> section describes the CIDR for a subnet that is used for all shoot worker nodes, i.e., VMs which later run your applications.
The specified CIDR range must be contained in the VNet CIDR specified above, or the VNet CIDR of your already existing VNet.
You can freely choose this CIDR and it is your responsibility to properly design the network layout to suit your needs.</p><p>In the <code>networks.serviceEndpoints[]</code> list you can specify the list of Azure service endpoints which shall be associated with the worker subnet. All available service endpoints and their technical names can be found in the (Azure Service Endpoint documentation](<a href=https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-service-endpoints-overview>https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-service-endpoints-overview</a>).</p><p>The <code>networks.natGateway</code> section contains configuration for the Azure NatGateway which can be attached to the worker subnet of a Shoot cluster. Here are some key information about the usage of the NatGateway for a Shoot cluster:</p><ul><li>NatGateway usage is optional and can be enabled or disabled via <code>.networks.natGateway.enabled</code>.</li><li>If the NatGateway is not used then the egress connections initiated within the Shoot cluster will be nated via the LoadBalancer of the clusters (default Azure behaviour, see <a href=https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-outbound-connections#scenarios>here</a>).</li><li>NatGateway is only available for zonal clusters <code>.zoned=true</code>.</li><li>The NatGateway is currently <strong>not</strong> zone redundantly deployed. That mean the NatGateway of a Shoot cluster will always be in just one zone. This zone can be optionally selected via <code>.networks.natGateway.zone</code>.</li><li><strong>Caution:</strong> Modifying the <code>.networks.natGateway.zone</code> setting requires a recreation of the NatGateway and the managed public ip (automatically used if no own public ip is specified, see below). That mean you will most likely get a different public ip for egress connections.</li><li>It is possible to bring own zonal public ip(s) via <code>networks.natGateway.ipAddresses</code>. Those public ip(s) need to be in the same zone as the NatGateway (see <code>networks.natGateway.zone</code>) and be of SKU <code>standard</code>. For each public ip the <code>name</code>, the <code>resourceGroup</code> and the <code>zone</code> need to be specified.</li><li>The field <code>networks.natGateway.idleConnectionTimeoutMinutes</code> allows the configuration of NAT Gateway&rsquo;s idle connection timeout property. The idle timeout value can be adjusted from 4 minutes, up to 120 minutes. Omitting this property will set the idle timeout to its default value according to <a href=https://docs.microsoft.com/en-us/azure/virtual-network/nat-gateway-resource#timers>NAT Gateway&rsquo;s documentation</a>.</li></ul><p>In the <code>identity</code> section you can specify an <a href=https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview#how-does-the-managed-identities-for-azure-resources-work>Azure user-assigned managed identity</a> which should be attached to all cluster worker machines. With <code>identity.name</code> you can specify the name of the identity and with <code>identity.resourceGroup</code> you can specify the resource group which contains the identity resource on Azure. The identity need to be created by the user upfront (manually, other tooling, &mldr;). Gardener/Azure Extension will only use the referenced one and won&rsquo;t create an identity. Furthermore the identity have to be in the same subscription as the Shoot cluster. Via the <code>identity.acrAccess</code> you can configure the worker machines to use the passed identity for pulling from an <a href=https://docs.microsoft.com/en-us/azure/container-registry/container-registry-intro>Azure Container Registry (ACR)</a>.
<strong>Caution:</strong> Adding, exchanging or removing the identity will require a rolling update of all worker machines in the Shoot cluster.</p><p>Apart from the VNet and the worker subnet the Azure extension will also create a dedicated resource group, route tables, security groups, and an availability set (if not using zoned clusters).</p><h3 id=infrastructureconfig-with-dedicated-subnets-per-zone>InfrastructureConfig with dedicated subnets per zone</h3><p>Another deployment option <strong>for zonal clusters only</strong>, is to create and configure a separate subnet per availability zone. This network layout is recommended to users that require fine-grained control over their network setup. One prevalent usecase is to create a zone-redundant NAT Gateway deployment by taking advantage of the ability to deploy separate NAT Gateways for each subnet.</p><p>To use this configuration the following requirements must be met:</p><ul><li>the <code>zoned</code> field must be set to <code>true</code>.</li><li>the <code>networks.vnet</code> section must not be empty and must contain a valid configuration. For existing clusters that were not using the <code>networks.vnet</code> section, it is enough if <code>networks.vnet.cidr</code> field is set to the current <code>networks.worker</code> value.</li></ul><p>For each of the target zones a subnet CIDR range must be specified. The specified CIDR range must be contained in the VNet CIDR specified above, or the VNet CIDR of your already existing VNet. In addition, the CIDR ranges must not overlap with the ranges of the other subnets.</p><p><em>ServiceEndpoints</em> and <em>NatGateways</em> can be configured per subnet. Respectively, when <code>networks.zones</code> is specified, the fields <code>networks.workers</code>, <code>networks.serviceEndpoints</code> and <code>networks.natGateway</code> cannot be set. All the configuration for the subnets must be done inside the respective zone&rsquo;s configuration.</p><p>Example:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
kind: InfrastructureConfig
networks:
  zoned: <span style=color:#00f>true</span>
  vnet: <span style=color:green># specify either &#39;name&#39; and &#39;resourceGroup&#39; or &#39;cidr&#39;</span>
    cidr: 10.250.0.0/16
  zones:
  - name: 1
    cidr: <span style=color:#a31515>&#34;10.250.0.0/24&#34;</span>
  - name: 2
    cidr: <span style=color:#a31515>&#34;10.250.0.0/24&#34;</span>
    natGateway:
      enabled: <span style=color:#00f>false</span>
</code></pre></div><h3 id=migrating-to-zonal-shoots-with-dedicated-subnets-per-zone>Migrating to zonal shoots with dedicated subnets per zone</h3><p>For existing zonal clusters it is possible to migrate to a network layout with dedicated subnets per zone. The migration works by creating additional network resources as specified in the configuration and progressively roll part of your existing nodes to use the new resources. To achieve the controlled rollout of your nodes, parts of the existing infrastructure must be preserved which is why the following constraint is imposed:</p><p>One of your specified zones must have the exact same CIDR range as the current <code>network.workers</code> field. Here is an example of such migration:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>infrastructureConfig:
  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
  kind: InfrastructureConfig
  networks:
    vnet:
      cidr: 10.250.0.0/16
    workers: 10.250.0.0/19
  zoned: <span style=color:#00f>true</span>
</code></pre></div><p>to</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>infrastructureConfig:
  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
  kind: InfrastructureConfig
  networks:
    vnet:
      cidr: 10.250.0.0/16
    zones:
      - name: 3
        cidr: 10.250.0.0/19 <span style=color:green># note the preservation of the &#39;workers&#39; CIDR</span>
<span style=color:green># optionally add other zones </span>
    <span style=color:green># - name: 2  </span>
    <span style=color:green>#   cidr: 10.250.32.0/19</span>
    <span style=color:green>#   natGateway:</span>
    <span style=color:green>#     enabled: true</span>
  zoned: <span style=color:#00f>true</span>
</code></pre></div><p>Another more advanced example with user-provided public IP addresses for the NAT Gateway and how it can be migrated:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>infrastructureConfig:
  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
  kind: InfrastructureConfig
  networks:
    vnet:
      cidr: 10.250.0.0/16
    workers: 10.250.0.0/19
    natGateway:
      enabled: <span style=color:#00f>true</span>
      zone: 1
      ipAddresses:
        - name: pip1
          resourceGroup: group
          zone: 1
        - name: pip2
          resourceGroup: group
          zone: 1
  zoned: <span style=color:#00f>true</span>
</code></pre></div><p>to</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>infrastructureConfig:
  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
  kind: InfrastructureConfig
  zoned: <span style=color:#00f>true</span>
  networks:
    vnet:
      cidr: 10.250.0.0/16
    zones:
      - name: 1
        cidr: 10.250.0.0/19 <span style=color:green># note the preservation of the &#39;workers&#39; CIDR</span>
        natGateway:
          enabled: <span style=color:#00f>true</span>
          ipAddresses:
            - name: pip1
              resourceGroup: group
              zone: 1
            - name: pip2
              resourceGroup: group
              zone: 1
<span style=color:green># optionally add other zones </span>
<span style=color:green>#     - name: 2  </span>
<span style=color:green>#       cidr: 10.250.32.0/19</span>
<span style=color:green>#       natGateway:</span>
<span style=color:green>#         enabled: true</span>
<span style=color:green>#         ipAddresses:</span>
<span style=color:green>#           - name: pip3</span>
<span style=color:green>#             resourceGroup: group</span>
</code></pre></div><p>You can apply such change to your shoot by issuing a <code>kubectl patch</code> command to replace your current <code>.spec.provider.infrastructureConfig</code> section:</p><pre><code>$ cat new-infra.json

[
  {
    &quot;op&quot;: &quot;replace&quot;,
    &quot;path&quot;: &quot;/spec/provider/infrastructureConfig&quot;,
    &quot;value&quot;: {
      &quot;apiVersion&quot;: &quot;azure.provider.extensions.gardener.cloud/v1alpha1&quot;,
      &quot;kind&quot;: &quot;InfrastructureConfig&quot;,
      &quot;networks&quot;: {
        &quot;vnet&quot;: {
          &quot;cidr&quot;: &quot;&lt;your-vnet-cidr&gt;&quot;
        },
        &quot;zones&quot;: [
          {
            &quot;name&quot;: 1,
            &quot;cidr&quot;: &quot;10.250.0.0/24&quot;,
            &quot;natGateway&quot;: {
              &quot;enabled&quot;: true
            }
          },
          {
            &quot;name&quot;: 1,
            &quot;cidr&quot;: &quot;10.250.1.0/24&quot;,
            &quot;natGateway&quot;: {
              &quot;enabled&quot;: true
            }
          },
        ]
      },
      &quot;zoned&quot;: true
    }
  }
]

kubectl patch --type=&quot;json&quot; --patch-file new-infra.json shoot &lt;my-shoot&gt;
</code></pre><p>⚠️ The migration to shoots with dedicated subnets per zone is a one-way process. Reverting the shoot to the previous configuration is not supported.</p><p>⚠️ During the migration a subset of the nodes will be rolled to the new subnets.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the Azure-specific control plane components.
Today, the only component deployed by the Azure extension is the <code>cloud-controller-manager</code>.</p><p>An example <code>ControlPlaneConfig</code> for the Azure extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
kind: ControlPlaneConfig
cloudControllerManager:
  featureGates:
    CustomResourceValidation: <span style=color:#00f>true</span>
</code></pre></div><p>The <code>cloudControllerManager.featureGates</code> contains a map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><h2 id=workerconfig><code>WorkerConfig</code></h2><p>The Azure extension does not support a specific <code>WorkerConfig</code> yet, however, it supports encryption for volumes plus support for additional data volumes per machine.
Please note that you cannot specify the <code>encrypted</code> flag for Azure disks as they are encrypted by default/out-of-the-box.
For each data volume, you have to specify a name.
The following YAML is a snippet of a <code>Shoot</code> resource:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:
  provider:
    workers:
    - name: cpu-worker
      ...
      volume:
        type: Standard_LRS
        size: 20Gi
      dataVolumes:
      - name: kubelet-dir
        type: Standard_LRS
        size: 25Gi
</code></pre></div><h2 id=example-shoot-manifest-non-zoned>Example <code>Shoot</code> manifest (non-zoned)</h2><p>Please find below an example <code>Shoot</code> manifest for a non-zoned cluster:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: Shoot
metadata:
  name: johndoe-azure
  namespace: garden-dev
spec:
  cloudProfileName: azure
  region: westeurope
  secretBindingName: core-azure
  provider:
    type: azure
    infrastructureConfig:
      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
      kind: InfrastructureConfig
      networks:
        vnet:
          cidr: 10.250.0.0/16
        workers: 10.250.0.0/19
      zoned: <span style=color:#00f>false</span>
    controlPlaneConfig:
      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
      kind: ControlPlaneConfig
    workers:
    - name: worker-xoluy
      machine:
        type: Standard_D4_v3
      minimum: 2
      maximum: 2
      volume:
        size: 50Gi
        type: Standard_LRS
  networking:
    type: calico
    pods: 100.96.0.0/11
    nodes: 10.250.0.0/16
    services: 100.64.0.0/13
  kubernetes:
    version: 1.16.1
  maintenance:
    autoUpdate:
      kubernetesVersion: <span style=color:#00f>true</span>
      machineImageVersion: <span style=color:#00f>true</span>
  addons:
    kubernetesDashboard:
      enabled: <span style=color:#00f>true</span>
    nginxIngress:
      enabled: <span style=color:#00f>true</span>
</code></pre></div><h2 id=example-shoot-manifest-zoned>Example <code>Shoot</code> manifest (zoned)</h2><p>Please find below an example <code>Shoot</code> manifest for a zoned cluster:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: Shoot
metadata:
  name: johndoe-azure
  namespace: garden-dev
spec:
  cloudProfileName: azure
  region: westeurope
  secretBindingName: core-azure
  provider:
    type: azure
    infrastructureConfig:
      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
      kind: InfrastructureConfig
      networks:
        vnet:
          cidr: 10.250.0.0/16
        workers: 10.250.0.0/19
      zoned: <span style=color:#00f>true</span>
    controlPlaneConfig:
      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
      kind: ControlPlaneConfig
    workers:
    - name: worker-xoluy
      machine:
        type: Standard_D4_v3
      minimum: 2
      maximum: 2
      volume:
        size: 50Gi
        type: Standard_LRS
      zones:
      - <span style=color:#a31515>&#34;1&#34;</span>
      - <span style=color:#a31515>&#34;2&#34;</span>
  networking:
    type: calico
    pods: 100.96.0.0/11
    nodes: 10.250.0.0/16
    services: 100.64.0.0/13
  kubernetes:
    version: 1.16.1
  maintenance:
    autoUpdate:
      kubernetesVersion: <span style=color:#00f>true</span>
      machineImageVersion: <span style=color:#00f>true</span>
  addons:
    kubernetesDashboard:
      enabled: <span style=color:#00f>true</span>
    nginxIngress:
      enabled: <span style=color:#00f>true</span>
</code></pre></div><h2 id=example-shoot-manifest-zoned-with-nat-gateways-per-zone>Example <code>Shoot</code> manifest (zoned with NAT Gateways per zone)</h2><p>Please find below an example <code>Shoot</code> manifest for a zoned cluster using NAT Gateways per zone:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: Shoot
metadata:
  name: johndoe-azure
  namespace: garden-dev
spec:
  cloudProfileName: azure
  region: westeurope
  secretBindingName: core-azure
  provider:
    type: azure
    infrastructureConfig:
      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
      kind: InfrastructureConfig
      networks:
        vnet:
          cidr: 10.250.0.0/16
        zones:
        - name: 1
          cidr: 10.250.0.0/24
          serviceEndpoints:
          - Microsoft.Storage
          - Microsoft.Sql
          natGateway:
            enabled: <span style=color:#00f>true</span>
            idleConnectionTimeoutMinutes: 4
        - name: 2
          cidr: 10.250.1.0/24
          serviceEndpoints:
          - Microsoft.Storage
          - Microsoft.Sql
          natGateway:
            enabled: <span style=color:#00f>true</span>
      zoned: <span style=color:#00f>true</span>
    controlPlaneConfig:
      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
      kind: ControlPlaneConfig
    workers:
    - name: worker-xoluy
      machine:
        type: Standard_D4_v3
      minimum: 2
      maximum: 2
      volume:
        size: 50Gi
        type: Standard_LRS
      zones:
      - <span style=color:#a31515>&#34;1&#34;</span>
      - <span style=color:#a31515>&#34;2&#34;</span>
  networking:
    type: calico
    pods: 100.96.0.0/11
    nodes: 10.250.0.0/16
    services: 100.64.0.0/13
  kubernetes:
    version: 1.16.1
  maintenance:
    autoUpdate:
      kubernetesVersion: <span style=color:#00f>true</span>
      machineImageVersion: <span style=color:#00f>true</span>
  addons:
    kubernetesDashboard:
      enabled: <span style=color:#00f>true</span>
    nginxIngress:
      enabled: <span style=color:#00f>true</span>
</code></pre></div><h2 id=csi-volume-provisioners>CSI volume provisioners</h2><p>Every Azure shoot cluster that has at least Kubernetes v1.21 will be deployed with the Azure Disk CSI driver and the Azure File CSI driver.
Both are compatible with the legacy in-tree volume provisioners that were deprecated by the Kubernetes community and will be removed in future versions of Kubernetes.
End-users might want to update their custom <code>StorageClass</code>es to the new <code>disk.csi.azure.com</code> or <code>file.csi.azure.com</code> provisioner, respectively.
Shoot clusters with Kubernetes v1.20 or less will use the in-tree <code>kubernetes.io/azure-disk</code> and <code>kubernetes.io/azure-file</code> volume provisioners in the kube-controller-manager and the kubelet.</p><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-azure@v1.25</code>.
Note that this feature is only usable for <code>Shoot</code>s whose <code>.spec.kubernetes.version</code> is greater or equal than the CSI migration version (<code>1.21</code>).</p><h2 id=miscellaneous>Miscellaneous</h2><h3 id=azure-accelerated-networking>Azure Accelerated Networking</h3><p>All worker machines of the cluster will be automatically configured to use <a href=https://docs.microsoft.com/en-us/azure/virtual-network/create-vm-accelerated-networking-cli>Azure Accelerated Networking</a> if the prerequisites are fulfilled.
The prerequisites are that the cluster must be zoned, and the used machine type and operating system image version are compatible for Accelerated Networking.
<code>Availability Set</code> based shoot clusters will not be enabled for accelerated networking even if the machine type and operating system support it, this is necessary because all machines from the availability set must be scheduled on special hardware, more daitls can be found <a href=https://github.com/MicrosoftDocs/azure-docs/issues/10536>here</a>.
Supported machine types are listed in the CloudProfile in <code>.spec.providerConfig.machineTypes[].acceleratedNetworking</code> and the supported operating system image versions are defined in <code>.spec.providerConfig.machineImages[].versions[].acceleratedNetworking</code>.</p><h3 id=preview-shoot-clusters-with-vmss-flexible-orchestration-vmss-flexvmo>Preview: Shoot clusters with VMSS Flexible Orchestration (VMSS Flex/VMO)</h3><p>The machines of an Azure cluster can be created while being attached to an <a href=https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-orchestration-modes#scale-sets-with-flexible-orchestration>Azure Virtual Machine ScaleSet with flexible orchestraion</a>.
The Virtual Machine ScaleSet with flexible orchestration feature is currently in preview and not yet general available on Azure.
Subscriptions need to <a href=https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-orchestration-modes#register-for-flexible-orchestration-mode>join the preview</a> to make use of the feature.</p><p>Azure VMSS Flex is intended to replace Azure AvailabilitySet for non-zoned Azure Shoot clusters in the mid-term (once the feature goes GA) as VMSS Flex come with less disadvantages like no blocking machine operations or compability with <code>Standard</code> SKU loadbalancer etc.</p><p>To configure an Azure Shoot cluster which make use of VMSS Flex you need to do the following:</p><ul><li>The <code>InfrastructureConfig</code> of the Shoot configuration need to contain <code>.zoned=false</code></li><li>Shoot resource need to have the following annotation assigned: <code>alpha.azure.provider.extensions.gardener.cloud/vmo=true</code></li></ul><p>Some key facts about VMSS Flex based clusters:</p><ul><li>Unlike regular non-zonal Azure Shoot clusters, which have a primary AvailabilitySet which is shared between all machines in all worker pools of a Shoot cluster, a VMSS Flex based cluster has an own VMSS for each workerpool</li><li>In case the configuration of the VMSS will change (e.g. amount of fault domains in a region change; configured in the CloudProfile) all machines of the worker pool need to be rolled</li><li>It is not possible to migrate an existing primary AvailabilitySet based Shoot cluster to VMSS Flex based Shoot cluster and vice versa</li><li>VMSS Flex based clusters are using <code>Standard</code> SKU LoadBalancers instead of <code>Basic</code> SKU LoadBalancers for AvailabilitySet based Shoot clusters</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-73b88a2c7d57533dd8df2fa1ec7d8e11>3.5 - Usage As Operator</h1><h1 id=using-the-azure-provider-extension-with-gardener-as-an-operator>Using the Azure provider extension with Gardener as an operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.
The <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml><code>core.gardener.cloud/v1beta1.Seed</code> resource</a> is structured similarly.
Additionally, it allows configuring settings for the backups of the main etcds' data of shoot clusters control planes running in this seed cluster.</p><p>This document explains the necessary configuration for the Azure provider extension.</p><h2 id=cloudprofile-resource><code>CloudProfile</code> resource</h2><p>This section describes, how the configuration for <code>CloudProfile</code>s looks like for Azure by providing an example <code>CloudProfile</code> manifest with minimal configuration that can be used to allow the creation of Azure shoot clusters.</p><h3 id=cloudprofileconfig><code>CloudProfileConfig</code></h3><p>The cloud profile configuration contains information about the real machine image IDs in the Azure environment (image <code>urn</code> or <code>id</code>).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> to an available VM image in your subscription.
The VM can be from the <a href="https://azuremarketplace.microsoft.com/en-us/marketplace/apps?filters=virtual-machine-images">Azure Marketplace</a> identified via an <code>urn</code>
or a custom VM image identified by <code>id</code> from a shared image gallery.</p><p>An example <code>CloudProfileConfig</code> for the Azure extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
kind: CloudProfileConfig
countUpdateDomains:
- region: westeurope
  count: 5
countFaultDomains:
- region: westeurope
  count: 3
machineTypes:
- name: Standard_D3_v2
  acceleratedNetworking: <span style=color:#00f>true</span>
- name: Standard_X
machineImages:
- name: coreos
  versions:
  - version: 2135.6.0
    urn: <span style=color:#a31515>&#34;CoreOS:CoreOS:Stable:2135.6.0&#34;</span>
    acceleratedNetworking: <span style=color:#00f>true</span>
- name: myimage
  versions:
  - version: 1.0.0
    id: <span style=color:#a31515>&#34;/subscriptions/&lt;subscription ID where the gallery is located&gt;/resourceGroups/myGalleryRG/providers/Microsoft.Compute/galleries/myGallery/images/myImageDefinition/versions/1.0.0&#34;</span>
</code></pre></div><p>The cloud profile configuration contains information about the update via <code>.countUpdateDomains[]</code> and failure domain via <code>.countFaultDomains[]</code> counts in the Azure regions you want to offer.</p><p>The <code>.machineTypes[]</code> list contain provider specific information to the machine types e.g. if the machine type support <a href=https://docs.microsoft.com/en-us/azure/virtual-network/create-vm-accelerated-networking-cli>Azure Accelerated Networking</a>, see <code>.machineTypes[].acceleratedNetworking</code>.</p><p>Additionally, it contains the real machine image identifiers in the Azure environment. You can provide either URN for Azure Market Place images or id of <a href=https://docs.microsoft.com/en-us/azure/virtual-machines/linux/shared-image-galleries>Shared Image Gallery</a> images.
When Shared Image Gallery is used, you have to ensure that the image is available in the desired regions and the end-user subscriptions have access to the image or to the whole gallery.
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the Azure extension knows the machine image identifiers for every version you want to offer.
Furthermore, you can specify for each image version via <code>.machineImages[].versions[].acceleratedNetworking</code> if Azure Accelerated Networking is supported.</p><h3 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h3><p>The possible values for <code>.spec.volumeTypes[].name</code> on Azure are <code>Standard_LRS</code>, <code>StandardSSD_LRS</code> and <code>Premium_LRS</code>. There is another volume type called <code>UltraSSD_LRS</code> but this type is not supported to use as os disk. If an end user select a volume type whose name is not equal to one of the valid values then the machine will be created with the default volume type which belong to the selected machine type. Therefore it is recommended to configure only the valid values for the <code>.spec.volumeType[].name</code> in the <code>CloudProfile</code>.</p><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: CloudProfile
metadata:
  name: azure
spec:
  type: azure
  kubernetes:
    versions:
    - version: 1.16.1
    - version: 1.16.0
      expirationDate: <span style=color:#a31515>&#34;2020-04-05T01:02:03Z&#34;</span>
  machineImages:
  - name: coreos
    versions:
    - version: 2135.6.0
  machineTypes:
  - name: Standard_D3_v2
    cpu: <span style=color:#a31515>&#34;4&#34;</span>
    gpu: <span style=color:#a31515>&#34;0&#34;</span>
    memory: 14Gi
  - name: Standard_D4_v3
    cpu: <span style=color:#a31515>&#34;4&#34;</span>
    gpu: <span style=color:#a31515>&#34;0&#34;</span>
    memory: 16Gi
  volumeTypes:
  - name: Standard_LRS
    class: standard
    usable: <span style=color:#00f>true</span>
  - name: StandardSSD_LRS
    class: premium
    usable: <span style=color:#00f>false</span>
  - name: Premium_LRS
    class: premium
    usable: <span style=color:#00f>false</span>
  regions:
  - name: westeurope
  providerConfig:
    apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
    kind: CloudProfileConfig
    machineTypes:
    - name: Standard_D3_v2
      acceleratedNetworking: <span style=color:#00f>true</span>
    - name: Standard_D4_v3
    countUpdateDomains:
    - region: westeurope
      count: 5
    countFaultDomains:
    - region: westeurope
      count: 3
    machineImages:
    - name: coreos
      versions:
      - version: 2303.3.0
        urn: CoreOS:CoreOS:Stable:2303.3.0
        acceleratedNetworking: <span style=color:#00f>true</span>
      - version: 2135.6.0
        urn: <span style=color:#a31515>&#34;CoreOS:CoreOS:Stable:2135.6.0&#34;</span>
</code></pre></div><h2 id=seed-resource><code>Seed</code> resource</h2><p>This provider extension does not support any provider configuration for the <code>Seed</code>&rsquo;s <code>.spec.provider.providerConfig</code> field.
However, it supports managing of backup infrastructure, i.e., you can specify a configuration for the <code>.spec.backup</code> field.</p><h3 id=backup-configuration>Backup configuration</h3><p>A Seed of type <code>azure</code> can be configured to perform backups for the main etcds' of the shoot clusters control planes using Azure Blob storage.</p><p>The location/region where the backups will be stored defaults to the region of the Seed (<code>spec.provider.region</code>), but can also be explicitly configured via the field <code>spec.backup.region</code>.
The region of the backup can be different from where the Seed cluster is running.
However, usually it makes sense to pick the same region for the backup bucket as used for the Seed cluster.</p><p>Please find below an example <code>Seed</code> manifest (partly) that configures backups using Azure Blob storage.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>---
apiVersion: core.gardener.cloud/v1beta1
kind: Seed
metadata:
  name: my-seed
spec:
  provider:
    type: azure
    region: westeurope
  backup:
    provider: azure
    region: westeurope <span style=color:green># default region</span>
    secretRef:
      name: backup-credentials
      namespace: garden
  ...
</code></pre></div><p>The referenced secret has to contain the provider credentials of the Azure subscription.
Please take a look <a href=https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal>here</a> on how to create an Azure Application, Service Principle and how to obtain credentials.
The example below demonstrates how the secret has to look like.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Secret
metadata:
  name: core-azure
  namespace: garden-dev
type: Opaque
data:
  clientID: base64(client-id)
  clientSecret: base64(client-secret)
  subscriptionID: base64(subscription-id)
  tenantID: base64(tenant-id)
</code></pre></div><h4 id=permissions-for-azure-blob-storage>Permissions for Azure Blob storage</h4><p>Please make sure the Azure application has the following IAM roles.</p><ul><li><a href=https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#contributor>Contributor</a></li></ul><h2 id=miscellaneous>Miscellaneous</h2><h3 id=gardener-managed-service-principals>Gardener managed Service Principals</h3><p>The operators of the Gardener Azure extension can provide a list of managed service principals (technical users) that can be used for Azure Shoots.
This eliminates the need for users to provide own service principals for their clusters.</p><p>The user would need to grant the managed service principal access to their subscription with proper permissions.</p><p>As service principals are managed in an Azure Active Directory for each supported Active Directory, an own service principal needs to be provided.</p><p>In case the user provides an own service principal in the Shoot secret, this one will be used instead of the managed one provided by the operator.</p><p>Each managed service principal will be maintained in a <code>Secret</code> like that:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Secret
metadata:
  name: service-principal-my-tenant
  namespace: extension-provider-azure
  labels:
    azure.provider.extensions.gardener.cloud/purpose: tenant-service-principal-secret
data:
  tenantID: base64(my-tenant)
  clientID: base64(my-service-princiapl-id)
  clientSecret: base64(my-service-princiapl-secret)
type: Opaque
</code></pre></div><p>The user needs to provide in its Shoot secret a <code>tenantID</code> and <code>subscriptionID</code>.</p><p>The managed service principal will be assigned based on the <code>tenantID</code>.
In case there is a managed service principal secret with a matching <code>tenantID</code>, this one will be used for the Shoot.
If there is no matching managed service principal secret then the next Shoot operation will fail.</p><p>One of the benefits of having managed service principals is that the operator controls the lifecycle of the service principal and can rotate its secrets.</p><p>After the service principal secret has been rotated and the corresponding secret is updated, all Shoot clusters using it need to be reconciled or the last operation to be retried.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b182a904236beecf338898551c7afb28>4 - Provider Equinix Metal</h1><div class=lead>Gardener extension controller for the Equinix Metal cloud provider</div><h1 id=gardener-extension-for-equinix-metal-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Equinix Metal provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-equinix-metal-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-equinix-metal-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-equinix-metal><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-equinix-metal alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=/docs/gardener/proposals/01-extensibility/>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the Equinix Metal provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-equinix-metal/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=/docs/gardener/proposals/01-extensibility/>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.23</td><td>untested</td><td>N/A</td></tr><tr><td>Kubernetes 1.22</td><td>untested</td><td>N/A</td></tr><tr><td>Kubernetes 1.21</td><td>untested</td><td>N/A</td></tr><tr><td>Kubernetes 1.20</td><td>untested</td><td>N/A</td></tr><tr><td>Kubernetes 1.19</td><td>untested</td><td>N/A</td></tr><tr><td>Kubernetes 1.18</td><td>untested</td><td>N/A</td></tr><tr><td>Kubernetes 1.17</td><td>untested</td><td>N/A</td></tr><tr><td>Kubernetes 1.16</td><td>1.16.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.15</td><td>1.15.0+</td><td>N/A</td></tr></tbody></table><p>Please take a look <a href=/docs/gardener/usage/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-equinix-metal/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=/docs/gardener/proposals/01-extensibility/>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=/docs/gardener/proposals/04-new-core-gardener-cloud-apis/>GEP-4 (New <code>core.gardener.cloud/v1alpha1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e44d3a514e91e1f451fbd09c9ff8af29>4.1 - Usage As End User</h1><h1 id=using-the-equinix-metal-provider-extension-with-gardener-as-end-user>Using the Equinix Metal provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for Equinix Metal and provide an example <code>Shoot</code> manifest with minimal configuration that you can use to create an Equinix Metal cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=provider-secret-data>Provider secret data</h2><p>Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of your Equinix Metal project.
This <code>Secret</code> must look as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Secret
metadata:
  name: my-secret
  namespace: garden-dev
type: Opaque
data:
  apiToken: base64(api-token)
  projectID: base64(project-id)
</code></pre></div><p>Please look up <a href=https://metal.equinix.com/developers/api/>https://metal.equinix.com/developers/api/</a> as well.</p><p>With <code>Secret</code> created, create a <code>SecretBinding</code> resource referencing it. It may look like this:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: SecretBinding
metadata:
  name: my-secret
  namespace: garden-dev
secretRef:
  name: my-secret
quotas: []
</code></pre></div><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>Currently, there is no infrastructure configuration possible for the Equinix Metal environment.</p><p>An example <code>InfrastructureConfig</code> for the Equinix Metal extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
kind: InfrastructureConfig
</code></pre></div><p>The Equinix Metal extension will only create a key pair.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the Equinix Metal-specific control plane components.
Today, the Equinix Metal extension deploys the <code>cloud-controller-manager</code> and the CSI controllers, however, it doesn&rsquo;t offer any configuration options at the moment.</p><p>An example <code>ControlPlaneConfig</code> for the Equinix Metal extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
kind: ControlPlaneConfig
</code></pre></div><h2 id=workerconfig><code>WorkerConfig</code></h2><p>The Equinix Metal extension supports specifying IDs for reserved devices that should be used for the machines of a specific worker pool.</p><p>An example <code>WorkerConfig</code> for the Equinix Metal extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
kind: WorkerConfig
reservationIDs:
- my-reserved-device-1
- my-reserved-device-2
reservedDevicesOnly: <span style=color:#00f>false</span>
</code></pre></div><p>The <code>.reservationIDs[]</code> list contains the list of IDs of the reserved devices.
The <code>.reservedDevicesOnly</code> field indicates whether only reserved devices from the provided list of reservation IDs should be used when new machines are created.
It always will attempt to create a device from one of the reservation IDs.
If none is available, the behaviour depends on the setting:</p><ul><li><code>true</code>: return an error</li><li><code>false</code>: request a regular on-demand device</li></ul><p>The default value is <code>false</code>.</p><h2 id=example-shoot-manifest>Example <code>Shoot</code> manifest</h2><p>Please find below an example <code>Shoot</code> manifest:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1alpha1
kind: Shoot
metadata:
  name: my-shoot
  namespace: garden-dev
spec:
  cloudProfileName: equinix-metal
  region: ny <span style=color:green># Corresponds to a metro</span>
  secretBindingName: my-secret
  provider:
    type: equinixmetal
    infrastructureConfig:
      apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
      kind: InfrastructureConfig
    controlPlaneConfig:
      apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
      kind: ControlPlaneConfig
    workers:
    - name: worker-pool1
      minimum: 2
      maximum: 2
      volume:
        size: 50Gi
        type: storage_1
      zones: <span style=color:green># Optional list of facilities, all of which MUST be in the metro; if not provided, then random facilities within the metro will be chosen for each machine.</span>
      - ewr1
      - ny5
    - name: reserved-pool
      machine:
        type: t1.small
      minimum: 1
      maximum: 2
      providerConfig:
        apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
        kind: WorkerConfig
        reservationIDs:
        - reserved-device1
        - reserved-device2
        reservedDevicesOnly: <span style=color:#00f>true</span>
      volume:
        size: 50Gi
        type: storage_1
  networking:
    type: calico
  kubernetes:
    version: 1.20.2
  maintenance:
    autoUpdate:
      kubernetesVersion: <span style=color:#00f>true</span>
      machineImageVersion: <span style=color:#00f>true</span>
  addons:
    kubernetes-dashboard:
      enabled: <span style=color:#00f>true</span>
    nginx-ingress:
      enabled: <span style=color:#00f>true</span>
</code></pre></div><p>⚠️ Note that if you specify multiple facilities in the <code>.spec.provider.workers[].zones[]</code> list then new machines are randomly created in one of the provided facilities.
Particularly, it is not ensured that all facilities are used or that all machines are equally or unequally distributed.</p><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-equinix-metal@v2.2</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7d4a62928e787b14269281e239057e96>4.2 - Usage As Operator</h1><h1 id=using-the-equinix-metal-provider-extension-with-gardener-as-operator>Using the Equinix Metal provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1alpha1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for Equinix Metal and provide an example <code>CloudProfile</code> manifest with minimal configuration that you can use to allow creating Equinix Metal shoot clusters.</p><h2 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h2><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: CloudProfile
metadata:
  name: equinix-metal
spec:
  type: equinixmetal
  kubernetes:
    versions:
    - version: 1.20.2
    - version: 1.19.7
    - version: 1.18.15
      <span style=color:green>#expirationDate: &#34;2020-04-05T01:02:03Z&#34;</span>
  machineImages:
  - name: flatcar
    versions:
    - version: 0.0.0-stable
  machineTypes:
  - name: t1.small
    cpu: <span style=color:#a31515>&#34;4&#34;</span>
    gpu: <span style=color:#a31515>&#34;0&#34;</span>
    memory: 8Gi
    usable: <span style=color:#00f>true</span>
  regions: <span style=color:green># List of offered metros</span>
  - name: ny
    zones: <span style=color:green># List of offered facilities within the respective metro</span>
    - name: ewr1
    - name: ny5
    - name: ny7
  providerConfig:
    apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
    kind: CloudProfileConfig
    machineImages:
    - name: flatcar
      versions:
      - version: 0.0.0-stable
        id: flatcar_stable
</code></pre></div><h2 id=cloudprofileconfig><code>CloudProfileConfig</code></h2><p>The cloud profile configuration contains information about the real machine image IDs in the Equinix Metal environment (IDs).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the Equinix Metal extension knows the ID for every version you want to offer.</p><p>An example <code>CloudProfileConfig</code> for the Equinix Metal extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
kind: CloudProfileConfig
machineImages:
- name: flatcar
  versions:
  - version: 0.0.0-stable
    id: flatcar_stable
</code></pre></div><blockquote><p>NOTE: <code>CloudProfileConfig</code> is not a Custom Resource, so you cannot create it directly.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-3958d75f5afea52ba6f339a00c82ee93>5 - Provider GCP</h1><div class=lead>Gardener extension controller for the GCP cloud provider</div><h1 id=gardener-extension-for-gcp-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for GCP provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-gcp-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-gcp-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-gcp><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-gcp alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=/docs/gardener/proposals/01-extensibility/>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the GCP provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=/docs/gardener/proposals/01-extensibility/>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.23</td><td>1.23.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.22</td><td>1.22.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.22%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.22%20GCE/tests_status?style=svg" alt="Gardener v1.22 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.21</td><td>1.21.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.21%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.21%20GCE/tests_status?style=svg" alt="Gardener v1.21 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.20</td><td>1.20.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.20%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.20%20GCE/tests_status?style=svg" alt="Gardener v1.20 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.19</td><td>1.19.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.19%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.19%20GCE/tests_status?style=svg" alt="Gardener v1.19 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.18</td><td>1.18.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.18%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.18%20GCE/tests_status?style=svg" alt="Gardener v1.18 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.17</td><td>1.17.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.17%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.17%20GCE/tests_status?style=svg" alt="Gardener v1.17 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.16</td><td>1.16.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.16%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.16%20GCE/tests_status?style=svg" alt="Gardener v1.16 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.15</td><td>1.15.0+</td><td>[1]</td></tr></tbody></table><p>[1] Conformance tests are still executed and validated, unfortunately <a href=https://github.com/kubernetes/test-infra/pull/18509#issuecomment-668204180>no longer shown in TestGrid</a>.</p><p>Please take a look <a href=/docs/gardener/usage/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-gcp/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=/docs/gardener/proposals/01-extensibility/>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=/docs/gardener/proposals/04-new-core-gardener-cloud-apis/>GEP-4 (New <code>core.gardener.cloud/v1alpha1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e4646c279fbcc1d9ba0ae7b2f8d3e9ea>5.1 - Deployment</h1><h1 id=deployment-of-the-gcp-provider-extension>Deployment of the GCP provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the GCP provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the GCP provider extension <a href=https://github.com/gardener/gardener-extension-provider-gcp>repository</a>.</p><h2 id=gardener-extension-admission-gcp>gardener-extension-admission-gcp</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-admission-gcp</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: &lt;CA-DATA&gt;
    server: https://default.kubernetes.svc.cluster.local
  name: garden
contexts:
- context:
    cluster: garden
    user: garden
  name: garden
current-context: garden
users:
- name: garden
  user:
    tokenFile: /var/run/secrets/projected/serviceaccount/token
</code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: &lt;CA-DATA&gt;
    server: https://virtual-garden.api
  name: virtual-garden
contexts:
- context:
    cluster: virtual-garden
    user: virtual-garden
  name: virtual-garden
current-context: virtual-garden
users:
- name: virtual-garden
  user:
    tokenFile: /var/run/secrets/projected/serviceaccount/token
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-26955d31b03a63ff367474277c84b6a2>5.2 - Local Setup</h1><h3 id=admission-gcp>admission-gcp</h3><p><code>admission-gcp</code> is an admission webhook server which is responsible for the validation of the cloud provider (GCP in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/development/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>make start-admission
</code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-gcp.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>./hack/dev-setup-admission-gcp.sh
</code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-gcp</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-1e2221f3be40cdc59da201d0e7877323>5.3 - Usage As End User</h1><h1 id=using-the-gcp-provider-extension-with-gardener-as-end-user>Using the GCP provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>This document describes the configurable options for GCP and provides an example <code>Shoot</code> manifest with minimal configuration that can be used to create a GCP cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=gcp-provider-credentials>GCP Provider Credentials</h2><p>In order for Gardener to create a Kubernetes cluster using GCP infrastructure components, a Shoot has to provide credentials with sufficient permissions to the desired GCP project.
Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of the GCP project.
The <code>SecretBinding</code> is configurable in the <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>Shoot cluster</a> with the field <code>secretBindingName</code>.</p><p>The required credentials for the GCP project are a <a href=https://cloud.google.com/iam/docs/service-accounts#service_account_keys>Service Account Key</a> to authenticate as a <a href=https://cloud.google.com/compute/docs/access/service-accounts>GCP Service Account</a>.
A service account is a special account that can be used by services and applications to interact with Google Cloud Platform APIs.
Applications can use service account credentials to authorize themselves to a set of APIs and perform actions within the permissions granted to the service account.</p><p>Make sure to <a href=https://cloud.google.com/service-usage/docs/enable-disable>enable the Google Identity and Access Management (IAM) API</a>.
<a href=https://cloud.google.com/iam/docs/creating-managing-service-accounts>Create a Service Account</a> that shall be used for the Shoot cluster.
<a href=https://cloud.google.com/iam/docs/granting-changing-revoking-access>Grant at least the following IAM roles</a> to the Service Account.</p><ul><li>Service Account Admin</li><li>Service Account Token Creator</li><li>Service Account User</li><li>Compute Admin</li></ul><p>Create a <a href=https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating_service_account_keys>JSON Service Account key</a> for the Service Account.
Provide it in the <code>Secret</code> (base64 encoded for field <code>serviceaccount.json</code>), that is being referenced by the <code>SecretBinding</code> in the Shoot cluster configuration.</p><p>This <code>Secret</code> must look as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Secret
metadata:
  name: core-gcp
  namespace: garden-dev
type: Opaque
data:
  serviceaccount.json: base64(serviceaccount-json)
</code></pre></div><p>⚠️ Depending on your API usage it can be problematic to reuse the same Service Account Key for different Shoot clusters due to rate limits.
Please consider spreading your Shoots over multiple Service Accounts on different GCP projects if you are hitting those limits, see <a href=https://cloud.google.com/compute/docs/api-rate-limits>https://cloud.google.com/compute/docs/api-rate-limits</a>.</p><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.</p><p>An example <code>InfrastructureConfig</code> for the GCP extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
kind: InfrastructureConfig
networks:
<span style=color:green># vpc:</span>
<span style=color:green>#   name: my-vpc</span>
<span style=color:green>#   cloudRouter:</span>
<span style=color:green>#     name: my-cloudrouter</span>
  workers: 10.250.0.0/16
<span style=color:green># internal: 10.251.0.0/16</span>
<span style=color:green># cloudNAT:</span>
<span style=color:green>#   minPortsPerVM: 2048</span>
<span style=color:green>#   natIPNames:</span>
<span style=color:green>#   - name: manualnat1</span>
<span style=color:green>#   - name: manualnat2</span>
<span style=color:green># flowLogs:</span>
<span style=color:green>#   aggregationInterval: INTERVAL_5_SEC</span>
<span style=color:green>#   flowSampling: 0.2</span>
<span style=color:green>#   metadata: INCLUDE_ALL_METADATA</span>
</code></pre></div><p>The <code>networks.vpc</code> section describes whether you want to create the shoot cluster in an already existing VPC or whether to create a new one:</p><ul><li><p>If <code>networks.vpc.name</code> is given then you have to specify the VPC name of the existing VPC that was created by other means (manually, other tooling, &mldr;).
If you want to get a fresh VPC for the shoot then just omit the <code>networks.vpc</code> field.</p></li><li><p>If a VPC name is not given then we will create the cloud router + NAT gateway to ensure that worker nodes don&rsquo;t get external IPs.</p></li><li><p>If a VPC name is given then a cloud router name must also be given, failure to do so would result in validation errors
and possibly clusters without egress connectivity.</p></li></ul><p>The <code>networks.workers</code> section describes the CIDR for a subnet that is used for all shoot worker nodes, i.e., VMs which later run your applications.</p><p>The <code>networks.internal</code> section is optional and can describe a CIDR for a subnet that is used for <a href=https://cloud.google.com/load-balancing/docs/internal/>internal load balancers</a>,</p><p>The <code>networks.cloudNAT.minPortsPerVM</code> is optional and is used to define the <a href=https://cloud.google.com/nat/docs/overview#number_of_nat_ports_and_connections>minimum number of ports allocated to a VM for the CloudNAT</a></p><p>The <code>networks.cloudNAT.natIPNames</code> is optional and is used to specify the names of the manual ip addresses which should be used by the nat gateway</p><p>The specified CIDR ranges must be contained in the VPC CIDR specified above, or the VPC CIDR of your already existing VPC.
You can freely choose these CIDRs and it is your responsibility to properly design the network layout to suit your needs.</p><p>The <code>networks.flowLogs</code> section describes the configuration for the VPC flow logs. In order to enable the VPC flow logs at least one of the following parameters needs to be specified in the flow log section:</p><ul><li><p><code>networks.flowLogs.aggregationInterval</code> an optional parameter describing the aggregation interval for collecting flow logs. For more details, see <a href=https://www.terraform.io/docs/providers/google/r/compute_subnetwork.html#aggregation_interval>aggregation_interval reference</a>.</p></li><li><p><code>networks.flowLogs.flowSampling</code> an optional parameter describing the sampling rate of VPC flow logs within the subnetwork where 1.0 means all collected logs are reported and 0.0 means no logs are reported. For more details, see <a href=https://www.terraform.io/docs/providers/google/r/compute_subnetwork.html#flow_sampling>flow_sampling reference</a>.</p></li><li><p><code>networks.flowLogs.metadata</code> an optional parameter describing whether metadata fields should be added to the reported VPC flow logs. For more details, see <a href=https://www.terraform.io/docs/providers/google/r/compute_subnetwork.html#metadata>metadata reference</a>.</p></li></ul><p>Apart from the VPC and the subnets the GCP extension will also create a dedicated service account for this shoot, and firewall rules.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the GCP-specific control plane components.
Today, the only component deployed by the GCP extension is the <code>cloud-controller-manager</code>.</p><p>An example <code>ControlPlaneConfig</code> for the GCP extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
kind: ControlPlaneConfig
zone: europe-west1-b
cloudControllerManager:
  featureGates:
    CustomResourceValidation: <span style=color:#00f>true</span>
</code></pre></div><p>The <code>zone</code> field tells the cloud-controller-manager in which zone it should mainly operate.
You can still create clusters in multiple availability zones, however, the cloud-controller-manager requires one &ldquo;main&rdquo; zone.
⚠️ You always have to specify this field!</p><p>The <code>cloudControllerManager.featureGates</code> contains a map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><h2 id=workerconfig>WorkerConfig</h2><p>Multiple zones can be configured for a worker group of a GCP Shoot. The minimum number of machines in every worker group should be equal to or greater than the number of zones configured for that worker-group.</p><blockquote><p>The predicate is - A worker group with N zones configured should have minimum N machines.</p></blockquote><p>⚠️ This is important because, as of today, Cluster Autoscaler does not support scale-from-zero on GCP.</p><p>The following YAML is a snippet of a <code>Shoot</code> resource:</p><pre><code>workers:
  - name: worker-vezh0
    machine:
      type: n1-standard-2
      image:
        name: gardenlinux
        version: 318.8.0
    maximum: 6
    minimum: 2 # the value should be equal to or greater than the number of zones
    maxSurge: 1
    maxUnavailable: 0
    volume:
      type: pd-standard
      size: 50Gi
    zones:
      - europe-west1-c
      - europe-west1-d
    systemComponents:
      allow: true
</code></pre><p>The worker configuration contains:</p><ul><li><p>Local SSD interface for the additional volumes attached to GCP worker machines.</p><p>If you attach the disk with <code>SCRATCH</code> type, either an <code>NVMe</code> interface or a <code>SCSI</code> interface must be specified.
It is only meaningful to provide this volume interface if only <code>SCRATCH</code> data volumes are used.</p></li><li><p>Service Account with their specified scopes, authorized for this worker.</p><p>Service accounts created in advance that generate access tokens that can be accessed through the metadata server and used to authenticate applications on the instance.</p></li></ul><p>An example <code>WorkerConfig</code> for the GCP looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
kind: WorkerConfig
volume:
  interface: NVME
serviceAccount:
  email: foo@bar.com
  scopes:
  - https://www.googleapis.com/auth/cloud-platform
</code></pre></div><h2 id=example-shoot-manifest>Example <code>Shoot</code> manifest</h2><p>Please find below an example <code>Shoot</code> manifest:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1alpha1
kind: Shoot
metadata:
  name: johndoe-gcp
  namespace: garden-dev
spec:
  cloudProfileName: gcp
  region: europe-west1
  secretBindingName: core-gcp
  provider:
    type: gcp
    infrastructureConfig:
      apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
      kind: InfrastructureConfig
      networks:
        workers: 10.250.0.0/16
    controlPlaneConfig:
      apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
      kind: ControlPlaneConfig
      zone: europe-west1-b
    workers:
    - name: worker-xoluy
      machine:
        type: n1-standard-4
      minimum: 2
      maximum: 2
      volume:
        size: 50Gi
        type: pd-standard
      zones:
      - europe-west1-b
  networking:
    nodes: 10.250.0.0/16
    type: calico
  kubernetes:
    version: 1.16.1
  maintenance:
    autoUpdate:
      kubernetesVersion: <span style=color:#00f>true</span>
      machineImageVersion: <span style=color:#00f>true</span>
  addons:
    kubernetes-dashboard:
      enabled: <span style=color:#00f>true</span>
    nginx-ingress:
      enabled: <span style=color:#00f>true</span>
</code></pre></div><h2 id=csi-volume-provisioners>CSI volume provisioners</h2><p>Every GCP shoot cluster that has at least Kubernetes v1.18 will be deployed with the GCP PD CSI driver.
It is compatible with the legacy in-tree volume provisioner that was deprecated by the Kubernetes community and will be removed in future versions of Kubernetes.
End-users might want to update their custom <code>StorageClass</code>es to the new <code>pd.csi.storage.gke.io</code> provisioner.
Shoot clusters with Kubernetes v1.17 or less will use the in-tree <code>kubernetes.io/gce-pd</code> volume provisioner in the kube-controller-manager and the kubelet.</p><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-gcp@v1.21</code>.
Note that this feature is only usable for <code>Shoot</code>s whose <code>.spec.kubernetes.version</code> is greater or equal than the CSI migration version (<code>1.18</code>).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d02efe54eb56f07360e0d78ae62e3a23>5.4 - Usage As Operator</h1><h1 id=using-the-gcp-provider-extension-with-gardener-as-operator>Using the GCP provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.
The <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml><code>core.gardener.cloud/v1beta1.Seed</code> resource</a> is structured similarly.
Additionally, it allows configuring settings for the backups of the main etcds' data of shoot clusters control planes running in this seed cluster.</p><p>This document explains the necessary configuration for this provider extension.</p><h2 id=cloudprofile-resource><code>CloudProfile</code> resource</h2><p>This section describes, how the configuration for <code>CloudProfile</code>s looks like for GCP by providing an example <code>CloudProfile</code> manifest with minimal configuration that can be used to allow the creation of GCP shoot clusters.</p><h3 id=cloudprofileconfig><code>CloudProfileConfig</code></h3><p>The cloud profile configuration contains information about the real machine image IDs in the GCP environment (image URLs).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the GCP extension knows the image URL for every version you want to offer.</p><p>An example <code>CloudProfileConfig</code> for the GCP extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
kind: CloudProfileConfig
machineImages:
- name: coreos
  versions:
  - version: 2135.6.0
    image: projects/coreos-cloud/global/images/coreos-stable-2135-6-0-v20190801
</code></pre></div><h3 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h3><p>If you want to allow that shoots can create VMs with local SSDs volumes then you have to specify the type of the disk with <code>SCRATCH</code> in the <code>.spec.volumeTypes[]</code> list.
Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: CloudProfile
metadata:
  name: gcp
spec:
  type: gcp
  kubernetes:
    versions:
    - version: 1.16.1
    - version: 1.16.0
      expirationDate: <span style=color:#a31515>&#34;2020-04-05T01:02:03Z&#34;</span>
  machineImages:
  - name: coreos
    versions:
    - version: 2135.6.0
  machineTypes:
  - name: n1-standard-4
    cpu: <span style=color:#a31515>&#34;4&#34;</span>
    gpu: <span style=color:#a31515>&#34;0&#34;</span>
    memory: 15Gi
  volumeTypes:
  - name: pd-standard
    class: standard
  - name: pd-ssd
    class: premium
  - name: SCRATCH
    class: standard
  regions:
  - region: europe-west1
    names:
    - europe-west1-b
    - europe-west1-c
    - europe-west1-d
  providerConfig:
    apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
    kind: CloudProfileConfig
    machineImages:
    - name: coreos
      versions:
      - version: 2135.6.0
        image: projects/coreos-cloud/global/images/coreos-stable-2135-6-0-v20190801
</code></pre></div><h2 id=seed-resource><code>Seed</code> resource</h2><p>This provider extension does not support any provider configuration for the <code>Seed</code>&rsquo;s <code>.spec.provider.providerConfig</code> field.
However, it supports to managing of backup infrastructure, i.e., you can specify a configuration for the <code>.spec.backup</code> field.</p><h3 id=backup-configuration>Backup configuration</h3><p>A Seed of type <code>gcp</code> can be configured to perform backups for the main etcds' of the shoot clusters control planes using Google Cloud Storage buckets.</p><p>The location/region where the backups will be stored defaults to the region of the Seed (<code>spec.provider.region</code>), but can also be explicitly configured via the field <code>spec.backup.region</code>.
The region of the backup can be different from where the seed cluster is running.
However, usually it makes sense to pick the same region for the backup bucket as used for the Seed cluster.</p><p>Please find below an example <code>Seed</code> manifest (partly) that configures backups using Google Cloud Storage buckets.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>---
apiVersion: core.gardener.cloud/v1beta1
kind: Seed
metadata:
  name: my-seed
spec:
  provider:
    type: gcp
    region: europe-west1
  backup:
    provider: gcp
    region: europe-west1 <span style=color:green># default region</span>
    secretRef:
      name: backup-credentials
      namespace: garden
  ...
</code></pre></div><p>An example of the referenced secret containing the credentials for the GCP Cloud storage can be found in the <a href=https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-etcd-backup-secret.yaml>example folder</a>.</p><h4 id=permissions-for-gcp-cloud-storage>Permissions for GCP Cloud Storage</h4><p>Please make sure the service account associated with the provided credentials has the following IAM roles.</p><ul><li><a href=https://cloud.google.com/storage/docs/access-control/iam-roles>Storage Admin</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1df72f22ba701b91a3b46d33ffb0a9ea>6 - Provider KubeVirt</h1><div class=lead>Gardener Extension Provider for KubeVirt</div><h1 id=gardener-extension-for-kubevirt-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for KubeVirt provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-kubevirt-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-kubevirt-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-kubevirt><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-kubevirt alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=/docs/gardener/proposals/01-extensibility/>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This extension implements Gardener&rsquo;s extension contract for the <a href=https://kubevirt.io>KubeVirt</a> provider.
It includes KubeVirt-specific controllers for <code>Infrastructure</code>, <code>ControlPlane</code>, and <code>Worker</code> resources, as well as KubeVirt-specific control plane webhooks.
Unlike other provider extensions, it does not include controllers for <code>BackupBucket</code> and <code>BackupEntry</code> resources, since KubeVirt as technology is not concerned with backup storage.
Use the Gardener extension for your respective cloud provider to backup and restore your ETCD data.
On OpenShift clusters, use <a href=https://github.com/gardener/gardener-extension-provider-openshift>Gardener extension for OpenShift provider</a>.</p><p>For more information about Gardener integration with KubeVirt see <a href=https://gardener.cloud/blog/2020-10/00/gardener-integrates-with-kubevirt/>this gardener.cloud blog post</a>.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register the controllers of this extension with Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=/docs/gardener/proposals/01-extensibility/>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.19</td><td>not tested</td><td>N/A</td></tr><tr><td>Kubernetes 1.18</td><td>1.18.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.17</td><td>1.17.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.16</td><td>not tested</td><td>N/A</td></tr><tr><td>Kubernetes 1.15</td><td>not tested</td><td>N/A</td></tr></tbody></table><p>Please take a look <a href=/docs/gardener/usage/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-locally>How to start using or developing this extension locally</h2><p>You can run the extension locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-kubevirt/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=/docs/gardener/proposals/01-extensibility/>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=/docs/gardener/proposals/04-new-core-gardener-cloud-apis/>GEP-4 (New <code>core.gardener.cloud/v1alpha1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-15865a12aa5d76c98dc989741c55dce3>6.1 - Dev Setup</h1><h1 id=development-setup>Development Setup</h1><p>This document describes the recommended development setup for the <a href=https://github.com/gardener/gardener-extension-provider-kubevirt>KubeVirt provider extension</a>. Following the guidelines presented here would allow you to test the full Gardener reconciliation and deletion flows with the KubeVirt provider extension and the <a href=https://github.com/gardener/machine-controller-manager-provider-kubevirt>KubeVirt MCM extension</a>.</p><p>In this setup, only Gardener itself is running in your local development cluster. All other components, as well as KubeVirt VMs, are deployed and run on external clusters, which avoids high CPU and memory load on your local laptop.</p><h2 id=prerequisites>Prerequisites</h2><p>Follow the steps outlined in <a href=/docs/gardener/development/local_setup/>Setting up a local development environment</a> for Gardener in order to install all needed prerequisites and enable running <code>gardener-apiserver</code>, <code>gardener-controller-manager</code>, and <code>gardenlet</code> locally. You can use either minikube, kind, or the nodeless cluster as your local development cluster.</p><p>Before continuing, copy all files from <code>docs/development/manifests</code> and <code>docs/development/scripts</code> to your <code>dev</code> directory and adapt them as needed. The sections that follow assume that you have already done this and all needed manifests and scripts can be found in your <code>dev</code> directory.</p><h2 id=creating-the-controllerregistrations>Creating the ControllerRegistrations</h2><p>Before you register seeds or create shoots, you need to register all needed extensions using <code>ControllerRegistration</code> resources. The easiest way to manage <code>ControllerRegistrations</code> is via <a href=https://github.com/gardener/gem>gem</a>.</p><p>After installing <code>gem</code>, create a <code>requirements.yaml</code> file similar to <a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/requirements.yaml>requirements.yaml</a>. The example file contains only the extensions needed for the development setup described here, but you could add any other Gardener extensions you may need.</p><p>In your <code>requirements.yaml</code> file you can refer to a released extension version, or to a revision (commit) from a Gardener repo or your fork of it. This version or revision is used to find the correct <code>controller-registration.yaml</code> file for the extension.</p><p>You can generate or update the <code>controller-registrations.yaml</code> file out of your <code>requirements.yaml</code> file by running:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>gem ensure --requirements dev/requirements.yaml --controller-registrations dev/controller-registrations.yaml
</code></pre></div><p>After generating or updating the <code>controller-registrations.yaml</code> file, review it and make sure all versions are the ones you want to use for your tests. For example, if you are working on a PR for the KubeVirt provider extension, in addition to specifying the revision in your fork in <code>requirements.yaml</code>, you may need to change the version from <code>0.1.0-dev</code> to something unique to you or your PR, e.g. <code>0.1.0-dev-johndoe</code>. You can also add <code>pullPolicy: Always</code> to ensure that if you push a new extension image with that version and delete the corresponding pod, the new image will always be pulled when the pod is recreated.</p><p>Once you are satisfied with your controller registrations, apply the <code>controller-registrations.yaml</code> to your local Gardener:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f dev/controller-registrations.yaml
</code></pre></div><h2 id=registering-the-seed-cluster>Registering the Seed Cluster</h2><p>Create or choose an external cluster, different from your local development cluster, to register as seed in your local Gardener. This can be any cluster and it can be the same or different from your <a href=#creating-the-provider-cluster>provider cluster</a>. It is recommended to use a different cluster to avoid confusion between the two. If you want to use your provider cluster as seed, first create it as described below and then return to this step.</p><p>To register your cluster as a seed, create the secret containing the kubeconfig for your seed cluster, the secret containing the credentials for your cloud provider (e.g. GCP), and the seed resource itself. See the following files as examples:</p><ul><li><a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/secret-gcp1-kubeconfig.yaml>secret-gcp1-kubeconfig.yaml</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/secret-seed-operator-gcp.yaml>secret-seed-operator-gcp.yaml</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/seed-gcp1.yaml>seed-gcp1.yaml</a></li></ul><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f dev/secret-gcp1-kubeconfig.yaml
kubectl apply -f dev/secret-seed-operator-gcp.yaml
kubectl apply -f dev/seed-gcp1.yaml
</code></pre></div><h2 id=creating-the-project>Creating the Project</h2><p>At this point, you should create a <code>dev</code> project in your local Gardener.</p><p>Create the project resource for your local <code>dev</code> project, see <a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/project-dev.yaml>project-dev</a> as an example.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f dev/project-dev.yaml
</code></pre></div><h2 id=creating-the-dns-domain-secrets>Creating the DNS Domain Secrets</h2><p>At this point, you should create the domain secrets used by the DNS extension.</p><p>If you want to use an external DNS provider (e.g. route53), create default and internal domain secrets similar to <a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/secret-default-domain.yaml>secret-default-domain.yaml</a> and <a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/secret-internal-domain.yaml>secret-internal-domain.yaml</a>.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f dev/secret-default-domain.yaml
kubectl apply -f dev/secret-internal-domain.yaml
</code></pre></div><p>Alternatively, if you don&rsquo;t want to use an external DNS provider and use <code>nip.io</code> addresses instead, create just an internal domain secret similar to <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-internal-domain-unmanaged.yaml>10-secret-internal-domain-unmanaged.yaml</a>. For more information, see <a href=/docs/gardener/development/local_setup/#prepare-the-gardener>Prepare the Gardener</a>.</p><h2 id=creating-the-provider-cluster>Creating the Provider Cluster</h2><p>Create or choose an external cluster, different from your local development cluster, to use as a provider cluster. The only requirement to this cluster is that virtualization extensions are supported on its nodes. You can check if this is the case as described in <a href=https://kubevirt.io/pages/cloud.html>Easy install using Cloud Providers</a>, by executing the command <code>egrep 'svm|vmx' /proc/cpuinfo</code> and checking for non-empty output.</p><h3 id=creating-an-os-image-with-nested-virtualization-enabled>Creating an OS Image with Nested Virtualization Enabled</h3><p>Before you can create such a cluster, you need to ensure that nested virtualizaton is enabled for its instances by using an appropriate OS image. To create such an image in GCP, follow the steps described in <a href=https://cloud.google.com/compute/docs/instances/enable-nested-virtualization-vm-instances>Enabling nested virtualization for VM instances</a>. For example, to create a custom Ubuntu image with nested virtualizaton enabled based on Ubuntu 18.04, execute the following commands:</p><pre><code>gcloud compute disks create ubuntu-disk1 
  --image-project ubuntu-os-cloud \
  --image ubuntu-1804-bionic-v20200916 \
  --zone us-central1-b
gcloud compute images create ubuntu-1804-bionic-v20200916-vmx-enabled \
  --source-disk ubuntu-disk1 \
  --source-disk-zone us-central1-b \
  --licenses &quot;https://compute.googleapis.com/compute/v1/projects/vm-options/global/licenses/enable-vmx&quot;
gcloud compute images list | grep ubuntu
</code></pre><p>Once the image has been created, to create the provider cluster, you could use any Kubernetes provisioning tool, including of course Gardener itself, to create a cluster using this image.</p><h3 id=creating-the-provider-cluster-using-gardener>Creating the Provider Cluster Using Gardener</h3><p>To create the provider cluster using Gardener, simply create a shoot in the seed you registered previously using a custom GCP cloud profile that contains the above image, such as <a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/cloudprofile-gcp.yaml>cloudprofile-gcp.yaml</a>. To do this, follow these steps:</p><ol><li><p>Create the custom GCP cloud profile, for example <a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/cloudprofile-gcp.yaml>cloudprofile-gcp.yaml</a>.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f dev/cloudprofile-gcp.yaml
</code></pre></div></li><li><p>Create the shoot secret binding, you could bind to the <code>seed-operator-gcp</code> secret you created previously for your seed, see <a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/secretbinding-shoot-operator-gcp.yaml>secretbinding-shoot-operator-gcp.yaml</a> as an example.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f dev/secretbinding-shoot-operator-gcp.yaml
</code></pre></div></li><li><p>Create the GCP shoot itself. See <a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/shoot-gcp-vmx.yaml>shoot-gcp-vmx.yaml</a> as an example. Note that this shoot should use the image with name <code>ubuntu</code> and version <code>18.4.20200916-vmx</code> from the custom GCP cloud profile you created previously. Also, please rename the shoot to contain an unique prefix such as your github username, e.g. <code>johndoe-gcp-vmx</code>, to avoid naming conflicts in GCP.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f dev/shoot-gcp-vmx.yaml
</code></pre></div><p>During the reconciliation by your local <code>gardenlet</code>, you may want to connect to the seed to monitor the shoot namespace <code>shoot--dev--&lt;prefix>-gcp-vmx</code>.</p></li><li><p>Once the shoot is successfully reconciled by your local <code>gardenlet</code>, get its kubeconfig by executing:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get secret &lt;prefix&gt;-gcp-vmx.kubeconfig -n garden-dev -o jsonpath={.data.kubeconfig} | base64 -d &gt; dev/kubeconfig-gcp-vmx.yaml
</code></pre></div></li></ol><h3 id=installing-kubevirt-cdi-and-multus-in-the-provider-cluster>Installing KubeVirt, CDI, and Multus in the Provider Cluster</h3><p>Once the provider cluster has been created (with Gardener or any other provisioning tool), you should install KubeVirt, CDI, and optionally Multus in it so that it can serve its purpose as a provider cluster.</p><ol><li><p>Install KubeVirt and CDI in this cluster by executing the <a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/hack/kubevirt/install-kubevirt.sh>install-kubevirt.sh</a> script:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>export KUBECONFIG=dev/kubeconfig-gcp-vmx.yaml
hack/kubevirt/install-kubevirt.sh
</code></pre></div></li><li><p>Optionally, to use networking features, install <a href=https://github.com/k8snetworkplumbingwg/multus-cni/blob/master/docs/quickstart.md>Multus CNI</a> as described in its documentation, or by applying the provided <a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/hack/kubevirt/multus.yaml>multus.yaml</a> manifest.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>export KUBECONFIG=dev/kubeconfig-gcp-vmx.yaml
kubectl apply -f hack/kubevirt/multus.yaml
</code></pre></div><p><strong>Note:</strong> In order to use any additional CNI plugins, the plugin binaries must be present in the <code>/opt/cni/bin</code> directory of the provider cluster nodes. For testing purposes, they can be installed manually by downloading a <a href=https://github.com/containernetworking/plugins>containernetworking/plugins</a> release and copying the needed plugins to the <code>/opt/cni/bin</code> directory of each provider cluster node.</p></li></ol><h2 id=testing-the-gardener-reconciliation-flow>Testing the Gardener Reconciliation Flow</h2><p>To test the Gardener reconciliation flow with the KubeVirt provider extensions, create the KubeVirt shoot cluster in your local <code>dev</code> project, by following these steps:</p><ol><li><p>Create the KubeVirt cloud profile, for example <a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/cloudprofile-kubevirt.yaml>cloudprofile-kubevirt.yaml</a>.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f dev/cloudprofile-kubevirt.yaml
</code></pre></div><p><strong>Note:</strong> The example cloud profile is intentionally rather simple and does not take advantage of some of the features supported by the KubeVirt provider extension. To test these features, modify the cloud profile manifest accordingly. For more information, see <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-kubevirt/docs/usage-as-operator/>Using the KubeVirt provider extension with Gardener as operator</a>.</p></li><li><p>Create the shoot secret and secret binding. You should create a secret containing the kubeconfig for your provider cluster, and a corresponding secret binding:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl create secret generic kubevirt-credentials -n garden-dev --from-file=kubeconfig=dev/kubeconfig-gcp-vmx.yaml
kubectl apply -f dev/secretbinding-kubevirt-credentials.yaml
</code></pre></div></li><li><p>Create the KubeVirt shoot itself. See <a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/manifests/shoot-kubevirt.yaml>shoot-kubevirt.yaml</a> as an example. Note that the nodes CIDR for this shoot must be the same range as the pods CIDR of your provider cluster.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl apply -f dev/shoot-kubevirt.yaml
</code></pre></div><p><strong>Note:</strong> The example shoot is intentionally very simple and does not take advantage of many of the features supported by the KubeVirt provider extension. To test these features, modify the shoot manifest accordingly. For more information, see <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-kubevirt/docs/usage-as-end-user/>Using the KubeVirt provider extension with Gardener as end-user</a>.</p></li><li><p>During the shoot reconciliation by your local <code>gardenlet</code>, you may want to:</p><ul><li>Monitor the <code>gardenlet</code> logs in your local console where <code>gardenlet</code> is running.</li><li>Connect to the seed to monitor the shoot namespace <code>shoot--dev--kubevirt</code> and the logs of the KubeVirt provider extension in the <code>extension-provider-kubevirt-*</code> namespace.</li><li>Connect to the provider cluster to monitor the <code>default</code> namespace where VMs and VMIs are being created.</li></ul></li><li><p>Once the shoot has been successfully reconciled, get its kubeconfig by executing:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl get secret kubevirt.kubeconfig -n garden-dev -o jsonpath={.data.kubeconfig} | base64 -d &gt; dev/kubeconfig-kubevirt.yaml
</code></pre></div><p>At this point, you may want to connect to the KubeVirt shoot and check if it&rsquo;s usable.</p></li></ol><h2 id=testing-the-gardener-deletion-flow>Testing the Gardener Deletion Flow</h2><p>To test the Gardener deletion flow with the KubeVirt provider extensions, delete the KubeVirt shoot cluster in your local <code>dev</code> project, by following these steps:</p><ol><li><p>Delete the KubeVirt shoot itself using the <a href=https://github.com/gardener/gardener/blob/master/hack/usage/delete>delete</a> script.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>kubectl annotate shoot kubevirt -n garden-dev confirmation.gardener.cloud/deletion=1
kubectl delete shoot kubevirt -n garden-dev
</code></pre></div></li><li><p>During the shoot deletion by your local <code>gardenlet</code>, you may want to:</p><ul><li>Monitor the <code>gardenlet</code> logs in your local console where <code>gardenlet</code> is running.</li><li>Connect to the seed to monitor the shoot namespace <code>shoot--dev--kubevirt</code> and the logs of the KubeVirt provider extension in the <code>extension-provider-kubevirt-*</code> namespace.</li><li>Connect to the provider cluster to monitor the <code>default</code> namespace where VMs and VMIs are being created.</li></ul></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-78a8317c0864839f105133c6758a4a96>6.2 - Local Setup Admission</h1><h3 id=admission-kubevirt>admission-kubevirt</h3><p><code>admission-kubevirt</code> is an admission webhook server which is responsible for the validation of the cloud provider (KubeVirt in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/development/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>make start-admission
</code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-kubevirt.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>./hack/dev-setup-admission-kubevirt.sh
</code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-kubevirt</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-67ed47f87cb01457e11c1d03e04e81bc>6.3 - Usage As End User</h1><h1 id=using-the-kubevirt-provider-extension-with-gardener-as-end-user>Using the KubeVirt provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>This document describes how this configuration looks like for KubeVirt and provides an example <code>Shoot</code> manifest with minimal configuration that you can use to create a KubeVirt shoot cluster (without the landscape-specific information such as cloud profile names, secret binding names, etc.).</p><h2 id=provider-secret-data>Provider Secret Data</h2><p>Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the kubeconfig of your <em>KubeVirt provider cluster</em>. This cluster is the cluster where KubeVirt itself is installed, and that hosts the KubeVirt virtual machines used as shoot worker nodes. This <code>Secret</code> must look as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Secret
metadata:
  name: provider-cluster-kubeconfig
  namespace: garden-dev
type: Opaque
data:
  kubeconfig: base64(kubeconfig)
</code></pre></div><h3 id=permissions>Permissions</h3><p>All KubeVirt resources (<code>VirtualMachines</code>, <code>DataVolumes</code>, etc.) are created in the namespace of the current context of the above kubeconfig, that is <code>my-shoot</code> in the example below:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>...
current-context: provider-cluster
contexts:
- name: provider-cluster
  context:
    cluster: provider-cluster
    namespace: my-shoot
    user: provider-cluster-token
...
</code></pre></div><p>If no namespace is specified, the <code>default</code> namespace is assumed. You can use the same namespace for multiple shoots. The user specified in the <code>kubeconfig</code> must have permissions to read and write KubeVirt and Kubernetes core resources in this namespace.</p><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration can contain additional networks used by the shoot worker nodes. If this configuration is empty, all KubeVirt virtual machines used as shoot worker nodes use only the pod network of the provider cluster.</p><p>An example <code>InfrastructureConfig</code> for the KubeVirt extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1
kind: InfrastructureConfig
networks:
  sharedNetworks:
  <span style=color:green># Reference to the network defined by the NetworkAttachmentDefinition default/net-conf</span>
  - name: net-conf
    namespace: default
  tenantNetworks:
  - name: network-1
    <span style=color:green># Configuration for the CNI plugins bridge and firewall</span>
    config: |<span style=color:#a31515>
</span><span style=color:#a31515>      {
</span><span style=color:#a31515>        &#34;cniVersion&#34;: &#34;0.4.0&#34;,
</span><span style=color:#a31515>        &#34;name&#34;: &#34;bridge-firewall&#34;,
</span><span style=color:#a31515>        &#34;plugins&#34;: [
</span><span style=color:#a31515>          {
</span><span style=color:#a31515>            &#34;type&#34;: &#34;bridge&#34;,
</span><span style=color:#a31515>            &#34;isGateway&#34;: true,
</span><span style=color:#a31515>            &#34;isDefaultGateway&#34;: true,
</span><span style=color:#a31515>            &#34;ipMasq&#34;: true,
</span><span style=color:#a31515>            &#34;ipam&#34;: {
</span><span style=color:#a31515>              &#34;type&#34;: &#34;host-local&#34;,
</span><span style=color:#a31515>              &#34;subnet&#34;: &#34;10.100.0.0/16&#34;
</span><span style=color:#a31515>            }
</span><span style=color:#a31515>          },
</span><span style=color:#a31515>          {
</span><span style=color:#a31515>            &#34;type&#34;: &#34;firewall&#34;
</span><span style=color:#a31515>          }
</span><span style=color:#a31515>        ]
</span><span style=color:#a31515>      }</span>      
    <span style=color:green># Don&#39;t attach the pod network at all, instead use this network as default</span>
    default: <span style=color:#00f>true</span>
</code></pre></div><p>A non-empty infrastructure configuration can contain:</p><ul><li>References to pre-existing, <em>shared</em> networks that can be shared between multiple shoots. These networks must exist in the provider cluster prior to shoot creation.</li><li>CNI configurations for <em>tenant</em> networks that are created, updated, and deleted together with the shoot. If one of these networks is marked as <code>default: true</code>, it becomes the default network instead of the pod network of the provider cluster. This can be used to achieve higher level of network isolation, since the networks of the different shoots can be isolated from each other, and in some cases better performance.</li></ul><p>Both shared and tenant networks are maintained in the provider cluster via <a href=https://github.com/intel/multus-cni/blob/master/README.md>Multus CNI</a> <a href=https://github.com/k8snetworkplumbingwg/multus-cni/blob/master/docs/quickstart.md>NetworkAttachmentDefinition</a> resources. For shared networks, these resources must be created in advance, while for tenant networks they are managed by the shoot reconciliation process.</p><p>In order to use any additional CNI plugins in a tenant network configuration, such as <code>bridge</code> or <code>firewall</code> in the above example, the plugin binaries must be present in the <code>/opt/cni/bin</code> directory of the provider cluster nodes. They can be installed manually by downloading a <a href=https://github.com/containernetworking/plugins>containernetworking/plugins</a> release (not recommended except for testing a new configuration). Alternatively, they can be installed via a specially prepared daemon set that ensures the existence of the plugin binaries on each provider cluster node.</p><p><strong>Note:</strong> Although it is possible to update the network configuration in <code>InfrastructureConfig</code>, any such changes will result in recreating all KubeVirt VMs, so that the new network configuration is properly taken into account. This will be done automatically by the MCM using rolling update.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration contains options for the KubeVirt-specific control plane components. Currently, the only component deployed by the KubeVirt extension is the <a href=https://github.com/kubevirt/cloud-provider-kubevirt>KubeVirt Cloud Controller Manager (CCM)</a>.</p><p>An example <code>ControlPlaneConfig</code> for the KubeVirt extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1
kind: ControlPlaneConfig
cloudControllerManager:
  featureGates:
    CustomResourceValidation: <span style=color:#00f>true</span>
</code></pre></div><p>The <code>cloudControllerManager.featureGates</code> contains a map of explicitly enabled or disabled feature gates. For production usage it&rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability. If you don&rsquo;t want to configure anything for the CCM, simply omit the key in the YAML specification.</p><h2 id=workerconfig><code>WorkerConfig</code></h2><p>The KubeVirt extension supports specifying additional data volumes per machine in the worker pool. For each data volume, you must specify a name and a type.</p><p>Below is an example <code>Shoot</code> resource snippet with root volume and data volumes:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:
  provider:
    workers:
    - name: cpu-worker
      ...
      volume:
        type: default
        size: 20Gi
      dataVolumes:
      - name: volume-1
        type: default
        size: 10Gi
</code></pre></div><p><strong>Note:</strong> The additional data volumes will be attached as blank disks to the KubeVirt VMs. These disks must be formatted and mounted manually to the VM before they can be used.</p><p>The KubeVirt extension does not currently support encryption for volumes.</p><p>Additionally, it is possible to specify additional KubeVirt-specific options for configuring the worker pools. They can be specified in <code>.spec.provider.workers[].providerConfig</code> and are evaluated by the KubeVirt worker controller when it reconciles the shoot machines.</p><p>An example <code>WorkerConfig</code> for the KubeVirt extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1
kind: WorkerConfig
devices:
  <span style=color:green># disks allow to customize disks attached to KubeVirt VM</span>
  <span style=color:green># check [link](https://kubevirt.io/user-guide/#/creation/disks-and-volumes?id=disks-and-volumes) for full specification and options</span>
  disks:
  <span style=color:green># name must match defined dataVolume name</span>
  <span style=color:green># to modify root volume the name must be equal to &#39;root-disk&#39;</span>
  - name: root-disk <span style=color:green># modify root-disk</span>
    <span style=color:green># disk type, check [link](https://kubevirt.io/user-guide/#/creation/disks-and-volumes?id=disks) for more types</span>
    disk:
      <span style=color:green># bus indicates the type of disk device to emulate.</span>
      bus: virtio
    <span style=color:green># set disk device cache</span>
    cache: writethrough
    <span style=color:green># dedicatedIOThread indicates this disk should have an exclusive IO Thread</span>
    dedicatedIOThread: <span style=color:#00f>true</span>
  - name: volume-1 <span style=color:green># modify dataVolume named volume-1</span>
    disk: {}
  <span style=color:green># whether to have random number generator from host</span>
  rng: {}
  <span style=color:green># whether or not to enable virtio multi-queue for block devices</span>
  blockMultiQueue: <span style=color:#00f>true</span>
  <span style=color:green># if specified, virtual network interfaces configured with a virtio bus will also enable the vhost multiqueue feature</span>
  networkInterfaceMultiQueue: <span style=color:#00f>true</span>
cpu:
  <span style=color:green># number of cores inside the VMI</span>
  cores: 1
  <span style=color:green># number of sockets inside the VMI</span>
  sockets: 2
  <span style=color:green># number of threads inside the VMI</span>
  threads: 1
  <span style=color:green># models specifies the CPU model of the VMI</span>
  <span style=color:green># list of available models https://github.com/libvirt/libvirt/tree/master/src/cpu_map.</span>
  <span style=color:green># and options https://libvirt.org/formatdomain.html#cpu-model-and-topology</span>
  model: <span style=color:#a31515>&#34;host-model&#34;</span>
  <span style=color:green># features specifies the CPU features list inside the VMI</span>
  features:
  - <span style=color:#a31515>&#34;pcid&#34;</span>
  <span style=color:green># dedicatedCPUPlacement requests the scheduler to place the VirtualMachineInstance on a node</span>
  <span style=color:green># with dedicated pCPUs and pin the vCPUs to it.</span>
  dedicatedCpuPlacement: <span style=color:#00f>false</span>
  <span style=color:green># isolateEmulatorThread requests one more dedicated pCPU to be allocated for the VMI to place the emulator thread on it.</span>
  isolateEmulatorThread: <span style=color:#00f>false</span>
<span style=color:green># memory configuration for KubeVirt VMs, allows to set &#39;hugepages&#39; and &#39;guest&#39; settings. </span>
<span style=color:green># See https://kubevirt.io/api-reference/master/definitions.html#_v1_memory</span>
memory:
  <span style=color:green># hugepages requires appropriate feature gate to be enabled, take a look at the following links for more details:</span>
  <span style=color:green># * k8s - https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/</span>
  <span style=color:green># * okd - https://docs.okd.io/latest/scalability_and_performance/what-huge-pages-do-and-how-they-are-consumed-by-apps.html</span>
  hugepages:
     pageSize: <span style=color:#a31515>&#34;2Mi&#34;</span>
  <span style=color:green># guest allows to specifying the amount of memory which is visible inside the Guest OS. It must lie between requests and limits.</span>
  <span style=color:green># Defaults to the requested memory in the machineTypes.</span>
  guest: <span style=color:#a31515>&#34;1Gi&#34;</span>
<span style=color:green># overcommitGuestOverhead informs the scheduler to not take the guest-management overhead into account. Instead</span>
<span style=color:green># put the overhead only into the container&#39;s memory limit. This can lead to crashes if</span>
<span style=color:green># all memory is in use on a node. Defaults to false.</span>
<span style=color:green># For more details take a look at https://kubevirt.io/user-guide/#/usage/overcommit?id=overcommit-the-guest-overhead</span>
overcommitGuestOverhead: <span style=color:#00f>true</span>
<span style=color:green># DNS policy for KubeVirt VMs. Valid values are &#39;ClusterFirstWithHostNet&#39;, &#39;ClusterFirst&#39;, &#39;Default&#39; or &#39;None&#39;.</span>
<span style=color:green># Defaults to &#39;ClusterFirst`.</span>
<span style=color:green># See https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/</span>
dnsPolicy: ClusterFirst
<span style=color:green># DNS configuration for KubeVirt VMs, merged with the generated DNS configuration based on dnsPolicy.</span>
<span style=color:green># See https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/</span>
dnsConfig:
  nameservers:
  - 8.8.8.8
<span style=color:green># Disable using pre-allocated data volumes. Defaults to &#39;false&#39;.</span>
disablePreAllocatedDataVolumes: <span style=color:#00f>true</span>
<span style=color:green># cpu allows to set the CPU topology of the VMI</span>
<span style=color:green># See https://kubevirt.io/api-reference/master/definitions.html#_v1_cpu</span>
</code></pre></div><p>Currently, these KubeVirt-specific options may include:</p><ul><li>The CPU topology and memory configuration of the KubVirt VMs. For more information, see <a href=https://kubevirt.io/api-reference/master/definitions.html#_v1_cpu>CPU.v1</a> and <a href=https://kubevirt.io/api-reference/master/definitions.html#_v1_memory>Memory.v1</a>.</li><li>The DNS policy and DNS configuration of the KubeVirt VMs. For more information, see <a href=https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/>DNS for Services and Pods</a>.</li><li>Whether to use <em>pre-allocated data volumes</em> with KubeVirt VMs. With pre-allocated data volumes (the default), a data volume is created in advance for each machine class, the OS image is imported into this volume only once, and actual KubeVirt VM data volumes are cloned from this data volume. Typically, this significantly speeds up the data volume creation process. You can disable this feature by setting the <code>disablePreAllocatedDataVolumes</code> option to <code>true</code>.</li></ul><h2 id=region-and-zone-support>Region and Zone Support</h2><p>Nodes in the provider cluster may belong to provider-specific regions and zones, and Kubernetes would then use this information to spread pods across zones as described in <a href=https://kubernetes.io/docs/setup/best-practices/multiple-zones/>Running in multiple zones</a>. You may want to take advantage of these capabilities in the shoot cluster as well.</p><p>To achieve this, the KubeVirt provider extension ensures that the region and zones specified in the <code>Shoot</code> resource are taken into account when creating the KubeVirt VMs used as shoot cluster nodes.</p><p>Below is an example <code>Shoot</code> resource snippet with region and zones:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>spec:  
  region: europe-west1
  provider:
    ...
    workers:
    - name: cpu-worker
      ...
      zones:
      - europe-west1-c
      - europe-west1-d
</code></pre></div><p>The shoot region and zones must correspond to the region and zones of the provider cluster. A KubeVirt VM designated for specific region and zone will only be scheduled on provider cluster nodes belonging to these region and zone. If there are no such nodes, or they have insufficient resources, the KubeVirt VM may remain in <code>Pending</code> state for a longer period and the shoot reconciliation may fail. Therefore, always make sure that the provider cluster contains nodes for all zones specified in the shoot.</p><p>If multiple zones are specified for a worker pool, the KubeVirt VMs will be equally distributed over these zones in the specified order.</p><p>If your provider cluster is not region and zone aware, or if it contains nodes that don&rsquo;t belong to any region or zone, you can use <code>default</code> as a region or zone name in the <code>Shoot</code> resource to target such nodes.</p><p>Note that the <code>region</code> and <code>zones</code> are mandatory fields in the <code>Shoot</code> resource, so you must specify either a concrete region / zone or <code>default</code>.</p><p>Once the KubeVirt VMs are scheduled on the correct provider cluster nodes, the KubeVirt Cloud Controller Manager (CCM) mentioned above will appropriately label the shoot worker nodes themselves with the appropriate <a href=https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/>region and zone labels</a>, by propagating the region and zone from the provider cluster nodes, so that Kubernetes multi-zone capabilities are also available in the shoot cluster.</p><h2 id=example-shoot-manifest>Example <code>Shoot</code> Manifest</h2><p>Please find below an example <code>Shoot</code> manifest for one availability zone:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: Shoot
metadata:
  name: johndoe-kubevirt
  namespace: garden-dev
spec:
  cloudProfileName: kubevirt
  secretBindingName: provider-cluster-kubeconfig
  region: europe-west1
  provider:
    type: kubevirt
<span style=color:green>#   infrastructureConfig:</span>
<span style=color:green>#     apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1</span>
<span style=color:green>#     kind: InfrastructureConfig</span>
<span style=color:green>#     networks:</span>
<span style=color:green>#       tenantNetworks:</span>
<span style=color:green>#       - name: network-1</span>
<span style=color:green>#         config: &#34;{...}&#34;</span>
<span style=color:green>#         default: true</span>
<span style=color:green>#   controlPlaneConfig:</span>
<span style=color:green>#     apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1</span>
<span style=color:green>#     kind: ControlPlaneConfig</span>
<span style=color:green>#     cloudControllerManager:</span>
<span style=color:green>#       featureGates:</span>
<span style=color:green>#         CustomResourceValidation: true</span>
    workers:
    - name: cpu-worker
      machine:
        type: standard-1
        image:
          name: ubuntu
          version: <span style=color:#a31515>&#34;18.04&#34;</span>
      minimum: 1
      maximum: 2
      volume:
        type: default
        size: 20Gi
<span style=color:green>#     dataVolumes:</span>
<span style=color:green>#     - name: volume-1</span>
<span style=color:green>#       type: default</span>
<span style=color:green>#       size: 10Gi</span>
<span style=color:green>#     providerConfig:</span>
<span style=color:green>#       apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1</span>
<span style=color:green>#       kind: WorkerConfig</span>
<span style=color:green>#       disablePreAllocatedDataVolumes: true</span>
      zones:
      - europe-west1-c
  networking:
    type: calico
    pods: 100.96.0.0/11
    <span style=color:green># Must match the IPAM subnet of the default tenant network, if present.</span>
    <span style=color:green># Otherwise, must be the same as the provider cluster pod network range.</span>
    nodes: 10.225.128.0/17 <span style=color:green># 10.100.0.0/16</span>
    services: 100.64.0.0/13
  kubernetes:
    version: 1.17.8
  maintenance:
    autoUpdate:
      kubernetesVersion: <span style=color:#00f>true</span>
      machineImageVersion: <span style=color:#00f>true</span>
  addons:
    kubernetesDashboard:
      enabled: <span style=color:#00f>true</span>
    nginxIngress:
      enabled: <span style=color:#00f>true</span>
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-c8ac00cf9e8750118007c79249658ad5>6.4 - Usage As Operator</h1><h1 id=using-the-kubevirt-provider-extension-with-gardener-as-operator>Using the KubeVirt provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration. The <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml><code>core.gardener.cloud/v1beta1.Seed</code> resource</a> is structured in a similar way. Additionally, it allows configuring settings for the backups of the main etcds' data of shoot clusters control planes running in this seed cluster.</p><p>This document explains what is necessary to configure for this provider extension.</p><h2 id=cloudprofile-resource><code>CloudProfile</code> resource</h2><p>In this section we are describing how the configuration for <code>CloudProfile</code>s looks like for KubeVirt and provide an example <code>CloudProfile</code> manifest with minimal configuration that you can use to allow creating KubeVirt shoot clusters.</p><h3 id=cloudprofileconfig><code>CloudProfileConfig</code></h3><p>The cloud profile configuration contains information about the machine images source URLs. You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here so that the KubeVirt extension could find the source URL for every version you want to offer.</p><p>An example <code>CloudProfileConfig</code> for the KubeVirt extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1
kind: CloudProfileConfig
machineImages:
- name: ubuntu
  versions:
  - version: <span style=color:#a31515>&#34;18.04&#34;</span>
    sourceURL: https://cloud-images.ubuntu.com/bionic/current/bionic-server-cloudimg-amd64.img
<span style=color:green># machineTypes extend cloud profile&#39;s spec.machineType object to KubeVirt provider specific config</span>
machineTypes:
<span style=color:green># name is used as a reference to the machineType object</span>
- name: standard-1  
  <span style=color:green># limits is equivalent to resource limits of pod</span>
  <span style=color:green># https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-requests-and-limits-of-pod-and-container</span>
  limits:
    cpu: <span style=color:#a31515>&#34;2&#34;</span>
    memory: 8Gi
</code></pre></div><h3 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h3><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: CloudProfile
metadata:
  name: kubevirt
spec:
  type: kubevirt
  providerConfig:
    apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1
    kind: CloudProfileConfig
    machineImages:
    - name: ubuntu
      versions:
      - version: <span style=color:#a31515>&#34;18.04&#34;</span>
        sourceURL: https://cloud-images.ubuntu.com/bionic/current/bionic-server-cloudimg-amd64.img
  kubernetes:
    versions:
    - version: 1.18.5
    - version: 1.17.8
  machineImages:
  - name: ubuntu
    versions:
    - version: <span style=color:#a31515>&#34;18.04&#34;</span>
  machineTypes:
  - name: standard-1
    cpu: <span style=color:#a31515>&#34;1&#34;</span>
    gpu: <span style=color:#a31515>&#34;0&#34;</span>
    memory: 4Gi
  volumeTypes:
  - name: default
    class: default
  regions:
  - name: europe-west1
    zones:
    - name: europe-west1-b
    - name: europe-west1-c
    - name: europe-west1-d
</code></pre></div><h2 id=seed-resource><code>Seed</code> resource</h2><p>This provider extension does not support any provider configuration for the <code>Seed</code>&rsquo;s <code>.spec.provider.providerConfig</code> field.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0c3a831b9c91a4a68d10631812e3f23a>7 - Provider OpenShift</h1><div class=lead>Gardener extension for the OpenShift cloud provider</div><h1 id=gardener-extension-for-openshift-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for OpenShift provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-openshift-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-openshift-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-openshift><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-openshift alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=/docs/gardener/proposals/01-extensibility/>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This extension implements Gardener&rsquo;s extension contract for the <a href=https://openshift.com/>OpenShift</a> provider.
It includes OpenShift-specific controllers for <code>BackupBucket</code> and <code>BackupEntry</code> resources.
Unlike other provider extensions, it does not include controllers for <code>Infrastructure</code>, <code>ControlPlane</code>, and <code>Worker</code> resources, as well as provider-specific control plane webhooks.
Since OpenShift Virtualization is based on <a href=https://kubevirt.io>KubeVirt</a>, use <a href=https://github.com/gardener/gardener-extension-provider-kubevirt>Gardener extension for KubeVirt provider</a> with an OpenShift cluster.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register the controllers of this extension with Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-openshift/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=/docs/gardener/proposals/01-extensibility/>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th></tr></thead><tbody><tr><td>Kubernetes 1.19</td><td>not tested</td></tr><tr><td>Kubernetes 1.18</td><td>1.18.0+</td></tr><tr><td>Kubernetes 1.17</td><td>1.17.0+</td></tr><tr><td>Kubernetes 1.16</td><td>not tested</td></tr><tr><td>Kubernetes 1.15</td><td>not tested</td></tr></tbody></table><p>Please take a look <a href=/docs/gardener/usage/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-locally>How to start using or developing this extension locally</h2><p>You can run the extension locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-openshift/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=/docs/gardener/proposals/01-extensibility/>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=/docs/gardener/proposals/04-new-core-gardener-cloud-apis/>GEP-4 (New <code>core.gardener.cloud/v1alpha1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b46674e0cabcf6e848e4d419e8c17465>8 - Provider Openstack</h1><div class=lead>Gardener extension controller for the OpenStack cloud provider</div><h1 id=gardener-extension-for-openstack-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for OpenStack provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-openstack-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-openstack-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-openstack><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-openstack alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=/docs/gardener/proposals/01-extensibility/>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the OpenStack provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=/docs/gardener/proposals/01-extensibility/>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.23</td><td>1.23.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.22</td><td>1.22.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.22%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.22%20OpenStack/tests_status?style=svg" alt="Gardener v1.22 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.21</td><td>1.21.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.21%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.21%20OpenStack/tests_status?style=svg" alt="Gardener v1.21 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.20</td><td>1.20.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.20%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.20%20OpenStack/tests_status?style=svg" alt="Gardener v1.20 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.19</td><td>1.19.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.19%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.19%20OpenStack/tests_status?style=svg" alt="Gardener v1.19 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.18</td><td>1.18.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.18%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.18%20OpenStack/tests_status?style=svg" alt="Gardener v1.18 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.17</td><td>1.17.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.17%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.17%20OpenStack/tests_status?style=svg" alt="Gardener v1.17 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.16</td><td>1.16.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.16%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.16%20OpenStack/tests_status?style=svg" alt="Gardener v1.16 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.15</td><td>1.15.0+</td><td>[1]</td></tr></tbody></table><p>[1] Conformance tests are still executed and validated, unfortunately <a href=https://github.com/kubernetes/test-infra/pull/18509#issuecomment-668204180>no longer shown in TestGrid</a>.</p><p>Please take a look <a href=/docs/gardener/usage/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=compatibility>Compatibility</h2><p>The following lists known compatibility issues of this extension controller with other Gardener components.</p><table><thead><tr><th>OpenStack Extension</th><th>Gardener</th><th>Action</th><th>Notes</th></tr></thead><tbody><tr><td><code>&lt; v1.12.0</code></td><td><code>> v1.10.0</code></td><td>Please update the provider version to <code>>= v1.12.0</code> or disable the feature gate <code>MountHostCADirectories</code> in the Gardenlet.</td><td>Applies if feature flag <code>MountHostCADirectories</code> in the Gardenlet is enabled. This is to prevent duplicate volume mounts to <code>/usr/share/ca-certificates</code> in the Shoot API Server.</td></tr></tbody></table><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-openstack/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=/docs/gardener/proposals/01-extensibility/>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=/docs/gardener/proposals/04-new-core-gardener-cloud-apis/>GEP-4 (New <code>core.gardener.cloud/v1alpha1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ec2c3c7ff8df2101c099ac6ffc355b52>8.1 - Deployment</h1><h1 id=deployment-of-the-openstack-provider-extension>Deployment of the OpenStack provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the OpenStack provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the OpenStack provider extension <a href=https://github.com/gardener/gardener-extension-provider-openstack>repository</a>.</p><h2 id=gardener-extension-admission-openstack>gardener-extension-admission-openstack</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-admission-openstack</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: &lt;CA-DATA&gt;
    server: https://default.kubernetes.svc.cluster.local
  name: garden
contexts:
- context:
    cluster: garden
    user: garden
  name: garden
current-context: garden
users:
- name: garden
  user:
    tokenFile: /var/run/secrets/projected/serviceaccount/token
</code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: &lt;CA-DATA&gt;
    server: https://virtual-garden.api
  name: virtual-garden
contexts:
- context:
    cluster: virtual-garden
    user: virtual-garden
  name: virtual-garden
current-context: virtual-garden
users:
- name: virtual-garden
  user:
    tokenFile: /var/run/secrets/projected/serviceaccount/token
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-af315a6fe4fa0e8ce5057c406a991397>8.2 - Local Setup</h1><h3 id=admission-openstack>admission-openstack</h3><p><code>admission-openstack</code> is an admission webhook server which is responsible for the validation of the cloud provider (OpenStack in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/development/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>make start-admission
</code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-openstack.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>./hack/dev-setup-admission-openstack.sh
</code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-openstack</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9185f7aa7b9d6eabd12b95706054e195>8.3 - Usage As End User</h1><h1 id=using-the-openstack-provider-extension-with-gardener-as-end-user>Using the OpenStack provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for OpenStack and provide an example <code>Shoot</code> manifest with minimal configuration that you can use to create an OpenStack cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=provider-secret-data>Provider Secret Data</h2><p>Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of your OpenStack tenant.
This <code>Secret</code> must look as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Secret
metadata:
  name: core-openstack
  namespace: garden-dev
type: Opaque
data:
  domainName: base64(domain-name)
  tenantName: base64(tenant-name)
  
  <span style=color:green># either use username/password</span>
  username: base64(user-name)
  password: base64(password)

  <span style=color:green># or application credentials</span>
  <span style=color:green>#applicationCredentialID: base64(app-credential-id)</span>
  <span style=color:green>#applicationCredentialName: base64(app-credential-name) # optional</span>
  <span style=color:green>#applicationCredentialSecret: base64(app-credential-secret)</span>
</code></pre></div><p>Please look up <a href=https://docs.openstack.org/keystone/pike/admin/identity-concepts.html>https://docs.openstack.org/keystone/pike/admin/identity-concepts.html</a> as well.</p><p>For authentication with username/password see <a href=https://docs.openstack.org/keystone/latest/user/supported_clients.html>Keystone username/password</a></p><p>Alternatively, for authentication with application credentials see <a href=https://docs.openstack.org/keystone/latest/user/application_credentials.html>Keystone Application Credentials</a>. Application Credentials are <strong>not</strong> supported for shoots with kubernetes versions <strong>less than v1.19</strong>.</p><p>⚠️ Depending on your API usage it can be problematic to reuse the same provider credentials for different Shoot clusters due to rate limits.
Please consider spreading your Shoots over multiple credentials from different tenants if you are hitting those limits.</p><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.</p><p>An example <code>InfrastructureConfig</code> for the OpenStack extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
kind: InfrastructureConfig
floatingPoolName: MY-FLOATING-POOL
<span style=color:green># floatingPoolSubnetName: my-floating-pool-subnet-name</span>
networks:
<span style=color:green># id: 12345678-abcd-efef-08af-0123456789ab</span>
<span style=color:green># router:</span>
<span style=color:green>#   id: 1234</span>
  workers: 10.250.0.0/19
</code></pre></div><p>The <code>floatingPoolName</code> is the name of the floating pool you want to use for your shoot.
If you don&rsquo;t know which floating pools are available look it up in the respective <code>CloudProfile</code>.</p><p>With <code>floatingPoolSubnetName</code> you can explicitly define to which subnet in the floating pool network (defined via <code>floatingPoolName</code>) the router should be attached to.</p><p>If <code>networks.id</code> is an optional field. If it is given, you can specify the uuid of an existing private Neutron network (created manually, by other tooling, &mldr;) that should be reused. A new subnet for the Shoot will be created in it.</p><p>The <code>networks.router</code> section describes whether you want to create the shoot cluster in an already existing router or whether to create a new one:</p><ul><li><p>If <code>networks.router.id</code> is given then you have to specify the router id of the existing router that was created by other means (manually, other tooling, &mldr;).
If you want to get a fresh router for the shoot then just omit the <code>networks.router</code> field.</p></li><li><p>In any case, the shoot cluster will be created in a <strong>new</strong> subnet.</p></li></ul><p>The <code>networks.workers</code> section describes the CIDR for a subnet that is used for all shoot worker nodes, i.e., VMs which later run your applications.</p><p>You can freely choose these CIDRs and it is your responsibility to properly design the network layout to suit your needs.</p><p>Apart from the router and the worker subnet the OpenStack extension will also create a network, router interfaces, security groups, and a key pair.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the OpenStack-specific control plane components.
Today, the only component deployed by the OpenStack extension is the <code>cloud-controller-manager</code>.</p><p>An example <code>ControlPlaneConfig</code> for the OpenStack extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
kind: ControlPlaneConfig
loadBalancerProvider: haproxy
loadBalancerClasses:
- name: lbclass-1
  purpose: default
  floatingNetworkID: fips-1-id
  floatingSubnetName: internet-*
- name: lbclass-2
  floatingNetworkID: fips-1-id
  floatingSubnetTags: internal,private
- name: lbclass-3
  purpose: private
  subnetID: internal-id
cloudControllerManager:
  featureGates:
    CustomResourceValidation: <span style=color:#00f>true</span>
</code></pre></div><p>The <code>loadBalancerProvider</code> is the provider name you want to use for load balancers in your shoot.
If you don&rsquo;t know which types are available look it up in the respective <code>CloudProfile</code>.</p><p>The <code>loadBalancerClasses</code> field contains an optional list of load balancer classes which will be available in the cluster. Each entry can have the following fields:</p><ul><li><code>name</code> to select the load balancer class via the kubernetes <a href=https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/openstack-cloud-controller-manager/expose-applications-using-loadbalancer-type-service.md#switching-between-floating-subnets-by-using-preconfigured-classes>service annotations</a> <code>loadbalancer.openstack.org/class=name</code></li><li><code>purpose</code> with values <code>default</code> or <code>private</code><ul><li>The configuration of the <code>default</code> load balancer class will be used as default for all other kubernetes loadbalancer services without a class annotation</li><li>The configuration of the <code>private</code> load balancer class will be also set to the global loadbalancer configuration of the cluster, but will be overridden by the <code>default</code> purpose</li></ul></li><li><code>floatingNetworkID</code> can be specified to receive an ip from an floating/external network, additionally the subnet in this network can be selected via<ul><li><code>floatingSubnetName</code> can be either a full subnet name or a regex/glob to match subnet name</li><li><code>floatingSubnetTags</code> a comma seperated list of subnet tags</li><li><code>floatingSubnetID</code> the id of a specific subnet</li></ul></li><li><code>subnetID</code> can be specified by to receive an ip from an internal subnet (will not have an effect in combination with floating/external network configuration)</li></ul><p>The <code>cloudControllerManager.featureGates</code> contains a map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommended to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><h2 id=workerconfig><code>WorkerConfig</code></h2><p>Each worker group in a shoot may contain provider-specific configurations and options. These are contained in the <code>providerConfig</code> section of a worker group and can be configured using a <code>WorkerConfig</code> object.
An example of a <code>WorkerConfig</code> looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
kind: WorkerConfig
serverGroup:
  policy: soft-anti-affinity
</code></pre></div><p>When you specify the <code>serverGroup</code> section in your worker group configuration, a new server group will be created with the configured policy for each worker group that enabled this setting and all machines managed by this worker group will be assigned as members of the created server group.</p><p>For users to have access to the server group feature, it must be enabled on the <code>CloudProfile</code> by your operator.
Existing clusters can take advantage of this feature by updating the server group configuration of their respective worker groups. Worker groups that are already configured with server groups can update their setting to change the policy used, or remove it altogether at any time.</p><p>Users must be aware that <strong>any change to the server group settings will result in a rolling deployment of new nodes for the affected worker group</strong>.</p><p>Please note the following restrictions when deploying workers with server groups:</p><ul><li>The <code>serverGroup</code> section is optional, but if it is included in the worker configuration, it must contain a valid policy value.</li><li>The available <code>policy</code> values that can be used, are defined in the provider specific section of <code>CloudProfile</code> by your operator.</li><li>Certain policy values may induce further constraints. Using the <code>affinity</code> policy is only allowed when the worker group utilizes a single zone.</li></ul><h2 id=example-shoot-manifest-one-availability-zone>Example <code>Shoot</code> manifest (one availability zone)</h2><p>Please find below an example <code>Shoot</code> manifest for one availability zone:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1alpha1
kind: Shoot
metadata:
  name: johndoe-openstack
  namespace: garden-dev
spec:
  cloudProfileName: openstack
  region: europe-1
  secretBindingName: core-openstack
  provider:
    type: openstack
    infrastructureConfig:
      apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
      kind: InfrastructureConfig
      floatingPoolName: MY-FLOATING-POOL
      networks:
        workers: 10.250.0.0/19
    controlPlaneConfig:
      apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
      kind: ControlPlaneConfig
      loadBalancerProvider: haproxy
    workers:
    - name: worker-xoluy
      machine:
        type: medium_4_8
      minimum: 2
      maximum: 2
      zones:
      - europe-1a
  networking:
    nodes: 10.250.0.0/16
    type: calico
  kubernetes:
    version: 1.16.1
  maintenance:
    autoUpdate:
      kubernetesVersion: <span style=color:#00f>true</span>
      machineImageVersion: <span style=color:#00f>true</span>
  addons:
    kubernetes-dashboard:
      enabled: <span style=color:#00f>true</span>
    nginx-ingress:
      enabled: <span style=color:#00f>true</span>
</code></pre></div><h2 id=csi-volume-provisioners>CSI volume provisioners</h2><p>Every OpenStack shoot cluster that has at least Kubernetes v1.19 will be deployed with the OpenStack Cinder CSI driver.
It is compatible with the legacy in-tree volume provisioner that was deprecated by the Kubernetes community and will be removed in future versions of Kubernetes.
End-users might want to update their custom <code>StorageClass</code>es to the new <code>cinder.csi.openstack.org</code> provisioner.
Shoot clusters with Kubernetes v1.18 or less will use the in-tree <code>kubernetes.io/cinder</code> volume provisioner in the kube-controller-manager and the kubelet.</p><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-openstack@v1.23</code>.
Note that this feature is only usable for <code>Shoot</code>s whose <code>.spec.kubernetes.version</code> is greater or equal than the CSI migration version (<code>1.19</code>).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-62d6c9c1665ce648d6bce255102f039f>8.4 - Usage As Operator</h1><h1 id=using-the-openstack-provider-extension-with-gardener-as-operator>Using the OpenStack provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1alpha1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for OpenStack and provide an example <code>CloudProfile</code> manifest with minimal configuration that you can use to allow creating OpenStack shoot clusters.</p><h2 id=cloudprofileconfig><code>CloudProfileConfig</code></h2><p>The cloud profile configuration contains information about the real machine image IDs in the OpenStack environment (image names).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the OpenStack extension knows the image ID for every version you want to offer.</p><p>It also contains optional default values for DNS servers that shall be used for shoots.
In the <code>dnsServers[]</code> list you can specify IP addresses that are used as DNS configuration for created shoot subnets.</p><p>Also, you have to specify the keystone URL in the <code>keystoneURL</code> field to your environment.</p><p>Additionally, you can influence the HTTP request timeout when talking to the OpenStack API in the <code>requestTimeout</code> field.
This may help when you have for example a long list of load balancers in your environment.</p><p>In case your OpenStack system uses <a href=https://docs.openstack.org/octavia/latest/>Octavia</a> for network load balancing then you have to set the <code>useOctavia</code> field to <code>true</code> such that the cloud-controller-manager for OpenStack gets correctly configured (it defaults to <code>false</code>).</p><p>Some hypervisors (especially those which are VMware-based) don&rsquo;t automatically send a new volume size to a Linux kernel when a volume is resized and in-use.
For those hypervisors you can enable the storage plugin interacting with Cinder to telling the SCSI block device to refresh its information to provide information about it&rsquo;s updated size to the kernel. You might need to enable this behavior depending on the underlying hypervisor of your OpenStack installation. The <code>rescanBlockStorageOnResize</code> field controls this. Please note that it only applies for Kubernetes versions where CSI is used.</p><p>Some openstack configurations do not allow to attach more volumes than a specific amount to a single node.
To tell the k8s scheduler to not over schedule volumes on a node, you can set <code>nodeVolumeAttachLimit</code> which defaults to 256.
Some openstack configurations have different names for volume and compute availability zones, which might cause pods to go into pending state as there are no nodes available in the detected volume AZ. To ignore the volume AZ when scheduling pods, you can set <code>ignoreVolumeAZ</code> to <code>true</code>, which is only supported for shoot kubernetes version 1.20.x and newer (it defaults to <code>false</code>).
See <a href=https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/cinder-csi-plugin/using-cinder-csi-plugin.md#block-storage>CSI Cinder driver</a>.</p><p>The cloud profile config also contains constraints for floating pools and load balancer providers that can be used in shoots.</p><p>If your OpenStack system supports server groups, the <code>serverGroupPolicies</code> property will enable your end-users to create shoots with workers where the nodes are managed by Nova&rsquo;s server groups.
Specifying <code>serverGroupPolicies</code> is optional and can be omitted. If enabled, the end-user can choose whether or not to use this feature for a shoot&rsquo;s workers. Gardener will handle the creation of the server group and node assignment.</p><p>To enable this feature, an operator should:</p><ul><li>specify the allowed policy values (e.g. <code>affintity</code>, <code>anti-affinity</code>) in this section. Only the policies in the allow-list will be available for end-users.</li><li>make sure that your OpenStack project has enough server group capacity. Otherwise, shoot creation will fail.</li></ul><p>An example <code>CloudProfileConfig</code> for the OpenStack extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
kind: CloudProfileConfig
machineImages:
- name: coreos
  versions:
  - version: 2135.6.0
    image: coreos-2135.6.0
<span style=color:green># keystoneURL: https://url-to-keystone/v3/</span>
<span style=color:green># keystoneURLs:</span>
<span style=color:green># - region: europe</span>
<span style=color:green>#   url: https://europe.example.com/v3/</span>
<span style=color:green># - region: asia</span>
<span style=color:green>#   url: https://asia.example.com/v3/</span>
<span style=color:green># dnsServers:</span>
<span style=color:green># - 10.10.10.11</span>
<span style=color:green># - 10.10.10.12</span>
<span style=color:green># requestTimeout: 60s</span>
<span style=color:green># useOctavia: true</span>
<span style=color:green># useSNAT: true</span>
<span style=color:green># rescanBlockStorageOnResize: true</span>
<span style=color:green># ignoreVolumeAZ: true</span>
<span style=color:green># nodeVolumeAttachLimit: 30</span>
<span style=color:green># serverGroupPolicies:</span>
<span style=color:green># - soft-anti-affinity</span>
<span style=color:green># - anti-affinity</span>
<span style=color:green># resolvConfOptions:</span>
<span style=color:green># - rotate</span>
<span style=color:green># - timeout:1</span>
constraints:
  floatingPools:
  - name: fp-pool-1
<span style=color:green>#   region: europe</span>
<span style=color:green>#   loadBalancerClasses:</span>
<span style=color:green>#   - name: lb-class-1</span>
<span style=color:green>#     floatingSubnetID: &#34;1234&#34;</span>
<span style=color:green>#     floatingNetworkID: &#34;4567&#34;</span>
<span style=color:green>#     subnetID: &#34;7890&#34;</span>
<span style=color:green># - name: &#34;fp-pool-*&#34;</span>
<span style=color:green>#   region: europe</span>
<span style=color:green>#   loadBalancerClasses:</span>
<span style=color:green>#   - name: lb-class-1</span>
<span style=color:green>#     floatingSubnetID: &#34;1234&#34;</span>
<span style=color:green>#     floatingNetworkID: &#34;4567&#34;</span>
<span style=color:green>#     subnetID: &#34;7890&#34;</span>
<span style=color:green># - name: &#34;fp-pool-eu-demo&#34;</span>
<span style=color:green>#   region: europe</span>
<span style=color:green>#   domain: demo</span>
<span style=color:green>#   loadBalancerClasses:</span>
<span style=color:green>#   - name: lb-class-1</span>
<span style=color:green>#     floatingSubnetID: &#34;1234&#34;</span>
<span style=color:green>#     floatingNetworkID: &#34;4567&#34;</span>
<span style=color:green>#     subnetID: &#34;7890&#34;</span>
<span style=color:green># - name: &#34;fp-pool-eu-dev&#34;</span>
<span style=color:green>#   region: europe</span>
<span style=color:green>#   domain: dev</span>
<span style=color:green>#   nonConstraining: true</span>
<span style=color:green>#   loadBalancerClasses:</span>
<span style=color:green>#   - name: lb-class-1</span>
<span style=color:green>#     floatingSubnetID: &#34;1234&#34;</span>
<span style=color:green>#     floatingNetworkID: &#34;4567&#34;</span>
<span style=color:green>#     subnetID: &#34;7890&#34;</span>
  loadBalancerProviders:
  - name: haproxy
<span style=color:green>#   region: europe</span>
<span style=color:green># - name: f5</span>
<span style=color:green>#   region: asia</span>
</code></pre></div><p>Please note that it is possible to configure a region mapping for keystone URLs, floating pools, and load balancer providers.
Additionally, floating pools can be constrainted to a keystone domain by specifying the <code>domain</code> field.
Floating pool names may also contains simple wildcard expressions, like <code>*</code> or <code>fp-pool-*</code> or <code>*-fp-pool</code>. Please note that the <code>*</code> must be either single or at the beginning or at the end. Consequently, <code>fp-*-pool</code> is not possible/allowed.
The default behavior is that, if found, the regional (and/or domain restricted) entry is taken.
If no entry for the given region exists then the fallback value is the most matching entry (w.r.t. wildcard matching) in the list without a <code>region</code> field (or the <code>keystoneURL</code> value for the keystone URLs).
If an additional floating pool should be selectable for a region and/or domain, you can mark it as non constraining
with setting the optional field <code>nonConstraining</code> to <code>true</code>.</p><p>The <code>loadBalancerClasses</code> field is an optional list of load balancer classes which can be when the corresponding floating pool network is choosen. The load balancer classes can be configured in the same way as in the <code>ControlPlaneConfig</code> in the <code>Shoot</code> resource, therefore see <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-openstack/docs/usage-as-end-user/#ControlPlaneConfig>here</a> for more details.</p><p>Some OpenStack environments don&rsquo;t need these regional mappings, hence, the <code>region</code> and <code>keystoneURLs</code> fields are optional.
If your OpenStack environment only has regional values and it doesn&rsquo;t make sense to provide a (non-regional) fallback then simply
omit <code>keystoneURL</code> and always specify <code>region</code>.</p><p>If Gardener creates and manages the router of a shoot cluster, it is additionally possible to specify that the <a href=https://registry.terraform.io/providers/terraform-provider-openstack/openstack/latest/docs/resources/networking_router_v2#enable_snat>enable_snat</a> field is set to <code>true</code> via <code>useSNAT: true</code> in the <code>CloudProfileConfig</code>.</p><p>On some OpenStack enviroments, there may be the need to set options in the file <code>/etc/resolv.conf</code> on worker nodes.
If the field <code>resolvConfOptions</code> is set, a systemd service will be installed which copies <code>/run/systemd/resolve/resolv.conf</code>
on every change to <code>/etc/resolv.conf</code> and appends the given options.</p><h2 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h2><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: CloudProfile
metadata:
  name: openstack
spec:
  type: openstack
  kubernetes:
    versions:
    - version: 1.16.1
    - version: 1.16.0
      expirationDate: <span style=color:#a31515>&#34;2020-04-05T01:02:03Z&#34;</span>
  machineImages:
  - name: coreos
    versions:
    - version: 2135.6.0
  machineTypes:
  - name: medium_4_8
    cpu: <span style=color:#a31515>&#34;4&#34;</span>
    gpu: <span style=color:#a31515>&#34;0&#34;</span>
    memory: 8Gi
    storage:
      class: standard
      type: default
      size: 40Gi
  regions:
  - name: europe-1
    zones:
    - name: europe-1a
    - name: europe-1b
    - name: europe-1c
  providerConfig:
    apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
    kind: CloudProfileConfig
    machineImages:
    - name: coreos
      versions:
      - version: 2135.6.0
        image: coreos-2135.6.0
    keystoneURL: https://url-to-keystone/v3/
    constraints:
      floatingPools:
      - name: fp-pool-1
      loadBalancerProviders:
      - name: haproxy
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-5b5f3488fb6707d456d4d70245b0f4ab>9 - Provider vSphere</h1><div class=lead>Gardener extension controller for the vSphere cloud provider</div><h1 id=gardener-extension-for-vsphere-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for vSphere provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-vsphere-main/jobs/main-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-vsphere-main/jobs/main-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-vsphere><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-vsphere alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=/docs/gardener/proposals/01-extensibility/>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the VMware vSphere provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-vsphere/blob/main/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=/docs/gardener/proposals/01-extensibility/>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.23</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.22</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.21</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.20</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.19</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.18</td><td>untested</td><td>not yet available</td></tr><tr><td>Kubernetes 1.17</td><td>1.17.0+</td><td>not yet available</td></tr></tbody></table><p>Please take a look <a href=/docs/gardener/usage/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-vsphere/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=/docs/gardener/proposals/01-extensibility/>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=/docs/gardener/proposals/04-new-core-gardener-cloud-apis/>GEP-4 (New <code>core.gardener.cloud/v1alpha1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-35a78d43d0bac7f0de61739ce0338d7f>9.1 - Deployment</h1><h1 id=deployment-of-the-vsphere-provider-extension>Deployment of the vSphere provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the vSphere provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the vSphere provider extension <a href=https://github.com/gardener/gardener-extension-provider-vsphere>repository</a>.</p><h2 id=gardener-extension-validator-vsphere>gardener-extension-validator-vsphere</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-validator-vsphere</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: &lt;CA-DATA&gt;
    server: https://default.kubernetes.svc.cluster.local
  name: garden
contexts:
- context:
    cluster: garden
    user: garden
  name: garden
current-context: garden
users:
- name: garden
  user:
    tokenFile: /var/run/secrets/projected/serviceaccount/token
</code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: &lt;CA-DATA&gt;
    server: https://virtual-garden.api
  name: virtual-garden
contexts:
- context:
    cluster: virtual-garden
    user: virtual-garden
  name: virtual-garden
current-context: virtual-garden
users:
- name: virtual-garden
  user:
    tokenFile: /var/run/secrets/projected/serviceaccount/token
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-e2908e6901d240206d5754a946ddf138>9.2 - Local Setup</h1><h3 id=admission-vsphere>admission-vsphere</h3><p><code>admission-vsphere</code> is an admission webhook server which is responsible for the validation of the cloud provider (vSphere in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/development/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>make start-admission
</code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-vsphere.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>./hack/dev-setup-admission-vsphere.sh
</code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-vsphere</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-636e168a1b7671fa4ceef9e39ea82a55>9.3 - Prepare Vsphere</h1><h1 id=vsphere--nsx-t-preparation-for-gardener-extension-vsphere-provider>vSphere / NSX-T Preparation for Gardener Extension &ldquo;vSphere Provider&rdquo;</h1><ul><li><a href=#vsphere--nsx-t-preparation-for-gardener-extension-vsphere-provider>vSphere / NSX-T Preparation for Gardener Extension &ldquo;vSphere Provider&rdquo;</a><ul><li><a href=#vsphere-preparation>vSphere Preparation</a><ul><li><a href=#create-folders>Create Folders</a></li><li><a href=#upload-vm-templates-for-worker-nodes>Upload VM Templates for Worker Nodes</a></li><li><a href=#prepare-for-kubernetes-zones-and-regions>Prepare for Kubernetes Zones and Regions</a><ul><li><a href=#create-resource-pools>Create Resource Pool(s)</a></li><li><a href=#tag-regions-and-zones>Tag Regions and Zones</a></li><li><a href=#storage-policies>Storage policies</a><ul><li><a href=#tag-zone-storages>Tag Zone Storages</a></li><li><a href=#create-or-clone-vm-storage-policy>Create or clone VM Storage Policy</a></li></ul></li></ul></li></ul></li><li><a href=#nsx-t-prepartion>NSX-T Prepartion</a><ul><li><a href=#create-ip-pools>Create IP pools</a><ul><li><a href=#sizing-the-ip-pools>Sizing the IP pools</a></li></ul></li><li><a href=#check-edge-cluster-sizing>Check edge cluster sizing</a></li></ul></li><li><a href=#get-vds-uuids>Get VDS UUIDs</a></li></ul></li></ul><p>Several preparational steps are necessary for VMware vSphere and NSX-T, before this extension can be used
to create Gardener shoot clusters.</p><p>The main version target of this extension is vSphere 7.x together with NSX-T 3.x.
The recommended environment is a system setup with VMware Cloud Foundation (VCF) 4.1.
Older versions like vSphere 6.7U3 with NSX-T 2.5 or 3.0 should still work, but are not tested extensively.</p><h2 id=vsphere-preparation>vSphere Preparation</h2><h3 id=create-folders>Create Folders</h3><p>Two folders need to be created:
- a folder which will contain the VMs of the shoots (cloud profile <code>spec.providerConfig.folder</code>)
- a folder containing templates (used by cloud profile <code>spec.providerConfig.machineImages[*].versions[*].path</code>)</p><p>In vSphere client:</p><ol><li>From the <em>Menu</em> in the vSphere Client toolbar choose <em>VMs and Templates</em></li><li>Select the vSphere Datacenter of the work load vCenter in the browser</li><li>From the context menu select <em>New Folder</em> > <em>New VM and Template Folder</em>, set folder name to e.g. &ldquo;gardener&rdquo;</li><li>From the context menu of the new folder <em>gardener</em> select <em>New Folder</em>, set folder name to &ldquo;templates&rdquo;</li></ol><h3 id=upload-vm-templates-for-worker-nodes>Upload VM Templates for Worker Nodes</h3><p>Upload gardenlinux OVA (or OVF/VMDK) templates.</p><ol><li>From the context menu of the folder <code>gardener/templates</code> choose <em>Deploy OVF Template&mldr;</em></li><li>Adjust name if needed</li><li>Select any compute cluster as compute resource</li><li>Select a storage (e.g. VSAN)</li><li>Select any network (not important)</li><li>No need to customize the template</li><li>After deployment is finished select from the context menu of the new deployed VM <em>Template</em> > <em>Convert To Template</em></li></ol><h3 id=prepare-for-kubernetes-zones-and-regions>Prepare for Kubernetes Zones and Regions</h3><p>If the vSphere infrastructure is setup for multiple availabilities zones and Kubernetes should be topology aware, there need to be defined two labels in the cloud profile (section <code>spec.providerConfig.failureDomainLabels</code>)</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>    failureDomainLabels:
      region: k8s-region
      zone: k8s-zone
</code></pre></div><p>See also: <a href=https://vsphere-csi-driver.sigs.k8s.io/driver-deployment/deploying_csi_with_zones.html>deploying_csi_with_zones</a></p><p>A Kubernetes zone can either be a vCenter or one of its datacenters</p><p>Zones must be subresources of it. If the region is a complete vCenter, the zone must specify datacenter and either compute cluster or resource pool.
Otherwise, i.e. tf the region is a datacenter, the zone must specify either compute cluster or resource pool.</p><p>In the following steps it is assumed:
- the region is specified by a datacenter
- the zone is specified by a compute clusters or one of its resource pools</p><h4 id=create-resource-pools>Create Resource Pool(s)</h4><p>Create a resource pool for every zone:</p><ol><li>From the <em>Menu</em> in the vSphere Client toolbar choose <em>Hosts and Clusters</em></li><li>From the context menu of the compute cluster select <em>New Resource Pool&mldr;</em> and provide the name of the zone. CPU and Memory settings are optional.</li></ol><h4 id=tag-regions-and-zones>Tag Regions and Zones</h4><p>Eeach zone must be tagged with the category defined by the label defined in the cloud profile (<code>spec.providerConfig.failureDomainLabels.region</code>).
Assuming that the region is a datacenter and the region label is <code>k8s-region</code>:</p><ol><li>From the <em>Menu</em> in the vSphere Client toolbar choose <em>Hosts and Clusters</em></li><li>Select the region&rsquo;s datacenter in the browser</li><li>In the <em>Summary</em> tab there is a subwindow titled <em>Tags</em>. Click the <em>Assign&mldr;</em> link.</li><li>In the <em>Assign Tag</em> dialog select the <em>ADD TAG</em> link above of the table</li><li>In the <em>Create Tag</em> dialog choose the <em>k8s-region</em> category. If it is not defined, click the <em>Create New Category</em> link to create the category.</li><li>Enter the <em>Name</em> of the region.</li><li>Back in the <em>Assign Tag</em> mark the checkbox of the region tag you just have created.</li><li>Click the <em>ASSIGN</em> button</li></ol><p>Assuiming that the zone are specified by resource pools and the zone label is <code>k8s-zone</code>:</p><ol><li>From the <em>Menu</em> in the vSphere Client toolbar choose <em>Hosts and Clusters</em></li><li>Select the zone&rsquo;s compute cluster in the browser</li><li>In the <em>Summary</em> tab there is a subwindow titled <em>Tags</em>. Click the <em>Assign&mldr;</em> link.</li><li>In the <em>Assign Tag</em> dialog select the <em>ADD TAG</em> link above of the table</li><li>In the <em>Create Tag</em> dialog choose the <em>k8s-zone</em> category. If it is not defined, click the <em>Create New Category</em> link to create the category.</li><li>Enter the <em>Name</em> of the zone.</li><li>Back in the <em>Assign Tag</em> mark the checkbox of the zone tag you just have created.</li><li>Click the <em>ASSIGN</em> button</li></ol><h4 id=storage-policies>Storage policies</h4><p>Each zone can have a separate storage. In this case a storage policy is needed to be compatible with all the zone storages.</p><h5 id=tag-zone-storages>Tag Zone Storages</h5><p>For each zone tag the storage with the corresponding <code>k8s-zone</code> tag for the zone.</p><ol><li>From the <em>Menu</em> in the vSphere Client toolbar choose <em>Storage</em></li><li>Select the zone&rsquo;s storage in the browser</li><li>In the <em>Summary</em> tab there is a subwindow titled <em>Tags</em>. Click the <em>Assign&mldr;</em> link.</li><li>In the <em>Assign Tag</em> dialog select the <em>ADD TAG</em> link above of the table</li><li>In the <em>Create Tag</em> dialog choose the <em>k8s-zone</em> category. If it is not defined, click the <em>Create New Category</em> link to create the category.</li><li>Enter the <em>Name</em> of the zone.</li><li>Back in the <em>Assign Tag</em> mark the checkbox of the zone tag you just have created.</li><li>Click the <em>ASSIGN</em> button</li></ol><h5 id=create-or-clone-vm-storage-policy>Create or clone VM Storage Policy</h5><ol><li><p>From the <em>Menu</em> in the vSphere Client toolbar choose <em>Policies and Profiles</em></p></li><li><p>In the <em>Policiies and Profiles</em> list select <em>VM Storage Policies</em></p></li><li><p>Create or clone an exisitng storage policy</p><p>a) set name, e.g. &ldquo;&lt;region-name> Storage Policy&rdquo; (will be needed for the cloud profile later)</p><p>b) On the page <em>Policy structure</em> check only the checkbox <em>Enable tag based placement rules</em></p><p>c) On the page <em>Tage based placement</em> press the <em>ADD TAG RULE</em> button.</p><p>d) For <em>Rule 1</em> select</p><pre><code>*Tag category* =  *k8s-zone*
*Usage option* = *Use storage tagged with*
*Tags* = *all zone tags*.
</code></pre><p>e) Validate the compatible storages on the page <em>Storage compatibility</em></p><p>f) Press <em>FINISH</em> on the <em>Review and finish</em> page</p></li></ol><h2 id=nsx-t-prepartion>NSX-T Prepartion</h2><p>A shared NSX-T is needed for all zones of a region.
External IP address ranges are needed for SNAT and load balancers.
Besides the edge cluster must sized large enough to deal with the load balancers of all shoots.</p><h3 id=create-ip-pools>Create IP pools</h3><p>Two IP pools are needed for external IP addresses.</p><ol><li>IP pool for <strong>SNAT</strong>
The IP pool name needs to be specified in the cloud profile at <code>spec.providerConfig.regions[*].snatIPPool</code>. Each shoot cluster needs one SNAT IP address for outgoing traffic.</li><li>IP pool(s) for the <strong>load balancers</strong>
The IP pool name(s) need to be specified in the cloud profile at <code>spec.providerConfig.contraints.loadBalancerConfig.classes[*].ipPoolName</code>. An IP address is needed for every port of every Kubernetes service of type <code>LoadBalancer</code>.</li></ol><p>To create them, follow these steps in the NSX-T Manager UI in the web browser:</p><ol><li>From the <em>toolbar</em> at the top of the page choose <em>Networking</em></li><li>From the left side list choose <em>IP Address Pools</em> below the <em>IP Management</em></li><li>Press the <em>ADD IP ADRESS POOL</em> button</li><li>Enter <em>Name</em></li><li>Enter at least one subnet by clicking on <em>Sets</em></li><li>Press the <em>Save</em> button</li></ol><h4 id=sizing-the-ip-pools>Sizing the IP pools</h4><p>Each shoot cluster needs one IP address for SNAT and at least two IP addresses for load balancers VIPs (kube-apiservcer and Gardener shoot-seed VPN). A third IP address may be needed for ingress.
Depending on the payload of a shoot cluster, there may be additional services of type <code>LoadBalancer</code>. An IP address is needed for every port of every Kubernetes service of type <code>LoadBalancer</code>.</p><h3 id=check-edge-cluster-sizing>Check edge cluster sizing</h3><p>For load balancer related configurations limitations of NSX-T, please see the web pages <a href="https://configmax.vmware.com/guest?vmwareproduct=NSX-T%20Data%20Center&release=NSX-T%20Data%20Center%203.1.0&categories=20-0">VMware Configuration Maximums</a>. The link shows the limitations for NSX-T 3.1, if you have another version, please select the version from the left panel under <em>Select Version</em> and press the <em>VIEW LIMITS</em> button to update the view.</p><p>By default settings, each shoot cluster has an own T1 gateway and an own LB service (instance) of &ldquo;T-shirt&rdquo; size <code>SMALL</code>.</p><p>Examples for limitations on NSX-T 3.1 using <em>Large Edge Node</em> and <em>SMALL</em> load balancers instances:</p><ol><li><p>There is a limit of 40 small LB instances per egde cluster (for HA 40 per pair of edge nodes)</p><p>=> maximum number of shoot clusters = 40 * (number of edge nodes) / 2</p></li><li><p>For <code>SMALL</code> load balancers, there is a maximum of 20 virtual servers. A virtual server is needed for every port of a service of type <code>LoadBalancer</code></p><p>=> maximum number of services/ports pairs = 20 * (number of edge nodes) / 2</p><p>The load balancer &ldquo;T-shirt&rdquo; size can be set on cloud profile level (<code>spec.providerConfig.contraints.loadBalancerConfig.size</code>) or in the shoot manifest (<code>spec.provider.controlPlaneConfig.loadBalancerSize</code>)</p></li><li><p>The number of pool members is limited to 7,500. For every K8s service port, every worker node is a pool member.</p><p>=> If every shoot cluster has an average number of 15 worker nodes, there can be 500 service/port pairs over all shoot clusters per pair of edge nodes</p></li></ol><h2 id=get-vds-uuids>Get VDS UUIDs</h2><p>This step is only needed, if there are several VDS (virtual distributed switches) for each zone.</p><p>In this case, their UUIDs need to be fetched and set in the cloud profile at <code>spec.providerConfig.regions[*].zones[*].switchUuid</code>.</p><p>Unfortunately, they are not displayed in the vSphere Client.</p><p>Here the command line tool <code>govc</code> is used to look them
up.</p><ol><li>Run <code>govc find / -type DistributedVirtualSwitch</code> to get the full path of all vds/dvs</li><li>For each switch run <code>govc dvs.portgroup.info &lt;switch-path> | grep DvsUuid</code></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-fdbdddf9cd1476016152695500fac711>9.4 - Tanzu Vsphere</h1><h2 id=create-tanzu-cluster>Create Tanzu Cluster</h2><p>For gardener a Tanzu Kubernetes „guest” cluster is used. Look here for the vSphere documentation <a href=https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-2597788E-2FA4-420E-B9BA-9423F8F7FD9F.html>Provisioning Tanzu Kubernetes Clusters</a></p><h3 id=virtual-machine-classes>Virtual Machine Classes</h3><p>For gardener the minimum Virtual Machine Classes must set to <code>best-effort-large</code>.</p><h3 id=network-settings>Network Settings</h3><p>For the deployment it is possible to provision the cluster with a minimal amount of configuration parameter. It is recommended to set the parameter <code>Default Pod CIDR</code>, <code>Default Services CIDR</code> with values which fit to your enviroment.</p><h3 id=storage-class-settings>Storage Class settings</h3><p>The <code>storageClass</code> Parameter should be defined to avoid problems during deployment.</p><p>Example:</p><pre><code>```yaml
apiVersion: run.tanzu.vmware.com/v1alpha1      #TKG API endpoint
kind: TanzuKubernetesCluster                   #required parameter
metadata:
name: tkg-cluster-1                          #cluster name, user defined
namespace: ns1                               #supervisor namespace
spec:
distribution:
    version: v1.17				 #resolved kubernetes version
topology:
    controlPlane:
    count: 1                                 #number of control plane nodes
    class: best-effort-small                 #vmclass for control plane nodes
    storageClass: vsan-default-storage-policy         #storageclass for control plane
    workers:
    count: 3                                 #number of worker nodes
    class: best-effort-large                 #vmclass for worker nodes
    storageClass: vsan-default-storage-policy         #storageclass for worker nodes
settings:
    network:
    cni:
        name: calico
    services:
        cidrBlocks: [&quot;198.51.100.0/12&quot;]        #Cannot overlap with Supervisor Cluster
    pods:
        cidrBlocks: [&quot;192.0.2.0/16&quot;]           #Cannot overlap with Supervisor Cluster
```
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-cdac9defb035df31c6dfc2cee8cbc281>9.5 - Usage As End User</h1><h1 id=using-the-vsphere-provider-extension-with-gardener-as-end-user>Using the vSphere provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1alpha1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for VMware vSphere and provide an example <code>Shoot</code> manifest with minimal configuration that you can use to create an vSphere cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=provider-secret-data>Provider secret data</h2><p>Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of your vSphere tenant.
It contains two authentication sets. One for the vSphere host and another for the NSX-T host, which is needed to set up the network infrastructure.
This <code>Secret</code> must look as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: v1
kind: Secret
metadata:
  name: core-vsphere
  namespace: garden-dev
type: Opaque
data:
  vspherePassword: base64(vsphere-password)
  vsphereInsecureSSL: base64(&#34;true&#34;|&#34;false&#34;)
  nsxtPassword: base64(NSX-T-password)
  nsxtInsecureSSL: base64(&#34;true&#34;|&#34;false&#34;)
</code></pre></div><p>Here <code>base64(...)</code> are only a placeholders for the Base64 encoded values.</p><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration is used for advanced scenarios only.
Nodes on all zones are using IP addresses from the common nodes network as the network is managed by NSX-T.
The infrastructure controller will create several network objects using NSX-T. A network segment is used as the subnet
for the VMs (nodes), a tier-1 gateway, a DHCP server, and a SNAT for the nodes.</p><p>An example <code>InfrastructureConfig</code> for the vSphere extension looks as follows.
You only need to specify it, if you either want to use an existing Tier-1 gateway and load balancer service pair
or if you want to overwrite the automatic selection of the NSX-T version.</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>infrastructureConfig:
  apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
  kind: InfrastructureConfig
  <span style=color:green>#overwriteNSXTInfraVersion: &#39;1&#39;</span>
  <span style=color:green>#networks:</span>
  <span style=color:green>#  tier1GatewayPath: /infra/tier-1s/tier1gw-b8213651-9659-4180-8bfd-1e16228e8dcb</span>
  <span style=color:green>#  loadBalancerServicePath: /infra/lb-services/708c5cb1-e5d0-4b16-906f-ec7177a1485d</span>
</code></pre></div><h3 id=advanced-configuration-settings>Advanced configuration settings</h3><h4 id=section-networks>Section networks</h4><p>By default, the infrastructure controller creates a separate Tier-1 gateway for each shoot cluster
and the cloud controller manager (<code>vsphere-cloud-provider</code>) creates a load balancer service.</p><p>If an existing Tier-1 gateway should be used, you can specify its &lsquo;path&rsquo;. In this case, there
must also be a load balancer service defined for this tier-1 gateway and its &lsquo;path&rsquo; needs to be specified, too.
In the NSX-T manager UI, the path of the tier-1 gateway can be found at <code>Networking / Tier-1 Gateways</code>.
Then select <code>Copy path to clipboard</code> from the context menu of the tier-1 gateway
(click on the three vertical dots on the left of the row). Do the same with the
corresponding load balancer at <code>Networking / Load balancing / Tab Load Balancers</code>
For security reasons the referenced Tier-1 gateway in NSX-T must have a tag with scope <code>authorized-shoots</code> and its
tag value consists of a comma-separated list of the allowed shoot names in the format <code>shoot--&lt;project>--&lt;name></code>
(optionally with wildcard <code>*</code>). Additionally, it must have a tag with scope <code>garden</code> set to the garden ID.</p><p>Example:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>infrastructureConfig:
  apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
  kind: InfrastructureConfig
  networks:
    tier1GatewayPath: /infra/tier-1s/tier1gw-b8213651-9659-4180-8bfd-1e16228e8dcb
    loadBalancerServicePath: /infra/lb-services/708c5cb1-e5d0-4b16-906f-ec7177a1485d
</code></pre></div><p>Please ensure, that the worker nodes cidr (shoot manifest <code>spec.networking.nodes</code>) do not overlap with
other existing segments of the selected tier-1 gateway.</p><h4 id=option-overwritensxtinfraversion>Option overwriteNSXTInfraVersion</h4><p>The option <code>overwriteNSXTInfraVersion</code> can be used to change the network objects created during the initial infrastructure creation.
By default the infra-version is automatically selected according to the NSX-T version. The infra-version <code>'1'</code> is used
for NSX-T 2.5, and infra-version <code>'2'</code> for NSX-T versions >= 3.0. The difference is creation of the the logical DHCP server.
For NSX-T 2.5, only the DHCP server of the &ldquo;Advanced API&rdquo; is usable. For NSX-T >= 3.0 the new DHCP server is default,
but for special purposes infra-version <code>'1'</code> is also allowed.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the vSphere-specific control plane components.
Today, the only component deployed by the vSphere extension is the <code>cloud-controller-manager</code>.</p><p>An example <code>ControlPlaneConfig</code> for the vSphere extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
kind: ControlPlaneConfig
loadBalancerClasses:
  - name: mypubliclbclass
  - name: myprivatelbclass
    ipPoolName: pool42 <span style=color:green># optional overwrite</span>
loadBalancerSize: SMALL
cloudControllerManager:
  featureGates:
    CustomResourceValidation: <span style=color:#00f>true</span>
</code></pre></div><p>The <code>loadBalancerClasses</code> optionally defines the load balancer classes to be used.
The specified names must be defined in the constraints section of the cloud profile.
If the list contains a load balancer named &ldquo;default&rdquo;, it is used as the default load balancer.
Otherwise the first one is also the default.
If no classes are specified the default load balancer class is used as defined in the cloud profile constraints section.
If the ipPoolName is overwritten, the corresponding IP pool object in NSX-T must have a tag with scope <code>authorized-shoots</code> and its
tag value consists of a comma-separated list of the allowed shoot names in the format <code>shoot--&lt;project>--&lt;name></code>
(optionally with wildcard <code>*</code>). Additionally, it must have a tag with scope <code>garden</code> set to the garden ID.</p><p>The <code>loadBalancerSize</code> is optional and overwrites the default value specified in the cloud profile config.
It must be one of the values <code>SMALL</code>, <code>MEDIUM</code>, or <code>LARGE</code>. <code>SMALL</code> can manage 10 service ports,
<code>MEDIUM</code> 100, and <code>LARGE</code> 1000.</p><p>The <code>cloudControllerManager.featureGates</code> contains an optional map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><h2 id=example-shoot-manifest-one-availability-zone>Example <code>Shoot</code> manifest (one availability zone)</h2><p>Please find below an example <code>Shoot</code> manifest for one availability zone:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1alpha1
kind: Shoot
metadata:
  name: johndoe-vsphere
  namespace: garden-dev
spec:
  cloudProfileName: vsphere
  region: europe-1
  secretBindingName: core-vsphere
  provider:
    type: vsphere
   
    <span style=color:green>#infrastructureConfig:</span>
    <span style=color:green>#  apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1</span>
    <span style=color:green>#  kind: InfrastructureConfig</span>
    <span style=color:green>#  overwriteNSXTInfraVersion: &#39;1&#39;</span>

    controlPlaneConfig:
      apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
      kind: ControlPlaneConfig
    <span style=color:green>#  loadBalancerClasses:</span>
    <span style=color:green>#  - name: mylbclass</span>

    workers:
    - name: worker-xoluy
      machine:
        type: std-04
      minimum: 2
      maximum: 2
      zones:
      - europe-1a
  networking:
    nodes: 10.250.0.0/16
    type: calico
  kubernetes:
    version: 1.16.1
  maintenance:
    autoUpdate:
      kubernetesVersion: <span style=color:#00f>true</span>
      machineImageVersion: <span style=color:#00f>true</span>
  addons:
    kubernetes-dashboard:
      enabled: <span style=color:#00f>true</span>
    nginx-ingress:
      enabled: <span style=color:#00f>true</span>
</code></pre></div><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-vsphere@v0.12</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0e203669ce71e2ac15660f0895763756>9.6 - Usage As Operator</h1><h1 id=using-the-vsphere-provider-extension-with-gardener-as-operator>Using the vSphere provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1alpha1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for VMware vSphere and provide an example <code>CloudProfile</code> manifest with minimal configuration that you can use to allow creating vSphere shoot clusters.</p><h2 id=cloudprofileconfig><code>CloudProfileConfig</code></h2><p>The cloud profile configuration contains information about the real machine image paths in the vSphere environment (image names).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the vSphere extension knows the image ID for every version you want to offer.</p><p>It also contains optional default values for DNS servers that shall be used for shoots.
In the <code>dnsServers[]</code> list you can specify IP addresses that are used as DNS configuration for created shoot subnets.</p><p>The <code>dhcpOptions</code> list allows to specify DHCP options. See <a href=https://www.iana.org/assignments/bootp-dhcp-parameters/bootp-dhcp-parameters.xhtml>BOOTP Vendor Extensions and DHCP Options</a>
for valid codes (tags) and details about values. The code <code>15</code> (domain name) is only allowed for
when using NSX-T 2.5. For NSX-T >= 3.0 use <code>119</code> (search domain).</p><p>The <code>dockerDaemonOptions</code> allow to adjust the docker daemon configuration.</p><ul><li>with <code>dockerDaemonOptions.httpProxyConf</code> the content of the proxy configuration file can be set.
See <a href=https://docs.docker.com/config/daemon/systemd/#httphttps-proxy>Docker HTTP/HTTPS proxy</a> for more details</li><li>with <code>dockerDaemonOptions.insecureRegistries</code> insecure registries can be specified. This
should only be used for development or evaluation purposes.</li></ul><p>Also, you have to specify several name of NSX-T objects in the constraints.</p><p>An example <code>CloudProfileConfig</code> for the vSphere extension looks as follows:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
kind: CloudProfileConfig
namePrefix: my_gardener
defaultClassStoragePolicyName: <span style=color:#a31515>&#34;vSAN Default Storage Policy&#34;</span>
folder: my-vsphere-vm-folder
regions:
- name: region1
  vsphereHost: my.vsphere.host
  vsphereInsecureSSL: <span style=color:#00f>true</span>
  nsxtHost: my.vsphere.host
  nsxtInsecureSSL: <span style=color:#00f>true</span>
  transportZone: <span style=color:#a31515>&#34;my-tz&#34;</span>
  logicalTier0Router: <span style=color:#a31515>&#34;my-tier0router&#34;</span>
  edgeCluster: <span style=color:#a31515>&#34;my-edgecluster&#34;</span>
  snatIpPool: <span style=color:#a31515>&#34;my-snat-ip-pool&#34;</span>
  datacenter: my-vsphere-dc
  zones:
  - name: zone1
    computeCluster: my-vsphere-computecluster1
    <span style=color:green># resourcePool: my-resource-pool1 # provide either computeCluster or resourcePool or hostSystem</span>
    <span style=color:green># hostSystem: my-host1 # provide either computeCluster or resourcePool or hostSystem</span>
    datastore: my-vsphere-datastore1
    <span style=color:green>#datastoreCluster: my-vsphere-datastore-cluster # provide either datastore or datastoreCluster</span>
  - name: zone2
    computeCluster: my-vsphere-computecluster2
    <span style=color:green># resourcePool: my-resource-pool2 # provide either computeCluster or resourcePool or hostSystem</span>
    <span style=color:green># hostSystem: my-host2 # provide either computeCluster or resourcePool or hostSystem</span>
    datastore: my-vsphere-datastore2
    <span style=color:green>#datastoreCluster: my-vsphere-datastore-cluster # provide either datastore or datastoreCluster</span>
constraints:
  loadBalancerConfig:
    size: MEDIUM
    classes:
    - name: default
      ipPoolName: gardener_lb_vip
<span style=color:green># optional DHCP options like 119 (search domain), 42 (NTP), 15 (domain name (only NSX-T 2.5))</span>
<span style=color:green>#dhcpOptions:</span>
<span style=color:green>#- code: 15</span>
<span style=color:green>#  values:</span>
<span style=color:green>#  - foo.bar.com</span>
<span style=color:green>#- code: 42</span>
<span style=color:green>#  values:</span>
<span style=color:green>#  - 136.243.202.118</span>
<span style=color:green>#  - 80.240.29.124</span>
<span style=color:green>#  - 78.46.53.8</span>
<span style=color:green>#  - 162.159.200.123</span>
dnsServers:
- 10.10.10.11
- 10.10.10.12
machineImages:
- name: coreos
  versions:
  - version: 2191.5.0
    path: gardener/templates/coreos-2191.5.0
    guestId: coreos64Guest
<span style=color:green>#dockerDaemonOptions:</span>
<span style=color:green>#  httpProxyConf: |</span>
<span style=color:green>#    [Service]</span>
<span style=color:green>#    Environment=&#34;HTTPS_PROXY=https://proxy.example.com:443&#34;</span>
<span style=color:green>#  insecureRegistries:</span>
<span style=color:green>#  - myregistrydomain.com:5000</span>
<span style=color:green>#  - blabla.mycompany.local</span>
</code></pre></div><h2 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h2><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>apiVersion: core.gardener.cloud/v1beta1
kind: CloudProfile
metadata:
  name: vsphere
spec:
  type: vsphere
  providerConfig:
    apiVersion: vsphere.provider.extensions.gardener.cloud/v1alpha1
    kind: CloudProfileConfig
    namePrefix: my_gardener
    defaultClassStoragePolicyName: <span style=color:#a31515>&#34;vSAN Default Storage Policy&#34;</span>
    folder: my-vsphere-vm-folder
    regions:
    - name: region1
      vsphereHost: my.vsphere.host
      vsphereInsecureSSL: <span style=color:#00f>true</span>
      nsxtHost: my.vsphere.host
      nsxtInsecureSSL: <span style=color:#00f>true</span>
      transportZone: <span style=color:#a31515>&#34;my-tz&#34;</span>
      logicalTier0Router: <span style=color:#a31515>&#34;my-tier0router&#34;</span>
      edgeCluster: <span style=color:#a31515>&#34;my-edgecluster&#34;</span>
      snatIpPool: <span style=color:#a31515>&#34;my-snat-ip-pool&#34;</span>
      datacenter: my-vsphere-dc
      zones:
      - name: zone1
        computeCluster: my-vsphere-computecluster1
        <span style=color:green># resourcePool: my-resource-pool1 # provide either computeCluster or resourcePool or hostSystem</span>
        <span style=color:green># hostSystem: my-host1 # provide either computeCluster or resourcePool or hostSystem</span>
        datastore: my-vsphere-datastore1
        <span style=color:green>#datastoreCluster: my-vsphere-datastore-cluster # provide either datastore or datastoreCluster</span>
      - name: zone2
        computeCluster: my-vsphere-computecluster2
        <span style=color:green># resourcePool: my-resource-pool2 # provide either computeCluster or resourcePool or hostSystem</span>
        <span style=color:green># hostSystem: my-host2 # provide either computeCluster or resourcePool or hostSystem</span>
        datastore: my-vsphere-datastore2
        <span style=color:green>#datastoreCluster: my-vsphere-datastore-cluster # provide either datastore or datastoreCluster</span>
    constraints:
      loadBalancerConfig:
        size: MEDIUM
        classes:
        - name: default
          ipPoolName: gardener_lb_vip
    dnsServers:
    - 10.10.10.11
    - 10.10.10.12
    machineImages:
    - name: coreos
      versions:
      - version: 2191.5.0
        path: gardener/templates/coreos-2191.5.0
        guestId: coreos64Guest
  kubernetes:
    versions:
    - version: 1.15.4
    - version: 1.16.0
    - version: 1.16.1
  machineImages:
  - name: coreos
    versions:
    - version: 2191.5.0
  machineTypes:
  - name: std-02
    cpu: <span style=color:#a31515>&#34;2&#34;</span>
    gpu: <span style=color:#a31515>&#34;0&#34;</span>
    memory: 8Gi
    usable: <span style=color:#00f>true</span>
  - name: std-04
    cpu: <span style=color:#a31515>&#34;4&#34;</span>
    gpu: <span style=color:#a31515>&#34;0&#34;</span>
    memory: 16Gi
    usable: <span style=color:#00f>true</span>
  - name: std-08
    cpu: <span style=color:#a31515>&#34;8&#34;</span>
    gpu: <span style=color:#a31515>&#34;0&#34;</span>
    memory: 32Gi
    usable: <span style=color:#00f>true</span>
  regions:
  - name: region1
    zones:
    - name: zone1
    - name: zone2
</code></pre></div><h2 id=which-versions-of-kubernetesvsphere-are-supported>Which versions of Kubernetes/vSphere are supported</h2><p>This extension targets Kubernetes >= <code>v1.15</code> and vSphere <code>6.7 U3</code> or later.</p><ul><li>vSphere CSI driver needs vSphere <code>6.7 U3</code> or later,
and Kubernetes >= <code>v1.14</code>
(see <a href=https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/2.0/vmware-vsphere-csp-getting-started/GUID-E59B13F5-6F49-4619-9877-DF710C365A1E.html>feature metrics</a> for more details)</li><li>vSpere CPI driver needs vSphere <code>6.7 U3</code> or later,
and Kubernetes >= <code>v1.11</code>
(see <a href=https://github.com/kubernetes/cloud-provider-vsphere/blob/master/docs/book/cloud_provider_interface.md#which-versions-of-kubernetesvsphere-support-it>cloud-provider-vsphere CPI - Cloud Provider Interface</a>)</li></ul><h2 id=supported-vm-images>Supported VM images</h2><p>Currently, only CoreOS and Flatcar (CoreOS fork) are supported.
Virtual Machine Hardware must be version 15 or higher, but images are upgraded
automatically if their hardware has an older version.</p></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2022 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js integrity=sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/mermaid@8.9.2/dist/mermaid.min.js integrity=sha384-uQikAXnCAqsMb3ygtdqBYvcwvHUkzGIpjdGyy9owhURXHUxLC5LgTcSxJQH/RzjK crossorigin=anonymous></script><script src=/js/main.min.ef8e0714aff556fd5a9768ed6ecabd2964dd962cd9f89762a373947bb53bc742.js integrity="sha256-744HFK/1Vv1al2jtbsq9KWTdlizZ+Jdio3OUe7U7x0I=" crossorigin=anonymous></script></body></html>