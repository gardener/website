<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.95.0"><link rel=canonical type=text/html href=https://gardener.cloud/docs/other-components/etcd-backup-restore/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/other-components/etcd-backup-restore/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Etcd Backup & Restore | Gardener</title><meta name=description content="A backup & restore sidecar for etcd (e.g. in the context of Gardener)"><meta property="og:title" content="Etcd Backup & Restore"><meta property="og:description" content="A backup & restore sidecar for etcd (e.g. in the context of Gardener)"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/other-components/etcd-backup-restore/"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="Etcd Backup & Restore"><meta itemprop=description content="A backup & restore sidecar for etcd (e.g. in the context of Gardener)"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="Etcd Backup & Restore"><meta name=twitter:description content="A backup & restore sidecar for etcd (e.g. in the context of Gardener)"><link rel=preload href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css as=style><link href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7N3XF5XLGV"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7N3XF5XLGV",{anonymize_ip:!1})}</script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.98cfe63dafab9306f41862bcbec5ed5b.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/other-components/etcd-backup-restore/>Return to the regular view of this page</a>.</p></div><h1 class=title>Etcd Backup & Restore</h1><div class=lead>A backup & restore sidecar for etcd (e.g. in the context of Gardener)</div><div class=content><h1 id=etcd-backup-restore>Etcd-Backup-Restore</h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/etcd-backup-restore-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/etcd-backup-restore-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/etcd-backup-restore><img src=https://goreportcard.com/badge/github.com/gardener/etcd-backup-restore alt="Go Report Card"></a>
<a href=https://godoc.org/github.com/gardener/etcd-backup-restore><img src=https://godoc.org/github.com/gardener/etcd-backup-restore?status.svg alt=GoDoc></a></p><p>Etcd-backup-restore is collection of components to backup and restore the <a href=https://github.com/etcd-io/etcd>etcd</a>. It also, provides the ability to validate the data directory, so that we could know the data directory is in good shape to bootstrap etcd successfully.</p><h2 id=documentation-index>Documentation Index</h2><h3 id=operations>Operations</h3><ul><li><a href=/docs/other-components/etcd-backup-restore/deployment/getting_started/>Getting started</a></li><li><a href=/docs/other-components/etcd-backup-restore/operations/manual_restoration/>Manual restoration</a></li><li><a href=/docs/other-components/etcd-backup-restore/operations/metrics/>Monitoring</a></li><li><a href=/docs/other-components/etcd-backup-restore/operations/generating_ssl_certificates/>Generating SSL certificates</a></li><li><a href=/docs/other-components/etcd-backup-restore/operations/leader_election/>Leader Election</a></li></ul><h3 id=design-and-proposals>Design and Proposals</h3><ul><li><a href=/docs/other-components/etcd-backup-restore/proposals/design/>Core design</a></li><li><a href=/docs/other-components/etcd-backup-restore/proposals/validation/>Etcd data validation</a></li><li><a href=/docs/other-components/etcd-backup-restore/proposals/restoration/>Data restoration</a></li><li><a href=/docs/other-components/etcd-backup-restore/proposals/high_watch_event_ingress_rate/>High watch events ingress rate issue</a></li></ul><h3 id=development>Development</h3><ul><li><a href=/docs/other-components/etcd-backup-restore/development/local_setup/>Setting up a local development environment</a></li><li><a href=/docs/other-components/etcd-backup-restore/development/testing_and_dependencies/>Testing and Dependency Management</a></li><li><a href=/docs/other-components/etcd-backup-restore/development/tests/>Tests</a></li><li><a href=/docs/other-components/etcd-backup-restore/development/new_cp_support/>Adding support for a new object store provider</a></li></ul></div></div><div class=td-content style=page-break-before:always><h1 id=pg-bb5b332b507db14aeb63bcb435a156cd>1 - Deployment</h1></div><div class=td-content><h1 id=pg-1cb52f42cbf679ee9bbda1f9e6714f17>1.1 - Getting Started</h1><h1 id=getting-started>Getting started</h1><p>Currently we don&rsquo;t publish the binary build with the release, but it is pretty straight forward to build it by following the steps mentioned <a href=/docs/other-components/etcd-backup-restore/development/local_setup/#build>here</a>. But we do publish the docker image with each release, please check the <a href=https://github.com/gardener/etcd-backup-restore/releases>release page</a> for the same. Currently, release docker images are pushed to <code>eu.gcr.io/gardener-project/gardener/etcdbrctl</code> to container registry.</p><h2 id=usage>Usage</h2><p>You can follow the <code>help</code> flag on <code>etcdbrctl</code> command and its sub-commands to know the usage details. Some of the common use cases are mentioned below. Although examples below use <code>AWS S3</code> as storage provider, etcd-backup-restore supports AWS, GCS, Azure, Openstack swift and Alicloud OSS object store. It also supports local disk as storage provider for development purposes, but it is not recommended to use this in a production environment.</p><h3 id=cloud-provider-credentials>Cloud Provider Credentials</h3><p>The procedure to provide credentials to access the cloud provider object store varies for different providers, there are various ways to pass credentials(<a href=#various-ways-to-pass-credentials>described below</a>), you can choose either ways but we recommend you to pass credentials through a file.</p><h3 id=various-ways-to-pass-credentials>Various ways to pass Credentials:</h3><ul><li><p>For <code>AWS S3</code>:</p><ol><li>The secret file should be provided and it&rsquo;s file path should be made available as environment variables: <code>AWS_APPLICATION_CREDENTIALS</code> or <code>AWS_APPLICATION_CREDENTIALS_JSON</code>.</li><li>For <code>S3-compatible providers</code> such as MinIO, <code>endpoint</code>, <code>s3ForcePathStyle</code>, <code>insecureSkipVerify</code> and <code>trustedCaCert</code>, can also be made available in a above file to configure the S3 client to communicate to a non-AWS provider.</li></ol></li><li><p>For <code>Google Cloud Storage</code>:</p><ol><li>The service account json file should be provided in the <code>~/.gcp</code> as a <code>service-account-file.json</code> file.</li><li>The service account json file should be provided and it&rsquo;s file path should be made available as environment variable: <code>GOOGLE_APPLICATION_CREDENTIALS</code></li></ol></li><li><p>For <code>Azure Blob storage</code>:</p><ol><li>The secret file should be provided and it&rsquo;s file path should be made available as environment variables: <code>AZURE_APPLICATION_CREDENTIALS</code> or <code>AZURE_APPLICATION_CREDENTIALS_JSON</code>.</li></ol></li><li><p>For <code>Openstack Swift</code>:</p><ol><li>The secret file should be provided and file path should be made available as environment variables: <code>OPENSTACK_APPLICATION_CREDENTIALS</code> or <code>OPENSTACK_APPLICATION_CREDENTIALS_JSON</code>.</li></ol></li><li><p>For <code>Alicloud OSS</code>:</p><ol><li>The secret file should be provided and file path should be made available as environment variables: <code>ALICLOUD_APPLICATION_CREDENTIALS</code> or <code>ALICLOUD_APPLICATION_CREDENTIALS_JSON</code>.</li></ol></li><li><p>For <code>Dell EMC ECS</code>:</p><ol><li><code>ECS_ENDPOINT</code>, <code>ECS_ACCESS_KEY_ID</code>, <code>ECS_SECRET_ACCESS_KEY</code> should be made available as environment variables. For development purposes, the environment variables <code>ECS_DISABLE_SSL</code> and <code>ECS_INSECURE_SKIP_VERIFY</code> can also be set to &ldquo;true&rdquo; or &ldquo;false&rdquo;.</li></ol></li><li><p>For <code>Openshift Container Storage (OCS)</code>:</p><ol><li>The secret file should be provided and file path should be made available as environment variables: <code>OPENSHIFT_APPLICATION_CREDENTIALS</code> or <code>OPENSHIFT_APPLICATION_CREDENTIALS_JSON</code>.
For development purposes, the environment variables <code>OCS_DISABLE_SSL</code> and <code>OCS_INSECURE_SKIP_VERIFY</code> can also be set to &ldquo;true&rdquo; or &ldquo;false&rdquo;.</li></ol></li></ul><p>Check the <a href=https://github.com/gardener/etcd-backup-restore/tree/master/example/storage-provider-secrets>example of storage provider secrets</a></p><h3 id=taking-scheduled-snapshot>Taking scheduled snapshot</h3><p>Sub-command <code>snapshot</code> takes scheduled backups, or <code>snapshots</code> of a running <code>etcd</code> cluster, which are pushed to one of the storage providers specified above (please note that <code>etcd</code> should already be running). One can apply standard cron format scheduling for regular backup of etcd. The cron schedule is used to take full backups. The delta snapshots are taken at regular intervals in the period in between full snapshots as indicated by the <code>delta-snapshot-period</code> flag. The default for the same is 20 seconds.</p><p>etcd-backup-restore has two garbage collection policies to clean up existing backups from the cloud bucket. The flag <code>garbage-collection-policy</code> is used to indicate the desired garbage collection policy.</p><ol><li><code>Exponential</code></li><li><code>LimitBased</code></li></ol><p>If using <code>LimitBased</code> policy, the <code>max-backups</code> flag should be provided to indicate the number of recent-most backups to persist at each garbage collection cycle.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>$</span> ./bin/etcdbrctl snapshot  <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--storage-provider=&#34;S3&#34; \
</span></span><span style=display:flex><span>--endpoints http://localhost:2379 \
</span></span><span style=display:flex><span>--schedule &#34;*/1 * * * *&#34; \
</span></span><span style=display:flex><span>--store-container=&#34;etcd-backup&#34; \
</span></span><span style=display:flex><span>--delta-snapshot-period=10s \
</span></span><span style=display:flex><span>--max-backups=10 \
</span></span><span style=display:flex><span>--garbage-collection-policy=&#39;LimitBased&#39;
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>INFO[0000] etcd-backup-restore Version: 0.7.0-dev
</span></span><span style=display:flex><span>INFO[0000] Git SHA: c03f75c
</span></span><span style=display:flex><span>INFO[0000] Go Version: go1.12.7
</span></span><span style=display:flex><span>INFO[0000] Go OS/Arch: darwin/amd64
</span></span><span style=display:flex><span>INFO[0000] Validating schedule...
</span></span><span style=display:flex><span>INFO[0000] Defragmentation period :72 hours
</span></span><span style=display:flex><span>INFO[0000] Taking scheduled snapshot for time: 2019-08-05 21:41:34.303439 +0530 IST
</span></span><span style=display:flex><span>INFO[0000] Successfully opened snapshot reader on etcd
</span></span><span style=display:flex><span>INFO[0001] Successfully initiated the multipart upload with upload ID : xhDeLNQsp9HAExmU1O4C3mCriUViVIRrrlPzdJ_.f4dtL046pNekEz54UD9GLYYOLjQUy.ZLZBLp4WeyNnFndDbvDZwhhCjAtwZQdqEbGw5.0HnX8fiP9Vvqk3_2j_Cf
</span></span><span style=display:flex><span>INFO[0001] Uploading snapshot of size: 22028320, chunkSize: 5242880, noOfChunks: 5
</span></span><span style=display:flex><span>INFO[0001] Triggered chunk upload for all chunks, total: 5
</span></span><span style=display:flex><span>INFO[0001] No of Chunks:= 5
</span></span><span style=display:flex><span>INFO[0001] Uploading chunk with id: 2, offset: 5242880, attempt: 0
</span></span><span style=display:flex><span>INFO[0001] Uploading chunk with id: 4, offset: 15728640, attempt: 0
</span></span><span style=display:flex><span>INFO[0001] Uploading chunk with id: 5, offset: 20971520, attempt: 0
</span></span><span style=display:flex><span>INFO[0001] Uploading chunk with id: 1, offset: 0, attempt: 0
</span></span><span style=display:flex><span>INFO[0001] Uploading chunk with id: 3, offset: 10485760, attempt: 0
</span></span><span style=display:flex><span>INFO[0008] Received chunk result for id: 5, offset: 20971520
</span></span><span style=display:flex><span>INFO[0012] Received chunk result for id: 3, offset: 10485760
</span></span><span style=display:flex><span>INFO[0014] Received chunk result for id: 4, offset: 15728640
</span></span><span style=display:flex><span>INFO[0015] Received chunk result for id: 2, offset: 5242880
</span></span><span style=display:flex><span>INFO[0018] Received chunk result for id: 1, offset: 0
</span></span><span style=display:flex><span>INFO[0018] Received successful chunk result for all chunks. Stopping workers.
</span></span><span style=display:flex><span>INFO[0018] Finishing the multipart upload with upload ID : xhDeLNQsp9HAExmU1O4C3mCriUViVIRrrlPzdJ_.f4dtL046pNekEz54UD9GLYYOLjQUy.ZLZBLp4WeyNnFndDbvDZwhhCjAtwZQdqEbGw5.0HnX8fiP9Vvqk3_2j_Cf
</span></span><span style=display:flex><span>INFO[0018] Total time to save snapshot: 17.934609 seconds.
</span></span><span style=display:flex><span>INFO[0018] Successfully saved full snapshot at: Backup-1565021494/Full-00000000-00009002-1565021494
</span></span><span style=display:flex><span>INFO[0018] Applied watch on etcd from revision: 9003
</span></span><span style=display:flex><span>INFO[0018] Stopping full snapshot...
</span></span><span style=display:flex><span>INFO[0018] Resetting full snapshot to run after 7.742179s
</span></span><span style=display:flex><span>INFO[0018] Will take next full snapshot at time: 2019-08-05 21:42:00 +0530 IST
</span></span><span style=display:flex><span>INFO[0018] Taking delta snapshot for time: 2019-08-05 21:41:52.258109 +0530 IST
</span></span><span style=display:flex><span>INFO[0018] No events received to save snapshot. Skipping delta snapshot.
</span></span></code></pre></div><p>The command mentioned above takes hourly snapshots and pushs it to S3 bucket named &ldquo;etcd-backup&rdquo;. It is configured to keep only last 10 backups in bucket.</p><p><code>Exponential</code> policy stores the snapshots in a condensed manner as mentioned below:</p><ul><li>All full backups and delta backups for the previous hour.</li><li>Latest full snapshot of each previous hour for the day.</li><li>Latest full snapshot of each previous day for 7 days.</li><li>Latest full snapshot of the previous 4 weeks.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>$</span> ./bin/etcdbrctl snapshot <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--storage-provider=&#34;S3&#34; \
</span></span><span style=display:flex><span>--endpoints http://localhost:2379 \
</span></span><span style=display:flex><span>--schedule &#34;*/1 * * * *&#34; \
</span></span><span style=display:flex><span>--store-container=&#34;etcd-backup&#34; \
</span></span><span style=display:flex><span>--delta-snapshot-period=10s \
</span></span><span style=display:flex><span>--garbage-collection-policy=&#39;Exponential&#39;
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>INFO[0000] etcd-backup-restore Version: 0.7.0-dev
</span></span><span style=display:flex><span>INFO[0000] Git SHA: c03f75c
</span></span><span style=display:flex><span>INFO[0000] Go Version: go1.12.7
</span></span><span style=display:flex><span>INFO[0000] Go OS/Arch: darwin/amd64
</span></span><span style=display:flex><span>INFO[0000] Validating schedule...
</span></span><span style=display:flex><span>INFO[0001] Taking scheduled snapshot for time: 2019-08-05 21:50:07.390127 +0530 IST
</span></span><span style=display:flex><span>INFO[0001] Defragmentation period :72 hours
</span></span><span style=display:flex><span>INFO[0001] There are no updates since last snapshot, skipping full snapshot.
</span></span><span style=display:flex><span>INFO[0001] Applied watch on etcd from revision: 9003
</span></span><span style=display:flex><span>INFO[0001] Stopping full snapshot...
</span></span><span style=display:flex><span>INFO[0001] Resetting full snapshot to run after 52.597795s
</span></span><span style=display:flex><span>INFO[0001] Will take next full snapshot at time: 2019-08-05 21:51:00 +0530 IST
</span></span><span style=display:flex><span>INFO[0001] Taking delta snapshot for time: 2019-08-05 21:50:07.402289 +0530 IST
</span></span><span style=display:flex><span>INFO[0001] No events received to save snapshot. Skipping delta snapshot.
</span></span><span style=display:flex><span>INFO[0001] Stopping delta snapshot...
</span></span><span style=display:flex><span>INFO[0001] Resetting delta snapshot to run after 10 secs.
</span></span><span style=display:flex><span>INFO[0011] Taking delta snapshot for time: 2019-08-05 21:50:17.403706 +0530 IST
</span></span><span style=display:flex><span>INFO[0011] No events received to save snapshot. Skipping delta snapshot.
</span></span><span style=display:flex><span>INFO[0011] Stopping delta snapshot...
</span></span><span style=display:flex><span>INFO[0011] Resetting delta snapshot to run after 10 secs.
</span></span><span style=display:flex><span>INFO[0021] Taking delta snapshot for time: 2019-08-05 21:50:27.406208 +0530 IST
</span></span><span style=display:flex><span>INFO[0021] No events received to save snapshot. Skipping delta snapshot.
</span></span></code></pre></div><p>The command mentioned above stores etcd snapshots as per the exponential policy mentioned above.</p><h3 id=etcd-data-directory-initialization>Etcd data directory initialization</h3><p>Sub-command <code>initialize</code> does the task of data directory validation. If the data directory is found to be corrupt, the controller will restore it from the latest snapshot in the cloud store. It restores the full snapshot first and then incrementally applies the delta snapshots. For more information regarding data restoration, please refer to <a href=/docs/other-components/etcd-backup-restore/proposals/restoration/>this guide</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>$</span> ./bin/etcdbrctl initialize <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--storage-provider=&#34;S3&#34; \
</span></span><span style=display:flex><span>--store-container=&#34;etcd-backup&#34; \
</span></span><span style=display:flex><span>--data-dir=&#34;default.etcd&#34;
</span></span><span style=display:flex><span>INFO[0000] Checking for data directory structure validity...
</span></span><span style=display:flex><span>INFO[0000] Checking for revision consistency...
</span></span><span style=display:flex><span>INFO[0000] Etcd revision inconsistent with latest snapshot revision: current etcd revision (770) is less than latest snapshot revision (9002): possible data loss
</span></span><span style=display:flex><span>INFO[0000] Finding latest set of snapshot to recover from...
</span></span><span style=display:flex><span>INFO[0001] Removing data directory(default.etcd.part) for snapshot restoration.
</span></span><span style=display:flex><span>INFO[0001] Restoring from base snapshot: Backup-1565021494/Full-00000000-00009002-1565021494
</span></span><span style=display:flex><span>2019-08-05 21:45:49.646232 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32
</span></span><span style=display:flex><span>INFO[0008] No delta snapshots present over base snapshot.
</span></span><span style=display:flex><span>INFO[0008] Removing data directory(default.etcd) for snapshot restoration.
</span></span><span style=display:flex><span>INFO[0008] Successfully restored the etcd data directory.
</span></span></code></pre></div><h3 id=etcdbrctl-server>Etcdbrctl server</h3><p>With sub-command <code>server</code> you can start a http server which exposes an endpoint to initialize etcd over REST interface. The server also keeps the backup schedule thread running to keep taking periodic backups. This is mainly made available to manage an etcd instance running in a Kubernetes cluster. You can deploy the example <a href=https://github.com/gardener/etcd-backup-restore/tree/master/chart/etcd-backup-restore>helm chart</a> on a Kubernetes cluster to have a fault-resilient, self-healing etcd cluster.</p><h2 id=etcdbrctl-copy>Etcdbrctl copy</h2><p>With sub-command <code>copy</code> you can copy all snapshots (Full and Delta) fom one snapstore to another. Using the two filter parameters <code>max-backups-to-copy</code> and <code>max-backup-age</code> you can also limit the number of snapshots that will be copied or target only the newest snapshots.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>$</span> ./bin/etcdbrctl copy <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--storage-provider=&#34;GCS&#34; \
</span></span><span style=display:flex><span>--snapstore-temp-directory=&#34;/temp&#34; \
</span></span><span style=display:flex><span>--store-prefix=&#34;target-prefix&#34; \
</span></span><span style=display:flex><span>--store-container=&#34;target-container&#34; \
</span></span><span style=display:flex><span>--source-store-prefix=&#34;prefix&#34; \
</span></span><span style=display:flex><span>--source-store-container=&#34;container&#34; \
</span></span><span style=display:flex><span>--source-storage-provider=&#34;GCS&#34; \
</span></span><span style=display:flex><span>--max-backup-age=15 \
</span></span><span style=display:flex><span>INFO[0000] etcd-backup-restore Version: v0.14.0-dev     
</span></span><span style=display:flex><span>INFO[0000] Git SHA: b821ee55                            
</span></span><span style=display:flex><span>INFO[0000] Go Version: go1.16.5                         
</span></span><span style=display:flex><span>INFO[0000] Go OS/Arch: darwin/amd64                     
</span></span><span style=display:flex><span>INFO[0000] Getting source backups...                     actor=copier
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>INFO[0026] Copying Incr snapshot Incr-ID.gz...  actor=copier
</span></span><span style=display:flex><span>INFO[0027] Uploading snapshot of size: 123456, chunkSize: 123456, noOfChunks: 1
</span></span><span style=display:flex><span>INFO[0027] Triggered chunk upload for all chunks, total: 1
</span></span><span style=display:flex><span>INFO[0027] No of Chunks:= 1
</span></span><span style=display:flex><span>INFO[0027] Uploading chunk with offset : 0, attempt: 0
</span></span><span style=display:flex><span>INFO[0027] Received chunk result for id: 1, offset: 0
</span></span><span style=display:flex><span>INFO[0027] Received successful chunk result for all chunks. Stopping workers.
</span></span><span style=display:flex><span>INFO[0027] All chunk uploaded successfully. Uploading composite object.
</span></span><span style=display:flex><span>INFO[0027] Composite object uploaded successfully.
</span></span><span style=display:flex><span>INFO[0027] Shutting down...
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-838154ed2032dc92538d4ffc1d083c5f>2 - Development</h1></div><div class=td-content><h1 id=pg-bcb7d032942a48f1af31824542963027>2.1 - Local Setup</h1><h2 id=prerequisites>Prerequisites</h2><p>Although the following installation instructions are for Mac OS X, similar alternate commands can be found for any Linux distribution.</p><h3 id=installing-golanghttpsgolangorg-environment>Installing <a href=https://golang.org/>Golang</a> environment</h3><p>Install the latest version of Golang (at least <code>v1.12</code> is required). For Mac OS, you may use <a href=https://brew.sh/>Homebrew</a>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>brew install golang
</span></span></code></pre></div><p>For other OSes, please check <a href=https://golang.org/doc/install>Go installation documentation</a>.</p><p>Make sure to set your <code>$GOPATH</code> environment variable properly (conventionally, it points to <code>$HOME/go</code>).</p><p>For your convenience, you can add the <code>bin</code> directory of the <code>$GOPATH</code> to your <code>$PATH</code>: <code>PATH=$PATH:$GOPATH/bin</code>, but it is not mandatory.</p><h3 id=golinthttpsgithubcomgolanglint><a href=https://github.com/golang/lint>Golint</a></h3><p>In order to perform linting on the Go source code, please install <a href=https://github.com/golang/lint>Golint</a>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>go get -u golang.org/x/lint/golint
</span></span></code></pre></div><h3 id=ginkgohttpsonsigithubioginkgo-and-gomegahttpsonsigithubiogomega><a href=https://onsi.github.io/ginkgo/>Ginkgo</a> and <a href=https://onsi.github.io/gomega/>Gomega</a></h3><p>In order to perform tests on the Go source code, please install <a href=https://onsi.github.io/ginkgo/>Ginkgo</a> and <a href=http://onsi.github.io/gomega/>Gomega</a>. Please make yourself familiar with both frameworks and read their introductions after installation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>go get -u github.com/onsi/ginkgo/ginkgo
</span></span><span style=display:flex><span>go get -u github.com/onsi/gomega
</span></span></code></pre></div><h3 id=installing-git>Installing <code>git</code></h3><p>We use <code>git</code> as VCS which you would need to install.</p><p>On Mac OS run</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>brew install git
</span></span></code></pre></div><h3 id=installing-gcloud-sdk-optional>Installing <code>gcloud</code> SDK (Optional)</h3><p>In case you have to create a new release or a new hotfix, you have to push the resulting Docker image into a Docker registry. Currently, we use the Google Container Registry (this could change in the future). Please follow the official <a href=https://cloud.google.com/sdk/downloads>installation instructions from Google</a>.</p><h2 id=build>Build</h2><p>Currently there are no binary builds available, but it is fairly simple to build it by following the steps mentioned below.</p><ul><li><p>First, you need to create a target folder structure before cloning and building <code>etcdbrctl</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>git clone https://github.com/gardener/etcd-backup-restore.git
</span></span><span style=display:flex><span>cd etcd-backup-restore
</span></span></code></pre></div></li><li><p>To build the binary in local machine environment, use <code>make</code> target <code>build-local</code>. It will build the binary <code>etcdbrctl</code> under <code>bin</code> directory.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>make build-local
</span></span></code></pre></div></li><li><p>Next you can make it available to use as shell command by moving the executable to <code>/usr/local/bin</code>, or by optionally including the <code>bin</code> directory in your <code>$PATH</code> environment variable.
You can verify the installation by running following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>$</span> etcdbrctl -v
</span></span><span style=display:flex><span>INFO[0000] etcd-backup-restore Version: v0.7.0-dev
</span></span><span style=display:flex><span>INFO[0000] Git SHA: 38979f0
</span></span><span style=display:flex><span>INFO[0000] Go Version: go1.12
</span></span><span style=display:flex><span>INFO[0000] Go OS/Arch: darwin/amd64
</span></span></code></pre></div></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f162271a2edeebbf78595646ef580158>2.2 - New Cp Support</h1><h1 id=adding-support-for-a-new-object-store-provider>Adding support for a new object store provider</h1><p>Currently the code design only allows in-tree support for different providers. Our roadmap includes the change to code design to allow out-of-tree support for different providers. For adding support for a new object store provider, follow the steps described below. Replace <code>provider</code> with your provider-name.</p><ol><li>Add the provider identifier constant in <code>pkg/snapstore/types.go</code>.</li><li>Add implementation for the <code>SnapStore</code> from <code>pkg/snapstore/types.go</code> interface to <code>pkg/snapstore/provider_snapstore.go</code>.<ul><li>⚠️ Please use environment variable(s) to pass the object store credentials.</li><li>Provide the factory method to create provider implementation object by loading required access credentials from environment variable(s).</li><li>Avoid introducing new command line flags for provider.</li></ul></li><li>Import the required SDK and any other libraries for provider using <code>GO111MODULE=on go get &lt;provider-sdk></code>. This will update the dependency in <a href=https://github.com/gardener/etcd-backup-restore/blob/master/go.mod>go.mod</a> and <a href=https://github.com/gardener/etcd-backup-restore/blob/master/go.sum>go.sum</a>.</li><li>Run <code>make revendor</code> to download the dependency library to <a href=https://github.com/gardener/etcd-backup-restore/tree/master/vendor>vendor</a> directory.</li><li>Update the <a href=https://github.com/gardener/etcd-backup-restore/blob/master/LICENSE.md>LICENSE.md</a> with license details of newly added dependencies.</li><li>Update the <code>GetSnapstore</code> method in <code>pkg/snapstore/utils.go</code> to add a new case in switch block to support creation of the new provider implementation object.</li><li>Add the fake implementation of provider SDK calls under <code>pkg/snapstore/provider_snapstore_test.go</code> for unit testing the provider implementation.</li><li>Register the provider implementation object for testing at the appropriate place under <code>pkg/snapstore/snapstore_test.go</code>. This will run generic test against provider implementation.</li><li>Update the <a href=/docs/other-components/etcd-backup-restore/deployment/getting_started/#cloud-provider-credentials>documentation</a> to provide info about passing provider credentials.</li><li>Update the <a href=https://github.com/gardener/etcd-backup-restore/tree/master/chart/etcd-backup-restore>helm chart</a> with provider details.<ul><li>Update the <a href=https://github.com/gardener/etcd-backup-restore/blob/master/chart/etcd-backup-restore/values.yaml>values.yaml</a> with configuration for provider.</li></ul></li><li>Refer <a href=https://github.com/gardener/etcd-backup-restore/pull/108/commits/9bcd4e0e96f85ce1f356f08c06a2ced293aaf20b>this commit</a> to for one of the already added provider support.</li><li>Finally test your code using <code>make verify</code>. And raise a PR for review. 😄</li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-460b7ce9672827fe5e284aee336c6792>2.3 - Testing And Dependencies</h1><h1 id=dependency-management>Dependency management</h1><p>We use go-modules to manage golang dependencies. In order to add a new package dependency to the project, you can perform <code>go get &lt;PACKAGE>@&lt;VERSION></code> or edit the <code>go.mod</code> file and append the package along with the version you want to use.</p><h3 id=updating-dependencies>Updating dependencies</h3><p>The <code>Makefile</code> contains a rule called <code>revendor</code> which performs <code>go mod vendor</code> and <code>go mod tidy</code>.</p><ul><li><code>go mod vendor</code> resets the main module&rsquo;s vendor directory to include all packages needed to build and test all the main module&rsquo;s packages. It does not include test code for vendored packages.</li><li><code>go mod tidy</code> makes sure go.mod matches the source code in the module.
It adds any missing modules necessary to build the current module&rsquo;s packages and dependencies, and it removes unused modules that don&rsquo;t provide any relevant packages.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>make revendor
</span></span></code></pre></div><p>The dependencies are installed into the <code>vendor</code> folder which <strong>should be added</strong> to the VCS.</p><p>⚠️ Make sure you test the code after you have updated the dependencies!</p><h1 id=testing>Testing</h1><p>This section describes the process to execute tests. For more details about kind of tests that are executed, please refer <a href=/docs/other-components/etcd-backup-restore/development/tests/>test documentation</a></p><h3 id=unit-tests>Unit tests</h3><p>We have created <code>make</code> target <code>verify</code> which will internally run different rules like <code>fmt</code> for formatting, <code>lint</code> for linting check and most importantly <code>test</code> which will check the code against predefined unit tests. As currently there aren&rsquo;t enough test cases written to cover the entire code, you must check for failure cases manually and include test cases before raising pull request. We will eventually add more test cases for complete code coverage.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>make verify
</span></span></code></pre></div><p>By default, we run tests without computing code coverage. To get the code coverage, you can set the environment variable <code>COVER</code> to <code>true</code>. This will log the code coverage percentage at the end of test logs. Also, all cover profile files will be accumulated under <code>test/output/coverprofile.out</code> directory. You can visualize the exact code coverage by running <code>make show-coverage</code> after running <code>make verify</code> with code coverage enabled.</p><h3 id=integration-tests>Integration tests</h3><p>You can also run integration tests for etcd-backup-restore on any given Kubernetes cluster. The test creates namespace <code>integration-test</code> on the cluster and deploys the <a href=https://github.com/gardener/etcd-backup-restore/tree/master/chart/etcd-backup-restore>etcd-backup-restore helm chart</a> which in turn deploys the required secrets, configmap, services and finally the statefulset which contains the pod that runs etcd and backup-restore as a sidecar.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>make integration-test-cluster
</span></span></code></pre></div><p>⚠️ Prerequisite for this command is to set the following environment variables:</p><ul><li>INTEGRATION_TEST_KUBECONFIG: kubeconfig to the cluster on which you wish to run the test</li><li>ETCD_VERSION: optional, defaults to <code>v3.4.13-bootstrap-1</code></li><li>ETCDBR_VERSION: optional, defaults to <code>v0.12.1</code></li><li>ACCESS_KEY_ID: S3 credentials</li><li>SECRET_ACCESS_KEY: S3 credentials</li><li>REGION: S3 credentials</li><li>STORAGE_CONTAINER: S3 bucket name</li></ul><p>If you have a working setup of <a href=https://github.com/gardener/test-infra>TestMachinery</a>, you can run the integration tests on a TM-generated cluster as well.</p><h3 id=performance-regression-tests>Performance regression tests</h3><p>Furthermore, you can check any regression in performance in terms of memory consumption and CPU utilization, bby running the provided performance regression tests.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>make perf-regression-test
</span></span></code></pre></div><p>⚠️ Prerequisite for this command is to set the following environment variables:</p><ul><li>PERF_TEST_KUBECONFIG: kubeconfig to the cluster on which you wish to run the test</li><li>ETCD_VERSION: optional, defaults to <code>v3.3.17</code></li><li>ETCDBR_VERSION: etcd-backup-restore version to test against</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-75c5b0fa9297454e8580ff6d4f10e99b>2.4 - Tests</h1><h1 id=tests>Tests</h1><p><code>etcd-backup-restore</code> makes use of three sets of tests - unit tests in each package, integration tests to ensure the working of the overall tool and performance regression tests to check changes in resource consumption between different versions of etcd-backup-restore.</p><h3 id=integration-tests>Integration tests</h3><p>Integration tests include the basic working of:</p><ul><li><p><strong>snapshotting</strong>: successfully upload full and delta snapshots to the configured snapstore according to the specified schedule</p></li><li><p><strong>garbage collection</strong>: garbage-collect old snapshots on the snapstore according to the specified policy</p></li><li><p><strong>defragmentation</strong>: etcd data should be defragmented periodically to reduce db size</p></li><li><p><strong>http server</strong>: http endpoints should work as expected:</p><ul><li><code>/snapshot/full</code>: should take an on-demand full snapshot</li><li><code>/snapshot/delta</code>: should take an on-demand delta snapshot</li><li><code>/snapshot/latest</code>: should list the latest set of snapshots (full + deltas)</li></ul></li><li><p><strong>data validation</strong>: corrupted etcd data should be marked for deletion and restoration should be triggered</p></li><li><p><strong>restoration</strong>: etcd data should be restored correctly from latest set of snapshots (full + deltas)</p></li></ul><h3 id=unit-tests>Unit tests</h3><p>Each package within this repo contains its own set of unit tests to test the functionality of the methods contained within the packages.</p><h3 id=performance-regression-tests>Performance regression tests</h3><p>These tests help check any regression in performance in terms of memory consumption and CPU utilization.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7174a8eb8f08712dc8b97698ac1a0a36>3 - Operations</h1></div><div class=td-content><h1 id=pg-d95aebbed89863cd72eaf98bf0cae846>3.1 - Generating Ssl Certificates</h1><h1 id=generating-certificates>Generating certificates</h1><p>If you wish to enable TLS authentication for either etcd or etcdbr server or both, please follow this guide. The SSL certificate configurations given here are meant to facilitate smooth deployment of the etcd setup via the provided <a href=https://github.com/gardener/etcd-backup-restore/tree/master/chart/etcd-backup-restore>helm chart</a>.</p><h2 id=certificates-structure>Certificates structure</h2><p>While deploying the etcd setup via the provided helm chart, TLS can be enabled for the etcd server and/or etcd-backup-restore server by adding the certificate data to the <code>values.yaml</code> file as necessary. This data is converted into the respective secrets and mounted onto the pod&rsquo;s containers according to the following directory structure:</p><ul><li><code>etcd</code> container</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>/
</span></span><span style=display:flex><span>└── var
</span></span><span style=display:flex><span>    ├── etcd                      Contains the CA and server TLS certs for etcd server
</span></span><span style=display:flex><span>    |   └── ssl
</span></span><span style=display:flex><span>    |       ├── ca
</span></span><span style=display:flex><span>    |       |   └── ca.crt
</span></span><span style=display:flex><span>    |       └── tls
</span></span><span style=display:flex><span>    |           ├── tls.crt
</span></span><span style=display:flex><span>    |           └── tls.key
</span></span><span style=display:flex><span>    └── etcdbr                    Contains the CA and server TLS certs for etcd backup-restore server
</span></span><span style=display:flex><span>        └── ssl
</span></span><span style=display:flex><span>            ├── ca
</span></span><span style=display:flex><span>            |   └── ca.crt
</span></span><span style=display:flex><span>            └── tls
</span></span><span style=display:flex><span>                ├── tls.crt
</span></span><span style=display:flex><span>                └── tls.key
</span></span></code></pre></div><br><ul><li><code>backup-restore</code> container</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>/
</span></span><span style=display:flex><span>└── var
</span></span><span style=display:flex><span>    ├── etcd                      Contains the CA and server certs for etcd server
</span></span><span style=display:flex><span>    |   └── ssl
</span></span><span style=display:flex><span>    |       ├── ca
</span></span><span style=display:flex><span>    |       |   └── ca.crt
</span></span><span style=display:flex><span>    |       └── tls
</span></span><span style=display:flex><span>    |           ├── tls.crt
</span></span><span style=display:flex><span>    |           └── tls.key
</span></span><span style=display:flex><span>    └── etcdbr                    Contains the CA cert for etcd backup-restore server
</span></span><span style=display:flex><span>        └── ssl
</span></span><span style=display:flex><span>            └── ca
</span></span><span style=display:flex><span>                └── ca.crt
</span></span></code></pre></div><h2 id=generating-the-certificates>Generating the certificates</h2><h3 id=installing-openssl>Installing openssl</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>#</span> For Mac users
</span></span><span style=display:flex><span>brew install openssl
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=font-weight:700>#</span> For other flavours of Unix
</span></span><span style=display:flex><span>apk install openssl
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>mkdir openssl &amp;&amp; cd openssl
</span></span></code></pre></div><h3 id=generating-certs-for-etcd-server-authentication>Generating certs for etcd server authentication</h3><h4 id=generating-ca-cert-bundle>Generating CA cert bundle</h4><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>openssl genrsa -out ca.key 2048
</span></span><span style=display:flex><span>openssl req -new -key ca.key -subj &#34;/CN=etcd&#34; -out ca.csr
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>cat &gt; ca.csr.conf &lt;&lt;EOF
</span></span><span style=display:flex><span>[ v3_ext ]
</span></span><span style=display:flex><span>keyUsage=critical,digitalSignature,keyEncipherment,keyCertSign,cRLSign
</span></span><span style=display:flex><span>basicConstraints=critical,CA:TRUE
</span></span><span style=display:flex><span>EOF
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt -sha256 -days 3653 -extensions v3_ext -extfile ca.csr.conf
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=font-weight:700>#</span> view contents of the generated certificate
</span></span><span style=display:flex><span>openssl x509 -in ca.crt -noout -text
</span></span></code></pre></div><h4 id=generating-tls-key-pair>Generating TLS key-pair</h4><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>openssl genrsa -out server.key 2048
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=font-weight:700>#</span> In the <span style=color:#a31515>`</span>alt_names<span style=color:#a31515>`</span> section of server.csr.conf, replace all occurrences of <span style=color:#a31515>`</span>mynamespace<span style=color:#a31515>`</span> with the namespace into which you<span>&#39;</span>ll deploy the helm chart
</span></span><span style=display:flex><span>cat &gt; server.csr.conf &lt;&lt;EOF
</span></span><span style=display:flex><span>[ req ]
</span></span><span style=display:flex><span>default_bits = 2048
</span></span><span style=display:flex><span>prompt = no
</span></span><span style=display:flex><span>default_md = sha256
</span></span><span style=display:flex><span>req_extensions = req_ext
</span></span><span style=display:flex><span>distinguished_name = dn
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>[ dn ]
</span></span><span style=display:flex><span>CN = etcd-server
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>[ req_ext ]
</span></span><span style=display:flex><span>subjectAltName = @alt_names
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>[ alt_names ]
</span></span><span style=display:flex><span>DNS.1 = main-etcd-0
</span></span><span style=display:flex><span>DNS.2 = main-etcd-local
</span></span><span style=display:flex><span>DNS.3 = main-etcd-client
</span></span><span style=display:flex><span>DNS.4 = main-etcd-client.mynamespace
</span></span><span style=display:flex><span>DNS.5 = main-etcd-client.mynamespace.svc
</span></span><span style=display:flex><span>DNS.6 = main-etcd-client.mynamespace.svc.cluster.local
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>[ v3_ext ]
</span></span><span style=display:flex><span>keyUsage=critical,digitalSignature,keyEncipherment
</span></span><span style=display:flex><span>extendedKeyUsage=serverAuth,clientAuth
</span></span><span style=display:flex><span>basicConstraints=critical,CA:FALSE
</span></span><span style=display:flex><span>subjectAltName=@alt_names
</span></span><span style=display:flex><span>EOF
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>openssl req -new -key server.key -out server.csr -config server.csr.conf
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -sha256 -days 3653 -extensions v3_ext -extfile server.csr.conf
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=font-weight:700>#</span> view contents of the generated certificate
</span></span><span style=display:flex><span>openssl x509 -in server.crt -noout -text
</span></span></code></pre></div><h3 id=generating-certs-for-etcdbr-server-authentication>Generating certs for etcdbr server authentication</h3><p>Follow the same steps as <a href=#generating-certs-for-etcd-server-authentication>generating certs for etcd</a>, but replace all occurrences of <code>etcd</code> with <code>etcdbr</code> for the <code>CN</code> fields and replace <code>main-etcd-client</code> with <code>main-backup-client</code> in the DNS names if you want to access the TLS-enabled backup-restore server via service. You will also need to add <code>localhost</code> to the SAN DNS list if you&rsquo;re deploying the etcd setup via the provided helm chart. If deploying by any other means, or if testing locally, please tweak the config accordingly.</p><h4 id=generating-ca-cert-bundle-1>Generating CA cert bundle</h4><p>Follow the same steps as <a href=#generating-CA-cert-bundle>generating CA cert for etcd</a>, but replace<code>CN=etcd</code> by <code>CN=etcdbr</code> while creating the <code>ca.csr</code>.</p><h4 id=generating-tls-key-pair-1>Generating TLS key-pair</h4><p>Follow the same steps as <a href=#generating-tls-key-pair>generating TLS key-pair for etcd</a>, but modify the <code>[ dn ]</code> section in <code>server.csr.conf</code> from <code>CN = etcd</code> by <code>CN = etcdbr</code>. Also change the <code>[ alt_names ]</code> section to the following:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>[ alt_names ]
</span></span><span style=display:flex><span>DNS.1 = localhost
</span></span><span style=display:flex><span>DNS.2 = main-backup-0
</span></span><span style=display:flex><span>DNS.3 = main-backup-local
</span></span><span style=display:flex><span>DNS.4 = main-backup-client
</span></span><span style=display:flex><span>DNS.5 = main-backup-client.mynamespace
</span></span><span style=display:flex><span>DNS.6 = main-backup-client.mynamespace.svc
</span></span><span style=display:flex><span>DNS.7 = main-backup-client.mynamespace.svc.cluster.local
</span></span></code></pre></div><p>Here, we add <code>localhost</code> to the DNS entries so that the etcd bootstrap script may be allowed to trigger data initialization on the backup sidecar via HTTPS.</p><h2 id=running-etcdbrctl-server-with-tls-enabled>Running <code>etcdbrctl server</code> with TLS enabled</h2><p>If you wish to develop/test <code>etcdbrctl</code> locally with TLS enabled, you can follow the steps to create the certs and pass them to the <code>etcdbrctl server</code> via the following flags.</p><h3 id=for-etcd-tls>For etcd TLS</h3><p>Pass the CA certificate file via <code>--cacert</code> flag, and etcd server TLS certificate and key via <code>--cert</code> and <code>--key</code> flags respectively.</p><h3 id=for-etcdbr-tls>For etcdbr TLS</h3><p>Pass the etcd backup-restore server TLS certificate and key via <code>--server-cert</code> and <code>--server-key</code> respectively.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2be463cd712869bb3bc79f07adfad44d>3.2 - Leader Election</h1><h3 id=leading-etcd-main-containers-sidecar-is-the-backup-leader>Leading ETCD main container’s sidecar is the backup leader</h3><ul><li><p>The <code>backup-restore sidecar</code> poll its corresponding etcd main container to see if it is the leading member in the etcd cluster. This information is used by the backup-restore sidecars to decide that sidecar of the leading etcd main container is the <code>backup leader</code>.</p></li><li><p>Only <code>backup leader</code> sidecar among the members have the responsibility to take/upload the snapshots(full as well as incremental) for a given Etcd cluster as well as to <a href=https://github.com/gardener/etcd-druid/tree/master/docs/proposals/multi-node#defragmentation>trigger the defragmentation</a> for each Etcd cluster member.</p></li></ul><h3 id=work-flow>Work flow</h3><p>Backup-restore can be in following 3 states:</p><ol><li>Follower:<ul><li>When the corresponding etcd main container is a <code>Follower</code> etcd.</li><li>The default state for every backup-restore member is the <code>Follower</code> State.</li></ul></li><li>Leader:<ul><li>When the corresponding etcd main container becomes the <code>leader</code> then sidecar will also become <code>leading sidecar</code>.</li></ul></li><li>Unknown:<ul><li>When there is no etcd leader present in the cluster or in the case of a quorum loss.</li><li>When corresponding etcd main container is down and <a href=https://github.com/etcd-io/etcd/blob/f82b5cb7768dacad9fb310232c1383b4e6718378/client/v3/maintenance.go#L53>endpoint status</a> api call fails.</li></ul></li></ol><h3 id=backup>Backup</h3><ul><li>Only <code>backup leader</code> among the backup-restore members have the responsibility to take/upload the snapshots(full as well as incremental) for a given Etcd cluster.</li><li>The <code>backup leader</code> also has the responsibility to <a href=https://github.com/gardener/etcd-backup-restore/blob/master/docs/operations/getting_started.md#taking-scheduled-snapshot>garbage-collect</a> the backups from the object storage bucket according to a configured garbage collection policy.</li></ul><h3 id=member-lease>Member-lease</h3><p>Each backup-restore member has the responsibility to renew its member lease periodically. This is intended to simulate as heartbeat which indicate that the backup-restore cluster members are in <code>Healthy</code> State.</p><h3 id=defragmentation>Defragmentation</h3><p>Defragmentation for all etcd cluster members is triggered by the <code>leading backup-restore</code> sidecar. The defragmentation is performed only when etcd cluster is in full health and it is done in a rolling manner for each member to avoid disruption.At first, <code>leading backup-restore</code> sidecar triggers defragmentation on all etcd follower members one by one and at last, on itself.</p><h3 id=complete-work-flow-leader-election-state-diagram>Complete work flow leader-election state diagram.</h3><p><img src=/__resources/leaderElection_stateDiagram_c49851.png alt=leader-election></p></div><div class=td-content style=page-break-before:always><h1 id=pg-b6d19d320b69db38b3a0312860680c40>3.3 - Manual Restoration</h1><h1 id=manual-restoration-of-etcd-data>Manual restoration of etcd data</h1><p>Please make sure that you have read through the <a href=/docs/other-components/etcd-backup-restore/proposals/restoration/>documentation on etcd data restoration</a> before reading this document and attempting to perform a manual restoration of etcd data.</p><p>As mentioned in previously, automatic restoration will be triggered if the etcd data gets corrupted, as the etcd process will crash. But if for some reason, restoration is not automatically triggered when it should have, you may choose to manually restore the etcd data.</p><h2 id=steps-to-perform-restoration>Steps to perform restoration</h2><p>You may choose to follow different methods of restoration, based on your etcd + backup sidecar setup:</p><ol><li><p>Deploying the <a href=https://github.com/gardener/etcd-backup-restore/tree/master/chart/etcd-backup-restore>provided helm chart</a>, in which etcdbrctl is started in <code>server</code> mode</p><ol><li><p>Exec into the <code>etcd</code> container of the <code>main-etcd-0</code> pod and delete the <code>member</code> directory under the data directory in order to invalidate it</p><ul><li><code>rm -rf /var/etcd/data/new.etcd/member</code></li><li>You may choose to rename the <code>member</code> directory instead of deleting it, if you wish to retain the old data for debugging</li></ul></li><li><p>This will crash the etcd container and when it restarts, the backup sidecar will perform a validation of the data directory, and seeing that the data is corrupt, it will restore the data from the latest backup</p><ul><li>⚠️ Keep in mind that the latest backup in the object storage bucket might not be up-to-date with the latest etcd data, and you could see a maximum data loss corresponding to the delta snapshot interval. For instance, setting <code>delta-snapshot-interval=5m</code> could result in a maximum data loss worth 5 minutes.</li></ul></li><li><p>If for some reason, automatic restoration isn&rsquo;t getting triggered even after removing the <code>member</code> directory, it may be required to temporarily modify the etcdbrctl command to <code>restore</code> mode to force a manual restoration of data, and then change the container spec back to its original form once the restoration is successful. Do not change any field in the container spec other than the <code>command</code> field, which is detailed below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>command:
</span></span><span style=display:flex><span>- etcdbrctl
</span></span><span style=display:flex><span>- restore
</span></span><span style=display:flex><span>- --data-dir=&lt;same as previous value&gt;
</span></span><span style=display:flex><span>- --storage-provider=&lt;same as previous value&gt;
</span></span><span style=display:flex><span>- --store-prefix=&lt;same as previous value&gt;
</span></span><span style=display:flex><span>- --embedded-etcd-quota-bytes=&lt;same as previous value&gt;
</span></span><span style=display:flex><span>- --snapstore-temp-directory=&lt;same as previous value&gt;
</span></span></code></pre></div><p>⚠️ Edit etcd-main statefulset to change command field. After saving it, ideally statefulset controller will recreate the etcd-main-0 pod itself. But, since it doesn&rsquo;t handle the case of unhealthy pod, delete etcd-main-0 pod maually using commnad <code>kubectl -n &lt;namespace> delete etcd-main-0</code>. Now, check etcd and backup-restore sidecar logs for successful restoration.</p><p>Once the spec is changed, monitor the logs to make sure restoration occurs. Once restoration is complete, change the container spec back to its previous state and restart the pod. This should purge any previous issues with etcd or backup sidecar, and start snapshotting successfully.</p></li></ol></li><li><p>Deploying etcd and etcdbrctl separately, where etcdbrctl is started in <code>server</code> mode</p><ol><li>If using <a href=https://github.com/gardener/etcd-custom-image/blob/master/etcd_bootstrap_script.sh>this bootstrap script</a> for starting etcd, then deleting the <code>member</code> directory under the etcd data directory should kill the etcd process, and subsequently the script finishes execution and exits. You will have to re-run the script and allow it to trigger data validation anf restoration by etcdbrctl.</li><li>If not using the bootstrap script, then:<ol><li>Delete the <code>member</code> directory and wait for etcd to crash</li><li><code>curl http://localhost:8080/initialization/status</code>, assuming etcdbrctl is running on port 8080</li><li><code>curl http://localhost:8080/initialization/start</code></li><li>Wait for the restoration to finish, by observing the logs from etcdbrctl</li><li>Again, <code>curl http://localhost:8080/initialization/status</code> to complete the initialization process, and etcdbrctl will resume regular snapshotting after this</li></ol></li></ol></li><li><p>Deploying etcd and etcdbrctl separately, where etcdbrctl is started in <code>snapshot</code> mode</p><ol><li>Delete the <code>member</code> directory and wait for etcd to crash</li><li>Kill the etcdbrctl process</li><li>Run etcdbrctl in <code>restore</code> mode to perform a restoration of the data<ul><li>It is highly recommended to run etcdbrctl in <code>initialize</code> mode rather than <code>restore</code> mode, as this performs the necessary validation checks on the data directory taking the decision to trigger a restoration.</li></ul></li><li>Start etcd again</li><li>Restart etcdbrctl in <code>snapshot</code> mode</li></ol><ul><li>⚠️ If running etcdbrctl in <code>snapshot</code> mode, it is necessary to stop this snapshotter process before triggering a restoration, to avoid data inconsistency.</li></ul></li></ol><p>⚠️ If in doubt about the validity of the etcd data, operators must first visually confirm any inconsistency in revision numbers between the latest snapshot-set in the object store and the running etcd instance using the <code>etcdctl</code> tool before deciding to perform a manual restoration.</p><p>⚠️ In order to successfully perform a restoration, the data directory must NOT contain the <code>member</code> directory, else the restoration will fail.</p><p>⚠️ <strong>Do not tamper with the object store in any way.</strong> Data once lost from the object store, cannot be recovered. The object store is considered as the source of truth for the restorer.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d2a063b23374ff2f7fa06ba3a896fa90>3.4 - Metrics</h1><h1 id=monitoring>Monitoring</h1><p>etcd-backup-restore uses <a href=http://prometheus.io/>Prometheus</a> for metrics reporting. The metrics can be used for real-time monitoring and debugging. It won&rsquo;t persist its metrics; if a member restarts, the metrics will be reset.</p><p>The simplest way to see the available metrics is to cURL the metrics endpoint <code>/metrics</code>. The format is described <a href=http://prometheus.io/docs/instrumenting/exposition_formats/>here</a>.</p><p>Follow the <a href=http://prometheus.io/docs/introduction/getting_started/>Prometheus getting started doc</a> to spin up a Prometheus server to collect etcd metrics.</p><p>The naming of metrics follows the suggested <a href=http://prometheus.io/docs/practices/naming/>Prometheus best practices</a>. All etcd-backup-restore related metrics are put under namespace <code>etcdbr</code>.</p><h2 id=etcd-metrics>ETCD metrics</h2><p>The metrics under the <code>etcd</code> prefix/namespace are carried forward from etcd library that we use. These metrics do not include details of the <code>etcd</code> deployment on which <code>etcd-backup-restore</code> utility operates. Instead, it helps in monitoring the <code>embedded etcd</code> we spawn during restoration process.</p><h3 id=snapshot>Snapshot</h3><p>These metrics describe the status of the snapshotter. In order to detect outages or problems for troubleshooting, these metrics should be closely monitored. The below mentioned metrics are listed as collection of series using prometheus labels <code>kind</code> and <code>succeeded</code>. <code>Kind</code> label indicates the snapshot kind i.e. full snapshot or incremental/delta snapshot in the context. And succeeded indicates whether the metrics is for successful operation or erroneous operation.</p><table><thead><tr><th>Name</th><th>Description</th><th>Type</th></tr></thead><tbody><tr><td>etcdbr_snapshot_duration_seconds</td><td>Total latency distribution of saving snapshot to object store.</td><td>Histogram</td></tr><tr><td>etcdbr_snapshot_gc_total</td><td>Total number of garbage collected snapshots.</td><td>Counter</td></tr><tr><td>etcdbr_snapshot_latest_revision</td><td>Revision number of latest snapshot taken.</td><td>Gauge</td></tr><tr><td>etcdbr_snapshot_latest_timestamp</td><td>Timestamp of latest snapshot taken.</td><td>Gauge</td></tr><tr><td>etcdbr_snapshot_required</td><td>Indicates whether a new snapshot is required to be taken.</td><td>Gauge</td></tr></tbody></table><p>Abnormally high snapshot duration (<code>etcdbr_snapshot_duration_seconds</code>) indicates disk issues and low network bandwidth.</p><p><code>etcdbr_snapshot_latest_timestamp</code> indicates the time when last snapshot was taken. If it has been a long time since a snapshot has been taken, then it indicates either the snapshots are being skipped because of no updates on etcd or ⚠️ something fishy is going on and a possible data loss might occur on the next restoration.</p><p><code>etcdbr_snapshot_gc_total</code> gives the total number of snapshots garbage collected since bootstrap. You can use this in coordination with <code>etcdbr_snapshot_duration_seconds_count</code> to get number of snapshots in object store.</p><p><code>etcdbr_snapshot_required</code> indicates whether a new snapshot is required to be taken. Acts as a boolean flag where zero value implies &lsquo;false&rsquo; and non-zero values imply &rsquo;true&rsquo;. ⚠️ This metric does not work as expected for the case where delta snapshots are disabled (by setting the etcdbrctl flag <code>delta-snapshot-period</code> to 0).</p><h3 id=defragmentation>Defragmentation</h3><p>The metrics for defragmentation is of type histogram, which gives the number of times defragmentation was triggered. ⚠️ The defragmentation latency should be as low as possible, since
defragmentation is indeed a costly operation, which results in unavailability of etcd for the period.</p><table><thead><tr><th>Name</th><th>Description</th><th>Type</th></tr></thead><tbody><tr><td>etcdbr_defragmentation_duration_seconds</td><td>Total latency distribution of defragmentation of etcd data directory.</td><td>Histogram</td></tr></tbody></table><h3 id=validation-and-restoration>Validation and Restoration</h3><p>Two major steps in initialization of etcd data directory are validation and restoration. It is necessary to monitor the count and time duration of these calls, from a high availability perspective.</p><table><thead><tr><th>Name</th><th>Description</th><th>Type</th></tr></thead><tbody><tr><td>etcdbr_validation_duration_seconds</td><td>Total latency distribution of validating data directory.</td><td>Histogram</td></tr><tr><td>etcdbr_restoration_duration_seconds</td><td>Total latency distribution of restoring from snapshot.</td><td>Histogram</td></tr></tbody></table><h3 id=snapstore>Snapstore</h3><p>These bucket-related metrics provide information about the latest set of delta snapshots stored in the snapstore. They provide a rough estimation of the amount of time required to perform a restoration from the latest set of snapshots.</p><table><thead><tr><th>Name</th><th>Description</th><th>Type</th></tr></thead><tbody><tr><td>etcdbr_snapstore_latest_deltas_total</td><td>Total number of delta snapshots taken since the latest full snapshot.</td><td>Gauge</td></tr><tr><td>etcdbr_snapstore_latest_deltas_revisions_total</td><td>Total number of revisions stored in delta snapshots taken since the latest full snapshot.</td><td>Gauge</td></tr></tbody></table><p><code>etcdbr_snapstore_latest_deltas_revisions_total</code> indicates the total number of etcd revisions (events) stored in the latest set of delta snapshots. The amount of time it would take to perform an etcd data restoration with the latest set of snapshots is directly proportional to this value.</p><h3 id=network>Network</h3><p>These metrics describe the status of the network usage. We use <code>/proc/&lt;etcdbr-pid>/net/dev</code> to get network usage details for the etcdbr process. Currently these metrics are only supported on linux-based distributions.</p><p>All these metrics are under subsystem <code>network</code>.</p><table><thead><tr><th>Name</th><th>Description</th><th>Type</th></tr></thead><tbody><tr><td>etcdbr_network_transmitted_bytes</td><td>The total number of bytes received over network.</td><td>Counter</td></tr><tr><td>etcdbr_network_received_bytes</td><td>The total number of bytes received over network.</td><td>Counter</td></tr></tbody></table><p><code>etcdbr_network_transmitted_bytes</code> counts the total number of bytes transmitted. Usually this reflects the data uploaded to object store as part of snapshot uploads.</p><p><code>etcdbr_network_received_bytes</code> counts the total number of bytes received. Usually this reflects the data received as part of snapshots from actual <code>etcd</code>. There could be a sudden spike in this at the time of restoration as well.</p><h2 id=grpc-requests>gRPC requests</h2><p>These metrics are exposed via <a href=https://github.com/grpc-ecosystem/go-grpc-prometheus>go-grpc-prometheus</a>.</p><h2 id=prometheus-supplied-metrics>Prometheus supplied metrics</h2><p>The Prometheus client library provides a number of metrics under the <code>go</code> and <code>process</code> namespaces.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-057ef38a419f89387af4b6a88b63c2b1>4 - Proposals</h1></div><div class=td-content><h1 id=pg-544f5cd1f7b51e199bec5d6920882be3>4.1 - Design</h1><h1 id=etcd-backup-restore-design>Etcd backup restore design</h1><h2 id=goal>Goal</h2><p>Main goal of this project to provide a solution to make <a href=https://github.com/etcd-io/etcd>etcd</a> instance backing the kubernetes cluster robust to failures. Etcd data is backed up at regular intervals. The etcd instance runs on a Seed Kubernetes cluster and stores the state of the Shoot Kubernetes clusters. In case of etcd instance failures, the etcd instance is reconciled and in the extreme case restored from the latest non-corrupt backup available.</p><h2 id=non-goal>Non Goal</h2><ul><li>Volume failures like EBS volume failures, PVC and PV deletion after etcd deletion (when the shoot goes down) are not handled at this moment. However, checks and remedial steps for the failure types mentioned previously will be implemented over time/later.</li><li>Allowing the user to pick from a list of backups or even point-in-time-recovery (PiTR) is not planned, because there is no reliable infrastructure reconciliation implemented in Kubernetes and the CRD extension concept (or API server extensions) allows to have resources with impact on the external/physical world that would not be reconciled either (if we are to restore a backup which is not the latest).</li><li>Backup validation is not repeatedly done, i.e. once a backup is taken, we assume the infrastructure preserves it in a healthy state (we do not validate backups by rehashing them and comparing them with initially taken hashes).</li></ul><h2 id=design>Design</h2><h3 id=assumption>Assumption</h3><ul><li><p>Etcd cluster is a single member cluster (Shoot cluster environment, much like a master in a Borg-hosted GKE cluster).</p></li><li><p>Etcd instance will be deployed as a StatefulSet with a PVC for its persistence, which will bring free reconciliation.</p></li></ul><h3 id=requirements>Requirements</h3><ul><li><p>ETCD pod fails and PVC with valid data directory is available: Etcd pod should restart and attach to same PVC to continue from last state before failure.</p></li><li><p>Data corruption check: There should be mechanism to check etcd failure due to etcd data directory corruption.</p></li><li><p>Data directory unavailable but backups available in the cloud store: Restore Etcd from the latest backup.</p></li><li><p>Backup etcd snapshots on different cloud object-stores.</p></li></ul><h3 id=rely-on-k8s-for-the-following>Rely on K8s for the Following</h3><ul><li>PVC deleted - Doesn&rsquo;t necessarily detach volume if in use.</li><li>IaaS failure<ul><li>Storage Volume detached: PV reattaches to the same volume. Restoration of Etcd data directory not required.</li><li>Storage Volume deleted: Expect StatefulSet to be robust to such failures. Restoration of etcd data directory necessary.</li><li>Node failure: K8s should reschedule the pod on a different node. Restoration of Etcd data directory not required.</li><li>ETCD Pod scheduling failed: Can’t do much (look pod configuration parameter for critical-pods).</li></ul></li></ul><h2 id=architecture>Architecture</h2><p><img src=/__resources/etcd-backup-restore_f13a0f.jpg alt=architecture></p><p>We will have a StatefulSet for etcd with two containers in it.</p><ul><li>ETCD container</li><li>Sidecar container</li></ul><h3 id=etcd-container>ETCD Container</h3><ul><li>Request the sidecar to validate/initialize the data directory.</li><li>The etcd process is started only if the <code>initialize</code> request to sidecar returns a success.</li></ul><h3 id=sidecar-container>Sidecar Container</h3><p>Sidecar container has two components</p><ul><li>Initializer</li><li>Prober</li></ul><h4 id=initializer>Initializer</h4><ul><li>On request from the Etcd container, check the data directory for data corruption.</li><li>if data directory is corrupt, restore the data directory from the latest snapshot.</li><li>return successful response on validation/restoration of data directory.</li></ul><h4 id=prober>Prober</h4><ul><li>Probe etcd container for liveliness of etcd process.</li><li>Probe is required to ensure that etcd is live before backups are triggered.</li><li>Schedule the backup operation (probably using cron library) which triggers full snapshot at regular intervals.</li><li>Store the snapshot in the configured cloud object store.</li></ul><p><strong>Init container is not used for the validation/restoration of etcd data directory. The rationale behind the decision was to avoid baking in pod restart logic in sidecar container in the event etcd process had died. In case etcd container died, init-container had to be run before etcd container was run to ensure that data directory was valid. This required the pod to be restarted. With the current design, the sidecar handles the data directory validation/restoration and periodic backups. Pod restart is not required.</strong></p><h2 id=workflow>Workflow</h2><p><img src=/__resources/etcd-backup-restore-sequence-diagram_12616f.jpg alt=sequence-diagram></p><h3 id=etcd-container-1>Etcd container</h3><ol><li>Etcd container starts and requests the sidecar for data directory initialization. It waits for the response.</li><li>On response,<ol><li>in case of success, start etcd process.</li><li>in case of failure/timeout, exit with error. (Container restarts)</li></ol></li></ol><h3 id=sidecar-container-1>Sidecar container</h3><ol><li>Start periodic liveliness prober on etcd. Start http server to serve <code>initialize</code> requests.</li><li>On receiving an <code>initialize</code> request, check the data directory for corruption.<ol><li>In case of data directory corruption, restore data directory from the latest cloud snapshot. Return success.</li><li>In case data directory is valid, return success.</li><li>In all other cases, return failure.</li></ol></li><li>Once the <code>initialize</code> request returns success, etcd process can be expected to start up in some time. The prober would then receive a successful probe of etcd&rsquo;s liveliness.</li><li>On successful probe, start taking periodic backup of etcd and store the snapshot to the cloud object store. Stop prober.<ul><li>In case of a failure to take a backup, exit with error. (Container restarts)</li></ul></li></ol><h3 id=handling-of-different-scenariosissues>Handling of Different Scenarios/Issues</h3><ul><li>DNS latency: Should not matter for single member Etcd cluster.</li><li>Etcd upgrade and downgrade for K8s compatibility: Should not be issue for v3.* series released so far. Simply restart pod. No data format change.</li><li>IaaS issue: Issues like unreachable object store, will be taken care by init container and backup container. Both container will keep retrying to reach out object store with exponential timeouts.</li><li>Corrupt backup: StatefulSet go in restart loop, and human operator will with customers concern delete the last corrupt backup from object store manually. So that, in next iteration it will recover from previous non-corrupt backup.</li></ul><h2 id=outlook>Outlook</h2><p>We want to develop incremental/continuous etcd backups (write watch logs in between full backups), to ensure our backups are fresh enough to avoid discrepancies between etcd backup and external/physical world.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-8becec8126a0477f480a7d4313991617>4.2 - High Watch Event Ingress Rate</h1><h1 id=handle-high-watch-event-ingress-rate>Handle high watch event ingress rate</h1><h2 id=problem-statement>Problem statement</h2><p>For large key-value sizes of etcd and watch on etcd yielding responses at a high rates, we see that the backup sidecar memory shoots up drastically but linearly.</p><p><img src=/__resources/profile-high-rate-events_b2b269.png alt=profile></p><h2 id=reason>Reason</h2><p>The memory consumption surge is associated with the watch implementation in the client library.
As can be seen <a href=https://github.com/etcd-io/etcd/blob/master/clientv3/watch.go#L814>here</a>, when the ingress rate of the events are more than the egress rate from the watch response channel the buffers holding the events tend to bloat. There is no throttling done between etcd and the client. Even if throttling was done, we would not be able to depend on it. As the revision of etcd would have moved ahead and due to throttling the watch events would be far behind. This lag could get progressively worse if the load on etcd persists. We would have to implement a different approach to watches if we stumble upon this issue in realtime.</p><p>When the delta snapshot logic was changed to run multiple goroutines to drain the watch channel and to short circuit writing the delta to cloud but instead just drop it, we see that the memory pressure on the watch client is drastically reduced. The finding is shown in the image below.</p><p><img src=/__resources/profile-fast-drain_a0257d.png alt=profile></p><h2 id=solution>Solution</h2><p>There are two areas to address in order to arrive at a solution:</p><ul><li>Fix the watch implementation of etcd client to throttle the ingress of watch response events from etcd. Implement a fix avoid the unchecked growth of <a href=https://github.com/etcd-io/etcd/blob/master/clientv3/watch.go#L814>buffer</a> so as to prevent the OOM Kill.</li><li>Try a polling-based approach (with parallel download of chunks of revisions) as against the watch-based approach now. The delta revisions yet to be archived in the cloud store can be polled in chunks. The amount of memory consumed from as part of the poll based events fetch can be done in a controlled manner.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-7bdeb069310894a2eed087bffee2946b>4.3 - Restoration</h1><h1 id=etcd-data-restoration>Etcd data restoration</h1><p>Etcd-backup-restore provides a simple and automated method of restoring single-node etcd clusters from their backups (snapshots) stored via the snapshotting process. Restoration kicks in when etcd data gets corrupted, either by the etcd process itself or due to PV corruption, or if the data is manipulated or erased due to human intervention. The restorer relies on the backups stored in an object storage bucket to restore the etcd data to its latest backed-up non-corrupt state.</p><p>In the context of etcd-backup-restore server, restoration is a conditional part of the initialization workflow, in which restoration is triggered when etcd data validation deems the data directory invalid or corrupt.</p><p>To learn about starting an etcd-backup-restore server, please follow the <a href=/docs/other-components/etcd-backup-restore/deployment/getting_started/>getting started guide</a>.</p><h2 id=assumption>Assumption</h2><ul><li>Data directory must not contain <code>member</code> directory before triggering restoration</li><li>Snapshotter should NOT be running<ul><li>This is taken care of by etcd-backup-restore when started in <code>server</code> mode</li><li>If running in <code>snapshot</code> mode, one must take precautions to stop the snapshotter before attempting to restore</li></ul></li></ul><h2 id=requirements>Requirements</h2><ul><li>Etcd backup stored in an object storage bucket like AWS S3, GCP GCS, Azure ABS, Openstack Swift or Alicloud OSS</li><li>The backup format must conform to that of etcd-backup-restore&rsquo;s snapshotter<ul><li>Each backup must have a full snapshot, optionally followed by a set of delta/incremental snapshots</li></ul></li></ul><h2 id=workflow>Workflow</h2><p><img src=/__resources/restorer-sequence-diagram_f768e9.png alt=workflow></p><ul><li>Full snapshot is essentially the etcd DB file encapsulated in a JSON object</li><li>The full snapshot is downloaded only if available. If not, the embedded etcd server takes care of initializing the DB file</li><li>Delta snapshots are downloaded and applied only if available. If not, this step is skipped and restoration is marked as complete.</li></ul><p>Restorer fetches the latest full snapshot present in the snapstore, named <code>etcd-main/v1/Backup-xxxxxxxxxx/Full-aaaaaaaa-bbbbbbbb-xxxxxxxxxx</code>, and restores from it. It then fetches the subsequent delta snapshots and applies them sequentially on top of the partially restored data directory. These delta snapshots are named <code>etcd-main/v1/Backup-xxxxxxxxxx/Incr-bbbbbbbb-cccccccc-yyyyyyyyyy</code> and so on. Please consider the following while reading the aforementioned snapshot naming patterns: <code>xxxxxxxxxx</code>,<code>yyyyyyyyyy</code> are UTC timestamps and <code>aaaaaaaa</code>,<code>bbbbbbbb</code>,<code>cccccccc</code> are etcd revision numbers. Each snapshot contains etcd events between two revisions inclusive, as specified in the snapshot name.</p><h2 id=automatic-restoration>Automatic restoration</h2><p>Restoration is an essential part of etcd-backup-restore&rsquo;s <code>initialization</code> workflow, which is triggered as the first step in the <code>server</code> sub-command workflow. Ideally, operators need not worry about restoration when deploying etcd-backup-restore via the provided helm chart, where the tool is deployed in the <code>server</code> mode, as restoration will be triggered if the etcd data directory is found to be corrupt.</p><h2 id=manual-restoration>Manual restoration</h2><p>Please refer to <a href=/docs/other-components/etcd-backup-restore/operations/manual_restoration/>this guide</a> to know how to manually restore the etcd data, the caveats that come with it, and how to avoid common pitfalls.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-aa14d387a9e60db3ae269a07fe96be99>4.4 - Validation</h1><h1 id=etcd-data-validation>Etcd data validation</h1><p>As etcd is being used to store the state of the K8s cluster, it is mandatory that etcd deployment has to be hardened against data loss. Sufficient checks have to be in place to prevent etcd from erroneously starting with stale/corrupt data and taking stale snapshots to the backing store. We have a data validation flow in place which prevents etcd from starting in case of data corruption.</p><h2 id=directory-validation>Directory validation</h2><p>The etcd data directory validation comprises of multiple checks as mentioned below:</p><h3 id=structure-validation>Structure validation</h3><p>The member directory, snap directory and wal directory are checked to ascertain that they adhere to the directory structure followed by etcd.</p><h3 id=content-validation>Content validation</h3><h4 id=corruption-check>Corruption check</h4><p>The contents for the data directory(db file, snap files and wal file) are checked for data corruption.</p><h4 id=revision-check>Revision check</h4><p>The revision of etcd data in the db file is checked with the revision of the latest snapshot in the backing store. If the revison in the backing store is greater than that of etcd data in the db file, etcd data is considered stale. This is to prevent etcd snapshots for stale revisions from overwriting legit recent snapshots.</p><h2 id=validation-flow>Validation flow</h2><p>Not all validation steps take the same time to complete. Some validation steps are dependent on the size of etcd data(eg. db file). If the db file is checked for data corruption before etcd startup, it would take longer for etcd to become servicable. Therefore, it is only imperative to perform validation checks on abnormal etcd events like etcd restart after a crash. The validation flow mentioned below is modeled with the aforementioned rationale in mind.</p><ul><li>Is the validation marker file present?</li><li>No<ul><li>Do directory structure validation.</li><li>Do directory content validation.</li><li>Start etcd</li></ul></li><li>Yes<ul><li>Check if previous exit was normal from the validation marker file<ul><li>Yes<ul><li>Do revision check</li><li>Do directory structure validation.</li><li>Start etcd</li></ul></li><li>No<ul><li>Do directory structure validation.</li><li>Do directory content validation.</li><li>Start etcd</li></ul></li></ul></li></ul></li></ul><h2 id=addition-design-decisions-to-be-made>Addition design decisions to be made</h2><p>Currently, we have the validation check triggered from a bash script in the etcd container. The status of the validation check is polled till its completed and based on the validation status, it is decided whether it is safe to start etcd. During validation if etcd directory is found to be corrupt or stale, the latest snapshot in the backing store is used to restore etcd data to the latest revision.</p><h3 id=question-1-should-the-sidecar-container-be-able-to-act-on-the-status-of-previous-etcd-run-status>Question 1: Should the sidecar container be able to act on the status of previous etcd run status?</h3><ul><li><p><strong>Option 1</strong>: Yes. The information of previous etcd run may be made available to the sidecar container via configmaps. The idea is that <code>validate</code> REST endpoint shall check the shared configmap for status, perform necessary validation and restore steps before etcd start.</p></li><li><p><strong>Option 2</strong>: No. If the above-mentioned level of granularity is to be available for validation checks, we would need to modify the REST endpoints to trigger the validation sub-checks. Should we modify the bash script to handle the cases and let the sidecar be agnostic to the status of the previous etcd run?</p></li></ul><p>We have chosen the approach were the script decides on the previous exit status of etcd, to call the necessary validation step. If etcd terminated normally then sanity validation is performed else we perform a full etcd data validation.</p><h3 id=question-2-how-should-status-for-previous-etcd-run-be-identified>Question 2: How should status for previous etcd run be identified?</h3><ul><li><strong>Option 1</strong>: The error logs of the etcd run can be dumped to an log file in the persistent disk. This can be checked on subsequent validation steps to identify the status of previous etcd run.</li><li><strong>Option 2</strong>: Via exit code stored in a file in the persistent disk. This can be checked on subsequent validation steps to identify the status of previous etcd run.</li></ul><p>Since we are do not do an analysis of the logs at this point of time, the log dump and subsequent analysis steps can be taken care of in the necessary PR.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f671996e185034cc61696c5a61a71934>5 - Usage</h1></div><div class=td-content><h1 id=pg-851685c24fb26bf7de8c2075e084ffa2>5.1 - Garbage Collection</h1><h1 id=garbage-collection-gc-feature>Garbage Collection (GC) Feature</h1><p>The etcd-backup-restore project incorporates a Garbage Collection (GC) feature designed to manage storage space effectively by systematically discarding older backups. The <a href=https://github.com/gardener/etcd-backup-restore/blob/master/docs/usage/pkg/snapshot/snapshotter/garbagecollector.go><code>RunGarbageCollector</code></a> function controls this process, marking older backups as disposable and subsequently removing them based on predefined rules.</p><h2 id=gc-policies>GC Policies</h2><p>Garbage Collection policies fall into two categories, each of which can be configured with appropriate flags:</p><ol><li><p><strong>Exponential Policy</strong>: This policy operates on the principle of retaining the most recent snapshots and discarding older ones, based on the age and capture time of the snapshots. You can configure this policy with the following flag: <code>--garbage-collection-policy='Exponential'</code>. The garbage collection process under this policy unfolds as follows:</p><ul><li>The most recent full snapshot and its associated delta snapshots are perpetually retained, irrespective of the <code>delta-snapshot-retention-period</code> setting. This mechanism is vital for potential data recovery.</li><li>All delta snapshots that fall within the <code>delta-snapshot-retention-period</code> are preserved.</li><li>Full snapshots are retained for the current hour.</li><li>For the past 24 hours, the most recent full snapshot from each hour is kept.</li><li>For the past week (up to 7 days), the most recent full snapshot from each day is kept.</li><li>For the past month (up to 4 weeks), the most recent full snapshot from each week is kept.</li><li>Full snapshots older than 5 weeks are discarded.</li></ul></li><li><p><strong>Limit-Based Policy</strong>: This policy aims to keep the snapshot count under a specific limit, as determined by the configuration. The policy prioritizes retaining recent snapshots and eliminating older ones. You can configure this policy with the following flags: <code>--max-backups=10</code> and <code>--garbage-collection-policy='LimitBased'</code>. The garbage collection process under this policy unfolds as follows:</p><ul><li>The most recent full snapshot and its associated delta snapshots are always retained, regardless of the <code>delta-snapshot-retention-period</code> setting. This is essential for potential data recovery.</li><li>All delta snapshots that fall within the <code>delta-snapshot-retention-period</code> are preserved.</li><li>Full snapshots are retained up to the limit set in the configuration. Any full snapshots beyond this limit are removed.</li></ul></li></ol><h2 id=retention-period-for-delta-snapshots>Retention Period for Delta Snapshots</h2><p>The <code>delta-snapshot-retention-period</code> setting determines the retention period for older delta snapshots. It does not include the most recent set of snapshots, which are always retained to ensure data safety. The default value for this configuration is 0.</p><blockquote><p><strong>Note</strong>: In both policies, the garbage collection process includes listing the snapshots, identifying those that meet the deletion criteria, and then removing them. The deletion operation encompasses the removal of associated chunks, which form parts of a larger snapshot.</p></blockquote></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2023 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid@8.13.4/dist/mermaid.min.js integrity="sha512-JERecFUBbsm75UpkVheAuDOE8NdHjQBrPACfEQYPwvPG+fjgCpHAz1Jw2ci9EXmd3DdfiWth3O3CQvcfEg8gsA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.7b24c0fb082ffb2de6cb14d6c95e9f8053053709ffcf8c761ef8e9ad2f8021e4.js integrity="sha256-eyTA+wgv+y3myxTWyV6fgFMFNwn/z4x2HvjprS+AIeQ=" crossorigin=anonymous></script></body></html>