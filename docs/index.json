[{"uri":"https://gardener.cloud/documentation/home/","title":"Home","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.12.8/home/","title":"Home","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.13.2/home/","title":"Home","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/concepts/networking/dns-managment/","title":"DNS Management","tags":[],"description":"","content":"External DNS Management The main artefact of this project is the DNS controller manager for managing DNS records, also nicknamed as the Gardener \u0026ldquo;DNS Controller\u0026rdquo;.\nIt contains provisioning controllers for creating DNS records in one of the DNS cloud services\n Amazon Route53, Google CloudDNS, AliCloud DNS, Azure DNS, OpenStack Designate, Cloudflare DNS, Infoblox,  and source controllers for services and ingresses to create DNS entries by annotations.\nThe configuration for the external DNS service is specified in a custom resource DNSProvider. Multiple DNSProvider can be used simultaneously and changed without restarting the DNS controller.\nDNS records are either created directly for a corresponding custom resource DNSEntry or by annotating a service or ingress.\nFor a detailed explanation of the model, see section The Model.\nFor extending or adapting this project with your own source or provisioning controllers, see section Extensions\nQuick start To install the DNS controller manager in your Kubernetes cluster, follow these steps.\n  Prerequisites\n  Check out or download the project to get a copy of the Helm charts. It is recommended to check out the tag of the last release, so that Helm values reference the newest released container image for the deployment.\n  Make sure, that you have installed Helm client (helm) locally and Helm server (tiller) on the Kubernetes cluster. See e.g. Helm installation for more details.\n    Install the DNS controller manager\nAs multiple Gardener DNS controllers can act on the same DNS Hosted Zone concurrently, each instance needs an owner identifier. Therefore choose an identifier sufficiently unique across these instances.\nThen install the DNS controller manager with\nhelm install charts/external-dns-management --name dns-controller --namespace=\u0026lt;my-namespace\u0026gt; --set configuration.identifier=\u0026lt;my-identifier\u0026gt; This will use the default configuration with all source and provisioning controllers enabled. The complete set of configuration variables can be found in charts/external-dns-management/values.yaml. Their meaning is explained by their corresponding command line options in section Using the DNS controller manager\nBy default, the DNS controller looks for custom resources in all namespaces. The choosen namespace is only relevant for the deployment itself.\n  Create a DNSProvider\nTo specify a DNS provider, you need to create a custom resource DNSProvider and a secret containing the credentials for your account at the provider. E.g. if you want to use AWS Route53, create a secret and provider with\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: aws-credentials namespace: default type: Opaque data: # replace \u0026#39;...\u0026#39; with values encoded as base64 # see https://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html AWS_ACCESS_KEY_ID: ... AWS_SECRET_ACCESS_KEY: ... EOF and\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: dns.gardener.cloud/v1alpha1 kind: DNSProvider metadata: name: aws namespace: default spec: type: aws-route53 secretRef: name: aws-credentials domains: include: # this must be replaced with a (sub)domain of the hosted zone - my.own.domain.com EOF Check the successful creation with\nkubectl get dnspr You should see something like\nNAME TYPE STATUS AGE aws aws-route53 Ready 12s   Create a DNSEntry\nCreate an DNS entry with\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: dns.gardener.cloud/v1alpha1 kind: DNSEntry metadata: name: mydnsentry namespace: default spec: dnsName: \u0026#34;myentry.my-own-domain.com\u0026#34; ttl: 600 targets: - 1.2.3.4 EOF Check the status of the DNS entry with\nkubectl get dnsentry You should see something like\nNAME DNS TYPE PROVIDER STATUS AGE mydnsentry myentry.my-own-domain.com aws-route53 default/aws Ready 24s As soon as the status of the entry is Ready, the provider has accepted the new DNS record. Depending on the provider and your DNS settings and cache, it may take up to a few minutes before the domain name can be resolved.\n  Wait for/check DNS record\nTo check the DNS resolution, use nslookup or dig.\nnslookup myentry.my-own-domain.com or with dig\n# or with dig dig +short myentry.my-own-domain.com Depending on your network settings, you may get a successful response faster using a public DNS server (e.g. 8.8.8.8, 8.8.4.4, or 1.1.1.1)\ndig @8.8.8.8 +short myentry.my-own-domain.com   For more examples about the custom resources and the annotations for services and ingresses see the examples directory.\nAutomatic creation of DNS entries for services and ingresses Using the source controllers, it is also possible to create DNS entries for services (of type LoadBalancer) and ingresses automatically. The resources only need to be annotated with some special values. In this case ensure that the source controllers are enabled on startup of the DNS controller manager, i.e. the value of the command line option --controllers must contain dnscontrollers or equal to all. The DNS source controllers watch resources on the default cluster and create DNS entries on the target cluster. As there can be multiple controllers active on the same cluster, you may need to set the correct DNSClass both for the controller and for the source resource by setting the annotation dns.gardener.cloud/class. The default value for the DNSClass is gardendns.\nNote that if you delegate the DNS management for shoot resources to Gardener via the shoot-dns-service extension, the correct annotation is dns.gardener.cloud/class=garden.\nHere is an example for annotating a service (same as examples/50-service-with-dns.yaml)]:\napiVersion:v1kind:Servicemetadata:annotations:dns.gardener.cloud/dnsnames:echo.my-dns-domain.comdns.gardener.cloud/ttl:\u0026#34;500\u0026#34;# If you are delegating the DNS Management to Gardener, uncomment the following line (see https://gardener.cloud/documentation/guides/administer_shoots/dns_names/)#`dns.gardener.cloud/class`: gardenname:test-servicenamespace:defaultspec:ports:- name:httpport:80protocol:TCPtargetPort:8080sessionAffinity:Nonetype:LoadBalancerThe Model This project provides a flexible model allowing to add DNS source objects and DNS provisioning environments by adding new independent controllers.\nThere is no single DNS controller anymore. The decoupling between the handling of DNS source objects, like ingresses or services, and the provisioning of DNS entries in an external DNS provider like Route53 or CloudDNS is achieved by introducing a new custom resource DNSEntry.\nThese objects can either be explicitly created to request dedicated DNS entries, or they are managed based on other resources like ingresses or services. For the latter dedicated DNS Source Controllers are used. There might be any number of such source controllers. They do not need to know anything about the various DNS environments. Their task is to figure out which DNS entries are required in their realm and manage appropriate DNSEntry objects. From these objects they can also read the provisioning status and report it back to the original source.\nProvisioning of DNS entries in external DNS providers is done by DNS Provisioning Controllers. They don\u0026rsquo;t need to know anything about the various DNS source objects. They watch DNSEntry objects and check whether they are responsible for such an object. If a provisioning controller feels responsible for an entry it manages the corresponding settings in the external DNS environment and reports the provisioning status back to the corresponding DNSEntry object.\nTo do this a provisioning controller is responsible for a dedicated environment (for example Route53). For every such environment the controller uses a dedicated type key. This key is used to look for DNSProvider objects. There might be multiple such objects per environment, specifying the credentials needed to access different external accounts. These accounts are then scanned for DNS zones and domain names they support. This information is then used to dynamically assign DNSEntry objects to dedicated DNSProvider objects. If such an assignment can be done by a provisioning controller then it is responsible for this entry and manages the corresponding entries in the external environment. DNSProvider objects can specify explicit inclusion and exclusion sets of domain names and/or DNS zone identifiers to override the scanning results of the account.\nOwner Identifiers Every DNS Provisioning Controller is responsible for a set of Owner Identifiers. DNS records in an external DNS environment are attached to such an identifier. This is used to identify the records in the DNS environment managed by a dedicated controller (manager). Every controller manager hosting DNS Provisioning Controllers offers an option to specify a default identifier. Additionally there might be dedicated DNSOwner objects that enable or disable additional owner ids.\nEvery DNSEntry object may specify a dedicated owner that is used to tag the records in the DNS environment. A DNS provisioning controller only acts of DNS entries it is responsible for. Other resources in the external DNS environment are not touched at all.\nThis way it is possbible to\n identify records in the external DNS management environment that are managed by the actual controller instance distinguish different DNS source environments sharing the same hosted zones in the external management environment cleanup unused entries, even if the whole resource set is already gone move the responsibility for dedicated sets of DNS entries among different kubernetes clusters or DNS source environments running different DNS Provisioning Controller without loosing the entries during the migration process.  If multiple DNS controller instances have access to the same DNS zones, it is very important, that every instance uses a unique owner identifier! Otherwise the cleanup of stale DNS record will delete entries created by another instance if they use the same identifier.\nDNS Classes Multiple sets of controllers of the DNS ecosystem can run in parallel in a kubernetes cluster working on different object set. They are separated by using different DNS Classes. Adding a DNS class annotation to an object of the DNS ecosytems assigns this object to such a dedicated set of DNS controllers. This way it is possible to maintain clearly separated set of DNS objects in a single kubernetes cluster.\nDNSAnnotation objects DNS source controllers support the creation of DNS entries for potentiialy any kind of resource originally not equipped to describe the generation of DNS entries. This is done by additionally annotations. Nevertheless it might be the case, that those objects are again the result of a generation process, ether by predefined helm starts or by other higher level controllers. It is not necessarily possible to influence those generation steps to additionally generate the deired DNS annotations.\nThe typical mechanis in Kubernetes to handle this is to provide mutating webhooks that enrich the generated objects accordingly. But this mechanism is basically not intended to support dedicated settings for dedicated instances. At least it is very strenous to provide web hooks for every such usecase.\nTherefore the DNS ecosystem provided by this project supports an additional extension mechanism to annotate any kind of object with additional annotations by supported a dedicated resource, the DNSAnnotation.\nThe handling of this resource is done by a dedicated controller, the annotation controller. It caches the annotation settings declared by those objects and makes them accessible for the DNS source controllers.\nThe DNS source controller responsible for a dedicated kind of resource (for example Service reads the object analyses the annotations and then decides what to do with it. Most of the flow is handled by a central library, only some dedicated resource dependent steps are implemented separately by a dedicated source controller. The DNSAnnotationresource slightly extends this flow: After reading the object the library additionally checks for the existence of a DNSAnnotation setting for this object by querying the annotation controller\u0026rsquo;s cache. If found, it adds annotations declared there to the original object prior to the next processing steps. This way, for example whenver a Service without any DNS related annotation is handled by the controller and it finds a matching DNSAnnotation setting, the set of actual annotations is enriched accordingly before the actual processing of the service object is done by the controller.\nThis DNSAnnotation object can be created before or even after the object to be annotated and will implicity cause a reprocessing of the original object by its DNS source controller.\nFor example, the following object enforces a DNS related annotation for the processing of the service object testapp/default by the service DNS source controller:\napiVersion:dns.gardener.cloud/v1alpha1kind:DNSAnnotationmetadata:name:testappspec:resourceRef:kind:ServiceapiVersion:v1name:testappannotations:dns.gardener.cloud/dnsnames:testapp.dns.gardener.clouddns.gardener.cloud/ttl:\u0026#34;500\u0026#34;Using the DNS controller manager The controllers to run can be selected with the --controllers option. Here the following controller groups can be used:\n  dnssources: all DNS Source Controllers. It includes the conrollers\n ingress-dns: handle DNS annotations for the standard kubernetes ingress resource service-dns: handle DNS annotations for the standard kubernetes service resource    dnscontrollers: all DNS Provisioning Controllers. It includes the controllers\n alicloud-dns: aws-route53: azure-dns: google-clouddns: openstack-designate: cloudflare-dns    all: (default) all controllers\n  It is also possible to list dedicated controllers by their name.\nIf a DNS Provisioning Controller is enabled it is important to specify a unique controller identity using the --identifier option. This identifier is stored in the DNS system to identify the DNS entries managed by a dedicated controller. There should never be two DNS controllers with the same identifier running at the same time for the same DNS domains/accounts.\nHere is the complete list of options provided:\nUsage: dns-controller-manager [flags] Flags: --accepted-maintainers string accepted maintainer key(s) for crds --alicloud-dns.cache-dir string Directory to store zone caches (for reload after restart) of controller alicloud-dns --alicloud-dns.cache-ttl int Time-to-live for provider hosted zone cache of controller alicloud-dns (default 120) --alicloud-dns.default.pool.size int Worker pool size for pool default of controller alicloud-dns (default 2) --alicloud-dns.disable-zone-state-caching disable use of cached dns zone state on changes of controller alicloud-dns --alicloud-dns.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller alicloud-dns (default \u0026#34;gardendns\u0026#34;) --alicloud-dns.dns-delay duration delay between two dns reconciliations of controller alicloud-dns (default 10s) --alicloud-dns.dns.pool.resync-period duration Period for resynchronization for pool dns of controller alicloud-dns (default 15m0s) --alicloud-dns.dns.pool.size int Worker pool size for pool dns of controller alicloud-dns (default 1) --alicloud-dns.dry-run just check, don\u0026#39;t modify of controller alicloud-dns --alicloud-dns.identifier string Identifier used to mark DNS entries in DNS system of controller alicloud-dns (default \u0026#34;dnscontroller\u0026#34;) --alicloud-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller alicloud-dns (default 1) --alicloud-dns.pool.resync-period duration Period for resynchronization of controller alicloud-dns --alicloud-dns.pool.size int Worker pool size of controller alicloud-dns --alicloud-dns.providers.pool.resync-period duration Period for resynchronization for pool providers of controller alicloud-dns (default 10m0s) --alicloud-dns.providers.pool.size int Worker pool size for pool providers of controller alicloud-dns (default 2) --alicloud-dns.ratelimiter.burst int number of burst requests for rate limiter of controller alicloud-dns --alicloud-dns.ratelimiter.enabled enables rate limiter for DNS provider requests of controller alicloud-dns --alicloud-dns.ratelimiter.qps int maximum requests/queries per second of controller alicloud-dns --alicloud-dns.reschedule-delay duration reschedule delay after losing provider of controller alicloud-dns (default 2m0s) --alicloud-dns.secrets.pool.size int Worker pool size for pool secrets of controller alicloud-dns (default 2) --alicloud-dns.setup int number of processors for controller setup of controller alicloud-dns (default 10) --alicloud-dns.statistic.pool.size int Worker pool size for pool statistic of controller alicloud-dns (default 1) --alicloud-dns.ttl int Default time-to-live for DNS entries of controller alicloud-dns (default 300) --annotation.default.pool.size int Worker pool size for pool default of controller annotation (default 5) --annotation.pool.size int Worker pool size of controller annotation --annotation.setup int number of processors for controller setup of controller annotation (default 10) --aws-route53.cache-dir string Directory to store zone caches (for reload after restart) of controller aws-route53 --aws-route53.cache-ttl int Time-to-live for provider hosted zone cache of controller aws-route53 (default 120) --aws-route53.default.pool.size int Worker pool size for pool default of controller aws-route53 (default 2) --aws-route53.disable-zone-state-caching disable use of cached dns zone state on changes of controller aws-route53 --aws-route53.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller aws-route53 (default \u0026#34;gardendns\u0026#34;) --aws-route53.dns-delay duration delay between two dns reconciliations of controller aws-route53 (default 10s) --aws-route53.dns.pool.resync-period duration Period for resynchronization for pool dns of controller aws-route53 (default 15m0s) --aws-route53.dns.pool.size int Worker pool size for pool dns of controller aws-route53 (default 1) --aws-route53.dry-run just check, don\u0026#39;t modify of controller aws-route53 --aws-route53.identifier string Identifier used to mark DNS entries in DNS system of controller aws-route53 (default \u0026#34;dnscontroller\u0026#34;) --aws-route53.ownerids.pool.size int Worker pool size for pool ownerids of controller aws-route53 (default 1) --aws-route53.pool.resync-period duration Period for resynchronization of controller aws-route53 --aws-route53.pool.size int Worker pool size of controller aws-route53 --aws-route53.providers.pool.resync-period duration Period for resynchronization for pool providers of controller aws-route53 (default 10m0s) --aws-route53.providers.pool.size int Worker pool size for pool providers of controller aws-route53 (default 2) --aws-route53.ratelimiter.burst int number of burst requests for rate limiter of controller aws-route53 --aws-route53.ratelimiter.enabled enables rate limiter for DNS provider requests of controller aws-route53 --aws-route53.ratelimiter.qps int maximum requests/queries per second of controller aws-route53 --aws-route53.reschedule-delay duration reschedule delay after losing provider of controller aws-route53 (default 2m0s) --aws-route53.secrets.pool.size int Worker pool size for pool secrets of controller aws-route53 (default 2) --aws-route53.setup int number of processors for controller setup of controller aws-route53 (default 10) --aws-route53.statistic.pool.size int Worker pool size for pool statistic of controller aws-route53 (default 1) --aws-route53.ttl int Default time-to-live for DNS entries of controller aws-route53 (default 300) --azure-dns.cache-dir string Directory to store zone caches (for reload after restart) of controller azure-dns --azure-dns.cache-ttl int Time-to-live for provider hosted zone cache of controller azure-dns (default 120) --azure-dns.default.pool.size int Worker pool size for pool default of controller azure-dns (default 2) --azure-dns.disable-zone-state-caching disable use of cached dns zone state on changes of controller azure-dns --azure-dns.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller azure-dns (default \u0026#34;gardendns\u0026#34;) --azure-dns.dns-delay duration delay between two dns reconciliations of controller azure-dns (default 10s) --azure-dns.dns.pool.resync-period duration Period for resynchronization for pool dns of controller azure-dns (default 15m0s) --azure-dns.dns.pool.size int Worker pool size for pool dns of controller azure-dns (default 1) --azure-dns.dry-run just check, don\u0026#39;t modify of controller azure-dns --azure-dns.identifier string Identifier used to mark DNS entries in DNS system of controller azure-dns (default \u0026#34;dnscontroller\u0026#34;) --azure-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller azure-dns (default 1) --azure-dns.pool.resync-period duration Period for resynchronization of controller azure-dns --azure-dns.pool.size int Worker pool size of controller azure-dns --azure-dns.providers.pool.resync-period duration Period for resynchronization for pool providers of controller azure-dns (default 10m0s) --azure-dns.providers.pool.size int Worker pool size for pool providers of controller azure-dns (default 2) --azure-dns.ratelimiter.burst int number of burst requests for rate limiter of controller azure-dns --azure-dns.ratelimiter.enabled enables rate limiter for DNS provider requests of controller azure-dns --azure-dns.ratelimiter.qps int maximum requests/queries per second of controller azure-dns --azure-dns.reschedule-delay duration reschedule delay after losing provider of controller azure-dns (default 2m0s) --azure-dns.secrets.pool.size int Worker pool size for pool secrets of controller azure-dns (default 2) --azure-dns.setup int number of processors for controller setup of controller azure-dns (default 10) --azure-dns.statistic.pool.size int Worker pool size for pool statistic of controller azure-dns (default 1) --azure-dns.ttl int Default time-to-live for DNS entries of controller azure-dns (default 300) --bind-address-http string HTTP server bind address --cache-dir string Directory to store zone caches (for reload after restart) --cache-ttl int Time-to-live for provider hosted zone cache --cloudflare-dns.cache-dir string Directory to store zone caches (for reload after restart) of controller cloudflare-dns --cloudflare-dns.cache-ttl int Time-to-live for provider hosted zone cache of controller cloudflare-dns (default 120) --cloudflare-dns.default.pool.size int Worker pool size for pool default of controller cloudflare-dns (default 2) --cloudflare-dns.disable-zone-state-caching disable use of cached dns zone state on changes of controller cloudflare-dns --cloudflare-dns.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller cloudflare-dns (default \u0026#34;gardendns\u0026#34;) --cloudflare-dns.dns-delay duration delay between two dns reconciliations of controller cloudflare-dns (default 10s) --cloudflare-dns.dns.pool.resync-period duration Period for resynchronization for pool dns of controller cloudflare-dns (default 15m0s) --cloudflare-dns.dns.pool.size int Worker pool size for pool dns of controller cloudflare-dns (default 1) --cloudflare-dns.dry-run just check, don\u0026#39;t modify of controller cloudflare-dns --cloudflare-dns.identifier string Identifier used to mark DNS entries in DNS system of controller cloudflare-dns (default \u0026#34;dnscontroller\u0026#34;) --cloudflare-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller cloudflare-dns (default 1) --cloudflare-dns.pool.resync-period duration Period for resynchronization of controller cloudflare-dns --cloudflare-dns.pool.size int Worker pool size of controller cloudflare-dns --cloudflare-dns.providers.pool.resync-period duration Period for resynchronization for pool providers of controller cloudflare-dns (default 10m0s) --cloudflare-dns.providers.pool.size int Worker pool size for pool providers of controller cloudflare-dns (default 2) --cloudflare-dns.ratelimiter.burst int number of burst requests for rate limiter of controller cloudflare-dns --cloudflare-dns.ratelimiter.enabled enables rate limiter for DNS provider requests of controller cloudflare-dns --cloudflare-dns.ratelimiter.qps int maximum requests/queries per second of controller cloudflare-dns --cloudflare-dns.reschedule-delay duration reschedule delay after losing provider of controller cloudflare-dns (default 2m0s) --cloudflare-dns.secrets.pool.size int Worker pool size for pool secrets of controller cloudflare-dns (default 2) --cloudflare-dns.setup int number of processors for controller setup of controller cloudflare-dns (default 10) --cloudflare-dns.statistic.pool.size int Worker pool size for pool statistic of controller cloudflare-dns (default 1) --cloudflare-dns.ttl int Default time-to-live for DNS entries of controller cloudflare-dns (default 300) --config string config file -c, --controllers string comma separated list of controllers to start (\u0026lt;name\u0026gt;,\u0026lt;group\u0026gt;,all) (default \u0026#34;all\u0026#34;) --cpuprofile string set file for cpu profiling --default.pool.resync-period duration Period for resynchronization for pool default --default.pool.size int Worker pool size for pool default --disable-namespace-restriction disable access restriction for namespace local access only --disable-zone-state-caching disable use of cached dns zone state on changes --dns-class string identifier used to differentiate responsible controllers for entries --dns-delay duration delay between two dns reconciliations --dns-target-class string identifier used to differentiate responsible dns controllers for target entries --dns.pool.resync-period duration Period for resynchronization for pool dns --dns.pool.size int Worker pool size for pool dns --dnsentry-source.default.pool.resync-period duration Period for resynchronization for pool default of controller dnsentry-source (default 2m0s) --dnsentry-source.default.pool.size int Worker pool size for pool default of controller dnsentry-source (default 2) --dnsentry-source.dns-class string identifier used to differentiate responsible controllers for entries of controller dnsentry-source (default \u0026#34;gardendns\u0026#34;) --dnsentry-source.dns-target-class string identifier used to differentiate responsible dns controllers for target entries of controller dnsentry-source --dnsentry-source.exclude-domains stringArray excluded domains of controller dnsentry-source --dnsentry-source.key string selecting key for annotation of controller dnsentry-source --dnsentry-source.pool.resync-period duration Period for resynchronization of controller dnsentry-source --dnsentry-source.pool.size int Worker pool size of controller dnsentry-source --dnsentry-source.target-creator-label-name string label name to store the creator for generated DNS entries of controller dnsentry-source (default \u0026#34;creator\u0026#34;) --dnsentry-source.target-creator-label-value string label value for creator label of controller dnsentry-source --dnsentry-source.target-name-prefix string name prefix in target namespace for cross cluster generation of controller dnsentry-source --dnsentry-source.target-namespace string target namespace for cross cluster generation of controller dnsentry-source --dnsentry-source.target-owner-id string owner id to use for generated DNS entries of controller dnsentry-source --dnsentry-source.target-realms string realm(s) to use for generated DNS entries of controller dnsentry-source --dnsentry-source.target-set-ignore-owners mark generated DNS entries to omit owner based access control of controller dnsentry-source --dnsentry-source.targets.pool.size int Worker pool size for pool targets of controller dnsentry-source (default 2) --dry-run just check, don\u0026#39;t modify --exclude-domains stringArray excluded domains --force-crd-update enforce update of crds even they are unmanaged --google-clouddns.cache-dir string Directory to store zone caches (for reload after restart) of controller google-clouddns --google-clouddns.cache-ttl int Time-to-live for provider hosted zone cache of controller google-clouddns (default 120) --google-clouddns.default.pool.size int Worker pool size for pool default of controller google-clouddns (default 2) --google-clouddns.disable-zone-state-caching disable use of cached dns zone state on changes of controller google-clouddns --google-clouddns.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller google-clouddns (default \u0026#34;gardendns\u0026#34;) --google-clouddns.dns-delay duration delay between two dns reconciliations of controller google-clouddns (default 10s) --google-clouddns.dns.pool.resync-period duration Period for resynchronization for pool dns of controller google-clouddns (default 15m0s) --google-clouddns.dns.pool.size int Worker pool size for pool dns of controller google-clouddns (default 1) --google-clouddns.dry-run just check, don\u0026#39;t modify of controller google-clouddns --google-clouddns.identifier string Identifier used to mark DNS entries in DNS system of controller google-clouddns (default \u0026#34;dnscontroller\u0026#34;) --google-clouddns.ownerids.pool.size int Worker pool size for pool ownerids of controller google-clouddns (default 1) --google-clouddns.pool.resync-period duration Period for resynchronization of controller google-clouddns --google-clouddns.pool.size int Worker pool size of controller google-clouddns --google-clouddns.providers.pool.resync-period duration Period for resynchronization for pool providers of controller google-clouddns (default 10m0s) --google-clouddns.providers.pool.size int Worker pool size for pool providers of controller google-clouddns (default 2) --google-clouddns.ratelimiter.burst int number of burst requests for rate limiter of controller google-clouddns --google-clouddns.ratelimiter.enabled enables rate limiter for DNS provider requests of controller google-clouddns --google-clouddns.ratelimiter.qps int maximum requests/queries per second of controller google-clouddns --google-clouddns.reschedule-delay duration reschedule delay after losing provider of controller google-clouddns (default 2m0s) --google-clouddns.secrets.pool.size int Worker pool size for pool secrets of controller google-clouddns (default 2) --google-clouddns.setup int number of processors for controller setup of controller google-clouddns (default 10) --google-clouddns.statistic.pool.size int Worker pool size for pool statistic of controller google-clouddns (default 1) --google-clouddns.ttl int Default time-to-live for DNS entries of controller google-clouddns (default 300) --grace-period duration inactivity grace period for detecting end of cleanup for shutdown -h, --help help for dns-controller-manager --identifier string Identifier used to mark DNS entries in DNS system --infoblox-dns.cache-dir string Directory to store zone caches (for reload after restart) of controller infoblox-dns --infoblox-dns.cache-ttl int Time-to-live for provider hosted zone cache of controller infoblox-dns (default 120) --infoblox-dns.default.pool.size int Worker pool size for pool default of controller infoblox-dns (default 2) --infoblox-dns.disable-zone-state-caching disable use of cached dns zone state on changes of controller infoblox-dns --infoblox-dns.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller infoblox-dns (default \u0026#34;gardendns\u0026#34;) --infoblox-dns.dns-delay duration delay between two dns reconciliations of controller infoblox-dns (default 10s) --infoblox-dns.dns.pool.resync-period duration Period for resynchronization for pool dns of controller infoblox-dns (default 15m0s) --infoblox-dns.dns.pool.size int Worker pool size for pool dns of controller infoblox-dns (default 1) --infoblox-dns.dry-run just check, don\u0026#39;t modify of controller infoblox-dns --infoblox-dns.identifier string Identifier used to mark DNS entries in DNS system of controller infoblox-dns (default \u0026#34;dnscontroller\u0026#34;) --infoblox-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller infoblox-dns (default 1) --infoblox-dns.pool.resync-period duration Period for resynchronization of controller infoblox-dns --infoblox-dns.pool.size int Worker pool size of controller infoblox-dns --infoblox-dns.providers.pool.resync-period duration Period for resynchronization for pool providers of controller infoblox-dns (default 10m0s) --infoblox-dns.providers.pool.size int Worker pool size for pool providers of controller infoblox-dns (default 2) --infoblox-dns.ratelimiter.burst int number of burst requests for rate limiter of controller infoblox-dns --infoblox-dns.ratelimiter.enabled enables rate limiter for DNS provider requests of controller infoblox-dns --infoblox-dns.ratelimiter.qps int maximum requests/queries per second of controller infoblox-dns --infoblox-dns.reschedule-delay duration reschedule delay after losing provider of controller infoblox-dns (default 2m0s) --infoblox-dns.secrets.pool.size int Worker pool size for pool secrets of controller infoblox-dns (default 2) --infoblox-dns.setup int number of processors for controller setup of controller infoblox-dns (default 10) --infoblox-dns.statistic.pool.size int Worker pool size for pool statistic of controller infoblox-dns (default 1) --infoblox-dns.ttl int Default time-to-live for DNS entries of controller infoblox-dns (default 300) --ingress-dns.default.pool.resync-period duration Period for resynchronization for pool default of controller ingress-dns (default 2m0s) --ingress-dns.default.pool.size int Worker pool size for pool default of controller ingress-dns (default 2) --ingress-dns.dns-class string identifier used to differentiate responsible controllers for entries of controller ingress-dns (default \u0026#34;gardendns\u0026#34;) --ingress-dns.dns-target-class string identifier used to differentiate responsible dns controllers for target entries of controller ingress-dns --ingress-dns.exclude-domains stringArray excluded domains of controller ingress-dns --ingress-dns.key string selecting key for annotation of controller ingress-dns --ingress-dns.pool.resync-period duration Period for resynchronization of controller ingress-dns --ingress-dns.pool.size int Worker pool size of controller ingress-dns --ingress-dns.target-creator-label-name string label name to store the creator for generated DNS entries of controller ingress-dns (default \u0026#34;creator\u0026#34;) --ingress-dns.target-creator-label-value string label value for creator label of controller ingress-dns --ingress-dns.target-name-prefix string name prefix in target namespace for cross cluster generation of controller ingress-dns --ingress-dns.target-namespace string target namespace for cross cluster generation of controller ingress-dns --ingress-dns.target-owner-id string owner id to use for generated DNS entries of controller ingress-dns --ingress-dns.target-realms string realm(s) to use for generated DNS entries of controller ingress-dns --ingress-dns.target-set-ignore-owners mark generated DNS entries to omit owner based access control of controller ingress-dns --ingress-dns.targets.pool.size int Worker pool size for pool targets of controller ingress-dns (default 2) --key string selecting key for annotation --kubeconfig string default cluster access --kubeconfig.disable-deploy-crds disable deployment of required crds for cluster default --kubeconfig.id string id for cluster default --kubeconfig.migration-ids string migration id for cluster default --lease-duration duration lease duration (default 15s) --lease-name string name for lease object --lease-renew-deadline duration lease renew deadline (default 10s) --lease-retry-period duration lease retry period (default 2s) -D, --log-level string logrus log level --maintainer string maintainer key for crds (default \u0026#34;dns-controller-manager\u0026#34;) --name string name used for controller manager (default \u0026#34;dns-controller-manager\u0026#34;) --namespace string namespace for lease (default \u0026#34;kube-system\u0026#34;) -n, --namespace-local-access-only enable access restriction for namespace local access only (deprecated) --omit-lease omit lease for development --openstack-designate.cache-dir string Directory to store zone caches (for reload after restart) of controller openstack-designate --openstack-designate.cache-ttl int Time-to-live for provider hosted zone cache of controller openstack-designate (default 120) --openstack-designate.default.pool.size int Worker pool size for pool default of controller openstack-designate (default 2) --openstack-designate.disable-zone-state-caching disable use of cached dns zone state on changes of controller openstack-designate --openstack-designate.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller openstack-designate (default \u0026#34;gardendns\u0026#34;) --openstack-designate.dns-delay duration delay between two dns reconciliations of controller openstack-designate (default 10s) --openstack-designate.dns.pool.resync-period duration Period for resynchronization for pool dns of controller openstack-designate (default 15m0s) --openstack-designate.dns.pool.size int Worker pool size for pool dns of controller openstack-designate (default 1) --openstack-designate.dry-run just check, don\u0026#39;t modify of controller openstack-designate --openstack-designate.identifier string Identifier used to mark DNS entries in DNS system of controller openstack-designate (default \u0026#34;dnscontroller\u0026#34;) --openstack-designate.ownerids.pool.size int Worker pool size for pool ownerids of controller openstack-designate (default 1) --openstack-designate.pool.resync-period duration Period for resynchronization of controller openstack-designate --openstack-designate.pool.size int Worker pool size of controller openstack-designate --openstack-designate.providers.pool.resync-period duration Period for resynchronization for pool providers of controller openstack-designate (default 10m0s) --openstack-designate.providers.pool.size int Worker pool size for pool providers of controller openstack-designate (default 2) --openstack-designate.ratelimiter.burst int number of burst requests for rate limiter of controller openstack-designate --openstack-designate.ratelimiter.enabled enables rate limiter for DNS provider requests of controller openstack-designate --openstack-designate.ratelimiter.qps int maximum requests/queries per second of controller openstack-designate --openstack-designate.reschedule-delay duration reschedule delay after losing provider of controller openstack-designate (default 2m0s) --openstack-designate.secrets.pool.size int Worker pool size for pool secrets of controller openstack-designate (default 2) --openstack-designate.setup int number of processors for controller setup of controller openstack-designate (default 10) --openstack-designate.statistic.pool.size int Worker pool size for pool statistic of controller openstack-designate (default 1) --openstack-designate.ttl int Default time-to-live for DNS entries of controller openstack-designate (default 300) --ownerids.pool.size int Worker pool size for pool ownerids --plugin-file string directory containing go plugins --pool.resync-period duration Period for resynchronization --pool.size int Worker pool size --providers string cluster to look for provider objects --providers.disable-deploy-crds disable deployment of required crds for cluster provider --providers.id string id for cluster provider --providers.migration-ids string migration id for cluster provider --providers.pool.resync-period duration Period for resynchronization for pool providers --providers.pool.size int Worker pool size for pool providers --ratelimiter.burst int number of burst requests for rate limiter --ratelimiter.enabled enables rate limiter for DNS provider requests --ratelimiter.qps int maximum requests/queries per second --reschedule-delay duration reschedule delay after losing provider --secrets.pool.size int Worker pool size for pool secrets --server-port-http int HTTP server port (serving /healthz, /metrics, ...) --service-dns.default.pool.resync-period duration Period for resynchronization for pool default of controller service-dns (default 2m0s) --service-dns.default.pool.size int Worker pool size for pool default of controller service-dns (default 2) --service-dns.dns-class string identifier used to differentiate responsible controllers for entries of controller service-dns (default \u0026#34;gardendns\u0026#34;) --service-dns.dns-target-class string identifier used to differentiate responsible dns controllers for target entries of controller service-dns --service-dns.exclude-domains stringArray excluded domains of controller service-dns --service-dns.key string selecting key for annotation of controller service-dns --service-dns.pool.resync-period duration Period for resynchronization of controller service-dns --service-dns.pool.size int Worker pool size of controller service-dns --service-dns.target-creator-label-name string label name to store the creator for generated DNS entries of controller service-dns (default \u0026#34;creator\u0026#34;) --service-dns.target-creator-label-value string label value for creator label of controller service-dns --service-dns.target-name-prefix string name prefix in target namespace for cross cluster generation of controller service-dns --service-dns.target-namespace string target namespace for cross cluster generation of controller service-dns --service-dns.target-owner-id string owner id to use for generated DNS entries of controller service-dns --service-dns.target-realms string realm(s) to use for generated DNS entries of controller service-dns --service-dns.target-set-ignore-owners mark generated DNS entries to omit owner based access control of controller service-dns --service-dns.targets.pool.size int Worker pool size for pool targets of controller service-dns (default 2) --setup int number of processors for controller setup --statistic.pool.size int Worker pool size for pool statistic --target string target cluster for dns requests --target-creator-label-name string label name to store the creator for generated DNS entries --target-creator-label-value string label value for creator label --target-name-prefix string name prefix in target namespace for cross cluster generation --target-namespace string target namespace for cross cluster generation --target-owner-id string owner id to use for generated DNS entries --target-realms string realm(s) to use for generated DNS entries --target-set-ignore-owners mark generated DNS entries to omit owner based access control --target.disable-deploy-crds disable deployment of required crds for cluster target --target.id string id for cluster target --target.migration-ids string migration id for cluster target --targets.pool.size int Worker pool size for pool targets --ttl int Default time-to-live for DNS entries (defaults to 300s). Defines how long the record is kept in cache by DNS servers or resolvers. --version version for dns-controller-manager Extensions This project can also be used as library to implement own source and provisioning controllers.\nHow to implement Source Controllers Based on the provided source controller library a source controller must implement the source.DNSSource interface and provide an appropriate creator function.\nA source controller can be implemented following this example:\npackage service import ( \u0026#34;github.com/gardener/controller-manager-library/pkg/resources\u0026#34; \u0026#34;github.com/gardener/external-dns-management/pkg/dns/source\u0026#34; ) var _MAIN_RESOURCE = resources.NewGroupKind(\u0026#34;core\u0026#34;, \u0026#34;Service\u0026#34;) func init() { source.DNSSourceController(source.NewDNSSouceTypeForExtractor(\u0026#34;service-dns\u0026#34;, _MAIN_RESOURCE, GetTargets),nil). FinalizerDomain(\u0026#34;dns.gardener.cloud\u0026#34;). MustRegister(source.CONTROLLER_GROUP_DNS_SOURCES) } Complete examples can be found in the sub packages of pkg/controller/source.\nHow to implement Provisioning Controllers Provisioning controllers can be implemented based on the provisioning controller library in this repository and must implement the provider.DNSHandlerFactory interface. This factory returns implementations of the provider.DNSHandler interface that does the effective work for a dedicated set of hosted zones.\nThese factories can be embedded into a final controller manager (the runnable instance) in several ways:\n The factory can be used to create a dedicated controller. This controller can then be embedded into a controller manager, either in its own controller manger or together with other controllers. The factory can be added to a compound factory, able to handle multiple infrastructures. This one can then be used to create a dedicated controller, again.  Embedding a Factory into a Controller A provisioning controller can be implemented following this example:\npackage controller import ( \u0026#34;github.com/gardener/external-dns-management/pkg/dns/provider\u0026#34; ) const CONTROLLER_NAME = \u0026#34;route53-dns-controller\u0026#34; func init() { provider.DNSController(CONTROLLER_NAME, \u0026amp;Factory{}). FinalizerDomain(\u0026#34;dns.gardener.cloud\u0026#34;). MustRegister(provider.CONTROLLER_GROUP_DNS_CONTROLLERS) } This controller can be embedded into a controller manager just by using an anonymous import of the controller package in the main package of a dedicated controller manager.\nComplete examples are available in the sub packages of pkg/controller/provider. They also show a typical set of implementation structures that help to structure the implementation of such controllers.\nThe provider implemented in this project always follow the same structure:\n the provider package contains the provider code the factory source file registers the factory at a default compound factory it contains a sub package controller, which contains the embedding of the factory into a dedicated controller  Embedding a Factory into a Compound Factory A provisioning controller based on a Compound Factory can be extended by a new provider factory by registering this factory at the compound factory. This could be done, for example, by using the default compound factory provided in package pkg/controller/provider/compound as shown here, where NewHandler is a function creating a dedicated handler for a dedicated provider type:\npackage aws import ( \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/compound\u0026#34; \u0026#34;github.com/gardener/external-dns-management/pkg/dns/provider\u0026#34; ) const TYPE_CODE = \u0026#34;aws-route53\u0026#34; var Factory = provider.NewDNSHandlerFactory(TYPE_CODE, NewHandler) func init() { compound.MustRegister(Factory) } The compound factory is then again embedded into a provisioning controller as shown in the previous section (see the controllersub package).\nSetting Up a Controller Manager One or multiple controller packages can be bundled into a controller manager, by implementing a main package like this:\npackage main import ( \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026#34; _ \u0026#34;github.com/\u0026lt;your controller package\u0026gt;\u0026#34; ... ) func main() { controllermanager.Start(\u0026#34;my-dns-controller-manager\u0026#34;, \u0026#34;dns controller manager\u0026#34;, \u0026#34;some description\u0026#34;) } Using the standard Compound Provisioning Controller If the standard Compound Provisioning Controller should be used it is required to additionally add the anonymous imports for the providers intended to be embedded into the compound factory like this:\n Example Coding package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/compound/controller\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/\u0026lt;your provider\u0026gt;\u0026#34; ... ) func main() { controllermanager.Start(\u0026#34;dns-controller-manager\u0026#34;, \u0026#34;dns controller manager\u0026#34;, \u0026#34;nothing\u0026#34;) }  Multiple Cluster Support The controller implementations provided in this project are prepared to work with multiple clusters by using the features of the used controller manager library.\nThe DNS Source Controllers support two clusters:\n the default cluster is used to scan for source objects the logical cluster target is used to maintain the DNSEnry objects.  The DNS Provisioning Controllers also support two clusters:\n the default cluster is used to scan for DNSEntry objects. It is mapped to the logical cluster target the logical cluster provider is used to look to the DNSProvider objects and their related secrets.  If those controller types should be combined in a single controller manager, it can be configured to support three potential clusters with the source objects, the one for the entry objects and the one with provider objects using cluster mappings.\nThis is shown in a complete example using the dns source controllers, the compound provisioning controller configured to support all the included DNS provider type factories:\n Example Coding package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager/cluster\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager/controller\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager/controller/mappings\u0026#34; dnsprovider \u0026#34;github.com/gardener/external-dns-management/pkg/dns/provider\u0026#34; dnssource \u0026#34;github.com/gardener/external-dns-management/pkg/dns/source\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/compound/controller\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/alicloud\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/aws\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/azure\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/google\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/openstack\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/cloudflare\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/source/ingress\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/source/service\u0026#34; ) func init() { // target cluster already defined in dns source controller package  cluster.Configure( dnsprovider.PROVIDER_CLUSTER, \u0026#34;providers\u0026#34;, \u0026#34;cluster to look for provider objects\u0026#34;, ).Fallback(dnssource.TARGET_CLUSTER) mappings.ForControllerGroup(dnsprovider.CONTROLLER_GROUP_DNS_CONTROLLERS). Map(controller.CLUSTER_MAIN, dnssource.TARGET_CLUSTER).MustRegister() } func main() { controllermanager.Start(\u0026#34;dns-controller-manager\u0026#34;, \u0026#34;dns controller manager\u0026#34;, \u0026#34;nothing\u0026#34;) }  Those clusters can the be separated by registering their names together with command line option names. These can be used to specify different kubeconfig files for those clusters.\nBy default all logical clusters are mapped to the default physical cluster specified via --kubeconfig or default cluster access.\nIf multiple physical clusters are defined they can be specified by a corresponding cluster option defining the kubeconfig file used to access this cluster. If no such option is specified the default is used.\nTherefore, even if the configuration is prepared for multiple clusters, such a controller manager can easily work on a single cluster if no special options are given on the command line.\nWhy not using the community external-dns solution? Some of the reasons are context-specific, i.e. relate to Gardener\u0026rsquo;s highly dynamic requirements.\n Custom resource for DNS entries  DNS entries are explicitly specified as custom resources. As an important side effect, each DNS entry provides an own status. Simply by querying the Kubernetes API, a client can check if a requested DNS entry has been successfully added to the DNS backend, or if an update has already been deployed, or if not to reason about the cause. It also opens for easy extensibility, as DNS entries can be created directly via the Kubernetes API. And it simplifies Day 2 operations, e.g. automatic cleanup of unused entries if a DNS provider is deleted.\nManagement of multiple DNS providers  The Gardener DNS controller uses a custom resource DNSProvider to dynamically manage the backend DNS services. While with external-dns you have to specify the single provider during startup, in the Gardener DNS controller you can add/update/delete providers during runtime with different credentials and/or backends. This is important for a multi-tenant environment as in Gardener, where users can bring their own accounts.\nA DNS provider can also restrict its actions on subset of the DNS domains (includes and excludes) for which the credentials are capable to edit.\nEach provider can define a separate owner identifier, to differentiate DNS entries in the same DNS zone from different providers.\nMulti cluster support  The Gardener DNS controller distinguish three different logical Kubernetes clusters: Source cluster, target cluster and runtime cluster. The source cluster is monitored by the DNS source controllers for annotations on ingress and service resources. These controllers then create DNS entries in the target cluster. DNS entries in the target cluster are then reconciliated/synchronized with the corresponding DNS backend service by the provider controller. The runtime cluster is the cluster the DNS controller runs on. For example, this enables needed flexibility in the Gardener deployment. The DNS controller runs on the seed cluster. This is also the target cluster. DNS providers and entries resources are created in the corresponding namespace of the shoot control plane, while the source cluster is the shoot cluster itself.\nOptimizations for handling hundreds of DNS entries  Some DNS backend services are restricted on the API calls per second (e.g. the AWS Route 53 API). To manage hundreds of DNS entries it is important to minimize the number of API calls. The Gardener DNS controller heavily makes usage of caches and batch processing for this reason.\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/networking/dns-managment/","title":"DNS Management","tags":[],"description":"","content":"External DNS Management The main artefact of this project is the DNS controller manager for managing DNS records, also nicknamed as the Gardener \u0026ldquo;DNS Controller\u0026rdquo;.\nIt contains provisioning controllers for creating DNS records in one of the DNS cloud services\n Amazon Route53, Google CloudDNS, AliCloud DNS, Azure DNS, OpenStack Designate, Cloudflare DNS, Infoblox,  and source controllers for services and ingresses to create DNS entries by annotations.\nThe configuration for the external DNS service is specified in a custom resource DNSProvider. Multiple DNSProvider can be used simultaneously and changed without restarting the DNS controller.\nDNS records are either created directly for a corresponding custom resource DNSEntry or by annotating a service or ingress.\nFor a detailed explanation of the model, see section The Model.\nFor extending or adapting this project with your own source or provisioning controllers, see section Extensions\nQuick start To install the DNS controller manager in your Kubernetes cluster, follow these steps.\n  Prerequisites\n  Check out or download the project to get a copy of the Helm charts. It is recommended to check out the tag of the last release, so that Helm values reference the newest released container image for the deployment.\n  Make sure, that you have installed Helm client (helm) locally and Helm server (tiller) on the Kubernetes cluster. See e.g. Helm installation for more details.\n    Install the DNS controller manager\nAs multiple Gardener DNS controllers can act on the same DNS Hosted Zone concurrently, each instance needs an owner identifier. Therefore choose an identifier sufficiently unique across these instances.\nThen install the DNS controller manager with\nhelm install charts/external-dns-management --name dns-controller --namespace=\u0026lt;my-namespace\u0026gt; --set configuration.identifier=\u0026lt;my-identifier\u0026gt; This will use the default configuration with all source and provisioning controllers enabled. The complete set of configuration variables can be found in charts/external-dns-management/values.yaml. Their meaning is explained by their corresponding command line options in section Using the DNS controller manager\nBy default, the DNS controller looks for custom resources in all namespaces. The choosen namespace is only relevant for the deployment itself.\n  Create a DNSProvider\nTo specify a DNS provider, you need to create a custom resource DNSProvider and a secret containing the credentials for your account at the provider. E.g. if you want to use AWS Route53, create a secret and provider with\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: aws-credentials namespace: default type: Opaque data: # replace \u0026#39;...\u0026#39; with values encoded as base64 # see https://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html AWS_ACCESS_KEY_ID: ... AWS_SECRET_ACCESS_KEY: ... EOF and\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: dns.gardener.cloud/v1alpha1 kind: DNSProvider metadata: name: aws namespace: default spec: type: aws-route53 secretRef: name: aws-credentials domains: include: # this must be replaced with a (sub)domain of the hosted zone - my.own.domain.com EOF Check the successful creation with\nkubectl get dnspr You should see something like\nNAME TYPE STATUS AGE aws aws-route53 Ready 12s   Create a DNSEntry\nCreate an DNS entry with\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: dns.gardener.cloud/v1alpha1 kind: DNSEntry metadata: name: mydnsentry namespace: default spec: dnsName: \u0026#34;myentry.my-own-domain.com\u0026#34; ttl: 600 targets: - 1.2.3.4 EOF Check the status of the DNS entry with\nkubectl get dnsentry You should see something like\nNAME DNS TYPE PROVIDER STATUS AGE mydnsentry myentry.my-own-domain.com aws-route53 default/aws Ready 24s As soon as the status of the entry is Ready, the provider has accepted the new DNS record. Depending on the provider and your DNS settings and cache, it may take up to a few minutes before the domain name can be resolved.\n  Wait for/check DNS record\nTo check the DNS resolution, use nslookup or dig.\nnslookup myentry.my-own-domain.com or with dig\n# or with dig dig +short myentry.my-own-domain.com Depending on your network settings, you may get a successful response faster using a public DNS server (e.g. 8.8.8.8, 8.8.4.4, or 1.1.1.1)\ndig @8.8.8.8 +short myentry.my-own-domain.com   For more examples about the custom resources and the annotations for services and ingresses see the examples directory.\nAutomatic creation of DNS entries for services and ingresses Using the source controllers, it is also possible to create DNS entries for services (of type LoadBalancer) and ingresses automatically. The resources only need to be annotated with some special values. In this case ensure that the source controllers are enabled on startup of the DNS controller manager, i.e. the value of the command line option --controllers must contain dnscontrollers or equal to all. The DNS source controllers watch resources on the default cluster and create DNS entries on the target cluster. As there can be multiple controllers active on the same cluster, you may need to set the correct DNSClass both for the controller and for the source resource by setting the annotation dns.gardener.cloud/class. The default value for the DNSClass is gardendns.\nNote that if you delegate the DNS management for shoot resources to Gardener via the shoot-dns-service extension, the correct annotation is dns.gardener.cloud/class=garden.\nHere is an example for annotating a service (same as examples/50-service-with-dns.yaml)]:\napiVersion:v1kind:Servicemetadata:annotations:dns.gardener.cloud/dnsnames:echo.my-dns-domain.comdns.gardener.cloud/ttl:\u0026#34;500\u0026#34;# If you are delegating the DNS Management to Gardener, uncomment the following line (see https://gardener.cloud/documentation/guides/administer_shoots/dns_names/)#`dns.gardener.cloud/class`: gardenname:test-servicenamespace:defaultspec:ports:- name:httpport:80protocol:TCPtargetPort:8080sessionAffinity:Nonetype:LoadBalancerThe Model This project provides a flexible model allowing to add DNS source objects and DNS provisioning environments by adding new independent controllers.\nThere is no single DNS controller anymore. The decoupling between the handling of DNS source objects, like ingresses or services, and the provisioning of DNS entries in an external DNS provider like Route53 or CloudDNS is achieved by introducing a new custom resource DNSEntry.\nThese objects can either be explicitly created to request dedicated DNS entries, or they are managed based on other resources like ingresses or services. For the latter dedicated DNS Source Controllers are used. There might be any number of such source controllers. They do not need to know anything about the various DNS environments. Their task is to figure out which DNS entries are required in their realm and manage appropriate DNSEntry objects. From these objects they can also read the provisioning status and report it back to the original source.\nProvisioning of DNS entries in external DNS providers is done by DNS Provisioning Controllers. They don\u0026rsquo;t need to know anything about the various DNS source objects. They watch DNSEntry objects and check whether they are responsible for such an object. If a provisioning controller feels responsible for an entry it manages the corresponding settings in the external DNS environment and reports the provisioning status back to the corresponding DNSEntry object.\nTo do this a provisioning controller is responsible for a dedicated environment (for example Route53). For every such environment the controller uses a dedicated type key. This key is used to look for DNSProvider objects. There might be multiple such objects per environment, specifying the credentials needed to access different external accounts. These accounts are then scanned for DNS zones and domain names they support. This information is then used to dynamically assign DNSEntry objects to dedicated DNSProvider objects. If such an assignment can be done by a provisioning controller then it is responsible for this entry and manages the corresponding entries in the external environment. DNSProvider objects can specify explicit inclusion and exclusion sets of domain names and/or DNS zone identifiers to override the scanning results of the account.\nOwner Identifiers Every DNS Provisioning Controller is responsible for a set of Owner Identifiers. DNS records in an external DNS environment are attached to such an identifier. This is used to identify the records in the DNS environment managed by a dedicated controller (manager). Every controller manager hosting DNS Provisioning Controllers offers an option to specify a default identifier. Additionally there might be dedicated DNSOwner objects that enable or disable additional owner ids.\nEvery DNSEntry object may specify a dedicated owner that is used to tag the records in the DNS environment. A DNS provisioning controller only acts of DNS entries it is responsible for. Other resources in the external DNS environment are not touched at all.\nThis way it is possbible to\n identify records in the external DNS management environment that are managed by the actual controller instance distinguish different DNS source environments sharing the same hosted zones in the external management environment cleanup unused entries, even if the whole resource set is already gone move the responsibility for dedicated sets of DNS entries among different kubernetes clusters or DNS source environments running different DNS Provisioning Controller without loosing the entries during the migration process.  If multiple DNS controller instances have access to the same DNS zones, it is very important, that every instance uses a unique owner identifier! Otherwise the cleanup of stale DNS record will delete entries created by another instance if they use the same identifier.\nDNS Classes Multiple sets of controllers of the DNS ecosystem can run in parallel in a kubernetes cluster working on different object set. They are separated by using different DNS Classes. Adding a DNS class annotation to an object of the DNS ecosytems assigns this object to such a dedicated set of DNS controllers. This way it is possible to maintain clearly separated set of DNS objects in a single kubernetes cluster.\nDNSAnnotation objects DNS source controllers support the creation of DNS entries for potentiialy any kind of resource originally not equipped to describe the generation of DNS entries. This is done by additionally annotations. Nevertheless it might be the case, that those objects are again the result of a generation process, ether by predefined helm starts or by other higher level controllers. It is not necessarily possible to influence those generation steps to additionally generate the deired DNS annotations.\nThe typical mechanis in Kubernetes to handle this is to provide mutating webhooks that enrich the generated objects accordingly. But this mechanism is basically not intended to support dedicated settings for dedicated instances. At least it is very strenous to provide web hooks for every such usecase.\nTherefore the DNS ecosystem provided by this project supports an additional extension mechanism to annotate any kind of object with additional annotations by supported a dedicated resource, the DNSAnnotation.\nThe handling of this resource is done by a dedicated controller, the annotation controller. It caches the annotation settings declared by those objects and makes them accessible for the DNS source controllers.\nThe DNS source controller responsible for a dedicated kind of resource (for example Service reads the object analyses the annotations and then decides what to do with it. Most of the flow is handled by a central library, only some dedicated resource dependent steps are implemented separately by a dedicated source controller. The DNSAnnotationresource slightly extends this flow: After reading the object the library additionally checks for the existence of a DNSAnnotation setting for this object by querying the annotation controller\u0026rsquo;s cache. If found, it adds annotations declared there to the original object prior to the next processing steps. This way, for example whenver a Service without any DNS related annotation is handled by the controller and it finds a matching DNSAnnotation setting, the set of actual annotations is enriched accordingly before the actual processing of the service object is done by the controller.\nThis DNSAnnotation object can be created before or even after the object to be annotated and will implicity cause a reprocessing of the original object by its DNS source controller.\nFor example, the following object enforces a DNS related annotation for the processing of the service object testapp/default by the service DNS source controller:\napiVersion:dns.gardener.cloud/v1alpha1kind:DNSAnnotationmetadata:name:testappspec:resourceRef:kind:ServiceapiVersion:v1name:testappannotations:dns.gardener.cloud/dnsnames:testapp.dns.gardener.clouddns.gardener.cloud/ttl:\u0026#34;500\u0026#34;Using the DNS controller manager The controllers to run can be selected with the --controllers option. Here the following controller groups can be used:\n  dnssources: all DNS Source Controllers. It includes the conrollers\n ingress-dns: handle DNS annotations for the standard kubernetes ingress resource service-dns: handle DNS annotations for the standard kubernetes service resource    dnscontrollers: all DNS Provisioning Controllers. It includes the controllers\n alicloud-dns: aws-route53: azure-dns: google-clouddns: openstack-designate: cloudflare-dns    all: (default) all controllers\n  It is also possible to list dedicated controllers by their name.\nIf a DNS Provisioning Controller is enabled it is important to specify a unique controller identity using the --identifier option. This identifier is stored in the DNS system to identify the DNS entries managed by a dedicated controller. There should never be two DNS controllers with the same identifier running at the same time for the same DNS domains/accounts.\nHere is the complete list of options provided:\nUsage: dns-controller-manager [flags] Flags: --accepted-maintainers string accepted maintainer key(s) for crds --alicloud-dns.cache-dir string Directory to store zone caches (for reload after restart) of controller alicloud-dns --alicloud-dns.cache-ttl int Time-to-live for provider hosted zone cache of controller alicloud-dns (default 120) --alicloud-dns.default.pool.size int Worker pool size for pool default of controller alicloud-dns (default 2) --alicloud-dns.disable-zone-state-caching disable use of cached dns zone state on changes of controller alicloud-dns --alicloud-dns.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller alicloud-dns (default \u0026#34;gardendns\u0026#34;) --alicloud-dns.dns-delay duration delay between two dns reconciliations of controller alicloud-dns (default 10s) --alicloud-dns.dns.pool.resync-period duration Period for resynchronization for pool dns of controller alicloud-dns (default 15m0s) --alicloud-dns.dns.pool.size int Worker pool size for pool dns of controller alicloud-dns (default 1) --alicloud-dns.dry-run just check, don\u0026#39;t modify of controller alicloud-dns --alicloud-dns.identifier string Identifier used to mark DNS entries in DNS system of controller alicloud-dns (default \u0026#34;dnscontroller\u0026#34;) --alicloud-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller alicloud-dns (default 1) --alicloud-dns.pool.resync-period duration Period for resynchronization of controller alicloud-dns --alicloud-dns.pool.size int Worker pool size of controller alicloud-dns --alicloud-dns.providers.pool.resync-period duration Period for resynchronization for pool providers of controller alicloud-dns (default 10m0s) --alicloud-dns.providers.pool.size int Worker pool size for pool providers of controller alicloud-dns (default 2) --alicloud-dns.ratelimiter.burst int number of burst requests for rate limiter of controller alicloud-dns --alicloud-dns.ratelimiter.enabled enables rate limiter for DNS provider requests of controller alicloud-dns --alicloud-dns.ratelimiter.qps int maximum requests/queries per second of controller alicloud-dns --alicloud-dns.reschedule-delay duration reschedule delay after losing provider of controller alicloud-dns (default 2m0s) --alicloud-dns.secrets.pool.size int Worker pool size for pool secrets of controller alicloud-dns (default 2) --alicloud-dns.setup int number of processors for controller setup of controller alicloud-dns (default 10) --alicloud-dns.statistic.pool.size int Worker pool size for pool statistic of controller alicloud-dns (default 1) --alicloud-dns.ttl int Default time-to-live for DNS entries of controller alicloud-dns (default 300) --annotation.default.pool.size int Worker pool size for pool default of controller annotation (default 5) --annotation.pool.size int Worker pool size of controller annotation --annotation.setup int number of processors for controller setup of controller annotation (default 10) --aws-route53.cache-dir string Directory to store zone caches (for reload after restart) of controller aws-route53 --aws-route53.cache-ttl int Time-to-live for provider hosted zone cache of controller aws-route53 (default 120) --aws-route53.default.pool.size int Worker pool size for pool default of controller aws-route53 (default 2) --aws-route53.disable-zone-state-caching disable use of cached dns zone state on changes of controller aws-route53 --aws-route53.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller aws-route53 (default \u0026#34;gardendns\u0026#34;) --aws-route53.dns-delay duration delay between two dns reconciliations of controller aws-route53 (default 10s) --aws-route53.dns.pool.resync-period duration Period for resynchronization for pool dns of controller aws-route53 (default 15m0s) --aws-route53.dns.pool.size int Worker pool size for pool dns of controller aws-route53 (default 1) --aws-route53.dry-run just check, don\u0026#39;t modify of controller aws-route53 --aws-route53.identifier string Identifier used to mark DNS entries in DNS system of controller aws-route53 (default \u0026#34;dnscontroller\u0026#34;) --aws-route53.ownerids.pool.size int Worker pool size for pool ownerids of controller aws-route53 (default 1) --aws-route53.pool.resync-period duration Period for resynchronization of controller aws-route53 --aws-route53.pool.size int Worker pool size of controller aws-route53 --aws-route53.providers.pool.resync-period duration Period for resynchronization for pool providers of controller aws-route53 (default 10m0s) --aws-route53.providers.pool.size int Worker pool size for pool providers of controller aws-route53 (default 2) --aws-route53.ratelimiter.burst int number of burst requests for rate limiter of controller aws-route53 --aws-route53.ratelimiter.enabled enables rate limiter for DNS provider requests of controller aws-route53 --aws-route53.ratelimiter.qps int maximum requests/queries per second of controller aws-route53 --aws-route53.reschedule-delay duration reschedule delay after losing provider of controller aws-route53 (default 2m0s) --aws-route53.secrets.pool.size int Worker pool size for pool secrets of controller aws-route53 (default 2) --aws-route53.setup int number of processors for controller setup of controller aws-route53 (default 10) --aws-route53.statistic.pool.size int Worker pool size for pool statistic of controller aws-route53 (default 1) --aws-route53.ttl int Default time-to-live for DNS entries of controller aws-route53 (default 300) --azure-dns.cache-dir string Directory to store zone caches (for reload after restart) of controller azure-dns --azure-dns.cache-ttl int Time-to-live for provider hosted zone cache of controller azure-dns (default 120) --azure-dns.default.pool.size int Worker pool size for pool default of controller azure-dns (default 2) --azure-dns.disable-zone-state-caching disable use of cached dns zone state on changes of controller azure-dns --azure-dns.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller azure-dns (default \u0026#34;gardendns\u0026#34;) --azure-dns.dns-delay duration delay between two dns reconciliations of controller azure-dns (default 10s) --azure-dns.dns.pool.resync-period duration Period for resynchronization for pool dns of controller azure-dns (default 15m0s) --azure-dns.dns.pool.size int Worker pool size for pool dns of controller azure-dns (default 1) --azure-dns.dry-run just check, don\u0026#39;t modify of controller azure-dns --azure-dns.identifier string Identifier used to mark DNS entries in DNS system of controller azure-dns (default \u0026#34;dnscontroller\u0026#34;) --azure-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller azure-dns (default 1) --azure-dns.pool.resync-period duration Period for resynchronization of controller azure-dns --azure-dns.pool.size int Worker pool size of controller azure-dns --azure-dns.providers.pool.resync-period duration Period for resynchronization for pool providers of controller azure-dns (default 10m0s) --azure-dns.providers.pool.size int Worker pool size for pool providers of controller azure-dns (default 2) --azure-dns.ratelimiter.burst int number of burst requests for rate limiter of controller azure-dns --azure-dns.ratelimiter.enabled enables rate limiter for DNS provider requests of controller azure-dns --azure-dns.ratelimiter.qps int maximum requests/queries per second of controller azure-dns --azure-dns.reschedule-delay duration reschedule delay after losing provider of controller azure-dns (default 2m0s) --azure-dns.secrets.pool.size int Worker pool size for pool secrets of controller azure-dns (default 2) --azure-dns.setup int number of processors for controller setup of controller azure-dns (default 10) --azure-dns.statistic.pool.size int Worker pool size for pool statistic of controller azure-dns (default 1) --azure-dns.ttl int Default time-to-live for DNS entries of controller azure-dns (default 300) --bind-address-http string HTTP server bind address --cache-dir string Directory to store zone caches (for reload after restart) --cache-ttl int Time-to-live for provider hosted zone cache --cloudflare-dns.cache-dir string Directory to store zone caches (for reload after restart) of controller cloudflare-dns --cloudflare-dns.cache-ttl int Time-to-live for provider hosted zone cache of controller cloudflare-dns (default 120) --cloudflare-dns.default.pool.size int Worker pool size for pool default of controller cloudflare-dns (default 2) --cloudflare-dns.disable-zone-state-caching disable use of cached dns zone state on changes of controller cloudflare-dns --cloudflare-dns.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller cloudflare-dns (default \u0026#34;gardendns\u0026#34;) --cloudflare-dns.dns-delay duration delay between two dns reconciliations of controller cloudflare-dns (default 10s) --cloudflare-dns.dns.pool.resync-period duration Period for resynchronization for pool dns of controller cloudflare-dns (default 15m0s) --cloudflare-dns.dns.pool.size int Worker pool size for pool dns of controller cloudflare-dns (default 1) --cloudflare-dns.dry-run just check, don\u0026#39;t modify of controller cloudflare-dns --cloudflare-dns.identifier string Identifier used to mark DNS entries in DNS system of controller cloudflare-dns (default \u0026#34;dnscontroller\u0026#34;) --cloudflare-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller cloudflare-dns (default 1) --cloudflare-dns.pool.resync-period duration Period for resynchronization of controller cloudflare-dns --cloudflare-dns.pool.size int Worker pool size of controller cloudflare-dns --cloudflare-dns.providers.pool.resync-period duration Period for resynchronization for pool providers of controller cloudflare-dns (default 10m0s) --cloudflare-dns.providers.pool.size int Worker pool size for pool providers of controller cloudflare-dns (default 2) --cloudflare-dns.ratelimiter.burst int number of burst requests for rate limiter of controller cloudflare-dns --cloudflare-dns.ratelimiter.enabled enables rate limiter for DNS provider requests of controller cloudflare-dns --cloudflare-dns.ratelimiter.qps int maximum requests/queries per second of controller cloudflare-dns --cloudflare-dns.reschedule-delay duration reschedule delay after losing provider of controller cloudflare-dns (default 2m0s) --cloudflare-dns.secrets.pool.size int Worker pool size for pool secrets of controller cloudflare-dns (default 2) --cloudflare-dns.setup int number of processors for controller setup of controller cloudflare-dns (default 10) --cloudflare-dns.statistic.pool.size int Worker pool size for pool statistic of controller cloudflare-dns (default 1) --cloudflare-dns.ttl int Default time-to-live for DNS entries of controller cloudflare-dns (default 300) --config string config file -c, --controllers string comma separated list of controllers to start (\u0026lt;name\u0026gt;,\u0026lt;group\u0026gt;,all) (default \u0026#34;all\u0026#34;) --cpuprofile string set file for cpu profiling --default.pool.resync-period duration Period for resynchronization for pool default --default.pool.size int Worker pool size for pool default --disable-namespace-restriction disable access restriction for namespace local access only --disable-zone-state-caching disable use of cached dns zone state on changes --dns-class string identifier used to differentiate responsible controllers for entries --dns-delay duration delay between two dns reconciliations --dns-target-class string identifier used to differentiate responsible dns controllers for target entries --dns.pool.resync-period duration Period for resynchronization for pool dns --dns.pool.size int Worker pool size for pool dns --dnsentry-source.default.pool.resync-period duration Period for resynchronization for pool default of controller dnsentry-source (default 2m0s) --dnsentry-source.default.pool.size int Worker pool size for pool default of controller dnsentry-source (default 2) --dnsentry-source.dns-class string identifier used to differentiate responsible controllers for entries of controller dnsentry-source (default \u0026#34;gardendns\u0026#34;) --dnsentry-source.dns-target-class string identifier used to differentiate responsible dns controllers for target entries of controller dnsentry-source --dnsentry-source.exclude-domains stringArray excluded domains of controller dnsentry-source --dnsentry-source.key string selecting key for annotation of controller dnsentry-source --dnsentry-source.pool.resync-period duration Period for resynchronization of controller dnsentry-source --dnsentry-source.pool.size int Worker pool size of controller dnsentry-source --dnsentry-source.target-creator-label-name string label name to store the creator for generated DNS entries of controller dnsentry-source (default \u0026#34;creator\u0026#34;) --dnsentry-source.target-creator-label-value string label value for creator label of controller dnsentry-source --dnsentry-source.target-name-prefix string name prefix in target namespace for cross cluster generation of controller dnsentry-source --dnsentry-source.target-namespace string target namespace for cross cluster generation of controller dnsentry-source --dnsentry-source.target-owner-id string owner id to use for generated DNS entries of controller dnsentry-source --dnsentry-source.target-realms string realm(s) to use for generated DNS entries of controller dnsentry-source --dnsentry-source.target-set-ignore-owners mark generated DNS entries to omit owner based access control of controller dnsentry-source --dnsentry-source.targets.pool.size int Worker pool size for pool targets of controller dnsentry-source (default 2) --dry-run just check, don\u0026#39;t modify --exclude-domains stringArray excluded domains --force-crd-update enforce update of crds even they are unmanaged --google-clouddns.cache-dir string Directory to store zone caches (for reload after restart) of controller google-clouddns --google-clouddns.cache-ttl int Time-to-live for provider hosted zone cache of controller google-clouddns (default 120) --google-clouddns.default.pool.size int Worker pool size for pool default of controller google-clouddns (default 2) --google-clouddns.disable-zone-state-caching disable use of cached dns zone state on changes of controller google-clouddns --google-clouddns.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller google-clouddns (default \u0026#34;gardendns\u0026#34;) --google-clouddns.dns-delay duration delay between two dns reconciliations of controller google-clouddns (default 10s) --google-clouddns.dns.pool.resync-period duration Period for resynchronization for pool dns of controller google-clouddns (default 15m0s) --google-clouddns.dns.pool.size int Worker pool size for pool dns of controller google-clouddns (default 1) --google-clouddns.dry-run just check, don\u0026#39;t modify of controller google-clouddns --google-clouddns.identifier string Identifier used to mark DNS entries in DNS system of controller google-clouddns (default \u0026#34;dnscontroller\u0026#34;) --google-clouddns.ownerids.pool.size int Worker pool size for pool ownerids of controller google-clouddns (default 1) --google-clouddns.pool.resync-period duration Period for resynchronization of controller google-clouddns --google-clouddns.pool.size int Worker pool size of controller google-clouddns --google-clouddns.providers.pool.resync-period duration Period for resynchronization for pool providers of controller google-clouddns (default 10m0s) --google-clouddns.providers.pool.size int Worker pool size for pool providers of controller google-clouddns (default 2) --google-clouddns.ratelimiter.burst int number of burst requests for rate limiter of controller google-clouddns --google-clouddns.ratelimiter.enabled enables rate limiter for DNS provider requests of controller google-clouddns --google-clouddns.ratelimiter.qps int maximum requests/queries per second of controller google-clouddns --google-clouddns.reschedule-delay duration reschedule delay after losing provider of controller google-clouddns (default 2m0s) --google-clouddns.secrets.pool.size int Worker pool size for pool secrets of controller google-clouddns (default 2) --google-clouddns.setup int number of processors for controller setup of controller google-clouddns (default 10) --google-clouddns.statistic.pool.size int Worker pool size for pool statistic of controller google-clouddns (default 1) --google-clouddns.ttl int Default time-to-live for DNS entries of controller google-clouddns (default 300) --grace-period duration inactivity grace period for detecting end of cleanup for shutdown -h, --help help for dns-controller-manager --identifier string Identifier used to mark DNS entries in DNS system --infoblox-dns.cache-dir string Directory to store zone caches (for reload after restart) of controller infoblox-dns --infoblox-dns.cache-ttl int Time-to-live for provider hosted zone cache of controller infoblox-dns (default 120) --infoblox-dns.default.pool.size int Worker pool size for pool default of controller infoblox-dns (default 2) --infoblox-dns.disable-zone-state-caching disable use of cached dns zone state on changes of controller infoblox-dns --infoblox-dns.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller infoblox-dns (default \u0026#34;gardendns\u0026#34;) --infoblox-dns.dns-delay duration delay between two dns reconciliations of controller infoblox-dns (default 10s) --infoblox-dns.dns.pool.resync-period duration Period for resynchronization for pool dns of controller infoblox-dns (default 15m0s) --infoblox-dns.dns.pool.size int Worker pool size for pool dns of controller infoblox-dns (default 1) --infoblox-dns.dry-run just check, don\u0026#39;t modify of controller infoblox-dns --infoblox-dns.identifier string Identifier used to mark DNS entries in DNS system of controller infoblox-dns (default \u0026#34;dnscontroller\u0026#34;) --infoblox-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller infoblox-dns (default 1) --infoblox-dns.pool.resync-period duration Period for resynchronization of controller infoblox-dns --infoblox-dns.pool.size int Worker pool size of controller infoblox-dns --infoblox-dns.providers.pool.resync-period duration Period for resynchronization for pool providers of controller infoblox-dns (default 10m0s) --infoblox-dns.providers.pool.size int Worker pool size for pool providers of controller infoblox-dns (default 2) --infoblox-dns.ratelimiter.burst int number of burst requests for rate limiter of controller infoblox-dns --infoblox-dns.ratelimiter.enabled enables rate limiter for DNS provider requests of controller infoblox-dns --infoblox-dns.ratelimiter.qps int maximum requests/queries per second of controller infoblox-dns --infoblox-dns.reschedule-delay duration reschedule delay after losing provider of controller infoblox-dns (default 2m0s) --infoblox-dns.secrets.pool.size int Worker pool size for pool secrets of controller infoblox-dns (default 2) --infoblox-dns.setup int number of processors for controller setup of controller infoblox-dns (default 10) --infoblox-dns.statistic.pool.size int Worker pool size for pool statistic of controller infoblox-dns (default 1) --infoblox-dns.ttl int Default time-to-live for DNS entries of controller infoblox-dns (default 300) --ingress-dns.default.pool.resync-period duration Period for resynchronization for pool default of controller ingress-dns (default 2m0s) --ingress-dns.default.pool.size int Worker pool size for pool default of controller ingress-dns (default 2) --ingress-dns.dns-class string identifier used to differentiate responsible controllers for entries of controller ingress-dns (default \u0026#34;gardendns\u0026#34;) --ingress-dns.dns-target-class string identifier used to differentiate responsible dns controllers for target entries of controller ingress-dns --ingress-dns.exclude-domains stringArray excluded domains of controller ingress-dns --ingress-dns.key string selecting key for annotation of controller ingress-dns --ingress-dns.pool.resync-period duration Period for resynchronization of controller ingress-dns --ingress-dns.pool.size int Worker pool size of controller ingress-dns --ingress-dns.target-creator-label-name string label name to store the creator for generated DNS entries of controller ingress-dns (default \u0026#34;creator\u0026#34;) --ingress-dns.target-creator-label-value string label value for creator label of controller ingress-dns --ingress-dns.target-name-prefix string name prefix in target namespace for cross cluster generation of controller ingress-dns --ingress-dns.target-namespace string target namespace for cross cluster generation of controller ingress-dns --ingress-dns.target-owner-id string owner id to use for generated DNS entries of controller ingress-dns --ingress-dns.target-realms string realm(s) to use for generated DNS entries of controller ingress-dns --ingress-dns.target-set-ignore-owners mark generated DNS entries to omit owner based access control of controller ingress-dns --ingress-dns.targets.pool.size int Worker pool size for pool targets of controller ingress-dns (default 2) --key string selecting key for annotation --kubeconfig string default cluster access --kubeconfig.disable-deploy-crds disable deployment of required crds for cluster default --kubeconfig.id string id for cluster default --kubeconfig.migration-ids string migration id for cluster default --lease-duration duration lease duration (default 15s) --lease-name string name for lease object --lease-renew-deadline duration lease renew deadline (default 10s) --lease-retry-period duration lease retry period (default 2s) -D, --log-level string logrus log level --maintainer string maintainer key for crds (default \u0026#34;dns-controller-manager\u0026#34;) --name string name used for controller manager (default \u0026#34;dns-controller-manager\u0026#34;) --namespace string namespace for lease (default \u0026#34;kube-system\u0026#34;) -n, --namespace-local-access-only enable access restriction for namespace local access only (deprecated) --omit-lease omit lease for development --openstack-designate.cache-dir string Directory to store zone caches (for reload after restart) of controller openstack-designate --openstack-designate.cache-ttl int Time-to-live for provider hosted zone cache of controller openstack-designate (default 120) --openstack-designate.default.pool.size int Worker pool size for pool default of controller openstack-designate (default 2) --openstack-designate.disable-zone-state-caching disable use of cached dns zone state on changes of controller openstack-designate --openstack-designate.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller openstack-designate (default \u0026#34;gardendns\u0026#34;) --openstack-designate.dns-delay duration delay between two dns reconciliations of controller openstack-designate (default 10s) --openstack-designate.dns.pool.resync-period duration Period for resynchronization for pool dns of controller openstack-designate (default 15m0s) --openstack-designate.dns.pool.size int Worker pool size for pool dns of controller openstack-designate (default 1) --openstack-designate.dry-run just check, don\u0026#39;t modify of controller openstack-designate --openstack-designate.identifier string Identifier used to mark DNS entries in DNS system of controller openstack-designate (default \u0026#34;dnscontroller\u0026#34;) --openstack-designate.ownerids.pool.size int Worker pool size for pool ownerids of controller openstack-designate (default 1) --openstack-designate.pool.resync-period duration Period for resynchronization of controller openstack-designate --openstack-designate.pool.size int Worker pool size of controller openstack-designate --openstack-designate.providers.pool.resync-period duration Period for resynchronization for pool providers of controller openstack-designate (default 10m0s) --openstack-designate.providers.pool.size int Worker pool size for pool providers of controller openstack-designate (default 2) --openstack-designate.ratelimiter.burst int number of burst requests for rate limiter of controller openstack-designate --openstack-designate.ratelimiter.enabled enables rate limiter for DNS provider requests of controller openstack-designate --openstack-designate.ratelimiter.qps int maximum requests/queries per second of controller openstack-designate --openstack-designate.reschedule-delay duration reschedule delay after losing provider of controller openstack-designate (default 2m0s) --openstack-designate.secrets.pool.size int Worker pool size for pool secrets of controller openstack-designate (default 2) --openstack-designate.setup int number of processors for controller setup of controller openstack-designate (default 10) --openstack-designate.statistic.pool.size int Worker pool size for pool statistic of controller openstack-designate (default 1) --openstack-designate.ttl int Default time-to-live for DNS entries of controller openstack-designate (default 300) --ownerids.pool.size int Worker pool size for pool ownerids --plugin-file string directory containing go plugins --pool.resync-period duration Period for resynchronization --pool.size int Worker pool size --providers string cluster to look for provider objects --providers.disable-deploy-crds disable deployment of required crds for cluster provider --providers.id string id for cluster provider --providers.migration-ids string migration id for cluster provider --providers.pool.resync-period duration Period for resynchronization for pool providers --providers.pool.size int Worker pool size for pool providers --ratelimiter.burst int number of burst requests for rate limiter --ratelimiter.enabled enables rate limiter for DNS provider requests --ratelimiter.qps int maximum requests/queries per second --reschedule-delay duration reschedule delay after losing provider --secrets.pool.size int Worker pool size for pool secrets --server-port-http int HTTP server port (serving /healthz, /metrics, ...) --service-dns.default.pool.resync-period duration Period for resynchronization for pool default of controller service-dns (default 2m0s) --service-dns.default.pool.size int Worker pool size for pool default of controller service-dns (default 2) --service-dns.dns-class string identifier used to differentiate responsible controllers for entries of controller service-dns (default \u0026#34;gardendns\u0026#34;) --service-dns.dns-target-class string identifier used to differentiate responsible dns controllers for target entries of controller service-dns --service-dns.exclude-domains stringArray excluded domains of controller service-dns --service-dns.key string selecting key for annotation of controller service-dns --service-dns.pool.resync-period duration Period for resynchronization of controller service-dns --service-dns.pool.size int Worker pool size of controller service-dns --service-dns.target-creator-label-name string label name to store the creator for generated DNS entries of controller service-dns (default \u0026#34;creator\u0026#34;) --service-dns.target-creator-label-value string label value for creator label of controller service-dns --service-dns.target-name-prefix string name prefix in target namespace for cross cluster generation of controller service-dns --service-dns.target-namespace string target namespace for cross cluster generation of controller service-dns --service-dns.target-owner-id string owner id to use for generated DNS entries of controller service-dns --service-dns.target-realms string realm(s) to use for generated DNS entries of controller service-dns --service-dns.target-set-ignore-owners mark generated DNS entries to omit owner based access control of controller service-dns --service-dns.targets.pool.size int Worker pool size for pool targets of controller service-dns (default 2) --setup int number of processors for controller setup --statistic.pool.size int Worker pool size for pool statistic --target string target cluster for dns requests --target-creator-label-name string label name to store the creator for generated DNS entries --target-creator-label-value string label value for creator label --target-name-prefix string name prefix in target namespace for cross cluster generation --target-namespace string target namespace for cross cluster generation --target-owner-id string owner id to use for generated DNS entries --target-realms string realm(s) to use for generated DNS entries --target-set-ignore-owners mark generated DNS entries to omit owner based access control --target.disable-deploy-crds disable deployment of required crds for cluster target --target.id string id for cluster target --target.migration-ids string migration id for cluster target --targets.pool.size int Worker pool size for pool targets --ttl int Default time-to-live for DNS entries (defaults to 300s). Defines how long the record is kept in cache by DNS servers or resolvers. --version version for dns-controller-manager Extensions This project can also be used as library to implement own source and provisioning controllers.\nHow to implement Source Controllers Based on the provided source controller library a source controller must implement the source.DNSSource interface and provide an appropriate creator function.\nA source controller can be implemented following this example:\npackage service import ( \u0026#34;github.com/gardener/controller-manager-library/pkg/resources\u0026#34; \u0026#34;github.com/gardener/external-dns-management/pkg/dns/source\u0026#34; ) var _MAIN_RESOURCE = resources.NewGroupKind(\u0026#34;core\u0026#34;, \u0026#34;Service\u0026#34;) func init() { source.DNSSourceController(source.NewDNSSouceTypeForExtractor(\u0026#34;service-dns\u0026#34;, _MAIN_RESOURCE, GetTargets),nil). FinalizerDomain(\u0026#34;dns.gardener.cloud\u0026#34;). MustRegister(source.CONTROLLER_GROUP_DNS_SOURCES) } Complete examples can be found in the sub packages of pkg/controller/source.\nHow to implement Provisioning Controllers Provisioning controllers can be implemented based on the provisioning controller library in this repository and must implement the provider.DNSHandlerFactory interface. This factory returns implementations of the provider.DNSHandler interface that does the effective work for a dedicated set of hosted zones.\nThese factories can be embedded into a final controller manager (the runnable instance) in several ways:\n The factory can be used to create a dedicated controller. This controller can then be embedded into a controller manager, either in its own controller manger or together with other controllers. The factory can be added to a compound factory, able to handle multiple infrastructures. This one can then be used to create a dedicated controller, again.  Embedding a Factory into a Controller A provisioning controller can be implemented following this example:\npackage controller import ( \u0026#34;github.com/gardener/external-dns-management/pkg/dns/provider\u0026#34; ) const CONTROLLER_NAME = \u0026#34;route53-dns-controller\u0026#34; func init() { provider.DNSController(CONTROLLER_NAME, \u0026amp;Factory{}). FinalizerDomain(\u0026#34;dns.gardener.cloud\u0026#34;). MustRegister(provider.CONTROLLER_GROUP_DNS_CONTROLLERS) } This controller can be embedded into a controller manager just by using an anonymous import of the controller package in the main package of a dedicated controller manager.\nComplete examples are available in the sub packages of pkg/controller/provider. They also show a typical set of implementation structures that help to structure the implementation of such controllers.\nThe provider implemented in this project always follow the same structure:\n the provider package contains the provider code the factory source file registers the factory at a default compound factory it contains a sub package controller, which contains the embedding of the factory into a dedicated controller  Embedding a Factory into a Compound Factory A provisioning controller based on a Compound Factory can be extended by a new provider factory by registering this factory at the compound factory. This could be done, for example, by using the default compound factory provided in package pkg/controller/provider/compound as shown here, where NewHandler is a function creating a dedicated handler for a dedicated provider type:\npackage aws import ( \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/compound\u0026#34; \u0026#34;github.com/gardener/external-dns-management/pkg/dns/provider\u0026#34; ) const TYPE_CODE = \u0026#34;aws-route53\u0026#34; var Factory = provider.NewDNSHandlerFactory(TYPE_CODE, NewHandler) func init() { compound.MustRegister(Factory) } The compound factory is then again embedded into a provisioning controller as shown in the previous section (see the controllersub package).\nSetting Up a Controller Manager One or multiple controller packages can be bundled into a controller manager, by implementing a main package like this:\npackage main import ( \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026#34; _ \u0026#34;github.com/\u0026lt;your controller package\u0026gt;\u0026#34; ... ) func main() { controllermanager.Start(\u0026#34;my-dns-controller-manager\u0026#34;, \u0026#34;dns controller manager\u0026#34;, \u0026#34;some description\u0026#34;) } Using the standard Compound Provisioning Controller If the standard Compound Provisioning Controller should be used it is required to additionally add the anonymous imports for the providers intended to be embedded into the compound factory like this:\n Example Coding package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/compound/controller\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/\u0026lt;your provider\u0026gt;\u0026#34; ... ) func main() { controllermanager.Start(\u0026#34;dns-controller-manager\u0026#34;, \u0026#34;dns controller manager\u0026#34;, \u0026#34;nothing\u0026#34;) }  Multiple Cluster Support The controller implementations provided in this project are prepared to work with multiple clusters by using the features of the used controller manager library.\nThe DNS Source Controllers support two clusters:\n the default cluster is used to scan for source objects the logical cluster target is used to maintain the DNSEnry objects.  The DNS Provisioning Controllers also support two clusters:\n the default cluster is used to scan for DNSEntry objects. It is mapped to the logical cluster target the logical cluster provider is used to look to the DNSProvider objects and their related secrets.  If those controller types should be combined in a single controller manager, it can be configured to support three potential clusters with the source objects, the one for the entry objects and the one with provider objects using cluster mappings.\nThis is shown in a complete example using the dns source controllers, the compound provisioning controller configured to support all the included DNS provider type factories:\n Example Coding package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager/cluster\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager/controller\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager/controller/mappings\u0026#34; dnsprovider \u0026#34;github.com/gardener/external-dns-management/pkg/dns/provider\u0026#34; dnssource \u0026#34;github.com/gardener/external-dns-management/pkg/dns/source\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/compound/controller\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/alicloud\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/aws\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/azure\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/google\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/openstack\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/cloudflare\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/source/ingress\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/source/service\u0026#34; ) func init() { // target cluster already defined in dns source controller package  cluster.Configure( dnsprovider.PROVIDER_CLUSTER, \u0026#34;providers\u0026#34;, \u0026#34;cluster to look for provider objects\u0026#34;, ).Fallback(dnssource.TARGET_CLUSTER) mappings.ForControllerGroup(dnsprovider.CONTROLLER_GROUP_DNS_CONTROLLERS). Map(controller.CLUSTER_MAIN, dnssource.TARGET_CLUSTER).MustRegister() } func main() { controllermanager.Start(\u0026#34;dns-controller-manager\u0026#34;, \u0026#34;dns controller manager\u0026#34;, \u0026#34;nothing\u0026#34;) }  Those clusters can the be separated by registering their names together with command line option names. These can be used to specify different kubeconfig files for those clusters.\nBy default all logical clusters are mapped to the default physical cluster specified via --kubeconfig or default cluster access.\nIf multiple physical clusters are defined they can be specified by a corresponding cluster option defining the kubeconfig file used to access this cluster. If no such option is specified the default is used.\nTherefore, even if the configuration is prepared for multiple clusters, such a controller manager can easily work on a single cluster if no special options are given on the command line.\nWhy not using the community external-dns solution? Some of the reasons are context-specific, i.e. relate to Gardener\u0026rsquo;s highly dynamic requirements.\n Custom resource for DNS entries  DNS entries are explicitly specified as custom resources. As an important side effect, each DNS entry provides an own status. Simply by querying the Kubernetes API, a client can check if a requested DNS entry has been successfully added to the DNS backend, or if an update has already been deployed, or if not to reason about the cause. It also opens for easy extensibility, as DNS entries can be created directly via the Kubernetes API. And it simplifies Day 2 operations, e.g. automatic cleanup of unused entries if a DNS provider is deleted.\nManagement of multiple DNS providers  The Gardener DNS controller uses a custom resource DNSProvider to dynamically manage the backend DNS services. While with external-dns you have to specify the single provider during startup, in the Gardener DNS controller you can add/update/delete providers during runtime with different credentials and/or backends. This is important for a multi-tenant environment as in Gardener, where users can bring their own accounts.\nA DNS provider can also restrict its actions on subset of the DNS domains (includes and excludes) for which the credentials are capable to edit.\nEach provider can define a separate owner identifier, to differentiate DNS entries in the same DNS zone from different providers.\nMulti cluster support  The Gardener DNS controller distinguish three different logical Kubernetes clusters: Source cluster, target cluster and runtime cluster. The source cluster is monitored by the DNS source controllers for annotations on ingress and service resources. These controllers then create DNS entries in the target cluster. DNS entries in the target cluster are then reconciliated/synchronized with the corresponding DNS backend service by the provider controller. The runtime cluster is the cluster the DNS controller runs on. For example, this enables needed flexibility in the Gardener deployment. The DNS controller runs on the seed cluster. This is also the target cluster. DNS providers and entries resources are created in the corresponding namespace of the shoot control plane, while the source cluster is the shoot cluster itself.\nOptimizations for handling hundreds of DNS entries  Some DNS backend services are restricted on the API calls per second (e.g. the AWS Route 53 API). To manage hundreds of DNS entries it is important to minimize the number of API calls. The Gardener DNS controller heavily makes usage of caches and batch processing for this reason.\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/networking/dns-managment/","title":"DNS Management","tags":[],"description":"","content":"External DNS Management The main artefact of this project is the DNS controller manager for managing DNS records, also nicknamed as the Gardener \u0026ldquo;DNS Controller\u0026rdquo;.\nIt contains provisioning controllers for creating DNS records in one of the DNS cloud services\n Amazon Route53, Google CloudDNS, AliCloud DNS, Azure DNS, OpenStack Designate, Cloudflare DNS, Infoblox,  and source controllers for services and ingresses to create DNS entries by annotations.\nThe configuration for the external DNS service is specified in a custom resource DNSProvider. Multiple DNSProvider can be used simultaneously and changed without restarting the DNS controller.\nDNS records are either created directly for a corresponding custom resource DNSEntry or by annotating a service or ingress.\nFor a detailed explanation of the model, see section The Model.\nFor extending or adapting this project with your own source or provisioning controllers, see section Extensions\nQuick start To install the DNS controller manager in your Kubernetes cluster, follow these steps.\n  Prerequisites\n  Check out or download the project to get a copy of the Helm charts. It is recommended to check out the tag of the last release, so that Helm values reference the newest released container image for the deployment.\n  Make sure, that you have installed Helm client (helm) locally and Helm server (tiller) on the Kubernetes cluster. See e.g. Helm installation for more details.\n    Install the DNS controller manager\nAs multiple Gardener DNS controllers can act on the same DNS Hosted Zone concurrently, each instance needs an owner identifier. Therefore choose an identifier sufficiently unique across these instances.\nThen install the DNS controller manager with\nhelm install charts/external-dns-management --name dns-controller --namespace=\u0026lt;my-namespace\u0026gt; --set configuration.identifier=\u0026lt;my-identifier\u0026gt; This will use the default configuration with all source and provisioning controllers enabled. The complete set of configuration variables can be found in charts/external-dns-management/values.yaml. Their meaning is explained by their corresponding command line options in section Using the DNS controller manager\nBy default, the DNS controller looks for custom resources in all namespaces. The choosen namespace is only relevant for the deployment itself.\n  Create a DNSProvider\nTo specify a DNS provider, you need to create a custom resource DNSProvider and a secret containing the credentials for your account at the provider. E.g. if you want to use AWS Route53, create a secret and provider with\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: aws-credentials namespace: default type: Opaque data: # replace \u0026#39;...\u0026#39; with values encoded as base64 # see https://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html AWS_ACCESS_KEY_ID: ... AWS_SECRET_ACCESS_KEY: ... EOF and\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: dns.gardener.cloud/v1alpha1 kind: DNSProvider metadata: name: aws namespace: default spec: type: aws-route53 secretRef: name: aws-credentials domains: include: # this must be replaced with a (sub)domain of the hosted zone - my.own.domain.com EOF Check the successful creation with\nkubectl get dnspr You should see something like\nNAME TYPE STATUS AGE aws aws-route53 Ready 12s   Create a DNSEntry\nCreate an DNS entry with\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: dns.gardener.cloud/v1alpha1 kind: DNSEntry metadata: name: mydnsentry namespace: default spec: dnsName: \u0026#34;myentry.my-own-domain.com\u0026#34; ttl: 600 targets: - 1.2.3.4 EOF Check the status of the DNS entry with\nkubectl get dnsentry You should see something like\nNAME DNS TYPE PROVIDER STATUS AGE mydnsentry myentry.my-own-domain.com aws-route53 default/aws Ready 24s As soon as the status of the entry is Ready, the provider has accepted the new DNS record. Depending on the provider and your DNS settings and cache, it may take up to a few minutes before the domain name can be resolved.\n  Wait for/check DNS record\nTo check the DNS resolution, use nslookup or dig.\nnslookup myentry.my-own-domain.com or with dig\n# or with dig dig +short myentry.my-own-domain.com Depending on your network settings, you may get a successful response faster using a public DNS server (e.g. 8.8.8.8, 8.8.4.4, or 1.1.1.1)\ndig @8.8.8.8 +short myentry.my-own-domain.com   For more examples about the custom resources and the annotations for services and ingresses see the examples directory.\nAutomatic creation of DNS entries for services and ingresses Using the source controllers, it is also possible to create DNS entries for services (of type LoadBalancer) and ingresses automatically. The resources only need to be annotated with some special values. In this case ensure that the source controllers are enabled on startup of the DNS controller manager, i.e. the value of the command line option --controllers must contain dnscontrollers or equal to all. The DNS source controllers watch resources on the default cluster and create DNS entries on the target cluster. As there can be multiple controllers active on the same cluster, you may need to set the correct DNSClass both for the controller and for the source resource by setting the annotation dns.gardener.cloud/class. The default value for the DNSClass is gardendns.\nNote that if you delegate the DNS management for shoot resources to Gardener via the shoot-dns-service extension, the correct annotation is dns.gardener.cloud/class=garden.\nHere is an example for annotating a service (same as examples/50-service-with-dns.yaml)]:\napiVersion:v1kind:Servicemetadata:annotations:dns.gardener.cloud/dnsnames:echo.my-dns-domain.comdns.gardener.cloud/ttl:\u0026#34;500\u0026#34;# If you are delegating the DNS Management to Gardener, uncomment the following line (see https://gardener.cloud/documentation/guides/administer_shoots/dns_names/)#`dns.gardener.cloud/class`: gardenname:test-servicenamespace:defaultspec:ports:- name:httpport:80protocol:TCPtargetPort:8080sessionAffinity:Nonetype:LoadBalancerThe Model This project provides a flexible model allowing to add DNS source objects and DNS provisioning environments by adding new independent controllers.\nThere is no single DNS controller anymore. The decoupling between the handling of DNS source objects, like ingresses or services, and the provisioning of DNS entries in an external DNS provider like Route53 or CloudDNS is achieved by introducing a new custom resource DNSEntry.\nThese objects can either be explicitly created to request dedicated DNS entries, or they are managed based on other resources like ingresses or services. For the latter dedicated DNS Source Controllers are used. There might be any number of such source controllers. They do not need to know anything about the various DNS environments. Their task is to figure out which DNS entries are required in their realm and manage appropriate DNSEntry objects. From these objects they can also read the provisioning status and report it back to the original source.\nProvisioning of DNS entries in external DNS providers is done by DNS Provisioning Controllers. They don\u0026rsquo;t need to know anything about the various DNS source objects. They watch DNSEntry objects and check whether they are responsible for such an object. If a provisioning controller feels responsible for an entry it manages the corresponding settings in the external DNS environment and reports the provisioning status back to the corresponding DNSEntry object.\nTo do this a provisioning controller is responsible for a dedicated environment (for example Route53). For every such environment the controller uses a dedicated type key. This key is used to look for DNSProvider objects. There might be multiple such objects per environment, specifying the credentials needed to access different external accounts. These accounts are then scanned for DNS zones and domain names they support. This information is then used to dynamically assign DNSEntry objects to dedicated DNSProvider objects. If such an assignment can be done by a provisioning controller then it is responsible for this entry and manages the corresponding entries in the external environment. DNSProvider objects can specify explicit inclusion and exclusion sets of domain names and/or DNS zone identifiers to override the scanning results of the account.\nOwner Identifiers Every DNS Provisioning Controller is responsible for a set of Owner Identifiers. DNS records in an external DNS environment are attached to such an identifier. This is used to identify the records in the DNS environment managed by a dedicated controller (manager). Every controller manager hosting DNS Provisioning Controllers offers an option to specify a default identifier. Additionally there might be dedicated DNSOwner objects that enable or disable additional owner ids.\nEvery DNSEntry object may specify a dedicated owner that is used to tag the records in the DNS environment. A DNS provisioning controller only acts of DNS entries it is responsible for. Other resources in the external DNS environment are not touched at all.\nThis way it is possbible to\n identify records in the external DNS management environment that are managed by the actual controller instance distinguish different DNS source environments sharing the same hosted zones in the external management environment cleanup unused entries, even if the whole resource set is already gone move the responsibility for dedicated sets of DNS entries among different kubernetes clusters or DNS source environments running different DNS Provisioning Controller without loosing the entries during the migration process.  If multiple DNS controller instances have access to the same DNS zones, it is very important, that every instance uses a unique owner identifier! Otherwise the cleanup of stale DNS record will delete entries created by another instance if they use the same identifier.\nDNS Classes Multiple sets of controllers of the DNS ecosystem can run in parallel in a kubernetes cluster working on different object set. They are separated by using different DNS Classes. Adding a DNS class annotation to an object of the DNS ecosytems assigns this object to such a dedicated set of DNS controllers. This way it is possible to maintain clearly separated set of DNS objects in a single kubernetes cluster.\nDNSAnnotation objects DNS source controllers support the creation of DNS entries for potentiialy any kind of resource originally not equipped to describe the generation of DNS entries. This is done by additionally annotations. Nevertheless it might be the case, that those objects are again the result of a generation process, ether by predefined helm starts or by other higher level controllers. It is not necessarily possible to influence those generation steps to additionally generate the deired DNS annotations.\nThe typical mechanis in Kubernetes to handle this is to provide mutating webhooks that enrich the generated objects accordingly. But this mechanism is basically not intended to support dedicated settings for dedicated instances. At least it is very strenous to provide web hooks for every such usecase.\nTherefore the DNS ecosystem provided by this project supports an additional extension mechanism to annotate any kind of object with additional annotations by supported a dedicated resource, the DNSAnnotation.\nThe handling of this resource is done by a dedicated controller, the annotation controller. It caches the annotation settings declared by those objects and makes them accessible for the DNS source controllers.\nThe DNS source controller responsible for a dedicated kind of resource (for example Service reads the object analyses the annotations and then decides what to do with it. Most of the flow is handled by a central library, only some dedicated resource dependent steps are implemented separately by a dedicated source controller. The DNSAnnotationresource slightly extends this flow: After reading the object the library additionally checks for the existence of a DNSAnnotation setting for this object by querying the annotation controller\u0026rsquo;s cache. If found, it adds annotations declared there to the original object prior to the next processing steps. This way, for example whenver a Service without any DNS related annotation is handled by the controller and it finds a matching DNSAnnotation setting, the set of actual annotations is enriched accordingly before the actual processing of the service object is done by the controller.\nThis DNSAnnotation object can be created before or even after the object to be annotated and will implicity cause a reprocessing of the original object by its DNS source controller.\nFor example, the following object enforces a DNS related annotation for the processing of the service object testapp/default by the service DNS source controller:\napiVersion:dns.gardener.cloud/v1alpha1kind:DNSAnnotationmetadata:name:testappspec:resourceRef:kind:ServiceapiVersion:v1name:testappannotations:dns.gardener.cloud/dnsnames:testapp.dns.gardener.clouddns.gardener.cloud/ttl:\u0026#34;500\u0026#34;Using the DNS controller manager The controllers to run can be selected with the --controllers option. Here the following controller groups can be used:\n  dnssources: all DNS Source Controllers. It includes the conrollers\n ingress-dns: handle DNS annotations for the standard kubernetes ingress resource service-dns: handle DNS annotations for the standard kubernetes service resource    dnscontrollers: all DNS Provisioning Controllers. It includes the controllers\n alicloud-dns: aws-route53: azure-dns: google-clouddns: openstack-designate: cloudflare-dns    all: (default) all controllers\n  It is also possible to list dedicated controllers by their name.\nIf a DNS Provisioning Controller is enabled it is important to specify a unique controller identity using the --identifier option. This identifier is stored in the DNS system to identify the DNS entries managed by a dedicated controller. There should never be two DNS controllers with the same identifier running at the same time for the same DNS domains/accounts.\nHere is the complete list of options provided:\nUsage: dns-controller-manager [flags] Flags: --accepted-maintainers string accepted maintainer key(s) for crds --alicloud-dns.cache-dir string Directory to store zone caches (for reload after restart) of controller alicloud-dns --alicloud-dns.cache-ttl int Time-to-live for provider hosted zone cache of controller alicloud-dns (default 120) --alicloud-dns.default.pool.size int Worker pool size for pool default of controller alicloud-dns (default 2) --alicloud-dns.disable-zone-state-caching disable use of cached dns zone state on changes of controller alicloud-dns --alicloud-dns.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller alicloud-dns (default \u0026#34;gardendns\u0026#34;) --alicloud-dns.dns-delay duration delay between two dns reconciliations of controller alicloud-dns (default 10s) --alicloud-dns.dns.pool.resync-period duration Period for resynchronization for pool dns of controller alicloud-dns (default 15m0s) --alicloud-dns.dns.pool.size int Worker pool size for pool dns of controller alicloud-dns (default 1) --alicloud-dns.dry-run just check, don\u0026#39;t modify of controller alicloud-dns --alicloud-dns.identifier string Identifier used to mark DNS entries in DNS system of controller alicloud-dns (default \u0026#34;dnscontroller\u0026#34;) --alicloud-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller alicloud-dns (default 1) --alicloud-dns.pool.resync-period duration Period for resynchronization of controller alicloud-dns --alicloud-dns.pool.size int Worker pool size of controller alicloud-dns --alicloud-dns.providers.pool.resync-period duration Period for resynchronization for pool providers of controller alicloud-dns (default 10m0s) --alicloud-dns.providers.pool.size int Worker pool size for pool providers of controller alicloud-dns (default 2) --alicloud-dns.ratelimiter.burst int number of burst requests for rate limiter of controller alicloud-dns --alicloud-dns.ratelimiter.enabled enables rate limiter for DNS provider requests of controller alicloud-dns --alicloud-dns.ratelimiter.qps int maximum requests/queries per second of controller alicloud-dns --alicloud-dns.reschedule-delay duration reschedule delay after losing provider of controller alicloud-dns (default 2m0s) --alicloud-dns.secrets.pool.size int Worker pool size for pool secrets of controller alicloud-dns (default 2) --alicloud-dns.setup int number of processors for controller setup of controller alicloud-dns (default 10) --alicloud-dns.statistic.pool.size int Worker pool size for pool statistic of controller alicloud-dns (default 1) --alicloud-dns.ttl int Default time-to-live for DNS entries of controller alicloud-dns (default 300) --annotation.default.pool.size int Worker pool size for pool default of controller annotation (default 5) --annotation.pool.size int Worker pool size of controller annotation --annotation.setup int number of processors for controller setup of controller annotation (default 10) --aws-route53.cache-dir string Directory to store zone caches (for reload after restart) of controller aws-route53 --aws-route53.cache-ttl int Time-to-live for provider hosted zone cache of controller aws-route53 (default 120) --aws-route53.default.pool.size int Worker pool size for pool default of controller aws-route53 (default 2) --aws-route53.disable-zone-state-caching disable use of cached dns zone state on changes of controller aws-route53 --aws-route53.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller aws-route53 (default \u0026#34;gardendns\u0026#34;) --aws-route53.dns-delay duration delay between two dns reconciliations of controller aws-route53 (default 10s) --aws-route53.dns.pool.resync-period duration Period for resynchronization for pool dns of controller aws-route53 (default 15m0s) --aws-route53.dns.pool.size int Worker pool size for pool dns of controller aws-route53 (default 1) --aws-route53.dry-run just check, don\u0026#39;t modify of controller aws-route53 --aws-route53.identifier string Identifier used to mark DNS entries in DNS system of controller aws-route53 (default \u0026#34;dnscontroller\u0026#34;) --aws-route53.ownerids.pool.size int Worker pool size for pool ownerids of controller aws-route53 (default 1) --aws-route53.pool.resync-period duration Period for resynchronization of controller aws-route53 --aws-route53.pool.size int Worker pool size of controller aws-route53 --aws-route53.providers.pool.resync-period duration Period for resynchronization for pool providers of controller aws-route53 (default 10m0s) --aws-route53.providers.pool.size int Worker pool size for pool providers of controller aws-route53 (default 2) --aws-route53.ratelimiter.burst int number of burst requests for rate limiter of controller aws-route53 --aws-route53.ratelimiter.enabled enables rate limiter for DNS provider requests of controller aws-route53 --aws-route53.ratelimiter.qps int maximum requests/queries per second of controller aws-route53 --aws-route53.reschedule-delay duration reschedule delay after losing provider of controller aws-route53 (default 2m0s) --aws-route53.secrets.pool.size int Worker pool size for pool secrets of controller aws-route53 (default 2) --aws-route53.setup int number of processors for controller setup of controller aws-route53 (default 10) --aws-route53.statistic.pool.size int Worker pool size for pool statistic of controller aws-route53 (default 1) --aws-route53.ttl int Default time-to-live for DNS entries of controller aws-route53 (default 300) --azure-dns.cache-dir string Directory to store zone caches (for reload after restart) of controller azure-dns --azure-dns.cache-ttl int Time-to-live for provider hosted zone cache of controller azure-dns (default 120) --azure-dns.default.pool.size int Worker pool size for pool default of controller azure-dns (default 2) --azure-dns.disable-zone-state-caching disable use of cached dns zone state on changes of controller azure-dns --azure-dns.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller azure-dns (default \u0026#34;gardendns\u0026#34;) --azure-dns.dns-delay duration delay between two dns reconciliations of controller azure-dns (default 10s) --azure-dns.dns.pool.resync-period duration Period for resynchronization for pool dns of controller azure-dns (default 15m0s) --azure-dns.dns.pool.size int Worker pool size for pool dns of controller azure-dns (default 1) --azure-dns.dry-run just check, don\u0026#39;t modify of controller azure-dns --azure-dns.identifier string Identifier used to mark DNS entries in DNS system of controller azure-dns (default \u0026#34;dnscontroller\u0026#34;) --azure-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller azure-dns (default 1) --azure-dns.pool.resync-period duration Period for resynchronization of controller azure-dns --azure-dns.pool.size int Worker pool size of controller azure-dns --azure-dns.providers.pool.resync-period duration Period for resynchronization for pool providers of controller azure-dns (default 10m0s) --azure-dns.providers.pool.size int Worker pool size for pool providers of controller azure-dns (default 2) --azure-dns.ratelimiter.burst int number of burst requests for rate limiter of controller azure-dns --azure-dns.ratelimiter.enabled enables rate limiter for DNS provider requests of controller azure-dns --azure-dns.ratelimiter.qps int maximum requests/queries per second of controller azure-dns --azure-dns.reschedule-delay duration reschedule delay after losing provider of controller azure-dns (default 2m0s) --azure-dns.secrets.pool.size int Worker pool size for pool secrets of controller azure-dns (default 2) --azure-dns.setup int number of processors for controller setup of controller azure-dns (default 10) --azure-dns.statistic.pool.size int Worker pool size for pool statistic of controller azure-dns (default 1) --azure-dns.ttl int Default time-to-live for DNS entries of controller azure-dns (default 300) --bind-address-http string HTTP server bind address --cache-dir string Directory to store zone caches (for reload after restart) --cache-ttl int Time-to-live for provider hosted zone cache --cloudflare-dns.cache-dir string Directory to store zone caches (for reload after restart) of controller cloudflare-dns --cloudflare-dns.cache-ttl int Time-to-live for provider hosted zone cache of controller cloudflare-dns (default 120) --cloudflare-dns.default.pool.size int Worker pool size for pool default of controller cloudflare-dns (default 2) --cloudflare-dns.disable-zone-state-caching disable use of cached dns zone state on changes of controller cloudflare-dns --cloudflare-dns.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller cloudflare-dns (default \u0026#34;gardendns\u0026#34;) --cloudflare-dns.dns-delay duration delay between two dns reconciliations of controller cloudflare-dns (default 10s) --cloudflare-dns.dns.pool.resync-period duration Period for resynchronization for pool dns of controller cloudflare-dns (default 15m0s) --cloudflare-dns.dns.pool.size int Worker pool size for pool dns of controller cloudflare-dns (default 1) --cloudflare-dns.dry-run just check, don\u0026#39;t modify of controller cloudflare-dns --cloudflare-dns.identifier string Identifier used to mark DNS entries in DNS system of controller cloudflare-dns (default \u0026#34;dnscontroller\u0026#34;) --cloudflare-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller cloudflare-dns (default 1) --cloudflare-dns.pool.resync-period duration Period for resynchronization of controller cloudflare-dns --cloudflare-dns.pool.size int Worker pool size of controller cloudflare-dns --cloudflare-dns.providers.pool.resync-period duration Period for resynchronization for pool providers of controller cloudflare-dns (default 10m0s) --cloudflare-dns.providers.pool.size int Worker pool size for pool providers of controller cloudflare-dns (default 2) --cloudflare-dns.ratelimiter.burst int number of burst requests for rate limiter of controller cloudflare-dns --cloudflare-dns.ratelimiter.enabled enables rate limiter for DNS provider requests of controller cloudflare-dns --cloudflare-dns.ratelimiter.qps int maximum requests/queries per second of controller cloudflare-dns --cloudflare-dns.reschedule-delay duration reschedule delay after losing provider of controller cloudflare-dns (default 2m0s) --cloudflare-dns.secrets.pool.size int Worker pool size for pool secrets of controller cloudflare-dns (default 2) --cloudflare-dns.setup int number of processors for controller setup of controller cloudflare-dns (default 10) --cloudflare-dns.statistic.pool.size int Worker pool size for pool statistic of controller cloudflare-dns (default 1) --cloudflare-dns.ttl int Default time-to-live for DNS entries of controller cloudflare-dns (default 300) --config string config file -c, --controllers string comma separated list of controllers to start (\u0026lt;name\u0026gt;,\u0026lt;group\u0026gt;,all) (default \u0026#34;all\u0026#34;) --cpuprofile string set file for cpu profiling --default.pool.resync-period duration Period for resynchronization for pool default --default.pool.size int Worker pool size for pool default --disable-namespace-restriction disable access restriction for namespace local access only --disable-zone-state-caching disable use of cached dns zone state on changes --dns-class string identifier used to differentiate responsible controllers for entries --dns-delay duration delay between two dns reconciliations --dns-target-class string identifier used to differentiate responsible dns controllers for target entries --dns.pool.resync-period duration Period for resynchronization for pool dns --dns.pool.size int Worker pool size for pool dns --dnsentry-source.default.pool.resync-period duration Period for resynchronization for pool default of controller dnsentry-source (default 2m0s) --dnsentry-source.default.pool.size int Worker pool size for pool default of controller dnsentry-source (default 2) --dnsentry-source.dns-class string identifier used to differentiate responsible controllers for entries of controller dnsentry-source (default \u0026#34;gardendns\u0026#34;) --dnsentry-source.dns-target-class string identifier used to differentiate responsible dns controllers for target entries of controller dnsentry-source --dnsentry-source.exclude-domains stringArray excluded domains of controller dnsentry-source --dnsentry-source.key string selecting key for annotation of controller dnsentry-source --dnsentry-source.pool.resync-period duration Period for resynchronization of controller dnsentry-source --dnsentry-source.pool.size int Worker pool size of controller dnsentry-source --dnsentry-source.target-creator-label-name string label name to store the creator for generated DNS entries of controller dnsentry-source (default \u0026#34;creator\u0026#34;) --dnsentry-source.target-creator-label-value string label value for creator label of controller dnsentry-source --dnsentry-source.target-name-prefix string name prefix in target namespace for cross cluster generation of controller dnsentry-source --dnsentry-source.target-namespace string target namespace for cross cluster generation of controller dnsentry-source --dnsentry-source.target-owner-id string owner id to use for generated DNS entries of controller dnsentry-source --dnsentry-source.target-realms string realm(s) to use for generated DNS entries of controller dnsentry-source --dnsentry-source.target-set-ignore-owners mark generated DNS entries to omit owner based access control of controller dnsentry-source --dnsentry-source.targets.pool.size int Worker pool size for pool targets of controller dnsentry-source (default 2) --dry-run just check, don\u0026#39;t modify --exclude-domains stringArray excluded domains --force-crd-update enforce update of crds even they are unmanaged --google-clouddns.cache-dir string Directory to store zone caches (for reload after restart) of controller google-clouddns --google-clouddns.cache-ttl int Time-to-live for provider hosted zone cache of controller google-clouddns (default 120) --google-clouddns.default.pool.size int Worker pool size for pool default of controller google-clouddns (default 2) --google-clouddns.disable-zone-state-caching disable use of cached dns zone state on changes of controller google-clouddns --google-clouddns.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller google-clouddns (default \u0026#34;gardendns\u0026#34;) --google-clouddns.dns-delay duration delay between two dns reconciliations of controller google-clouddns (default 10s) --google-clouddns.dns.pool.resync-period duration Period for resynchronization for pool dns of controller google-clouddns (default 15m0s) --google-clouddns.dns.pool.size int Worker pool size for pool dns of controller google-clouddns (default 1) --google-clouddns.dry-run just check, don\u0026#39;t modify of controller google-clouddns --google-clouddns.identifier string Identifier used to mark DNS entries in DNS system of controller google-clouddns (default \u0026#34;dnscontroller\u0026#34;) --google-clouddns.ownerids.pool.size int Worker pool size for pool ownerids of controller google-clouddns (default 1) --google-clouddns.pool.resync-period duration Period for resynchronization of controller google-clouddns --google-clouddns.pool.size int Worker pool size of controller google-clouddns --google-clouddns.providers.pool.resync-period duration Period for resynchronization for pool providers of controller google-clouddns (default 10m0s) --google-clouddns.providers.pool.size int Worker pool size for pool providers of controller google-clouddns (default 2) --google-clouddns.ratelimiter.burst int number of burst requests for rate limiter of controller google-clouddns --google-clouddns.ratelimiter.enabled enables rate limiter for DNS provider requests of controller google-clouddns --google-clouddns.ratelimiter.qps int maximum requests/queries per second of controller google-clouddns --google-clouddns.reschedule-delay duration reschedule delay after losing provider of controller google-clouddns (default 2m0s) --google-clouddns.secrets.pool.size int Worker pool size for pool secrets of controller google-clouddns (default 2) --google-clouddns.setup int number of processors for controller setup of controller google-clouddns (default 10) --google-clouddns.statistic.pool.size int Worker pool size for pool statistic of controller google-clouddns (default 1) --google-clouddns.ttl int Default time-to-live for DNS entries of controller google-clouddns (default 300) --grace-period duration inactivity grace period for detecting end of cleanup for shutdown -h, --help help for dns-controller-manager --identifier string Identifier used to mark DNS entries in DNS system --infoblox-dns.cache-dir string Directory to store zone caches (for reload after restart) of controller infoblox-dns --infoblox-dns.cache-ttl int Time-to-live for provider hosted zone cache of controller infoblox-dns (default 120) --infoblox-dns.default.pool.size int Worker pool size for pool default of controller infoblox-dns (default 2) --infoblox-dns.disable-zone-state-caching disable use of cached dns zone state on changes of controller infoblox-dns --infoblox-dns.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller infoblox-dns (default \u0026#34;gardendns\u0026#34;) --infoblox-dns.dns-delay duration delay between two dns reconciliations of controller infoblox-dns (default 10s) --infoblox-dns.dns.pool.resync-period duration Period for resynchronization for pool dns of controller infoblox-dns (default 15m0s) --infoblox-dns.dns.pool.size int Worker pool size for pool dns of controller infoblox-dns (default 1) --infoblox-dns.dry-run just check, don\u0026#39;t modify of controller infoblox-dns --infoblox-dns.identifier string Identifier used to mark DNS entries in DNS system of controller infoblox-dns (default \u0026#34;dnscontroller\u0026#34;) --infoblox-dns.ownerids.pool.size int Worker pool size for pool ownerids of controller infoblox-dns (default 1) --infoblox-dns.pool.resync-period duration Period for resynchronization of controller infoblox-dns --infoblox-dns.pool.size int Worker pool size of controller infoblox-dns --infoblox-dns.providers.pool.resync-period duration Period for resynchronization for pool providers of controller infoblox-dns (default 10m0s) --infoblox-dns.providers.pool.size int Worker pool size for pool providers of controller infoblox-dns (default 2) --infoblox-dns.ratelimiter.burst int number of burst requests for rate limiter of controller infoblox-dns --infoblox-dns.ratelimiter.enabled enables rate limiter for DNS provider requests of controller infoblox-dns --infoblox-dns.ratelimiter.qps int maximum requests/queries per second of controller infoblox-dns --infoblox-dns.reschedule-delay duration reschedule delay after losing provider of controller infoblox-dns (default 2m0s) --infoblox-dns.secrets.pool.size int Worker pool size for pool secrets of controller infoblox-dns (default 2) --infoblox-dns.setup int number of processors for controller setup of controller infoblox-dns (default 10) --infoblox-dns.statistic.pool.size int Worker pool size for pool statistic of controller infoblox-dns (default 1) --infoblox-dns.ttl int Default time-to-live for DNS entries of controller infoblox-dns (default 300) --ingress-dns.default.pool.resync-period duration Period for resynchronization for pool default of controller ingress-dns (default 2m0s) --ingress-dns.default.pool.size int Worker pool size for pool default of controller ingress-dns (default 2) --ingress-dns.dns-class string identifier used to differentiate responsible controllers for entries of controller ingress-dns (default \u0026#34;gardendns\u0026#34;) --ingress-dns.dns-target-class string identifier used to differentiate responsible dns controllers for target entries of controller ingress-dns --ingress-dns.exclude-domains stringArray excluded domains of controller ingress-dns --ingress-dns.key string selecting key for annotation of controller ingress-dns --ingress-dns.pool.resync-period duration Period for resynchronization of controller ingress-dns --ingress-dns.pool.size int Worker pool size of controller ingress-dns --ingress-dns.target-creator-label-name string label name to store the creator for generated DNS entries of controller ingress-dns (default \u0026#34;creator\u0026#34;) --ingress-dns.target-creator-label-value string label value for creator label of controller ingress-dns --ingress-dns.target-name-prefix string name prefix in target namespace for cross cluster generation of controller ingress-dns --ingress-dns.target-namespace string target namespace for cross cluster generation of controller ingress-dns --ingress-dns.target-owner-id string owner id to use for generated DNS entries of controller ingress-dns --ingress-dns.target-realms string realm(s) to use for generated DNS entries of controller ingress-dns --ingress-dns.target-set-ignore-owners mark generated DNS entries to omit owner based access control of controller ingress-dns --ingress-dns.targets.pool.size int Worker pool size for pool targets of controller ingress-dns (default 2) --key string selecting key for annotation --kubeconfig string default cluster access --kubeconfig.disable-deploy-crds disable deployment of required crds for cluster default --kubeconfig.id string id for cluster default --kubeconfig.migration-ids string migration id for cluster default --lease-duration duration lease duration (default 15s) --lease-name string name for lease object --lease-renew-deadline duration lease renew deadline (default 10s) --lease-retry-period duration lease retry period (default 2s) -D, --log-level string logrus log level --maintainer string maintainer key for crds (default \u0026#34;dns-controller-manager\u0026#34;) --name string name used for controller manager (default \u0026#34;dns-controller-manager\u0026#34;) --namespace string namespace for lease (default \u0026#34;kube-system\u0026#34;) -n, --namespace-local-access-only enable access restriction for namespace local access only (deprecated) --omit-lease omit lease for development --openstack-designate.cache-dir string Directory to store zone caches (for reload after restart) of controller openstack-designate --openstack-designate.cache-ttl int Time-to-live for provider hosted zone cache of controller openstack-designate (default 120) --openstack-designate.default.pool.size int Worker pool size for pool default of controller openstack-designate (default 2) --openstack-designate.disable-zone-state-caching disable use of cached dns zone state on changes of controller openstack-designate --openstack-designate.dns-class string Class identifier used to differentiate responsible controllers for entry resources of controller openstack-designate (default \u0026#34;gardendns\u0026#34;) --openstack-designate.dns-delay duration delay between two dns reconciliations of controller openstack-designate (default 10s) --openstack-designate.dns.pool.resync-period duration Period for resynchronization for pool dns of controller openstack-designate (default 15m0s) --openstack-designate.dns.pool.size int Worker pool size for pool dns of controller openstack-designate (default 1) --openstack-designate.dry-run just check, don\u0026#39;t modify of controller openstack-designate --openstack-designate.identifier string Identifier used to mark DNS entries in DNS system of controller openstack-designate (default \u0026#34;dnscontroller\u0026#34;) --openstack-designate.ownerids.pool.size int Worker pool size for pool ownerids of controller openstack-designate (default 1) --openstack-designate.pool.resync-period duration Period for resynchronization of controller openstack-designate --openstack-designate.pool.size int Worker pool size of controller openstack-designate --openstack-designate.providers.pool.resync-period duration Period for resynchronization for pool providers of controller openstack-designate (default 10m0s) --openstack-designate.providers.pool.size int Worker pool size for pool providers of controller openstack-designate (default 2) --openstack-designate.ratelimiter.burst int number of burst requests for rate limiter of controller openstack-designate --openstack-designate.ratelimiter.enabled enables rate limiter for DNS provider requests of controller openstack-designate --openstack-designate.ratelimiter.qps int maximum requests/queries per second of controller openstack-designate --openstack-designate.reschedule-delay duration reschedule delay after losing provider of controller openstack-designate (default 2m0s) --openstack-designate.secrets.pool.size int Worker pool size for pool secrets of controller openstack-designate (default 2) --openstack-designate.setup int number of processors for controller setup of controller openstack-designate (default 10) --openstack-designate.statistic.pool.size int Worker pool size for pool statistic of controller openstack-designate (default 1) --openstack-designate.ttl int Default time-to-live for DNS entries of controller openstack-designate (default 300) --ownerids.pool.size int Worker pool size for pool ownerids --plugin-file string directory containing go plugins --pool.resync-period duration Period for resynchronization --pool.size int Worker pool size --providers string cluster to look for provider objects --providers.disable-deploy-crds disable deployment of required crds for cluster provider --providers.id string id for cluster provider --providers.migration-ids string migration id for cluster provider --providers.pool.resync-period duration Period for resynchronization for pool providers --providers.pool.size int Worker pool size for pool providers --ratelimiter.burst int number of burst requests for rate limiter --ratelimiter.enabled enables rate limiter for DNS provider requests --ratelimiter.qps int maximum requests/queries per second --reschedule-delay duration reschedule delay after losing provider --secrets.pool.size int Worker pool size for pool secrets --server-port-http int HTTP server port (serving /healthz, /metrics, ...) --service-dns.default.pool.resync-period duration Period for resynchronization for pool default of controller service-dns (default 2m0s) --service-dns.default.pool.size int Worker pool size for pool default of controller service-dns (default 2) --service-dns.dns-class string identifier used to differentiate responsible controllers for entries of controller service-dns (default \u0026#34;gardendns\u0026#34;) --service-dns.dns-target-class string identifier used to differentiate responsible dns controllers for target entries of controller service-dns --service-dns.exclude-domains stringArray excluded domains of controller service-dns --service-dns.key string selecting key for annotation of controller service-dns --service-dns.pool.resync-period duration Period for resynchronization of controller service-dns --service-dns.pool.size int Worker pool size of controller service-dns --service-dns.target-creator-label-name string label name to store the creator for generated DNS entries of controller service-dns (default \u0026#34;creator\u0026#34;) --service-dns.target-creator-label-value string label value for creator label of controller service-dns --service-dns.target-name-prefix string name prefix in target namespace for cross cluster generation of controller service-dns --service-dns.target-namespace string target namespace for cross cluster generation of controller service-dns --service-dns.target-owner-id string owner id to use for generated DNS entries of controller service-dns --service-dns.target-realms string realm(s) to use for generated DNS entries of controller service-dns --service-dns.target-set-ignore-owners mark generated DNS entries to omit owner based access control of controller service-dns --service-dns.targets.pool.size int Worker pool size for pool targets of controller service-dns (default 2) --setup int number of processors for controller setup --statistic.pool.size int Worker pool size for pool statistic --target string target cluster for dns requests --target-creator-label-name string label name to store the creator for generated DNS entries --target-creator-label-value string label value for creator label --target-name-prefix string name prefix in target namespace for cross cluster generation --target-namespace string target namespace for cross cluster generation --target-owner-id string owner id to use for generated DNS entries --target-realms string realm(s) to use for generated DNS entries --target-set-ignore-owners mark generated DNS entries to omit owner based access control --target.disable-deploy-crds disable deployment of required crds for cluster target --target.id string id for cluster target --target.migration-ids string migration id for cluster target --targets.pool.size int Worker pool size for pool targets --ttl int Default time-to-live for DNS entries (defaults to 300s). Defines how long the record is kept in cache by DNS servers or resolvers. --version version for dns-controller-manager Extensions This project can also be used as library to implement own source and provisioning controllers.\nHow to implement Source Controllers Based on the provided source controller library a source controller must implement the source.DNSSource interface and provide an appropriate creator function.\nA source controller can be implemented following this example:\npackage service import ( \u0026#34;github.com/gardener/controller-manager-library/pkg/resources\u0026#34; \u0026#34;github.com/gardener/external-dns-management/pkg/dns/source\u0026#34; ) var _MAIN_RESOURCE = resources.NewGroupKind(\u0026#34;core\u0026#34;, \u0026#34;Service\u0026#34;) func init() { source.DNSSourceController(source.NewDNSSouceTypeForExtractor(\u0026#34;service-dns\u0026#34;, _MAIN_RESOURCE, GetTargets),nil). FinalizerDomain(\u0026#34;dns.gardener.cloud\u0026#34;). MustRegister(source.CONTROLLER_GROUP_DNS_SOURCES) } Complete examples can be found in the sub packages of pkg/controller/source.\nHow to implement Provisioning Controllers Provisioning controllers can be implemented based on the provisioning controller library in this repository and must implement the provider.DNSHandlerFactory interface. This factory returns implementations of the provider.DNSHandler interface that does the effective work for a dedicated set of hosted zones.\nThese factories can be embedded into a final controller manager (the runnable instance) in several ways:\n The factory can be used to create a dedicated controller. This controller can then be embedded into a controller manager, either in its own controller manger or together with other controllers. The factory can be added to a compound factory, able to handle multiple infrastructures. This one can then be used to create a dedicated controller, again.  Embedding a Factory into a Controller A provisioning controller can be implemented following this example:\npackage controller import ( \u0026#34;github.com/gardener/external-dns-management/pkg/dns/provider\u0026#34; ) const CONTROLLER_NAME = \u0026#34;route53-dns-controller\u0026#34; func init() { provider.DNSController(CONTROLLER_NAME, \u0026amp;Factory{}). FinalizerDomain(\u0026#34;dns.gardener.cloud\u0026#34;). MustRegister(provider.CONTROLLER_GROUP_DNS_CONTROLLERS) } This controller can be embedded into a controller manager just by using an anonymous import of the controller package in the main package of a dedicated controller manager.\nComplete examples are available in the sub packages of pkg/controller/provider. They also show a typical set of implementation structures that help to structure the implementation of such controllers.\nThe provider implemented in this project always follow the same structure:\n the provider package contains the provider code the factory source file registers the factory at a default compound factory it contains a sub package controller, which contains the embedding of the factory into a dedicated controller  Embedding a Factory into a Compound Factory A provisioning controller based on a Compound Factory can be extended by a new provider factory by registering this factory at the compound factory. This could be done, for example, by using the default compound factory provided in package pkg/controller/provider/compound as shown here, where NewHandler is a function creating a dedicated handler for a dedicated provider type:\npackage aws import ( \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/compound\u0026#34; \u0026#34;github.com/gardener/external-dns-management/pkg/dns/provider\u0026#34; ) const TYPE_CODE = \u0026#34;aws-route53\u0026#34; var Factory = provider.NewDNSHandlerFactory(TYPE_CODE, NewHandler) func init() { compound.MustRegister(Factory) } The compound factory is then again embedded into a provisioning controller as shown in the previous section (see the controllersub package).\nSetting Up a Controller Manager One or multiple controller packages can be bundled into a controller manager, by implementing a main package like this:\npackage main import ( \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026#34; _ \u0026#34;github.com/\u0026lt;your controller package\u0026gt;\u0026#34; ... ) func main() { controllermanager.Start(\u0026#34;my-dns-controller-manager\u0026#34;, \u0026#34;dns controller manager\u0026#34;, \u0026#34;some description\u0026#34;) } Using the standard Compound Provisioning Controller If the standard Compound Provisioning Controller should be used it is required to additionally add the anonymous imports for the providers intended to be embedded into the compound factory like this:\n Example Coding package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/compound/controller\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/\u0026lt;your provider\u0026gt;\u0026#34; ... ) func main() { controllermanager.Start(\u0026#34;dns-controller-manager\u0026#34;, \u0026#34;dns controller manager\u0026#34;, \u0026#34;nothing\u0026#34;) }  Multiple Cluster Support The controller implementations provided in this project are prepared to work with multiple clusters by using the features of the used controller manager library.\nThe DNS Source Controllers support two clusters:\n the default cluster is used to scan for source objects the logical cluster target is used to maintain the DNSEnry objects.  The DNS Provisioning Controllers also support two clusters:\n the default cluster is used to scan for DNSEntry objects. It is mapped to the logical cluster target the logical cluster provider is used to look to the DNSProvider objects and their related secrets.  If those controller types should be combined in a single controller manager, it can be configured to support three potential clusters with the source objects, the one for the entry objects and the one with provider objects using cluster mappings.\nThis is shown in a complete example using the dns source controllers, the compound provisioning controller configured to support all the included DNS provider type factories:\n Example Coding package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager/cluster\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager/controller\u0026#34; \u0026#34;github.com/gardener/controller-manager-library/pkg/controllermanager/controller/mappings\u0026#34; dnsprovider \u0026#34;github.com/gardener/external-dns-management/pkg/dns/provider\u0026#34; dnssource \u0026#34;github.com/gardener/external-dns-management/pkg/dns/source\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/compound/controller\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/alicloud\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/aws\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/azure\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/google\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/openstack\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/provider/cloudflare\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/source/ingress\u0026#34; _ \u0026#34;github.com/gardener/external-dns-management/pkg/controller/source/service\u0026#34; ) func init() { // target cluster already defined in dns source controller package  cluster.Configure( dnsprovider.PROVIDER_CLUSTER, \u0026#34;providers\u0026#34;, \u0026#34;cluster to look for provider objects\u0026#34;, ).Fallback(dnssource.TARGET_CLUSTER) mappings.ForControllerGroup(dnsprovider.CONTROLLER_GROUP_DNS_CONTROLLERS). Map(controller.CLUSTER_MAIN, dnssource.TARGET_CLUSTER).MustRegister() } func main() { controllermanager.Start(\u0026#34;dns-controller-manager\u0026#34;, \u0026#34;dns controller manager\u0026#34;, \u0026#34;nothing\u0026#34;) }  Those clusters can the be separated by registering their names together with command line option names. These can be used to specify different kubeconfig files for those clusters.\nBy default all logical clusters are mapped to the default physical cluster specified via --kubeconfig or default cluster access.\nIf multiple physical clusters are defined they can be specified by a corresponding cluster option defining the kubeconfig file used to access this cluster. If no such option is specified the default is used.\nTherefore, even if the configuration is prepared for multiple clusters, such a controller manager can easily work on a single cluster if no special options are given on the command line.\nWhy not using the community external-dns solution? Some of the reasons are context-specific, i.e. relate to Gardener\u0026rsquo;s highly dynamic requirements.\n Custom resource for DNS entries  DNS entries are explicitly specified as custom resources. As an important side effect, each DNS entry provides an own status. Simply by querying the Kubernetes API, a client can check if a requested DNS entry has been successfully added to the DNS backend, or if an update has already been deployed, or if not to reason about the cause. It also opens for easy extensibility, as DNS entries can be created directly via the Kubernetes API. And it simplifies Day 2 operations, e.g. automatic cleanup of unused entries if a DNS provider is deleted.\nManagement of multiple DNS providers  The Gardener DNS controller uses a custom resource DNSProvider to dynamically manage the backend DNS services. While with external-dns you have to specify the single provider during startup, in the Gardener DNS controller you can add/update/delete providers during runtime with different credentials and/or backends. This is important for a multi-tenant environment as in Gardener, where users can bring their own accounts.\nA DNS provider can also restrict its actions on subset of the DNS domains (includes and excludes) for which the credentials are capable to edit.\nEach provider can define a separate owner identifier, to differentiate DNS entries in the same DNS zone from different providers.\nMulti cluster support  The Gardener DNS controller distinguish three different logical Kubernetes clusters: Source cluster, target cluster and runtime cluster. The source cluster is monitored by the DNS source controllers for annotations on ingress and service resources. These controllers then create DNS entries in the target cluster. DNS entries in the target cluster are then reconciliated/synchronized with the corresponding DNS backend service by the provider controller. The runtime cluster is the cluster the DNS controller runs on. For example, this enables needed flexibility in the Gardener deployment. The DNS controller runs on the seed cluster. This is also the target cluster. DNS providers and entries resources are created in the corresponding namespace of the shoot control plane, while the source cluster is the shoot cluster itself.\nOptimizations for handling hundreds of DNS entries  Some DNS backend services are restricted on the API calls per second (e.g. the AWS Route 53 API). To manage hundreds of DNS entries it is important to minimize the number of API calls. The Gardener DNS controller heavily makes usage of caches and batch processing for this reason.\n"},{"uri":"https://gardener.cloud/documentation/concepts/core-components/api-server/","title":"Gardener API Server","tags":[],"description":"","content":"Gardener API server The Gardener API server is a Kubernetes-native extension based on its aggregation layer. It is registered via an APIService object and designed to run inside a Kubernetes cluster whose API it wants to extend.\nAfter registration, it exposes the following resources:\nCloudProfiles CloudProfiles are resources that describe a specific environment of an underlying infrastructure provider, e.g. AWS, Azure, etc. Each shoot has to reference a CloudProfile to declare the environment it should be created in. In a CloudProfile the gardener operator specifies certain constraints like available machine types, regions, which Kubernetes versions he wants to offer, etc. End-users can read CloudProfiles to see these values, but only operators can change the content or create/delete them. When a shoot is created or updated then an admission plugin checks that only values are used that are allowed via the referenced CloudProfile.\nAdditionally, a CloudProfile may contain a providerConfig which is a special configuration dedicated for the infrastructure provider. Gardener does not evaluate or understand this config, but extension controllers might need for declaration of provider-specific constraints, or global settings.\nPlease see this example manifest and consult the documentation of your provider extension controller to get information about its providerConfig.\nSeeds Seeds are resources that represent seed clusters. Gardener does not care about how a seed cluster got created - the only requirement is that it is of at least Kubernetes v1.11 and passes the Kubernetes conformance tests. The Gardener operator has to either deploy the Gardenlet into the cluster he wants to use as seed (recommended, then the Gardenlet will create the Seed object itself after bootstrapping), or he provides the kubeconfig to the cluster inside a secret (that is referenced by the Seed resource) and creates the Seed resource himself.\nPlease see this, this(, and optionally this) example manifests.\nQuotas In order to allow end-user not having their own dedicated infrastructure account to try out Gardener the operator can register an account owned by him that he allows to be used for trial clusters. Trial clusters can be put under quota such that they don\u0026rsquo;t consume too many resources (resulting in costs), and so that one user cannot consume all resources on his own. These clusters are automatically terminated after a specified time, but end-users may extend the lifetime manually if needed.\nPlease see this example manifest.\nProjects The first thing before creating a shoot cluster is to create a Project. A project is used to group multiple shoot clusters together. End-users can invite colleagues to the project to enable collaboration, and they can either make them admin or viewer. After an end-user has created a project he will get a dedicated namespace in the garden cluster for all his shoots.\nPlease see this example manifest.\nSecretBindings Now that the end-user has a namespace the next step is registering his infrastructure provider account.\nPlease see this example manifest and consult the documentation of the extension controller for the respective infrastructure provider to get information about which keys are required in this secret.\nAfter the secret has been created the end-user has to create a special SecretBinding resource that binds this secret. Later when creating shoot clusters he will reference such a binding.\nPlease see this example manifest.\nShoots Shoot cluster contain various settings that influence how end-user Kubernetes clusters will look like in the end. As Gardener heavily relies on extension controllers for operating system configuration, networking, and infrastructure specifics, the end-user has the possibility (and responsibility) to provide these provider-specific configurations as well. Such configurations are not evaluated by Gardener (because it doesn\u0026rsquo;t know/understand them), but they are only transported to the respective extension controller.\n:warning: This means that any configuration issues/mistake on the end-user side that relates to a provider-specific flag or setting cannot be caught during the update request itself but only later during the reconciliation (unless a validator webhook has been registered in the garden cluster by an operator).\nPlease see this example manifest and consult the documentation of the provider extension controller to get information about its spec.provider.controlPlaneConfig, .spec.provider.infrastructureConfig, and .spec.provider.workers[].providerConfig.\n(Cluster)OpenIDConnectPresets Please see this separate documentation file.\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/core-components/api-server/","title":"Gardener API Server","tags":[],"description":"","content":"Gardener API server The Gardener API server is a Kubernetes-native extension based on its aggregation layer. It is registered via an APIService object and designed to run inside a Kubernetes cluster whose API it wants to extend.\nAfter registration, it exposes the following resources:\nCloudProfiles CloudProfiles are resources that describe a specific environment of an underlying infrastructure provider, e.g. AWS, Azure, etc. Each shoot has to reference a CloudProfile to declare the environment it should be created in. In a CloudProfile the gardener operator specifies certain constraints like available machine types, regions, which Kubernetes versions he wants to offer, etc. End-users can read CloudProfiles to see these values, but only operators can change the content or create/delete them. When a shoot is created or updated then an admission plugin checks that only values are used that are allowed via the referenced CloudProfile.\nAdditionally, a CloudProfile may contain a providerConfig which is a special configuration dedicated for the infrastructure provider. Gardener does not evaluate or understand this config, but extension controllers might need for declaration of provider-specific constraints, or global settings.\nPlease see this example manifest and consult the documentation of your provider extension controller to get information about its providerConfig.\nSeeds Seeds are resources that represent seed clusters. Gardener does not care about how a seed cluster got created - the only requirement is that it is of at least Kubernetes v1.11 and passes the Kubernetes conformance tests. The Gardener operator has to either deploy the Gardenlet into the cluster he wants to use as seed (recommended, then the Gardenlet will create the Seed object itself after bootstrapping), or he provides the kubeconfig to the cluster inside a secret (that is referenced by the Seed resource) and creates the Seed resource himself.\nPlease see this, this(, and optionally this) example manifests.\nQuotas In order to allow end-user not having their own dedicated infrastructure account to try out Gardener the operator can register an account owned by him that he allows to be used for trial clusters. Trial clusters can be put under quota such that they don\u0026rsquo;t consume too many resources (resulting in costs), and so that one user cannot consume all resources on his own. These clusters are automatically terminated after a specified time, but end-users may extend the lifetime manually if needed.\nPlease see this example manifest.\nProjects The first thing before creating a shoot cluster is to create a Project. A project is used to group multiple shoot clusters together. End-users can invite colleagues to the project to enable collaboration, and they can either make them admin or viewer. After an end-user has created a project he will get a dedicated namespace in the garden cluster for all his shoots.\nPlease see this example manifest.\nSecretBindings Now that the end-user has a namespace the next step is registering his infrastructure provider account.\nPlease see this example manifest and consult the documentation of the extension controller for the respective infrastructure provider to get information about which keys are required in this secret.\nAfter the secret has been created the end-user has to create a special SecretBinding resource that binds this secret. Later when creating shoot clusters he will reference such a binding.\nPlease see this example manifest.\nShoots Shoot cluster contain various settings that influence how end-user Kubernetes clusters will look like in the end. As Gardener heavily relies on extension controllers for operating system configuration, networking, and infrastructure specifics, the end-user has the possibility (and responsibility) to provide these provider-specific configurations as well. Such configurations are not evaluated by Gardener (because it doesn\u0026rsquo;t know/understand them), but they are only transported to the respective extension controller.\n:warning: This means that any configuration issues/mistake on the end-user side that relates to a provider-specific flag or setting cannot be caught during the update request itself but only later during the reconciliation (unless a validator webhook has been registered in the garden cluster by an operator).\nPlease see this example manifest and consult the documentation of the provider extension controller to get information about its spec.provider.controlPlaneConfig, .spec.provider.infrastructureConfig, and .spec.provider.workers[].providerConfig.\n(Cluster)OpenIDConnectPresets Please see this separate documentation file.\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/core-components/api-server/","title":"Gardener API Server","tags":[],"description":"","content":"Gardener API server The Gardener API server is a Kubernetes-native extension based on its aggregation layer. It is registered via an APIService object and designed to run inside a Kubernetes cluster whose API it wants to extend.\nAfter registration, it exposes the following resources:\nCloudProfiles CloudProfiles are resources that describe a specific environment of an underlying infrastructure provider, e.g. AWS, Azure, etc. Each shoot has to reference a CloudProfile to declare the environment it should be created in. In a CloudProfile the gardener operator specifies certain constraints like available machine types, regions, which Kubernetes versions he wants to offer, etc. End-users can read CloudProfiles to see these values, but only operators can change the content or create/delete them. When a shoot is created or updated then an admission plugin checks that only values are used that are allowed via the referenced CloudProfile.\nAdditionally, a CloudProfile may contain a providerConfig which is a special configuration dedicated for the infrastructure provider. Gardener does not evaluate or understand this config, but extension controllers might need for declaration of provider-specific constraints, or global settings.\nPlease see this example manifest and consult the documentation of your provider extension controller to get information about its providerConfig.\nSeeds Seeds are resources that represent seed clusters. Gardener does not care about how a seed cluster got created - the only requirement is that it is of at least Kubernetes v1.11 and passes the Kubernetes conformance tests. The Gardener operator has to either deploy the Gardenlet into the cluster he wants to use as seed (recommended, then the Gardenlet will create the Seed object itself after bootstrapping), or he provides the kubeconfig to the cluster inside a secret (that is referenced by the Seed resource) and creates the Seed resource himself.\nPlease see this, this(, and optionally this) example manifests.\nQuotas In order to allow end-user not having their own dedicated infrastructure account to try out Gardener the operator can register an account owned by him that he allows to be used for trial clusters. Trial clusters can be put under quota such that they don\u0026rsquo;t consume too many resources (resulting in costs), and so that one user cannot consume all resources on his own. These clusters are automatically terminated after a specified time, but end-users may extend the lifetime manually if needed.\nPlease see this example manifest.\nProjects The first thing before creating a shoot cluster is to create a Project. A project is used to group multiple shoot clusters together. End-users can invite colleagues to the project to enable collaboration, and they can either make them admin or viewer. After an end-user has created a project he will get a dedicated namespace in the garden cluster for all his shoots.\nPlease see this example manifest.\nSecretBindings Now that the end-user has a namespace the next step is registering his infrastructure provider account.\nPlease see this example manifest and consult the documentation of the extension controller for the respective infrastructure provider to get information about which keys are required in this secret.\nAfter the secret has been created the end-user has to create a special SecretBinding resource that binds this secret. Later when creating shoot clusters he will reference such a binding.\nPlease see this example manifest.\nShoots Shoot cluster contain various settings that influence how end-user Kubernetes clusters will look like in the end. As Gardener heavily relies on extension controllers for operating system configuration, networking, and infrastructure specifics, the end-user has the possibility (and responsibility) to provide these provider-specific configurations as well. Such configurations are not evaluated by Gardener (because it doesn\u0026rsquo;t know/understand them), but they are only transported to the respective extension controller.\n:warning: This means that any configuration issues/mistake on the end-user side that relates to a provider-specific flag or setting cannot be caught during the update request itself but only later during the reconciliation (unless a validator webhook has been registered in the garden cluster by an operator).\nPlease see this example manifest and consult the documentation of the provider extension controller to get information about its spec.provider.controlPlaneConfig, .spec.provider.infrastructureConfig, and .spec.provider.workers[].providerConfig.\n(Cluster)OpenIDConnectPresets Please see this separate documentation file.\n"},{"uri":"https://gardener.cloud/documentation/concepts/architecture/","title":"Architecture","tags":[],"description":"","content":"Official Definition - What is Kubernetes?  \u0026ldquo;Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\u0026rdquo;\n Introduction - Basic Principle The foundation of the Gardener (providing Kubernetes Clusters as a Service) is Kubernetes itself, because Kubernetes is the go-to solution to manage software in the Cloud, even when it\u0026rsquo;s Kubernetes itself (see also OpenStack which is provisioned more and more on top of Kubernetes as well).\nWhile self-hosting, meaning to run Kubernetes components inside Kubernetes, is a popular topic in the community, we apply a special pattern catering to the needs of our cloud platform to provision hundreds or even thousands of clusters. We take a so-called \u0026ldquo;seed\u0026rdquo; cluster and seed the control plane (such as the API server, scheduler, controllers, etcd persistence and others) of an end-user cluster, which we call \u0026ldquo;shoot\u0026rdquo; cluster, as pods into the \u0026ldquo;seed\u0026rdquo; cluster. That means one \u0026ldquo;seed\u0026rdquo; cluster, of which we will have one per IaaS and region, hosts the control planes of multiple \u0026ldquo;shoot\u0026rdquo; clusters. That allows us to avoid dedicated hardware/virtual machines for the \u0026ldquo;shoot\u0026rdquo; cluster control planes. We simply put the control plane into pods/containers and since the \u0026ldquo;seed\u0026rdquo; cluster watches them, they can be deployed with a replica count of 1 and only need to be scaled out when the control plane gets under pressure, but no longer for HA reasons. At the same time, the deployments get simpler (standard Kubernetes deployment) and easier to update (standard Kubernetes rolling update). The actual \u0026ldquo;shoot\u0026rdquo; cluster consists only out of the worker nodes (no control plane) and therefore the users may get full administrative access to their clusters.\nSetting The Scene - Components and Procedure We provide a central operator UI, which we call the \u0026ldquo;Gardener Dashboard\u0026rdquo;. It talks to a dedicated cluster, which we call the \u0026ldquo;Garden\u0026rdquo; cluster and uses custom resources managed by an aggregated API server, one of the general extension concepts of Kubernetes) to represent \u0026ldquo;shoot\u0026rdquo; clusters. In this \u0026ldquo;Garden\u0026rdquo; cluster runs the \u0026ldquo;Gardener\u0026rdquo;, which is basically a Kubernetes controller that watches the custom resources and acts upon them, i.e. creates, updates/modifies, or deletes \u0026ldquo;shoot\u0026rdquo; clusters. The creation follows basically these steps:\n Create a namespace in the \u0026ldquo;seed\u0026rdquo; cluster for the \u0026ldquo;shoot\u0026rdquo; cluster which will host the \u0026ldquo;shoot\u0026rdquo; cluster control plane Generate secrets and credentials which the worker nodes will need to talk to the control plane Create the infrastructure (using Terraform), which basically consists out of the network setup) Deploy the \u0026ldquo;shoot\u0026rdquo; cluster control plane into the \u0026ldquo;shoot\u0026rdquo; namespace in the \u0026ldquo;seed\u0026rdquo; cluster, containing the \u0026ldquo;machine-controller-manager\u0026rdquo; pod Create machine CRDs in the \u0026ldquo;seed\u0026rdquo; cluster, describing the configuration and the number of worker machines for the \u0026ldquo;shoot\u0026rdquo; (the machine-controller-manager watches the CRDs and creates virtual machines out of it) Wait for the \u0026ldquo;shoot\u0026rdquo; cluster API server to become responsive (pods will be scheduled, persistent volumes and load balancers are created by Kubernetes via the respective cloud provider) Finally we deploy kube-system daemons like kube-proxy and further add-ons like the dashboard into the \u0026ldquo;shoot\u0026rdquo; cluster and the cluster becomes active  Overview Architecture Diagram Note: The kubelet as well as the pods inside the \u0026ldquo;shoot\u0026rdquo; cluster talk through the front-door (load balancer IP; public Internet) to its \u0026ldquo;shoot\u0026rdquo; cluster API server running in the \u0026ldquo;seed\u0026rdquo; cluster. The reverse communication from the API server to the pod, service, and node networks happens through a VPN connection that we deploy into \u0026ldquo;seed\u0026rdquo; and \u0026ldquo;shoot\u0026rdquo; clusters.\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/architecture/","title":"Architecture","tags":[],"description":"","content":"Official Definition - What is Kubernetes?  \u0026ldquo;Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\u0026rdquo;\n Introduction - Basic Principle The foundation of the Gardener (providing Kubernetes Clusters as a Service) is Kubernetes itself, because Kubernetes is the go-to solution to manage software in the Cloud, even when it\u0026rsquo;s Kubernetes itself (see also OpenStack which is provisioned more and more on top of Kubernetes as well).\nWhile self-hosting, meaning to run Kubernetes components inside Kubernetes, is a popular topic in the community, we apply a special pattern catering to the needs of our cloud platform to provision hundreds or even thousands of clusters. We take a so-called \u0026ldquo;seed\u0026rdquo; cluster and seed the control plane (such as the API server, scheduler, controllers, etcd persistence and others) of an end-user cluster, which we call \u0026ldquo;shoot\u0026rdquo; cluster, as pods into the \u0026ldquo;seed\u0026rdquo; cluster. That means one \u0026ldquo;seed\u0026rdquo; cluster, of which we will have one per IaaS and region, hosts the control planes of multiple \u0026ldquo;shoot\u0026rdquo; clusters. That allows us to avoid dedicated hardware/virtual machines for the \u0026ldquo;shoot\u0026rdquo; cluster control planes. We simply put the control plane into pods/containers and since the \u0026ldquo;seed\u0026rdquo; cluster watches them, they can be deployed with a replica count of 1 and only need to be scaled out when the control plane gets under pressure, but no longer for HA reasons. At the same time, the deployments get simpler (standard Kubernetes deployment) and easier to update (standard Kubernetes rolling update). The actual \u0026ldquo;shoot\u0026rdquo; cluster consists only out of the worker nodes (no control plane) and therefore the users may get full administrative access to their clusters.\nSetting The Scene - Components and Procedure We provide a central operator UI, which we call the \u0026ldquo;Gardener Dashboard\u0026rdquo;. It talks to a dedicated cluster, which we call the \u0026ldquo;Garden\u0026rdquo; cluster and uses custom resources managed by an aggregated API server, one of the general extension concepts of Kubernetes) to represent \u0026ldquo;shoot\u0026rdquo; clusters. In this \u0026ldquo;Garden\u0026rdquo; cluster runs the \u0026ldquo;Gardener\u0026rdquo;, which is basically a Kubernetes controller that watches the custom resources and acts upon them, i.e. creates, updates/modifies, or deletes \u0026ldquo;shoot\u0026rdquo; clusters. The creation follows basically these steps:\n Create a namespace in the \u0026ldquo;seed\u0026rdquo; cluster for the \u0026ldquo;shoot\u0026rdquo; cluster which will host the \u0026ldquo;shoot\u0026rdquo; cluster control plane Generate secrets and credentials which the worker nodes will need to talk to the control plane Create the infrastructure (using Terraform), which basically consists out of the network setup) Deploy the \u0026ldquo;shoot\u0026rdquo; cluster control plane into the \u0026ldquo;shoot\u0026rdquo; namespace in the \u0026ldquo;seed\u0026rdquo; cluster, containing the \u0026ldquo;machine-controller-manager\u0026rdquo; pod Create machine CRDs in the \u0026ldquo;seed\u0026rdquo; cluster, describing the configuration and the number of worker machines for the \u0026ldquo;shoot\u0026rdquo; (the machine-controller-manager watches the CRDs and creates virtual machines out of it) Wait for the \u0026ldquo;shoot\u0026rdquo; cluster API server to become responsive (pods will be scheduled, persistent volumes and load balancers are created by Kubernetes via the respective cloud provider) Finally we deploy kube-system daemons like kube-proxy and further add-ons like the dashboard into the \u0026ldquo;shoot\u0026rdquo; cluster and the cluster becomes active  Overview Architecture Diagram Note: The kubelet as well as the pods inside the \u0026ldquo;shoot\u0026rdquo; cluster talk through the front-door (load balancer IP; public Internet) to its \u0026ldquo;shoot\u0026rdquo; cluster API server running in the \u0026ldquo;seed\u0026rdquo; cluster. The reverse communication from the API server to the pod, service, and node networks happens through a VPN connection that we deploy into \u0026ldquo;seed\u0026rdquo; and \u0026ldquo;shoot\u0026rdquo; clusters.\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/architecture/","title":"Architecture","tags":[],"description":"","content":"Official Definition - What is Kubernetes?  \u0026ldquo;Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\u0026rdquo;\n Introduction - Basic Principle The foundation of the Gardener (providing Kubernetes Clusters as a Service) is Kubernetes itself, because Kubernetes is the go-to solution to manage software in the Cloud, even when it\u0026rsquo;s Kubernetes itself (see also OpenStack which is provisioned more and more on top of Kubernetes as well).\nWhile self-hosting, meaning to run Kubernetes components inside Kubernetes, is a popular topic in the community, we apply a special pattern catering to the needs of our cloud platform to provision hundreds or even thousands of clusters. We take a so-called \u0026ldquo;seed\u0026rdquo; cluster and seed the control plane (such as the API server, scheduler, controllers, etcd persistence and others) of an end-user cluster, which we call \u0026ldquo;shoot\u0026rdquo; cluster, as pods into the \u0026ldquo;seed\u0026rdquo; cluster. That means one \u0026ldquo;seed\u0026rdquo; cluster, of which we will have one per IaaS and region, hosts the control planes of multiple \u0026ldquo;shoot\u0026rdquo; clusters. That allows us to avoid dedicated hardware/virtual machines for the \u0026ldquo;shoot\u0026rdquo; cluster control planes. We simply put the control plane into pods/containers and since the \u0026ldquo;seed\u0026rdquo; cluster watches them, they can be deployed with a replica count of 1 and only need to be scaled out when the control plane gets under pressure, but no longer for HA reasons. At the same time, the deployments get simpler (standard Kubernetes deployment) and easier to update (standard Kubernetes rolling update). The actual \u0026ldquo;shoot\u0026rdquo; cluster consists only out of the worker nodes (no control plane) and therefore the users may get full administrative access to their clusters.\nSetting The Scene - Components and Procedure We provide a central operator UI, which we call the \u0026ldquo;Gardener Dashboard\u0026rdquo;. It talks to a dedicated cluster, which we call the \u0026ldquo;Garden\u0026rdquo; cluster and uses custom resources managed by an aggregated API server, one of the general extension concepts of Kubernetes) to represent \u0026ldquo;shoot\u0026rdquo; clusters. In this \u0026ldquo;Garden\u0026rdquo; cluster runs the \u0026ldquo;Gardener\u0026rdquo;, which is basically a Kubernetes controller that watches the custom resources and acts upon them, i.e. creates, updates/modifies, or deletes \u0026ldquo;shoot\u0026rdquo; clusters. The creation follows basically these steps:\n Create a namespace in the \u0026ldquo;seed\u0026rdquo; cluster for the \u0026ldquo;shoot\u0026rdquo; cluster which will host the \u0026ldquo;shoot\u0026rdquo; cluster control plane Generate secrets and credentials which the worker nodes will need to talk to the control plane Create the infrastructure (using Terraform), which basically consists out of the network setup) Deploy the \u0026ldquo;shoot\u0026rdquo; cluster control plane into the \u0026ldquo;shoot\u0026rdquo; namespace in the \u0026ldquo;seed\u0026rdquo; cluster, containing the \u0026ldquo;machine-controller-manager\u0026rdquo; pod Create machine CRDs in the \u0026ldquo;seed\u0026rdquo; cluster, describing the configuration and the number of worker machines for the \u0026ldquo;shoot\u0026rdquo; (the machine-controller-manager watches the CRDs and creates virtual machines out of it) Wait for the \u0026ldquo;shoot\u0026rdquo; cluster API server to become responsive (pods will be scheduled, persistent volumes and load balancers are created by Kubernetes via the respective cloud provider) Finally we deploy kube-system daemons like kube-proxy and further add-ons like the dashboard into the \u0026ldquo;shoot\u0026rdquo; cluster and the cluster becomes active  Overview Architecture Diagram Note: The kubelet as well as the pods inside the \u0026ldquo;shoot\u0026rdquo; cluster talk through the front-door (load balancer IP; public Internet) to its \u0026ldquo;shoot\u0026rdquo; cluster API server running in the \u0026ldquo;seed\u0026rdquo; cluster. The reverse communication from the API server to the pod, service, and node networks happens through a VPN connection that we deploy into \u0026ldquo;seed\u0026rdquo; and \u0026ldquo;shoot\u0026rdquo; clusters.\n"},{"uri":"https://gardener.cloud/documentation/concepts/networking/cert-managment/","title":"Certificates Management","tags":[],"description":"","content":"cert-management \nManages TLS certificates in Kubernetes clusters using custom resources.\nIn a multi-cluster environment like Gardener, using existing open source projects for certificate management like cert-manager becomes cumbersome. With this project the separation of concerns between multiple clusters is realized more easily. The cert-controller-manager runs in a secured cluster where the issuer secrets are stored. At the same time it watches an untrusted source cluster and can provide certificates for it. The cert-controller-manager relies on DNS challenges (ACME only) for validating the domain names of the certificates. For this purpose it creates DNSEntry custom resources (in a possible separate dns cluster) to be handled by the compagnion dns-controller-manager from external-dns-management.\nCurrently, the cert-controller-manager supports certificate authorities via:\n Automatic Certificate Management Environment (ACME) protocol like Let\u0026rsquo;s Encrypt. Certificate Authority (CA): an existing certificate and a private key provided as a TLS Secret.  Setting up Issuers Before you can obtain certificates from a certificate authority (CA), you need to set up an issuer. The issuer is specified in the default cluster, while the certificates are specified in the source cluster.\nThe issuer custom resource contains the configuration and registration data for your account at the CA.\nAutomatic Certificate Management Environment (ACME) Two modes are supported:\n auto registration using an existing account  Auto registration Auto registration is mainly used for development and test environments. You only need to provide the server URL and an email address. The registration process is done automatically for you by creating a private key and performing the registration at the CA. Optionally you can provide the target secret with the privateKeySecretRef section.\nFor example see examples/20-issuer-staging.yaml:\napiVersion:cert.gardener.cloud/v1alpha1kind:Issuermetadata:name:issuer-stagingnamespace:defaultspec:acme:server:https://acme-staging-v02.api.letsencrypt.org/directoryemail:some.user@mydomain.comautoRegistration:true# with \u0026#39;autoRegistration: true\u0026#39; a new account will be created if the secretRef is not existingprivateKeySecretRef:name:issuer-staging-secretnamespace:defaultUsing existing account If you already have an existing account at the certificate authority, you need to specify email address and reference the private key from a secret.\napiVersion:v1kind:Secretmetadata:name:my-issuer-secretnamespace:defaulttype:Opaquedata:privateKey:LS0tLS1...apiVersion:cert.gardener.cloud/v1alpha1kind:Issuermetadata:name:my-issuernamespace:defaultspec:acme:server:https://acme-v02.api.letsencrypt.org/directoryemail:my.account@mydomain.comprivateKeySecretRef:name:my-issuer-secretnamespace:defaultIn both cases, the state of an issuer resource can be checked on the default cluster with\n kubectl get issuer NAME SERVER EMAIL STATUS TYPE AGE issuer-staging https://acme-staging-v02.api.letsencrypt.org/directory some.user@mydomain.com Ready acme 8s Certificate Authority (CA) This issuer is meant to be used where a central Certificate Authority is already in place. The operator must request/provide by its own means a CA or an intermediate CA. This is mainly used for on-premises and airgapped environements.\nIt can also be used for developement or testing purproses. In this case a Self-signed Certificate Authority can be created by following the section below.\nCreate a Self-signed Certificate Authority (optional)\n openssl genrsa -out CA-key.pem 4096  export CONFIG=\u0026#34; [req] distinguished_name=dn [ dn ] [ ext ] basicConstraints=CA:TRUE,pathlen:0 \u0026#34;  openssl req \\  -new -nodes -x509 -config \u0026lt;(echo \u0026#34;$CONFIG\u0026#34;) -key CA-key.pem \\  -subj \u0026#34;/CN=Hello\u0026#34; -extensions ext -days 1000 -out CA-cert.pem Create a TLS secret from the certificate CA-cert.pem and the private key CA-key.pem\n kubectl -n default create secret tls issuer-ca-secret \\  --cert=CA-cert.pem --key=CA-key.pem -oyaml \\  --dry-run=client \u0026gt; secret.yaml The content of the secret.yaml should look like the following, for a full example see examples/20-issuer-ca.yaml\napiVersion:v1data:tls.crt:{base64certificate}tls.key:{base64privatekey}kind:Secretmetadata:name:issuer-ca-secrettype:kubernetes.io/tlsApply the secrets in the cluster and create the issuer, for example see examples/20-issuer-ca.yaml\n---apiVersion:cert.gardener.cloud/v1alpha1kind:Issuermetadata:name:issuer-canamespace:defaultspec:ca:privateKeySecretRef:name:issuer-ca-secretnamespace:defaultThe state of the issuer resource can be checked on the default cluster with\n kubectl get issuer NAME SERVER EMAIL STATUS TYPE AGE issuer-ca Ready ca 6s Some details about the CA can be found in the status of the issuer.\n kubectl get issuer issuer-ca -ojsonpath=\u0026#39;{.status}\u0026#39; | jq \u0026#39;.\u0026#39; { \u0026#34;ca\u0026#34;: { \u0026#34;NotAfter\u0026#34;: \u0026#34;2023-05-31T14:55:55Z\u0026#34;, \u0026#34;NotBefore\u0026#34;: \u0026#34;2020-09-03T14:55:55Z\u0026#34;, \u0026#34;Subject\u0026#34;: { \u0026#34;CommonName\u0026#34;: \u0026#34;my-domain.com\u0026#34;, \u0026#34;Country\u0026#34;: [ \u0026#34;DE\u0026#34; ], \u0026#34;Locality\u0026#34;: [ \u0026#34;Walldorf\u0026#34; ], \u0026#34;Organization\u0026#34;: [ \u0026#34;Gardener\u0026#34; ], \u0026#34;OrganizationalUnit\u0026#34;: [ \u0026#34;Gardener\u0026#34; ], \u0026#34;PostalCode\u0026#34;: null, \u0026#34;Province\u0026#34;: [ \u0026#34;BW\u0026#34; ], \u0026#34;SerialNumber\u0026#34;: \u0026#34;1E04A2C8F057AC890F45FEC5446AE4DDA73EA1D5\u0026#34;, \u0026#34;StreetAddress\u0026#34;: null } }, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;requestsPerDayQuota\u0026#34;: 10000, \u0026#34;state\u0026#34;: \u0026#34;Ready\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ca\u0026#34; } Requesting a Certificate To obtain a certificate for a domain, you specify a certificate custom resource on the source cluster. You can specify the issuer explicitly by reference. If there is no issuer reference, the default issuer is used (provided as command line option). You must either specify the commonName and further optional dnsNames or you can also start with a certificate signing request (CSR).\nFor domain validation, the cert-controller-manager only supports DNS challenges. For this purpose it relies on the dns-controller-manager from the external-dns-management project. If any domain name (commonName or any item from dnsNames) needs to be validated, it creates a custom resource DNSEntry in the dns cluster. When the certificate authority sees the temporary DNS record, the certificate is stored in a secret finally. The name of the secret can be specified explicitly with secretName and will be stored in the same namespace as the certificate on the source cluster.\nThe certificate is checked for renewal periodically. The renewal is performed automatically and the secret is updated. Default values for periodical check is daily, the certificate is renewed if its validity expires within 60 days.\nUsing commonName and optional dnsNames For example see examples/30-cert-simple.yaml:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-simplenamespace:defaultspec:commonName:cert1.mydomain.comdnsNames:- cert1-foo.mydomain.com- cert1-bar.mydomain.com# if issuer is not specified, the default issuer is usedissuerRef:name:issuer-stagingUsing a certificate signing request (CSR) You can provide a complete CSR in PEM format (and encoded as Base64).\nFor example see examples/30-cert-csr.yaml:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-csrnamespace:defaultspec:csr:LS0tLS1CRUd...issuerRef:name:issuer-staging:warning: Using a CSR is only available for ACME Issuer\nRequesting a Certificate for Ingress Add the annotation cert.gardener.cloud/purpose: managed to the Ingress resource. The cert-controller-manager will then automatically request a certificate for all domains given by the hosts in the tls section of the Ingress spec.\nFor compatibility with the Gardener Cert-Broker, you can alternatively use the deprecated label garden.sapcloud.io/purpose: managed-cert for the same outcome.\nSee also examples/40-ingress-echoheaders.yaml:\nProcess   Create the Ingress Resource (optional)\nIn order to request a certificate for a domain managed by cert-controller-manager an Ingress is required. In case you dont already have one, take the following as an example:\napiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:vuejs-ingressspec:tls:# Gardener managed default domain.# The first host is used as common name and must not exceed 64 characters- hosts:- test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.example.com# Certificate and private key reside in this secret.secretName:testsecret-tlsrules:- host:test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.example.comhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080  Annotate the Ingress Resource\nThe annotation cert.gardener.cloud/purpose: managed instructs cert-controller-manager to handle certificate issuance for the domains found in labeled Ingress.\napiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:tls-example-ingressannotations:# Let Gardener manage certificates for this Ingress.cert.gardener.cloud/purpose:managed#dns.gardener.cloud/class: garden # needed on Gardener shoot clusters for managed DNS record creation (if not covered by `*.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.example.com)#cert.gardener.cloud/commonname: \u0026#34;*.demo.mydomain.com\u0026#34; # optional, if not specified the first name from spec.tls[].hosts is used as common name#cert.gardener.cloud/dnsnames: \u0026#34;\u0026#34; # optional, if not specified the names from spec.tls[].hosts are usedspec:tls:- hosts:- echoheaders.demo.mydomain.comsecretName:cert-echoheadersrules:- host:echoheaders.demo.mydomain.comhttp:paths:- backend:serviceName:echoheadersservicePort:80path:/The annotation cert.gardener.cloud/commonname can be set to explicitly specify the common name. If no set, the first name of spec.tls.hosts is used as common name. The annotation cert.gardener.cloud/dnsnames can be used to explicitly specify the alternative DNS names. If no set, the names of spec.tls.hosts are used.\n  Check status\nA certificate custom resource is created in the same namespace of the source cluster. You can either check the status of this certificate resource with kubectl get cert or you can check the events for the ingress with kubectl get events\nThe certificate is stored in the secret as specified in the Ingress resource.\n  Requesting a Certificate for Service If you have a service of type LoadBalancer, you can use the annotation cert.gardener.cloud/secretname together with the annotation dns.gardener.cloud/dnsnames from the dns-controller-manager to trigger automatic creation of a certificate. If you wan to share a certificate between multiple services and ingresses, using the annotations cert.gardener.cloud/commonname and cert.gardener.cloud/dnsnames may be helpful.\napiVersion:v1kind:Servicemetadata:annotations:cert.gardener.cloud/secretname:test-service-secretdns.gardener.cloud/dnsnames:test-service.demo.mydomain.com#dns.gardener.cloud/class: garden # needed on Gardener shoot clusters for managed DNS record creation#cert.gardener.cloud/commonname: \u0026#34;*.demo.mydomain.com\u0026#34; # optional, if not specified the first name from dns.gardener.cloud/dnsnames is used as common name#cert.gardener.cloud/dnsnames: \u0026#34;\u0026#34; # optional, if specified overrides dns.gardener.cloud/dnsnames annotation for certificate namesdns.gardener.cloud/ttl:\u0026#34;600\u0026#34;name:test-servicenamespace:defaultspec:ports:- name:httpport:80protocol:TCPtargetPort:8080type:LoadBalancerThe annotation cert.gardener.cloud/commonname is optional. If not specified, the first name of the annotation dns.gardener.cloud/dnsnames is used as common name. It is useful to specify it explicitly, if no DNSEntry should be created for the common name by the dns-controller-manager. A typical use case is if the common name (limited to 64 characters) is set only to deal with real domain names specified with dns.gardener.cloud/dnsnames which are longer than 64 characters. The annotation cert.gardener.cloud/dnsnames can be used to explicitly specify the alternative DNS names. If set, it overrides the values from the annotation dns.gardener.cloud/dnsnames for the certificate (but not for creating DNS records by the dns-controller-manager).\nIf you want to share a certificate between multiple services and ingresses, using the annotations cert.gardener.cloud/commonname and cert.gardener.cloud/dnsnames may be helpful. For example, to share a wildcard certificate, you should add these two annotations\ncert.gardener.cloud/commonname:\u0026#34;*.demo.mydomain.com\u0026#34;cert.gardener.cloud/dnsnames:\u0026#34;\u0026#34;This will create or reuse a certificate for *.demo.mydomain.com. An existing certificate is automatically reused, if it has exactly the same common name and DNS names.\nDemo quick start   Run dns-controller-manager with:\n./dns-controller-manager --controllers=azure-dns --identifier=myOwnerId --disable-namespace-restriction   Ensure provider and its secret, e.g.\nkubectl apply -f azure-secret.yaml kubectl apply -f azure-provider.yaml   check with\n kubectl get dnspr NAME TYPE STATUS AGE azure-playground azure-dns Ready 28m     Create test namespace\nkubectl create ns test   Run cert-controller-manager\n./cert-controller-manager   Register user some.user@mydomain.com at let\u0026rsquo;s encrypt\nkubectl apply -f examples/20-issuer-staging.yaml   check with\n kubectl get issuer NAME SERVER EMAIL STATUS TYPE AGE issuer-staging https://acme-staging-v02.api.letsencrypt.org/directory some.user@mydomain.com Ready acme 8s     Request a certificate for cert1.martin.test6227.ml\nkubectl apply -f examples/30-cert-simple.yaml If this certificate has been already registered for the same issuer before, it will be returned immediately from the ACME server. Otherwise a DNS challenge is started using a temporary DNSEntry to be set by dns-controller-manager\n  check with\n kubectl get cert -o wide NAME COMMON NAME ISSUER STATUS EXPIRATION_DATE DNS_NAMES AGE cert-simple cert1.mydomain.com issuer-staging Ready 2019-11-10T09:48:17Z [cert1.my-domain.com] 34s     Using the cert-controller-manager The cert-controller-manager communicated with up to four different clusters:\n default used for managing issuers and lease management. The path to the kubeconfig is specified with command line option --kubeconfig. source used for watching resources ingresses, services and certificates The path to the kubeconfig is specified with command line option --source. If option is omitted, the default cluster is used for source. dns used to write temporary DNSEntries for DNS challenges The path to the kubeconfig is specified with command line option --dns. If option is omitted, the default cluster is used for dns. target used for storing generated certificates The path to the kubeconfig is specified with command line option --target. If option is omitted, the source cluster is also used for target.  Usage The complete list of options is:\nUsage: cert-controller-manager [flags] Flags: --accepted-maintainers string accepted maintainer key(s) for crds --bind-address-http string HTTP server bind address --cascade-delete If true, certificate secrets are deleted if dependent resources (certificate, ingress) are deleted --cert-class string Identifier used to differentiate responsible controllers for entries --cert-target-class string Identifier used to differentiate responsible dns controllers for target entries --config string config file -c, --controllers string comma separated list of controllers to start (\u0026lt;name\u0026gt;,\u0026lt;group\u0026gt;,all) (default \u0026#34;all\u0026#34;) --cpuprofile string set file for cpu profiling --default-issuer string name of default issuer (from default cluster) --default-issuer-domain-ranges string domain range restrictions when using default issuer separated by comma --default-requests-per-day-quota int Default value for requestsPerDayQuota if not set explicitly in the issuer spec. --default.pool.resync-period duration Period for resynchronization for pool default --default.pool.size int Worker pool size for pool default --disable-namespace-restriction disable access restriction for namespace local access only --dns string cluster for writing challenge DNS entries --dns-class string class for creating challenge DNSEntries (in DNS cluster) --dns-namespace string namespace for creating challenge DNSEntries (in DNS cluster) --dns-owner-id string ownerId for creating challenge DNSEntries --dns.disable-deploy-crds disable deployment of required crds for cluster dns --dns.id string id for cluster dns --dns.migration-ids string migration id for cluster dns --force-crd-update enforce update of crds even they are unmanaged --grace-period duration inactivity grace period for detecting end of cleanup for shutdown -h, --help help for cert-controller-manager --ingress-cert.cert-class string Identifier used to differentiate responsible controllers for entries of controller ingress-cert (default \u0026#34;gardencert\u0026#34;) --ingress-cert.cert-target-class string Identifier used to differentiate responsible dns controllers for target entries of controller ingress-cert --ingress-cert.default.pool.resync-period duration Period for resynchronization for pool default of controller ingress-cert (default 2m0s) --ingress-cert.default.pool.size int Worker pool size for pool default of controller ingress-cert (default 2) --ingress-cert.pool.resync-period duration Period for resynchronization of controller ingress-cert --ingress-cert.pool.size int Worker pool size of controller ingress-cert --ingress-cert.target-name-prefix string name prefix in target namespace for cross cluster generation of controller ingress-cert --ingress-cert.target-namespace string target namespace for cross cluster generation of controller ingress-cert --ingress-cert.targets.pool.size int Worker pool size for pool targets of controller ingress-cert (default 2) --issuer-namespace string namespace to lookup issuers on default cluster --issuer.cascade-delete If true, certificate secrets are deleted if dependent resources (certificate, ingress) are deleted of controller issuer --issuer.cert-class string Identifier used to differentiate responsible controllers for entries of controller issuer --issuer.default-issuer string name of default issuer (from default cluster) of controller issuer (default \u0026#34;default-issuer\u0026#34;) --issuer.default-issuer-domain-ranges string domain range restrictions when using default issuer separated by comma of controller issuer --issuer.default-requests-per-day-quota int Default value for requestsPerDayQuota if not set explicitly in the issuer spec. of controller issuer (default 10000) --issuer.default.pool.resync-period duration Period for resynchronization for pool default of controller issuer (default 24h0m0s) --issuer.default.pool.size int Worker pool size for pool default of controller issuer (default 2) --issuer.dns-class string class for creating challenge DNSEntries (in DNS cluster) of controller issuer --issuer.dns-namespace string namespace for creating challenge DNSEntries (in DNS cluster) of controller issuer --issuer.dns-owner-id string ownerId for creating challenge DNSEntries of controller issuer --issuer.issuer-namespace string namespace to lookup issuers on default cluster of controller issuer (default \u0026#34;default\u0026#34;) --issuer.issuers.pool.size int Worker pool size for pool issuers of controller issuer (default 1) --issuer.pool.resync-period duration Period for resynchronization of controller issuer --issuer.pool.size int Worker pool size of controller issuer --issuer.precheck-additional-wait duration additional wait time after DNS propagation check of controller issuer (default 10s) --issuer.precheck-nameservers string DNS nameservers used for checking DNS propagation. If explicity set empty, it is tried to read them from /etc/resolv.conf of controller issuer (default \u0026#34;8.8.8.8:53,8.8.4.4:53\u0026#34;) --issuer.propagation-timeout duration propagation timeout for DNS challenge of controller issuer (default 1m0s) --issuer.renewal-overdue-window duration certificate is counted as \u0026#39;renewal overdue\u0026#39; if its validity period is shorter (metrics cert_management_overdue_renewal_certificates) of controller issuer (default 600h0m0s) --issuer.renewal-window duration certificate is renewed if its validity period is shorter of controller issuer (default 720h0m0s) --issuer.secrets.pool.size int Worker pool size for pool secrets of controller issuer (default 1) --issuers.pool.size int Worker pool size for pool issuers --kubeconfig string default cluster access --kubeconfig.disable-deploy-crds disable deployment of required crds for cluster default --kubeconfig.id string id for cluster default --kubeconfig.migration-ids string migration id for cluster default --lease-duration duration lease duration (default 15s) --lease-name string name for lease object --lease-renew-deadline duration lease renew deadline (default 10s) --lease-retry-period duration lease retry period (default 2s) -D, --log-level string logrus log level --maintainer string maintainer key for crds (default \u0026#34;cert-controller-manager\u0026#34;) --name string name used for controller manager (default \u0026#34;cert-controller-manager\u0026#34;) --namespace string namespace for lease (default \u0026#34;kube-system\u0026#34;) -n, --namespace-local-access-only enable access restriction for namespace local access only (deprecated) --omit-lease omit lease for development --plugin-file string directory containing go plugins --pool.resync-period duration Period for resynchronization --pool.size int Worker pool size --precheck-additional-wait duration additional wait time after DNS propagation check --precheck-nameservers string DNS nameservers used for checking DNS propagation. If explicity set empty, it is tried to read them from /etc/resolv.conf --propagation-timeout duration propagation timeout for DNS challenge --renewal-overdue-window duration certificate is counted as \u0026#39;renewal overdue\u0026#39; if its validity period is shorter (metrics cert_management_overdue_renewal_certificates) --renewal-window duration certificate is renewed if its validity period is shorter --secrets.pool.size int Worker pool size for pool secrets --server-port-http int HTTP server port (serving /healthz, /metrics, ...) --service-cert.cert-class string Identifier used to differentiate responsible controllers for entries of controller service-cert (default \u0026#34;gardencert\u0026#34;) --service-cert.cert-target-class string Identifier used to differentiate responsible dns controllers for target entries of controller service-cert --service-cert.default.pool.resync-period duration Period for resynchronization for pool default of controller service-cert (default 2m0s) --service-cert.default.pool.size int Worker pool size for pool default of controller service-cert (default 2) --service-cert.pool.resync-period duration Period for resynchronization of controller service-cert --service-cert.pool.size int Worker pool size of controller service-cert --service-cert.target-name-prefix string name prefix in target namespace for cross cluster generation of controller service-cert --service-cert.target-namespace string target namespace for cross cluster generation of controller service-cert --service-cert.targets.pool.size int Worker pool size for pool targets of controller service-cert (default 2) --source string source cluster to watch for ingresses and services --source.disable-deploy-crds disable deployment of required crds for cluster source --source.id string id for cluster source --source.migration-ids string migration id for cluster source --target string target cluster for certificates --target-name-prefix string name prefix in target namespace for cross cluster generation --target-namespace string target namespace for cross cluster generation --target.disable-deploy-crds disable deployment of required crds for cluster target --target.id string id for cluster target --target.migration-ids string migration id for cluster target --targets.pool.size int Worker pool size for pool targets -v, --version version for cert-controller-manager Development For development it is recommended to use the issuer-staging\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/networking/cert-managment/","title":"Certificates Management","tags":[],"description":"","content":"cert-management \nManages TLS certificates in Kubernetes clusters using custom resources.\nIn a multi-cluster environment like Gardener, using existing open source projects for certificate management like cert-manager becomes cumbersome. With this project the separation of concerns between multiple clusters is realized more easily. The cert-controller-manager runs in a secured cluster where the issuer secrets are stored. At the same time it watches an untrusted source cluster and can provide certificates for it. The cert-controller-manager relies on DNS challenges (ACME only) for validating the domain names of the certificates. For this purpose it creates DNSEntry custom resources (in a possible separate dns cluster) to be handled by the compagnion dns-controller-manager from external-dns-management.\nCurrently, the cert-controller-manager supports certificate authorities via:\n Automatic Certificate Management Environment (ACME) protocol like Let\u0026rsquo;s Encrypt. Certificate Authority (CA): an existing certificate and a private key provided as a TLS Secret.  Setting up Issuers Before you can obtain certificates from a certificate authority (CA), you need to set up an issuer. The issuer is specified in the default cluster, while the certificates are specified in the source cluster.\nThe issuer custom resource contains the configuration and registration data for your account at the CA.\nAutomatic Certificate Management Environment (ACME) Two modes are supported:\n auto registration using an existing account  Auto registration Auto registration is mainly used for development and test environments. You only need to provide the server URL and an email address. The registration process is done automatically for you by creating a private key and performing the registration at the CA. Optionally you can provide the target secret with the privateKeySecretRef section.\nFor example see examples/20-issuer-staging.yaml:\napiVersion:cert.gardener.cloud/v1alpha1kind:Issuermetadata:name:issuer-stagingnamespace:defaultspec:acme:server:https://acme-staging-v02.api.letsencrypt.org/directoryemail:some.user@mydomain.comautoRegistration:true# with \u0026#39;autoRegistration: true\u0026#39; a new account will be created if the secretRef is not existingprivateKeySecretRef:name:issuer-staging-secretnamespace:defaultUsing existing account If you already have an existing account at the certificate authority, you need to specify email address and reference the private key from a secret.\napiVersion:v1kind:Secretmetadata:name:my-issuer-secretnamespace:defaulttype:Opaquedata:privateKey:LS0tLS1...apiVersion:cert.gardener.cloud/v1alpha1kind:Issuermetadata:name:my-issuernamespace:defaultspec:acme:server:https://acme-v02.api.letsencrypt.org/directoryemail:my.account@mydomain.comprivateKeySecretRef:name:my-issuer-secretnamespace:defaultIn both cases, the state of an issuer resource can be checked on the default cluster with\n kubectl get issuer NAME SERVER EMAIL STATUS TYPE AGE issuer-staging https://acme-staging-v02.api.letsencrypt.org/directory some.user@mydomain.com Ready acme 8s Certificate Authority (CA) This issuer is meant to be used where a central Certificate Authority is already in place. The operator must request/provide by its own means a CA or an intermediate CA. This is mainly used for on-premises and airgapped environements.\nIt can also be used for developement or testing purproses. In this case a Self-signed Certificate Authority can be created by following the section below.\nCreate a Self-signed Certificate Authority (optional)\n openssl genrsa -out CA-key.pem 4096  export CONFIG=\u0026#34; [req] distinguished_name=dn [ dn ] [ ext ] basicConstraints=CA:TRUE,pathlen:0 \u0026#34;  openssl req \\  -new -nodes -x509 -config \u0026lt;(echo \u0026#34;$CONFIG\u0026#34;) -key CA-key.pem \\  -subj \u0026#34;/CN=Hello\u0026#34; -extensions ext -days 1000 -out CA-cert.pem Create a TLS secret from the certificate CA-cert.pem and the private key CA-key.pem\n kubectl -n default create secret tls issuer-ca-secret \\  --cert=CA-cert.pem --key=CA-key.pem -oyaml \\  --dry-run=client \u0026gt; secret.yaml The content of the secret.yaml should look like the following, for a full example see examples/20-issuer-ca.yaml\napiVersion:v1data:tls.crt:{base64certificate}tls.key:{base64privatekey}kind:Secretmetadata:name:issuer-ca-secrettype:kubernetes.io/tlsApply the secrets in the cluster and create the issuer, for example see examples/20-issuer-ca.yaml\n---apiVersion:cert.gardener.cloud/v1alpha1kind:Issuermetadata:name:issuer-canamespace:defaultspec:ca:privateKeySecretRef:name:issuer-ca-secretnamespace:defaultThe state of the issuer resource can be checked on the default cluster with\n kubectl get issuer NAME SERVER EMAIL STATUS TYPE AGE issuer-ca Ready ca 6s Some details about the CA can be found in the status of the issuer.\n kubectl get issuer issuer-ca -ojsonpath=\u0026#39;{.status}\u0026#39; | jq \u0026#39;.\u0026#39; { \u0026#34;ca\u0026#34;: { \u0026#34;NotAfter\u0026#34;: \u0026#34;2023-05-31T14:55:55Z\u0026#34;, \u0026#34;NotBefore\u0026#34;: \u0026#34;2020-09-03T14:55:55Z\u0026#34;, \u0026#34;Subject\u0026#34;: { \u0026#34;CommonName\u0026#34;: \u0026#34;my-domain.com\u0026#34;, \u0026#34;Country\u0026#34;: [ \u0026#34;DE\u0026#34; ], \u0026#34;Locality\u0026#34;: [ \u0026#34;Walldorf\u0026#34; ], \u0026#34;Organization\u0026#34;: [ \u0026#34;Gardener\u0026#34; ], \u0026#34;OrganizationalUnit\u0026#34;: [ \u0026#34;Gardener\u0026#34; ], \u0026#34;PostalCode\u0026#34;: null, \u0026#34;Province\u0026#34;: [ \u0026#34;BW\u0026#34; ], \u0026#34;SerialNumber\u0026#34;: \u0026#34;1E04A2C8F057AC890F45FEC5446AE4DDA73EA1D5\u0026#34;, \u0026#34;StreetAddress\u0026#34;: null } }, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;requestsPerDayQuota\u0026#34;: 10000, \u0026#34;state\u0026#34;: \u0026#34;Ready\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ca\u0026#34; } Requesting a Certificate To obtain a certificate for a domain, you specify a certificate custom resource on the source cluster. You can specify the issuer explicitly by reference. If there is no issuer reference, the default issuer is used (provided as command line option). You must either specify the commonName and further optional dnsNames or you can also start with a certificate signing request (CSR).\nFor domain validation, the cert-controller-manager only supports DNS challenges. For this purpose it relies on the dns-controller-manager from the external-dns-management project. If any domain name (commonName or any item from dnsNames) needs to be validated, it creates a custom resource DNSEntry in the dns cluster. When the certificate authority sees the temporary DNS record, the certificate is stored in a secret finally. The name of the secret can be specified explicitly with secretName and will be stored in the same namespace as the certificate on the source cluster.\nThe certificate is checked for renewal periodically. The renewal is performed automatically and the secret is updated. Default values for periodical check is daily, the certificate is renewed if its validity expires within 60 days.\nUsing commonName and optional dnsNames For example see examples/30-cert-simple.yaml:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-simplenamespace:defaultspec:commonName:cert1.mydomain.comdnsNames:- cert1-foo.mydomain.com- cert1-bar.mydomain.com# if issuer is not specified, the default issuer is usedissuerRef:name:issuer-stagingUsing a certificate signing request (CSR) You can provide a complete CSR in PEM format (and encoded as Base64).\nFor example see examples/30-cert-csr.yaml:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-csrnamespace:defaultspec:csr:LS0tLS1CRUd...issuerRef:name:issuer-staging:warning: Using a CSR is only available for ACME Issuer\nRequesting a Certificate for Ingress Add the annotation cert.gardener.cloud/purpose: managed to the Ingress resource. The cert-controller-manager will then automatically request a certificate for all domains given by the hosts in the tls section of the Ingress spec.\nFor compatibility with the Gardener Cert-Broker, you can alternatively use the deprecated label garden.sapcloud.io/purpose: managed-cert for the same outcome.\nSee also examples/40-ingress-echoheaders.yaml:\nProcess   Create the Ingress Resource (optional)\nIn order to request a certificate for a domain managed by cert-controller-manager an Ingress is required. In case you dont already have one, take the following as an example:\napiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:vuejs-ingressspec:tls:# Gardener managed default domain.# The first host is used as common name and must not exceed 64 characters- hosts:- test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.example.com# Certificate and private key reside in this secret.secretName:testsecret-tlsrules:- host:test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.example.comhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080  Annotate the Ingress Resource\nThe annotation cert.gardener.cloud/purpose: managed instructs cert-controller-manager to handle certificate issuance for the domains found in labeled Ingress.\napiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:tls-example-ingressannotations:# Let Gardener manage certificates for this Ingress.cert.gardener.cloud/purpose:managed#dns.gardener.cloud/class: garden # needed on Gardener shoot clusters for managed DNS record creation (if not covered by `*.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.example.com)#cert.gardener.cloud/commonname: \u0026#34;*.demo.mydomain.com\u0026#34; # optional, if not specified the first name from spec.tls[].hosts is used as common name#cert.gardener.cloud/dnsnames: \u0026#34;\u0026#34; # optional, if not specified the names from spec.tls[].hosts are usedspec:tls:- hosts:- echoheaders.demo.mydomain.comsecretName:cert-echoheadersrules:- host:echoheaders.demo.mydomain.comhttp:paths:- backend:serviceName:echoheadersservicePort:80path:/The annotation cert.gardener.cloud/commonname can be set to explicitly specify the common name. If no set, the first name of spec.tls.hosts is used as common name. The annotation cert.gardener.cloud/dnsnames can be used to explicitly specify the alternative DNS names. If no set, the names of spec.tls.hosts are used.\n  Check status\nA certificate custom resource is created in the same namespace of the source cluster. You can either check the status of this certificate resource with kubectl get cert or you can check the events for the ingress with kubectl get events\nThe certificate is stored in the secret as specified in the Ingress resource.\n  Requesting a Certificate for Service If you have a service of type LoadBalancer, you can use the annotation cert.gardener.cloud/secretname together with the annotation dns.gardener.cloud/dnsnames from the dns-controller-manager to trigger automatic creation of a certificate. If you wan to share a certificate between multiple services and ingresses, using the annotations cert.gardener.cloud/commonname and cert.gardener.cloud/dnsnames may be helpful.\napiVersion:v1kind:Servicemetadata:annotations:cert.gardener.cloud/secretname:test-service-secretdns.gardener.cloud/dnsnames:test-service.demo.mydomain.com#dns.gardener.cloud/class: garden # needed on Gardener shoot clusters for managed DNS record creation#cert.gardener.cloud/commonname: \u0026#34;*.demo.mydomain.com\u0026#34; # optional, if not specified the first name from dns.gardener.cloud/dnsnames is used as common name#cert.gardener.cloud/dnsnames: \u0026#34;\u0026#34; # optional, if specified overrides dns.gardener.cloud/dnsnames annotation for certificate namesdns.gardener.cloud/ttl:\u0026#34;600\u0026#34;name:test-servicenamespace:defaultspec:ports:- name:httpport:80protocol:TCPtargetPort:8080type:LoadBalancerThe annotation cert.gardener.cloud/commonname is optional. If not specified, the first name of the annotation dns.gardener.cloud/dnsnames is used as common name. It is useful to specify it explicitly, if no DNSEntry should be created for the common name by the dns-controller-manager. A typical use case is if the common name (limited to 64 characters) is set only to deal with real domain names specified with dns.gardener.cloud/dnsnames which are longer than 64 characters. The annotation cert.gardener.cloud/dnsnames can be used to explicitly specify the alternative DNS names. If set, it overrides the values from the annotation dns.gardener.cloud/dnsnames for the certificate (but not for creating DNS records by the dns-controller-manager).\nIf you want to share a certificate between multiple services and ingresses, using the annotations cert.gardener.cloud/commonname and cert.gardener.cloud/dnsnames may be helpful. For example, to share a wildcard certificate, you should add these two annotations\ncert.gardener.cloud/commonname:\u0026#34;*.demo.mydomain.com\u0026#34;cert.gardener.cloud/dnsnames:\u0026#34;\u0026#34;This will create or reuse a certificate for *.demo.mydomain.com. An existing certificate is automatically reused, if it has exactly the same common name and DNS names.\nDemo quick start   Run dns-controller-manager with:\n./dns-controller-manager --controllers=azure-dns --identifier=myOwnerId --disable-namespace-restriction   Ensure provider and its secret, e.g.\nkubectl apply -f azure-secret.yaml kubectl apply -f azure-provider.yaml   check with\n kubectl get dnspr NAME TYPE STATUS AGE azure-playground azure-dns Ready 28m     Create test namespace\nkubectl create ns test   Run cert-controller-manager\n./cert-controller-manager   Register user some.user@mydomain.com at let\u0026rsquo;s encrypt\nkubectl apply -f examples/20-issuer-staging.yaml   check with\n kubectl get issuer NAME SERVER EMAIL STATUS TYPE AGE issuer-staging https://acme-staging-v02.api.letsencrypt.org/directory some.user@mydomain.com Ready acme 8s     Request a certificate for cert1.martin.test6227.ml\nkubectl apply -f examples/30-cert-simple.yaml If this certificate has been already registered for the same issuer before, it will be returned immediately from the ACME server. Otherwise a DNS challenge is started using a temporary DNSEntry to be set by dns-controller-manager\n  check with\n kubectl get cert -o wide NAME COMMON NAME ISSUER STATUS EXPIRATION_DATE DNS_NAMES AGE cert-simple cert1.mydomain.com issuer-staging Ready 2019-11-10T09:48:17Z [cert1.my-domain.com] 34s     Using the cert-controller-manager The cert-controller-manager communicated with up to four different clusters:\n default used for managing issuers and lease management. The path to the kubeconfig is specified with command line option --kubeconfig. source used for watching resources ingresses, services and certificates The path to the kubeconfig is specified with command line option --source. If option is omitted, the default cluster is used for source. dns used to write temporary DNSEntries for DNS challenges The path to the kubeconfig is specified with command line option --dns. If option is omitted, the default cluster is used for dns. target used for storing generated certificates The path to the kubeconfig is specified with command line option --target. If option is omitted, the source cluster is also used for target.  Usage The complete list of options is:\nUsage: cert-controller-manager [flags] Flags: --accepted-maintainers string accepted maintainer key(s) for crds --bind-address-http string HTTP server bind address --cascade-delete If true, certificate secrets are deleted if dependent resources (certificate, ingress) are deleted --cert-class string Identifier used to differentiate responsible controllers for entries --cert-target-class string Identifier used to differentiate responsible dns controllers for target entries --config string config file -c, --controllers string comma separated list of controllers to start (\u0026lt;name\u0026gt;,\u0026lt;group\u0026gt;,all) (default \u0026#34;all\u0026#34;) --cpuprofile string set file for cpu profiling --default-issuer string name of default issuer (from default cluster) --default-issuer-domain-ranges string domain range restrictions when using default issuer separated by comma --default-requests-per-day-quota int Default value for requestsPerDayQuota if not set explicitly in the issuer spec. --default.pool.resync-period duration Period for resynchronization for pool default --default.pool.size int Worker pool size for pool default --disable-namespace-restriction disable access restriction for namespace local access only --dns string cluster for writing challenge DNS entries --dns-class string class for creating challenge DNSEntries (in DNS cluster) --dns-namespace string namespace for creating challenge DNSEntries (in DNS cluster) --dns-owner-id string ownerId for creating challenge DNSEntries --dns.disable-deploy-crds disable deployment of required crds for cluster dns --dns.id string id for cluster dns --dns.migration-ids string migration id for cluster dns --force-crd-update enforce update of crds even they are unmanaged --grace-period duration inactivity grace period for detecting end of cleanup for shutdown -h, --help help for cert-controller-manager --ingress-cert.cert-class string Identifier used to differentiate responsible controllers for entries of controller ingress-cert (default \u0026#34;gardencert\u0026#34;) --ingress-cert.cert-target-class string Identifier used to differentiate responsible dns controllers for target entries of controller ingress-cert --ingress-cert.default.pool.resync-period duration Period for resynchronization for pool default of controller ingress-cert (default 2m0s) --ingress-cert.default.pool.size int Worker pool size for pool default of controller ingress-cert (default 2) --ingress-cert.pool.resync-period duration Period for resynchronization of controller ingress-cert --ingress-cert.pool.size int Worker pool size of controller ingress-cert --ingress-cert.target-name-prefix string name prefix in target namespace for cross cluster generation of controller ingress-cert --ingress-cert.target-namespace string target namespace for cross cluster generation of controller ingress-cert --ingress-cert.targets.pool.size int Worker pool size for pool targets of controller ingress-cert (default 2) --issuer-namespace string namespace to lookup issuers on default cluster --issuer.cascade-delete If true, certificate secrets are deleted if dependent resources (certificate, ingress) are deleted of controller issuer --issuer.cert-class string Identifier used to differentiate responsible controllers for entries of controller issuer --issuer.default-issuer string name of default issuer (from default cluster) of controller issuer (default \u0026#34;default-issuer\u0026#34;) --issuer.default-issuer-domain-ranges string domain range restrictions when using default issuer separated by comma of controller issuer --issuer.default-requests-per-day-quota int Default value for requestsPerDayQuota if not set explicitly in the issuer spec. of controller issuer (default 10000) --issuer.default.pool.resync-period duration Period for resynchronization for pool default of controller issuer (default 24h0m0s) --issuer.default.pool.size int Worker pool size for pool default of controller issuer (default 2) --issuer.dns-class string class for creating challenge DNSEntries (in DNS cluster) of controller issuer --issuer.dns-namespace string namespace for creating challenge DNSEntries (in DNS cluster) of controller issuer --issuer.dns-owner-id string ownerId for creating challenge DNSEntries of controller issuer --issuer.issuer-namespace string namespace to lookup issuers on default cluster of controller issuer (default \u0026#34;default\u0026#34;) --issuer.issuers.pool.size int Worker pool size for pool issuers of controller issuer (default 1) --issuer.pool.resync-period duration Period for resynchronization of controller issuer --issuer.pool.size int Worker pool size of controller issuer --issuer.precheck-additional-wait duration additional wait time after DNS propagation check of controller issuer (default 10s) --issuer.precheck-nameservers string DNS nameservers used for checking DNS propagation. If explicity set empty, it is tried to read them from /etc/resolv.conf of controller issuer (default \u0026#34;8.8.8.8:53,8.8.4.4:53\u0026#34;) --issuer.propagation-timeout duration propagation timeout for DNS challenge of controller issuer (default 1m0s) --issuer.renewal-overdue-window duration certificate is counted as \u0026#39;renewal overdue\u0026#39; if its validity period is shorter (metrics cert_management_overdue_renewal_certificates) of controller issuer (default 600h0m0s) --issuer.renewal-window duration certificate is renewed if its validity period is shorter of controller issuer (default 720h0m0s) --issuer.secrets.pool.size int Worker pool size for pool secrets of controller issuer (default 1) --issuers.pool.size int Worker pool size for pool issuers --kubeconfig string default cluster access --kubeconfig.disable-deploy-crds disable deployment of required crds for cluster default --kubeconfig.id string id for cluster default --kubeconfig.migration-ids string migration id for cluster default --lease-duration duration lease duration (default 15s) --lease-name string name for lease object --lease-renew-deadline duration lease renew deadline (default 10s) --lease-retry-period duration lease retry period (default 2s) -D, --log-level string logrus log level --maintainer string maintainer key for crds (default \u0026#34;cert-controller-manager\u0026#34;) --name string name used for controller manager (default \u0026#34;cert-controller-manager\u0026#34;) --namespace string namespace for lease (default \u0026#34;kube-system\u0026#34;) -n, --namespace-local-access-only enable access restriction for namespace local access only (deprecated) --omit-lease omit lease for development --plugin-file string directory containing go plugins --pool.resync-period duration Period for resynchronization --pool.size int Worker pool size --precheck-additional-wait duration additional wait time after DNS propagation check --precheck-nameservers string DNS nameservers used for checking DNS propagation. If explicity set empty, it is tried to read them from /etc/resolv.conf --propagation-timeout duration propagation timeout for DNS challenge --renewal-overdue-window duration certificate is counted as \u0026#39;renewal overdue\u0026#39; if its validity period is shorter (metrics cert_management_overdue_renewal_certificates) --renewal-window duration certificate is renewed if its validity period is shorter --secrets.pool.size int Worker pool size for pool secrets --server-port-http int HTTP server port (serving /healthz, /metrics, ...) --service-cert.cert-class string Identifier used to differentiate responsible controllers for entries of controller service-cert (default \u0026#34;gardencert\u0026#34;) --service-cert.cert-target-class string Identifier used to differentiate responsible dns controllers for target entries of controller service-cert --service-cert.default.pool.resync-period duration Period for resynchronization for pool default of controller service-cert (default 2m0s) --service-cert.default.pool.size int Worker pool size for pool default of controller service-cert (default 2) --service-cert.pool.resync-period duration Period for resynchronization of controller service-cert --service-cert.pool.size int Worker pool size of controller service-cert --service-cert.target-name-prefix string name prefix in target namespace for cross cluster generation of controller service-cert --service-cert.target-namespace string target namespace for cross cluster generation of controller service-cert --service-cert.targets.pool.size int Worker pool size for pool targets of controller service-cert (default 2) --source string source cluster to watch for ingresses and services --source.disable-deploy-crds disable deployment of required crds for cluster source --source.id string id for cluster source --source.migration-ids string migration id for cluster source --target string target cluster for certificates --target-name-prefix string name prefix in target namespace for cross cluster generation --target-namespace string target namespace for cross cluster generation --target.disable-deploy-crds disable deployment of required crds for cluster target --target.id string id for cluster target --target.migration-ids string migration id for cluster target --targets.pool.size int Worker pool size for pool targets -v, --version version for cert-controller-manager Development For development it is recommended to use the issuer-staging\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/networking/cert-managment/","title":"Certificates Management","tags":[],"description":"","content":"cert-management \nManages TLS certificates in Kubernetes clusters using custom resources.\nIn a multi-cluster environment like Gardener, using existing open source projects for certificate management like cert-manager becomes cumbersome. With this project the separation of concerns between multiple clusters is realized more easily. The cert-controller-manager runs in a secured cluster where the issuer secrets are stored. At the same time it watches an untrusted source cluster and can provide certificates for it. The cert-controller-manager relies on DNS challenges (ACME only) for validating the domain names of the certificates. For this purpose it creates DNSEntry custom resources (in a possible separate dns cluster) to be handled by the compagnion dns-controller-manager from external-dns-management.\nCurrently, the cert-controller-manager supports certificate authorities via:\n Automatic Certificate Management Environment (ACME) protocol like Let\u0026rsquo;s Encrypt. Certificate Authority (CA): an existing certificate and a private key provided as a TLS Secret.  Setting up Issuers Before you can obtain certificates from a certificate authority (CA), you need to set up an issuer. The issuer is specified in the default cluster, while the certificates are specified in the source cluster.\nThe issuer custom resource contains the configuration and registration data for your account at the CA.\nAutomatic Certificate Management Environment (ACME) Two modes are supported:\n auto registration using an existing account  Auto registration Auto registration is mainly used for development and test environments. You only need to provide the server URL and an email address. The registration process is done automatically for you by creating a private key and performing the registration at the CA. Optionally you can provide the target secret with the privateKeySecretRef section.\nFor example see examples/20-issuer-staging.yaml:\napiVersion:cert.gardener.cloud/v1alpha1kind:Issuermetadata:name:issuer-stagingnamespace:defaultspec:acme:server:https://acme-staging-v02.api.letsencrypt.org/directoryemail:some.user@mydomain.comautoRegistration:true# with \u0026#39;autoRegistration: true\u0026#39; a new account will be created if the secretRef is not existingprivateKeySecretRef:name:issuer-staging-secretnamespace:defaultUsing existing account If you already have an existing account at the certificate authority, you need to specify email address and reference the private key from a secret.\napiVersion:v1kind:Secretmetadata:name:my-issuer-secretnamespace:defaulttype:Opaquedata:privateKey:LS0tLS1...apiVersion:cert.gardener.cloud/v1alpha1kind:Issuermetadata:name:my-issuernamespace:defaultspec:acme:server:https://acme-v02.api.letsencrypt.org/directoryemail:my.account@mydomain.comprivateKeySecretRef:name:my-issuer-secretnamespace:defaultIn both cases, the state of an issuer resource can be checked on the default cluster with\n kubectl get issuer NAME SERVER EMAIL STATUS TYPE AGE issuer-staging https://acme-staging-v02.api.letsencrypt.org/directory some.user@mydomain.com Ready acme 8s Certificate Authority (CA) This issuer is meant to be used where a central Certificate Authority is already in place. The operator must request/provide by its own means a CA or an intermediate CA. This is mainly used for on-premises and airgapped environements.\nIt can also be used for developement or testing purproses. In this case a Self-signed Certificate Authority can be created by following the section below.\nCreate a Self-signed Certificate Authority (optional)\n openssl genrsa -out CA-key.pem 4096  export CONFIG=\u0026#34; [req] distinguished_name=dn [ dn ] [ ext ] basicConstraints=CA:TRUE,pathlen:0 \u0026#34;  openssl req \\  -new -nodes -x509 -config \u0026lt;(echo \u0026#34;$CONFIG\u0026#34;) -key CA-key.pem \\  -subj \u0026#34;/CN=Hello\u0026#34; -extensions ext -days 1000 -out CA-cert.pem Create a TLS secret from the certificate CA-cert.pem and the private key CA-key.pem\n kubectl -n default create secret tls issuer-ca-secret \\  --cert=CA-cert.pem --key=CA-key.pem -oyaml \\  --dry-run=client \u0026gt; secret.yaml The content of the secret.yaml should look like the following, for a full example see examples/20-issuer-ca.yaml\napiVersion:v1data:tls.crt:{base64certificate}tls.key:{base64privatekey}kind:Secretmetadata:name:issuer-ca-secrettype:kubernetes.io/tlsApply the secrets in the cluster and create the issuer, for example see examples/20-issuer-ca.yaml\n---apiVersion:cert.gardener.cloud/v1alpha1kind:Issuermetadata:name:issuer-canamespace:defaultspec:ca:privateKeySecretRef:name:issuer-ca-secretnamespace:defaultThe state of the issuer resource can be checked on the default cluster with\n kubectl get issuer NAME SERVER EMAIL STATUS TYPE AGE issuer-ca Ready ca 6s Some details about the CA can be found in the status of the issuer.\n kubectl get issuer issuer-ca -ojsonpath=\u0026#39;{.status}\u0026#39; | jq \u0026#39;.\u0026#39; { \u0026#34;ca\u0026#34;: { \u0026#34;NotAfter\u0026#34;: \u0026#34;2023-05-31T14:55:55Z\u0026#34;, \u0026#34;NotBefore\u0026#34;: \u0026#34;2020-09-03T14:55:55Z\u0026#34;, \u0026#34;Subject\u0026#34;: { \u0026#34;CommonName\u0026#34;: \u0026#34;my-domain.com\u0026#34;, \u0026#34;Country\u0026#34;: [ \u0026#34;DE\u0026#34; ], \u0026#34;Locality\u0026#34;: [ \u0026#34;Walldorf\u0026#34; ], \u0026#34;Organization\u0026#34;: [ \u0026#34;Gardener\u0026#34; ], \u0026#34;OrganizationalUnit\u0026#34;: [ \u0026#34;Gardener\u0026#34; ], \u0026#34;PostalCode\u0026#34;: null, \u0026#34;Province\u0026#34;: [ \u0026#34;BW\u0026#34; ], \u0026#34;SerialNumber\u0026#34;: \u0026#34;1E04A2C8F057AC890F45FEC5446AE4DDA73EA1D5\u0026#34;, \u0026#34;StreetAddress\u0026#34;: null } }, \u0026#34;observedGeneration\u0026#34;: 1, \u0026#34;requestsPerDayQuota\u0026#34;: 10000, \u0026#34;state\u0026#34;: \u0026#34;Ready\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ca\u0026#34; } Requesting a Certificate To obtain a certificate for a domain, you specify a certificate custom resource on the source cluster. You can specify the issuer explicitly by reference. If there is no issuer reference, the default issuer is used (provided as command line option). You must either specify the commonName and further optional dnsNames or you can also start with a certificate signing request (CSR).\nFor domain validation, the cert-controller-manager only supports DNS challenges. For this purpose it relies on the dns-controller-manager from the external-dns-management project. If any domain name (commonName or any item from dnsNames) needs to be validated, it creates a custom resource DNSEntry in the dns cluster. When the certificate authority sees the temporary DNS record, the certificate is stored in a secret finally. The name of the secret can be specified explicitly with secretName and will be stored in the same namespace as the certificate on the source cluster.\nThe certificate is checked for renewal periodically. The renewal is performed automatically and the secret is updated. Default values for periodical check is daily, the certificate is renewed if its validity expires within 60 days.\nUsing commonName and optional dnsNames For example see examples/30-cert-simple.yaml:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-simplenamespace:defaultspec:commonName:cert1.mydomain.comdnsNames:- cert1-foo.mydomain.com- cert1-bar.mydomain.com# if issuer is not specified, the default issuer is usedissuerRef:name:issuer-stagingUsing a certificate signing request (CSR) You can provide a complete CSR in PEM format (and encoded as Base64).\nFor example see examples/30-cert-csr.yaml:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-csrnamespace:defaultspec:csr:LS0tLS1CRUd...issuerRef:name:issuer-staging:warning: Using a CSR is only available for ACME Issuer\nRequesting a Certificate for Ingress Add the annotation cert.gardener.cloud/purpose: managed to the Ingress resource. The cert-controller-manager will then automatically request a certificate for all domains given by the hosts in the tls section of the Ingress spec.\nFor compatibility with the Gardener Cert-Broker, you can alternatively use the deprecated label garden.sapcloud.io/purpose: managed-cert for the same outcome.\nSee also examples/40-ingress-echoheaders.yaml:\nProcess   Create the Ingress Resource (optional)\nIn order to request a certificate for a domain managed by cert-controller-manager an Ingress is required. In case you dont already have one, take the following as an example:\napiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:vuejs-ingressspec:tls:# Gardener managed default domain.# The first host is used as common name and must not exceed 64 characters- hosts:- test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.example.com# Certificate and private key reside in this secret.secretName:testsecret-tlsrules:- host:test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.example.comhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080  Annotate the Ingress Resource\nThe annotation cert.gardener.cloud/purpose: managed instructs cert-controller-manager to handle certificate issuance for the domains found in labeled Ingress.\napiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:tls-example-ingressannotations:# Let Gardener manage certificates for this Ingress.cert.gardener.cloud/purpose:managed#dns.gardener.cloud/class: garden # needed on Gardener shoot clusters for managed DNS record creation (if not covered by `*.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.example.com)#cert.gardener.cloud/commonname: \u0026#34;*.demo.mydomain.com\u0026#34; # optional, if not specified the first name from spec.tls[].hosts is used as common name#cert.gardener.cloud/dnsnames: \u0026#34;\u0026#34; # optional, if not specified the names from spec.tls[].hosts are usedspec:tls:- hosts:- echoheaders.demo.mydomain.comsecretName:cert-echoheadersrules:- host:echoheaders.demo.mydomain.comhttp:paths:- backend:serviceName:echoheadersservicePort:80path:/The annotation cert.gardener.cloud/commonname can be set to explicitly specify the common name. If no set, the first name of spec.tls.hosts is used as common name. The annotation cert.gardener.cloud/dnsnames can be used to explicitly specify the alternative DNS names. If no set, the names of spec.tls.hosts are used.\n  Check status\nA certificate custom resource is created in the same namespace of the source cluster. You can either check the status of this certificate resource with kubectl get cert or you can check the events for the ingress with kubectl get events\nThe certificate is stored in the secret as specified in the Ingress resource.\n  Requesting a Certificate for Service If you have a service of type LoadBalancer, you can use the annotation cert.gardener.cloud/secretname together with the annotation dns.gardener.cloud/dnsnames from the dns-controller-manager to trigger automatic creation of a certificate. If you wan to share a certificate between multiple services and ingresses, using the annotations cert.gardener.cloud/commonname and cert.gardener.cloud/dnsnames may be helpful.\napiVersion:v1kind:Servicemetadata:annotations:cert.gardener.cloud/secretname:test-service-secretdns.gardener.cloud/dnsnames:test-service.demo.mydomain.com#dns.gardener.cloud/class: garden # needed on Gardener shoot clusters for managed DNS record creation#cert.gardener.cloud/commonname: \u0026#34;*.demo.mydomain.com\u0026#34; # optional, if not specified the first name from dns.gardener.cloud/dnsnames is used as common name#cert.gardener.cloud/dnsnames: \u0026#34;\u0026#34; # optional, if specified overrides dns.gardener.cloud/dnsnames annotation for certificate namesdns.gardener.cloud/ttl:\u0026#34;600\u0026#34;name:test-servicenamespace:defaultspec:ports:- name:httpport:80protocol:TCPtargetPort:8080type:LoadBalancerThe annotation cert.gardener.cloud/commonname is optional. If not specified, the first name of the annotation dns.gardener.cloud/dnsnames is used as common name. It is useful to specify it explicitly, if no DNSEntry should be created for the common name by the dns-controller-manager. A typical use case is if the common name (limited to 64 characters) is set only to deal with real domain names specified with dns.gardener.cloud/dnsnames which are longer than 64 characters. The annotation cert.gardener.cloud/dnsnames can be used to explicitly specify the alternative DNS names. If set, it overrides the values from the annotation dns.gardener.cloud/dnsnames for the certificate (but not for creating DNS records by the dns-controller-manager).\nIf you want to share a certificate between multiple services and ingresses, using the annotations cert.gardener.cloud/commonname and cert.gardener.cloud/dnsnames may be helpful. For example, to share a wildcard certificate, you should add these two annotations\ncert.gardener.cloud/commonname:\u0026#34;*.demo.mydomain.com\u0026#34;cert.gardener.cloud/dnsnames:\u0026#34;\u0026#34;This will create or reuse a certificate for *.demo.mydomain.com. An existing certificate is automatically reused, if it has exactly the same common name and DNS names.\nDemo quick start   Run dns-controller-manager with:\n./dns-controller-manager --controllers=azure-dns --identifier=myOwnerId --disable-namespace-restriction   Ensure provider and its secret, e.g.\nkubectl apply -f azure-secret.yaml kubectl apply -f azure-provider.yaml   check with\n kubectl get dnspr NAME TYPE STATUS AGE azure-playground azure-dns Ready 28m     Create test namespace\nkubectl create ns test   Run cert-controller-manager\n./cert-controller-manager   Register user some.user@mydomain.com at let\u0026rsquo;s encrypt\nkubectl apply -f examples/20-issuer-staging.yaml   check with\n kubectl get issuer NAME SERVER EMAIL STATUS TYPE AGE issuer-staging https://acme-staging-v02.api.letsencrypt.org/directory some.user@mydomain.com Ready acme 8s     Request a certificate for cert1.martin.test6227.ml\nkubectl apply -f examples/30-cert-simple.yaml If this certificate has been already registered for the same issuer before, it will be returned immediately from the ACME server. Otherwise a DNS challenge is started using a temporary DNSEntry to be set by dns-controller-manager\n  check with\n kubectl get cert -o wide NAME COMMON NAME ISSUER STATUS EXPIRATION_DATE DNS_NAMES AGE cert-simple cert1.mydomain.com issuer-staging Ready 2019-11-10T09:48:17Z [cert1.my-domain.com] 34s     Using the cert-controller-manager The cert-controller-manager communicated with up to four different clusters:\n default used for managing issuers and lease management. The path to the kubeconfig is specified with command line option --kubeconfig. source used for watching resources ingresses, services and certificates The path to the kubeconfig is specified with command line option --source. If option is omitted, the default cluster is used for source. dns used to write temporary DNSEntries for DNS challenges The path to the kubeconfig is specified with command line option --dns. If option is omitted, the default cluster is used for dns. target used for storing generated certificates The path to the kubeconfig is specified with command line option --target. If option is omitted, the source cluster is also used for target.  Usage The complete list of options is:\nUsage: cert-controller-manager [flags] Flags: --accepted-maintainers string accepted maintainer key(s) for crds --bind-address-http string HTTP server bind address --cascade-delete If true, certificate secrets are deleted if dependent resources (certificate, ingress) are deleted --cert-class string Identifier used to differentiate responsible controllers for entries --cert-target-class string Identifier used to differentiate responsible dns controllers for target entries --config string config file -c, --controllers string comma separated list of controllers to start (\u0026lt;name\u0026gt;,\u0026lt;group\u0026gt;,all) (default \u0026#34;all\u0026#34;) --cpuprofile string set file for cpu profiling --default-issuer string name of default issuer (from default cluster) --default-issuer-domain-ranges string domain range restrictions when using default issuer separated by comma --default-requests-per-day-quota int Default value for requestsPerDayQuota if not set explicitly in the issuer spec. --default.pool.resync-period duration Period for resynchronization for pool default --default.pool.size int Worker pool size for pool default --disable-namespace-restriction disable access restriction for namespace local access only --dns string cluster for writing challenge DNS entries --dns-class string class for creating challenge DNSEntries (in DNS cluster) --dns-namespace string namespace for creating challenge DNSEntries (in DNS cluster) --dns-owner-id string ownerId for creating challenge DNSEntries --dns.disable-deploy-crds disable deployment of required crds for cluster dns --dns.id string id for cluster dns --dns.migration-ids string migration id for cluster dns --force-crd-update enforce update of crds even they are unmanaged --grace-period duration inactivity grace period for detecting end of cleanup for shutdown -h, --help help for cert-controller-manager --ingress-cert.cert-class string Identifier used to differentiate responsible controllers for entries of controller ingress-cert (default \u0026#34;gardencert\u0026#34;) --ingress-cert.cert-target-class string Identifier used to differentiate responsible dns controllers for target entries of controller ingress-cert --ingress-cert.default.pool.resync-period duration Period for resynchronization for pool default of controller ingress-cert (default 2m0s) --ingress-cert.default.pool.size int Worker pool size for pool default of controller ingress-cert (default 2) --ingress-cert.pool.resync-period duration Period for resynchronization of controller ingress-cert --ingress-cert.pool.size int Worker pool size of controller ingress-cert --ingress-cert.target-name-prefix string name prefix in target namespace for cross cluster generation of controller ingress-cert --ingress-cert.target-namespace string target namespace for cross cluster generation of controller ingress-cert --ingress-cert.targets.pool.size int Worker pool size for pool targets of controller ingress-cert (default 2) --issuer-namespace string namespace to lookup issuers on default cluster --issuer.cascade-delete If true, certificate secrets are deleted if dependent resources (certificate, ingress) are deleted of controller issuer --issuer.cert-class string Identifier used to differentiate responsible controllers for entries of controller issuer --issuer.default-issuer string name of default issuer (from default cluster) of controller issuer (default \u0026#34;default-issuer\u0026#34;) --issuer.default-issuer-domain-ranges string domain range restrictions when using default issuer separated by comma of controller issuer --issuer.default-requests-per-day-quota int Default value for requestsPerDayQuota if not set explicitly in the issuer spec. of controller issuer (default 10000) --issuer.default.pool.resync-period duration Period for resynchronization for pool default of controller issuer (default 24h0m0s) --issuer.default.pool.size int Worker pool size for pool default of controller issuer (default 2) --issuer.dns-class string class for creating challenge DNSEntries (in DNS cluster) of controller issuer --issuer.dns-namespace string namespace for creating challenge DNSEntries (in DNS cluster) of controller issuer --issuer.dns-owner-id string ownerId for creating challenge DNSEntries of controller issuer --issuer.issuer-namespace string namespace to lookup issuers on default cluster of controller issuer (default \u0026#34;default\u0026#34;) --issuer.issuers.pool.size int Worker pool size for pool issuers of controller issuer (default 1) --issuer.pool.resync-period duration Period for resynchronization of controller issuer --issuer.pool.size int Worker pool size of controller issuer --issuer.precheck-additional-wait duration additional wait time after DNS propagation check of controller issuer (default 10s) --issuer.precheck-nameservers string DNS nameservers used for checking DNS propagation. If explicity set empty, it is tried to read them from /etc/resolv.conf of controller issuer (default \u0026#34;8.8.8.8:53,8.8.4.4:53\u0026#34;) --issuer.propagation-timeout duration propagation timeout for DNS challenge of controller issuer (default 1m0s) --issuer.renewal-overdue-window duration certificate is counted as \u0026#39;renewal overdue\u0026#39; if its validity period is shorter (metrics cert_management_overdue_renewal_certificates) of controller issuer (default 600h0m0s) --issuer.renewal-window duration certificate is renewed if its validity period is shorter of controller issuer (default 720h0m0s) --issuer.secrets.pool.size int Worker pool size for pool secrets of controller issuer (default 1) --issuers.pool.size int Worker pool size for pool issuers --kubeconfig string default cluster access --kubeconfig.disable-deploy-crds disable deployment of required crds for cluster default --kubeconfig.id string id for cluster default --kubeconfig.migration-ids string migration id for cluster default --lease-duration duration lease duration (default 15s) --lease-name string name for lease object --lease-renew-deadline duration lease renew deadline (default 10s) --lease-retry-period duration lease retry period (default 2s) -D, --log-level string logrus log level --maintainer string maintainer key for crds (default \u0026#34;cert-controller-manager\u0026#34;) --name string name used for controller manager (default \u0026#34;cert-controller-manager\u0026#34;) --namespace string namespace for lease (default \u0026#34;kube-system\u0026#34;) -n, --namespace-local-access-only enable access restriction for namespace local access only (deprecated) --omit-lease omit lease for development --plugin-file string directory containing go plugins --pool.resync-period duration Period for resynchronization --pool.size int Worker pool size --precheck-additional-wait duration additional wait time after DNS propagation check --precheck-nameservers string DNS nameservers used for checking DNS propagation. If explicity set empty, it is tried to read them from /etc/resolv.conf --propagation-timeout duration propagation timeout for DNS challenge --renewal-overdue-window duration certificate is counted as \u0026#39;renewal overdue\u0026#39; if its validity period is shorter (metrics cert_management_overdue_renewal_certificates) --renewal-window duration certificate is renewed if its validity period is shorter --secrets.pool.size int Worker pool size for pool secrets --server-port-http int HTTP server port (serving /healthz, /metrics, ...) --service-cert.cert-class string Identifier used to differentiate responsible controllers for entries of controller service-cert (default \u0026#34;gardencert\u0026#34;) --service-cert.cert-target-class string Identifier used to differentiate responsible dns controllers for target entries of controller service-cert --service-cert.default.pool.resync-period duration Period for resynchronization for pool default of controller service-cert (default 2m0s) --service-cert.default.pool.size int Worker pool size for pool default of controller service-cert (default 2) --service-cert.pool.resync-period duration Period for resynchronization of controller service-cert --service-cert.pool.size int Worker pool size of controller service-cert --service-cert.target-name-prefix string name prefix in target namespace for cross cluster generation of controller service-cert --service-cert.target-namespace string target namespace for cross cluster generation of controller service-cert --service-cert.targets.pool.size int Worker pool size for pool targets of controller service-cert (default 2) --source string source cluster to watch for ingresses and services --source.disable-deploy-crds disable deployment of required crds for cluster source --source.id string id for cluster source --source.migration-ids string migration id for cluster source --target string target cluster for certificates --target-name-prefix string name prefix in target namespace for cross cluster generation --target-namespace string target namespace for cross cluster generation --target.disable-deploy-crds disable deployment of required crds for cluster target --target.id string id for cluster target --target.migration-ids string migration id for cluster target --targets.pool.size int Worker pool size for pool targets -v, --version version for cert-controller-manager Development For development it is recommended to use the issuer-staging\n"},{"uri":"https://gardener.cloud/documentation/concepts/core-components/scheduler/","title":"Gardener Scheduler","tags":[],"description":"","content":"Gardener Scheduler The Gardener Scheduler is in essence a controller that watches newly created shoots and assigns a seed cluster to them. Conceptually, the task of the Gardener Scheduler is very similar to the task of the Kubernetes Scheduler: finding a seed for a shoot instead of a node for a pod.\nEither the scheduling strategy or the shoot cluster purpose hereby determines how the scheduler is operating. The following sections explain the configuration and flow in greater detail.\nWhy is the Gardener Scheduler needed? 1. Decoupling Previously, an admission plugin in the Gardener API server conducted the scheduling decisions. This implies changes to the API server whenever adjustments of the scheduling are needed. Decoupling the API server and the scheduler comes with greater flexibility to develop these components independently from each other.\n2. Extensibility It should be possible to easily extend and tweak the scheduler in the future. Possibly, similar to the Kubernetes scheduler, hooks could be provided which influence the scheduling decisions. It should be also possible to completely replace the standard Gardener Scheduler with a custom implementation.\nAlgorithm overview The following sequence describes the steps involved to determine a seed candidate:\n Determine usable seeds with \u0026ldquo;usable\u0026rdquo; defined as follows:  no .metadata.deletionTimestamp .spec.settings.scheduling.visible is true conditions Bootstrapped, GardenletReady are true   Filter seeds:  matching .spec.seedSelector in CloudProfile used by the Shoot matching .spec.seedSelector in Shoot having no network intersection with the Shoot's networks (due to the VPN connectivity between seeds and shoots their networks must be disjoint) having .spec.settings.shootDNS.enabled=false (only if the shoot specifies a DNS domain or does not use the unmanaged DNS provider) whose taints (.spec.taints) are tolerated by the Shoot (.spec.tolerations) whose capacity for shoots would not be exceeded if the shoot is scheduled onto the seed, see Ensuring seeds capacity for shoots is not exceeded   Apply active strategy e.g., Minimal Distance strategy Choose least utilized seed, i.e., the one with the least number of shoot control planes, will be the winner and written to the .spec.seedName field of the Shoot.  Configuration The Gardener Scheduler configuration has to be supplied on startup. It is a mandatory and also the only available flag. Here is an example scheduler configuration.\nMost of the configuration options are the same as in the Gardener Controller Manager (leader election, client connection, \u0026hellip;). However, the Gardener Scheduler on the other hand does not need a TLS configuration, because there are currently no webhooks configurable.\nStrategies The scheduling strategy is defined in the candidateDeterminationStrategy of the scheduler\u0026rsquo;s configuration and can have the possible values SameRegion and MinimalDistance. The SameRegion strategy is the default strategy.\n  Same Region strategy\nThe Gardener Scheduler reads the spec.provider.type and .spec.region fields from the Shoot resource. It tries to find a seed that has the identical .spec.provider.type and .spec.provider.region fields set. If it cannot find a suitable seed, it adds an event to the shoot stating, that it is unschedulable.\n  Minimal Distance strategy\nThe Gardener Scheduler tries to find a valid seed with minimal distance to the shoot\u0026rsquo;s intended region. The distance is calculated based on the Levenshtein distance of the region. Therefore the region name is split into a base name and an orientation. Possible orientations are north, south, east, west and central. The distance then is twice the Levenshtein distance of the region\u0026rsquo;s base name plus a correction value based on the orientation and the provider.\nIf the orientations of shoot and seed candidate match, the correction value is 0, if they differ it is 2 and if either the seed\u0026rsquo;s or the shoot\u0026rsquo;s region does not have an orientation it is 1. If the provider differs the correction value is additionally incremented by 2.\nBecause of this a matching region with a matching provider is always prefered.\n  In order to put the scheduling decision into effect, the scheduler sends an update request for the Shoot resource to the API server. After validation, the Gardener Aggregated API server updates the shoot to have the spec.seedName field set. Subsequently, the Gardenlet picks up and starts to create the cluster on the specified seed.\nSpecial handling based on shoot cluster purpose  Every shoot cluster can have a purpose that describes what the cluster is used for, and also influences how the cluster is setup (see this document for more information).\nIn case the shoot has the testing purpose then the scheduler only reads the .spec.provider.type from the Shoot resource and tries to find a Seed that has the identical .spec.provider.type. The region does not matter, i.e., testing shoots may also be scheduled on a seed in a complete different region if it is better for balancing the whole Gardener system.\nseedSelector field in the Shoot specification Similar to the .spec.nodeSelector field in Pods, the Shoot specification has an optional .spec.seedSelector field. It allows the user to provide a label selector that must match the labels of Seeds in order to be scheduled to one of them. The labels on Seeds are usually controlled by Gardener administrators/operators - end users cannot add arbitrary labels themselves. If provided, the Gardener Scheduler will only consider those seeds as \u0026ldquo;suitable\u0026rdquo; whose labels match those provided in the .spec.seedSelector of the Shoot.\nBy default only seeds with the same provider than the shoot are selected. By adding a providerTypes field to the seedSelector a dedicated set of possible providers (* means all provider types) can be selected.\nEnsuring seeds capacity for shoots is not exceeded Seeds have a practical limit of how many shoots they can accommodate. Exceeding this limit is undesirable as the system performance will be noticeably impacted. Therefore, the scheduler ensures that a seed\u0026rsquo;s capacity for shoots is not exceeded by taking into account a maximum number of shoots that can be scheduled onto a seed.\nThis mechanism works as follows:\n The gardenlet is configured with certain resources and their total capacity (and, for certain resources, the amount reserved for Gardener), see /example/20-componentconfig-gardenlet.yaml. Currently, the only such resource is the maximum number of shoots that can be scheduled onto a seed. The gardenlet seed controller updates the capacity and allocatable fields in Seed status with the capacity of each resource and how much of it is actually available to be consumed by shoots. The allocatable value of a resource is equal to capacity minus reserved. When scheduling shoots, the scheduler filters out all candidate seeds whose allocatable capacity for shoots would be exceeded if the shoot is scheduled onto the seed.  Failure to determine a suitable seed In case the scheduler fails to find a suitable seed, the operation is being retried with an exponential backoff - starting with the retrySyncPeriod (default of 15s).\nCurrent Limitation / Future Plans  Azure has unfortunately a geographically non-hierarchical naming pattern and does not start with the continent. This is the reason why we will exchange the implementation of the MinimalRegion strategy with a more suitable one in the future.  "},{"uri":"https://gardener.cloud/v1.12.8/concepts/core-components/scheduler/","title":"Gardener Scheduler","tags":[],"description":"","content":"Gardener Scheduler The Gardener Scheduler is in essence a controller that watches newly created shoots and assigns a seed cluster to them. Conceptually, the task of the Gardener Scheduler is very similar to the task of the Kubernetes Scheduler: finding a seed for a shoot instead of a node for a pod.\nEither the scheduling strategy or the shoot cluster purpose hereby determines how the scheduler is operating. The following sections explain the configuration and flow in greater detail.\nWhy is the Gardener Scheduler needed? 1. Decoupling Previously, an admission plugin in the Gardener API server conducted the scheduling decisions. This implies changes to the API server whenever adjustments of the scheduling are needed. Decoupling the API server and the scheduler comes with greater flexibility to develop these components independently from each other.\n2. Extensibility It should be possible to easily extend and tweak the scheduler in the future. Possibly, similar to the Kubernetes scheduler, hooks could be provided which influence the scheduling decisions. It should be also possible to completely replace the standard Gardener Scheduler with a custom implementation.\nAlgorithm overview The following sequence describes the steps involved to determine a seed candidate:\n Determine usable seeds with \u0026ldquo;usable\u0026rdquo; defined as follows:  no .metadata.deletionTimestamp .spec.settings.scheduling.visible is true conditions Bootstrapped, GardenletReady are true   Filter seeds:  matching .spec.seedSelector in CloudProfile used by the Shoot matching .spec.seedSelector in Shoot having no network intersection with the Shoot's networks (due to the VPN connectivity between seeds and shoots their networks must be disjoint) having .spec.settings.shootDNS.enabled=false (only if the shoot specifies a DNS domain or does not use the unmanaged DNS provider) whose taints (.spec.taints) are tolerated by the Shoot (.spec.tolerations) whose capacity for shoots would not be exceeded if the shoot is scheduled onto the seed, see Ensuring seeds capacity for shoots is not exceeded   Apply active strategy e.g., Minimal Distance strategy Choose least utilized seed, i.e., the one with the least number of shoot control planes, will be the winner and written to the .spec.seedName field of the Shoot.  Configuration The Gardener Scheduler configuration has to be supplied on startup. It is a mandatory and also the only available flag. Here is an example scheduler configuration.\nMost of the configuration options are the same as in the Gardener Controller Manager (leader election, client connection, \u0026hellip;). However, the Gardener Scheduler on the other hand does not need a TLS configuration, because there are currently no webhooks configurable.\nStrategies The scheduling strategy is defined in the candidateDeterminationStrategy of the scheduler\u0026rsquo;s configuration and can have the possible values SameRegion and MinimalDistance. The SameRegion strategy is the default strategy.\n  Same Region strategy\nThe Gardener Scheduler reads the spec.provider.type and .spec.region fields from the Shoot resource. It tries to find a seed that has the identical .spec.provider.type and .spec.provider.region fields set. If it cannot find a suitable seed, it adds an event to the shoot stating, that it is unschedulable.\n  Minimal Distance strategy\nThe Gardener Scheduler tries to find a valid seed with minimal distance to the shoot\u0026rsquo;s intended region. The distance is calculated based on the Levenshtein distance of the region. Therefore the region name is split into a base name and an orientation. Possible orientations are north, south, east, west and central. The distance then is twice the Levenshtein distance of the region\u0026rsquo;s base name plus a correction value based on the orientation and the provider.\nIf the orientations of shoot and seed candidate match, the correction value is 0, if they differ it is 2 and if either the seed\u0026rsquo;s or the shoot\u0026rsquo;s region does not have an orientation it is 1. If the provider differs the correction value is additionally incremented by 2.\nBecause of this a matching region with a matching provider is always prefered.\n  In order to put the scheduling decision into effect, the scheduler sends an update request for the Shoot resource to the API server. After validation, the Gardener Aggregated API server updates the shoot to have the spec.seedName field set. Subsequently, the Gardenlet picks up and starts to create the cluster on the specified seed.\nSpecial handling based on shoot cluster purpose  Every shoot cluster can have a purpose that describes what the cluster is used for, and also influences how the cluster is setup (see this document for more information).\nIn case the shoot has the testing purpose then the scheduler only reads the .spec.provider.type from the Shoot resource and tries to find a Seed that has the identical .spec.provider.type. The region does not matter, i.e., testing shoots may also be scheduled on a seed in a complete different region if it is better for balancing the whole Gardener system.\nseedSelector field in the Shoot specification Similar to the .spec.nodeSelector field in Pods, the Shoot specification has an optional .spec.seedSelector field. It allows the user to provide a label selector that must match the labels of Seeds in order to be scheduled to one of them. The labels on Seeds are usually controlled by Gardener administrators/operators - end users cannot add arbitrary labels themselves. If provided, the Gardener Scheduler will only consider those seeds as \u0026ldquo;suitable\u0026rdquo; whose labels match those provided in the .spec.seedSelector of the Shoot.\nBy default only seeds with the same provider than the shoot are selected. By adding a providerTypes field to the seedSelector a dedicated set of possible providers (* means all provider types) can be selected.\nEnsuring seeds capacity for shoots is not exceeded Seeds have a practical limit of how many shoots they can accommodate. Exceeding this limit is undesirable as the system performance will be noticeably impacted. Therefore, the scheduler ensures that a seed\u0026rsquo;s capacity for shoots is not exceeded by taking into account a maximum number of shoots that can be scheduled onto a seed.\nThis mechanism works as follows:\n The gardenlet is configured with certain resources and their total capacity (and, for certain resources, the amount reserved for Gardener), see /example/20-componentconfig-gardenlet.yaml. Currently, the only such resource is the maximum number of shoots that can be scheduled onto a seed. The gardenlet seed controller updates the capacity and allocatable fields in Seed status with the capacity of each resource and how much of it is actually available to be consumed by shoots. The allocatable value of a resource is equal to capacity minus reserved. When scheduling shoots, the scheduler filters out all candidate seeds whose allocatable capacity for shoots would be exceeded if the shoot is scheduled onto the seed.  Failure to determine a suitable seed In case the scheduler fails to find a suitable seed, the operation is being retried with an exponential backoff - starting with the retrySyncPeriod (default of 15s).\nCurrent Limitation / Future Plans  Azure has unfortunately a geographically non-hierarchical naming pattern and does not start with the continent. This is the reason why we will exchange the implementation of the MinimalRegion strategy with a more suitable one in the future.  "},{"uri":"https://gardener.cloud/v1.13.2/concepts/core-components/scheduler/","title":"Gardener Scheduler","tags":[],"description":"","content":"Gardener Scheduler The Gardener Scheduler is in essence a controller that watches newly created shoots and assigns a seed cluster to them. Conceptually, the task of the Gardener Scheduler is very similar to the task of the Kubernetes Scheduler: finding a seed for a shoot instead of a node for a pod.\nEither the scheduling strategy or the shoot cluster purpose hereby determines how the scheduler is operating. The following sections explain the configuration and flow in greater detail.\nWhy is the Gardener Scheduler needed? 1. Decoupling Previously, an admission plugin in the Gardener API server conducted the scheduling decisions. This implies changes to the API server whenever adjustments of the scheduling are needed. Decoupling the API server and the scheduler comes with greater flexibility to develop these components independently from each other.\n2. Extensibility It should be possible to easily extend and tweak the scheduler in the future. Possibly, similar to the Kubernetes scheduler, hooks could be provided which influence the scheduling decisions. It should be also possible to completely replace the standard Gardener Scheduler with a custom implementation.\nAlgorithm overview The following sequence describes the steps involved to determine a seed candidate:\n Determine usable seeds with \u0026ldquo;usable\u0026rdquo; defined as follows:  no .metadata.deletionTimestamp .spec.settings.scheduling.visible is true conditions Bootstrapped, GardenletReady are true   Filter seeds:  matching .spec.seedSelector in CloudProfile used by the Shoot matching .spec.seedSelector in Shoot having no network intersection with the Shoot's networks (due to the VPN connectivity between seeds and shoots their networks must be disjoint) having .spec.settings.shootDNS.enabled=false (only if the shoot specifies a DNS domain or does not use the unmanaged DNS provider) whose taints (.spec.taints) are tolerated by the Shoot (.spec.tolerations) whose capacity for shoots would not be exceeded if the shoot is scheduled onto the seed, see Ensuring seeds capacity for shoots is not exceeded   Apply active strategy e.g., Minimal Distance strategy Choose least utilized seed, i.e., the one with the least number of shoot control planes, will be the winner and written to the .spec.seedName field of the Shoot.  Configuration The Gardener Scheduler configuration has to be supplied on startup. It is a mandatory and also the only available flag. Here is an example scheduler configuration.\nMost of the configuration options are the same as in the Gardener Controller Manager (leader election, client connection, \u0026hellip;). However, the Gardener Scheduler on the other hand does not need a TLS configuration, because there are currently no webhooks configurable.\nStrategies The scheduling strategy is defined in the candidateDeterminationStrategy of the scheduler\u0026rsquo;s configuration and can have the possible values SameRegion and MinimalDistance. The SameRegion strategy is the default strategy.\n  Same Region strategy\nThe Gardener Scheduler reads the spec.provider.type and .spec.region fields from the Shoot resource. It tries to find a seed that has the identical .spec.provider.type and .spec.provider.region fields set. If it cannot find a suitable seed, it adds an event to the shoot stating, that it is unschedulable.\n  Minimal Distance strategy\nThe Gardener Scheduler tries to find a valid seed with minimal distance to the shoot\u0026rsquo;s intended region. The distance is calculated based on the Levenshtein distance of the region. Therefore the region name is split into a base name and an orientation. Possible orientations are north, south, east, west and central. The distance then is twice the Levenshtein distance of the region\u0026rsquo;s base name plus a correction value based on the orientation and the provider.\nIf the orientations of shoot and seed candidate match, the correction value is 0, if they differ it is 2 and if either the seed\u0026rsquo;s or the shoot\u0026rsquo;s region does not have an orientation it is 1. If the provider differs the correction value is additionally incremented by 2.\nBecause of this a matching region with a matching provider is always prefered.\n  In order to put the scheduling decision into effect, the scheduler sends an update request for the Shoot resource to the API server. After validation, the Gardener Aggregated API server updates the shoot to have the spec.seedName field set. Subsequently, the Gardenlet picks up and starts to create the cluster on the specified seed.\nSpecial handling based on shoot cluster purpose  Every shoot cluster can have a purpose that describes what the cluster is used for, and also influences how the cluster is setup (see this document for more information).\nIn case the shoot has the testing purpose then the scheduler only reads the .spec.provider.type from the Shoot resource and tries to find a Seed that has the identical .spec.provider.type. The region does not matter, i.e., testing shoots may also be scheduled on a seed in a complete different region if it is better for balancing the whole Gardener system.\nseedSelector field in the Shoot specification Similar to the .spec.nodeSelector field in Pods, the Shoot specification has an optional .spec.seedSelector field. It allows the user to provide a label selector that must match the labels of Seeds in order to be scheduled to one of them. The labels on Seeds are usually controlled by Gardener administrators/operators - end users cannot add arbitrary labels themselves. If provided, the Gardener Scheduler will only consider those seeds as \u0026ldquo;suitable\u0026rdquo; whose labels match those provided in the .spec.seedSelector of the Shoot.\nBy default only seeds with the same provider than the shoot are selected. By adding a providerTypes field to the seedSelector a dedicated set of possible providers (* means all provider types) can be selected.\nEnsuring seeds capacity for shoots is not exceeded Seeds have a practical limit of how many shoots they can accommodate. Exceeding this limit is undesirable as the system performance will be noticeably impacted. Therefore, the scheduler ensures that a seed\u0026rsquo;s capacity for shoots is not exceeded by taking into account a maximum number of shoots that can be scheduled onto a seed.\nThis mechanism works as follows:\n The gardenlet is configured with certain resources and their total capacity (and, for certain resources, the amount reserved for Gardener), see /example/20-componentconfig-gardenlet.yaml. Currently, the only such resource is the maximum number of shoots that can be scheduled onto a seed. The gardenlet seed controller updates the capacity and allocatable fields in Seed status with the capacity of each resource and how much of it is actually available to be consumed by shoots. The allocatable value of a resource is equal to capacity minus reserved. When scheduling shoots, the scheduler filters out all candidate seeds whose allocatable capacity for shoots would be exceeded if the shoot is scheduled onto the seed.  Failure to determine a suitable seed In case the scheduler fails to find a suitable seed, the operation is being retried with an exponential backoff - starting with the retrySyncPeriod (default of 15s).\nCurrent Limitation / Future Plans  Azure has unfortunately a geographically non-hierarchical naming pattern and does not start with the continent. This is the reason why we will exchange the implementation of the MinimalRegion strategy with a more suitable one in the future.  "},{"uri":"https://gardener.cloud/documentation/guides/client_tools/","title":"Set Up Client Tools","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.12.8/guides/client_tools/","title":"Set Up Client Tools","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.13.2/guides/client_tools/","title":"Set Up Client Tools","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/concepts/core-components/seed-admission-controller/","title":"Gardener Seed Admission Controller","tags":[],"description":"","content":"Gardener Seed Admission Controller The Gardener Seed admission controller is deployed by the Gardenlet as part of its seed bootstrapping phase and, consequently, running in every seed cluster. It\u0026rsquo;s main purpose is to serve webhooks (validating or mutating) in order to admit or deny certain requests to the seed\u0026rsquo;s API server.\nWhat is it doing concretely? Validating Webhooks Unconfirmed Deletion Prevention As part of Gardener\u0026rsquo;s extensibility concepts a lot of CustomResourceDefinitions are deployed to the seed clusters that serve as extension points for provider-specific controllers. For example, the Infrastructure CRD triggers the provider extension to prepare the IaaS infrastructure of the underlying cloud provider for a to-be-created shoot cluster. Consequently, these extension CRDs have a lot of power and control large portions of the end-user\u0026rsquo;s shoot cluster. Accidental or undesired deletions of those resource can cause tremendous and hard-to-recover-from outages and should be prevented.\nTogether with the deployment of the Gardener seed admission controller a ValidatingWebhookConfiguration for CustomResourceDefinitions and most (custom) resources in the extensions.gardener.cloud/v1alpha1 API group is registered. It prevents DELETE requests for those CustomResourceDefinitions labeled with gardener.cloud/deletion-protected=true, and for all mentioned custom resources if they were not previously annotated with the confirmation.gardener.cloud/deletion=true. This prevents that undesired kubectl delete \u0026lt;...\u0026gt; requests are accepted.\nMutating Webhooks The admission controller endpoint /webhooks/default-pod-scheduler-name/gardener-kube-scheduler mutates pods and adds gardener-kube-scheduler to .spec.scheduleName.\nWhen SeedKubeScheduler feature gate is enabled, all control plane components are mutated. The scheduler scores Nodes with most resource usage higher than the rest, resulting in greater resource utilization.\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/core-components/seed-admission-controller/","title":"Gardener Seed Admission Controller","tags":[],"description":"","content":"Gardener Seed Admission Controller The Gardener Seed admission controller is deployed by the Gardenlet as part of its seed bootstrapping phase and, consequently, running in every seed cluster. It\u0026rsquo;s main purpose is to serve webhooks (validating or mutating) in order to admit or deny certain requests to the seed\u0026rsquo;s API server.\nWhat is it doing concretely? Validating Webhooks Unconfirmed Deletion Prevention As part of Gardener\u0026rsquo;s extensibility concepts a lot of CustomResourceDefinitions are deployed to the seed clusters that serve as extension points for provider-specific controllers. For example, the Infrastructure CRD triggers the provider extension to prepare the IaaS infrastructure of the underlying cloud provider for a to-be-created shoot cluster. Consequently, these extension CRDs have a lot of power and control large portions of the end-user\u0026rsquo;s shoot cluster. Accidental or undesired deletions of those resource can cause tremendous and hard-to-recover-from outages and should be prevented.\nTogether with the deployment of the Gardener seed admission controller a ValidatingWebhookConfiguration for CustomResourceDefinitions and most (custom) resources in the extensions.gardener.cloud/v1alpha1 API group is registered. It prevents DELETE requests for those CustomResourceDefinitions labeled with gardener.cloud/deletion-protected=true, and for all mentioned custom resources if they were not previously annotated with the confirmation.gardener.cloud/deletion=true. This prevents that undesired kubectl delete \u0026lt;...\u0026gt; requests are accepted.\nMutating Webhooks The admission controller endpoint /webhooks/default-pod-scheduler-name/gardener-kube-scheduler mutates pods and adds gardener-kube-scheduler to .spec.scheduleName.\nWhen SeedKubeScheduler feature gate is enabled, all control plane components are mutated. The scheduler scores Nodes with most resource usage higher than the rest, resulting in greater resource utilization.\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/core-components/seed-admission-controller/","title":"Gardener Seed Admission Controller","tags":[],"description":"","content":"Gardener Seed Admission Controller The Gardener Seed admission controller is deployed by the Gardenlet as part of its seed bootstrapping phase and, consequently, running in every seed cluster. It\u0026rsquo;s main purpose is to serve webhooks (validating or mutating) in order to admit or deny certain requests to the seed\u0026rsquo;s API server.\nWhat is it doing concretely? Validating Webhooks Unconfirmed Deletion Prevention As part of Gardener\u0026rsquo;s extensibility concepts a lot of CustomResourceDefinitions are deployed to the seed clusters that serve as extension points for provider-specific controllers. For example, the Infrastructure CRD triggers the provider extension to prepare the IaaS infrastructure of the underlying cloud provider for a to-be-created shoot cluster. Consequently, these extension CRDs have a lot of power and control large portions of the end-user\u0026rsquo;s shoot cluster. Accidental or undesired deletions of those resource can cause tremendous and hard-to-recover-from outages and should be prevented.\nTogether with the deployment of the Gardener seed admission controller a ValidatingWebhookConfiguration for CustomResourceDefinitions and most (custom) resources in the extensions.gardener.cloud/v1alpha1 API group is registered. It prevents DELETE requests for those CustomResourceDefinitions labeled with gardener.cloud/deletion-protected=true, and for all mentioned custom resources if they were not previously annotated with the confirmation.gardener.cloud/deletion=true. This prevents that undesired kubectl delete \u0026lt;...\u0026gt; requests are accepted.\nMutating Webhooks The admission controller endpoint /webhooks/default-pod-scheduler-name/gardener-kube-scheduler mutates pods and adds gardener-kube-scheduler to .spec.scheduleName.\nWhen SeedKubeScheduler feature gate is enabled, all control plane components are mutated. The scheduler scores Nodes with most resource usage higher than the rest, resulting in greater resource utilization.\n"},{"uri":"https://gardener.cloud/documentation/concepts/","title":"Concepts","tags":[],"description":"Explore the concepts on which Gardener is built","content":""},{"uri":"https://gardener.cloud/v1.12.8/concepts/","title":"Concepts","tags":[],"description":"Explore the concepts on which Gardener is built","content":""},{"uri":"https://gardener.cloud/v1.13.2/concepts/","title":"Concepts","tags":[],"description":"Explore the concepts on which Gardener is built","content":""},{"uri":"https://gardener.cloud/documentation/concepts/core-components/","title":"Core Components","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.12.8/concepts/core-components/","title":"Core Components","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.13.2/concepts/core-components/","title":"Core Components","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/concepts/core-components/gardenlet/","title":"gardenlet","tags":[],"description":"","content":"Gardenlet Gardener is implemented using the operator pattern: It uses custom controllers that act on our own custom resources, and apply Kubernetes principles to manage clusters instead of containers. Following this analogy, you can recognize components of the Gardener architecture as well-known Kubernetes components, for example, shoot clusters can be compared with pods, and seed clusters can be seen as worker nodes.\nThe following Gardener components play a similar role as the corresponding components in the Kubernetes architecture:\n   Gardener Component Kubernetes Component     gardener-apiserver kube-apiserver   gardener-controller-manager kube-controller-manager   gardener-scheduler kube-scheduler   gardenlet kubelet    Similar to how the kube-scheduler of Kubernetes finds an appropriate node for newly created pods, the gardener-scheduler of Gardener finds an appropriate seed cluster to host the control plane for newly ordered clusters. By providing multiple seed clusters for a region or provider, and distributing the workload, Gardener also reduces the blast radius of potential issues.\nKubernetes runs a primary \u0026ldquo;agent\u0026rdquo; on every node, the kubelet, which is responsible for managing pods and containers on its particular node. Decentralizing the responsibility to the kubelet has the advantage that the overall system is scalable. Gardener achieves the same for cluster management by using a gardenlet as primary \u0026ldquo;agent\u0026rdquo; on every seed cluster, and is only responsible for shoot clusters located in its particular seed cluster:\nThe gardener-controller-manager has control loops to manage resources of the Gardener API. However, instead of letting the gardener-controller-manager talk directly to seed clusters or shoot clusters, the responsibility isnt only delegated to the gardenlet, but also managed using a reversed control flow: It\u0026rsquo;s up to the gardenlet to contact the Gardener API server, for example, to share a status for its managed seed clusters.\nReversing the control flow allows placing seed clusters or shoot clusters behind firewalls without the necessity of direct access via VPN tunnels anymore.\nTLS Bootstrapping Kubernetes doesnt manage worker nodes itself, and its also not responsible for the lifecycle of the kubelet running on the workers. Similarly, Gardener doesnt manage seed clusters itself, so Gardener is also not responsible for the lifecycle of the gardenlet running on the seeds. As a consequence, both the gardenlet and the kubelet need to prepare a trusted connection to the Gardener API server and the Kubernetes API server correspondingly.\nTo prepare a trusted connection between the gardenlet and the Gardener API server, the gardenlet initializes a bootstrapping process after you deployed it into your seed clusters:\n  The gardenlet starts up with a bootstrap kubeconfig having a bootstrap token that allows to create CertificateSigningRequest (CSR) resources.\n  After the CSR is signed, the gardenlet downloads the created client certificate, creates a new kubeconfig with it, and stores it inside a Secret in the seed cluster.\n  The gardenlet deletes the bootstrap kubeconfig secret, and starts up with its new kubeconfig.\n  The gardenlet starts normal operation.\n  The gardener-controller-manager runs a control loop that automatically signs CSRs created by gardenlets.\n The gardenlet bootstrapping process is based on the kubelet bootstrapping process. More information: Kubelet\u0026rsquo;s TLS bootstrapping.\n If you don\u0026rsquo;t want to run this bootstrap process you can create a kubeconfig pointing to the garden cluster for the gardenlet yourself, and use field gardenClientConnection.kubeconfig in the gardenlet configuration to share it with the gardenlet.\nGardenlet Certificate Rotation The certificate used to authenticate the gardenlet against the API server is valid for a year. After about 10 months, the gardenlet tries to automatically replace the current certificate with a new one (certificate rotation).\nTo use certificate rotation, you need to specify the secret to store the kubeconfig with the rotated certificate in field .gardenClientConnection.kubeconfigSecret of the gardenlet component configuration.\nRotate certificates using bootstrap kubeconfig If the gardenlet created the certificate during the initial TLS Bootstrapping using the Bootstrap kubeconfig, certificates can be rotated automatically. The same control loop in the gardener-controller-manager that signs the CSRs during the initial TLS Bootstrapping also automatically signs the CSR during a certificate rotation.\nRotate Certificate Using Custom kubeconfig When trying to rotate a custom certificate that wasnt created by gardenlet as part of the TLS Bootstrap, the x509 certificate\u0026rsquo;s Subject field needs to conform to the following:\n the Common Name (CN) is prefixed with gardener.cloud:system:seed: the Organization (O) equals gardener.cloud:system:seeds  Otherwise, the gardener-controller-manager doesnt automatically sign the CSR. In this case, an external component or user needs to approve the CSR manually, for example, using command kubectl certificate approve seed-csr-\u0026lt;...\u0026gt;). If that doesnt happen within 15 minutes, the gardenlet repeats the process and creates another CSR.\nSeed Config versus Seed Selector The usage of the gardenlet is flexible:\n   Usage Description     seedConfig Run one gardenlet per seed cluster inside the seed cluster itself.   seedSelector Use one gardenlet to manage multiple seed clusters. The gardenlet can run outside of the seed cluster.     For production use its recommended to go for the seedConfig option, because it makes scaling easier and leads to a better distribution of responsibilities.\n   Provide a seedConfig that contains information about the seed cluster itself if you want the gardenlet in the standard way, see the example gardenlet configuration. Once bootstrapped, the gardenlet creates and updates its Seed object itself.\n  Provide a seedSelector that incorporates a label selector for the targeted seed clusters if you want the gardenlet to manage multiple seeds, see the example gardenlet configuration. In this case, you have to create the Seed objects together with a kubeconfig pointing to the cluster yourself.\n  Component Configuration In the component configuration for the gardenlet, its possible to define:\n settings for the Kubernetes clients interacting with the various clusters settings for the control loops inside the gardenlet settings for leader election and log levels, feature gates, and seed selection or seed configuration.  More information: Example Gardenlet Component Configuration.\nHeartbeats Similar to how Kubernetes uses Lease objects for node heart beats (see KEP), the gardenlet is using Lease objects for heart beats of the seed cluster. Every two seconds, the gardenlet checks that the seed cluster\u0026rsquo;s /healthz endpoint returns HTTP status code 200. If that is the case, the gardenlet renews the lease in the Garden cluster in the gardener-system-seed-lease namespace and updates the GardenletReady condition in the status.conditions field of the Seed resource(s).\nSimilarly to the node-lifecycle-controller inside the kube-controller-manager, the gardener-controller-manager features a seed-lifecycle-controller that sets the GardenletReady condition to Unknown in case the gardenlet fails to renew the lease. As a consequence, the gardener-scheduler doesnt consider this seed cluster for newly created shoot clusters anymore.\n/healthz Endpoint The gardenlet includes an HTTPS server that serves a /healthz endpoint. Its used as a liveness probe in the Deployment of the gardenlet. If the gardenlet fails to renew its lease then the endpoint returns 500 Internal Server Error, otherwise it returns 200 OK.\n \nIn case the gardenlet is managing multiple seeds (that is, a seed selector is used) then the /healthz reports 500 Internal Server Error if there is at least one seed for which it couldnt renew its lease. Only if it can renew the lease for all seeds it reports 200 OK.\n Please note, that the /healthz only indicates whether the gardenlet could successfully probe the Seed\u0026rsquo;s API server and renew the lease with the Garden cluster. It does not show that the Gardener extension API server (with the Gardener resource groups) is available. However, the Gardenlet is designed to withstand such connection outages and retries until the connection is reestablished.\nShooted Seeds Gardener users can use shoot clusters as seed clusters, so-called \u0026ldquo;shooted seeds\u0026rdquo;, by using shoot cluster annotation shoot.gardener.cloud/use-as-seed. By default, the gardenlet that manages this shoot cluster then automatically creates a clone of itself with the same version and the same configuration that it currently has. Then it deploys the gardenlet clone into the shooted seed cluster.\nIf you want to prevent the automatic gardenlet deployment, use the no-gardenlet value in the shoot.gardener.cloud/use-as-seed annotation. In this case, you have to deploy the gardenlet on your own into the seed cluster.\n For example, if you annotate the shoot cluster with shoot.gardener.cloud/use-as-seed=\u0026quot;true,no-gardenlet,invisible\u0026quot; the shooted seed is created without gardenlet, and the garden-scheduler ignores it (its invisible).\n More information: Create Shooted Seed Cluster\nMigrating from Previous Gardener Versions If your Gardener version doesnt support gardenlets yet, no special migration is required, but the following prerequisites must be met:\n Your Gardener version is at least 0.31 before upgrading to v1. You have to make sure that your garden cluster is exposed in a way that its reachable from all your seed clusters.  With previous Gardener versions, you had deployed the Gardener Helm chart (incorporating the API server, controller-manager, and scheduler). With v1, this stays the same, but you now have to deploy the gardenlet Helm chart as well into all of your seeds (if they arent shooted, as mentioned earlier).\nMore information: Deploy a Gardenlet for all instructions.\nRelated Links Gardener Architecture\nIssue #356: Implement Gardener Scheduler\nPR #2309: Add /healthz endpoint for Gardenlet\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/core-components/gardenlet/","title":"gardenlet","tags":[],"description":"","content":"Gardenlet Gardener is implemented using the operator pattern: It uses custom controllers that act on our own custom resources, and apply Kubernetes principles to manage clusters instead of containers. Following this analogy, you can recognize components of the Gardener architecture as well-known Kubernetes components, for example, shoot clusters can be compared with pods, and seed clusters can be seen as worker nodes.\nThe following Gardener components play a similar role as the corresponding components in the Kubernetes architecture:\n   Gardener Component Kubernetes Component     gardener-apiserver kube-apiserver   gardener-controller-manager kube-controller-manager   gardener-scheduler kube-scheduler   gardenlet kubelet    Similar to how the kube-scheduler of Kubernetes finds an appropriate node for newly created pods, the gardener-scheduler of Gardener finds an appropriate seed cluster to host the control plane for newly ordered clusters. By providing multiple seed clusters for a region or provider, and distributing the workload, Gardener also reduces the blast radius of potential issues.\nKubernetes runs a primary \u0026ldquo;agent\u0026rdquo; on every node, the kubelet, which is responsible for managing pods and containers on its particular node. Decentralizing the responsibility to the kubelet has the advantage that the overall system is scalable. Gardener achieves the same for cluster management by using a gardenlet as primary \u0026ldquo;agent\u0026rdquo; on every seed cluster, and is only responsible for shoot clusters located in its particular seed cluster:\nThe gardener-controller-manager has control loops to manage resources of the Gardener API. However, instead of letting the gardener-controller-manager talk directly to seed clusters or shoot clusters, the responsibility isnt only delegated to the gardenlet, but also managed using a reversed control flow: It\u0026rsquo;s up to the gardenlet to contact the Gardener API server, for example, to share a status for its managed seed clusters.\nReversing the control flow allows placing seed clusters or shoot clusters behind firewalls without the necessity of direct access via VPN tunnels anymore.\nTLS Bootstrapping Kubernetes doesnt manage worker nodes itself, and its also not responsible for the lifecycle of the kubelet running on the workers. Similarly, Gardener doesnt manage seed clusters itself, so Gardener is also not responsible for the lifecycle of the gardenlet running on the seeds. As a consequence, both the gardenlet and the kubelet need to prepare a trusted connection to the Gardener API server and the Kubernetes API server correspondingly.\nTo prepare a trusted connection between the gardenlet and the Gardener API server, the gardenlet initializes a bootstrapping process after you deployed it into your seed clusters:\n  The gardenlet starts up with a bootstrap kubeconfig having a bootstrap token that allows to create CertificateSigningRequest (CSR) resources.\n  After the CSR is signed, the gardenlet downloads the created client certificate, creates a new kubeconfig with it, and stores it inside a Secret in the seed cluster.\n  The gardenlet deletes the bootstrap kubeconfig secret, and starts up with its new kubeconfig.\n  The gardenlet starts normal operation.\n  The gardener-controller-manager runs a control loop that automatically signs CSRs created by gardenlets.\n The gardenlet bootstrapping process is based on the kubelet bootstrapping process. More information: Kubelet\u0026rsquo;s TLS bootstrapping.\n If you don\u0026rsquo;t want to run this bootstrap process you can create a kubeconfig pointing to the garden cluster for the gardenlet yourself, and use field gardenClientConnection.kubeconfig in the gardenlet configuration to share it with the gardenlet.\nGardenlet Certificate Rotation The certificate used to authenticate the gardenlet against the API server is valid for a year. After about 10 months, the gardenlet tries to automatically replace the current certificate with a new one (certificate rotation).\nTo use certificate rotation, you need to specify the secret to store the kubeconfig with the rotated certificate in field .gardenClientConnection.kubeconfigSecret of the gardenlet component configuration.\nRotate certificates using bootstrap kubeconfig If the gardenlet created the certificate during the initial TLS Bootstrapping using the Bootstrap kubeconfig, certificates can be rotated automatically. The same control loop in the gardener-controller-manager that signs the CSRs during the initial TLS Bootstrapping also automatically signs the CSR during a certificate rotation.\nRotate Certificate Using Custom kubeconfig When trying to rotate a custom certificate that wasnt created by gardenlet as part of the TLS Bootstrap, the x509 certificate\u0026rsquo;s Subject field needs to conform to the following:\n the Common Name (CN) is prefixed with gardener.cloud:system:seed: the Organization (O) equals gardener.cloud:system:seeds  Otherwise, the gardener-controller-manager doesnt automatically sign the CSR. In this case, an external component or user needs to approve the CSR manually, for example, using command kubectl certificate approve seed-csr-\u0026lt;...\u0026gt;). If that doesnt happen within 15 minutes, the gardenlet repeats the process and creates another CSR.\nSeed Config versus Seed Selector The usage of the gardenlet is flexible:\n   Usage Description     seedConfig Run one gardenlet per seed cluster inside the seed cluster itself.   seedSelector Use one gardenlet to manage multiple seed clusters. The gardenlet can run outside of the seed cluster.     For production use its recommended to go for the seedConfig option, because it makes scaling easier and leads to a better distribution of responsibilities.\n   Provide a seedConfig that contains information about the seed cluster itself if you want the gardenlet in the standard way, see the example gardenlet configuration. Once bootstrapped, the gardenlet creates and updates its Seed object itself.\n  Provide a seedSelector that incorporates a label selector for the targeted seed clusters if you want the gardenlet to manage multiple seeds, see the example gardenlet configuration. In this case, you have to create the Seed objects together with a kubeconfig pointing to the cluster yourself.\n  Component Configuration In the component configuration for the gardenlet, its possible to define:\n settings for the Kubernetes clients interacting with the various clusters settings for the control loops inside the gardenlet settings for leader election and log levels, feature gates, and seed selection or seed configuration.  More information: Example Gardenlet Component Configuration.\nHeartbeats Similar to how Kubernetes uses Lease objects for node heart beats (see KEP), the gardenlet is using Lease objects for heart beats of the seed cluster. Every two seconds, the gardenlet checks that the seed cluster\u0026rsquo;s /healthz endpoint returns HTTP status code 200. If that is the case, the gardenlet renews the lease in the Garden cluster in the gardener-system-seed-lease namespace and updates the GardenletReady condition in the status.conditions field of the Seed resource(s).\nSimilarly to the node-lifecycle-controller inside the kube-controller-manager, the gardener-controller-manager features a seed-lifecycle-controller that sets the GardenletReady condition to Unknown in case the gardenlet fails to renew the lease. As a consequence, the gardener-scheduler doesnt consider this seed cluster for newly created shoot clusters anymore.\n/healthz Endpoint The gardenlet includes an HTTPS server that serves a /healthz endpoint. Its used as a liveness probe in the Deployment of the gardenlet. If the gardenlet fails to renew its lease then the endpoint returns 500 Internal Server Error, otherwise it returns 200 OK.\n \nIn case the gardenlet is managing multiple seeds (that is, a seed selector is used) then the /healthz reports 500 Internal Server Error if there is at least one seed for which it couldnt renew its lease. Only if it can renew the lease for all seeds it reports 200 OK.\n Please note, that the /healthz only indicates whether the gardenlet could successfully probe the Seed\u0026rsquo;s API server and renew the lease with the Garden cluster. It does not show that the Gardener extension API server (with the Gardener resource groups) is available. However, the Gardenlet is designed to withstand such connection outages and retries until the connection is reestablished.\nShooted Seeds Gardener users can use shoot clusters as seed clusters, so-called \u0026ldquo;shooted seeds\u0026rdquo;, by using shoot cluster annotation shoot.gardener.cloud/use-as-seed. By default, the gardenlet that manages this shoot cluster then automatically creates a clone of itself with the same version and the same configuration that it currently has. Then it deploys the gardenlet clone into the shooted seed cluster.\nIf you want to prevent the automatic gardenlet deployment, use the no-gardenlet value in the shoot.gardener.cloud/use-as-seed annotation. In this case, you have to deploy the gardenlet on your own into the seed cluster.\n For example, if you annotate the shoot cluster with shoot.gardener.cloud/use-as-seed=\u0026quot;true,no-gardenlet,invisible\u0026quot; the shooted seed is created without gardenlet, and the garden-scheduler ignores it (its invisible).\n More information: Create Shooted Seed Cluster\nMigrating from Previous Gardener Versions If your Gardener version doesnt support gardenlets yet, no special migration is required, but the following prerequisites must be met:\n Your Gardener version is at least 0.31 before upgrading to v1. You have to make sure that your garden cluster is exposed in a way that its reachable from all your seed clusters.  With previous Gardener versions, you had deployed the Gardener Helm chart (incorporating the API server, controller-manager, and scheduler). With v1, this stays the same, but you now have to deploy the gardenlet Helm chart as well into all of your seeds (if they arent shooted, as mentioned earlier).\nMore information: Deploy a Gardenlet for all instructions.\nRelated Links Gardener Architecture\nIssue #356: Implement Gardener Scheduler\nPR #2309: Add /healthz endpoint for Gardenlet\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/core-components/gardenlet/","title":"gardenlet","tags":[],"description":"","content":"Gardenlet Gardener is implemented using the operator pattern: It uses custom controllers that act on our own custom resources, and apply Kubernetes principles to manage clusters instead of containers. Following this analogy, you can recognize components of the Gardener architecture as well-known Kubernetes components, for example, shoot clusters can be compared with pods, and seed clusters can be seen as worker nodes.\nThe following Gardener components play a similar role as the corresponding components in the Kubernetes architecture:\n   Gardener Component Kubernetes Component     gardener-apiserver kube-apiserver   gardener-controller-manager kube-controller-manager   gardener-scheduler kube-scheduler   gardenlet kubelet    Similar to how the kube-scheduler of Kubernetes finds an appropriate node for newly created pods, the gardener-scheduler of Gardener finds an appropriate seed cluster to host the control plane for newly ordered clusters. By providing multiple seed clusters for a region or provider, and distributing the workload, Gardener also reduces the blast radius of potential issues.\nKubernetes runs a primary \u0026ldquo;agent\u0026rdquo; on every node, the kubelet, which is responsible for managing pods and containers on its particular node. Decentralizing the responsibility to the kubelet has the advantage that the overall system is scalable. Gardener achieves the same for cluster management by using a gardenlet as primary \u0026ldquo;agent\u0026rdquo; on every seed cluster, and is only responsible for shoot clusters located in its particular seed cluster:\nThe gardener-controller-manager has control loops to manage resources of the Gardener API. However, instead of letting the gardener-controller-manager talk directly to seed clusters or shoot clusters, the responsibility isnt only delegated to the gardenlet, but also managed using a reversed control flow: It\u0026rsquo;s up to the gardenlet to contact the Gardener API server, for example, to share a status for its managed seed clusters.\nReversing the control flow allows placing seed clusters or shoot clusters behind firewalls without the necessity of direct access via VPN tunnels anymore.\nTLS Bootstrapping Kubernetes doesnt manage worker nodes itself, and its also not responsible for the lifecycle of the kubelet running on the workers. Similarly, Gardener doesnt manage seed clusters itself, so Gardener is also not responsible for the lifecycle of the gardenlet running on the seeds. As a consequence, both the gardenlet and the kubelet need to prepare a trusted connection to the Gardener API server and the Kubernetes API server correspondingly.\nTo prepare a trusted connection between the gardenlet and the Gardener API server, the gardenlet initializes a bootstrapping process after you deployed it into your seed clusters:\n  The gardenlet starts up with a bootstrap kubeconfig having a bootstrap token that allows to create CertificateSigningRequest (CSR) resources.\n  After the CSR is signed, the gardenlet downloads the created client certificate, creates a new kubeconfig with it, and stores it inside a Secret in the seed cluster.\n  The gardenlet deletes the bootstrap kubeconfig secret, and starts up with its new kubeconfig.\n  The gardenlet starts normal operation.\n  The gardener-controller-manager runs a control loop that automatically signs CSRs created by gardenlets.\n The gardenlet bootstrapping process is based on the kubelet bootstrapping process. More information: Kubelet\u0026rsquo;s TLS bootstrapping.\n If you don\u0026rsquo;t want to run this bootstrap process you can create a kubeconfig pointing to the garden cluster for the gardenlet yourself, and use field gardenClientConnection.kubeconfig in the gardenlet configuration to share it with the gardenlet.\nGardenlet Certificate Rotation The certificate used to authenticate the gardenlet against the API server is valid for a year. After about 10 months, the gardenlet tries to automatically replace the current certificate with a new one (certificate rotation).\nTo use certificate rotation, you need to specify the secret to store the kubeconfig with the rotated certificate in field .gardenClientConnection.kubeconfigSecret of the gardenlet component configuration.\nRotate certificates using bootstrap kubeconfig If the gardenlet created the certificate during the initial TLS Bootstrapping using the Bootstrap kubeconfig, certificates can be rotated automatically. The same control loop in the gardener-controller-manager that signs the CSRs during the initial TLS Bootstrapping also automatically signs the CSR during a certificate rotation.\nRotate Certificate Using Custom kubeconfig When trying to rotate a custom certificate that wasnt created by gardenlet as part of the TLS Bootstrap, the x509 certificate\u0026rsquo;s Subject field needs to conform to the following:\n the Common Name (CN) is prefixed with gardener.cloud:system:seed: the Organization (O) equals gardener.cloud:system:seeds  Otherwise, the gardener-controller-manager doesnt automatically sign the CSR. In this case, an external component or user needs to approve the CSR manually, for example, using command kubectl certificate approve seed-csr-\u0026lt;...\u0026gt;). If that doesnt happen within 15 minutes, the gardenlet repeats the process and creates another CSR.\nSeed Config versus Seed Selector The usage of the gardenlet is flexible:\n   Usage Description     seedConfig Run one gardenlet per seed cluster inside the seed cluster itself.   seedSelector Use one gardenlet to manage multiple seed clusters. The gardenlet can run outside of the seed cluster.     For production use its recommended to go for the seedConfig option, because it makes scaling easier and leads to a better distribution of responsibilities.\n   Provide a seedConfig that contains information about the seed cluster itself if you want the gardenlet in the standard way, see the example gardenlet configuration. Once bootstrapped, the gardenlet creates and updates its Seed object itself.\n  Provide a seedSelector that incorporates a label selector for the targeted seed clusters if you want the gardenlet to manage multiple seeds, see the example gardenlet configuration. In this case, you have to create the Seed objects together with a kubeconfig pointing to the cluster yourself.\n  Component Configuration In the component configuration for the gardenlet, its possible to define:\n settings for the Kubernetes clients interacting with the various clusters settings for the control loops inside the gardenlet settings for leader election and log levels, feature gates, and seed selection or seed configuration.  More information: Example Gardenlet Component Configuration.\nHeartbeats Similar to how Kubernetes uses Lease objects for node heart beats (see KEP), the gardenlet is using Lease objects for heart beats of the seed cluster. Every two seconds, the gardenlet checks that the seed cluster\u0026rsquo;s /healthz endpoint returns HTTP status code 200. If that is the case, the gardenlet renews the lease in the Garden cluster in the gardener-system-seed-lease namespace and updates the GardenletReady condition in the status.conditions field of the Seed resource(s).\nSimilarly to the node-lifecycle-controller inside the kube-controller-manager, the gardener-controller-manager features a seed-lifecycle-controller that sets the GardenletReady condition to Unknown in case the gardenlet fails to renew the lease. As a consequence, the gardener-scheduler doesnt consider this seed cluster for newly created shoot clusters anymore.\n/healthz Endpoint The gardenlet includes an HTTPS server that serves a /healthz endpoint. Its used as a liveness probe in the Deployment of the gardenlet. If the gardenlet fails to renew its lease then the endpoint returns 500 Internal Server Error, otherwise it returns 200 OK.\n \nIn case the gardenlet is managing multiple seeds (that is, a seed selector is used) then the /healthz reports 500 Internal Server Error if there is at least one seed for which it couldnt renew its lease. Only if it can renew the lease for all seeds it reports 200 OK.\n Please note, that the /healthz only indicates whether the gardenlet could successfully probe the Seed\u0026rsquo;s API server and renew the lease with the Garden cluster. It does not show that the Gardener extension API server (with the Gardener resource groups) is available. However, the Gardenlet is designed to withstand such connection outages and retries until the connection is reestablished.\nShooted Seeds Gardener users can use shoot clusters as seed clusters, so-called \u0026ldquo;shooted seeds\u0026rdquo;, by using shoot cluster annotation shoot.gardener.cloud/use-as-seed. By default, the gardenlet that manages this shoot cluster then automatically creates a clone of itself with the same version and the same configuration that it currently has. Then it deploys the gardenlet clone into the shooted seed cluster.\nIf you want to prevent the automatic gardenlet deployment, use the no-gardenlet value in the shoot.gardener.cloud/use-as-seed annotation. In this case, you have to deploy the gardenlet on your own into the seed cluster.\n For example, if you annotate the shoot cluster with shoot.gardener.cloud/use-as-seed=\u0026quot;true,no-gardenlet,invisible\u0026quot; the shooted seed is created without gardenlet, and the garden-scheduler ignores it (its invisible).\n More information: Create Shooted Seed Cluster\nMigrating from Previous Gardener Versions If your Gardener version doesnt support gardenlets yet, no special migration is required, but the following prerequisites must be met:\n Your Gardener version is at least 0.31 before upgrading to v1. You have to make sure that your garden cluster is exposed in a way that its reachable from all your seed clusters.  With previous Gardener versions, you had deployed the Gardener Helm chart (incorporating the API server, controller-manager, and scheduler). With v1, this stays the same, but you now have to deploy the gardenlet Helm chart as well into all of your seeds (if they arent shooted, as mentioned earlier).\nMore information: Deploy a Gardenlet for all instructions.\nRelated Links Gardener Architecture\nIssue #356: Implement Gardener Scheduler\nPR #2309: Add /healthz endpoint for Gardenlet\n"},{"uri":"https://gardener.cloud/documentation/guides/install_gardener/","title":"Install Gardener","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.12.8/guides/install_gardener/","title":"Install Gardener","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.13.2/guides/install_gardener/","title":"Install Gardener","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/concepts/networking/network_policies/","title":"Network Policies","tags":[],"description":"","content":"Network Policies in Gardener As Seed clusters can host the Kubernetes control planes of many Shoot clusters, it is necessary to isolate the control planes from each other for security reasons. Besides deploying each control plane in its own namespace, Gardener creates network policies to also isolate the networks. Essentially, network policies make sure that pods can only talk to other pods over the network they are supposed to. As such, network policies are an important part of Gardener\u0026rsquo;s tenant isolation.\nGardener deploys network policies into\n each namespace hosting the Kubernetes control plane of the Shoot cluster. the namespace dedicated to Gardener seed-wide global controllers. This namespace is often called garden and contains e.g. the Gardenlet. the kube-system namespace in the Shoot.  The aforementioned namespaces in the Seed contain a deny-all network policy that denies all ingress and egress traffic. This secure by default setting requires pods to allow network traffic. This is done by pods having labels matching to the selectors of the network policies deployed by Gardener.\nMore details on the deployed network policies can be found in the development and usage sections.\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/networking/network_policies/","title":"Network Policies","tags":[],"description":"","content":"Network Policies in Gardener As Seed clusters can host the Kubernetes control planes of many Shoot clusters, it is necessary to isolate the control planes from each other for security reasons. Besides deploying each control plane in its own namespace, Gardener creates network policies to also isolate the networks. Essentially, network policies make sure that pods can only talk to other pods over the network they are supposed to. As such, network policies are an important part of Gardener\u0026rsquo;s tenant isolation.\nGardener deploys network policies into\n each namespace hosting the Kubernetes control plane of the Shoot cluster. the namespace dedicated to Gardener seed-wide global controllers. This namespace is often called garden and contains e.g. the Gardenlet. the kube-system namespace in the Shoot.  The aforementioned namespaces in the Seed contain a deny-all network policy that denies all ingress and egress traffic. This secure by default setting requires pods to allow network traffic. This is done by pods having labels matching to the selectors of the network policies deployed by Gardener.\nMore details on the deployed network policies can be found in the development and usage sections.\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/networking/network_policies/","title":"Network Policies","tags":[],"description":"","content":"Network Policies in Gardener As Seed clusters can host the Kubernetes control planes of many Shoot clusters, it is necessary to isolate the control planes from each other for security reasons. Besides deploying each control plane in its own namespace, Gardener creates network policies to also isolate the networks. Essentially, network policies make sure that pods can only talk to other pods over the network they are supposed to. As such, network policies are an important part of Gardener\u0026rsquo;s tenant isolation.\nGardener deploys network policies into\n each namespace hosting the Kubernetes control plane of the Shoot cluster. the namespace dedicated to Gardener seed-wide global controllers. This namespace is often called garden and contains e.g. the Gardenlet. the kube-system namespace in the Shoot.  The aforementioned namespaces in the Seed contain a deny-all network policy that denies all ingress and egress traffic. This secure by default setting requires pods to allow network traffic. This is done by pods having labels matching to the selectors of the network policies deployed by Gardener.\nMore details on the deployed network policies can be found in the development and usage sections.\n"},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/","title":"Administer Client (Shoot) Clusters","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.12.8/guides/administer_shoots/","title":"Administer Client (Shoot) Clusters","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.13.2/guides/administer_shoots/","title":"Administer Client (Shoot) Clusters","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/guides/","title":"How-To Guides","tags":[],"description":"Learn how to execute concrete tasks","content":""},{"uri":"https://gardener.cloud/v1.12.8/guides/","title":"How-To Guides","tags":[],"description":"Learn how to execute concrete tasks","content":""},{"uri":"https://gardener.cloud/v1.13.2/guides/","title":"How-To Guides","tags":[],"description":"Learn how to execute concrete tasks","content":""},{"uri":"https://gardener.cloud/documentation/concepts/backup-restore/backup-restore/","title":"Backup and Restore","tags":[],"description":"","content":"Backup and restore Kubernetes uses Etcd as the key-value store for its resource definitions. Gardener supports the backup and restore of etcd. It is the responsibility of the shoot owners to backup the workload data.\nGardener uses etcd-backup-restore component to backup the etcd backing the Shoot cluster regularly and restore in case of disaster. It is deployed as sidecar via etcd-druid. This doc mainly focuses on the backup and restore configuration used by Gardener when deploying these components. For more details on the design and internal implementation details, please refer GEP-06 and documentation on individual repository.\nBucket provisioning Refer the backup bucket extension document to know details about configuring backup bucket.\nBackup Policy etcd-backup-restore supports full snapshot and delta snapshots over full snapshot. In Gardener, this configuration is currently hard-coded to following parameters:\n Full Snapshot Schedule:  Daily, 24hr interval. For each Shoot, the schedule time in a day is randomized based on the configured Shoot maintenance window.   Delta Snapshot schedule:  At 5min interval. If aggregated events size since last snapshot goes beyond 100Mib.   Backup History / Garbage backup deletion policy:  Gardener configure backup restore to have Exponential garbage collection policy. As per policy, following backups are retained. All full backups and delta backups for the previous hour. Latest full snapshot of each previous hour for the day. Latest full snapshot of each previous day for 7 days. Latest full snapshot of the previous 4 weeks. Garbage Collection is configured at 12hr interval.   Listing:  Gardener don\u0026rsquo;t have any API to list out the backups. To find the backup list, admin can checkout the BackupEntry resource associated with Shoot which holds the bucket and prefix details on object store.    Restoration Restoration process of etcd is automated through the etcd-backup-restore component from latest snapshot. Gardener dosen\u0026rsquo;t support Point-In-Time-Recovery (PITR) of etcd. In case of etcd disaster, the etcd is recovered from latest backup automatically. For further details, please refer the doc. Post restoration of etcd, the Shoot reconciliation loop brings back the cluster to same state.\nAgain, Shoot owner is responsible for maintaining the backup/restore of his workload. Gardener does only take care of the cluster\u0026rsquo;s etcd.\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/backup-restore/backup-restore/","title":"Backup and Restore","tags":[],"description":"","content":"Backup and restore Kubernetes uses Etcd as the key-value store for its resource definitions. Gardener supports the backup and restore of etcd. It is the responsibility of the shoot owners to backup the workload data.\nGardener uses etcd-backup-restore component to backup the etcd backing the Shoot cluster regularly and restore in case of disaster. It is deployed as sidecar via etcd-druid. This doc mainly focuses on the backup and restore configuration used by Gardener when deploying these components. For more details on the design and internal implementation details, please refer GEP-06 and documentation on individual repository.\nBucket provisioning Refer the backup bucket extension document to know details about configuring backup bucket.\nBackup Policy etcd-backup-restore supports full snapshot and delta snapshots over full snapshot. In Gardener, this configuration is currently hard-coded to following parameters:\n Full Snapshot Schedule:  Daily, 24hr interval. For each Shoot, the schedule time in a day is randomized based on the configured Shoot maintenance window.   Delta Snapshot schedule:  At 5min interval. If aggregated events size since last snapshot goes beyond 100Mib.   Backup History / Garbage backup deletion policy:  Gardener configure backup restore to have Exponential garbage collection policy. As per policy, following backups are retained. All full backups and delta backups for the previous hour. Latest full snapshot of each previous hour for the day. Latest full snapshot of each previous day for 7 days. Latest full snapshot of the previous 4 weeks. Garbage Collection is configured at 12hr interval.   Listing:  Gardener don\u0026rsquo;t have any API to list out the backups. To find the backup list, admin can checkout the BackupEntry resource associated with Shoot which holds the bucket and prefix details on object store.    Restoration Restoration process of etcd is automated through the etcd-backup-restore component from latest snapshot. Gardener dosen\u0026rsquo;t support Point-In-Time-Recovery (PITR) of etcd. In case of etcd disaster, the etcd is recovered from latest backup automatically. For further details, please refer the doc. Post restoration of etcd, the Shoot reconciliation loop brings back the cluster to same state.\nAgain, Shoot owner is responsible for maintaining the backup/restore of his workload. Gardener does only take care of the cluster\u0026rsquo;s etcd.\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/backup-restore/backup-restore/","title":"Backup and Restore","tags":[],"description":"","content":"Backup and restore Kubernetes uses Etcd as the key-value store for its resource definitions. Gardener supports the backup and restore of etcd. It is the responsibility of the shoot owners to backup the workload data.\nGardener uses etcd-backup-restore component to backup the etcd backing the Shoot cluster regularly and restore in case of disaster. It is deployed as sidecar via etcd-druid. This doc mainly focuses on the backup and restore configuration used by Gardener when deploying these components. For more details on the design and internal implementation details, please refer GEP-06 and documentation on individual repository.\nBucket provisioning Refer the backup bucket extension document to know details about configuring backup bucket.\nBackup Policy etcd-backup-restore supports full snapshot and delta snapshots over full snapshot. In Gardener, this configuration is currently hard-coded to following parameters:\n Full Snapshot Schedule:  Daily, 24hr interval. For each Shoot, the schedule time in a day is randomized based on the configured Shoot maintenance window.   Delta Snapshot schedule:  At 5min interval. If aggregated events size since last snapshot goes beyond 100Mib.   Backup History / Garbage backup deletion policy:  Gardener configure backup restore to have Exponential garbage collection policy. As per policy, following backups are retained. All full backups and delta backups for the previous hour. Latest full snapshot of each previous hour for the day. Latest full snapshot of each previous day for 7 days. Latest full snapshot of the previous 4 weeks. Garbage Collection is configured at 12hr interval.   Listing:  Gardener don\u0026rsquo;t have any API to list out the backups. To find the backup list, admin can checkout the BackupEntry resource associated with Shoot which holds the bucket and prefix details on object store.    Restoration Restoration process of etcd is automated through the etcd-backup-restore component from latest snapshot. Gardener dosen\u0026rsquo;t support Point-In-Time-Recovery (PITR) of etcd. In case of etcd disaster, the etcd is recovered from latest backup automatically. For further details, please refer the doc. Post restoration of etcd, the Shoot reconciliation loop brings back the cluster to same state.\nAgain, Shoot owner is responsible for maintaining the backup/restore of his workload. Gardener does only take care of the cluster\u0026rsquo;s etcd.\n"},{"uri":"https://gardener.cloud/documentation/tutorials/","title":"Tutorials","tags":[],"description":"Walkthroughs of common use case implementations and goals that require a set of tasks to accomplish","content":""},{"uri":"https://gardener.cloud/v1.12.8/tutorials/","title":"Tutorials","tags":[],"description":"Walkthroughs of common use case implementations and goals that require a set of tasks to accomplish","content":""},{"uri":"https://gardener.cloud/v1.13.2/tutorials/","title":"Tutorials","tags":[],"description":"Walkthroughs of common use case implementations and goals that require a set of tasks to accomplish","content":""},{"uri":"https://gardener.cloud/documentation/guides/monitoring_and_troubleshooting/","title":"Monitor and Troubleshoot","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.12.8/guides/monitoring_and_troubleshooting/","title":"Monitor and Troubleshoot","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.13.2/guides/monitoring_and_troubleshooting/","title":"Monitor and Troubleshoot","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/guides/applications/","title":"Applications","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.12.8/guides/applications/","title":"Applications","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.13.2/guides/applications/","title":"Applications","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/concepts/extensions/","title":"Extensions","tags":[],"description":"","content":"Extensibility overview Initially, everything was developed in-tree in the Gardener project. All cloud providers and the configuration for all the supported operating systems were released together with the Gardener core itself. But as the project grew, it got more and more difficult to add new providers and maintain the existing code base. As a consequence and in order to become agile and flexible again, we proposed GEP-1 (Gardener Enhancement Proposal). The document describes an out-of-tree extension architecture that keeps the Gardener core logic independent of provider-specific knowledge (similar to what Kubernetes has achieved with out-of-tree cloud providers or with CSI volume plugins).\nBasic concepts Gardener keeps running in the \u0026ldquo;garden cluster\u0026rdquo; and implements the core logic of shoot cluster reconciliation/deletion. Extensions are Kubernetes controllers themselves (like Gardener) and run in the seed clusters. As usual, we try to use Kubernetes wherever applicable. We rely on Kubernetes extension concepts in order to enable extensibility for Gardener. The main ideas of GEP-1 are the following:\n  During the shoot reconciliation process Gardener will write CRDs into the seed cluster that are watched and managed by the extension controllers. They will reconcile (based on the .spec) and report whether everything went well or errors occurred in the CRD\u0026rsquo;s .status field.\n  Gardener keeps deploying the provider-independent control plane components (etcd, kube-apiserver, etc.). However, some of these components might still need little customization by providers, e.g., additional configuration, flags, etc. In this case, the extension controllers register webhooks in order to manipulate the manifests.\n  Example 1:\nGardener creates a new AWS shoot cluster and requires the preparation of infrastructure in order to proceed (networks, security groups, etc.). It writes the following CRD into the seed cluster:\napiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--core--aws-01spec:type:awsproviderConfig:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:vpc:cidr:10.250.0.0/16internal:- 10.250.112.0/22public:- 10.250.96.0/22workers:- 10.250.0.0/19zones:- eu-west-1adns:apiserver:api.aws-01.core.example.comregion:eu-west-1secretRef:name:my-aws-credentialssshPublicKey:| base64(key)Please note that the .spec.providerConfig is a raw blob and not evaluated or known in any way by Gardener. Instead, it was specified by the user (in the Shoot resource) and just \u0026ldquo;forwarded\u0026rdquo; to the extension controller. Only the AWS controller understands this configuration and will now start provisioning/reconciling the infrastructure. It reports in the .status field the result:\nstatus:observedGeneration:...state:...lastError:..lastOperation:...providerStatus:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusvpc:id:vpc-1234subnets:- id:subnet-acbd1234name:workerszone:eu-west-1securityGroups:- id:sg-xyz12345name:workersiam:nodesRoleARN:\u0026lt;some-arn\u0026gt; instanceProfileName: fooec2:keyName:barGardener waits until the .status.lastOperation/.status.lastError indicates that the operation reached a final state and either continuous with the next step or stops and reports the potential error. The extension-specific output in .status.providerStatus is - similar to .spec.providerConfig - not evaluated and simply forwarded to CRDs in subsequent steps.\nExample 2:\nGardener deploys the control plane components into the seed cluster, e.g. the kube-controller-manager deployment with the following flags:\napiVersion:apps/v1kind:Deployment...spec:template:spec:containers:- command:- /usr/local/bin/kube-controller-manager- --allocate-node-cidrs=true- --attach-detach-reconcile-sync-period=1m0s- --controllers=*,bootstrapsigner,tokencleaner- --cluster-cidr=100.96.0.0/11- --cluster-name=shoot--core--aws-01- --cluster-signing-cert-file=/srv/kubernetes/ca/ca.crt- --cluster-signing-key-file=/srv/kubernetes/ca/ca.key- --concurrent-deployment-syncs=10- --concurrent-replicaset-syncs=10...The AWS controller requires some additional flags in order to make the cluster functional. It needs to provide a Kubernetes cloud-config and also some cloud-specific flags. Consequently, it registers a MutatingWebhookConfiguration on Deployments and adds these flags to the container:\n- --cloud-provider=external- --external-cloud-volume-plugin=aws- --cloud-config=/etc/kubernetes/cloudprovider/cloudprovider.confOf course, it would have needed to create a ConfigMap containing the cloud config and to add the proper volume and volumeMounts to the manifest as well.\n(Please note for this special example: The Kubernetes community is also working on making the kube-controller-manager provider-independent. However, there will most probably be still components other than the kube-controller-manager which need to be adapted by extensions.)\nIf you are interested in writing an extension, or generally in digging deeper to find out the nitty-gritty details of the extension concepts please read GEP-1. We are truly looking forward to your feedback!\nCurrent status Meanwhile, the out-of-tree extension architecture of Gardener is in place and has been productively validated. We are tracking all internal and external extensions of Gardener in the repo: Gardener Extensions Library.\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/","title":"Extensions","tags":[],"description":"","content":"Extensibility overview Initially, everything was developed in-tree in the Gardener project. All cloud providers and the configuration for all the supported operating systems were released together with the Gardener core itself. But as the project grew, it got more and more difficult to add new providers and maintain the existing code base. As a consequence and in order to become agile and flexible again, we proposed GEP-1 (Gardener Enhancement Proposal). The document describes an out-of-tree extension architecture that keeps the Gardener core logic independent of provider-specific knowledge (similar to what Kubernetes has achieved with out-of-tree cloud providers or with CSI volume plugins).\nBasic concepts Gardener keeps running in the \u0026ldquo;garden cluster\u0026rdquo; and implements the core logic of shoot cluster reconciliation/deletion. Extensions are Kubernetes controllers themselves (like Gardener) and run in the seed clusters. As usual, we try to use Kubernetes wherever applicable. We rely on Kubernetes extension concepts in order to enable extensibility for Gardener. The main ideas of GEP-1 are the following:\n  During the shoot reconciliation process Gardener will write CRDs into the seed cluster that are watched and managed by the extension controllers. They will reconcile (based on the .spec) and report whether everything went well or errors occurred in the CRD\u0026rsquo;s .status field.\n  Gardener keeps deploying the provider-independent control plane components (etcd, kube-apiserver, etc.). However, some of these components might still need little customization by providers, e.g., additional configuration, flags, etc. In this case, the extension controllers register webhooks in order to manipulate the manifests.\n  Example 1:\nGardener creates a new AWS shoot cluster and requires the preparation of infrastructure in order to proceed (networks, security groups, etc.). It writes the following CRD into the seed cluster:\napiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--core--aws-01spec:type:awsproviderConfig:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:vpc:cidr:10.250.0.0/16internal:- 10.250.112.0/22public:- 10.250.96.0/22workers:- 10.250.0.0/19zones:- eu-west-1adns:apiserver:api.aws-01.core.example.comregion:eu-west-1secretRef:name:my-aws-credentialssshPublicKey:| base64(key)Please note that the .spec.providerConfig is a raw blob and not evaluated or known in any way by Gardener. Instead, it was specified by the user (in the Shoot resource) and just \u0026ldquo;forwarded\u0026rdquo; to the extension controller. Only the AWS controller understands this configuration and will now start provisioning/reconciling the infrastructure. It reports in the .status field the result:\nstatus:observedGeneration:...state:...lastError:..lastOperation:...providerStatus:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusvpc:id:vpc-1234subnets:- id:subnet-acbd1234name:workerszone:eu-west-1securityGroups:- id:sg-xyz12345name:workersiam:nodesRoleARN:\u0026lt;some-arn\u0026gt; instanceProfileName: fooec2:keyName:barGardener waits until the .status.lastOperation/.status.lastError indicates that the operation reached a final state and either continuous with the next step or stops and reports the potential error. The extension-specific output in .status.providerStatus is - similar to .spec.providerConfig - not evaluated and simply forwarded to CRDs in subsequent steps.\nExample 2:\nGardener deploys the control plane components into the seed cluster, e.g. the kube-controller-manager deployment with the following flags:\napiVersion:apps/v1kind:Deployment...spec:template:spec:containers:- command:- /usr/local/bin/kube-controller-manager- --allocate-node-cidrs=true- --attach-detach-reconcile-sync-period=1m0s- --controllers=*,bootstrapsigner,tokencleaner- --cluster-cidr=100.96.0.0/11- --cluster-name=shoot--core--aws-01- --cluster-signing-cert-file=/srv/kubernetes/ca/ca.crt- --cluster-signing-key-file=/srv/kubernetes/ca/ca.key- --concurrent-deployment-syncs=10- --concurrent-replicaset-syncs=10...The AWS controller requires some additional flags in order to make the cluster functional. It needs to provide a Kubernetes cloud-config and also some cloud-specific flags. Consequently, it registers a MutatingWebhookConfiguration on Deployments and adds these flags to the container:\n- --cloud-provider=external- --external-cloud-volume-plugin=aws- --cloud-config=/etc/kubernetes/cloudprovider/cloudprovider.confOf course, it would have needed to create a ConfigMap containing the cloud config and to add the proper volume and volumeMounts to the manifest as well.\n(Please note for this special example: The Kubernetes community is also working on making the kube-controller-manager provider-independent. However, there will most probably be still components other than the kube-controller-manager which need to be adapted by extensions.)\nIf you are interested in writing an extension, or generally in digging deeper to find out the nitty-gritty details of the extension concepts please read GEP-1. We are truly looking forward to your feedback!\nCurrent status Meanwhile, the out-of-tree extension architecture of Gardener is in place and has been productively validated. We are tracking all internal and external extensions of Gardener in the repo: Gardener Extensions Library.\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/","title":"Extensions","tags":[],"description":"","content":"Extensibility overview Initially, everything was developed in-tree in the Gardener project. All cloud providers and the configuration for all the supported operating systems were released together with the Gardener core itself. But as the project grew, it got more and more difficult to add new providers and maintain the existing code base. As a consequence and in order to become agile and flexible again, we proposed GEP-1 (Gardener Enhancement Proposal). The document describes an out-of-tree extension architecture that keeps the Gardener core logic independent of provider-specific knowledge (similar to what Kubernetes has achieved with out-of-tree cloud providers or with CSI volume plugins).\nBasic concepts Gardener keeps running in the \u0026ldquo;garden cluster\u0026rdquo; and implements the core logic of shoot cluster reconciliation/deletion. Extensions are Kubernetes controllers themselves (like Gardener) and run in the seed clusters. As usual, we try to use Kubernetes wherever applicable. We rely on Kubernetes extension concepts in order to enable extensibility for Gardener. The main ideas of GEP-1 are the following:\n  During the shoot reconciliation process Gardener will write CRDs into the seed cluster that are watched and managed by the extension controllers. They will reconcile (based on the .spec) and report whether everything went well or errors occurred in the CRD\u0026rsquo;s .status field.\n  Gardener keeps deploying the provider-independent control plane components (etcd, kube-apiserver, etc.). However, some of these components might still need little customization by providers, e.g., additional configuration, flags, etc. In this case, the extension controllers register webhooks in order to manipulate the manifests.\n  Example 1:\nGardener creates a new AWS shoot cluster and requires the preparation of infrastructure in order to proceed (networks, security groups, etc.). It writes the following CRD into the seed cluster:\napiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--core--aws-01spec:type:awsproviderConfig:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:vpc:cidr:10.250.0.0/16internal:- 10.250.112.0/22public:- 10.250.96.0/22workers:- 10.250.0.0/19zones:- eu-west-1adns:apiserver:api.aws-01.core.example.comregion:eu-west-1secretRef:name:my-aws-credentialssshPublicKey:| base64(key)Please note that the .spec.providerConfig is a raw blob and not evaluated or known in any way by Gardener. Instead, it was specified by the user (in the Shoot resource) and just \u0026ldquo;forwarded\u0026rdquo; to the extension controller. Only the AWS controller understands this configuration and will now start provisioning/reconciling the infrastructure. It reports in the .status field the result:\nstatus:observedGeneration:...state:...lastError:..lastOperation:...providerStatus:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusvpc:id:vpc-1234subnets:- id:subnet-acbd1234name:workerszone:eu-west-1securityGroups:- id:sg-xyz12345name:workersiam:nodesRoleARN:\u0026lt;some-arn\u0026gt; instanceProfileName: fooec2:keyName:barGardener waits until the .status.lastOperation/.status.lastError indicates that the operation reached a final state and either continuous with the next step or stops and reports the potential error. The extension-specific output in .status.providerStatus is - similar to .spec.providerConfig - not evaluated and simply forwarded to CRDs in subsequent steps.\nExample 2:\nGardener deploys the control plane components into the seed cluster, e.g. the kube-controller-manager deployment with the following flags:\napiVersion:apps/v1kind:Deployment...spec:template:spec:containers:- command:- /usr/local/bin/kube-controller-manager- --allocate-node-cidrs=true- --attach-detach-reconcile-sync-period=1m0s- --controllers=*,bootstrapsigner,tokencleaner- --cluster-cidr=100.96.0.0/11- --cluster-name=shoot--core--aws-01- --cluster-signing-cert-file=/srv/kubernetes/ca/ca.crt- --cluster-signing-key-file=/srv/kubernetes/ca/ca.key- --concurrent-deployment-syncs=10- --concurrent-replicaset-syncs=10...The AWS controller requires some additional flags in order to make the cluster functional. It needs to provide a Kubernetes cloud-config and also some cloud-specific flags. Consequently, it registers a MutatingWebhookConfiguration on Deployments and adds these flags to the container:\n- --cloud-provider=external- --external-cloud-volume-plugin=aws- --cloud-config=/etc/kubernetes/cloudprovider/cloudprovider.confOf course, it would have needed to create a ConfigMap containing the cloud config and to add the proper volume and volumeMounts to the manifest as well.\n(Please note for this special example: The Kubernetes community is also working on making the kube-controller-manager provider-independent. However, there will most probably be still components other than the kube-controller-manager which need to be adapted by extensions.)\nIf you are interested in writing an extension, or generally in digging deeper to find out the nitty-gritty details of the extension concepts please read GEP-1. We are truly looking forward to your feedback!\nCurrent status Meanwhile, the out-of-tree extension architecture of Gardener is in place and has been productively validated. We are tracking all internal and external extensions of Gardener in the repo: Gardener Extensions Library.\n"},{"uri":"https://gardener.cloud/documentation/concepts/networking/","title":"Networking","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.12.8/concepts/networking/","title":"Networking","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.13.2/concepts/networking/","title":"Networking","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/references/","title":"API Reference","tags":[],"description":"Reference documentation for the Gardener API","content":""},{"uri":"https://gardener.cloud/v1.12.8/references/","title":"API Reference","tags":[],"description":"Reference documentation for the Gardener API","content":""},{"uri":"https://gardener.cloud/v1.13.2/references/","title":"API Reference","tags":[],"description":"Reference documentation for the Gardener API","content":""},{"uri":"https://gardener.cloud/documentation/concepts/monitoring/","title":"Monitoring","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.12.8/concepts/monitoring/","title":"Monitoring","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.13.2/concepts/monitoring/","title":"Monitoring","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/contribute/","title":"Contribute","tags":[],"description":"Contributors guides for code and documentation","content":"Contributing to Gardener Code of conduct All members of the Gardener community must abide by the CNCF Code of Conduct. Only by respecting each other can we develop a productive, collaborative community. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting gardener.opensource@sap.com and/or a Gardener project maintainer.\nContributing Gardener uses GitHub to manage reviews of pull requests.\n  If you are a new contributor see: Steps to Contribute\n  If you have a trivial fix or improvement, go ahead and create a pull request, addressing (with @...) a suitable maintainer of this repository (see CODEOWNERS of the repository you want to contribute to) in the description of the pull request.\n  If you plan to do something more involved, first discuss your ideas on our mailing list. This will avoid unnecessary work and surely give you and us a good deal of inspiration.\n  Relevant coding style guidelines are the Go Code Review Comments and the Formatting and style section of Peter Bourgon\u0026rsquo;s Go: Best Practices for Production Environments.\n  Steps to Contribute Should you wish to work on an issue, please claim it first by commenting on the GitHub issue that you want to work on it. This is to prevent duplicated efforts from contributors on the same issue.\nIf you have questions about one of the issues, with or without the tag, please comment on them and one of the maintainers will clarify it.\nWe kindly ask you to follow the Pull Request Checklist to ensure reviews can happen accordingly.\nContributing Code You are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions:\n Contributions must be licensed under the Apache 2.0 License You need to sign the Developer Certificate of Origin.  Contributing Documentation You are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions:\n Contributions must be licensed under the Creative Commons Attribution 4.0 International License You need to sign the Developer Certificate of Origin.  Developer Certificate of Origin Due to legal reasons, contributors will be asked to accept a Developer Certificate of Origin (DCO) before they submit the first pull request to this projects, this happens in an automated fashion during the submission process. We use the standard DCO text of the Linux Foundation.\nPull Request Checklist   Branch from the master branch and, if needed, rebase to the current master branch before submitting your pull request. If it doesn\u0026rsquo;t merge cleanly with master you may be asked to rebase your changes.\n  Commits should be as small as possible, while ensuring that each commit is correct independently (i.e., each commit should compile and pass tests).\n  Test your changes as thoroughly as possible before your commit them. Preferably, automate your test by unit / integration (e.g. Gardener integration testing) tests. If tested manually, provide information about the test scope in the PR description (e.g. Test passed: Upgrade K8s version from 1.14.5 to 1.15.2 on AWS, Azure, GCP, Alicloud, Openstack.).\n  Create Work In Progress [WIP] pull requests only if you need a clarification or an explicit review before you can continue your work item.\n  If your patch is not getting reviewed or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment, or you can ask for a review on our mailing list.\n  Post review:\n If a review requires you to change your commit(s), please test the changes again. Amend the affected commit(s) and force push onto your branch. Set respective comments in your GitHub review to resolved. Create a general PR comment to notify the reviewers that your amendments are ready for another round of review.    Issues and Planning We use GitHub issues to track bugs and enhancement requests. Please provide as much context as possible when you open an issue. The information you provide must be comprehensive enough to reproduce that issue for the assignee. Therefore, contributors may use but aren\u0026rsquo;t restricted to the issue template provided by the Gardener maintainers.\nZenHub is used for planning:\n Install the ZenHub Chrome plugin Login to ZenHub Open the Gardener ZenHub workspace  Security Release Process See Security Release Process\nCommunity Slack Channel #gardener, sign up here\nMailing List gardener@googlegroups.com\nThe mailing list is hosted through Google Groups. To receive the lists\u0026rsquo; emails, join the group, as you would any other Google Group.\nTwitter Follow @GardenerProject on Twitter. Please mention @GardenerProject in your own posts about Gardener.\nAccessing community documents In order to foster real time collaboration there are working documents and notes that are taken in Google Docs, and then transferred to this repository if appropriate.\nTo gain edit access for these documents, you must subscribe to the gardener mailing list, as these documents are shared automatically with anyone who subscribes to that list.\nWeekly Meeting We have a PUBLIC and RECORDED weekly meeting. We meet every Friday at 10:00 CET over Zoom. Find recordings in the Gardener Youtube channel. Let us know if you want to participate and live in a timezone where 10:00 CET is in the night, we can also schedule meetings on Thursday 17:00 CET.\nSee the meeting calendar on the web at calendar.google.com, or paste this iCal url into any iCal client.\nIf you have a topic you\u0026rsquo;d like to present or would like to see discussed, please propose a specific date on the Gardener Community Meeting Agenda. Find minutes in the same document. Please upload slides or other documents you presented to the Gardener Community Meeting folder. Subscribe to the gardener mailing list to get edit permissions.\n"},{"uri":"https://gardener.cloud/v1.12.8/contribute/","title":"Contribute","tags":[],"description":"Contributors guides for code and documentation","content":"Contributing to Gardener Code of conduct All members of the Gardener community must abide by the CNCF Code of Conduct. Only by respecting each other can we develop a productive, collaborative community. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting gardener.opensource@sap.com and/or a Gardener project maintainer.\nContributing Gardener uses GitHub to manage reviews of pull requests.\n  If you are a new contributor see: Steps to Contribute\n  If you have a trivial fix or improvement, go ahead and create a pull request, addressing (with @...) a suitable maintainer of this repository (see CODEOWNERS of the repository you want to contribute to) in the description of the pull request.\n  If you plan to do something more involved, first discuss your ideas on our mailing list. This will avoid unnecessary work and surely give you and us a good deal of inspiration.\n  Relevant coding style guidelines are the Go Code Review Comments and the Formatting and style section of Peter Bourgon\u0026rsquo;s Go: Best Practices for Production Environments.\n  Steps to Contribute Should you wish to work on an issue, please claim it first by commenting on the GitHub issue that you want to work on it. This is to prevent duplicated efforts from contributors on the same issue.\nIf you have questions about one of the issues, with or without the tag, please comment on them and one of the maintainers will clarify it.\nWe kindly ask you to follow the Pull Request Checklist to ensure reviews can happen accordingly.\nContributing Code You are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions:\n Contributions must be licensed under the Apache 2.0 License You need to sign the Developer Certificate of Origin.  Contributing Documentation You are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions:\n Contributions must be licensed under the Creative Commons Attribution 4.0 International License You need to sign the Developer Certificate of Origin.  Developer Certificate of Origin Due to legal reasons, contributors will be asked to accept a Developer Certificate of Origin (DCO) before they submit the first pull request to this projects, this happens in an automated fashion during the submission process. We use the standard DCO text of the Linux Foundation.\nPull Request Checklist   Branch from the master branch and, if needed, rebase to the current master branch before submitting your pull request. If it doesn\u0026rsquo;t merge cleanly with master you may be asked to rebase your changes.\n  Commits should be as small as possible, while ensuring that each commit is correct independently (i.e., each commit should compile and pass tests).\n  Test your changes as thoroughly as possible before your commit them. Preferably, automate your test by unit / integration (e.g. Gardener integration testing) tests. If tested manually, provide information about the test scope in the PR description (e.g. Test passed: Upgrade K8s version from 1.14.5 to 1.15.2 on AWS, Azure, GCP, Alicloud, Openstack.).\n  Create Work In Progress [WIP] pull requests only if you need a clarification or an explicit review before you can continue your work item.\n  If your patch is not getting reviewed or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment, or you can ask for a review on our mailing list.\n  Post review:\n If a review requires you to change your commit(s), please test the changes again. Amend the affected commit(s) and force push onto your branch. Set respective comments in your GitHub review to resolved. Create a general PR comment to notify the reviewers that your amendments are ready for another round of review.    Issues and Planning We use GitHub issues to track bugs and enhancement requests. Please provide as much context as possible when you open an issue. The information you provide must be comprehensive enough to reproduce that issue for the assignee. Therefore, contributors may use but aren\u0026rsquo;t restricted to the issue template provided by the Gardener maintainers.\nZenHub is used for planning:\n Install the ZenHub Chrome plugin Login to ZenHub Open the Gardener ZenHub workspace  Security Release Process See Security Release Process\nCommunity Slack Channel #gardener, sign up here\nMailing List gardener@googlegroups.com\nThe mailing list is hosted through Google Groups. To receive the lists\u0026rsquo; emails, join the group, as you would any other Google Group.\nTwitter Follow @GardenerProject on Twitter. Please mention @GardenerProject in your own posts about Gardener.\nAccessing community documents In order to foster real time collaboration there are working documents and notes that are taken in Google Docs, and then transferred to this repository if appropriate.\nTo gain edit access for these documents, you must subscribe to the gardener mailing list, as these documents are shared automatically with anyone who subscribes to that list.\nWeekly Meeting We have a PUBLIC and RECORDED weekly meeting. We meet every Friday at 10:00 CET over Zoom. Find recordings in the Gardener Youtube channel. Let us know if you want to participate and live in a timezone where 10:00 CET is in the night, we can also schedule meetings on Thursday 17:00 CET.\nSee the meeting calendar on the web at calendar.google.com, or paste this iCal url into any iCal client.\nIf you have a topic you\u0026rsquo;d like to present or would like to see discussed, please propose a specific date on the Gardener Community Meeting Agenda. Find minutes in the same document. Please upload slides or other documents you presented to the Gardener Community Meeting folder. Subscribe to the gardener mailing list to get edit permissions.\n"},{"uri":"https://gardener.cloud/v1.13.2/contribute/","title":"Contribute","tags":[],"description":"Contributors guides for code and documentation","content":"Contributing to Gardener Code of conduct All members of the Gardener community must abide by the CNCF Code of Conduct. Only by respecting each other can we develop a productive, collaborative community. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting gardener.opensource@sap.com and/or a Gardener project maintainer.\nContributing Gardener uses GitHub to manage reviews of pull requests.\n  If you are a new contributor see: Steps to Contribute\n  If you have a trivial fix or improvement, go ahead and create a pull request, addressing (with @...) a suitable maintainer of this repository (see CODEOWNERS of the repository you want to contribute to) in the description of the pull request.\n  If you plan to do something more involved, first discuss your ideas on our mailing list. This will avoid unnecessary work and surely give you and us a good deal of inspiration.\n  Relevant coding style guidelines are the Go Code Review Comments and the Formatting and style section of Peter Bourgon\u0026rsquo;s Go: Best Practices for Production Environments.\n  Steps to Contribute Should you wish to work on an issue, please claim it first by commenting on the GitHub issue that you want to work on it. This is to prevent duplicated efforts from contributors on the same issue.\nIf you have questions about one of the issues, with or without the tag, please comment on them and one of the maintainers will clarify it.\nWe kindly ask you to follow the Pull Request Checklist to ensure reviews can happen accordingly.\nContributing Code You are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions:\n Contributions must be licensed under the Apache 2.0 License You need to sign the Developer Certificate of Origin.  Contributing Documentation You are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions:\n Contributions must be licensed under the Creative Commons Attribution 4.0 International License You need to sign the Developer Certificate of Origin.  Developer Certificate of Origin Due to legal reasons, contributors will be asked to accept a Developer Certificate of Origin (DCO) before they submit the first pull request to this projects, this happens in an automated fashion during the submission process. We use the standard DCO text of the Linux Foundation.\nPull Request Checklist   Branch from the master branch and, if needed, rebase to the current master branch before submitting your pull request. If it doesn\u0026rsquo;t merge cleanly with master you may be asked to rebase your changes.\n  Commits should be as small as possible, while ensuring that each commit is correct independently (i.e., each commit should compile and pass tests).\n  Test your changes as thoroughly as possible before your commit them. Preferably, automate your test by unit / integration (e.g. Gardener integration testing) tests. If tested manually, provide information about the test scope in the PR description (e.g. Test passed: Upgrade K8s version from 1.14.5 to 1.15.2 on AWS, Azure, GCP, Alicloud, Openstack.).\n  Create Work In Progress [WIP] pull requests only if you need a clarification or an explicit review before you can continue your work item.\n  If your patch is not getting reviewed or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment, or you can ask for a review on our mailing list.\n  Post review:\n If a review requires you to change your commit(s), please test the changes again. Amend the affected commit(s) and force push onto your branch. Set respective comments in your GitHub review to resolved. Create a general PR comment to notify the reviewers that your amendments are ready for another round of review.    Issues and Planning We use GitHub issues to track bugs and enhancement requests. Please provide as much context as possible when you open an issue. The information you provide must be comprehensive enough to reproduce that issue for the assignee. Therefore, contributors may use but aren\u0026rsquo;t restricted to the issue template provided by the Gardener maintainers.\nZenHub is used for planning:\n Install the ZenHub Chrome plugin Login to ZenHub Open the Gardener ZenHub workspace  Security Release Process See Security Release Process\nCommunity Slack Channel #gardener, sign up here\nMailing List gardener@googlegroups.com\nThe mailing list is hosted through Google Groups. To receive the lists\u0026rsquo; emails, join the group, as you would any other Google Group.\nTwitter Follow @GardenerProject on Twitter. Please mention @GardenerProject in your own posts about Gardener.\nAccessing community documents In order to foster real time collaboration there are working documents and notes that are taken in Google Docs, and then transferred to this repository if appropriate.\nTo gain edit access for these documents, you must subscribe to the gardener mailing list, as these documents are shared automatically with anyone who subscribes to that list.\nWeekly Meeting We have a PUBLIC and RECORDED weekly meeting. We meet every Friday at 10:00 CET over Zoom. Find recordings in the Gardener Youtube channel. Let us know if you want to participate and live in a timezone where 10:00 CET is in the night, we can also schedule meetings on Thursday 17:00 CET.\nSee the meeting calendar on the web at calendar.google.com, or paste this iCal url into any iCal client.\nIf you have a topic you\u0026rsquo;d like to present or would like to see discussed, please propose a specific date on the Gardener Community Meeting Agenda. Find minutes in the same document. Please upload slides or other documents you presented to the Gardener Community Meeting folder. Subscribe to the gardener mailing list to get edit permissions.\n"},{"uri":"https://gardener.cloud/blog/2020-12/00/","title":"STACKIT Kubernetes Engine with Gardener","tags":[],"description":"","content":"STACKIT is a digital brand of Europes biggest retailer, the Schwarz Group, which consists of Lidl, Kaufland, as well as production and recycling companies. Following the industry trend, the Schwarz Group is in the process of a digital transformation. STACKIT enables this transformation by helping to modernize the internal IT of the company branches.\nWhat is STACKIT and the STACKIT Kubernetes Engine (SKE)? STACKIT started with colocation solutions for internal and external customers in Europe-based data centers, which was then expanded to a full cloud platform stack providing an IaaS layer with VMs, storage and network, as well as a PaaS layer including Cloud Foundry and a growing set of cloud services, like databases, messaging, etc.\nWith containers and Kubernetes becoming the lingua franca of the cloud, we are happy to announce the STACKIT Kubernetes Engine (SKE), which has been released as Beta in November this year. We decided to use Gardener as the cluster management engine underneath SKE - for good reasons as you will see  and we would like to share our experiences with Gardener when working on the SKE Beta release, and serve as a testimonial for this technology.\nFigure 1: STACKIT Component Diagram Why we chose Gardener as a cluster management tool We started with the Kubernetes endeavor in the beginning of 2020 with a newly formed agile team that consisted of software engineers, highly experienced in IT operations and development. After some exploration and a short conceptual phase, we had a clear-cut opinion on how the cluster management for STACKIT should look like: we were looking for a highly customizable tool that could be adapted to the specific needs of STACKIT and the Schwarz Group, e.g. in terms of network setup or the infrastructure layer it should be running on. Moreover, the tool should be scalable to a high number of managed Kubernetes clusters and should therefore provide a fully automated operation experience. As an open source project, contributing and influencing the tool, as well as collaborating with a larger community were important aspects that motivated us. Furthermore, we aimed to offer cluster management as a self-service in combination with an excellent user experience. Our objective was to have the managed clusters come with enterprise-grade SLAs  i.e. with batteries included, as some say.\nWith this mission, we started our quest through the world of Kubernetes and soon found Gardener to be a hot candidate of cluster management tools that seemed to fulfill our demands. We quickly got in contact and received a warm welcome from the Gardener community. As interested potential adopter, but in the early days of the COVID-19 lockdown, we managed to organize an online workshop during which we got an introduction and deep dive into Gardener and discussed the STACKIT use cases. We learned that Gardener is extensible in many dimensions, and that contributions are always welcome and encouraged. Once we understood the basic Gardener concepts of Garden, Shoot and Seed clusters, its inception design and how this extends Kubernetes concepts in a natural way, we were eager to evaluate this tool in more detail.\nAfter this evaluation, we were convinced that this tool fulfilled all our requirements - a decision was made and off we went.\nHow Gardener was adapted and extended by SKE After becoming familiar with Gardener, we started to look into its code base to adapt it to the specific needs of the STACKIT OpenStack environment. Changes and extensions were made in order to get it integrated into the STACKIT environment, and whenever reasonable, we contributed those changes back:\n To run smoothly with the STACKIT OpenStack layer, the Gardener configuration was adapted in different places, e.g. to support CSI driver or to configure the domains of shoot API server or ingress. Gardener was extended to support shoots and shooted seeds in dual stack and dual home setup. This is used in SKE for the communication between shooted seeds and the Garden cluster. SKE uses a private image registry for Gardener installation to resolve dependencies to public image registries and to have more control over the used Gardener versions. To install and run Gardener with the private image registry, some new configurations need to be introduced into Gardener. Gardener is a first-class API based service what allowed us to smoothly integrate it into the STACKIT User Interface. We were able to jump-start and utilize also the Gardener Dashboard for our Beta release by merely adjusting the look-\u0026amp;-feel, i.e. colors, labels and icons.  Figure 2: Gardener Dashboard adapted to STACKIT UI style Experience with Gardener operations As no OpenStack installation is identical to one another, getting Gardener to run stable on the STACKIT IaaS layer revealed some operational challenges. For instance, it was challenging to find the right configuration for Cinder CSI.\nTo test for its resilience, we tried to break the managed clusters with a Chaos Monkey test, e.g. by deleting services or components needed by Kubernetes and Gardener to work properly. The reconciliation feature of Gardener fixed all those problems automatically, so that damaged Shoot clusters became operational again after a short period of time. Thus, we were not able to break Shoot clusters from an end user perspective permanently, despite our efforts. Which again speaks for Gardeners first-class cloud native design.\nWe also participated in a fruitful community support: For several challenges we contacted the community channel and help was provided in a timely manner. A lesson learned was that raising an issue in the community early on, before getting stuck too long on your own with an unresolved problem, is essential and efficient.\nSummary Gardener is used by SKE to provide a managed Kubernetes offering for internal use cases of the Schwarz Group as well as for the public cloud offering of STACKIT. Thanks to Gardener, it was possible to get from zero to a Beta release in only about half a years time  this speaks for itself. Within this period, we were able to integrate Gardener into the STACKIT environment, i.e. in its OpenStack IaaS layer, its management tools and its identity provisioning solution.\nGardener has become a vital building block in STACKIT\u0026rsquo;s cloud native platform offering. For the future, the possibility to manage clusters also on other infrastructures and hyperscalers is seen as another great opportunity for extended use cases. The open co-innovation exchange with the Gardener community member companies has also opened the door to commercial co-operation.\n"},{"uri":"https://gardener.cloud/documentation/tutorials/oidc-login/","title":"Authenticating with an Identity Provider","tags":[],"description":"Authenticating with an Identity Provider using OpenID Connect","content":"Use an identity provider to authenticate users to access shoot clusters.\nPrerequisites Please read the following background material on Authenticating.\nOverview Kubernetes on its own doesnt provide any user management. In other words, users arent managed through Kubernetes resources. Whenever you refer to a human user its sufficient to use a unique ID, for example, an email address. Nevertheless, Gardener project owners can use an identity provider to authenticate user access for shoot clusters in the following way:\n Configure an Identity Provider using OpenID Connect (OIDC). Configure a local kubectl oidc-login to enable oidc-login. Configure the shoot cluster to share details of the OIDC-compliant identity provider with the Kubernetes API Server. Authorize an authenticated user using role-based access control (RBAC). Verify the result   Gardener allows administrators to modify aspects of the control plane setup. It gives administrators full control of how the control plane is parameterized. While this offers much flexibility, administrators need to ensure that they dont configure a control plane that goes beyond the service level agreements of the responsible operators team.\n Configure an Identity Provider Create a tenant in an OIDC compatible Identity Provider. For simplicity, we use Auth0, which has a free plan.\n  In your tenant, create a client application to use authentication with kubectl:\n  Provide a Name, choose Native as application type, and choose CREATE.\n  On tab Settings, copy the following parameters to a local text file:\n  Domain\n Corresponds to the issuer in OIDC. It must be an https-secured endpoint (Auth0 requires a trailing / at the end). More information: Issuer Identifier.\n   Client ID\n  Client Secret\n    Configure the client to have a callback url of http://localhost:8000. This callback connects to your local kubectl oidc-login plugin:\n  Save your changes.\n  Verify that https://\u0026lt;Auth0 Domain\u0026gt;/.well-known/openid-configuration is reachable.\n  Choose Users \u0026amp; Roles \u0026gt; Users \u0026gt; CREATE USERS to create a user with a user and password:\n Users must have a verified email address.\n   Configure a local kubectl oidc-login   Install the kubectl plugin oidc-login. We highly recommend the krew install tool, which also makes other plugins easily available.\nkubectl krew install oidc-login The response looks like this:\nUpdated the local copy of plugin index. Installing plugin: oidc-login CAVEATS: \\ | You need to setup the OIDC provider, Kubernetes API server, role binding and kubeconfig. | See https://github.com/int128/kubelogin for more. / Installed plugin: oidc-login   Prepare a kubeconfig for later use:\ncp ~/.kube/config ~/.kube/config-oidc   Modify the configuration of ~/.kube/config-oidc as follows:\napiVersion:v1kind:Config...contexts:- context:cluster:shoot--project--myclusteruser:my-oidcname:shoot--project--mycluster...users:- name:my-oidcuser:exec:apiVersion:client.authentication.k8s.io/v1beta1command:kubectlargs:- oidc-login- get-token- --oidc-issuer-url=https://\u0026lt;Issuer\u0026gt;/- --oidc-client-id=\u0026lt;ClientID\u0026gt; - --oidc-client-secret=\u0026lt;Client Secret\u0026gt;- --oidc-extra-scope=email,offline_access,profile  To test our OIDC-based authentication, context shoot--project--mycluster of ~/.kube/config-oidc is used in a later step. For now, continue to use the configuration ~/.kube/config with administration rights for your cluster.\nConfigure the shoot cluster Modify the shoot cluster YAML as follows, using the client ID and the domain (as issuer) from the settings of the client application you created in Auth0:\nkind:ShootapiVersion:garden.sapcloud.io/v1beta1metadata:name:myclusternamespace:garden-project...spec:kubernetes:kubeAPIServer:oidcConfig:clientID:\u0026lt;ClientID\u0026gt; issuerURL: \u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;usernameClaim:emailThis change of the Shoot manifest triggers a reconciliation. Once the reconciliation is finished, your OIDC configuration is applied. It doesn\u0026rsquo;t invalidate other certificate-based authentication methods. Wait for Gardener to reconcile the change. It can take up to 5 minutes.\nAuthorize an authenticated user In Auth0, you created a user with a verified email address, test@test.com in our example. For simplicity, we authorize a single user identified by this email address with cluster role view:\napiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:viewer-testroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:viewsubjects:- apiGroup:rbac.authorization.k8s.iokind:Username:test@test.comAs administrator, apply the cluster role binding in your shoot cluster.\nVerify the result   To step into the shoes of your user, use the prepared kubeconfig file ~/.kube/config-oidc, and switch to the context that uses oidc-login:\ncd ~/.kube export KUBECONFIG=$(pwd)/config-oidc kubectl config use-context `shoot--project--mycluster`   kubectl delegates the authentication to plugin oidc-login the first time the user uses kubectl to contact the API server, for example:\nkubectl get all The plugin opens a browser for an interactive authentication session with Auth0, and in parallel serves a local webserver for the configured callback.\n  Enter your login credentials.\nYou should get a successful response from the API server:\nOpening in existing browser session. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 100.64.0.1 \u0026lt;none\u0026gt; 443/TCP 86m  After a successful login, kubectl uses a token for authentication so that you dont have to provide user and password for every new kubectl command. How long the token is valid can be configured. If you want to log in again earlier, reset plugin oidc-login:\n Delete directory ~/.kube/cache/oidc-login. Delete the browser cache.     To see if your user uses cluster role view, do some checks with kubectl auth can-i.\n  The response for the following commands should be no:\nkubectl auth can-i create clusterrolebindings kubectl auth can-i get secrets kubectl auth can-i describe secrets   The response for the following commands should be yes:\nkubectl auth can-i list pods kubectl auth can-i get pods     If the last step is successful, youve configured your cluster to authenticate against an identity provider using OIDC.\nRelated Links Auth0 Pricing\n"},{"uri":"https://gardener.cloud/v1.12.8/tutorials/oidc-login/","title":"Authenticating with an Identity Provider","tags":[],"description":"Authenticating with an Identity Provider using OpenID Connect","content":"Use an identity provider to authenticate users to access shoot clusters.\nPrerequisites Please read the following background material on Authenticating.\nOverview Kubernetes on its own doesnt provide any user management. In other words, users arent managed through Kubernetes resources. Whenever you refer to a human user its sufficient to use a unique ID, for example, an email address. Nevertheless, Gardener project owners can use an identity provider to authenticate user access for shoot clusters in the following way:\n Configure an Identity Provider using OpenID Connect (OIDC). Configure a local kubectl oidc-login to enable oidc-login. Configure the shoot cluster to share details of the OIDC-compliant identity provider with the Kubernetes API Server. Authorize an authenticated user using role-based access control (RBAC). Verify the result   Gardener allows administrators to modify aspects of the control plane setup. It gives administrators full control of how the control plane is parameterized. While this offers much flexibility, administrators need to ensure that they dont configure a control plane that goes beyond the service level agreements of the responsible operators team.\n Configure an Identity Provider Create a tenant in an OIDC compatible Identity Provider. For simplicity, we use Auth0, which has a free plan.\n  In your tenant, create a client application to use authentication with kubectl:\n  Provide a Name, choose Native as application type, and choose CREATE.\n  On tab Settings, copy the following parameters to a local text file:\n  Domain\n Corresponds to the issuer in OIDC. It must be an https-secured endpoint (Auth0 requires a trailing / at the end). More information: Issuer Identifier.\n   Client ID\n  Client Secret\n    Configure the client to have a callback url of http://localhost:8000. This callback connects to your local kubectl oidc-login plugin:\n  Save your changes.\n  Verify that https://\u0026lt;Auth0 Domain\u0026gt;/.well-known/openid-configuration is reachable.\n  Choose Users \u0026amp; Roles \u0026gt; Users \u0026gt; CREATE USERS to create a user with a user and password:\n Users must have a verified email address.\n   Configure a local kubectl oidc-login   Install the kubectl plugin oidc-login. We highly recommend the krew install tool, which also makes other plugins easily available.\nkubectl krew install oidc-login The response looks like this:\nUpdated the local copy of plugin index. Installing plugin: oidc-login CAVEATS: \\ | You need to setup the OIDC provider, Kubernetes API server, role binding and kubeconfig. | See https://github.com/int128/kubelogin for more. / Installed plugin: oidc-login   Prepare a kubeconfig for later use:\ncp ~/.kube/config ~/.kube/config-oidc   Modify the configuration of ~/.kube/config-oidc as follows:\napiVersion:v1kind:Config...contexts:- context:cluster:shoot--project--myclusteruser:my-oidcname:shoot--project--mycluster...users:- name:my-oidcuser:exec:apiVersion:client.authentication.k8s.io/v1beta1command:kubectlargs:- oidc-login- get-token- --oidc-issuer-url=https://\u0026lt;Issuer\u0026gt;/- --oidc-client-id=\u0026lt;ClientID\u0026gt; - --oidc-client-secret=\u0026lt;Client Secret\u0026gt;- --oidc-extra-scope=email,offline_access,profile  To test our OIDC-based authentication, context shoot--project--mycluster of ~/.kube/config-oidc is used in a later step. For now, continue to use the configuration ~/.kube/config with administration rights for your cluster.\nConfigure the shoot cluster Modify the shoot cluster YAML as follows, using the client ID and the domain (as issuer) from the settings of the client application you created in Auth0:\nkind:ShootapiVersion:garden.sapcloud.io/v1beta1metadata:name:myclusternamespace:garden-project...spec:kubernetes:kubeAPIServer:oidcConfig:clientID:\u0026lt;ClientID\u0026gt; issuerURL: \u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;usernameClaim:emailThis change of the Shoot manifest triggers a reconciliation. Once the reconciliation is finished, your OIDC configuration is applied. It doesn\u0026rsquo;t invalidate other certificate-based authentication methods. Wait for Gardener to reconcile the change. It can take up to 5 minutes.\nAuthorize an authenticated user In Auth0, you created a user with a verified email address, test@test.com in our example. For simplicity, we authorize a single user identified by this email address with cluster role view:\napiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:viewer-testroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:viewsubjects:- apiGroup:rbac.authorization.k8s.iokind:Username:test@test.comAs administrator, apply the cluster role binding in your shoot cluster.\nVerify the result   To step into the shoes of your user, use the prepared kubeconfig file ~/.kube/config-oidc, and switch to the context that uses oidc-login:\ncd ~/.kube export KUBECONFIG=$(pwd)/config-oidc kubectl config use-context `shoot--project--mycluster`   kubectl delegates the authentication to plugin oidc-login the first time the user uses kubectl to contact the API server, for example:\nkubectl get all The plugin opens a browser for an interactive authentication session with Auth0, and in parallel serves a local webserver for the configured callback.\n  Enter your login credentials.\nYou should get a successful response from the API server:\nOpening in existing browser session. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 100.64.0.1 \u0026lt;none\u0026gt; 443/TCP 86m  After a successful login, kubectl uses a token for authentication so that you dont have to provide user and password for every new kubectl command. How long the token is valid can be configured. If you want to log in again earlier, reset plugin oidc-login:\n Delete directory ~/.kube/cache/oidc-login. Delete the browser cache.     To see if your user uses cluster role view, do some checks with kubectl auth can-i.\n  The response for the following commands should be no:\nkubectl auth can-i create clusterrolebindings kubectl auth can-i get secrets kubectl auth can-i describe secrets   The response for the following commands should be yes:\nkubectl auth can-i list pods kubectl auth can-i get pods     If the last step is successful, youve configured your cluster to authenticate against an identity provider using OIDC.\nRelated Links Auth0 Pricing\n"},{"uri":"https://gardener.cloud/v1.13.2/tutorials/oidc-login/","title":"Authenticating with an Identity Provider","tags":[],"description":"Authenticating with an Identity Provider using OpenID Connect","content":"Use an identity provider to authenticate users to access shoot clusters.\nPrerequisites Please read the following background material on Authenticating.\nOverview Kubernetes on its own doesnt provide any user management. In other words, users arent managed through Kubernetes resources. Whenever you refer to a human user its sufficient to use a unique ID, for example, an email address. Nevertheless, Gardener project owners can use an identity provider to authenticate user access for shoot clusters in the following way:\n Configure an Identity Provider using OpenID Connect (OIDC). Configure a local kubectl oidc-login to enable oidc-login. Configure the shoot cluster to share details of the OIDC-compliant identity provider with the Kubernetes API Server. Authorize an authenticated user using role-based access control (RBAC). Verify the result   Gardener allows administrators to modify aspects of the control plane setup. It gives administrators full control of how the control plane is parameterized. While this offers much flexibility, administrators need to ensure that they dont configure a control plane that goes beyond the service level agreements of the responsible operators team.\n Configure an Identity Provider Create a tenant in an OIDC compatible Identity Provider. For simplicity, we use Auth0, which has a free plan.\n  In your tenant, create a client application to use authentication with kubectl:\n  Provide a Name, choose Native as application type, and choose CREATE.\n  On tab Settings, copy the following parameters to a local text file:\n  Domain\n Corresponds to the issuer in OIDC. It must be an https-secured endpoint (Auth0 requires a trailing / at the end). More information: Issuer Identifier.\n   Client ID\n  Client Secret\n    Configure the client to have a callback url of http://localhost:8000. This callback connects to your local kubectl oidc-login plugin:\n  Save your changes.\n  Verify that https://\u0026lt;Auth0 Domain\u0026gt;/.well-known/openid-configuration is reachable.\n  Choose Users \u0026amp; Roles \u0026gt; Users \u0026gt; CREATE USERS to create a user with a user and password:\n Users must have a verified email address.\n   Configure a local kubectl oidc-login   Install the kubectl plugin oidc-login. We highly recommend the krew install tool, which also makes other plugins easily available.\nkubectl krew install oidc-login The response looks like this:\nUpdated the local copy of plugin index. Installing plugin: oidc-login CAVEATS: \\ | You need to setup the OIDC provider, Kubernetes API server, role binding and kubeconfig. | See https://github.com/int128/kubelogin for more. / Installed plugin: oidc-login   Prepare a kubeconfig for later use:\ncp ~/.kube/config ~/.kube/config-oidc   Modify the configuration of ~/.kube/config-oidc as follows:\napiVersion:v1kind:Config...contexts:- context:cluster:shoot--project--myclusteruser:my-oidcname:shoot--project--mycluster...users:- name:my-oidcuser:exec:apiVersion:client.authentication.k8s.io/v1beta1command:kubectlargs:- oidc-login- get-token- --oidc-issuer-url=https://\u0026lt;Issuer\u0026gt;/- --oidc-client-id=\u0026lt;ClientID\u0026gt; - --oidc-client-secret=\u0026lt;Client Secret\u0026gt;- --oidc-extra-scope=email,offline_access,profile  To test our OIDC-based authentication, context shoot--project--mycluster of ~/.kube/config-oidc is used in a later step. For now, continue to use the configuration ~/.kube/config with administration rights for your cluster.\nConfigure the shoot cluster Modify the shoot cluster YAML as follows, using the client ID and the domain (as issuer) from the settings of the client application you created in Auth0:\nkind:ShootapiVersion:garden.sapcloud.io/v1beta1metadata:name:myclusternamespace:garden-project...spec:kubernetes:kubeAPIServer:oidcConfig:clientID:\u0026lt;ClientID\u0026gt; issuerURL: \u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;usernameClaim:emailThis change of the Shoot manifest triggers a reconciliation. Once the reconciliation is finished, your OIDC configuration is applied. It doesn\u0026rsquo;t invalidate other certificate-based authentication methods. Wait for Gardener to reconcile the change. It can take up to 5 minutes.\nAuthorize an authenticated user In Auth0, you created a user with a verified email address, test@test.com in our example. For simplicity, we authorize a single user identified by this email address with cluster role view:\napiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:viewer-testroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:viewsubjects:- apiGroup:rbac.authorization.k8s.iokind:Username:test@test.comAs administrator, apply the cluster role binding in your shoot cluster.\nVerify the result   To step into the shoes of your user, use the prepared kubeconfig file ~/.kube/config-oidc, and switch to the context that uses oidc-login:\ncd ~/.kube export KUBECONFIG=$(pwd)/config-oidc kubectl config use-context `shoot--project--mycluster`   kubectl delegates the authentication to plugin oidc-login the first time the user uses kubectl to contact the API server, for example:\nkubectl get all The plugin opens a browser for an interactive authentication session with Auth0, and in parallel serves a local webserver for the configured callback.\n  Enter your login credentials.\nYou should get a successful response from the API server:\nOpening in existing browser session. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 100.64.0.1 \u0026lt;none\u0026gt; 443/TCP 86m  After a successful login, kubectl uses a token for authentication so that you dont have to provide user and password for every new kubectl command. How long the token is valid can be configured. If you want to log in again earlier, reset plugin oidc-login:\n Delete directory ~/.kube/cache/oidc-login. Delete the browser cache.     To see if your user uses cluster role view, do some checks with kubectl auth can-i.\n  The response for the following commands should be no:\nkubectl auth can-i create clusterrolebindings kubectl auth can-i get secrets kubectl auth can-i describe secrets   The response for the following commands should be yes:\nkubectl auth can-i list pods kubectl auth can-i get pods     If the last step is successful, youve configured your cluster to authenticate against an identity provider using OIDC.\nRelated Links Auth0 Pricing\n"},{"uri":"https://gardener.cloud/","title":"Gardener","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/","title":"Home","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.12.8/","title":"Home","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.13.2/","title":"Home","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/blog/2020-11/02/","title":"Gardener v1.13 Released","tags":[],"description":"","content":"Dear community, we\u0026rsquo;re happy to announce a new minor release of Gardener, in fact, the 16th in 2020! v1.13 came out just today after a couple of weeks of code improvements and feature implementations. As usual, this blog post provides brief summaries for the most notable changes that we introduce with this version. Behind the scenes (and not explicitly highlighted below) we are progressing on internal code restructurings and refactorings to ease further extensions and to enhance development productivity. Speaking of those: You might be interested in watching the recording of the last Gardener Community Meeting which includes a detailed session for v2 of Terraformer, a complete rewrite in Golang and improved state handling.\nNotable Changes in v1.13 The main themes of Gardener\u0026rsquo;s v1.13 release are increments for feature gate promotions, scalability and robustness, and cleanups and refactorings. The community plans to continue on those and wants to deliver at least one more release in 2020.\nAutomatic Quotas for Gardener Resources (gardener/gardener#3072) Gardener already supports ResourceQuotas since the last release, however, it was still up to operators/administrators to create these objects in project namespaces. Obviously, in large Gardener installations with thousands of projects, this is a quite challenging task. With this release, we are shipping an improvement in the Project controller in the gardener-controller-manager that allows to automatically create ResourceQuotas based on configuration. Operators can distinguish via project label selectors which default quotas shall be defined for various projects. Please find more details here!\nResource Capacity and Reservations for Seeds (gardener/gardener#3075) The larger the Gardener landscape, the more seed cluster you require. Naturally, they have (based on constraints of the underlying infrastructure provider and/or seed cluster configuration) limits of how many shoots they can accommodate. Until this release, there were no means to prevent seed cluster from becoming overloaded (and potentially die due to this load). Now you define resource capacity and reservations in the gardenlet\u0026rsquo;s component configuration, similar to how the kubelet announces allocatable resources for Node objects. We are defaulting this to 250 shoots, but you might want to adapt this value for your own environment.\nDistributed Gardenlet Rollout for Shooted Seeds (gardener/gardener#3135) With the same motivation, i.e., to improve catering with large landscapes, we allow operators to configure distributed rollouts of gardenlets for shooted seeds. When a new Gardener version is being deployed in landscapes with a high number of shooted seeds, gardenlets of earlier versions were immediately re-deploying copies of themselves into the shooted seeds they manage. This leads to a large number of new gardenlet pods that all roughly start at the same time. Depending on the size of the landscape, this may trouble the gardener-apiservers as all of them are starting to fill their caches and create watches at the same time. By default, this rollout is now randomized within a 5m time window, i.e., it may take up to 5m until all gardenlets in all seeds have been updated.\nProgressing on Beta-Promotion for APIServerSNI Feature Gate (gardener/gardener#3082, gardener/gardener#3143) The alpha APIServerSNI feature will drastically reduce the costs for load balancers in the seed clusters, thus, it is effectively contributing to Gardener\u0026rsquo;s \u0026ldquo;minimal TCO\u0026rdquo; goal. In this release we are introducing an important improvement that optimizes the connectivity when pods talk to their control plane by avoiding an extra network hop. This is realized by a MutatingWebhookConfiguration whose server runs as a sidecar container in the kube-apiserver pod in the seed (only when the APIServerSNI feature gate is enabled). The webhook injects a KUBERNETES_SERVICE_HOST environment variable into pods in the shoot which prevents the additional network hop to the apiserver-proxy on all worker nodes. You can read more about it in this document.\nMore Control Plane Configurability (gardener/gardener#3141, gardener/gardener#3139) A main capability beloved by Gardener users is its openness when it comes to configurability and fine-tuning of the Kubernetes control plane components. Most managed Kubernetes offerings are not exposing options of the master components, but Gardener\u0026rsquo;s Shoot API offers a selected set of settings. With this release we are allowing to change the maximum number of (non-)mutating requests for the kube-apiserver of shoot clusters. Similarly, the grace period before deleting pods on failed nodes can now be fine-grained for the kube-controller-manager.\nImproved Project Resource Handling (gardener/gardener#3137, gardener/gardener#3136, gardener/gardener#3179) Projects are an important resource in the Gardener ecosystem as they enable collaboration with team members. A couple of improvements have landed into this release. Firstly, duplicates in the member list were not validated so far. With this release, the gardener-apiserver is automatically merging them, and in future releases requests with duplicates will be denied. Secondly, specific Projects may now be excluded from the stale checks if desired. Lastly, namespaces for Projects that were adopted (i.e., those that exist before the Project already) will now no longer deleted when the Project is being deleted. Please note that this only applies for newly created Projects.\nRemoval of Deprecated Labels and Annotations (gardener/gardener#3094) The core.gardener.cloud API group succeeded the old garden.sapcloud.io API group in the beginning of 2020, however, a lot of labels and annotations with the old API group name were still supported. We have continued with the process of removing those deprecated (but replaced with the new API group name) names. Concretely, the project labels garden.sapcloud.io/role=project and project.garden.sapcloud.io/name=\u0026lt;project-name\u0026gt; are no longer supported now. Similarly, the shoot.garden.sapcloud.io/use-as-seed and shoot.garden.sapcloud.io/ignore-alerts annotations got deleted. We are not finished yet, but we do small increments and plan to progress on the topic until we finally got rid of all artifacts with the old API group name.\nNodeLocalDNS Network Policy Rules Adapted (gardener/gardener#3184) The alpha NodeLocalDNS feature was already introduced and explained with Gardener v1.8 with the motivation to overcome certain bottlenecks with the horizontally auto-scaled CoreDNS in all shoot cluster. Unfortunately, due to a bug in the network policy rules, it was not working in all environments. We have fixed this one now, so it should be ready for further tests and investigations. Come give it a try!\nPlease bear in mind that this blog post only highlights the most noticeable changes and improvements, but there is a whole bunch more, including a ton of bug fixes in older versions! Come check out the full release notes and share your feedback in our #gardener Slack channel!\n"},{"uri":"https://gardener.cloud/blog/2020-11/01/","title":"Case Study: Migrating ETCD Volumes in Production","tags":[],"description":"In this case study, our friends from metal-stack lead you through their journey of migrating Gardener ETCD volumes in their production environment.","content":" This is a guest commentary from metal-stack.\nmetal-stack is a software that provides an API for provisioning and managing physical servers in the data center. To categorize this product, the terms \u0026ldquo;Metal-as-a-Service\u0026rdquo; (MaaS) or \u0026ldquo;bare metal cloud\u0026rdquo; are commonly used.\n One reason you stumble upon this blog post could be that you saw errors like the following in your ETCD instances:\netcd-main-0 etcd 2020-09-03 06:00:07.556157 W | etcdserver: read-only range request \u0026#34;key:\\\u0026#34;/registry/deployments/shoot--pwhhcd--devcluster2/kube-apiserver\\\u0026#34; \u0026#34; with result \u0026#34;range_response_count:1 size:9566\u0026#34; took too long (13.95374909s) to execute As it turns out, 14 seconds are way too slow for running Kubernetes API servers. It makes them go into the crash loop (leader election fails). Even worse, this whole thing is self-amplifying: The longer a response takes, the more requests queue up, leading to response times increasing further and further. The system is very unlikely to recover. \nOn Github, you can easily find the reason for this problem. Most probably your disks are too slow (see etcd-io/etcd#10860). So, when you are (like in our case) on GKE and run your ETCD on their default persistent volumes, consider moving from standard disks to SSDs and the error messages should disappear. A guide on how to use SSD volumes on GKE can be found here.\nCase closed? Well. For some people it might. But when you are seeing this in your Gardener infrastructure, likely, there is something going wrong. The entire ETCD management is fully managed by the Gardener, which makes the problem a bit more interesting to look at. This blog post strives topics such as:\n Gardener operating principles Gardener architecture and ETCD management Pitfalls with multi-cloud environments Migrating GCP volumes to a new storage class  We from metal-stack learned quite a lot about the capabilities of Gardener through this problem. We are happy to share this experience with a broader audience. Gardener adopters and operators read on.\nHow Gardener Manages ETCDs In our infrastructure, we use the Gardener to provision Kubernetes clusters on bare metal machines in our own data centers using metal-stack. Even if the entire stack could be running on-premise, our initial seed cluster and the metal control plane are hosted on GKE. This way, we do not need to manage a single Kubernetes cluster in our entire landscape manually. As soon as we have Gardener deployed on this initial cluster, we can spin up further Seeds in our own data centers through the concept of shooted seeds.\nTo make this easier to understand, let us give you a simplified picture of how our Gardener production setup looks like:\nFigure 1: Simplified View on Our Production Setup For every shoot cluster, Gardener deploys an individual, standalone ETCD as a stateful set into a shoot namespace. The deployment of the ETCD stateful set is managed by a controller called etcd-druid, which reconciles a special resource of the kind etcds.druid.gardener.cloud. This Etcd resource is getting deployed during the shoot provisioning flow in the Gardenlet.\nFor failure-safety, the etcd-druid deploys the official ETCD container image along with a sidecar project called etcd-backup-restore. The sidecar automatically takes backups of the ETCD and stores them at a cloud provider, e.g. in S3 Buckets, Google Buckets, or similar. In case the ETCD comes up without or with corrupted data, the sidecar looks into the backup buckets and automatically restores the latest backup before ETCD starts up. This entire approach basically takes away the pain for operators to manually have to restore data in the event of data loss.\n We found the etcd-backup-restore project very intriguing. It was the inspiration for us to come up with a similar sidecar for the databases we use with metal-stack. This project is called backup-restore-sidecar. We can cope with postgres and rethinkdb database at the moment and more to come. Feel free to check it out when you are interested.\n As it\u0026rsquo;s the nature for multi-cloud applications to act upon a variety of cloud providers, with a single installation of Gardener, it is easily possible to spin up new Kubernetes clusters not only on GCP, but on other supported cloud platforms, too.\nWhen the Gardenlet deploys a resource like the Etcd resource into a shoot namespace, a provider-specific extension-controller has the chance to manipulate it through a mutating webhook. This way, a cloud provider can adjust the generic Gardener resource to fit his provider-specific needs. For every cloud that Gardener supports, there is such an extension-controller. For metal-stack, we also maintain one, it\u0026rsquo;s called gardener-extension-provider-metal.\n A side note for cloud providers: Meanwhile, new cloud providers can be added fully out-of-tree, i.e. without touching any of the Gardener sources. This works through API extensions and CRDs. The Gardener handles generic resources and backpacks provider-specific configuration through raw extensions. When you are a cloud provider on your own, this is really encouraging because you can integrate with Gardener without any burdens. You can find documentation on how to integrate your cloud into the Gardener here and here.\n The Mistake Is in the Deployment  This section contains code examples from Gardener v1.8.\n Now that we know how the ETCDs are managed by the Gardener, we can come back to the original problem from the beginning of this article. It turned out that the real problem was a misconfiguration in our deployment. The Gardener actually does use SSD-backed storage on GCP for ETCDs by default. During reconciliation, the gardener-extension-controller-gcp deploys a storage class called gardener.cloud-fast that enables accessing SSDs on GCP.\nBut for some reason, in our cluster we did not find such a storage class. And even more interesting, we did not use the gardener-extension-provider-gcp for any shoot reconciliation, only for ETCD backup purposes. And that was the big mistake we made: We reconciled the shoot control plane completely with gardener-extension-provider-metal even though our initial Seed actually runs on GKE and specific parts of the shoot control plane should be reconciled by the GCP extension-controller instead!\nThis is how the initial Seed resource looked like:\napiVersion: core.gardener.cloud/v1beta1 kind: Seed metadata: name: initial-seed spec: ... provider: region: gke type: metal ... ... Surprisingly, this configuration was working pretty well for a long time. The initial seed properly produced the Kubernetes control planes of our shooted seeds that looked like this:\n$ kubectl get controlplanes.extensions.gardener.cloud NAME TYPE PURPOSE STATUS AGE fra-equ01 metal Succeeded 85d fra-equ01-exposure metal exposure Succeeded 85d And this is another interesting observation: There are two ControlPlane resources. One regular resource and one with an exposure purpose. Gardener distinguishes between two types for this exact reason: Environments where the shoot control plane runs on a different cloud provider than the Kubernetes worker nodes. The regular ControlPlane resource gets reconciled by the provider configured in the Shoot resource, the exposure type ControlPlane by the provider configured in the Seed resource.\nWith the existing configuration the gardener-extension-provider-gcp does not kick in and hence, it neither deploys the gardener.cloud-fast storage class nor does it mutate the Etcd resource to point to it. And in the end, we are left with ETCD volumes using the default storage class (which is what we do for ETCD stateful sets in the metal-stack seeds, because our default storage class uses csi-lvm that writes into logical volumes on the SSD disks in our physical servers).\nThe correction we had to make was a one-liner: Setting the provider type of the initial Seed resource to gcp.\n$ kubectl get seed initial-seed -o yaml apiVersion: core.gardener.cloud/v1beta1 kind: Seed metadata: name: initial-seed spec: ... provider: region: gke type: gcp # \u0026lt;-- here ... ... This change moved over the control plane exposure reconciliation to the gardener-extension-provider-gcp:\n$ kubectl get -n \u0026lt;shoot-namespace\u0026gt; controlplanes.extensions.gardener.cloud NAME TYPE PURPOSE STATUS AGE fra-equ01 metal Succeeded 85d fra-equ01-exposure gcp exposure Succeeded 85d And boom, after some time of waiting for all sorts of magic reconciliations taking place in the background, the missing storage class suddenly appeared:\n$ kubectl get sc NAME PROVISIONER gardener.cloud-fast kubernetes.io/gce-pd standard (default) kubernetes.io/gce-pd Also, the Etcd resource was now configured properly to point to the new storage class:\n$ kubectl get -n \u0026lt;shoot-namespace\u0026gt; etcd etcd-main -o yaml apiVersion: druid.gardener.cloud/v1alpha1 kind: Etcd metadata: ... name: etcd-main spec: ... storageClass: gardener.cloud-fast # \u0026lt;-- was pointing to default storage class before! volumeClaimTemplate: main-etcd ...  Only the etcd-main storage class gets changed to gardener.cloud-fast. The etcd-events configuration will still point to standard disk storage because this ETCD is much less occupied as compared to the etcd-main stateful set.\n The Migration Now that the deployment was in place such that this mistake would not repeat in the future, we still had the ETCDs running on the default storage class. The reconciliation does not delete the existing persistent volumes (PVs) on its own.\nTo bring production back up quickly, we temporarily moved the ETCD pods to other nodes in the GKE cluster. These were nodes which were less occupied, such that the disk throughput was a little higher than before. But surely that was not a final solution.\nFor a proper solution we had to move the ETCD data out of the standard disk PV into a SSD-based PV.\nEven though we had the etcd-backup-restore sidecar, we did not want to fully rely on the restore mechanism to do the migration. The backup should only be there for emergency situations when something goes wrong. Thus, we came up with another approach to introduce the SSD volume: GCP disk snapshots. This is how we did the migration:\n Scale down etcd-druid to zero in order to prevent it from disturbing your migration Scale down the kube-apiservers deployment to zero, then wait for the ETCD stateful to take another clean snapshot Scale down the ETCD stateful set to zero as well (in order to prevent Gardener from trying to bring up the downscaled resources, we used small shell constructs like while true; do kubectl scale deploy etcd-druid --replicas 0 -n garden; sleep 1; done) Take a drive snapshot in GCP from the volume that is referenced by the ETCD PVC Create a new disk in GCP from the snapshot on a SSD disk Delete the existing PVC and PV of the ETCD (oops, data is now gone!) Manually deploy a PV into your Kubernetes cluster that references this new SSD disk Manually deploy a PVC with the name of the original PVC and let it reference the PV that you have just created Scale up the ETCD stateful set and check that ETCD is running properly (if something went terribly wrong, you still have the backup from the etcd-backup-restore sidecar, delete the PVC and PV again and let the sidecar bring up ETCD instead) Scale up the kube-apiserver deployment again Scale up etcd-druid again (stop your shell hacks ;D)  This approach worked very well for us and we were able to fix our production deployment issue. And what happened: We have never seen any crashing kube-apiservers again. \nConclusion As bad as problems in production are, they are the best way for learning from your mistakes. For new users of the Gardener it can be pretty overwhelming to understand the rich configuration possibilities that the Gardener brings. However, once you get a hang of how the Gardener works, the application offers an exceptional versatility that makes it very much suitable for production use-cases like ours.\nThis example has shown how Gardener:\n Can handle arbitrary layers of infrastructure hosted by different cloud providers. Allows provider-specific tweaks to gain ideal performance for every cloud you want to support. Leverages Kubernetes core principles across the entire project architecture, making it vastly extensible and resilient. Brings useful disaster recovery mechanisms to your infrastructure (e.g. with etcd-backup-restore).  We hope that you could take away something new through this blog post. With this article we also want to thank the SAP Gardener team for helping us to integrate Gardener with metal-stack. It\u0026rsquo;s been a great experience so far.  \n"},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/hibernate-cluster/","title":"Hibernate a Cluster","tags":["task"],"description":"","content":"Clusters are only needed 24 hours a day if they run productive workload. So whenever you do development in a cluster, or just use it for tests or demo purposes, you can save much money if you scale-down your Kubernetes resources whenever you don\u0026rsquo;t need them. However, scaling them down manually can become time-consuming the more resources you have.\nGardener offers a clever way to automatically scale-down all resources to zero: cluster hibernation. You can either hibernate a cluster by pushing a button or by defining a hibernation schedule.\n To save costs, it\u0026rsquo;s recommended to define a hibernation schedule before the creation of a cluster. You can hibernate your cluster or wake up your cluster manually even if there\u0026rsquo;s a schedule for its hibernation.\n  What is hibernated? What isnt affected by the hibernation? Hibernate your cluster manually Wake up your cluster manually Create a schedule to hibernate your cluster  What is hibernated? When a cluster is hibernated, Gardener scales down worker nodes and deletes the cluster\u0026rsquo;s control plane to free resources at the IaaS provider. This affects:\n Your workload, for example, pods, deployments, custom resources. The virtual machines running your workload. The resources of the control plane of your cluster.  What isnt affected by the hibernation? To scale up everything where it was before hibernation, Gardener doesnt delete state-related information, that is, information stored in persistent volumes. The cluster state as persistent in etcd is also preserved.\nHibernate your cluster manually   On the Gardener dashboard, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; OVERVIEW \u0026gt; Lifecycle \u0026gt; Hibernation \u0026gt; Hibernate Cluster.\n  To confirm the hibernation, enter the name of your cluster and choose HIBERNATE.\n   You can also hibernate your cluster by setting spec.hibernation.enabled to true in the cluster\u0026rsquo;s YAML file. To change it on the dashboard, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; YAML.\n As soon as the cluster was hibernated successfully, its status is shown as ZZZ in the list of clusters:\nWake up your cluster manually   On the Gardener dashboard, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; OVERVIEW \u0026gt; Lifecycle \u0026gt; Hibernation \u0026gt; Wake up Cluster.\n  To confirm waking up the cluster, choose Wake Up.\n   You can also wake up your cluster by setting spec.hibernation.enabled to false in the cluster\u0026rsquo;s YAML file. To change it on the dashboard, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; YAML.\n Create a schedule to hibernate your cluster   You can create a hibernation schedule for a cluster in the creation dialog of the Gardener dashboard. To create it later or to change it, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; OVERVIEW \u0026gt; Lifecycle \u0026gt; Hibernation \u0026gt; Configure Hibernation Schedule.\n  Changes made on the Gardener dashboard for your cluster are immediately written to the cluster\u0026rsquo;s YAML file on tab CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; YAML. You can find the hibernation schedule in field spec.hibernation.schedules. The schedule is defined like a cron job in Linux. More information: HibernationSchedule.\n  "},{"uri":"https://gardener.cloud/v1.12.8/guides/administer_shoots/hibernate-cluster/","title":"Hibernate a Cluster","tags":["task"],"description":"","content":"Clusters are only needed 24 hours a day if they run productive workload. So whenever you do development in a cluster, or just use it for tests or demo purposes, you can save much money if you scale-down your Kubernetes resources whenever you don\u0026rsquo;t need them. However, scaling them down manually can become time-consuming the more resources you have.\nGardener offers a clever way to automatically scale-down all resources to zero: cluster hibernation. You can either hibernate a cluster by pushing a button or by defining a hibernation schedule.\n To save costs, it\u0026rsquo;s recommended to define a hibernation schedule before the creation of a cluster. You can hibernate your cluster or wake up your cluster manually even if there\u0026rsquo;s a schedule for its hibernation.\n  What is hibernated? What isnt affected by the hibernation? Hibernate your cluster manually Wake up your cluster manually Create a schedule to hibernate your cluster  What is hibernated? When a cluster is hibernated, Gardener scales down worker nodes and deletes the cluster\u0026rsquo;s control plane to free resources at the IaaS provider. This affects:\n Your workload, for example, pods, deployments, custom resources. The virtual machines running your workload. The resources of the control plane of your cluster.  What isnt affected by the hibernation? To scale up everything where it was before hibernation, Gardener doesnt delete state-related information, that is, information stored in persistent volumes. The cluster state as persistent in etcd is also preserved.\nHibernate your cluster manually   On the Gardener dashboard, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; OVERVIEW \u0026gt; Lifecycle \u0026gt; Hibernation \u0026gt; Hibernate Cluster.\n  To confirm the hibernation, enter the name of your cluster and choose HIBERNATE.\n   You can also hibernate your cluster by setting spec.hibernation.enabled to true in the cluster\u0026rsquo;s YAML file. To change it on the dashboard, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; YAML.\n As soon as the cluster was hibernated successfully, its status is shown as ZZZ in the list of clusters:\nWake up your cluster manually   On the Gardener dashboard, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; OVERVIEW \u0026gt; Lifecycle \u0026gt; Hibernation \u0026gt; Wake up Cluster.\n  To confirm waking up the cluster, choose Wake Up.\n   You can also wake up your cluster by setting spec.hibernation.enabled to false in the cluster\u0026rsquo;s YAML file. To change it on the dashboard, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; YAML.\n Create a schedule to hibernate your cluster   You can create a hibernation schedule for a cluster in the creation dialog of the Gardener dashboard. To create it later or to change it, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; OVERVIEW \u0026gt; Lifecycle \u0026gt; Hibernation \u0026gt; Configure Hibernation Schedule.\n  Changes made on the Gardener dashboard for your cluster are immediately written to the cluster\u0026rsquo;s YAML file on tab CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; YAML. You can find the hibernation schedule in field spec.hibernation.schedules. The schedule is defined like a cron job in Linux. More information: HibernationSchedule.\n  "},{"uri":"https://gardener.cloud/v1.13.2/guides/administer_shoots/hibernate-cluster/","title":"Hibernate a Cluster","tags":["task"],"description":"","content":"Clusters are only needed 24 hours a day if they run productive workload. So whenever you do development in a cluster, or just use it for tests or demo purposes, you can save much money if you scale-down your Kubernetes resources whenever you don\u0026rsquo;t need them. However, scaling them down manually can become time-consuming the more resources you have.\nGardener offers a clever way to automatically scale-down all resources to zero: cluster hibernation. You can either hibernate a cluster by pushing a button or by defining a hibernation schedule.\n To save costs, it\u0026rsquo;s recommended to define a hibernation schedule before the creation of a cluster. You can hibernate your cluster or wake up your cluster manually even if there\u0026rsquo;s a schedule for its hibernation.\n  What is hibernated? What isnt affected by the hibernation? Hibernate your cluster manually Wake up your cluster manually Create a schedule to hibernate your cluster  What is hibernated? When a cluster is hibernated, Gardener scales down worker nodes and deletes the cluster\u0026rsquo;s control plane to free resources at the IaaS provider. This affects:\n Your workload, for example, pods, deployments, custom resources. The virtual machines running your workload. The resources of the control plane of your cluster.  What isnt affected by the hibernation? To scale up everything where it was before hibernation, Gardener doesnt delete state-related information, that is, information stored in persistent volumes. The cluster state as persistent in etcd is also preserved.\nHibernate your cluster manually   On the Gardener dashboard, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; OVERVIEW \u0026gt; Lifecycle \u0026gt; Hibernation \u0026gt; Hibernate Cluster.\n  To confirm the hibernation, enter the name of your cluster and choose HIBERNATE.\n   You can also hibernate your cluster by setting spec.hibernation.enabled to true in the cluster\u0026rsquo;s YAML file. To change it on the dashboard, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; YAML.\n As soon as the cluster was hibernated successfully, its status is shown as ZZZ in the list of clusters:\nWake up your cluster manually   On the Gardener dashboard, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; OVERVIEW \u0026gt; Lifecycle \u0026gt; Hibernation \u0026gt; Wake up Cluster.\n  To confirm waking up the cluster, choose Wake Up.\n   You can also wake up your cluster by setting spec.hibernation.enabled to false in the cluster\u0026rsquo;s YAML file. To change it on the dashboard, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; YAML.\n Create a schedule to hibernate your cluster   You can create a hibernation schedule for a cluster in the creation dialog of the Gardener dashboard. To create it later or to change it, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; OVERVIEW \u0026gt; Lifecycle \u0026gt; Hibernation \u0026gt; Configure Hibernation Schedule.\n  Changes made on the Gardener dashboard for your cluster are immediately written to the cluster\u0026rsquo;s YAML file on tab CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; YAML. You can find the hibernation schedule in field spec.hibernation.schedules. The schedule is defined like a cron job in Linux. More information: HibernationSchedule.\n  "},{"uri":"https://gardener.cloud/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/tags/task/","title":"task","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/blog/2020-11/00/","title":"Gardener v1.11 and v1.12 Released","tags":[],"description":"","content":"Two months after our last Gardener release update, we are happy again to present release v1.11 and v1.12 in this blog post. Control plane migration, load balancer consolidation, new security features are just a few topics we progressed with. As always, a detailed list of features, improvements, and bug fixes can be found in the release notes of each release. If you are going to update from a previous Gardener version, please take your time to go through the action items in the release notes.\nNotable Changes in v1.12 Release v1.12, fresh from the oven, is shipped with plenty of improvements, features and some API changes we want to pick up in the next sections.\nDrop Functionless DNS Providers (gardener/gardener#3036) This release drops the support for so-called functionless DNS providers. Those are providers in a shoots specification (.spec.dns.providers) which dont serve the shoots domain (.spec.dns.domain), but are created by Gardener in the seed cluster to serve DNS requests coming from the shoot cluster. If such providers dont specify a type or secretName the creation or update request for the corresponding shoot is denied.\nSeed Taints (gardener/gardener#2955) In an earlier release, we reserved a dedicated section in seed.spec.settings as a replacement for disable-capacity-reservation, disable-dns, invisible taints. These already deprecated taints were still considered and synced, which gave operators enough time to switch their integration to the new settings field. As of version v1.12, support for them has been discontinued and they are automatically removed from seed objects. You may use the actual taint names in a future release of Gardener again.\nLoad Balancer Events During Shoot Reconciliation (gardener/gardener#3028) As Gardener is capable of managing thousands of clusters, it is crucial to keep operation efforts at a minimum. This release demonstrates this endeavor by further improving error reporting to the end user. During a shoots reconciliation, Gardener creates Services of type LoadBalancer in the shoot cluster, e.g. for VPN or Nginx-Ingress addon, and waits for a successful creation. However, in the past we experienced that occurring issues caused by the party creating the load balancer (typically Cloud-Controller-Manager) are only exposed in the logs or as events. Gardener now fetches these event messages and propagates them to the shoot status in case of a failure. Users can then often fix the problem themselves, if for example the failure discloses an exhausted quota on the cloud provider.\nKonnectivityTunnel Feature Per Shoot(gardener/gardener#3007) Since release v1.6 Gardener has been capable of reversing the tunnel direction from the seed to the shoot via the KonnectivityTunnel feature gate (more information). With this release we make it possible to control the feature per shoot. We recommend to selectively enable the KonnectivityTunnel, as it is still in alpha state.\nReference Protection (gardener/gardener#2771, gardener/gardener 1708419) Shoot clusters may refer to external objects, like Secrets for specified DNS providers or they have a reference to an audit policy ConfigMap. Deleting those objects while any shoot still references them causes sever errors, often only recoverable by an immense amount of manual operations effort. To prevent such scenarios, Gardener now adds a new finalizer gardener.cloud/reference-protection to these objects and removes it as soon as the object itself becomes releasable. Due to compatibility reasons, we decided that the handling for audit policy ConfigMaps is delivered as an opt-in feature first, so please familiarize yourself with the necessary settings in the Gardener Controller Manager component config if you already plan to enable it.\nSupport For Resource Quotas (gardener/gardener#2627) After the Kubernetes upstream change (kubernetes/kubernetes#93537) for externalizing the backing admission plugin has been accepted, we are happy to announce the support of ResourceQuotas for Gardener offered resource kinds. ResourceQuotas allow you to specify a maximum number of objects per namespace, especially for end-user objects like Shoots or SecretBindings in a project namespace. Even though the admission plugin is enabled by default in the Gardener API Server, make sure the Kube Controller Manager runs the resourcequota controller as well.\nWatch Out Developers, Terraformer v2 Is Coming! (gardener/gardener#3034) Although not only related to Gardener core, but still an important milestone to mention, is the preparation towards Terraformer v2 in the extensions library. With Terraformer v2, Gardener extensions using Terraform scripts will benefit from great consistency improvements. Please check out #3034) which demonstrates necessary steps to transition to Terraformer v2 as soon as its been released.\nNotable Changes in v1.11 The Gardener community worked eagerly to deliver plenty of improvements with version v1.11. Those help us to further progress with topics like control plane migration, which is actively being worked on, or to harden our load balancer consolidation (APIServerSNI) feature. Besides improvements and fixes (full list available in release notes), this release as well contains major features and we dont want to miss a chance to walk you through them.\nGardener Admission Controller (gardener/gardener#2832), (gardener/gardener#2781) In this release, all admission related HTTP handlers moved from the Gardener Controller Manager (GCM) to the new component Gardener Admission Controller. The admission controller is rather a small component as opposed to GCM with regards to memory footprint and CPU consumption, and thus allows you to run multiple replicas of it much cheaper than it was before. We certainly recommend specifying the admission controller deployment with more than one replica, since it reduces the odds of a system-wide outage and increases the performance of your Gardener service.\nBesides the already known Namespace and Kubeconfig Secret validation, a new admission handler Resource-Size-Validator was added to the admission controller. It allows operators to restrict the size for all kinds of Kubernetes objects, especially sent by end-users to the Kubernetes or Gardener API Server. We address a security concern with this feature to prevent denial of service attacks in which an attacker artificially increases the size of objects to exhaust your object store, API server caches, or to let Gardener and Kubernetes controllers run out-of-memory. The documentation reveals an approach of finding the right resource size for your setup and why you should create exceptions for technical users and operators.\nDeferring Shoot Progress Reporting (gardener/gardener#2909), Shoot progress reporting is the continuous update process of a shoots .status.lastOperation field while the shoot is being reconciled by Gardener. Many steps are involved during reconciliation and depending on the size of your setup, the updates might become an issue for the Gardener API Server which will refrain to process further requests for a certain period. With .controllers.shoot.progressReportPeriod in Gardenlets component configuration, you can now delay these updates for the specified period.\nNew Policy For Controller Registrations (gardener/gardener#2896), A while ago, we added support for different policies in ControllerRegistrations which determine under which circumstances the deployments of registration controllers happen in affected seed clusters. If you specify the new policy AlwaysExceptNoShoots, the respective extension controller will be deployed to all seed cluster hosting at least one shoot cluster. After all shoot clusters from a seed are gone, the extension deployment will be deleted again. A full list of supported policies can be found here.\n"},{"uri":"https://gardener.cloud/blog/2020-10/00/","title":"Gardener Integrates with KubeVirt","tags":[],"description":"","content":"The Gardener team is happy to announce that Gardener now offers support for an additional, often requested, infrastructure/virtualization technology, namely KubeVirt! Gardener can now provide Kubernetes-conformant clusters using KubeVirt managed Virtual Machines in the environment of your choice. This integration has been tested and works with any qualified Kubernetes (provider) cluster that is compatibly configured to host the required KubeVirt components, in particular for example Red Hat OpenShift Virtualization.\nGardener enables Kubernetes consumers to centralize and operate efficiently homogenous Kubernetes clusters across different IaaS providers and even private environments. This way the same cloud-based application version can be hosted and operated by its vendor or consumer on a variety of infrastructures. When a new customer or your development team demands for a new infrastructure provider, Gardener helps you to quickly and easily on-board your workload. Furthermore, on this new infrastructure, Gardener keeps the seamless Kubernetes management experience for your Kubernetes operators, while upholding the consistency of the CI/CD pipeline of your software development team.\nArchitecture and Workflow Gardener is based on the idea of three types of clusters  Garden cluster, Seed cluster and Shoot cluster (see Figure 1). The Garden cluster is used to control the entire Kubernetes environment centrally in a highly scalable design. The highly available seed clusters are used to host the end users (shoot) clusters control planes. Finally, the shoot clusters consist only of worker nodes to host the cloud native applications.\nFigure 1: Gardener Architecture An integration of the Gardener open source project with a new cloud provider follows a standard Gardener extensibility approach. The integration requires two new components: a provider extension and a Machine Controller Manager (MCM) extension. Both components together enable Gardener to instruct the new cloud provider. They run in the Gardener seed clusters that host the control planes of the shoots based on that cloud provider. The role of the provider extension is to manage the provider-specific aspects of the shoot clusters lifecycle, including infrastructure, control plane, worker nodes, and others. It works in cooperation with the MCM extension, which in particular is responsible to handle machines that are provisioned as worker nodes for the shoot clusters. To get this job done, the MCM extension leverages the VM management/API capabilities available with the respective cloud provider.\nSetting up a Kubernetes cluster always involves a flow of interdependent steps (see Figure 2), beginning with the generation of certificates and preparation of the infrastructure, continuing with the provisioning of the control plane and the worker nodes, and ending with the deployment of system components. Gardener can be configured to utilize the KubeVirt extensions in its generic workflow at the right extension points, and deliver the desired outcome of a KubeVirt backed cluster.\nFigure 2: Generic cluster reconciliation flow with extension points Gardener Integration with KubeVirt in Detail Integration with KubeVirt follows the Gardener extensibility concept and introduces the two new components mentioned above: the KubeVirt Provider Extension and the KubeVirt Machine Controller Manager (MCM) Extension.\nFigure 3: Gardener integration with KubeVirt The KubeVirt Provider Extension consists of three separate controllers that handle respectively the infrastructure, the control plane, and the worker nodes of the shoot cluster.\nThe Infrastructure Controller configures the network communication between the shoot worker nodes. By default, shoot worker nodes only use the provider clusters pod network. To achieve higher level of network isolation and better performance, it is possible to add more networks and replace the default pod network with a different network using container network interface (CNI) plugins available in the provider cluster. This is currently based on Multus CNI and NetworkAttachmentDefinitions.\nExample infrastructure configuration in a shoot definition:\nprovider:type:kubevirtinfrastructureConfig:apiVersion:kubevirt.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:tenantNetworks:- name:network-1config:| {\u0026#34;cniVersion\u0026#34;: \u0026#34;0.4.0\u0026#34;,\u0026#34;name\u0026#34;: \u0026#34;bridge-firewall\u0026#34;,\u0026#34;plugins\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;,\u0026#34;isGateway\u0026#34;: true,\u0026#34;isDefaultGateway\u0026#34;: true,\u0026#34;ipMasq\u0026#34;: true,\u0026#34;ipam\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;,\u0026#34;subnet\u0026#34;: \u0026#34;10.100.0.0/16\u0026#34;}},{\u0026#34;type\u0026#34;: \u0026#34;firewall\u0026#34;}]}default:trueThe Control Plane Controller deploys a Cloud Controller Manager (CCM). This is a Kubernetes control plane component that embeds cloud-specific control logic. As any other CCM, it runs the Node controller that is responsible for initializing Node objects, annotating and labeling them with cloud-specific information, obtaining the nodes hostname and IP addresses, and verifying the nodes health. It also runs the Service controller that is responsible for setting up load balancers and other infrastructure components for Service resources that require them.\nFinally, the Worker Controller is responsible for managing the worker nodes of the Gardener shoot clusters.\nExample worker configuration in a shoot definition:\nprovider:type:kubevirtworkers:- name:cpu-workerminimum:1maximum:2machine:type:standard-1image:name:ubuntuversion:\u0026#34;18.04\u0026#34;volume:type:defaultsize:20Gizones:- europe-west1-cFor more information about configuring the KubeVirt Provider Extension as an end-user, see Using the KubeVirt provider extension with Gardener as end-user.\nEnabling Your Gardener Setup to Leverage a KubeVirt Compatible Environment The very first step required is to define the machine types (VM types) for VMs that will be available. This is achieved via the CloudProfile custom resource. The machine types configuration includes details such as CPU, GPU, memory, OS image, and more.\nExample CloudProfile custom resource:\napiVersion:core.gardener.cloud/v1beta1kind:CloudProfilemetadata:name:kubevirtspec:type:kubevirtproviderConfig:apiVersion:kubevirt.provider.extensions.gardener.cloud/v1alpha1kind:CloudProfileConfigmachineImages:- name:ubuntuversions:- version:\u0026#34;18.04\u0026#34;sourceURL:\u0026#34;https://cloud-images.ubuntu.com/bionic/current/bionic-server-cloudimg-amd64.img\u0026#34;kubernetes:versions:- version:\u0026#34;1.18.5\u0026#34;machineImages:- name:ubuntuversions:- version:\u0026#34;18.04\u0026#34;machineTypes:- name:standard-1cpu:\u0026#34;1\u0026#34;gpu:\u0026#34;0\u0026#34;memory:4GivolumeTypes:- name:defaultclass:defaultregions:- name:europe-west1zones:- name:europe-west1-b- name:europe-west1-c- name:europe-west1-dOnce a machine type is defined, it can be referenced in shoot definitions. This information is used by the KubeVirt Provider Extension to generate MachineDeployment and MachineClass custom resources required by the KubeVirt MCM extension for managing the worker nodes of the shoot clusters during the reconciliation process.\nFor more information about configuring the KubeVirt Provider Extension as an operator, see Using the KubeVirt provider extension with Gardener as operator.\nKubeVirt Machine Controller Manager (MCM) Extension The KubeVirt MCM Extension is responsible for managing the VMs that are used as worker nodes of the Gardener shoot clusters using the virtualization capabilities of KubeVirt. This extension handles all necessary lifecycle management activities, such as machines creation, fetching, updating, listing, and deletion.\nThe KubeVirt MCM Extension implements the Gardeners common driver interface for managing VMs in different cloud providers. As already mentioned, the KubeVirt MCM Extension is using the MachineDeployments and MachineClasses  an abstraction layer that follows the Kubernetes native declarative approach - to get instructions from the KubeVirt Provider Extension about the required machines for the shoot worker nodes. Also, the cluster austoscaler integrates with the scale subresource of the MachineDeployment resource. This way, Gardener offers a homogeneous autoscaling experience across all supported providers.\nWhen a new shoot cluster is created or when a new worker node is needed for an existing shoot cluster, a new Machine will be created, and at that time, the KubeVirt MCM extension will create a new KubeVirt VirtualMachine in the provider cluster. This VirtualMachine will be created based on a set of configurations in the MachineClass that follows the specification of the KubeVirt provider.\nThe KubeVirt MCM Extension has two main components. The MachinePlugin is responsible for handling the machine objects, and the PluginSPI is in charge of making calls to the cloud provider interface, to manage its resources.\nFigure 4: KubeVirt MCM extension workflow and architecture As shown in Figure 4, the MachinePlugin receives a machine request from the MCM and starts its processing by decoding the request, doing partial validation, extracting the relevant information, and sending it to the PluginSPI.\nThe PluginSPI then creates, gets, or deletes VirtualMachines depending on the method called by the MachinePlugin. It extracts the kubeconfig of the provider cluster and handles all other required KubeVirt resources such as the secret that holds the cloud-init configurations, and DataVolumes that are mounted as disks to the VMs.\nSupported Environments The Gardener KubeVirt support is currently qualified on:\n KubeVirt v0.32.0 (and later) Red Hat OpenShift Container Platform 4.4 (and later)  There are also plans for further improvements and new features, for example integration with CSI drivers for storage management. Details about the implementation progress can be found in the Gardener project on GitHub.\nYou can find further resources about the open source project Gardener at https://gardener.cloud.\n"},{"uri":"https://gardener.cloud/blog/2020-10/01/","title":"Shoot Reconciliation Details","tags":[],"description":"","content":"Do you want to understand how Gardener creates and updates Kubernetes clusters (Shoots)? Well, it\u0026rsquo;s complicated, but if you are not afraid of large diagrams and are a visual learner like me, this might be useful to you.\nIntroduction In this blog post I will share a technical diagram which attempts to tie together the various components involved when Gardener creates a Kubernetes cluster. I have created and curated the diagram, which visualizes the Shoot reconciliation flow since I started developing on Gardener. Aside from serving as a memory aid for myself, I created it in hopes that it may potentially help contributors to understand a core piece of the complex Gardener machinery. Please be advised that the diagram and components involved are large. Although it can be easily divided into multiple diagrams, I want to show all the components and connections in a single diagram to create an overview of the reconciliation flow.\nThe goal is to visualize the interactions of the components involved in the Shoot creation. It is not intended to serve as a documentation of every component involved.\nBackground Taking a step back, the Gardener READ.me states\n In essence, Gardener is an extension API server that comes along with a bundle of custom controllers. It introduces new API objects in an existing Kubernetes cluster (which is called garden cluster) in order to use them for the management of end-user Kubernetes clusters (which are called shoot clusters). These shoot clusters are described via declarative cluster specifications which are observed by the controllers. They will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.\n This means that Gardener, just like any Kubernetes controller, creates Kubernetes clusters (Shoots) using a reconciliation loop.\nThe Gardenlet contains the controller and reconciliation loop responsible for the creation, update, deletion and migration of Shoot cluster (there are more, but we spare them in this article). In addition, the Gardener Controller Manager also reconciles Shoot resources, but only for seed-independent functionality such as Shoot hibernation, Shoot maintenance or quota control.\nThis blog post is about the reconciliation loop in the Gardenlet responsible for creating and updating Shoot clusters. The code can be found here. The reconciliation loops of the extension controllers can be found in their individual repositories.\nShoot reconciliation flow diagram When Gardner creates a Shoot cluster, there are three conceptual layers involved: the Garden cluster, the Seed cluster and the Shoot cluster. Each layer represents a top-level section in the diagram (similar to a lane in a BPMN diagram).\nIt might seem confusing, that the Shoot cluster itself is a layer, because the whole flow in the first place is about creating the Shoot cluster. I decided to introduce this separate layer to make a clear distinction between which resources exist in the Seed API server (managed by Gardener) and which in the Shoot API server (accessible by the Shoot owner).\nEach section contains several components. Components are mostly Kubernetes resources in a Gardener installation (e.g. the gardenlet deployment in the Seed cluster).\nThis is the list of components:\n(Virtual) Garden Cluster\n Gardener Extension API server Validating Provider Webhooks Project Namespace  Seed Cluster\n Gardenlet Seed API server  every Shoot Control Plane has a dedicated namespace in the Seed.   Cloud Provider (owned by Stakeholder).  Arguably part of the Shoot cluster but used by components in the Seed cluster to create the infrastructure for the Shoot.   Gardener DNS extension Provider Extension (such as gardener-extension-provider-aws) Gardener Extension ETCD Druid Gardener Resource Manager Operating System Extension (such as gardener-extension-os-gardenlinux) Networking extension (such as gardener-extension-networking-cilium) Machine Controller Manager ContainerRuntime Extension (such as gardener-extension-runtime-gvisor) Shoot API server (in the Shoot Namespace in the Seed cluster)  Shoot Cluster\n Cloud Provider compute API (owned by Stakeholder) - for VM/Node creation. VM / Bare metal node hosted by Cloud Provider (in Stakeholder owned account).  How to use the diagram The diagram\n should be read from top to bottom - starting in the top left corner with the creation of the Shoot resource via the Gardener Extension API server. should not require an encompassing documentation / description. More detailed documentation on the components itself, can usually be found in the respective repository. does not show which activities execute in parallel (many) and also does not describe the exact dependencies between the steps. This can be found out by looking at the source code. It however tries to put the activities in a logical order of executing during the reconciliation flow.  Occasionally, there is an info box with additional information next to parts in the diagram that in my point of view require further explanation. Large example resource for the Gardener CRDs (e.g Worker CRD, Infrastructure CRD) are placed on the left side and are referenced by a dotted line (\u0026mdash;\u0026ndash;).\nBe aware, that Gardener is an evolving project, so the diagram will most likely be already outdated by the time you are reading this. Nevertheless, it should give a solid starting point for further explorations into the details of Gardener.\nFlow diagram The diagram can be found below and on Github.com. There are multiple formats available (svg, vsdx, draw.io, html).\nPlease open an issue or open a PR in the repository if information is missing or is incorrect. Thanks!\n\n"},{"uri":"https://gardener.cloud/blog/","title":"Blogs","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/blog/2020-09/","title":"Gardener v1.9 and v1.10 Released","tags":[],"description":"","content":"Summer holidays aren\u0026rsquo;t over yet, still, the Gardener community was able to release two new minor versions in the past weeks. Despite being limited in capacity these days, we were able to reach some major milestones, like adding Kubernetes v1.19 support and the long-delayed automated gardenlet certificate rotation. Whilst we continue to work on topics related to scalability, robustness, and better observability, we agreed to adjust our focus a little more into the areas of development productivity, code quality and unit/integration testing for the upcoming releases.\nNotable Changes in v1.10 Gardener v1.10 was a comparatively small release (measured by the number of changes) but it comes with some major features!\nKubernetes 1.19 support (gardener/gardener#2799) The newest minor release of Kubernetes is now supported by Gardener (and all the maintained provider extensions)! Predominantly, we have enabled CSI migration for OpenStack now that it got promoted to beta, i.e. 1.19 shoots will no longer use the in-tree Cinder volume provisioner. The CSI migration enablement for Azure got postponed (to at least 1.20) due to some issues that the Kubernetes community is trying to fix in the 1.20 release cycle. As usual, the 1.19 release notes should be considered before upgrading your shoot clusters.\nAutomated certificate rotation for gardenlet (gardener/gardener#2542) Similar to the kubelet, the gardenlet supports TLS bootstrapping when deployed into a new seed cluster. It will request a client certificate for the garden cluster using the CertificateSigningRequest API of Kubernetes and store the generated results in a Secret object in the garden namespace of its seed. These certificates are usually valid for one year. We have now added support for automatic renewals if the expiration dates are approaching.\nImproved monitoring alerts (gardener/gardener#2776) We have worked on a larger refactoring to improve reliability and accuracy of our monitoring alerts for both shoot control planes in the seed as well as shoot system components running on worker nodes. The improvements are primarily for operators and should result in less false positive alerts. Also, the alerts should fire less frequently and are better grouped in order to reduce to overall amount of alerts.\nSeed deletion protection (gardener/gardener#2732) Our validation to improve robustness and countermeasures against accidental mistakes has been improved. Earlier, it was possible to remove the use-as-seed annotation for shooted seeds or directly set the deletionTimestamp on Seed objects, despite of the fact that they might still run shoot control planes. Seed deletion would not start in these cases, although, it would disrupt the system unnecessarily, and result in some unexpected behaviour. The Gardener API server is now forbidding such requests if the seeds are not completely empty yet.\nLogging improvements for Loki (multiple PRs) After we released our large logging stack refactoring (from EFK to Loki) with Gardener v1.8, we have continued to work on reliability, quality and user feedback in general. We aren\u0026rsquo;t done yet, though, Gardener v1.10 includes a bunch of improvements which will help to graduate the Logging feature gate to beta and GA, eventually.\nNotable Changes in v1.9 The v1.9 release contained tons of small improvements and adjustments in various areas of the code base and a little less new major features. However, we don\u0026rsquo;t want to miss the opportunity to highlight a few of them.\nCRI validation in CloudProfiles (gardener/gardener#2137) A couple of releases back we have introduced support for containerd and the ContainerRuntime extension API. The supported container runtimes are operating system specific, and until now it wasn\u0026rsquo;t possible for end-users to easily figure out whether they can enable containerd or other ContainerRuntime extensions for their shoots. With this change, Gardener administrators/operators can now provide that information in the .spec.machineImages section in the CloudProfile resource. This also allows for enhanced validation and prevents misconfigurations.\nNew shoot event controller (gardener/gardener#2649) The shoot controllers in both the gardener-controller-manager and gardenlet fire several Events for some important operations (e.g., automated hibernation/wake-up due to hibernation schedule, automated Kubernetes/machine image version update during maintenance, etc.). Earlier, the only way to prolong the lifetime of these events was to modify the --event-ttl command line parameter of the garden cluster\u0026rsquo;s kube-apiserver. This came with the disadvantage that all events were kept for a longer time (not only those related to Shoots that an operator is usually interested in and ideally wants to store for a couple of days). The new shoot event controller allows to achieve this by deleting non-shoot events. This helps operators and end-users to better understand which changes were applied to their shoots by Gardener.\nEarly deployment of the logging stack for new shoots (gardener/gardener#2750) Since the first introduction of the Logging feature gate two years back the logging stack was only deployed at the very end of the shoot creation. This had the disadvantage that control plane pod logs were not kept in case the shoot creation flow is interrupted before the logging stack could be deployed. In some situations, this was preventing fetching relevant information about why a certain control plane component crashed. We now deploy the logging stack very early in the shoot creation flow to always have access to such information.\n"},{"uri":"https://gardener.cloud/blog/2020-08/00/","title":"Gardener v1.8.0 Released","tags":[],"description":"","content":"Even if we are in the midst of the summer holidays, a new Gardener release came out yesterday: v1.8.0! It\u0026rsquo;s main themes are the large change of our logging stack to Loki (which was already explained in detail on a blog post on grafana.com), more configuration options to optimize the utilization of a shoot, node-local DNS, new project roles, and significant improvements for the Kubernetes client that Gardener uses to interact with the many different clusters.\nNotable Changes Logging 2.0: EFK stack replaced by Loki (gardener/gardener#2515) Since two years or so Gardener could optionally provision a dedicated logging stack per seed and per shoot which was based on fluent-bit, fluentd, ElasticSearch and Kibana. This feature was still hidden behind an alpha-level feature gate and never got promoted to beta so far. Due to various limitations of this solution we decided to replace the EFK stack with Loki. As we already have Prometheus and Grafana deployments for both users and operators by default for all clusters the choice was just natural. Please find out more on this topic at this dedicated blog post.\nCluster identities and DNSOwner objects (gardener/gardener#2471, gardener/gardener#2576) The shoot control plane migration topic is ongoing since a few months already, and we are very much progressing with it. A first alpha version will probably make it out soon. As part of these endeavors, we introduced cluster identities and the usage of DNSOwner objects in this release. Both are needed to gracefully migrate the DNSEntry extension objects from the old seed to the new seed as part of the control plane migration process. Please find out more on this topic at this blog post.\nNew uam role for Project members to limit user access management privileges (gardener/gardener#2611) In order to allow external user access management system to integrate with Gardener and to fulfil certain compliance aspects, we have introduced a new role called uam for Project members (next to admin and viewer). Only if a user has this role then he/she is allowed to add/remove other human users to the respective Project. By default, all newly created Projects assign this role only to the owner while, for backwards-compatibility reasons, it will be assigned for all members for existing projects. Project owners can steadily revoke this access as desired. Interestingly, the uam role is backed by a custom RBAC verb called manage-members, i.e., the Gardener API server is only admitting changes to the human Project members if the respective user is bound to this RBAC verb.\nNew node-local DNS feature for shoots (gardener/gardener#2528) By default, we are using CoreDNS as DNS plugin in shoot clusters which we auto-scale horizontally using HPA. However, in some situations we are discovering certain bottlenecks with it, e.g., unreliable UDP connections, unnecessary node hopping, inefficient load balancing, etc. To further optimize the DNS performance for shoot clusters, it is now possible to enable a new alpha-level feature gate in the gardenlet\u0026rsquo;s componentconfig: NodeLocalDNS. If enabled, all shoots will get a new DaemonSet to run a DNS server on each node.\nMore kubelet and API server configurability (gardener/gardener#2574, gardener/gardener#2668) One large benefit of Gardener is that it allows you to optimize the usage of your control plane as well as worker nodes by exposing relevant configuration parameters in the Shoot API. In this version, we are adding support to configure kubelet\u0026rsquo;s values for systemReserved and kubeReserved resources as well as the kube-apiserver\u0026rsquo;s watch cache sizes. This allows end-users to get to better node utilization and/or performance for their shoot clusters.\nConfigurable timeout settings for machine-controller-manager (gardener/gardener#2563) One very central component in Project Gardener is the machine-controller-manager for managing the worker nodes of shoot clusters. It has extensive qualities with respect to node lifecycle management and rolling updates. As such, it uses certain timeout values, e.g. when creating or draining nodes, or when checking their health. Earlier, those were not customizable by end-users, but we are adding this possibility now. You can fine-grain these settings per worker pool in the Shoot API such that you can optimize the lifecycle management of your worker nodes even more!\nImproved usage of cached client to reduce network I/O (gardener/gardener#2635, gardener/gardener#2637) In the last Gardener release v1.7 we have introduced a huge refactoring the clients that we use to interact with the many different Kubernetes clusters. This is to further optimize the network I/O performed by leveraging watches and caches as good as possible. It\u0026rsquo;s still an alpha-level feature that must be explicitly enabled in the Gardenlet\u0026rsquo;s component configuration, though, with this release we have improved certain things in order to pave the way for beta promotion. For example, we were initially also using a cached client when interacting with shoots. However, as the gardenlet runs in the seed as well (and thus can communicate cluster-internally with the kube-apiservers of the respective shoots) this cache is not necessary and just memory overhead. We have removed it again and saw the memory usage getting lower again. More to come!\nAWS EBS volume encryption by default (gardener/gardener-extension-provider-aws#147) The Shoot API already exposed the possibility to encrypt the root disks of worker nodes since quite a while, but it was disabled by default (for backwards-compatibility reasons). With this release we have change this default, so new shoot worker nodes will be provisioned with encrypted root disks out-of-the-box. However, the g4dn instance types of AWS don\u0026rsquo;t support this encryption, so when you use them you have to explicitly disable the encryption in the worker pool configuration.\nLiveness probe for Gardener API server deployment (gardener/gardener#2647) A small, but very valuable improvement is the introduction of a liveness probe for our Gardener API server. As it\u0026rsquo;s built with the same library like the Kubernetes API server, it exposes two endpoints at /livez and /readyz which were created exactly for the purpose of live- and readiness probes. With Gardener v1.8 the Helm chart contains a liveness probe configuration by default, and we are awaiting an upstream fix (kubernetes/kubernetes#93599) to also enable the readiness probe. This will help in a smoother rolling update of the Gardener API server pods, i.e., preventing clients from talking to a not yet initialized or already terminating API server instance.\nWebhook ports changed to enable OpenShift (gardener/gardener#2660) In order to make it possible to run Gardener on OpenShift clusters as well, we had to make a change in the port configuration for the webhooks we are using in both Gardener and the extension controllers. Earlier, all the webhook servers directly exposed port 443, i.e., a system port which is a security concern and disallowed in OpenShift. We have changed this port now across all places and also adapted our network policies accordingly. This is most likely not the last necessary change to enable this scenario, however, it\u0026rsquo;s a great improvement to push the project forward.\nIf you\u0026rsquo;re interested in more details and even more improvements you can find all release notes for Gardener v1.8.0 here: https://github.com/gardener/gardener/releases/tag/v1.8.0\n"},{"uri":"https://gardener.cloud/blog/2020-05/00/","title":"PingCAPs Experience in Implementing their Managed TiDB Service with Gardener","tags":[],"description":"","content":"Gardener is showing successful collaboration with its growing community of contributors and adopters. With this come some success stories, including PingCAP using Gardener to implement its managed service.\nAbout PingCAP and its TiDB Cloud PingCAP started in 2015, when three seasoned infrastructure engineers working at leading Internet companies got sick and tired of the way databases were managed, scaled and maintained. Seeing no good solution on the market, they decided to build their own - the open-source way. With the help of a first-class team and hundreds of contributors from around the globe, PingCAP is building a distributed NewSQL, hybrid transactional and analytical processing (HTAP) database.\nIts flagship project, TiDB, is a cloud-native distributed SQL database with MySQL compatibility, and one of the most popular open-source database projects - with 23.5K+ stars and 400+ contributors. Its sister project TiKV is a Cloud Native Interactive Landscape project.\nPingCAP envisioned their managed TiDB service, known as TiDB Cloud, to be multi-tenant, secure, cost-efficient, and to be compatible with different cloud providers. As a result, the company turned to Gardener to build their managed TiDB cloud service offering.\nTiDB Cloud Beta Preview Limitations with other public managed Kubernetes services Previously, PingCAP encountered issues while using other public managed K8s cluster services, to develop the first version of its TiDB Cloud. Their worst pain point was that they felt helpless when encountering certain malfunctions. PingCAP wasnt able to do much to resolve these issues, except waiting for the providers help. More specifically, they experienced problems due to cloud-provider specific Kubernetes system upgrades, delays in the support response (which could be avoided in exchange of a costly support fee), and no control over when things got fixed.\nThere was also a lot of cloud-specific integration work needed to follow a multi-cloud strategy, which proved to be expensive both to produce and maintain. With one of these managed K8s services, you would have to integrate the instance API, as opposed to a solution like Gardener, which provides a unified API for all clouds. Such a unified API eliminates the need to worry about cloud specific-integration work altogether.\nWhy PingCAP chose Gardener to build TiDB Cloud  Gardener has similar concepts to Kubernetes. Each Kubernetes cluster is just like a Kubernetes pod, so the similar concepts apply, and the controller pattern makes Gardener easy to manage. It was also easy to extend, as the team was already very familiar with Kubernetes, so it wasnt hard for us to extend Gardener. We also saw that Gardener has a very active community, which is always a plus!- Aylei Wu, (Cloud Engineer) at PingCAP\n At first glance, PingCAP had initial reservations about using Gardener - mainly due to its adoption level (still at the beginning) and an apparent complexity of use. However, these were soon eliminated as they learned more about the solution. As Aylei Wu mentioned during the last Gardener community meeting, a good product speaks for itself, and once the company got familiar with Gardener, they quickly noticed that the concepts were very similar to Kubernetes, which they were already familiar with.\nThey recognized that Gardener would be their best option, as it is highly extensible and provides a unified abstraction API layer. In essence, the machines can be managed via a machine controller manager for different cloud providers - without having to worry about the individual cloud APIs.\nThey agreed that Gardeners solution, although complex, was definitely worth it. Even though it is a relatively new solution, meaning they didnt have access to other user testimonials, they decided to go with the service since it checked all the boxes (and as SAP was running it productively with a huge fleet). PingCAP also came to the conclusion that building a managed Kubernetes service themselves would not be easy. Even if they were to build a managed K8s service, they would have to heavily invest in development and would still end up with an even more complex platform than Gardeners. For all these reasons combined, PingCAP decided to go with Gardener to build its TiDB Cloud.\nHere are certain features of Gardener that PingCAP found appealing:\n Cloud agnostic: Gardeners abstractions for cloud-specific integrations dramatically reduce the investment in supporting more than one cloud infrastructure. Once the integration with Amazon Web Services was done, moving on to Google Cloud Platform proved to be relatively easy. (At the moment, TiDB Cloud has subscription plans available for both GCP and AWS, and they are planning to support Alibaba Cloud in the future.) Familiar concepts: Gardener is K8s native; its concepts are easily related to core Kubernetes concepts. As such, it was easy to onboard for a K8s experienced team like PingCAPs SRE team. Easy to manage and extend: Gardeners API and extensibility are easy to implement, which has a positive impact on the implementation, maintenance costs and time-to-market. Active community: Prompt and quality responses on Slack from the Gardener team tremendously helped to quickly onboard and produce an efficient solution.  How PingCAP built TiDB Cloud with Gardener On a technical level, PingCAPs set-up overview includes the following:\n A Base Cluster globally, which is the top-level control plane of TiDB Cloud A Seed Cluster per cloud provider per region, which makes up the fundamental data plane of TiDB Cloud A Shoot Cluster is dynamically provisioned per tenant per cloud provider per region when requested A tenant may create one or more TiDB clusters in a Shoot Cluster  As a real world example, PingCAP sets up the Base Cluster and Seed Clusters in advance. When a tenant creates its first TiDB cluster under the us-west-2 region of AWS, a Shoot Cluster will be dynamically provisioned in this region, and will host all the TiDB clusters of this tenant under us-west-2. Nevertheless, if another tenant requests a TiDB cluster in the same region, a new Shoot Cluster will be provisioned. Since different Shoot Clusters are located in different VPCs and can even be hosted under different AWS accounts, TiDB Cloud is able to achieve hard isolation between tenants and meet the critical security requirements for our customers.\nTo automate these processes, PingCAP creates a service in the Base Cluster, known as the TiDB Cloud Central service. The Central is responsible for managing shoots and the TiDB clusters in the Shoot Clusters. As shown in the following diagram, user operations go to the Central, being authenticated, authorized, validated, stored and then applied asynchronously in a controller manner. The Central will talk to the Gardener API Server to create and scale Shoot clusters. The Central will also access the Shoot API Service to deploy and reconcile components in the Shoot cluster, including control components (TiDB Operator, API Proxy, Usage Reporter for billing, etc.) and the TiDB clusters.\nTiDB Cloud on Gardener Architecture Overview Whats next for PingCAP and Gardener With the initial success of using the project to build TiDB Cloud, PingCAP is now working heavily on the stability and day-to-day operations of TiDB Cloud on Gardener. This includes writing Infrastructure-as-Code scripts/controllers with it to achieve GitOps, building tools to help diagnose problems across regions and clusters, as well as running chaos tests to identify and eliminate potential risks. After benefiting greatly from the community, PingCAP will continue to contribute back to Gardener.\nIn the future, PingCAP also plans to support more cloud providers like AliCloud and Azure. Moreover, PingCAP may explore the opportunity of running TiDB Cloud in on-premise data centers with the constantly expanding support this project provides. Engineers at PingCAP enjoy the ease of learning from Gardeners kubernetes-like concepts and being able to apply them everywhere. Gone are the days of heavy integrations with different clouds and worrying about vendor stability. With this project, PingCAP now sees broader opportunities to land TiDB Cloud on various infrastructures to meet the needs of their global user group.\nStay tuned, more blog posts to come on how Gardener is collaborating with its contributors and adopters to bring fully-managed clusters at scale everywhere! If you want to join in on the fun, connect with our community.\n"},{"uri":"https://gardener.cloud/blog/2020_week_20/00/","title":"New Website, Same Green Flower","tags":[],"description":"","content":"The Gardener project website just received a serious facelift. Here are some of the highlights:\n A completely new landing page, emphasizing both on Gardener\u0026rsquo;s value proposition and the open community behind it. The Community page was reconstructed for quick access to the various community channels and will soon merge the Adopters page. It will provide a better insight into success stories from the communty. A completely new News section and content type available at /documentation/news. Use metadata such as publishdate and archivedate to schedule for news publish and archive automatically, regardless of when you contributed them. You can now track what\u0026rsquo;s happening from the landing page or in the dedicated News section on the website and share. Improved blogs layout. One-click sharing options are available starting with simple URL copy link and twitter button and others will closely follow up. While we are at it, give it a try. Spread the word.  Website builds also got to a new level with:\n Containerization. The whole build environment is containerized now, eliminating differences between local and CI/CD setup and reducing content developers focus only to the /documentation repository. Running a local server for live preview of changes as you make them when developing content for the website, is now as easy as runing make serve in your local /documentation clone. Numerous improvements to the buld scripts. More configuration options, authenticated requests, fault tollerance and performance. Good news for Windows WSL users who will now nejoy a significantly support. See the updated README for details on that. A number of improvements in layouts styles, site assets and hugo site-building techniques.  But hey, THAT\u0026rsquo;S NOT ALL!\nStay tuned for more improvements around the corner. The biggest ones are aligning the documentation with the new theme and restructuring it along, more emphasis on community success stories all around, more sharing options and more than a handful of shortcodes for content development and \u0026hellip; let\u0026rsquo;s cut the spoilers here.\nI hope you will like it. Let us know what you think about it. Feel free to leave comments and discuss on Twitter and Slack, or in case of issues - on GitHub.\nGo ahead and help us spread the word: https://gardener.cloud\n  "},{"uri":"https://gardener.cloud/blog/2019_week_21/","title":"Cluster Overprovisioning","tags":[],"description":"","content":"This tutorial describes how to overprovisioning of cluster nodes for scaling and failover. This is desired when you have work load that need to scale up quickly without waiting for the new cluster nodes to be created and join the cluster.\nA similar problem occurs when crashing a node from the Hyperscaler. This must be replaced by Kubernetes as fast as possible. The solution can be overprovisioning of nodes\n..read some more on Cluster Overprovisioning.\n"},{"uri":"https://gardener.cloud/blog/2019_week_21_2/","title":"Feature Flags in Kubernetes Applications","tags":[],"description":"","content":"Feature flags are used to change the behavior of a program at runtime without forcing a restart.\nAlthough they are essential in a native cloud environment, they cannot be implemented without significant effort on some platforms. Kubernetes has made this trivial. Here we will implement them through labels and annotations, but you can also implement them by connecting directly to the Kubernetes API Server.\nPossible Use Cases\n turn on/off a specific instance turn on/off profiling of a specific instance change the logging level, to capture detailed logs during a specific event change caching strategy at runtime change timeouts in production toggle on/off some special verification  ..read some more on Feature Flags for App.\n"},{"uri":"https://gardener.cloud/blog/2019_week_06/","title":"Manually adding a node to an existing cluster","tags":[],"description":"","content":"Gardener has an excellent ability to automatically scale machines for the cluster. From the point of view of scalability, there is no need for manual intervention.\nThis tutorial is useful for those end-users who need specifically configured nodes, which are not yet supported by Gardener. For example: an end-user who wants some workload that requires runnc instead of runc as container runtime.\n..read some more on Adding Nodes to a Cluster.\n"},{"uri":"https://gardener.cloud/blog/2019_week_02/","title":"Organizing Access Using kubeconfig Files","tags":[],"description":"","content":"The kubectl command-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.\n What happens if your kubeconfig file of your production cluster is leaked or published by accident?\n Since there is no possibility to rotate or revoke the initial kubeconfig, there is only one way to protect your infrastructure or application if it is has leaked - delete the cluster.\n..learn more on Work with kubeconfig files.\n"},{"uri":"https://gardener.cloud/blog/2018_week_40/","title":"Hibernate a Cluster to save money","tags":[],"description":"","content":"You want to experiment with Kubernetes or have set up a customer scenario, but you don\u0026rsquo;t want to run the cluster 24 / 7 for reasons of cost?\nThe Gardener gives you the possibility to scale your cluster down to zero nodes.\n..read some more on Hibernate a Cluster.\n"},{"uri":"https://gardener.cloud/blog/2018_week_22/","title":"Anti Patterns","tags":[],"description":"","content":"Running as root user Whenever possible, do not run containers as root users. One could be tempted to say that Kubernetes Pods and Node are well separated. The host and the container share the same kernel. If the container is compromised, a root user can damage the underlying node. Use RUN groupadd -r anygroup \u0026amp;\u0026amp; useradd -r -g anygroup myuser to create a group and a user in it. Use the USER command to switch to this user.\nStoring data or logs in containers Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the container, as they are lost when the container is closed. If absolutely necessary, you can use persistence volumes instead to persist them outside the containers. However, an ELK stack is preferred for storing and processing log files.\n..read some more on Common Kubernetes Antipattern.\n"},{"uri":"https://gardener.cloud/blog/2018_week_46/","title":"Auditing Kubernetes for Secure Setup","tags":[],"description":"","content":"In summer 2018, the Gardener project team asked Kinvolk to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work is to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a Control-Plane-as-a-Service with a network air gap.\nAlong the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.\n..read some more on Auditing Kubernetes for Secure Setup.\n"},{"uri":"https://gardener.cloud/blog/2018_week_07/","title":"Big things come in small packages","tags":[],"description":"","content":"Microservices tend to use smaller runtimes but you can use what you have today - and this can be a problem in kubernetes.\nSwitching your architecture from a monolith to microservices has many advantages, both in the way you write software and the way it is used throughout its lifecycle. In this post, my attempt is to cover one problem which does not get as much attention and discussion - size of the technology stack.\nGeneral purpose technology stack There is a tendency to be more generalized in development and to apply this pattern to all services. One feels that a homogeneous image of the technology stack is good if it is the same for all services.\nOne forgets, however, that a large percentage of the integrated infrastructure is not used by all services in the same way, and is therefore only a burden. Thus, resources are wasted and the entire application becomes expensive in operation and scales very badly.\nLight technology stack Due to the lightweight nature of your service, you can run more containers on a physical server and virtual machines. The result is higher resource utilization.\nAdditionally, microservices are developed and deployed as containers independently of each another. This means that a development team can develop, optimize and deploy a microservice without impacting other subsystems.\n"},{"uri":"https://gardener.cloud/blog/2018_week_51/","title":"Cookies are dangerous...","tags":[],"description":"","content":"\u0026hellip;they mess up the figure.\nFor a team event during the Christmas season we decided to completely reinterpret the topic cookies\u0026hellip; since the vegetables have gone on a well-deserved vacation. :-)\nGet recipe on Gardener Cookies.\n"},{"uri":"https://gardener.cloud/blog/2018_week_17/","title":"Frontend HTTPS","tags":[],"description":"","content":"For encrypted communication between the client to the load balancer, you need to specify a TLS private key and certificate to be used by the ingress controller.\nCreate a secret in the namespace of the ingress containing the TLS private key and certificate. Then configure the secret name in the TLS configuration section of the ingress specification.\n..read on HTTPS - Self Signed Certificates how to configure it.\n"},{"uri":"https://gardener.cloud/blog/2018_week_50/","title":"Hardening the Gardener Community Setup","tags":[],"description":"","content":"The Gardener project team has analyzed the impact of the Gardener CVE-2018-2475 and the Kubernetes CVE-2018-1002105 on the Gardener Community Setup. Following some recommendations it is possible to mitigate both vulnerabilities.\nRead more on Hardening the Gardener Community Setup.\n"},{"uri":"https://gardener.cloud/blog/2018_week_06/","title":"Kubernetes is available in Docker for Mac 17.12 CE","tags":[],"description":"","content":"    Kubernetes is only available in Docker for Mac 17.12 CE and higher on the Edge channel. Kubernetes support is not included in Docker for Mac Stable releases. To find out more about Stable and Edge channels and how to switch between them, see general configuration.     Docker for Mac 17.12 CE (and higher) Edge includes a standalone Kubernetes server that runs on Mac, so that you can test deploying your Docker workloads on Kubernetes. The Kubernetes client command, kubectl, is included and configured to connect to the local Kubernetes server. If you have kubectl already installed and pointing to some other environment, such as minikube or a GKE cluster, be sure to change context so that kubectl is pointing to docker-for-desktop:\n\u0026hellip;see more on Docker.com\nI recommend to setup your shell to see which KUBECONFIG is active.\n"},{"uri":"https://gardener.cloud/blog/2018_week_09/","title":"Namespace Isolation","tags":[],"description":"","content":"\u0026hellip;or DENY all traffic from other namespaces\nYou can configure a NetworkPolicy to deny all traffic from other namespaces while allowing all traffic coming from the same namespace the pod is deployed to. There are many reasons why you may chose to configure Kubernetes network policies:\n Isolate multi-tenant deployments Regulatory compliance Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each another  ..read on Namespace Isolation how to configure it.\n"},{"uri":"https://gardener.cloud/blog/2018_week_08_2/","title":"Namespace Scope","tags":[],"description":"","content":"Should I use:\n one namespace per user/developer?  one namespace per team?  one per service type?  one namespace per application type?  one namespace per running instance of your application?  Apply the Principle of Least Privilege\nAll user accounts should run at all times as few privileges as possible, and also launch applications with as few privileges as possible. If you share a cluster for different user separated by a namespace, all user has access to all namespaces and services per default. It can happen that a user accidentally uses and destroys the namespace of a productive application or the namespace of another developer.\nKeep in mind: By default namespaces don\u0026rsquo;t provide:\n Network isolation Access Control Audit Logging on user level  "},{"uri":"https://gardener.cloud/blog/2018_week_27/","title":"ReadWriteMany - Dynamically Provisioned Persistent Volumes Using Amazon EFS","tags":[],"description":"","content":"The efs-provisioner allows you to mount EFS storage as PersistentVolumes in kubernetes. It consists of a container that has access to an AWS EFS resource. The container reads a configmap containing the EFS filesystem ID, the AWS region and the name identifying the efs-provisioner. This name will be used later when you create a storage class.\nWhy EFS  When you have application running on multiple nodes which require shared access to a file system When you have an application that requires multiple virtual machines to access the same file system at the same time, AWS EFS is a tool that you can use. EFS supports encryption. EFS is SSD based storage and its storage capacity and pricing will scale in or out as needed, so there is no need for the system administrator to do additional operations. It can grow to a petabyte scale. EFS now supports NFSv4 lock upgrading and downgrading, so yes, you can use sqlite with EFS even if it was possible before. Easy to setup  Why Not EFS  Sometimes when you think about using a service like EFS, you may also think about vendor lock-in and its negative sides Making an EFS backup may decrease your production FS performance; the throughput used by backup counts towards your total file system throughput. EFS is expensive compared to EBS (roughly twice the price of EBS storage) EFS is not the magical solution for all your distributed FS problems, it can be slow in many cases. Test, benchmark and measure to ensure your if EFS is a good solution for your use case. EFS distributed architecture results in a latency overhead for each file read/write operation. If you have the possibility to use a CDN, dont use EFS, use it for the files which can\u0026rsquo;t be stored in a CDN. Dont use EFS as a caching system, sometimes you could be doing this unintentionally. Last but not least, even if EFS is a fully managed NFS, you will face performance problems in many cases, resolving them takes time and needs effort.  "},{"uri":"https://gardener.cloud/blog/2018_week_10/","title":"Shared storage with S3 backend","tags":[],"description":"","content":"The storage is definitely the most complex and important part of an application setup, once this part is completed, one of the most problematic parts could be solved.\nMounting a S3 bucket into a pod using FUSE allows to access data stored in S3 via the filesystem. The mount is a pointer to an S3 location, so the data is never synced locally. Once mounted, any pod can read or even write from that directory without the need for explicit keys.\nHowever, it can be used to import and parse large amounts of data into a database.\n..read on Shared S3 Storage how to configure it.\n"},{"uri":"https://gardener.cloud/blog/2018_week_08/","title":"Watching logs of several pods","tags":[],"description":"","content":"One thing that always bothered me was that I couldn\u0026rsquo;t get logs of several pods at once with kubectl. A simple tail -f \u0026lt;path-to-logfile\u0026gt; isn\u0026rsquo;t possible. Certainly you can use kubectl logs -f \u0026lt;pod-id\u0026gt;, but it doesn\u0026rsquo;t help if you want to monitor more than one pod at a time.\nThis is something you really need a lot, at least if you run several instances of a pod behind a deployment and you don\u0026rsquo;t have setup a log viewer service like Kibana.\nkubetail comes to the rescue, it is a small bash script that allows you to aggregate log files of several pods at the same time in a simple way. The script is called kubetail and is available at GitHub.\n"},{"uri":"https://gardener.cloud/documentation/guides/applications/access_pod_from_local/","title":"Access a port of a pod locally","tags":[],"description":"","content":"Question You deployed an application with a web UI or an internal endpoint in your Kubernetes (K8s) cluster. How can I access this endpoint without an external load balancer (e.g. Ingress)? This tutorial presents two options:\n Using Kubernetes port forward Using Kubernetes apiserver proxy  Please note that the options described here are mostly for quick testing or troubleshooting your application. For enabling access to your application for productive environment, please refer to Access my service\nSolution 1: Using Kubernetes port forward You could use the port forwarding functionality of kubectl to access the pods from your local host without involving a service.\nTo access any pod follow these steps:\n Run kubectl get pods Note down the name of the pod in question as \u0026lt;your-pod-name\u0026gt; Run kubectl port-forward \u0026lt;your-pod-name\u0026gt; \u0026lt;local-port\u0026gt;:\u0026lt;your-app-port\u0026gt; Run a web browser or curl locally and enter the URL http(s)://localhost:\u0026lt;local-port\u0026gt;  In addition, kubectl port-forward allows to use a resource name. such as a deployment or service name, to select a matching pod to port forward. Find more details in the Kubernetes documentation.\nThe main drawback of this approach is that the pod\u0026rsquo;s name will change as soon as it is restarted. Moreover, you need to have a web browser on your client and you need to make sure that the local port is not already used by an application running on your system. Finally, sometimes port forwarding is canceled due to non obvious reasons. This leads to a kind of shaky approach. A more robust approach is to access the application using kube-proxy.\nSolution 2: Using apiserver proxy There are several different proxies used with Kubernetes, the official documentation provides a good overview.\nIn this tutorial we are using apiserver proxy to enable access to services running in Kubernetes without using an Ingress. Different from the first solution, a service is required for this solution .\nUse the following URL to access a service via apiserver proxy. For details about apiserver proxy URLs read Discovering builtin services\nhttps://\u0026lt;cluster-master\u0026gt;/api/v1/namespace/\u0026lt;namespace\u0026gt;/services/\u0026lt;service\u0026gt;:\u0026lt;service-port\u0026gt;/proxy/\u0026lt;service-endpoint\u0026gt;\nExample:\n   cluster-master namespace service yservice-port service-endpoint url to access service     api.testclstr.cpet.k8s.sapcloud.io default nginx-svc 80 / url   api.testclstr.cpet.k8s.sapcloud.io default docker-nodejs-svc 4500 /cpu?baseNumber=4 url    There are applications, which do not yet support relative URLs like Prometheus (as of end of November, 2017). This typically leads to missing JavaScript objects when trying to open the URL in a browser. In this case use the port-forward approach described above.\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/applications/access_pod_from_local/","title":"Access a port of a pod locally","tags":[],"description":"","content":"Question You deployed an application with a web UI or an internal endpoint in your Kubernetes (K8s) cluster. How can I access this endpoint without an external load balancer (e.g. Ingress)? This tutorial presents two options:\n Using Kubernetes port forward Using Kubernetes apiserver proxy  Please note that the options described here are mostly for quick testing or troubleshooting your application. For enabling access to your application for productive environment, please refer to Access my service\nSolution 1: Using Kubernetes port forward You could use the port forwarding functionality of kubectl to access the pods from your local host without involving a service.\nTo access any pod follow these steps:\n Run kubectl get pods Note down the name of the pod in question as \u0026lt;your-pod-name\u0026gt; Run kubectl port-forward \u0026lt;your-pod-name\u0026gt; \u0026lt;local-port\u0026gt;:\u0026lt;your-app-port\u0026gt; Run a web browser or curl locally and enter the URL http(s)://localhost:\u0026lt;local-port\u0026gt;  In addition, kubectl port-forward allows to use a resource name. such as a deployment or service name, to select a matching pod to port forward. Find more details in the Kubernetes documentation.\nThe main drawback of this approach is that the pod\u0026rsquo;s name will change as soon as it is restarted. Moreover, you need to have a web browser on your client and you need to make sure that the local port is not already used by an application running on your system. Finally, sometimes port forwarding is canceled due to non obvious reasons. This leads to a kind of shaky approach. A more robust approach is to access the application using kube-proxy.\nSolution 2: Using apiserver proxy There are several different proxies used with Kubernetes, the official documentation provides a good overview.\nIn this tutorial we are using apiserver proxy to enable access to services running in Kubernetes without using an Ingress. Different from the first solution, a service is required for this solution .\nUse the following URL to access a service via apiserver proxy. For details about apiserver proxy URLs read Discovering builtin services\nhttps://\u0026lt;cluster-master\u0026gt;/api/v1/namespace/\u0026lt;namespace\u0026gt;/services/\u0026lt;service\u0026gt;:\u0026lt;service-port\u0026gt;/proxy/\u0026lt;service-endpoint\u0026gt;\nExample:\n   cluster-master namespace service yservice-port service-endpoint url to access service     api.testclstr.cpet.k8s.sapcloud.io default nginx-svc 80 / url   api.testclstr.cpet.k8s.sapcloud.io default docker-nodejs-svc 4500 /cpu?baseNumber=4 url    There are applications, which do not yet support relative URLs like Prometheus (as of end of November, 2017). This typically leads to missing JavaScript objects when trying to open the URL in a browser. In this case use the port-forward approach described above.\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/applications/access_pod_from_local/","title":"Access a port of a pod locally","tags":[],"description":"","content":"Question You deployed an application with a web UI or an internal endpoint in your Kubernetes (K8s) cluster. How can I access this endpoint without an external load balancer (e.g. Ingress)? This tutorial presents two options:\n Using Kubernetes port forward Using Kubernetes apiserver proxy  Please note that the options described here are mostly for quick testing or troubleshooting your application. For enabling access to your application for productive environment, please refer to Access my service\nSolution 1: Using Kubernetes port forward You could use the port forwarding functionality of kubectl to access the pods from your local host without involving a service.\nTo access any pod follow these steps:\n Run kubectl get pods Note down the name of the pod in question as \u0026lt;your-pod-name\u0026gt; Run kubectl port-forward \u0026lt;your-pod-name\u0026gt; \u0026lt;local-port\u0026gt;:\u0026lt;your-app-port\u0026gt; Run a web browser or curl locally and enter the URL http(s)://localhost:\u0026lt;local-port\u0026gt;  In addition, kubectl port-forward allows to use a resource name. such as a deployment or service name, to select a matching pod to port forward. Find more details in the Kubernetes documentation.\nThe main drawback of this approach is that the pod\u0026rsquo;s name will change as soon as it is restarted. Moreover, you need to have a web browser on your client and you need to make sure that the local port is not already used by an application running on your system. Finally, sometimes port forwarding is canceled due to non obvious reasons. This leads to a kind of shaky approach. A more robust approach is to access the application using kube-proxy.\nSolution 2: Using apiserver proxy There are several different proxies used with Kubernetes, the official documentation provides a good overview.\nIn this tutorial we are using apiserver proxy to enable access to services running in Kubernetes without using an Ingress. Different from the first solution, a service is required for this solution .\nUse the following URL to access a service via apiserver proxy. For details about apiserver proxy URLs read Discovering builtin services\nhttps://\u0026lt;cluster-master\u0026gt;/api/v1/namespace/\u0026lt;namespace\u0026gt;/services/\u0026lt;service\u0026gt;:\u0026lt;service-port\u0026gt;/proxy/\u0026lt;service-endpoint\u0026gt;\nExample:\n   cluster-master namespace service yservice-port service-endpoint url to access service     api.testclstr.cpet.k8s.sapcloud.io default nginx-svc 80 / url   api.testclstr.cpet.k8s.sapcloud.io default docker-nodejs-svc 4500 /cpu?baseNumber=4 url    There are applications, which do not yet support relative URLs like Prometheus (as of end of November, 2017). This typically leads to missing JavaScript objects when trying to open the URL in a browser. In this case use the port-forward approach described above.\n"},{"uri":"https://gardener.cloud/documentation/guides/applications/service-access/","title":"Access service from outside Kubernetes cluster","tags":[],"description":"Is there an ingress deployed and how is it configured","content":"TL;DR To expose your application / service for access from outside the cluster, following options exist:\n Kubernetes Service of type LoadBalancer Kubernetes Service of type \u0026lsquo;NodePort\u0026rsquo; + Ingress  This tutorial discusses how to enable access to your application from outside the Kubernetes cluster (sometimes called North-South traffic). For internal communication amongst pods and services (sometimes called East-West traffic) there are many examples, here is one brief example.\nService Types A Service in Kubernetes is an abstraction defining a logical set of Pods and an access policy.\nServices can be exposed in different ways by specifying a type in the service spec, and different types determine accessibility from inside and outside of cluster.\n ClusterIP NodePort LoadBalancer  Type ExternalName is a special case of service and not discussed here.\nType ClusterIP A service of type ClusterIP exposes a service on an internal IP in the cluster, which makes the service only reachable from within the cluster. This is the default value if no type is specified.\napiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginx-appreplicas:1template:metadata:labels:app:nginx-appspec:containers:- name:nginximage:nginx:1.13.12ports:- containerPort:80---apiVersion:v1kind:Servicemetadata:labels:app:nginx-appname:nginx-svcnamespace:defaultspec:type:ClusterIP# use ClusterIP as type hereports:- port:80selector:app:nginx-appExecute following commands to create deployment and service\nkubectl create -f \u0026lt;Your yaml file name\u0026gt; Checking the service status\n$ kubectl get svc nginx-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-svc ClusterIP 100.66.125.61 \u0026lt;none\u0026gt; 80/TCP 45m As shown above, the service is assigned with a cluster ip address and port 80 as defined in configuration file. You can test the service like this:\n# list all existing pods in cluster $ kubectl get pods NAME READY STATUS RESTARTS AGE docker-nodejs-app-76b77494-vwv4d 1/1 Running 0 11d nginx-deployment-74d949bf69-nvdzs 1/1 Running 0 1h privileged-pod 1/1 Running 0 11d # test service from within the cluster on the same pod $ kubectl exec -it nginx-deployment-74d949bf69-nvdzs curl 100.66.125.61:80 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 612 100 612 0 0 1006k 0 --:--:-- --:--:-- --:--:-- 597k \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; ...   Tip\n The service is also accessible from any other container (even from different pods) within the same cluster, e.g. kubectl -it exec \u0026lt;another POD_NAME\u0026gt; curl \u0026lt;YourServiceClusterIP:YourPort\u0026gt;. You need to make sure command curl is installed in the container. You can also find out the dns name of the ClusterIP by command kubectl exec -it \u0026lt;POD_NAME\u0026gt; nslookup \u0026lt;ClusterIP\u0026gt;, replace the IP address with the resolved name in your test. The resolved name typically looks like nginx-svc.default.svc.cluster.local where nginx-svc is the name of your service defined in the configuration file.   Type NodePort Follow the previous example, just replace the type with NodePort\n...spec:type:NodePortports:- port:80...A service of type NodePort is a ClusterIP service with an additional capability: it is reachable at the IP address of the node as well as at the assigned cluster IP on the services network. The way this is accomplished is pretty straightforward: when Kubernetes creates a NodePort service kube-proxy allocates a port in the range 3000032767 and opens this port on every node (thus the name NodePort). Connections to this port are forwarded to the services cluster IP. If we create the service above and run kubectl get svc \u0026lt;your-service\u0026gt;, we can see the NodePort that has been allocated for it.\nNote that in the in following example, in addition to port 80, port 32521 has been opened as well on the node, in contrast to the output of \u0026ldquo;ClusterIP\u0026rdquo; case where only port 80 is opened.\n$ kubectl get svc nginx-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-svc NodePort 100.70.105.182 \u0026lt;none\u0026gt; 80:32521/TCP 16m Therefore you can access the service from within the cluster in two ways:\n Access via ClusterIP:port  #via ClusterIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl 100.70.105.182:80 #via internal name of ClusterIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl nginx-svc.default.svc.cluster.local:80  Access via NodeIP:NodePort  # First find out the Node IP address $ kubectl describe node Name: ip-10-250-20-203.eu-central-1.compute.internal Roles: node Addresses: InternalIP: 10.250.20.203 InternalDNS: ip-10-250-20-203.eu-central-1.compute.internal Hostname: ip-10-250-20-203.eu-central-1.compute.internal ... #via NodeIP:NodePort kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl 10.250.20.203:32521 #via internal name of NodeIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl ip-10-250-20-203.eu-central-1.compute.internal:32521 Type LoadBalancer The LoadBalancer type is the simplest approach, which is created by specifying type as LoadBalancer.\napiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginx-appreplicas:1template:metadata:labels:app:nginx-appspec:containers:- name:nginximage:nginx:1.13.12ports:- containerPort:80---apiVersion:v1kind:Servicemetadata:labels:app:nginx-appname:nginx-svcnamespace:defaultspec:type:LoadBalancer# use LoadBalancer as type hereports:- port:80selector:app:nginx-appOnce the service is created, it has an external IP address as shown here:\n$ kubectl get services -l app=nginx-app -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR nginx-svc LoadBalancer 100.67.182.148 a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com 80:31196/TCP 9m app=nginx-app A service of type LoadBalancer combines the capabilities of a NodePort with the ability to setup a complete ingress path.\nHence the service can be accessible from outside the cluster without the need for additional components like an Ingress.\nTo test the external IP run this curl command from your local machine:\n$ curl http://a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com StatusCode : 200 StatusDescription : OK Content : \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;... RawContent : HTTP/1.1 200 OK ... Obviously the service can also is accessed from within the cluster. You can test this in the same way as described in section NodePort.\nLoadBalancer vs. Ingress As presented in the previous section, only the service type LoadBalancer enables access from outside the cluster. However this approach has its own limitation. You cannot configure a LoadBalancer to terminate HTTPS traffic, virtual hosts or path-based routing. In Kubernetes 1.2 a separate resource called Ingress is introduced for this purpose.\nWhy an Ingress LoadBalancer services are all about extending a service to support external clients. By contrast an Ingress is a a separate resource that configures a LoadBalancer in a more flexible way. The Ingress API supports TLS termination, virtual hosts, and path-based routing. It can easily set up a load balancer to handle multiple backend services. In addition routing traffic is realised in a different way. In the case of the LoadBalancer service, the traffic entering through the external load balancer is forwarded to the kube-proxy that in turn forwards the traffic to the selected pods. In contrast, the Ingress LoadBalancer forwards the traffic straight to the selected pods which is more efficient.\nTypically a service of type LoadBalancer costs at least 40$ per month. This means if your applications needs 10 of them you already pay 400$ per month just for load balancing.\nHow to use the ingress? In the cluster, a nginx-ingress controller has been deployed for you as an LoadBalancer and also registered the DNS record. Depending on how your cluster is defined, the DNS registration is performed under following conventions:\n k8s-hana.ondemand.com  \u0026lt;gardener_cluster_name\u0026gt;.\u0026lt;gardener_project_name\u0026gt;.shoot.canary.k8s-hana.ondemand.com.\nBoth \u0026lt;gardener_cluster_name\u0026gt; and \u0026lt;gardener_project_name\u0026gt; are defined in Gardener which can be determined on Gardener dashboard.\nThis results in the following default DNS endpoints:\n api.\u0026lt;cluster_domain\u0026gt; Kubernetes API *.ingress.\u0026lt;cluster_domain\u0026gt; Internal nginx ingress  Example: Configure an Ingress resource with Service type: NodePort With the configuration below you can reach your service nginx-svc with:\nhttp://test.ingress.\u0026amp;lt;GARDENER-CLUSTER-NAME\u0026amp;gt;.\u0026amp;lt;GARDENER-PROJECT-NAME\u0026amp;gt;.shoot.canary.k8s-hana.ondemand.com\napiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginx-appreplicas:1template:metadata:labels:app:nginx-appspec:containers:- name:nginximage:nginx:1.13.12ports:- containerPort:80---apiVersion:v1kind:Servicemetadata:labels:app:nginx-appname:nginx-svcnamespace:defaultspec:type:NodePortports:- port:80selector:app:nginx-app---apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:nginxsvc-ingressspec:rules:- host:nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:nginx-svcservicePort:80Show the newly created ingress and test it :\n$ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE nginxsvc-ingress nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com 10.250.20.203 80 29s $ curl nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com StatusCode : 200 StatusDescription : OK Content : \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; ... Reference:  Concepts: Kubernetes Service Concepts: Connecting Applications with Services Tutorial: Using a Service to Expose Your App Tutorial: Using Source IP Kubernetes Networking Accessing Kubernetes Pods from Outside of the Cluster  "},{"uri":"https://gardener.cloud/v1.12.8/guides/applications/service-access/","title":"Access service from outside Kubernetes cluster","tags":[],"description":"Is there an ingress deployed and how is it configured","content":"TL;DR To expose your application / service for access from outside the cluster, following options exist:\n Kubernetes Service of type LoadBalancer Kubernetes Service of type \u0026lsquo;NodePort\u0026rsquo; + Ingress  This tutorial discusses how to enable access to your application from outside the Kubernetes cluster (sometimes called North-South traffic). For internal communication amongst pods and services (sometimes called East-West traffic) there are many examples, here is one brief example.\nService Types A Service in Kubernetes is an abstraction defining a logical set of Pods and an access policy.\nServices can be exposed in different ways by specifying a type in the service spec, and different types determine accessibility from inside and outside of cluster.\n ClusterIP NodePort LoadBalancer  Type ExternalName is a special case of service and not discussed here.\nType ClusterIP A service of type ClusterIP exposes a service on an internal IP in the cluster, which makes the service only reachable from within the cluster. This is the default value if no type is specified.\napiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginx-appreplicas:1template:metadata:labels:app:nginx-appspec:containers:- name:nginximage:nginx:1.13.12ports:- containerPort:80---apiVersion:v1kind:Servicemetadata:labels:app:nginx-appname:nginx-svcnamespace:defaultspec:type:ClusterIP# use ClusterIP as type hereports:- port:80selector:app:nginx-appExecute following commands to create deployment and service\nkubectl create -f \u0026lt;Your yaml file name\u0026gt; Checking the service status\n$ kubectl get svc nginx-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-svc ClusterIP 100.66.125.61 \u0026lt;none\u0026gt; 80/TCP 45m As shown above, the service is assigned with a cluster ip address and port 80 as defined in configuration file. You can test the service like this:\n# list all existing pods in cluster $ kubectl get pods NAME READY STATUS RESTARTS AGE docker-nodejs-app-76b77494-vwv4d 1/1 Running 0 11d nginx-deployment-74d949bf69-nvdzs 1/1 Running 0 1h privileged-pod 1/1 Running 0 11d # test service from within the cluster on the same pod $ kubectl exec -it nginx-deployment-74d949bf69-nvdzs curl 100.66.125.61:80 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 612 100 612 0 0 1006k 0 --:--:-- --:--:-- --:--:-- 597k \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; ...   Tip\n The service is also accessible from any other container (even from different pods) within the same cluster, e.g. kubectl -it exec \u0026lt;another POD_NAME\u0026gt; curl \u0026lt;YourServiceClusterIP:YourPort\u0026gt;. You need to make sure command curl is installed in the container. You can also find out the dns name of the ClusterIP by command kubectl exec -it \u0026lt;POD_NAME\u0026gt; nslookup \u0026lt;ClusterIP\u0026gt;, replace the IP address with the resolved name in your test. The resolved name typically looks like nginx-svc.default.svc.cluster.local where nginx-svc is the name of your service defined in the configuration file.   Type NodePort Follow the previous example, just replace the type with NodePort\n...spec:type:NodePortports:- port:80...A service of type NodePort is a ClusterIP service with an additional capability: it is reachable at the IP address of the node as well as at the assigned cluster IP on the services network. The way this is accomplished is pretty straightforward: when Kubernetes creates a NodePort service kube-proxy allocates a port in the range 3000032767 and opens this port on every node (thus the name NodePort). Connections to this port are forwarded to the services cluster IP. If we create the service above and run kubectl get svc \u0026lt;your-service\u0026gt;, we can see the NodePort that has been allocated for it.\nNote that in the in following example, in addition to port 80, port 32521 has been opened as well on the node, in contrast to the output of \u0026ldquo;ClusterIP\u0026rdquo; case where only port 80 is opened.\n$ kubectl get svc nginx-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-svc NodePort 100.70.105.182 \u0026lt;none\u0026gt; 80:32521/TCP 16m Therefore you can access the service from within the cluster in two ways:\n Access via ClusterIP:port  #via ClusterIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl 100.70.105.182:80 #via internal name of ClusterIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl nginx-svc.default.svc.cluster.local:80  Access via NodeIP:NodePort  # First find out the Node IP address $ kubectl describe node Name: ip-10-250-20-203.eu-central-1.compute.internal Roles: node Addresses: InternalIP: 10.250.20.203 InternalDNS: ip-10-250-20-203.eu-central-1.compute.internal Hostname: ip-10-250-20-203.eu-central-1.compute.internal ... #via NodeIP:NodePort kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl 10.250.20.203:32521 #via internal name of NodeIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl ip-10-250-20-203.eu-central-1.compute.internal:32521 Type LoadBalancer The LoadBalancer type is the simplest approach, which is created by specifying type as LoadBalancer.\napiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginx-appreplicas:1template:metadata:labels:app:nginx-appspec:containers:- name:nginximage:nginx:1.13.12ports:- containerPort:80---apiVersion:v1kind:Servicemetadata:labels:app:nginx-appname:nginx-svcnamespace:defaultspec:type:LoadBalancer# use LoadBalancer as type hereports:- port:80selector:app:nginx-appOnce the service is created, it has an external IP address as shown here:\n$ kubectl get services -l app=nginx-app -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR nginx-svc LoadBalancer 100.67.182.148 a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com 80:31196/TCP 9m app=nginx-app A service of type LoadBalancer combines the capabilities of a NodePort with the ability to setup a complete ingress path.\nHence the service can be accessible from outside the cluster without the need for additional components like an Ingress.\nTo test the external IP run this curl command from your local machine:\n$ curl http://a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com StatusCode : 200 StatusDescription : OK Content : \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;... RawContent : HTTP/1.1 200 OK ... Obviously the service can also is accessed from within the cluster. You can test this in the same way as described in section NodePort.\nLoadBalancer vs. Ingress As presented in the previous section, only the service type LoadBalancer enables access from outside the cluster. However this approach has its own limitation. You cannot configure a LoadBalancer to terminate HTTPS traffic, virtual hosts or path-based routing. In Kubernetes 1.2 a separate resource called Ingress is introduced for this purpose.\nWhy an Ingress LoadBalancer services are all about extending a service to support external clients. By contrast an Ingress is a a separate resource that configures a LoadBalancer in a more flexible way. The Ingress API supports TLS termination, virtual hosts, and path-based routing. It can easily set up a load balancer to handle multiple backend services. In addition routing traffic is realised in a different way. In the case of the LoadBalancer service, the traffic entering through the external load balancer is forwarded to the kube-proxy that in turn forwards the traffic to the selected pods. In contrast, the Ingress LoadBalancer forwards the traffic straight to the selected pods which is more efficient.\nTypically a service of type LoadBalancer costs at least 40$ per month. This means if your applications needs 10 of them you already pay 400$ per month just for load balancing.\nHow to use the ingress? In the cluster, a nginx-ingress controller has been deployed for you as an LoadBalancer and also registered the DNS record. Depending on how your cluster is defined, the DNS registration is performed under following conventions:\n k8s-hana.ondemand.com  \u0026lt;gardener_cluster_name\u0026gt;.\u0026lt;gardener_project_name\u0026gt;.shoot.canary.k8s-hana.ondemand.com.\nBoth \u0026lt;gardener_cluster_name\u0026gt; and \u0026lt;gardener_project_name\u0026gt; are defined in Gardener which can be determined on Gardener dashboard.\nThis results in the following default DNS endpoints:\n api.\u0026lt;cluster_domain\u0026gt; Kubernetes API *.ingress.\u0026lt;cluster_domain\u0026gt; Internal nginx ingress  Example: Configure an Ingress resource with Service type: NodePort With the configuration below you can reach your service nginx-svc with:\nhttp://test.ingress.\u0026amp;lt;GARDENER-CLUSTER-NAME\u0026amp;gt;.\u0026amp;lt;GARDENER-PROJECT-NAME\u0026amp;gt;.shoot.canary.k8s-hana.ondemand.com\napiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginx-appreplicas:1template:metadata:labels:app:nginx-appspec:containers:- name:nginximage:nginx:1.13.12ports:- containerPort:80---apiVersion:v1kind:Servicemetadata:labels:app:nginx-appname:nginx-svcnamespace:defaultspec:type:NodePortports:- port:80selector:app:nginx-app---apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:nginxsvc-ingressspec:rules:- host:nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:nginx-svcservicePort:80Show the newly created ingress and test it :\n$ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE nginxsvc-ingress nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com 10.250.20.203 80 29s $ curl nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com StatusCode : 200 StatusDescription : OK Content : \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; ... Reference:  Concepts: Kubernetes Service Concepts: Connecting Applications with Services Tutorial: Using a Service to Expose Your App Tutorial: Using Source IP Kubernetes Networking Accessing Kubernetes Pods from Outside of the Cluster  "},{"uri":"https://gardener.cloud/v1.13.2/guides/applications/service-access/","title":"Access service from outside Kubernetes cluster","tags":[],"description":"Is there an ingress deployed and how is it configured","content":"TL;DR To expose your application / service for access from outside the cluster, following options exist:\n Kubernetes Service of type LoadBalancer Kubernetes Service of type \u0026lsquo;NodePort\u0026rsquo; + Ingress  This tutorial discusses how to enable access to your application from outside the Kubernetes cluster (sometimes called North-South traffic). For internal communication amongst pods and services (sometimes called East-West traffic) there are many examples, here is one brief example.\nService Types A Service in Kubernetes is an abstraction defining a logical set of Pods and an access policy.\nServices can be exposed in different ways by specifying a type in the service spec, and different types determine accessibility from inside and outside of cluster.\n ClusterIP NodePort LoadBalancer  Type ExternalName is a special case of service and not discussed here.\nType ClusterIP A service of type ClusterIP exposes a service on an internal IP in the cluster, which makes the service only reachable from within the cluster. This is the default value if no type is specified.\napiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginx-appreplicas:1template:metadata:labels:app:nginx-appspec:containers:- name:nginximage:nginx:1.13.12ports:- containerPort:80---apiVersion:v1kind:Servicemetadata:labels:app:nginx-appname:nginx-svcnamespace:defaultspec:type:ClusterIP# use ClusterIP as type hereports:- port:80selector:app:nginx-appExecute following commands to create deployment and service\nkubectl create -f \u0026lt;Your yaml file name\u0026gt; Checking the service status\n$ kubectl get svc nginx-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-svc ClusterIP 100.66.125.61 \u0026lt;none\u0026gt; 80/TCP 45m As shown above, the service is assigned with a cluster ip address and port 80 as defined in configuration file. You can test the service like this:\n# list all existing pods in cluster $ kubectl get pods NAME READY STATUS RESTARTS AGE docker-nodejs-app-76b77494-vwv4d 1/1 Running 0 11d nginx-deployment-74d949bf69-nvdzs 1/1 Running 0 1h privileged-pod 1/1 Running 0 11d # test service from within the cluster on the same pod $ kubectl exec -it nginx-deployment-74d949bf69-nvdzs curl 100.66.125.61:80 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 612 100 612 0 0 1006k 0 --:--:-- --:--:-- --:--:-- 597k \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; ...   Tip\n The service is also accessible from any other container (even from different pods) within the same cluster, e.g. kubectl -it exec \u0026lt;another POD_NAME\u0026gt; curl \u0026lt;YourServiceClusterIP:YourPort\u0026gt;. You need to make sure command curl is installed in the container. You can also find out the dns name of the ClusterIP by command kubectl exec -it \u0026lt;POD_NAME\u0026gt; nslookup \u0026lt;ClusterIP\u0026gt;, replace the IP address with the resolved name in your test. The resolved name typically looks like nginx-svc.default.svc.cluster.local where nginx-svc is the name of your service defined in the configuration file.   Type NodePort Follow the previous example, just replace the type with NodePort\n...spec:type:NodePortports:- port:80...A service of type NodePort is a ClusterIP service with an additional capability: it is reachable at the IP address of the node as well as at the assigned cluster IP on the services network. The way this is accomplished is pretty straightforward: when Kubernetes creates a NodePort service kube-proxy allocates a port in the range 3000032767 and opens this port on every node (thus the name NodePort). Connections to this port are forwarded to the services cluster IP. If we create the service above and run kubectl get svc \u0026lt;your-service\u0026gt;, we can see the NodePort that has been allocated for it.\nNote that in the in following example, in addition to port 80, port 32521 has been opened as well on the node, in contrast to the output of \u0026ldquo;ClusterIP\u0026rdquo; case where only port 80 is opened.\n$ kubectl get svc nginx-svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-svc NodePort 100.70.105.182 \u0026lt;none\u0026gt; 80:32521/TCP 16m Therefore you can access the service from within the cluster in two ways:\n Access via ClusterIP:port  #via ClusterIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl 100.70.105.182:80 #via internal name of ClusterIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl nginx-svc.default.svc.cluster.local:80  Access via NodeIP:NodePort  # First find out the Node IP address $ kubectl describe node Name: ip-10-250-20-203.eu-central-1.compute.internal Roles: node Addresses: InternalIP: 10.250.20.203 InternalDNS: ip-10-250-20-203.eu-central-1.compute.internal Hostname: ip-10-250-20-203.eu-central-1.compute.internal ... #via NodeIP:NodePort kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl 10.250.20.203:32521 #via internal name of NodeIP kubectl exec -it nginx-deployment-74d949bf69-7n6bs curl ip-10-250-20-203.eu-central-1.compute.internal:32521 Type LoadBalancer The LoadBalancer type is the simplest approach, which is created by specifying type as LoadBalancer.\napiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginx-appreplicas:1template:metadata:labels:app:nginx-appspec:containers:- name:nginximage:nginx:1.13.12ports:- containerPort:80---apiVersion:v1kind:Servicemetadata:labels:app:nginx-appname:nginx-svcnamespace:defaultspec:type:LoadBalancer# use LoadBalancer as type hereports:- port:80selector:app:nginx-appOnce the service is created, it has an external IP address as shown here:\n$ kubectl get services -l app=nginx-app -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR nginx-svc LoadBalancer 100.67.182.148 a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com 80:31196/TCP 9m app=nginx-app A service of type LoadBalancer combines the capabilities of a NodePort with the ability to setup a complete ingress path.\nHence the service can be accessible from outside the cluster without the need for additional components like an Ingress.\nTo test the external IP run this curl command from your local machine:\n$ curl http://a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com StatusCode : 200 StatusDescription : OK Content : \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;... RawContent : HTTP/1.1 200 OK ... Obviously the service can also is accessed from within the cluster. You can test this in the same way as described in section NodePort.\nLoadBalancer vs. Ingress As presented in the previous section, only the service type LoadBalancer enables access from outside the cluster. However this approach has its own limitation. You cannot configure a LoadBalancer to terminate HTTPS traffic, virtual hosts or path-based routing. In Kubernetes 1.2 a separate resource called Ingress is introduced for this purpose.\nWhy an Ingress LoadBalancer services are all about extending a service to support external clients. By contrast an Ingress is a a separate resource that configures a LoadBalancer in a more flexible way. The Ingress API supports TLS termination, virtual hosts, and path-based routing. It can easily set up a load balancer to handle multiple backend services. In addition routing traffic is realised in a different way. In the case of the LoadBalancer service, the traffic entering through the external load balancer is forwarded to the kube-proxy that in turn forwards the traffic to the selected pods. In contrast, the Ingress LoadBalancer forwards the traffic straight to the selected pods which is more efficient.\nTypically a service of type LoadBalancer costs at least 40$ per month. This means if your applications needs 10 of them you already pay 400$ per month just for load balancing.\nHow to use the ingress? In the cluster, a nginx-ingress controller has been deployed for you as an LoadBalancer and also registered the DNS record. Depending on how your cluster is defined, the DNS registration is performed under following conventions:\n k8s-hana.ondemand.com  \u0026lt;gardener_cluster_name\u0026gt;.\u0026lt;gardener_project_name\u0026gt;.shoot.canary.k8s-hana.ondemand.com.\nBoth \u0026lt;gardener_cluster_name\u0026gt; and \u0026lt;gardener_project_name\u0026gt; are defined in Gardener which can be determined on Gardener dashboard.\nThis results in the following default DNS endpoints:\n api.\u0026lt;cluster_domain\u0026gt; Kubernetes API *.ingress.\u0026lt;cluster_domain\u0026gt; Internal nginx ingress  Example: Configure an Ingress resource with Service type: NodePort With the configuration below you can reach your service nginx-svc with:\nhttp://test.ingress.\u0026amp;lt;GARDENER-CLUSTER-NAME\u0026amp;gt;.\u0026amp;lt;GARDENER-PROJECT-NAME\u0026amp;gt;.shoot.canary.k8s-hana.ondemand.com\napiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginx-appreplicas:1template:metadata:labels:app:nginx-appspec:containers:- name:nginximage:nginx:1.13.12ports:- containerPort:80---apiVersion:v1kind:Servicemetadata:labels:app:nginx-appname:nginx-svcnamespace:defaultspec:type:NodePortports:- port:80selector:app:nginx-app---apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:nginxsvc-ingressspec:rules:- host:nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:nginx-svcservicePort:80Show the newly created ingress and test it :\n$ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE nginxsvc-ingress nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com 10.250.20.203 80 29s $ curl nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com StatusCode : 200 StatusDescription : OK Content : \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; ... Reference:  Concepts: Kubernetes Service Concepts: Connecting Applications with Services Tutorial: Using a Service to Expose Your App Tutorial: Using Source IP Kubernetes Networking Accessing Kubernetes Pods from Outside of the Cluster  "},{"uri":"https://gardener.cloud/adopter/","title":"Adopters","tags":[],"description":"","content":"See who is using Gardener Gardener adopters in production environments that have publicly shared details of their usage.       SAP uses Gardener to deploy and manage Kubernetes clusters at scale in a uniform way across infrastructures (AWS, Azure, GCP, Alicloud, as well as generic interfaces to OpenStack and vSphere). Workloads include Databases (SAP HANA Cloud), Big Data (SAP Data Intelligence), Kyma, many other cloud native applications, and diverse business workloads.    ScaleUp Technologies runs Gardener within their public Openstack Clouds (Hamburg, Berlin, Dsseldorf). Their clients run all kinds of workloads on top of Gardener maintained Kubernetes clusters ranging from databases to Software-as-a-Service applications.    Finanz Informatik Technologie Services GmbH uses Gardener to offer k8s as a service for customers in the financial industry in Germany. It is built on top of a \"metal as a service\" infrastructure implemented from scratch for k8s workloads in mind. The result is k8s on top of bare metal in minutes.    PingCAP TiDB, is a cloud-native distributed SQL database with MySQL compatibility, and one of the most popular open-source database projects - with 23.5K+ stars and 400+ contributors. Its sister project TiKV is a Cloud Native Interactive Landscape project. PingCAP envisioned their managed TiDB service, known as TiDB Cloud, to be multi-tenant, secure, cost-efficient, and to be compatible with different cloud providers and they chose Gardener.    Beezlabs uses Gardener to deliver Intelligent Process Automation platform, on multiple cloud providers and reduce costs and lock-in risks.    bnerd uses Gardener as the core technology for its own managed Kubernetes as a Service solution and operates multiple Gardener installations for several cloud hosting service providers.    STACKIT is a digital brand of Europes biggest retailer, the Schwarz Group, which includes Lidl, Kaufland, but also production and recycling companies. It uses Gardener to offer public and private Kubernetes as a service in own data centers in Europe and targets to become the cloud provider for German and European small and mid-sized companies.    Supporting and managing multiple application landscapes on-premises and across different hyperscaler infrastructures can be painful. At T-Systems we use Gardener both for internal usage and to manage clusters for our customers. We love the openness of the project, the flexibility and the architecture that allows us to manage clusters around the world with only one team from one single pane of glass and to meet industry specific certification standards. The sovereignty by design is another great value, the technology implicitly brings along.   If youre using Gardener and you arent on this list, submit a pull request!    "},{"uri":"https://gardener.cloud/documentation/concepts/monitoring/alerting/","title":"Alerting","tags":[],"description":"","content":"Alerting Gardener uses Prometheus to gather metrics from each component. A Prometheus is deployed in each shoot control plane (on the seed) which is responsible for gathering control plane and cluster metrics. Prometheus can be configured to fire alerts based on these metrics and send them to an alertmanager. The alertmanager is responsible for sending the alerts to users and operators. This document describes how to setup alerting for:\n end-users/stakeholders/customers operators/administrators  Alerting for Users To receive email alerts as a user set the following values in the shoot spec:\nspec:monitoring:alerting:emailReceivers:- john.doe@example.comemailReceivers is a list of emails that will receive alerts if something is wrong with the shoot cluster. A list of alerts for users can be found here.\nAlerting for Operators Currently, Gardener supports two options for alerting:\n Email Alerting Sending Alerts to an external alertmanager  A list of operator alerts can be found here.\nEmail Alerting Gardener provides the option to deploy an alertmanager into each seed. This alertmanager is responsible for sending out alerts to operators for each shoot cluster in the seed. Only email alerts are supported by the alertmanager managed by Gardener. This is configurable by setting the Gardener controller manager configuration values alerting. See this on how to configure the Gardener\u0026rsquo;s SMTP secret. If the values are set, a secret with the label gardener.cloud/role: alerting will be created in the garden namespace of the garden cluster. This secret will be used by each alertmanager in each seed.\nExternal Alertmanager The alertmanager supports different kinds of alerting configurations. The alertmanager provided by Gardener only supports email alerts. If email is not sufficient, then alerts can be sent to an external alertmanager. Prometheus will send alerts to a URL and then alerts will be handled by the external alertmanager. This external alertmanager is operated and configured by the operator (i.e. Gardener does not configure or deploy this alertmanager). To configure sending alerts to an external alertmanager, create a secret in the virtual garden cluster in the garden namespace with the label: gardener.cloud/role: alerting. This secret needs to contain a URL to the external alertmanager and information regarding authentication. Supported authentication types are:\n No Authentication (none) Basic Authentication (basic) Mutual TLS (certificate)  Remote Alertmanager Examples Note: the url value cannot be prepended with http or https.\n# No AuthenticationapiVersion:v1kind:Secretmetadata:labels:gardener.cloud/role:alertingname:alerting-authnamespace:gardendata:# No Authenticationauth_type:base64(none)url:base64(external.alertmanager.foo)# Basic Authauth_type:base64(basic)url:base64(extenal.alertmanager.foo)username:base64(admin)password:base64(password)# Mutual TLSauth_type:base64(certificate)url:base64(external.alertmanager.foo)ca.crt:base64(ca)tls.crt:base64(certificate)tls.key:base64(key)# Email Alerts (internal alertmanager)auth_type:base64(smtp)auth_identity:base64(internal.alertmanager.auth_identity)auth_password:base64(internal.alertmanager.auth_password)auth_username:base64(internal.alertmanager.auth_username)from:base64(internal.alertmanager.from)smarthost:base64(internal.alertmanager.smarthost)to:base64(internal.alertmanager.to)type:OpaqueConfiguring your External Alertmanager Please refer to the alertmanager documentation on how to configure an alertmanager.\nWe recommend you use at least the following inhibition rules in your alertmanager configuration to prevent excessive alerts:\ninhibit_rules:# Apply inhibition if the alert name is the same.- source_match:severity:criticaltarget_match:severity:warningequal:[\u0026#39;alertname\u0026#39;,\u0026#39;service\u0026#39;,\u0026#39;cluster\u0026#39;]# Stop all alerts for type=shoot if there are VPN problems.- source_match:service:vpntarget_match_re:type:shootequal:[\u0026#39;type\u0026#39;,\u0026#39;cluster\u0026#39;]# Stop warning and critical alerts if there is a blocker- source_match:severity:blockertarget_match_re:severity:^(critical|warning)$equal:[\u0026#39;cluster\u0026#39;]# If the API server is down inhibit no worker nodes alert. No worker nodes depends on kube-state-metrics which depends on the API server.- source_match:service:kube-apiservertarget_match_re:service:nodesequal:[\u0026#39;cluster\u0026#39;]# If API server is down inhibit kube-state-metrics alerts.- source_match:service:kube-apiservertarget_match_re:severity:infoequal:[\u0026#39;cluster\u0026#39;]# No Worker nodes depends on kube-state-metrics. Inhibit no worker nodes if kube-state-metrics is down.- source_match:service:kube-state-metrics-shoottarget_match_re:service:nodesequal:[\u0026#39;cluster\u0026#39;]Below is a graph visualizing the inhibition rules:\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/monitoring/alerting/","title":"Alerting","tags":[],"description":"","content":"Alerting Gardener uses Prometheus to gather metrics from each component. A Prometheus is deployed in each shoot control plane (on the seed) which is responsible for gathering control plane and cluster metrics. Prometheus can be configured to fire alerts based on these metrics and send them to an alertmanager. The alertmanager is responsible for sending the alerts to users and operators. This document describes how to setup alerting for:\n end-users/stakeholders/customers operators/administrators  Alerting for Users To receive email alerts as a user set the following values in the shoot spec:\nspec:monitoring:alerting:emailReceivers:- john.doe@example.comemailReceivers is a list of emails that will receive alerts if something is wrong with the shoot cluster. A list of alerts for users can be found here.\nAlerting for Operators Currently, Gardener supports two options for alerting:\n Email Alerting Sending Alerts to an external alertmanager  A list of operator alerts can be found here.\nEmail Alerting Gardener provides the option to deploy an alertmanager into each seed. This alertmanager is responsible for sending out alerts to operators for each shoot cluster in the seed. Only email alerts are supported by the alertmanager managed by Gardener. This is configurable by setting the Gardener controller manager configuration values alerting. See this on how to configure the Gardener\u0026rsquo;s SMTP secret. If the values are set, a secret with the label gardener.cloud/role: alerting will be created in the garden namespace of the garden cluster. This secret will be used by each alertmanager in each seed.\nExternal Alertmanager The alertmanager supports different kinds of alerting configurations. The alertmanager provided by Gardener only supports email alerts. If email is not sufficient, then alerts can be sent to an external alertmanager. Prometheus will send alerts to a URL and then alerts will be handled by the external alertmanager. This external alertmanager is operated and configured by the operator (i.e. Gardener does not configure or deploy this alertmanager). To configure sending alerts to an external alertmanager, create a secret in the virtual garden cluster in the garden namespace with the label: gardener.cloud/role: alerting. This secret needs to contain a URL to the external alertmanager and information regarding authentication. Supported authentication types are:\n No Authentication (none) Basic Authentication (basic) Mutual TLS (certificate)  Remote Alertmanager Examples Note: the url value cannot be prepended with http or https.\n# No AuthenticationapiVersion:v1kind:Secretmetadata:labels:gardener.cloud/role:alertingname:alerting-authnamespace:gardendata:# No Authenticationauth_type:base64(none)url:base64(external.alertmanager.foo)# Basic Authauth_type:base64(basic)url:base64(extenal.alertmanager.foo)username:base64(admin)password:base64(password)# Mutual TLSauth_type:base64(certificate)url:base64(external.alertmanager.foo)ca.crt:base64(ca)tls.crt:base64(certificate)tls.key:base64(key)# Email Alerts (internal alertmanager)auth_type:base64(smtp)auth_identity:base64(internal.alertmanager.auth_identity)auth_password:base64(internal.alertmanager.auth_password)auth_username:base64(internal.alertmanager.auth_username)from:base64(internal.alertmanager.from)smarthost:base64(internal.alertmanager.smarthost)to:base64(internal.alertmanager.to)type:OpaqueConfiguring your External Alertmanager Please refer to the alertmanager documentation on how to configure an alertmanager.\nWe recommend you use at least the following inhibition rules in your alertmanager configuration to prevent excessive alerts:\ninhibit_rules:# Apply inhibition if the alert name is the same.- source_match:severity:criticaltarget_match:severity:warningequal:[\u0026#39;alertname\u0026#39;,\u0026#39;service\u0026#39;,\u0026#39;cluster\u0026#39;]# Stop all alerts for type=shoot if there are VPN problems.- source_match:service:vpntarget_match_re:type:shootequal:[\u0026#39;type\u0026#39;,\u0026#39;cluster\u0026#39;]# Stop warning and critical alerts if there is a blocker- source_match:severity:blockertarget_match_re:severity:^(critical|warning)$equal:[\u0026#39;cluster\u0026#39;]# If the API server is down inhibit no worker nodes alert. No worker nodes depends on kube-state-metrics which depends on the API server.- source_match:service:kube-apiservertarget_match_re:service:nodesequal:[\u0026#39;cluster\u0026#39;]# If API server is down inhibit kube-state-metrics alerts.- source_match:service:kube-apiservertarget_match_re:severity:infoequal:[\u0026#39;cluster\u0026#39;]# No Worker nodes depends on kube-state-metrics. Inhibit no worker nodes if kube-state-metrics is down.- source_match:service:kube-state-metrics-shoottarget_match_re:service:nodesequal:[\u0026#39;cluster\u0026#39;]Below is a graph visualizing the inhibition rules:\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/monitoring/alerting/","title":"Alerting","tags":[],"description":"","content":"Alerting Gardener uses Prometheus to gather metrics from each component. A Prometheus is deployed in each shoot control plane (on the seed) which is responsible for gathering control plane and cluster metrics. Prometheus can be configured to fire alerts based on these metrics and send them to an alertmanager. The alertmanager is responsible for sending the alerts to users and operators. This document describes how to setup alerting for:\n end-users/stakeholders/customers operators/administrators  Alerting for Users To receive email alerts as a user set the following values in the shoot spec:\nspec:monitoring:alerting:emailReceivers:- john.doe@example.comemailReceivers is a list of emails that will receive alerts if something is wrong with the shoot cluster. A list of alerts for users can be found here.\nAlerting for Operators Currently, Gardener supports two options for alerting:\n Email Alerting Sending Alerts to an external alertmanager  A list of operator alerts can be found here.\nEmail Alerting Gardener provides the option to deploy an alertmanager into each seed. This alertmanager is responsible for sending out alerts to operators for each shoot cluster in the seed. Only email alerts are supported by the alertmanager managed by Gardener. This is configurable by setting the Gardener controller manager configuration values alerting. See this on how to configure the Gardener\u0026rsquo;s SMTP secret. If the values are set, a secret with the label gardener.cloud/role: alerting will be created in the garden namespace of the garden cluster. This secret will be used by each alertmanager in each seed.\nExternal Alertmanager The alertmanager supports different kinds of alerting configurations. The alertmanager provided by Gardener only supports email alerts. If email is not sufficient, then alerts can be sent to an external alertmanager. Prometheus will send alerts to a URL and then alerts will be handled by the external alertmanager. This external alertmanager is operated and configured by the operator (i.e. Gardener does not configure or deploy this alertmanager). To configure sending alerts to an external alertmanager, create a secret in the virtual garden cluster in the garden namespace with the label: gardener.cloud/role: alerting. This secret needs to contain a URL to the external alertmanager and information regarding authentication. Supported authentication types are:\n No Authentication (none) Basic Authentication (basic) Mutual TLS (certificate)  Remote Alertmanager Examples Note: the url value cannot be prepended with http or https.\n# No AuthenticationapiVersion:v1kind:Secretmetadata:labels:gardener.cloud/role:alertingname:alerting-authnamespace:gardendata:# No Authenticationauth_type:base64(none)url:base64(external.alertmanager.foo)# Basic Authauth_type:base64(basic)url:base64(extenal.alertmanager.foo)username:base64(admin)password:base64(password)# Mutual TLSauth_type:base64(certificate)url:base64(external.alertmanager.foo)ca.crt:base64(ca)tls.crt:base64(certificate)tls.key:base64(key)# Email Alerts (internal alertmanager)auth_type:base64(smtp)auth_identity:base64(internal.alertmanager.auth_identity)auth_password:base64(internal.alertmanager.auth_password)auth_username:base64(internal.alertmanager.auth_username)from:base64(internal.alertmanager.from)smarthost:base64(internal.alertmanager.smarthost)to:base64(internal.alertmanager.to)type:OpaqueConfiguring your External Alertmanager Please refer to the alertmanager documentation on how to configure an alertmanager.\nWe recommend you use at least the following inhibition rules in your alertmanager configuration to prevent excessive alerts:\ninhibit_rules:# Apply inhibition if the alert name is the same.- source_match:severity:criticaltarget_match:severity:warningequal:[\u0026#39;alertname\u0026#39;,\u0026#39;service\u0026#39;,\u0026#39;cluster\u0026#39;]# Stop all alerts for type=shoot if there are VPN problems.- source_match:service:vpntarget_match_re:type:shootequal:[\u0026#39;type\u0026#39;,\u0026#39;cluster\u0026#39;]# Stop warning and critical alerts if there is a blocker- source_match:severity:blockertarget_match_re:severity:^(critical|warning)$equal:[\u0026#39;cluster\u0026#39;]# If the API server is down inhibit no worker nodes alert. No worker nodes depends on kube-state-metrics which depends on the API server.- source_match:service:kube-apiservertarget_match_re:service:nodesequal:[\u0026#39;cluster\u0026#39;]# If API server is down inhibit kube-state-metrics alerts.- source_match:service:kube-apiservertarget_match_re:severity:infoequal:[\u0026#39;cluster\u0026#39;]# No Worker nodes depends on kube-state-metrics. Inhibit no worker nodes if kube-state-metrics is down.- source_match:service:kube-state-metrics-shoottarget_match_re:service:nodesequal:[\u0026#39;cluster\u0026#39;]Below is a graph visualizing the inhibition rules:\n"},{"uri":"https://gardener.cloud/documentation/guides/applications/insecure-configuration/","title":"Auditing Kubernetes for Secure Setup","tags":[],"description":"A few insecure configurations in Kubernetes","content":"Auditing Kubernetes for Secure Setup \u0026lt;object type=\u0026quot;image/svg+xml\u0026quot; data=\u0026quot;./images/teaser.svg\u0026quot; style=\u0026quot;;visibility:hidden; margin: 3rem auto;display: block;\u0026quot; class=\u0026quot;inline reveal-fast drop-shadow\u0026quot;\u0026gt;\u0026lt;/object\u0026gt;  Increasing the Security of all Gardener Stakeholders In summer 2018, the Gardener project team asked Kinvolk to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work is to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a Control-Plane-as-a-Service with a network air gap.\nAlong the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.\nMajor Findings From this experience, wed like to share a few examples of security issues that could happen on a Kubernetes installation and how to fix them.\nAlban Crequy (Kinvolk) and Dirk Marwinski (SAP SE) gave a presentation entitled Hardening Multi-Cloud Kubernetes Clusters as a Service at KubeCon 2018 in Shanghai presenting some of the findings.\nHere is a summary of the findings:\n  Privilege escalation due to insecure configuration of the Kubernetes API server\n Root cause: Same certificate authority (CA) is used for both the API server and the proxy that allows accessing the API server. Risk: Users can get access to the API server. Recommendation: Always use different CAs.    Exploration of the control plane network with malicious HTTP-redirects\n  Root cause: See detailed description below.\n  Risk: Provoked error message contains full HTTP payload from an existing endpoint which can be exploited. The contents of the payload depends on your setup, but can potentially be user data, configuration data, and credentials.\n  Recommendation:\n Use the latest version of Gardener Ensure the seed cluster\u0026rsquo;s container network supports network policies. Clusters that have been created with Kubify are not protected as Flannel is used there which doesn\u0026rsquo;t support network policies.      Reading private AWS metadata via Grafana\n Root cause: It is possible to configuring a new custom data source in Grafana, we could send HTTP requests to target the control Risk: Users can get the \u0026ldquo;user-data\u0026rdquo; for the seed cluster from the metadata service and retrieve a kubeconfig for that Kubernetes cluster Recommendation: Lockdown Grafana features to only what\u0026rsquo;s necessary in this setup, block all unnecessary outgoing traffic, move Grafana to a different network, lockdown unauthenticated endpoints    Scenario 1: Privilege Escalation with Insecure API Server In most configurations, different components connect directly to the Kubernetes API server, often using a kubeconfig with a client certificate. The API server is started with the flag:\n/hyperkube apiserver --client-ca-file=/srv/kubernetes/ca/ca.crt ... The API server will check whether the client certificate presented by kubectl, kubelet, scheduler or another component is really signed by the configured certificate authority for clients.\n The API server can have many clients of various kinds  However, it is possible to configure the API server differently for use with an intermediate authenticating proxy. The proxy will authenticate the client with its own custom method and then issue HTTP requests to the API server with additional HTTP headers specifying the user name and group name. The API server should only accept HTTP requests with HTTP headers from a legitimate proxy. To allow the API server to check incoming requests, you need pass on a list of certificate authorities (CAs) to it. Requests coming from a proxy are only accepted if they use a client certificate that is signed by one of the CAs of that list.\n--requestheader-client-ca-file=/srv/kubernetes/ca/ca-proxy.crt --requestheader-username-headers=X-Remote-User --requestheader-group-headers=X-Remote-Group  API server clients can reach the API server through an authenticating proxy  So far, so good. But what happens if malicious user Mallory tries to connect directly to the API server and reuses the HTTP headers to pretend to be someone else?\n What happens when a client bypasses the proxy, connecting directly to the API server?  With a correct configuration, Mallorys kubeconfig will have a certificate signed by the API server certificate authority but not signed by the proxy certificate authority. So the API server will not accept the extra HTTP header X-Remote-Group: system:masters.\nYou only run into an issue when the same certificate authority is used for both the API server and the proxy. Then, any Kubernetes client certificate can be used to take the role of different user or group as the API server will accept the user header and group header.\nThe kubectl tool does not normally add those HTTP headers but its pretty easy to generate the corresponding HTTP requests manually.\nWe worked on improving the Kubernetes documentation to make clearer that this configuration should be avoided.\nScenario 2: Exploration of the Control Plane Network with Malicious HTTP-Redirects The API server is a central component of Kubernetes and many components initiate connections to it, including the Kubelet running on worker nodes. Most of the requests from those clients will end up updating Kubernetes objects (pods, services, deployments, and so on) in the etcd database but the API server usually does not need to initiate TCP connections itself.\n The API server is mostly a component that receives requests  However, there are exceptions. Some kubectl commands will trigger the API server to open a new connection to the Kubelet. Kubectl exec is one of those commands. In order to get the standard I/Os from the pod, the API server will start an HTTP connection to the Kubelet on the worker node where the pod is running. Depending on the container runtime used, it can be done in different ways, but one way to do it is for the Kubelet to reply with a HTTP-302 redirection to the Container Runtime Interface (CRI). Basically, the Kubelet is telling the API server to get the streams from CRI itself directly instead of forwarding. The redirection from the Kubelet will only change the port and path from the URL; the IP address will not be changed because the Kubelet and the CRI component run on the same worker node.\n But the API server also initiates some connections, for example, to worker nodes  Its often quite easy for users of a Kubernetes cluster to get access to worker nodes and tamper with the Kubelet. They could be given explicit SSH access or they could be given a kubeconfig with enough privileges to create privileged pods or even just pods with host volumes.\nIn contrast, users  even those with system:masters permissions or root rights  are often not given access to the control plane. On setups like for example GKE or Gardener, the control plane is running on separate nodes, with a different administrative access. It could be hosted on a different cloud provider account. So users are not free to explore the internal network in the control plane.\nWhat would happen if a user was tampering with the Kubelet to make it maliciously redirect kubectl exec requests to a different random endpoint? Most likely the given endpoint would not speak the streaming server protocol, so there would be an error. However, the full HTTP payload from the endpoint is included in the error message printed by kubectl exec.\n The API server is tricked to connect to other components  The impact of this issue depends on the specific setup. But in many configurations, we could find a metadata service (such as the AWS metadata service) containing user data, configurations and credentials. The setup we explored had a different AWS account and a different EC2 instance profile for the worker nodes and the control plane. This issue allowed users to get access to the AWS metadata service in the context of the control plane, which they should not have access to.\nWe have reported this issue to the Kubernetes Security mailing list and the public pull request that addresses the issue has been merged PR#66516. It provides a way to enforce HTTP redirect validation (disabled by default).\nBut there are several other ways that users could trigger the API server to generate HTTP requests and get the reply payload back, so it is advised to isolate the API server and other components from the network as additional precautious measures. Depending on where the API server runs, it could be with Kubernetes Network Policies, EC2 Security Groups or just iptables directly. Following the defense in depth principle, it is a good idea to apply the API server HTTP redirect validation when it is available as well as firewall rules.\nIn Gardener, this has been fixed with Kubernetes network policies along with changes to ensure the API server does not need to contact the metadata service. You can see more details in the announcements on the Gardener mailing list. This is tracked in CVE-2018-2475.\nTo be protected from this issue, stakeholders should:\n Use the latest version of Gardener Ensure the seed clusters container network supports network policies. Clusters that have been created with Kubify are not protected as Flannel is used there which doesnt support network policies.  Scenario 3: Reading Private AWS Metadata via Grafana For our tests, we had access to a Kubernetes setup where users are not only given access to the API server in the control plane, but also to a Grafana instance that is used to gather data from their Kubernetes clusters via Prometheus. The control plane is managed and users dont have access to the nodes that it runs. They can only access the API server and Grafana via a load balancer. The internal network of the control plane is therefore hidden to users.\n Prometheus and Grafana can be used to monitor worker nodes  Unfortunately, that setup was not protecting the control plane network from nosy users. By configuring a new custom data source in Grafana, we could send HTTP requests to target the control plane network, for example the AWS metadata service. The reply payload is not displayed on the Grafana Web UI but it is possible to access it from the debugging console of the Chrome browser.\n Credentials can be retrieved from the debugging console of Chrome   Adding a Grafana data source is a way to issue HTTP requests to arbitrary targets  In that installation, users could get the user-data for the seed cluster from the metadata service and retrieve a kubeconfig for that Kubernetes cluster.\nThere are many possible measures to avoid this situation: lockdown Grafana features to only whats necessary in this setup, block all unnecessary outgoing traffic, move Grafana to a different network, lockdown unauthenticated endpoints.\nConclusion The three scenarios above show pitfalls with a Kubernetes setup. A lot of them were specific to the Kubernetes installation: different cloud providers or different configurations will show different weakness. Users should no longer be given access to Grafana.\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/applications/insecure-configuration/","title":"Auditing Kubernetes for Secure Setup","tags":[],"description":"A few insecure configurations in Kubernetes","content":"Auditing Kubernetes for Secure Setup \u0026lt;object type=\u0026quot;image/svg+xml\u0026quot; data=\u0026quot;./images/teaser.svg\u0026quot; style=\u0026quot;;visibility:hidden; margin: 3rem auto;display: block;\u0026quot; class=\u0026quot;inline reveal-fast drop-shadow\u0026quot;\u0026gt;\u0026lt;/object\u0026gt;  Increasing the Security of all Gardener Stakeholders In summer 2018, the Gardener project team asked Kinvolk to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work is to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a Control-Plane-as-a-Service with a network air gap.\nAlong the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.\nMajor Findings From this experience, wed like to share a few examples of security issues that could happen on a Kubernetes installation and how to fix them.\nAlban Crequy (Kinvolk) and Dirk Marwinski (SAP SE) gave a presentation entitled Hardening Multi-Cloud Kubernetes Clusters as a Service at KubeCon 2018 in Shanghai presenting some of the findings.\nHere is a summary of the findings:\n  Privilege escalation due to insecure configuration of the Kubernetes API server\n Root cause: Same certificate authority (CA) is used for both the API server and the proxy that allows accessing the API server. Risk: Users can get access to the API server. Recommendation: Always use different CAs.    Exploration of the control plane network with malicious HTTP-redirects\n  Root cause: See detailed description below.\n  Risk: Provoked error message contains full HTTP payload from an existing endpoint which can be exploited. The contents of the payload depends on your setup, but can potentially be user data, configuration data, and credentials.\n  Recommendation:\n Use the latest version of Gardener Ensure the seed cluster\u0026rsquo;s container network supports network policies. Clusters that have been created with Kubify are not protected as Flannel is used there which doesn\u0026rsquo;t support network policies.      Reading private AWS metadata via Grafana\n Root cause: It is possible to configuring a new custom data source in Grafana, we could send HTTP requests to target the control Risk: Users can get the \u0026ldquo;user-data\u0026rdquo; for the seed cluster from the metadata service and retrieve a kubeconfig for that Kubernetes cluster Recommendation: Lockdown Grafana features to only what\u0026rsquo;s necessary in this setup, block all unnecessary outgoing traffic, move Grafana to a different network, lockdown unauthenticated endpoints    Scenario 1: Privilege Escalation with Insecure API Server In most configurations, different components connect directly to the Kubernetes API server, often using a kubeconfig with a client certificate. The API server is started with the flag:\n/hyperkube apiserver --client-ca-file=/srv/kubernetes/ca/ca.crt ... The API server will check whether the client certificate presented by kubectl, kubelet, scheduler or another component is really signed by the configured certificate authority for clients.\n The API server can have many clients of various kinds  However, it is possible to configure the API server differently for use with an intermediate authenticating proxy. The proxy will authenticate the client with its own custom method and then issue HTTP requests to the API server with additional HTTP headers specifying the user name and group name. The API server should only accept HTTP requests with HTTP headers from a legitimate proxy. To allow the API server to check incoming requests, you need pass on a list of certificate authorities (CAs) to it. Requests coming from a proxy are only accepted if they use a client certificate that is signed by one of the CAs of that list.\n--requestheader-client-ca-file=/srv/kubernetes/ca/ca-proxy.crt --requestheader-username-headers=X-Remote-User --requestheader-group-headers=X-Remote-Group  API server clients can reach the API server through an authenticating proxy  So far, so good. But what happens if malicious user Mallory tries to connect directly to the API server and reuses the HTTP headers to pretend to be someone else?\n What happens when a client bypasses the proxy, connecting directly to the API server?  With a correct configuration, Mallorys kubeconfig will have a certificate signed by the API server certificate authority but not signed by the proxy certificate authority. So the API server will not accept the extra HTTP header X-Remote-Group: system:masters.\nYou only run into an issue when the same certificate authority is used for both the API server and the proxy. Then, any Kubernetes client certificate can be used to take the role of different user or group as the API server will accept the user header and group header.\nThe kubectl tool does not normally add those HTTP headers but its pretty easy to generate the corresponding HTTP requests manually.\nWe worked on improving the Kubernetes documentation to make clearer that this configuration should be avoided.\nScenario 2: Exploration of the Control Plane Network with Malicious HTTP-Redirects The API server is a central component of Kubernetes and many components initiate connections to it, including the Kubelet running on worker nodes. Most of the requests from those clients will end up updating Kubernetes objects (pods, services, deployments, and so on) in the etcd database but the API server usually does not need to initiate TCP connections itself.\n The API server is mostly a component that receives requests  However, there are exceptions. Some kubectl commands will trigger the API server to open a new connection to the Kubelet. Kubectl exec is one of those commands. In order to get the standard I/Os from the pod, the API server will start an HTTP connection to the Kubelet on the worker node where the pod is running. Depending on the container runtime used, it can be done in different ways, but one way to do it is for the Kubelet to reply with a HTTP-302 redirection to the Container Runtime Interface (CRI). Basically, the Kubelet is telling the API server to get the streams from CRI itself directly instead of forwarding. The redirection from the Kubelet will only change the port and path from the URL; the IP address will not be changed because the Kubelet and the CRI component run on the same worker node.\n But the API server also initiates some connections, for example, to worker nodes  Its often quite easy for users of a Kubernetes cluster to get access to worker nodes and tamper with the Kubelet. They could be given explicit SSH access or they could be given a kubeconfig with enough privileges to create privileged pods or even just pods with host volumes.\nIn contrast, users  even those with system:masters permissions or root rights  are often not given access to the control plane. On setups like for example GKE or Gardener, the control plane is running on separate nodes, with a different administrative access. It could be hosted on a different cloud provider account. So users are not free to explore the internal network in the control plane.\nWhat would happen if a user was tampering with the Kubelet to make it maliciously redirect kubectl exec requests to a different random endpoint? Most likely the given endpoint would not speak the streaming server protocol, so there would be an error. However, the full HTTP payload from the endpoint is included in the error message printed by kubectl exec.\n The API server is tricked to connect to other components  The impact of this issue depends on the specific setup. But in many configurations, we could find a metadata service (such as the AWS metadata service) containing user data, configurations and credentials. The setup we explored had a different AWS account and a different EC2 instance profile for the worker nodes and the control plane. This issue allowed users to get access to the AWS metadata service in the context of the control plane, which they should not have access to.\nWe have reported this issue to the Kubernetes Security mailing list and the public pull request that addresses the issue has been merged PR#66516. It provides a way to enforce HTTP redirect validation (disabled by default).\nBut there are several other ways that users could trigger the API server to generate HTTP requests and get the reply payload back, so it is advised to isolate the API server and other components from the network as additional precautious measures. Depending on where the API server runs, it could be with Kubernetes Network Policies, EC2 Security Groups or just iptables directly. Following the defense in depth principle, it is a good idea to apply the API server HTTP redirect validation when it is available as well as firewall rules.\nIn Gardener, this has been fixed with Kubernetes network policies along with changes to ensure the API server does not need to contact the metadata service. You can see more details in the announcements on the Gardener mailing list. This is tracked in CVE-2018-2475.\nTo be protected from this issue, stakeholders should:\n Use the latest version of Gardener Ensure the seed clusters container network supports network policies. Clusters that have been created with Kubify are not protected as Flannel is used there which doesnt support network policies.  Scenario 3: Reading Private AWS Metadata via Grafana For our tests, we had access to a Kubernetes setup where users are not only given access to the API server in the control plane, but also to a Grafana instance that is used to gather data from their Kubernetes clusters via Prometheus. The control plane is managed and users dont have access to the nodes that it runs. They can only access the API server and Grafana via a load balancer. The internal network of the control plane is therefore hidden to users.\n Prometheus and Grafana can be used to monitor worker nodes  Unfortunately, that setup was not protecting the control plane network from nosy users. By configuring a new custom data source in Grafana, we could send HTTP requests to target the control plane network, for example the AWS metadata service. The reply payload is not displayed on the Grafana Web UI but it is possible to access it from the debugging console of the Chrome browser.\n Credentials can be retrieved from the debugging console of Chrome   Adding a Grafana data source is a way to issue HTTP requests to arbitrary targets  In that installation, users could get the user-data for the seed cluster from the metadata service and retrieve a kubeconfig for that Kubernetes cluster.\nThere are many possible measures to avoid this situation: lockdown Grafana features to only whats necessary in this setup, block all unnecessary outgoing traffic, move Grafana to a different network, lockdown unauthenticated endpoints.\nConclusion The three scenarios above show pitfalls with a Kubernetes setup. A lot of them were specific to the Kubernetes installation: different cloud providers or different configurations will show different weakness. Users should no longer be given access to Grafana.\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/applications/insecure-configuration/","title":"Auditing Kubernetes for Secure Setup","tags":[],"description":"A few insecure configurations in Kubernetes","content":"Auditing Kubernetes for Secure Setup \u0026lt;object type=\u0026quot;image/svg+xml\u0026quot; data=\u0026quot;./images/teaser.svg\u0026quot; style=\u0026quot;;visibility:hidden; margin: 3rem auto;display: block;\u0026quot; class=\u0026quot;inline reveal-fast drop-shadow\u0026quot;\u0026gt;\u0026lt;/object\u0026gt;  Increasing the Security of all Gardener Stakeholders In summer 2018, the Gardener project team asked Kinvolk to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work is to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a Control-Plane-as-a-Service with a network air gap.\nAlong the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.\nMajor Findings From this experience, wed like to share a few examples of security issues that could happen on a Kubernetes installation and how to fix them.\nAlban Crequy (Kinvolk) and Dirk Marwinski (SAP SE) gave a presentation entitled Hardening Multi-Cloud Kubernetes Clusters as a Service at KubeCon 2018 in Shanghai presenting some of the findings.\nHere is a summary of the findings:\n  Privilege escalation due to insecure configuration of the Kubernetes API server\n Root cause: Same certificate authority (CA) is used for both the API server and the proxy that allows accessing the API server. Risk: Users can get access to the API server. Recommendation: Always use different CAs.    Exploration of the control plane network with malicious HTTP-redirects\n  Root cause: See detailed description below.\n  Risk: Provoked error message contains full HTTP payload from an existing endpoint which can be exploited. The contents of the payload depends on your setup, but can potentially be user data, configuration data, and credentials.\n  Recommendation:\n Use the latest version of Gardener Ensure the seed cluster\u0026rsquo;s container network supports network policies. Clusters that have been created with Kubify are not protected as Flannel is used there which doesn\u0026rsquo;t support network policies.      Reading private AWS metadata via Grafana\n Root cause: It is possible to configuring a new custom data source in Grafana, we could send HTTP requests to target the control Risk: Users can get the \u0026ldquo;user-data\u0026rdquo; for the seed cluster from the metadata service and retrieve a kubeconfig for that Kubernetes cluster Recommendation: Lockdown Grafana features to only what\u0026rsquo;s necessary in this setup, block all unnecessary outgoing traffic, move Grafana to a different network, lockdown unauthenticated endpoints    Scenario 1: Privilege Escalation with Insecure API Server In most configurations, different components connect directly to the Kubernetes API server, often using a kubeconfig with a client certificate. The API server is started with the flag:\n/hyperkube apiserver --client-ca-file=/srv/kubernetes/ca/ca.crt ... The API server will check whether the client certificate presented by kubectl, kubelet, scheduler or another component is really signed by the configured certificate authority for clients.\n The API server can have many clients of various kinds  However, it is possible to configure the API server differently for use with an intermediate authenticating proxy. The proxy will authenticate the client with its own custom method and then issue HTTP requests to the API server with additional HTTP headers specifying the user name and group name. The API server should only accept HTTP requests with HTTP headers from a legitimate proxy. To allow the API server to check incoming requests, you need pass on a list of certificate authorities (CAs) to it. Requests coming from a proxy are only accepted if they use a client certificate that is signed by one of the CAs of that list.\n--requestheader-client-ca-file=/srv/kubernetes/ca/ca-proxy.crt --requestheader-username-headers=X-Remote-User --requestheader-group-headers=X-Remote-Group  API server clients can reach the API server through an authenticating proxy  So far, so good. But what happens if malicious user Mallory tries to connect directly to the API server and reuses the HTTP headers to pretend to be someone else?\n What happens when a client bypasses the proxy, connecting directly to the API server?  With a correct configuration, Mallorys kubeconfig will have a certificate signed by the API server certificate authority but not signed by the proxy certificate authority. So the API server will not accept the extra HTTP header X-Remote-Group: system:masters.\nYou only run into an issue when the same certificate authority is used for both the API server and the proxy. Then, any Kubernetes client certificate can be used to take the role of different user or group as the API server will accept the user header and group header.\nThe kubectl tool does not normally add those HTTP headers but its pretty easy to generate the corresponding HTTP requests manually.\nWe worked on improving the Kubernetes documentation to make clearer that this configuration should be avoided.\nScenario 2: Exploration of the Control Plane Network with Malicious HTTP-Redirects The API server is a central component of Kubernetes and many components initiate connections to it, including the Kubelet running on worker nodes. Most of the requests from those clients will end up updating Kubernetes objects (pods, services, deployments, and so on) in the etcd database but the API server usually does not need to initiate TCP connections itself.\n The API server is mostly a component that receives requests  However, there are exceptions. Some kubectl commands will trigger the API server to open a new connection to the Kubelet. Kubectl exec is one of those commands. In order to get the standard I/Os from the pod, the API server will start an HTTP connection to the Kubelet on the worker node where the pod is running. Depending on the container runtime used, it can be done in different ways, but one way to do it is for the Kubelet to reply with a HTTP-302 redirection to the Container Runtime Interface (CRI). Basically, the Kubelet is telling the API server to get the streams from CRI itself directly instead of forwarding. The redirection from the Kubelet will only change the port and path from the URL; the IP address will not be changed because the Kubelet and the CRI component run on the same worker node.\n But the API server also initiates some connections, for example, to worker nodes  Its often quite easy for users of a Kubernetes cluster to get access to worker nodes and tamper with the Kubelet. They could be given explicit SSH access or they could be given a kubeconfig with enough privileges to create privileged pods or even just pods with host volumes.\nIn contrast, users  even those with system:masters permissions or root rights  are often not given access to the control plane. On setups like for example GKE or Gardener, the control plane is running on separate nodes, with a different administrative access. It could be hosted on a different cloud provider account. So users are not free to explore the internal network in the control plane.\nWhat would happen if a user was tampering with the Kubelet to make it maliciously redirect kubectl exec requests to a different random endpoint? Most likely the given endpoint would not speak the streaming server protocol, so there would be an error. However, the full HTTP payload from the endpoint is included in the error message printed by kubectl exec.\n The API server is tricked to connect to other components  The impact of this issue depends on the specific setup. But in many configurations, we could find a metadata service (such as the AWS metadata service) containing user data, configurations and credentials. The setup we explored had a different AWS account and a different EC2 instance profile for the worker nodes and the control plane. This issue allowed users to get access to the AWS metadata service in the context of the control plane, which they should not have access to.\nWe have reported this issue to the Kubernetes Security mailing list and the public pull request that addresses the issue has been merged PR#66516. It provides a way to enforce HTTP redirect validation (disabled by default).\nBut there are several other ways that users could trigger the API server to generate HTTP requests and get the reply payload back, so it is advised to isolate the API server and other components from the network as additional precautious measures. Depending on where the API server runs, it could be with Kubernetes Network Policies, EC2 Security Groups or just iptables directly. Following the defense in depth principle, it is a good idea to apply the API server HTTP redirect validation when it is available as well as firewall rules.\nIn Gardener, this has been fixed with Kubernetes network policies along with changes to ensure the API server does not need to contact the metadata service. You can see more details in the announcements on the Gardener mailing list. This is tracked in CVE-2018-2475.\nTo be protected from this issue, stakeholders should:\n Use the latest version of Gardener Ensure the seed clusters container network supports network policies. Clusters that have been created with Kubify are not protected as Flannel is used there which doesnt support network policies.  Scenario 3: Reading Private AWS Metadata via Grafana For our tests, we had access to a Kubernetes setup where users are not only given access to the API server in the control plane, but also to a Grafana instance that is used to gather data from their Kubernetes clusters via Prometheus. The control plane is managed and users dont have access to the nodes that it runs. They can only access the API server and Grafana via a load balancer. The internal network of the control plane is therefore hidden to users.\n Prometheus and Grafana can be used to monitor worker nodes  Unfortunately, that setup was not protecting the control plane network from nosy users. By configuring a new custom data source in Grafana, we could send HTTP requests to target the control plane network, for example the AWS metadata service. The reply payload is not displayed on the Grafana Web UI but it is possible to access it from the debugging console of the Chrome browser.\n Credentials can be retrieved from the debugging console of Chrome   Adding a Grafana data source is a way to issue HTTP requests to arbitrary targets  In that installation, users could get the user-data for the seed cluster from the metadata service and retrieve a kubeconfig for that Kubernetes cluster.\nThere are many possible measures to avoid this situation: lockdown Grafana features to only whats necessary in this setup, block all unnecessary outgoing traffic, move Grafana to a different network, lockdown unauthenticated endpoints.\nConclusion The three scenarios above show pitfalls with a Kubernetes setup. A lot of them were specific to the Kubernetes installation: different cloud providers or different configurations will show different weakness. Users should no longer be given access to Grafana.\n"},{"uri":"https://gardener.cloud/documentation/guides/client_tools/oidc-login/","title":"Authenticating with an Identity Provider","tags":[],"description":"Authenticating with an Identity Provider using OpenID Connect","content":"Authenticating In this blog you will learn how to:\n Configure an Identity Provider using OpenID Connect. Configure a local kubectl plugin to enable oidc-login . Configure the K8s API Server of Gardener managed Kubernetes cluster. Create an RBAC rule to authorize an authenticated user.  Motivation As a project owner of Gardener, I want my Kubernetes level user to be authenticated by an identity provider.\nPrerequisite Knowledge Please read the following background material on Authenticating\nInsights About Gardener The Gardener allows the administrator to modify every aspect of the control plane setup, e.g. all feature gateways and even configurations are programmatically accessible. In this way, every administrative user of the Gardener has full control of how the control plane should be parameterized. But with this power, the user can easily configure a control plane that is beyond any SLA that the Gardener team can arguably support. Therefore, use this power wisely! A configuration that enables experimental features for production becomes an operational responsibility of the cluster owner\u0026rsquo;s team.\nBut Gardener does not stop you from experimenting!\nThere are currently no default IdP parameters.\nConfigure an Identity Provider Create a tenant in an OpenID-Connect compatible Identity Provider. For sake of simplicity, we shall use Auth0, which has a free plan for experimentations.\nIn your tenant, setup a native client/application that will use the authentication: Configure the client to have a callback url of http://localhost:8000. This callback will connect to your local kubectl oidc-login plugin: Note down the following parameters:\n Domain or Issuer url. It must be an https-secured endpoint (In case of Auth0, notice the trailing / at the end). Client ID Client Secret  Verify that https://\u0026lt;Issuer\u0026gt;/.well-known/openid-configuration is reachable.\nNow create some users (or connect to a user store): Notice that the users must have a verified email address. In doubt, just override that setting manually.\nConfigure kubectl oidc-login Please install the kubectl plugin oidc-login. We highly recommend the krew install tool, which also makes other plugins easily available.\n$ kubectl krew install oidc-login Updated the local copy of plugin index. Installing plugin: oidc-login CAVEATS: \\ | You need to setup the OIDC provider, Kubernetes API server, role binding and kubeconfig. | See https://github.com/int128/kubelogin for more. / Installed plugin: oidc-login Prepare a kubeconfig for later use:\n$ cp ~/.kube/config ~/.kube/config-oidc Modify the configurations as follows:\napiVersion:v1kind:Config...contexts:- context:cluster:shoot--project--myclusteruser:my-oidcname:shoot--project--mycluster...users:- name:my-oidcuser:auth-provider:config:client-id:\u0026lt;ClientID\u0026gt; client-secret: \u0026lt;Client Secret\u0026gt;idp-issuer-url:\u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;extra-scopes:email,offline_access,profilename:oidcEnsure that the modified context is the active context current-context: shoot--project--mycluster.\nConfigure the Gardener Shoot Spec Modify the Gardener shoot/cluster manifest as follows:\napiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:name:myclusternamespace:garden-project...spec:kubernetes:kubeAPIServer:oidcConfig:clientID:\u0026lt;ClientID\u0026gt; issuerURL: \u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;usernameClaim:emailThis change of the shoot manifest triggers a reconciliation. Once the reconciliation is finished, your oidc configuration is applied. It does not invalidate other certificate based authentication methods. Wait for Gardener to reconcile the change. It can take upto 5min.\nAuthorize an authenticated user For simplicity, we will just authorize a single user with the all encompassing cluster role cluster-admin:\napiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:cluster-admin-testroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:cluster-adminsubjects:- apiGroup:rbac.authorization.k8s.iokind:Username:test@test.comAs administrator, activate/apply the above cluster role binding for test@test.com.\nVerify the Result Now activate the prepared kubeconfig-oidc and perform a login:\n$ export KUBECONFIG=~/.kube/config-oidc $ kubectl oidc-login Open http://localhost:8000 for authentication The plugin opens a browser for an interctive authentication session, and in parallel serves a local webserver for the configured callback.\nIf you successfully verified your user, then the console will display the validity of your returned token:\nYou got a valid token until 2019-08-14 06:26:49 +0200 CEST Inspect the kubeconfig-oidc. You will find two additional parameters:\n...users:- name:my-oidcuser:auth-provider:config:client-id:\u0026lt;ClientID\u0026gt; client-secret: \u0026lt;Client Secret\u0026gt;idp-issuer-url:\u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;extra-scopes:email,offline_access,profileid-token:eyJ0eX...4In0.QQKS...TTTwrefresh-token:LFt...0Skjname:oidcThe plugin persisted the id-token and refresh-token in your configuration file.\nVerify that your user actually has the cluster-admin role:\n$ kubectl get po --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system blackbox-exporter-954dd954b-tk9vl 1/1 Running 0 7d5h kube-system calico-kube-controllers-5f4b46ffb5-ggb7z 1/1 Running 0 7d5h ... $ kubectl who-can create clusterrolebinding No subjects found with permissions to create clusterrolebinding assigned through RoleBindings CLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE cluster-admin system:masters Group cluster-admin-test test@test.com User ... Congratulations, you have just configured your cluster to authenticate against an Identity Provider using OpenID Connect!\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/client_tools/oidc-login/","title":"Authenticating with an Identity Provider","tags":[],"description":"Authenticating with an Identity Provider using OpenID Connect","content":"Authenticating In this blog you will learn how to:\n Configure an Identity Provider using OpenID Connect. Configure a local kubectl plugin to enable oidc-login . Configure the K8s API Server of Gardener managed Kubernetes cluster. Create an RBAC rule to authorize an authenticated user.  Motivation As a project owner of Gardener, I want my Kubernetes level user to be authenticated by an identity provider.\nPrerequisite Knowledge Please read the following background material on Authenticating\nInsights About Gardener The Gardener allows the administrator to modify every aspect of the control plane setup, e.g. all feature gateways and even configurations are programmatically accessible. In this way, every administrative user of the Gardener has full control of how the control plane should be parameterized. But with this power, the user can easily configure a control plane that is beyond any SLA that the Gardener team can arguably support. Therefore, use this power wisely! A configuration that enables experimental features for production becomes an operational responsibility of the cluster owner\u0026rsquo;s team.\nBut Gardener does not stop you from experimenting!\nThere are currently no default IdP parameters.\nConfigure an Identity Provider Create a tenant in an OpenID-Connect compatible Identity Provider. For sake of simplicity, we shall use Auth0, which has a free plan for experimentations.\nIn your tenant, setup a native client/application that will use the authentication: Configure the client to have a callback url of http://localhost:8000. This callback will connect to your local kubectl oidc-login plugin: Note down the following parameters:\n Domain or Issuer url. It must be an https-secured endpoint (In case of Auth0, notice the trailing / at the end). Client ID Client Secret  Verify that https://\u0026lt;Issuer\u0026gt;/.well-known/openid-configuration is reachable.\nNow create some users (or connect to a user store): Notice that the users must have a verified email address. In doubt, just override that setting manually.\nConfigure kubectl oidc-login Please install the kubectl plugin oidc-login. We highly recommend the krew install tool, which also makes other plugins easily available.\n$ kubectl krew install oidc-login Updated the local copy of plugin index. Installing plugin: oidc-login CAVEATS: \\ | You need to setup the OIDC provider, Kubernetes API server, role binding and kubeconfig. | See https://github.com/int128/kubelogin for more. / Installed plugin: oidc-login Prepare a kubeconfig for later use:\n$ cp ~/.kube/config ~/.kube/config-oidc Modify the configurations as follows:\napiVersion:v1kind:Config...contexts:- context:cluster:shoot--project--myclusteruser:my-oidcname:shoot--project--mycluster...users:- name:my-oidcuser:auth-provider:config:client-id:\u0026lt;ClientID\u0026gt; client-secret: \u0026lt;Client Secret\u0026gt;idp-issuer-url:\u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;extra-scopes:email,offline_access,profilename:oidcEnsure that the modified context is the active context current-context: shoot--project--mycluster.\nConfigure the Gardener Shoot Spec Modify the Gardener shoot/cluster manifest as follows:\napiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:name:myclusternamespace:garden-project...spec:kubernetes:kubeAPIServer:oidcConfig:clientID:\u0026lt;ClientID\u0026gt; issuerURL: \u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;usernameClaim:emailThis change of the shoot manifest triggers a reconciliation. Once the reconciliation is finished, your oidc configuration is applied. It does not invalidate other certificate based authentication methods. Wait for Gardener to reconcile the change. It can take upto 5min.\nAuthorize an authenticated user For simplicity, we will just authorize a single user with the all encompassing cluster role cluster-admin:\napiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:cluster-admin-testroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:cluster-adminsubjects:- apiGroup:rbac.authorization.k8s.iokind:Username:test@test.comAs administrator, activate/apply the above cluster role binding for test@test.com.\nVerify the Result Now activate the prepared kubeconfig-oidc and perform a login:\n$ export KUBECONFIG=~/.kube/config-oidc $ kubectl oidc-login Open http://localhost:8000 for authentication The plugin opens a browser for an interctive authentication session, and in parallel serves a local webserver for the configured callback.\nIf you successfully verified your user, then the console will display the validity of your returned token:\nYou got a valid token until 2019-08-14 06:26:49 +0200 CEST Inspect the kubeconfig-oidc. You will find two additional parameters:\n...users:- name:my-oidcuser:auth-provider:config:client-id:\u0026lt;ClientID\u0026gt; client-secret: \u0026lt;Client Secret\u0026gt;idp-issuer-url:\u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;extra-scopes:email,offline_access,profileid-token:eyJ0eX...4In0.QQKS...TTTwrefresh-token:LFt...0Skjname:oidcThe plugin persisted the id-token and refresh-token in your configuration file.\nVerify that your user actually has the cluster-admin role:\n$ kubectl get po --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system blackbox-exporter-954dd954b-tk9vl 1/1 Running 0 7d5h kube-system calico-kube-controllers-5f4b46ffb5-ggb7z 1/1 Running 0 7d5h ... $ kubectl who-can create clusterrolebinding No subjects found with permissions to create clusterrolebinding assigned through RoleBindings CLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE cluster-admin system:masters Group cluster-admin-test test@test.com User ... Congratulations, you have just configured your cluster to authenticate against an Identity Provider using OpenID Connect!\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/client_tools/oidc-login/","title":"Authenticating with an Identity Provider","tags":[],"description":"Authenticating with an Identity Provider using OpenID Connect","content":"Authenticating In this blog you will learn how to:\n Configure an Identity Provider using OpenID Connect. Configure a local kubectl plugin to enable oidc-login . Configure the K8s API Server of Gardener managed Kubernetes cluster. Create an RBAC rule to authorize an authenticated user.  Motivation As a project owner of Gardener, I want my Kubernetes level user to be authenticated by an identity provider.\nPrerequisite Knowledge Please read the following background material on Authenticating\nInsights About Gardener The Gardener allows the administrator to modify every aspect of the control plane setup, e.g. all feature gateways and even configurations are programmatically accessible. In this way, every administrative user of the Gardener has full control of how the control plane should be parameterized. But with this power, the user can easily configure a control plane that is beyond any SLA that the Gardener team can arguably support. Therefore, use this power wisely! A configuration that enables experimental features for production becomes an operational responsibility of the cluster owner\u0026rsquo;s team.\nBut Gardener does not stop you from experimenting!\nThere are currently no default IdP parameters.\nConfigure an Identity Provider Create a tenant in an OpenID-Connect compatible Identity Provider. For sake of simplicity, we shall use Auth0, which has a free plan for experimentations.\nIn your tenant, setup a native client/application that will use the authentication: Configure the client to have a callback url of http://localhost:8000. This callback will connect to your local kubectl oidc-login plugin: Note down the following parameters:\n Domain or Issuer url. It must be an https-secured endpoint (In case of Auth0, notice the trailing / at the end). Client ID Client Secret  Verify that https://\u0026lt;Issuer\u0026gt;/.well-known/openid-configuration is reachable.\nNow create some users (or connect to a user store): Notice that the users must have a verified email address. In doubt, just override that setting manually.\nConfigure kubectl oidc-login Please install the kubectl plugin oidc-login. We highly recommend the krew install tool, which also makes other plugins easily available.\n$ kubectl krew install oidc-login Updated the local copy of plugin index. Installing plugin: oidc-login CAVEATS: \\ | You need to setup the OIDC provider, Kubernetes API server, role binding and kubeconfig. | See https://github.com/int128/kubelogin for more. / Installed plugin: oidc-login Prepare a kubeconfig for later use:\n$ cp ~/.kube/config ~/.kube/config-oidc Modify the configurations as follows:\napiVersion:v1kind:Config...contexts:- context:cluster:shoot--project--myclusteruser:my-oidcname:shoot--project--mycluster...users:- name:my-oidcuser:auth-provider:config:client-id:\u0026lt;ClientID\u0026gt; client-secret: \u0026lt;Client Secret\u0026gt;idp-issuer-url:\u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;extra-scopes:email,offline_access,profilename:oidcEnsure that the modified context is the active context current-context: shoot--project--mycluster.\nConfigure the Gardener Shoot Spec Modify the Gardener shoot/cluster manifest as follows:\napiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:name:myclusternamespace:garden-project...spec:kubernetes:kubeAPIServer:oidcConfig:clientID:\u0026lt;ClientID\u0026gt; issuerURL: \u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;usernameClaim:emailThis change of the shoot manifest triggers a reconciliation. Once the reconciliation is finished, your oidc configuration is applied. It does not invalidate other certificate based authentication methods. Wait for Gardener to reconcile the change. It can take upto 5min.\nAuthorize an authenticated user For simplicity, we will just authorize a single user with the all encompassing cluster role cluster-admin:\napiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:cluster-admin-testroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:cluster-adminsubjects:- apiGroup:rbac.authorization.k8s.iokind:Username:test@test.comAs administrator, activate/apply the above cluster role binding for test@test.com.\nVerify the Result Now activate the prepared kubeconfig-oidc and perform a login:\n$ export KUBECONFIG=~/.kube/config-oidc $ kubectl oidc-login Open http://localhost:8000 for authentication The plugin opens a browser for an interctive authentication session, and in parallel serves a local webserver for the configured callback.\nIf you successfully verified your user, then the console will display the validity of your returned token:\nYou got a valid token until 2019-08-14 06:26:49 +0200 CEST Inspect the kubeconfig-oidc. You will find two additional parameters:\n...users:- name:my-oidcuser:auth-provider:config:client-id:\u0026lt;ClientID\u0026gt; client-secret: \u0026lt;Client Secret\u0026gt;idp-issuer-url:\u0026#34;https://\u0026lt;Issuer\u0026gt;/\u0026#34;extra-scopes:email,offline_access,profileid-token:eyJ0eX...4In0.QQKS...TTTwrefresh-token:LFt...0Skjname:oidcThe plugin persisted the id-token and refresh-token in your configuration file.\nVerify that your user actually has the cluster-admin role:\n$ kubectl get po --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system blackbox-exporter-954dd954b-tk9vl 1/1 Running 0 7d5h kube-system calico-kube-controllers-5f4b46ffb5-ggb7z 1/1 Running 0 7d5h ... $ kubectl who-can create clusterrolebinding No subjects found with permissions to create clusterrolebinding assigned through RoleBindings CLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE cluster-admin system:masters Group cluster-admin-test test@test.com User ... Congratulations, you have just configured your cluster to authenticate against an Identity Provider using OpenID Connect!\n"},{"uri":"https://gardener.cloud/documentation/guides/client_tools/kubectl-apiserver/","title":"Automated deployment","tags":[],"description":"Automated deployment with kubectl","content":"Introduction With kubectl you can easily deploy an image from your local environment.\nHowever, what if you want to use a automated deployment script on a CI server (e.g. Jenkins), but don\u0026rsquo;t want to store the KUBECONFIG on that server?\nYou can use kubectl and connect to the API-server of your cluster.\nPrerequisites   Create a service account user\nkubectl create serviceaccount deploy-user -n default   Bind a role to the newly created serviceuser\n !!! Warning !!! In this example the preconfigured role edit and the namespace default is being used, please adjust the role to a more strict scope! see https://kubernetes.io/docs/admin/authorization/rbac/\n kubectl create rolebinding deploy-default-role --clusterrole=edit --serviceaccount=default:deploy-user --namespace=default   Get the URL of your API-server\nAPISERVER=$(kubectl config view | grep server | cut -f 2- -d \u0026#34;:\u0026#34; | tr -d \u0026#34; \u0026#34;)   Get the service account\nSERVICEACCOUNT=$(kubectl get serviceaccount deploy-user -n default -o=jsonpath={.secrets[0].name})   Generate a token for the serviceaccount\nTOKEN=$(kubectl get secret -n default $SERVICEACCOUNT -o=jsonpath={.data.token} | base64 -D)   Usage You can deploy your app without setting the kubeconfig locally, you just need to pass the environment variables (e.g. store them in the Jenkins credentials store)\nkubectl --server=${APIServer} --token=${TOKEN} --insecure-skip-tls-verify=true apply --filename myapp.yaml "},{"uri":"https://gardener.cloud/v1.12.8/guides/client_tools/kubectl-apiserver/","title":"Automated deployment","tags":[],"description":"Automated deployment with kubectl","content":"Introduction With kubectl you can easily deploy an image from your local environment.\nHowever, what if you want to use a automated deployment script on a CI server (e.g. Jenkins), but don\u0026rsquo;t want to store the KUBECONFIG on that server?\nYou can use kubectl and connect to the API-server of your cluster.\nPrerequisites   Create a service account user\nkubectl create serviceaccount deploy-user -n default   Bind a role to the newly created serviceuser\n !!! Warning !!! In this example the preconfigured role edit and the namespace default is being used, please adjust the role to a more strict scope! see https://kubernetes.io/docs/admin/authorization/rbac/\n kubectl create rolebinding deploy-default-role --clusterrole=edit --serviceaccount=default:deploy-user --namespace=default   Get the URL of your API-server\nAPISERVER=$(kubectl config view | grep server | cut -f 2- -d \u0026#34;:\u0026#34; | tr -d \u0026#34; \u0026#34;)   Get the service account\nSERVICEACCOUNT=$(kubectl get serviceaccount deploy-user -n default -o=jsonpath={.secrets[0].name})   Generate a token for the serviceaccount\nTOKEN=$(kubectl get secret -n default $SERVICEACCOUNT -o=jsonpath={.data.token} | base64 -D)   Usage You can deploy your app without setting the kubeconfig locally, you just need to pass the environment variables (e.g. store them in the Jenkins credentials store)\nkubectl --server=${APIServer} --token=${TOKEN} --insecure-skip-tls-verify=true apply --filename myapp.yaml "},{"uri":"https://gardener.cloud/v1.13.2/guides/client_tools/kubectl-apiserver/","title":"Automated deployment","tags":[],"description":"Automated deployment with kubectl","content":"Introduction With kubectl you can easily deploy an image from your local environment.\nHowever, what if you want to use a automated deployment script on a CI server (e.g. Jenkins), but don\u0026rsquo;t want to store the KUBECONFIG on that server?\nYou can use kubectl and connect to the API-server of your cluster.\nPrerequisites   Create a service account user\nkubectl create serviceaccount deploy-user -n default   Bind a role to the newly created serviceuser\n !!! Warning !!! In this example the preconfigured role edit and the namespace default is being used, please adjust the role to a more strict scope! see https://kubernetes.io/docs/admin/authorization/rbac/\n kubectl create rolebinding deploy-default-role --clusterrole=edit --serviceaccount=default:deploy-user --namespace=default   Get the URL of your API-server\nAPISERVER=$(kubectl config view | grep server | cut -f 2- -d \u0026#34;:\u0026#34; | tr -d \u0026#34; \u0026#34;)   Get the service account\nSERVICEACCOUNT=$(kubectl get serviceaccount deploy-user -n default -o=jsonpath={.secrets[0].name})   Generate a token for the serviceaccount\nTOKEN=$(kubectl get secret -n default $SERVICEACCOUNT -o=jsonpath={.data.token} | base64 -D)   Usage You can deploy your app without setting the kubeconfig locally, you just need to pass the environment variables (e.g. store them in the Jenkins credentials store)\nkubectl --server=${APIServer} --token=${TOKEN} --insecure-skip-tls-verify=true apply --filename myapp.yaml "},{"uri":"https://gardener.cloud/documentation/concepts/deployment/deploy_gardenlet_automatically/","title":"Automatic Deployment of Gardenlets","tags":[],"description":"","content":"Automatic Deployment of Gardenlets The gardenlet can automatically deploy itself into shoot clusters, and register this cluster as a seed cluster. These clusters are called shooted seeds. This procedure is the preferred way to add additional seed clusters, because shoot clusters already come with production-grade qualities that are also demanded for seed clusters.\nPrerequisites The only prerequisite is to register an initial cluster as a seed cluster that has already a gardenlet deployed:\n This gardenlet was either deployed as part of a Gardener installation using an installation tool (for example, gardener/garden-setup) or the gardenlet was deployed manually (more information: Deploy a Gardenlet Manually)   The initial cluster can be the garden cluster itself.\n Self-Deployment of Gardenlets in Additional Shooted Seed Clusters For a better scalability, you usually need more seed clusters that you can create as follows:\n Use the initial cluster as the seed cluster for other shooted seed clusters. It hosts the control planes of the other seed clusters. The gardenlet deployed in the initial cluster deploys itself automatically into the shooted seed clusters.  The advantage of this approach is that theres only one initial gardenlet installation required. Every other shooted seed cluster has a gardenlet deployed automatically.\nRelated Links Create Shooted Seed Cluster\ngarden-setup\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/deployment/deploy_gardenlet_automatically/","title":"Automatic Deployment of Gardenlets","tags":[],"description":"","content":"Automatic Deployment of Gardenlets The gardenlet can automatically deploy itself into shoot clusters, and register this cluster as a seed cluster. These clusters are called shooted seeds. This procedure is the preferred way to add additional seed clusters, because shoot clusters already come with production-grade qualities that are also demanded for seed clusters.\nPrerequisites The only prerequisite is to register an initial cluster as a seed cluster that has already a gardenlet deployed:\n This gardenlet was either deployed as part of a Gardener installation using an installation tool (for example, gardener/garden-setup) or the gardenlet was deployed manually (more information: Deploy a Gardenlet Manually)   The initial cluster can be the garden cluster itself.\n Self-Deployment of Gardenlets in Additional Shooted Seed Clusters For a better scalability, you usually need more seed clusters that you can create as follows:\n Use the initial cluster as the seed cluster for other shooted seed clusters. It hosts the control planes of the other seed clusters. The gardenlet deployed in the initial cluster deploys itself automatically into the shooted seed clusters.  The advantage of this approach is that theres only one initial gardenlet installation required. Every other shooted seed cluster has a gardenlet deployed automatically.\nRelated Links Create Shooted Seed Cluster\ngarden-setup\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/deployment/deploy_gardenlet_automatically/","title":"Automatic Deployment of Gardenlets","tags":[],"description":"","content":"Automatic Deployment of Gardenlets The gardenlet can automatically deploy itself into shoot clusters, and register this cluster as a seed cluster. These clusters are called shooted seeds. This procedure is the preferred way to add additional seed clusters, because shoot clusters already come with production-grade qualities that are also demanded for seed clusters.\nPrerequisites The only prerequisite is to register an initial cluster as a seed cluster that has already a gardenlet deployed:\n This gardenlet was either deployed as part of a Gardener installation using an installation tool (for example, gardener/garden-setup) or the gardenlet was deployed manually (more information: Deploy a Gardenlet Manually)   The initial cluster can be the garden cluster itself.\n Self-Deployment of Gardenlets in Additional Shooted Seed Clusters For a better scalability, you usually need more seed clusters that you can create as follows:\n Use the initial cluster as the seed cluster for other shooted seed clusters. It hosts the control planes of the other seed clusters. The gardenlet deployed in the initial cluster deploys itself automatically into the shooted seed clusters.  The advantage of this approach is that theres only one initial gardenlet installation required. Every other shooted seed cluster has a gardenlet deployed automatically.\nRelated Links Create Shooted Seed Cluster\ngarden-setup\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/backupbucket/","title":"BackupBucket resource","tags":[],"description":"","content":"Contract: BackupBucket resource The Gardener project features a sub-project called etcd-backup-restore to take periodic backups of etcd backing Shoot clusters. It demands the bucket (or its equivalent in different object store providers) to be created and configured externally with appropriate credentials. The BackupBucket resource takes this responsibility in Gardener.\nBefore introducing the BackupBucket extension resource Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see here). Now, Gardener commissions an external, provider-specific controller to take over this task. You can also refer to backupInfra proposal documentation to get idea about how the transition was done and understand the resource in broader scope.\nWhat is the scope of bucket? A bucket will be provisioned per Seed. So, backup of every Shoot created on that Seed will be stored under different shoot specific prefix under the bucket. For the backup of the Shoot rescheduled on different Seed it will continue to use the same bucket.\nWhat is the lifespan of BackupBucket? The bucket associated with BackupBucket will be created at creation of Seed. And as per current implementation, it will be deleted on deletion of Seed and there isn\u0026rsquo;t any BackupEntry resource associated with it.\nIn the future, we plan to introduce schedule for BackupBucket the deletion logic for BackupBucket resource, which will reschedule the it on different available Seed, on deletion or failure of health check for current associated seed. In that case, BackupBucket will be deleted only if there isn\u0026rsquo;t any schedulable Seed available and there isn\u0026rsquo;t any associated BackupEntry resource.\nWhat needs to be implemented to support a new infrastructure provider? As part of the seed flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:BackupBucketmetadata:name:foospec:type:azureproviderConfig:\u0026lt;some-optional-provider-specific-backupbucket-configuration\u0026gt; region: eu-west-1secretRef:name:backupprovidernamespace:shoot--foo--barThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. This provider secret will be configured by Gardener operator in the Seed resource and propagated over there by seed controller.\nAfter your controller has created the required bucket, if required it generates the secret to access the objects in buckets and put reference to it in status. This secret is supposed to be used by Gardener or eventually BackupEntry resource and etcd-backup-restore component to backup the etcd.\nIn order to support a new infrastructure provider you need to write a controller that watches all BackupBuckets with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the Azure provider.\nReferences and additional resources  BackupBucket API Reference Exemplary implementation for the Azure provider BackupEntry resource documentation Shared bucket proposal  "},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/backupbucket/","title":"BackupBucket resource","tags":[],"description":"","content":"Contract: BackupBucket resource The Gardener project features a sub-project called etcd-backup-restore to take periodic backups of etcd backing Shoot clusters. It demands the bucket (or its equivalent in different object store providers) to be created and configured externally with appropriate credentials. The BackupBucket resource takes this responsibility in Gardener.\nBefore introducing the BackupBucket extension resource Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see here). Now, Gardener commissions an external, provider-specific controller to take over this task. You can also refer to backupInfra proposal documentation to get idea about how the transition was done and understand the resource in broader scope.\nWhat is the scope of bucket? A bucket will be provisioned per Seed. So, backup of every Shoot created on that Seed will be stored under different shoot specific prefix under the bucket. For the backup of the Shoot rescheduled on different Seed it will continue to use the same bucket.\nWhat is the lifespan of BackupBucket? The bucket associated with BackupBucket will be created at creation of Seed. And as per current implementation, it will be deleted on deletion of Seed and there isn\u0026rsquo;t any BackupEntry resource associated with it.\nIn the future, we plan to introduce schedule for BackupBucket the deletion logic for BackupBucket resource, which will reschedule the it on different available Seed, on deletion or failure of health check for current associated seed. In that case, BackupBucket will be deleted only if there isn\u0026rsquo;t any schedulable Seed available and there isn\u0026rsquo;t any associated BackupEntry resource.\nWhat needs to be implemented to support a new infrastructure provider? As part of the seed flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:BackupBucketmetadata:name:foospec:type:azureproviderConfig:\u0026lt;some-optional-provider-specific-backupbucket-configuration\u0026gt; region: eu-west-1secretRef:name:backupprovidernamespace:shoot--foo--barThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. This provider secret will be configured by Gardener operator in the Seed resource and propagated over there by seed controller.\nAfter your controller has created the required bucket, if required it generates the secret to access the objects in buckets and put reference to it in status. This secret is supposed to be used by Gardener or eventually BackupEntry resource and etcd-backup-restore component to backup the etcd.\nIn order to support a new infrastructure provider you need to write a controller that watches all BackupBuckets with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the Azure provider.\nReferences and additional resources  BackupBucket API Reference Exemplary implementation for the Azure provider BackupEntry resource documentation Shared bucket proposal  "},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/backupbucket/","title":"BackupBucket resource","tags":[],"description":"","content":"Contract: BackupBucket resource The Gardener project features a sub-project called etcd-backup-restore to take periodic backups of etcd backing Shoot clusters. It demands the bucket (or its equivalent in different object store providers) to be created and configured externally with appropriate credentials. The BackupBucket resource takes this responsibility in Gardener.\nBefore introducing the BackupBucket extension resource Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see here). Now, Gardener commissions an external, provider-specific controller to take over this task. You can also refer to backupInfra proposal documentation to get idea about how the transition was done and understand the resource in broader scope.\nWhat is the scope of bucket? A bucket will be provisioned per Seed. So, backup of every Shoot created on that Seed will be stored under different shoot specific prefix under the bucket. For the backup of the Shoot rescheduled on different Seed it will continue to use the same bucket.\nWhat is the lifespan of BackupBucket? The bucket associated with BackupBucket will be created at creation of Seed. And as per current implementation, it will be deleted on deletion of Seed and there isn\u0026rsquo;t any BackupEntry resource associated with it.\nIn the future, we plan to introduce schedule for BackupBucket the deletion logic for BackupBucket resource, which will reschedule the it on different available Seed, on deletion or failure of health check for current associated seed. In that case, BackupBucket will be deleted only if there isn\u0026rsquo;t any schedulable Seed available and there isn\u0026rsquo;t any associated BackupEntry resource.\nWhat needs to be implemented to support a new infrastructure provider? As part of the seed flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:BackupBucketmetadata:name:foospec:type:azureproviderConfig:\u0026lt;some-optional-provider-specific-backupbucket-configuration\u0026gt; region: eu-west-1secretRef:name:backupprovidernamespace:shoot--foo--barThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. This provider secret will be configured by Gardener operator in the Seed resource and propagated over there by seed controller.\nAfter your controller has created the required bucket, if required it generates the secret to access the objects in buckets and put reference to it in status. This secret is supposed to be used by Gardener or eventually BackupEntry resource and etcd-backup-restore component to backup the etcd.\nIn order to support a new infrastructure provider you need to write a controller that watches all BackupBuckets with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the Azure provider.\nReferences and additional resources  BackupBucket API Reference Exemplary implementation for the Azure provider BackupEntry resource documentation Shared bucket proposal  "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/backupentry/","title":"BackupEntry resource","tags":[],"description":"","content":"Contract: BackupEntry resource The Gardener project features a sub-project called etcd-backup-restore to take periodic backups of etcd backing Shoot clusters. It demands the bucket (or its equivalent in different object store providers) access credentials to be created and configured externally with appropriate credentials. The BackupEntry resource takes this responsibility in Gardener to provide this information by creating a secret specific to the component. Said that, the core motivation for introducing this resource was to support retention of backups post deletion of Shoot. The etcd-backup-restore components takes responsibility of garbage collecting old backups out of the defined period. Once a shoot is deleted, we need to persist the backups for few days. Hence, Gardener uses the BackupEntry resource for this housekeeping work post deletion of a Shoot. The BackupEntry resource is responsible for shoot specific prefix under referred bucket.\nBefore introducing the BackupEntry extension resource Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see here). Now, Gardener commissions an external, provider-specific controller to take over this task. You can also refer to backupInfra proposal documentation to get idea about how the transition was done and understand the resource in broader scope.\nWhat is the lifespan of BackupEntry? The bucket associated with BackupEntry will be created at using BackupBucket resource. The BackupEntry resource will be created as a part of a Shoot creation. But resource continue to exists post deletion of a Shoot. The deletionGracePeriod of a BackupEntry resource i.e. time after the shoot deletion is configurable globally for Gardener via gardener-controller-manager component config. You can find the configuration option in reference.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:BackupEntrymetadata:name:shoot--foo--barspec:type:azureproviderConfig:\u0026lt;some-optional-provider-specific-backup-bucket-configuration\u0026gt; backupBucketProviderStatus:\u0026lt;some-optional-provider-specific-backup-bucket-status\u0026gt; region: eu-west-1bucketName:foosecretRef:name:backupprovidernamespace:shoot--foo--barThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. This provider secret will be propagated from BackupBucket resource by Shoot controller.\nYour controller is supposed to create the etcd-backup secret in control-plane namespace of a shoot. This secret is supposed to be used by Gardener or eventually the etcd-backup-restore component to backup the etcd. The controller implementation should cleanup the objects created under shoot specific prefix in bucket equivalent to name of BackupEntry resource.\nIn order to support a new infrastructure provider you need to write a controller that watches all BackupBuckets with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the Azure provider.\nReferences and additional resources  BackupEntry API Reference Exemplary implementation for the Azure provider BackupBucket resource documentation Shared bucket proposal Gardener-controller-manager-component-config API specification  "},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/backupentry/","title":"BackupEntry resource","tags":[],"description":"","content":"Contract: BackupEntry resource The Gardener project features a sub-project called etcd-backup-restore to take periodic backups of etcd backing Shoot clusters. It demands the bucket (or its equivalent in different object store providers) access credentials to be created and configured externally with appropriate credentials. The BackupEntry resource takes this responsibility in Gardener to provide this information by creating a secret specific to the component. Said that, the core motivation for introducing this resource was to support retention of backups post deletion of Shoot. The etcd-backup-restore components takes responsibility of garbage collecting old backups out of the defined period. Once a shoot is deleted, we need to persist the backups for few days. Hence, Gardener uses the BackupEntry resource for this housekeeping work post deletion of a Shoot. The BackupEntry resource is responsible for shoot specific prefix under referred bucket.\nBefore introducing the BackupEntry extension resource Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see here). Now, Gardener commissions an external, provider-specific controller to take over this task. You can also refer to backupInfra proposal documentation to get idea about how the transition was done and understand the resource in broader scope.\nWhat is the lifespan of BackupEntry? The bucket associated with BackupEntry will be created at using BackupBucket resource. The BackupEntry resource will be created as a part of a Shoot creation. But resource continue to exists post deletion of a Shoot. The deletionGracePeriod of a BackupEntry resource i.e. time after the shoot deletion is configurable globally for Gardener via gardener-controller-manager component config. You can find the configuration option in reference.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:BackupEntrymetadata:name:shoot--foo--barspec:type:azureproviderConfig:\u0026lt;some-optional-provider-specific-backup-bucket-configuration\u0026gt; backupBucketProviderStatus:\u0026lt;some-optional-provider-specific-backup-bucket-status\u0026gt; region: eu-west-1bucketName:foosecretRef:name:backupprovidernamespace:shoot--foo--barThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. This provider secret will be propagated from BackupBucket resource by Shoot controller.\nYour controller is supposed to create the etcd-backup secret in control-plane namespace of a shoot. This secret is supposed to be used by Gardener or eventually the etcd-backup-restore component to backup the etcd. The controller implementation should cleanup the objects created under shoot specific prefix in bucket equivalent to name of BackupEntry resource.\nIn order to support a new infrastructure provider you need to write a controller that watches all BackupBuckets with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the Azure provider.\nReferences and additional resources  BackupEntry API Reference Exemplary implementation for the Azure provider BackupBucket resource documentation Shared bucket proposal Gardener-controller-manager-component-config API specification  "},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/backupentry/","title":"BackupEntry resource","tags":[],"description":"","content":"Contract: BackupEntry resource The Gardener project features a sub-project called etcd-backup-restore to take periodic backups of etcd backing Shoot clusters. It demands the bucket (or its equivalent in different object store providers) access credentials to be created and configured externally with appropriate credentials. The BackupEntry resource takes this responsibility in Gardener to provide this information by creating a secret specific to the component. Said that, the core motivation for introducing this resource was to support retention of backups post deletion of Shoot. The etcd-backup-restore components takes responsibility of garbage collecting old backups out of the defined period. Once a shoot is deleted, we need to persist the backups for few days. Hence, Gardener uses the BackupEntry resource for this housekeeping work post deletion of a Shoot. The BackupEntry resource is responsible for shoot specific prefix under referred bucket.\nBefore introducing the BackupEntry extension resource Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see here). Now, Gardener commissions an external, provider-specific controller to take over this task. You can also refer to backupInfra proposal documentation to get idea about how the transition was done and understand the resource in broader scope.\nWhat is the lifespan of BackupEntry? The bucket associated with BackupEntry will be created at using BackupBucket resource. The BackupEntry resource will be created as a part of a Shoot creation. But resource continue to exists post deletion of a Shoot. The deletionGracePeriod of a BackupEntry resource i.e. time after the shoot deletion is configurable globally for Gardener via gardener-controller-manager component config. You can find the configuration option in reference.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:BackupEntrymetadata:name:shoot--foo--barspec:type:azureproviderConfig:\u0026lt;some-optional-provider-specific-backup-bucket-configuration\u0026gt; backupBucketProviderStatus:\u0026lt;some-optional-provider-specific-backup-bucket-status\u0026gt; region: eu-west-1bucketName:foosecretRef:name:backupprovidernamespace:shoot--foo--barThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. This provider secret will be propagated from BackupBucket resource by Shoot controller.\nYour controller is supposed to create the etcd-backup secret in control-plane namespace of a shoot. This secret is supposed to be used by Gardener or eventually the etcd-backup-restore component to backup the etcd. The controller implementation should cleanup the objects created under shoot specific prefix in bucket equivalent to name of BackupEntry resource.\nIn order to support a new infrastructure provider you need to write a controller that watches all BackupBuckets with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the Azure provider.\nReferences and additional resources  BackupEntry API Reference Exemplary implementation for the Azure provider BackupBucket resource documentation Shared bucket proposal Gardener-controller-manager-component-config API specification  "},{"uri":"https://gardener.cloud/components/bouquet/","title":"Bouquet","tags":[],"description":"","content":"Bouquet Bouquet is a draft addon manager for the Gardener. It incorporates some of the requested features of the community but not yet all of them.\n Caution: This software is early alpha. It is not meant for production use and shall (currently) only serve as a possible outlook of what is possible with pre-deployed software on Gardener Kubernetes clusters.\n Installation If you want to deploy Bouquet on a target Gardener cluster, run the following:\nhelm install charts/bouquet \\  --name gardener-bouquet \\  --namespace garden This will deploy Bouquet with the required permissions into your garden cluster.\nStructure As of now, Bouquet comes with two new custom resources: AddonManifest and AddonInstance.\nAn AddonManifest can be considered equivalent to a Helm template. The manifest itself only contains metadata (like the name, default values etc.). The actual content of a manifest is specified via its source attribute. Currently, the only available source is a ConfigMap.\nAn AddonInstance references an AddonManifest and a target Shoot. It may also contain value overrides in its spec. As soon as an AddonInstance is created, Bouquet will apply the values to the templates and then ensure that the objects exist in the target shoot. If an AddonInstance is deleted, Bouquet will also make sure that the created objects are deleted as well.\nExample use case Say you want your cluster to contain istio right from the start. How can you do that?\nFirst you need to get the .yaml files necessary to deploy istio into your cluster. Download an istio release as follows:\nwget -O istio.yaml https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/istio/noauth/istio.yaml This will fetch a .yaml file containing all necessary kubernetes objects of istio. To make this data available in your garden cluster, create a configmap in your cluster via\nkubectl -n garden create configmap istio-files --from-file ./istio.yaml Now you need to create an AddonManifest that references this file and push it to Kubernetes. The file could look like the following:\napiVersion:\u0026#34;garden.sapcloud.io/v1alpha1\u0026#34;kind:\u0026#34;AddonManifest\u0026#34;metadata:name:\u0026#34;istio-0.0.1\u0026#34;spec:configMap:\u0026#34;istio-files\u0026#34;You can submit this manifest to Kubernetes via kubectl (given that you saved the file to addonmanifest.yaml:\nkubectl -n garden apply -f addonmanifest.yaml Once this is done, the only thing left to do is to create an AddonInstance referencing both your target Shoot and your AddonManifest. This AddonInstance has to be in the same namespace as your target Shoot:\napiVersion:\u0026#34;garden.sapcloud.io/v1alpha1\u0026#34;kind:\u0026#34;AddonInstance\u0026#34;metadata:name:\u0026#34;example\u0026#34;finalizers:- \u0026#34;bouquet\u0026#34;spec:manifest:namespace:\u0026#34;garden\u0026#34;name:\u0026#34;istio\u0026#34;version:\u0026#34;0.0.1\u0026#34;target:shoot:\u0026#34;addon-test\u0026#34;And apply it via kubectl (given that you saved the file to addoninstance.yaml):\nkubectl -n garden-addon-test apply -f addoninstance.yaml Bouquet will then start deploying your objects to the target Shoot once it is ready.\nOutlook / Future Since this is just a tech-preview, features like value / chart updates, more efficient templating, company addon guidelines etc. are not yet implemented / yet to come / yet to be discussed. It is also not yet clear whether this should eventually move into the Gardener or remain as a stand-alone component.\nCore points that have to be tackled are:\n Fire and forget mode (only deploy objects once, don\u0026rsquo;t monitor afterwards) Reconciliation (currently, updating behavior is not correctly implemented) Updates of an addon (-\u0026gt; Update strategies) Dependent addons / dependency resolution / dependency lifecycle  As such, contributions and help on shaping this topic is highly appreciated.\n"},{"uri":"https://gardener.cloud/components/bouquet/","title":"Bouquet","tags":[],"description":"","content":"Bouquet Bouquet is a draft addon manager for the Gardener. It incorporates some of the requested features of the community but not yet all of them.\n Caution: This software is early alpha. It is not meant for production use and shall (currently) only serve as a possible outlook of what is possible with pre-deployed software on Gardener Kubernetes clusters.\n Installation If you want to deploy Bouquet on a target Gardener cluster, run the following:\nhelm install charts/bouquet \\  --name gardener-bouquet \\  --namespace garden This will deploy Bouquet with the required permissions into your garden cluster.\nStructure As of now, Bouquet comes with two new custom resources: AddonManifest and AddonInstance.\nAn AddonManifest can be considered equivalent to a Helm template. The manifest itself only contains metadata (like the name, default values etc.). The actual content of a manifest is specified via its source attribute. Currently, the only available source is a ConfigMap.\nAn AddonInstance references an AddonManifest and a target Shoot. It may also contain value overrides in its spec. As soon as an AddonInstance is created, Bouquet will apply the values to the templates and then ensure that the objects exist in the target shoot. If an AddonInstance is deleted, Bouquet will also make sure that the created objects are deleted as well.\nExample use case Say you want your cluster to contain istio right from the start. How can you do that?\nFirst you need to get the .yaml files necessary to deploy istio into your cluster. Download an istio release as follows:\nwget -O istio.yaml https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/istio/noauth/istio.yaml This will fetch a .yaml file containing all necessary kubernetes objects of istio. To make this data available in your garden cluster, create a configmap in your cluster via\nkubectl -n garden create configmap istio-files --from-file ./istio.yaml Now you need to create an AddonManifest that references this file and push it to Kubernetes. The file could look like the following:\napiVersion:\u0026#34;garden.sapcloud.io/v1alpha1\u0026#34;kind:\u0026#34;AddonManifest\u0026#34;metadata:name:\u0026#34;istio-0.0.1\u0026#34;spec:configMap:\u0026#34;istio-files\u0026#34;You can submit this manifest to Kubernetes via kubectl (given that you saved the file to addonmanifest.yaml:\nkubectl -n garden apply -f addonmanifest.yaml Once this is done, the only thing left to do is to create an AddonInstance referencing both your target Shoot and your AddonManifest. This AddonInstance has to be in the same namespace as your target Shoot:\napiVersion:\u0026#34;garden.sapcloud.io/v1alpha1\u0026#34;kind:\u0026#34;AddonInstance\u0026#34;metadata:name:\u0026#34;example\u0026#34;finalizers:- \u0026#34;bouquet\u0026#34;spec:manifest:namespace:\u0026#34;garden\u0026#34;name:\u0026#34;istio\u0026#34;version:\u0026#34;0.0.1\u0026#34;target:shoot:\u0026#34;addon-test\u0026#34;And apply it via kubectl (given that you saved the file to addoninstance.yaml):\nkubectl -n garden-addon-test apply -f addoninstance.yaml Bouquet will then start deploying your objects to the target Shoot once it is ready.\nOutlook / Future Since this is just a tech-preview, features like value / chart updates, more efficient templating, company addon guidelines etc. are not yet implemented / yet to come / yet to be discussed. It is also not yet clear whether this should eventually move into the Gardener or remain as a stand-alone component.\nCore points that have to be tackled are:\n Fire and forget mode (only deploy objects once, don\u0026rsquo;t monitor afterwards) Reconciliation (currently, updating behavior is not correctly implemented) Updates of an addon (-\u0026gt; Update strategies) Dependent addons / dependency resolution / dependency lifecycle  As such, contributions and help on shaping this topic is highly appreciated.\n"},{"uri":"https://gardener.cloud/components/bouquet/","title":"Bouquet","tags":[],"description":"","content":"Bouquet Bouquet is a draft addon manager for the Gardener. It incorporates some of the requested features of the community but not yet all of them.\n Caution: This software is early alpha. It is not meant for production use and shall (currently) only serve as a possible outlook of what is possible with pre-deployed software on Gardener Kubernetes clusters.\n Installation If you want to deploy Bouquet on a target Gardener cluster, run the following:\nhelm install charts/bouquet \\  --name gardener-bouquet \\  --namespace garden This will deploy Bouquet with the required permissions into your garden cluster.\nStructure As of now, Bouquet comes with two new custom resources: AddonManifest and AddonInstance.\nAn AddonManifest can be considered equivalent to a Helm template. The manifest itself only contains metadata (like the name, default values etc.). The actual content of a manifest is specified via its source attribute. Currently, the only available source is a ConfigMap.\nAn AddonInstance references an AddonManifest and a target Shoot. It may also contain value overrides in its spec. As soon as an AddonInstance is created, Bouquet will apply the values to the templates and then ensure that the objects exist in the target shoot. If an AddonInstance is deleted, Bouquet will also make sure that the created objects are deleted as well.\nExample use case Say you want your cluster to contain istio right from the start. How can you do that?\nFirst you need to get the .yaml files necessary to deploy istio into your cluster. Download an istio release as follows:\nwget -O istio.yaml https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/istio/noauth/istio.yaml This will fetch a .yaml file containing all necessary kubernetes objects of istio. To make this data available in your garden cluster, create a configmap in your cluster via\nkubectl -n garden create configmap istio-files --from-file ./istio.yaml Now you need to create an AddonManifest that references this file and push it to Kubernetes. The file could look like the following:\napiVersion:\u0026#34;garden.sapcloud.io/v1alpha1\u0026#34;kind:\u0026#34;AddonManifest\u0026#34;metadata:name:\u0026#34;istio-0.0.1\u0026#34;spec:configMap:\u0026#34;istio-files\u0026#34;You can submit this manifest to Kubernetes via kubectl (given that you saved the file to addonmanifest.yaml:\nkubectl -n garden apply -f addonmanifest.yaml Once this is done, the only thing left to do is to create an AddonInstance referencing both your target Shoot and your AddonManifest. This AddonInstance has to be in the same namespace as your target Shoot:\napiVersion:\u0026#34;garden.sapcloud.io/v1alpha1\u0026#34;kind:\u0026#34;AddonInstance\u0026#34;metadata:name:\u0026#34;example\u0026#34;finalizers:- \u0026#34;bouquet\u0026#34;spec:manifest:namespace:\u0026#34;garden\u0026#34;name:\u0026#34;istio\u0026#34;version:\u0026#34;0.0.1\u0026#34;target:shoot:\u0026#34;addon-test\u0026#34;And apply it via kubectl (given that you saved the file to addoninstance.yaml):\nkubectl -n garden-addon-test apply -f addoninstance.yaml Bouquet will then start deploying your objects to the target Shoot once it is ready.\nOutlook / Future Since this is just a tech-preview, features like value / chart updates, more efficient templating, company addon guidelines etc. are not yet implemented / yet to come / yet to be discussed. It is also not yet clear whether this should eventually move into the Gardener or remain as a stand-alone component.\nCore points that have to be tackled are:\n Fire and forget mode (only deploy objects once, don\u0026rsquo;t monitor afterwards) Reconciliation (currently, updating behavior is not correctly implemented) Updates of an addon (-\u0026gt; Update strategies) Dependent addons / dependency resolution / dependency lifecycle  As such, contributions and help on shaping this topic is highly appreciated.\n"},{"uri":"https://gardener.cloud/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/contribute/10_code/14_cicd/","title":"CI/CD","tags":[],"description":"","content":"CI/CD As an execution environment for CI/CD workloads, we use Concourse. We however abstract from the underlying \u0026ldquo;build executor\u0026rdquo; and instead offer a Pipeline Definition Contract, through which components declare their build pipelines as required.\nIn order to run continuous delivery workloads for all components contributing to the Gardener project, we operate a central service.\nTypical workloads encompass the execution of tests and builds of a variety of technologies, as well as building and publishing container images, typically containing build results.\nWe are building our CI/CD offering around some principles:\n container-native - each workload is executed within a container environment. Components may customise used container images automation - pipelines are generated without manual interaction self-service - components customise their pipelines by changing their sources standardisation  Learn more on our: Build Pipeline Reference Manual\n"},{"uri":"https://gardener.cloud/v1.12.8/contribute/10_code/14_cicd/","title":"CI/CD","tags":[],"description":"","content":"CI/CD As an execution environment for CI/CD workloads, we use Concourse. We however abstract from the underlying \u0026ldquo;build executor\u0026rdquo; and instead offer a Pipeline Definition Contract, through which components declare their build pipelines as required.\nIn order to run continuous delivery workloads for all components contributing to the Gardener project, we operate a central service.\nTypical workloads encompass the execution of tests and builds of a variety of technologies, as well as building and publishing container images, typically containing build results.\nWe are building our CI/CD offering around some principles:\n container-native - each workload is executed within a container environment. Components may customise used container images automation - pipelines are generated without manual interaction self-service - components customise their pipelines by changing their sources standardisation  Learn more on our: Build Pipeline Reference Manual\n"},{"uri":"https://gardener.cloud/v1.13.2/contribute/10_code/14_cicd/","title":"CI/CD","tags":[],"description":"","content":"CI/CD As an execution environment for CI/CD workloads, we use Concourse. We however abstract from the underlying \u0026ldquo;build executor\u0026rdquo; and instead offer a Pipeline Definition Contract, through which components declare their build pipelines as required.\nIn order to run continuous delivery workloads for all components contributing to the Gardener project, we operate a central service.\nTypical workloads encompass the execution of tests and builds of a variety of technologies, as well as building and publishing container images, typically containing build results.\nWe are building our CI/CD offering around some principles:\n container-native - each workload is executed within a container environment. Components may customise used container images automation - pipelines are generated without manual interaction self-service - components customise their pipelines by changing their sources standardisation  Learn more on our: Build Pipeline Reference Manual\n"},{"uri":"https://gardener.cloud/documentation/tutorials/node-overprovisioning/","title":"Cluster Overprovisioning","tags":[],"description":"How to overprovision cluster nodes for quick scaling and failover","content":"Cluster Overprovisioning This tutorial describes how to overprovisioning of cluster nodes for scaling and failover. This is desired when you have work loads that need to scale up quickly without waiting for the new cluster nodes to be created and join the cluster.\nA similar problem occurs when crashing a node from the Hyperscaler. This must be replaced by Kubernetes as fast as possible. The solution can be overprovisioning of nodes\n Overprovisioning: Allocating more computer resources than is strictly necessary\nhttps://en.wikipedia.org/wiki/Overprovisioning\n When does the autoscaler change the size of the cluster? Below is a description of how the cluster behaves when there is a requirement to scale.\nScaling without overprovisioning  load hits the cluster (or a node is crashed) cannot schedule application-pods due to insufficient resources, scaling fails  cluster-autoscaler notices and begins to provision new instance wait for instance to be provisioned, boot, join the cluster and become ready kube-scheduler will notice there is somewhere to put the application-pods and will schedule them  Scaling with Overprovisioning  load hits the cluster (or a node is crashed) placeholder-pods are evicted, scaling of application-pod is immediately successful placeholder-pods cannot be scheduled due to insufficient resources wait for instance to be provisioned, boot, join the cluster and become ready kube-scheduler will notice there is somewhere to put the placeholder pods and will schedule them  You can apply the above scenario one-to-one to the case when a node of the Hyperscaler dies.\nReal Scenario Test We executed normal and overprovisioning tests on a gardener cluster on different infrastructure provider (aws, azure, gcp, alicloud). All of them tested the downtime of the application pod running in the cluster, when a node dies.\nThe test results for the different IaaS provider are shown below.\nResults The results provided should only show how long the downtimes can be approximately.\n The downtime results could vary +- 1 min, because the minimum request interval in UpTime is 1 minute\n Amazon Normal Overprovisioning Azure Normal Overprovisioning GCP Normal Overprovisioning AliCloud Normal Overprovisioning Summary of results Normal    Provider AWS Azure GCP AliCloud     Node deleted 08:56 09:32 09:39 09:53   Pod rescheduled 09:17 09:50 09:53 10:14   Downtime 21 min 18 min 14 min 21 min    Overprovisioning    Provider AWS Azure GCP AliCloud     Node deleted 14:20 06:00 06:05 08:23   Pod rescheduled 14:22 06:02 06:06 08:25   Downtime 2 min 2 min 1 min 2 min    Test Description We deployed a nginx web server and a service of type LoadBalancer to expose it. So we are able to call our endpoint with external tools like UpTime to check the availability of our nginx. It takes only a few seconds to deploy a nginx web server on kubernetes, so we could say: when your endpoint works, your node is up and running.\nWe wanted to test how much time it takes, when your node gets killed and your cluster has to create a new one to run your application on it.\nkubectl get nodes # select the node where your nginx is running on kubectl delete node \u0026lt;NGINX-HOSTED-NODE\u0026gt; The downtime is tested with UpTime, which does every minute a request to our endpoint. Further we checked manually, if the node startup time and the timestamps on UpTime are almost similar.\nNext, deploy the overprovisioned version of our demo application and kill the node with the NGINX. As you can see - the pod comes up very fast and can serve content again.\n"},{"uri":"https://gardener.cloud/v1.12.8/tutorials/node-overprovisioning/","title":"Cluster Overprovisioning","tags":[],"description":"How to overprovision cluster nodes for quick scaling and failover","content":"Cluster Overprovisioning This tutorial describes how to overprovisioning of cluster nodes for scaling and failover. This is desired when you have work loads that need to scale up quickly without waiting for the new cluster nodes to be created and join the cluster.\nA similar problem occurs when crashing a node from the Hyperscaler. This must be replaced by Kubernetes as fast as possible. The solution can be overprovisioning of nodes\n Overprovisioning: Allocating more computer resources than is strictly necessary\nhttps://en.wikipedia.org/wiki/Overprovisioning\n When does the autoscaler change the size of the cluster? Below is a description of how the cluster behaves when there is a requirement to scale.\nScaling without overprovisioning  load hits the cluster (or a node is crashed) cannot schedule application-pods due to insufficient resources, scaling fails  cluster-autoscaler notices and begins to provision new instance wait for instance to be provisioned, boot, join the cluster and become ready kube-scheduler will notice there is somewhere to put the application-pods and will schedule them  Scaling with Overprovisioning  load hits the cluster (or a node is crashed) placeholder-pods are evicted, scaling of application-pod is immediately successful placeholder-pods cannot be scheduled due to insufficient resources wait for instance to be provisioned, boot, join the cluster and become ready kube-scheduler will notice there is somewhere to put the placeholder pods and will schedule them  You can apply the above scenario one-to-one to the case when a node of the Hyperscaler dies.\nReal Scenario Test We executed normal and overprovisioning tests on a gardener cluster on different infrastructure provider (aws, azure, gcp, alicloud). All of them tested the downtime of the application pod running in the cluster, when a node dies.\nThe test results for the different IaaS provider are shown below.\nResults The results provided should only show how long the downtimes can be approximately.\n The downtime results could vary +- 1 min, because the minimum request interval in UpTime is 1 minute\n Amazon Normal Overprovisioning Azure Normal Overprovisioning GCP Normal Overprovisioning AliCloud Normal Overprovisioning Summary of results Normal    Provider AWS Azure GCP AliCloud     Node deleted 08:56 09:32 09:39 09:53   Pod rescheduled 09:17 09:50 09:53 10:14   Downtime 21 min 18 min 14 min 21 min    Overprovisioning    Provider AWS Azure GCP AliCloud     Node deleted 14:20 06:00 06:05 08:23   Pod rescheduled 14:22 06:02 06:06 08:25   Downtime 2 min 2 min 1 min 2 min    Test Description We deployed a nginx web server and a service of type LoadBalancer to expose it. So we are able to call our endpoint with external tools like UpTime to check the availability of our nginx. It takes only a few seconds to deploy a nginx web server on kubernetes, so we could say: when your endpoint works, your node is up and running.\nWe wanted to test how much time it takes, when your node gets killed and your cluster has to create a new one to run your application on it.\nkubectl get nodes # select the node where your nginx is running on kubectl delete node \u0026lt;NGINX-HOSTED-NODE\u0026gt; The downtime is tested with UpTime, which does every minute a request to our endpoint. Further we checked manually, if the node startup time and the timestamps on UpTime are almost similar.\nNext, deploy the overprovisioned version of our demo application and kill the node with the NGINX. As you can see - the pod comes up very fast and can serve content again.\n"},{"uri":"https://gardener.cloud/v1.13.2/tutorials/node-overprovisioning/","title":"Cluster Overprovisioning","tags":[],"description":"How to overprovision cluster nodes for quick scaling and failover","content":"Cluster Overprovisioning This tutorial describes how to overprovisioning of cluster nodes for scaling and failover. This is desired when you have work loads that need to scale up quickly without waiting for the new cluster nodes to be created and join the cluster.\nA similar problem occurs when crashing a node from the Hyperscaler. This must be replaced by Kubernetes as fast as possible. The solution can be overprovisioning of nodes\n Overprovisioning: Allocating more computer resources than is strictly necessary\nhttps://en.wikipedia.org/wiki/Overprovisioning\n When does the autoscaler change the size of the cluster? Below is a description of how the cluster behaves when there is a requirement to scale.\nScaling without overprovisioning  load hits the cluster (or a node is crashed) cannot schedule application-pods due to insufficient resources, scaling fails  cluster-autoscaler notices and begins to provision new instance wait for instance to be provisioned, boot, join the cluster and become ready kube-scheduler will notice there is somewhere to put the application-pods and will schedule them  Scaling with Overprovisioning  load hits the cluster (or a node is crashed) placeholder-pods are evicted, scaling of application-pod is immediately successful placeholder-pods cannot be scheduled due to insufficient resources wait for instance to be provisioned, boot, join the cluster and become ready kube-scheduler will notice there is somewhere to put the placeholder pods and will schedule them  You can apply the above scenario one-to-one to the case when a node of the Hyperscaler dies.\nReal Scenario Test We executed normal and overprovisioning tests on a gardener cluster on different infrastructure provider (aws, azure, gcp, alicloud). All of them tested the downtime of the application pod running in the cluster, when a node dies.\nThe test results for the different IaaS provider are shown below.\nResults The results provided should only show how long the downtimes can be approximately.\n The downtime results could vary +- 1 min, because the minimum request interval in UpTime is 1 minute\n Amazon Normal Overprovisioning Azure Normal Overprovisioning GCP Normal Overprovisioning AliCloud Normal Overprovisioning Summary of results Normal    Provider AWS Azure GCP AliCloud     Node deleted 08:56 09:32 09:39 09:53   Pod rescheduled 09:17 09:50 09:53 10:14   Downtime 21 min 18 min 14 min 21 min    Overprovisioning    Provider AWS Azure GCP AliCloud     Node deleted 14:20 06:00 06:05 08:23   Pod rescheduled 14:22 06:02 06:06 08:25   Downtime 2 min 2 min 1 min 2 min    Test Description We deployed a nginx web server and a service of type LoadBalancer to expose it. So we are able to call our endpoint with external tools like UpTime to check the availability of our nginx. It takes only a few seconds to deploy a nginx web server on kubernetes, so we could say: when your endpoint works, your node is up and running.\nWe wanted to test how much time it takes, when your node gets killed and your cluster has to create a new one to run your application on it.\nkubectl get nodes # select the node where your nginx is running on kubectl delete node \u0026lt;NGINX-HOSTED-NODE\u0026gt; The downtime is tested with UpTime, which does every minute a request to our endpoint. Further we checked manually, if the node startup time and the timestamps on UpTime are almost similar.\nNext, deploy the overprovisioned version of our demo application and kill the node with the NGINX. As you can see - the pod comes up very fast and can serve content again.\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/cluster/","title":"Cluster resource","tags":[],"description":"","content":"Cluster resource As part of the extensibility epic a lot of responsibility that was previously taken over by Gardener directly has now been shifted to extension controllers running in the seed clusters. These extensions often serve a well-defined purpose, e.g. the management of DNS records, infrastructure, etc. We have introduced a couple of extension CRDs in the seeds whose specification is written by Gardener, and which are acted up by the extensions.\nHowever, the extensions sometimes require more information that is not directly part of the specification. One example of that is the GCP infrastructure controller which needs to know the shoot\u0026rsquo;s pod and service network. Another example is the Azure infrastructure controller which requires some information out of the CloudProfile resource. The problem is that Gardener does not know which extension requires which information so that it can write it into their specific CRDs.\nIn order to deal with this problem we have introduced the Cluster extension resource. This CRD is written into the seeds, however, it does not contain a status, so it is not expected that something acts upon it. Instead, you can treat it like a ConfigMap which contains data that might be interesting for you. In the context of Gardener, seeds and shoots, and extensibility the Cluster resource contains the CloudProfile, Seed, and Shoot manifest. Extension controllers can take whatever information they want out of it that might help completing their individual tasks.\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Clustermetadata:name:shoot--foo--barspec:cloudProfile:apiVersion:core.gardener.cloud/v1beta1kind:CloudProfile...seed:apiVersion:core.gardener.cloud/v1beta1kind:Seed...shoot:apiVersion:core.gardener.cloud/v1beta1kind:Shoot...The resource is written by Gardener before it starts the reconciliation flow of the shoot.\n:warning: All Gardener components use the core.gardener.cloud/v1beta1 version, i.e., the Cluster resource will contain the objects in this version.\nImportant information that should be taken into account There are some fields in the Shoot specification that might be interesting to take into account.\n .spec.hibernation.enabled={true,false}: Extension controllers might want to behave differently if the shoot is hibernated or not (probably they might want to scale down their control plane components, for example). .status.lastOperation.state=Failed: If Gardener sets the shoot\u0026rsquo;s last operation state to Failed it means that Gardener won\u0026rsquo;t automatically retry to finish the reconciliation/deletion flow because an error occurred that could not be resolved within the last 24h (default). In this case end-users are expected to manually re-trigger the reconciliation flow in case they want Gardener to try again. Extension controllers are expected to follow the same principle. This means they have to read the shoot state out of the Cluster resource.  References and additional resources  Cluster API (Golang specification)  "},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/cluster/","title":"Cluster resource","tags":[],"description":"","content":"Cluster resource As part of the extensibility epic a lot of responsibility that was previously taken over by Gardener directly has now been shifted to extension controllers running in the seed clusters. These extensions often serve a well-defined purpose, e.g. the management of DNS records, infrastructure, etc. We have introduced a couple of extension CRDs in the seeds whose specification is written by Gardener, and which are acted up by the extensions.\nHowever, the extensions sometimes require more information that is not directly part of the specification. One example of that is the GCP infrastructure controller which needs to know the shoot\u0026rsquo;s pod and service network. Another example is the Azure infrastructure controller which requires some information out of the CloudProfile resource. The problem is that Gardener does not know which extension requires which information so that it can write it into their specific CRDs.\nIn order to deal with this problem we have introduced the Cluster extension resource. This CRD is written into the seeds, however, it does not contain a status, so it is not expected that something acts upon it. Instead, you can treat it like a ConfigMap which contains data that might be interesting for you. In the context of Gardener, seeds and shoots, and extensibility the Cluster resource contains the CloudProfile, Seed, and Shoot manifest. Extension controllers can take whatever information they want out of it that might help completing their individual tasks.\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Clustermetadata:name:shoot--foo--barspec:cloudProfile:apiVersion:core.gardener.cloud/v1beta1kind:CloudProfile...seed:apiVersion:core.gardener.cloud/v1beta1kind:Seed...shoot:apiVersion:core.gardener.cloud/v1beta1kind:Shoot...The resource is written by Gardener before it starts the reconciliation flow of the shoot.\n:warning: All Gardener components use the core.gardener.cloud/v1beta1 version, i.e., the Cluster resource will contain the objects in this version.\nImportant information that should be taken into account There are some fields in the Shoot specification that might be interesting to take into account.\n .spec.hibernation.enabled={true,false}: Extension controllers might want to behave differently if the shoot is hibernated or not (probably they might want to scale down their control plane components, for example). .status.lastOperation.state=Failed: If Gardener sets the shoot\u0026rsquo;s last operation state to Failed it means that Gardener won\u0026rsquo;t automatically retry to finish the reconciliation/deletion flow because an error occurred that could not be resolved within the last 24h (default). In this case end-users are expected to manually re-trigger the reconciliation flow in case they want Gardener to try again. Extension controllers are expected to follow the same principle. This means they have to read the shoot state out of the Cluster resource.  References and additional resources  Cluster API (Golang specification)  "},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/cluster/","title":"Cluster resource","tags":[],"description":"","content":"Cluster resource As part of the extensibility epic a lot of responsibility that was previously taken over by Gardener directly has now been shifted to extension controllers running in the seed clusters. These extensions often serve a well-defined purpose, e.g. the management of DNS records, infrastructure, etc. We have introduced a couple of extension CRDs in the seeds whose specification is written by Gardener, and which are acted up by the extensions.\nHowever, the extensions sometimes require more information that is not directly part of the specification. One example of that is the GCP infrastructure controller which needs to know the shoot\u0026rsquo;s pod and service network. Another example is the Azure infrastructure controller which requires some information out of the CloudProfile resource. The problem is that Gardener does not know which extension requires which information so that it can write it into their specific CRDs.\nIn order to deal with this problem we have introduced the Cluster extension resource. This CRD is written into the seeds, however, it does not contain a status, so it is not expected that something acts upon it. Instead, you can treat it like a ConfigMap which contains data that might be interesting for you. In the context of Gardener, seeds and shoots, and extensibility the Cluster resource contains the CloudProfile, Seed, and Shoot manifest. Extension controllers can take whatever information they want out of it that might help completing their individual tasks.\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Clustermetadata:name:shoot--foo--barspec:cloudProfile:apiVersion:core.gardener.cloud/v1beta1kind:CloudProfile...seed:apiVersion:core.gardener.cloud/v1beta1kind:Seed...shoot:apiVersion:core.gardener.cloud/v1beta1kind:Shoot...The resource is written by Gardener before it starts the reconciliation flow of the shoot.\n:warning: All Gardener components use the core.gardener.cloud/v1beta1 version, i.e., the Cluster resource will contain the objects in this version.\nImportant information that should be taken into account There are some fields in the Shoot specification that might be interesting to take into account.\n .spec.hibernation.enabled={true,false}: Extension controllers might want to behave differently if the shoot is hibernated or not (probably they might want to scale down their control plane components, for example). .status.lastOperation.state=Failed: If Gardener sets the shoot\u0026rsquo;s last operation state to Failed it means that Gardener won\u0026rsquo;t automatically retry to finish the reconciliation/deletion flow because an error occurred that could not be resolved within the last 24h (default). In this case end-users are expected to manually re-trigger the reconciliation flow in case they want Gardener to try again. Extension controllers are expected to follow the same principle. This means they have to read the shoot state out of the Cluster resource.  References and additional resources  Cluster API (Golang specification)  "},{"uri":"https://gardener.cloud/contribute/code/","title":"Code","tags":[],"description":"","content":" Contributing Code How to Contribute to the Open Source Project Gardener    You are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions:\n Contributions must be licensed under the Apache 2.0 License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.  "},{"uri":"https://gardener.cloud/contribute/code/","title":"Code","tags":[],"description":"","content":" Contributing Code How to Contribute to the Open Source Project Gardener    You are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions:\n Contributions must be licensed under the Apache 2.0 License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.  "},{"uri":"https://gardener.cloud/contribute/code/","title":"Code","tags":[],"description":"","content":" Contributing Code How to Contribute to the Open Source Project Gardener    You are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions:\n Contributions must be licensed under the Apache 2.0 License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.  "},{"uri":"https://gardener.cloud/community/","title":"Community","tags":[],"description":"","content":"Gardener Community Follow - Engage - Contribute\n@GardenerProject  Follow the latest project updates on Twitter  Community Meetings  You are welcome on our community meetings where you can engage with other contributors in person. See calendar for schedule or watch past recordings to get the idea.  GitHub  Eveyone is welcome to contribute with what they can - an issue or a pull request. Check Gardener project there and our contributors guide to help you get started.   Gardener Project  Watch videos and community meetings recordings on our YouTube channel  #gardener  Discuss Gardener on our Slack channel in the Kubernetes workspace   COMMUNITY The Gardener development process is an open process. Here are the general communication channels we use to communicate. We work with the wider community to create a strong, vibrant codebase. 60+ Committer  1300+ Merged Pull Requests  1400+ Github Stars  500+ Closed Community Issues   We are cordially inviting interested parties to join our weekly meetings. Here you can address questions regarding the direction of the project, technical problems and support.   Our Slack Channel is the best way to contact the experts in all questions about Kubernetes and the Gardener and share your ideas with them or ask for support.   Find out more about the project and consider making a contribution..     "},{"uri":"https://gardener.cloud/documentation/contribute/10_code/15_conf_secrets/","title":"Configuration and Usage","tags":[],"description":"","content":"Gardener Configuration and Usage Gardener automates the full lifecycle of Kubernetes clusters as a service. Additionally, it has several extension points allowing external controllers to plug-in to the lifecycle. As a consequence, there are several configuration options for the various custom resources that are partially required.\nThis document describes the\n configuration and usage of Gardener as operator/administrator. configuration and usage of Gardener as end-user/stakeholder/customer.  Configuration and Usage of Gardener as Operator/Administrator When we use the terms \u0026ldquo;operator/administrator\u0026rdquo; we refer to both the people deploying and operating Gardener. Gardener consists out of four components:\n gardener-apiserver, a Kubernetes-native API extension that serves custom resources in the Kubernetes-style (like Seeds and Shoots), and a component that contains multiple admission plugins. gardener-controller-manager, a component consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for maintaining Shoots, reconciling Plants, etc.). gardener-scheduler, a component that assigns newly created Shoot clusters to appropriate Seed clusters. gardenlet, a component running in seed clusters and consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for reconciliation and deletion of Shoots).  Each of these components have various configuration options. The gardener-apiserver uses the standard API server library maintained by the Kubernetes community, and as such it mainly supports command line flags. The two other components are using so-called componentconfig files that describe their configuration in a Kubernetes-style versioned object.\nConfiguration file for Gardener controller manager The Gardener controller manager does only support one command line flag which should be a path to a valid controller-manager configuration file. Please take a look at this example configuration.\nConfiguration file for Gardener scheduler The Gardener scheduler also only supports one command line flag which should be a path to a valid scheduler configuration file. Please take a look at this example configuration. Information about the concepts of the Gardener scheduler can be found here\nConfiguration file for Gardenlet The Gardenlet also only supports one command line flag which should be a path to a valid gardenlet configuration file. Please take a look at this example configuration. Information about the concepts of the Gardenlet can be found here\nSystem configuration After successful deployment of the four components you need to setup the system. Let\u0026rsquo;s first focus on some \u0026ldquo;static\u0026rdquo; configuration. When the gardenlet starts it scans the garden namespace of the garden cluster for Secrets that have influence on its reconciliation loops, mainly the Shoot reconciliation:\n  Internal domain secret, contains the DNS provider credentials (having appropriate privileges) which will be used to create/delete so-called \u0026ldquo;internal\u0026rdquo; DNS records for the Shoot clusters, please see this for an example.\n This secret is used in order to establish a stable endpoint for shoot clusters which is used internally by all control plane components. The DNS records are normal DNS records but called \u0026ldquo;internal\u0026rdquo; in our scenario because only the kubeconfigs for the control plane components use this endpoint when talking to the shoot clusters. It is forbidden to change the internal domain secret if there are existing shoot clusters.    Default domain secrets (optional), contain the DNS provider credentials (having appropriate privileges) which will be used to create/delete DNS records for a default domain for shoots (e.g., example.com), please see this for an example.\n Not every end-user/stakeholder/customer has its own domain, however, Gardener needs to create a DNS record for every shoot cluster. As landscape operator you might want to define a default domain owned and controlled by you that is used for all shoot clusters that don\u0026rsquo;t specify their own domain.    :warning: Please note that the mentioned domain secrets are only needed if you have at least one seed cluster that is not tainted with seed.gardener.cloud/disable-dns. Seeds with this taint don\u0026rsquo;t create any DNS records for shoots scheduled on it, hence, if you only have such seeds, you don\u0026rsquo;t need to create the domain secrets.\n  Alerting secrets (optional), contain the alerting configuration and credentials for the AlertManager to send email alerts. It is also possible to configure the monitoring stack to send alerts to an AlertManager not deployed by Gardener to handle alerting. Please see this for an example.\n If email alerting is configured:  An AlertManager is deployed into each seed cluster that handles the alerting for all shoots on the seed cluster. Gardener will inject the SMTP credentials into the configuration of the AlertManager. The AlertManager will send emails to the configured email address in case any alerts are firing.   If an external AlertManager is configured:  Each shoot has a Prometheus responsible for monitoring components and sending out alerts. The alerts will be sent to a URL configured in the alerting secret. This external AlertManager is not managed by Gardener and can be configured however the operator sees fit. Supported authentication types are no authentication, basic, or mutual TLS.      OpenVPN Diffie-Hellmann Key secret (optional), contains the self-generated Diffie-Hellmann key used by OpenVPN in your landscape, please see this for an example.\n If you don\u0026rsquo;t specify a custom key then a default key is used, but for productive landscapes it\u0026rsquo;s recommend to create a landscape-specific key and define it.    Global monitoring secrets (optional), contains basic authentication credentials for the Prometheus aggregating metrics for all clusters.\n These secrets are synced to each seed cluster and used to gain access to the aggregate monitoring components.    Apart from this \u0026ldquo;static\u0026rdquo; configuration there are several custom resources extending the Kubernetes API and used by Gardener. As an operator/administrator you have to configure some of them to make the system work.\nConfiguration and Usage of Gardener as End-User/Stakeholder/Customer As an end-user/stakeholder/customer you are using a Gardener landscape that has been setup for you by another team. You don\u0026rsquo;t need to care about how Gardener itself has to be configured or how it has to be deployed. Take a look at this document - it describes which resources are offered by Gardener. You may want to have a more detailed look for Projects, SecretBindings, Shoots, Plants, and (Cluster)OpenIDConnectPresets.\n"},{"uri":"https://gardener.cloud/v1.12.8/contribute/10_code/15_conf_secrets/","title":"Configuration and Usage","tags":[],"description":"","content":"Gardener Configuration and Usage Gardener automates the full lifecycle of Kubernetes clusters as a service. Additionally, it has several extension points allowing external controllers to plug-in to the lifecycle. As a consequence, there are several configuration options for the various custom resources that are partially required.\nThis document describes the\n configuration and usage of Gardener as operator/administrator. configuration and usage of Gardener as end-user/stakeholder/customer.  Configuration and Usage of Gardener as Operator/Administrator When we use the terms \u0026ldquo;operator/administrator\u0026rdquo; we refer to both the people deploying and operating Gardener. Gardener consists out of four components:\n gardener-apiserver, a Kubernetes-native API extension that serves custom resources in the Kubernetes-style (like Seeds and Shoots), and a component that contains multiple admission plugins. gardener-controller-manager, a component consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for maintaining Shoots, reconciling Plants, etc.). gardener-scheduler, a component that assigns newly created Shoot clusters to appropriate Seed clusters. gardenlet, a component running in seed clusters and consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for reconciliation and deletion of Shoots).  Each of these components have various configuration options. The gardener-apiserver uses the standard API server library maintained by the Kubernetes community, and as such it mainly supports command line flags. The two other components are using so-called componentconfig files that describe their configuration in a Kubernetes-style versioned object.\nConfiguration file for Gardener controller manager The Gardener controller manager does only support one command line flag which should be a path to a valid controller-manager configuration file. Please take a look at this example configuration.\nConfiguration file for Gardener scheduler The Gardener scheduler also only supports one command line flag which should be a path to a valid scheduler configuration file. Please take a look at this example configuration. Information about the concepts of the Gardener scheduler can be found here\nConfiguration file for Gardenlet The Gardenlet also only supports one command line flag which should be a path to a valid gardenlet configuration file. Please take a look at this example configuration. Information about the concepts of the Gardenlet can be found here\nSystem configuration After successful deployment of the four components you need to setup the system. Let\u0026rsquo;s first focus on some \u0026ldquo;static\u0026rdquo; configuration. When the gardenlet starts it scans the garden namespace of the garden cluster for Secrets that have influence on its reconciliation loops, mainly the Shoot reconciliation:\n  Internal domain secret, contains the DNS provider credentials (having appropriate privileges) which will be used to create/delete so-called \u0026ldquo;internal\u0026rdquo; DNS records for the Shoot clusters, please see this for an example.\n This secret is used in order to establish a stable endpoint for shoot clusters which is used internally by all control plane components. The DNS records are normal DNS records but called \u0026ldquo;internal\u0026rdquo; in our scenario because only the kubeconfigs for the control plane components use this endpoint when talking to the shoot clusters. It is forbidden to change the internal domain secret if there are existing shoot clusters.    Default domain secrets (optional), contain the DNS provider credentials (having appropriate privileges) which will be used to create/delete DNS records for a default domain for shoots (e.g., example.com), please see this for an example.\n Not every end-user/stakeholder/customer has its own domain, however, Gardener needs to create a DNS record for every shoot cluster. As landscape operator you might want to define a default domain owned and controlled by you that is used for all shoot clusters that don\u0026rsquo;t specify their own domain.    :warning: Please note that the mentioned domain secrets are only needed if you have at least one seed cluster that is not tainted with seed.gardener.cloud/disable-dns. Seeds with this taint don\u0026rsquo;t create any DNS records for shoots scheduled on it, hence, if you only have such seeds, you don\u0026rsquo;t need to create the domain secrets.\n  Alerting secrets (optional), contain the alerting configuration and credentials for the AlertManager to send email alerts. It is also possible to configure the monitoring stack to send alerts to an AlertManager not deployed by Gardener to handle alerting. Please see this for an example.\n If email alerting is configured:  An AlertManager is deployed into each seed cluster that handles the alerting for all shoots on the seed cluster. Gardener will inject the SMTP credentials into the configuration of the AlertManager. The AlertManager will send emails to the configured email address in case any alerts are firing.   If an external AlertManager is configured:  Each shoot has a Prometheus responsible for monitoring components and sending out alerts. The alerts will be sent to a URL configured in the alerting secret. This external AlertManager is not managed by Gardener and can be configured however the operator sees fit. Supported authentication types are no authentication, basic, or mutual TLS.      OpenVPN Diffie-Hellmann Key secret (optional), contains the self-generated Diffie-Hellmann key used by OpenVPN in your landscape, please see this for an example.\n If you don\u0026rsquo;t specify a custom key then a default key is used, but for productive landscapes it\u0026rsquo;s recommend to create a landscape-specific key and define it.    Global monitoring secrets (optional), contains basic authentication credentials for the Prometheus aggregating metrics for all clusters.\n These secrets are synced to each seed cluster and used to gain access to the aggregate monitoring components.    Apart from this \u0026ldquo;static\u0026rdquo; configuration there are several custom resources extending the Kubernetes API and used by Gardener. As an operator/administrator you have to configure some of them to make the system work.\nConfiguration and Usage of Gardener as End-User/Stakeholder/Customer As an end-user/stakeholder/customer you are using a Gardener landscape that has been setup for you by another team. You don\u0026rsquo;t need to care about how Gardener itself has to be configured or how it has to be deployed. Take a look at this document - it describes which resources are offered by Gardener. You may want to have a more detailed look for Projects, SecretBindings, Shoots, Plants, and (Cluster)OpenIDConnectPresets.\n"},{"uri":"https://gardener.cloud/v1.13.2/contribute/10_code/15_conf_secrets/","title":"Configuration and Usage","tags":[],"description":"","content":"Gardener Configuration and Usage Gardener automates the full lifecycle of Kubernetes clusters as a service. Additionally, it has several extension points allowing external controllers to plug-in to the lifecycle. As a consequence, there are several configuration options for the various custom resources that are partially required.\nThis document describes the\n configuration and usage of Gardener as operator/administrator. configuration and usage of Gardener as end-user/stakeholder/customer.  Configuration and Usage of Gardener as Operator/Administrator When we use the terms \u0026ldquo;operator/administrator\u0026rdquo; we refer to both the people deploying and operating Gardener. Gardener consists out of four components:\n gardener-apiserver, a Kubernetes-native API extension that serves custom resources in the Kubernetes-style (like Seeds and Shoots), and a component that contains multiple admission plugins. gardener-controller-manager, a component consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for maintaining Shoots, reconciling Plants, etc.). gardener-scheduler, a component that assigns newly created Shoot clusters to appropriate Seed clusters. gardenlet, a component running in seed clusters and consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for reconciliation and deletion of Shoots).  Each of these components have various configuration options. The gardener-apiserver uses the standard API server library maintained by the Kubernetes community, and as such it mainly supports command line flags. The two other components are using so-called componentconfig files that describe their configuration in a Kubernetes-style versioned object.\nConfiguration file for Gardener controller manager The Gardener controller manager does only support one command line flag which should be a path to a valid controller-manager configuration file. Please take a look at this example configuration.\nConfiguration file for Gardener scheduler The Gardener scheduler also only supports one command line flag which should be a path to a valid scheduler configuration file. Please take a look at this example configuration. Information about the concepts of the Gardener scheduler can be found here\nConfiguration file for Gardenlet The Gardenlet also only supports one command line flag which should be a path to a valid gardenlet configuration file. Please take a look at this example configuration. Information about the concepts of the Gardenlet can be found here\nSystem configuration After successful deployment of the four components you need to setup the system. Let\u0026rsquo;s first focus on some \u0026ldquo;static\u0026rdquo; configuration. When the gardenlet starts it scans the garden namespace of the garden cluster for Secrets that have influence on its reconciliation loops, mainly the Shoot reconciliation:\n  Internal domain secret, contains the DNS provider credentials (having appropriate privileges) which will be used to create/delete so-called \u0026ldquo;internal\u0026rdquo; DNS records for the Shoot clusters, please see this for an example.\n This secret is used in order to establish a stable endpoint for shoot clusters which is used internally by all control plane components. The DNS records are normal DNS records but called \u0026ldquo;internal\u0026rdquo; in our scenario because only the kubeconfigs for the control plane components use this endpoint when talking to the shoot clusters. It is forbidden to change the internal domain secret if there are existing shoot clusters.    Default domain secrets (optional), contain the DNS provider credentials (having appropriate privileges) which will be used to create/delete DNS records for a default domain for shoots (e.g., example.com), please see this for an example.\n Not every end-user/stakeholder/customer has its own domain, however, Gardener needs to create a DNS record for every shoot cluster. As landscape operator you might want to define a default domain owned and controlled by you that is used for all shoot clusters that don\u0026rsquo;t specify their own domain.    :warning: Please note that the mentioned domain secrets are only needed if you have at least one seed cluster that is not tainted with seed.gardener.cloud/disable-dns. Seeds with this taint don\u0026rsquo;t create any DNS records for shoots scheduled on it, hence, if you only have such seeds, you don\u0026rsquo;t need to create the domain secrets.\n  Alerting secrets (optional), contain the alerting configuration and credentials for the AlertManager to send email alerts. It is also possible to configure the monitoring stack to send alerts to an AlertManager not deployed by Gardener to handle alerting. Please see this for an example.\n If email alerting is configured:  An AlertManager is deployed into each seed cluster that handles the alerting for all shoots on the seed cluster. Gardener will inject the SMTP credentials into the configuration of the AlertManager. The AlertManager will send emails to the configured email address in case any alerts are firing.   If an external AlertManager is configured:  Each shoot has a Prometheus responsible for monitoring components and sending out alerts. The alerts will be sent to a URL configured in the alerting secret. This external AlertManager is not managed by Gardener and can be configured however the operator sees fit. Supported authentication types are no authentication, basic, or mutual TLS.      OpenVPN Diffie-Hellmann Key secret (optional), contains the self-generated Diffie-Hellmann key used by OpenVPN in your landscape, please see this for an example.\n If you don\u0026rsquo;t specify a custom key then a default key is used, but for productive landscapes it\u0026rsquo;s recommend to create a landscape-specific key and define it.    Global monitoring secrets (optional), contains basic authentication credentials for the Prometheus aggregating metrics for all clusters.\n These secrets are synced to each seed cluster and used to gain access to the aggregate monitoring components.    Apart from this \u0026ldquo;static\u0026rdquo; configuration there are several custom resources extending the Kubernetes API and used by Gardener. As an operator/administrator you have to configure some of them to make the system work.\nConfiguration and Usage of Gardener as End-User/Stakeholder/Customer As an end-user/stakeholder/customer you are using a Gardener landscape that has been setup for you by another team. You don\u0026rsquo;t need to care about how Gardener itself has to be configured or how it has to be deployed. Take a look at this document - it describes which resources are offered by Gardener. You may want to have a more detailed look for Projects, SecretBindings, Shoots, Plants, and (Cluster)OpenIDConnectPresets.\n"},{"uri":"https://gardener.cloud/documentation/guides/applications/missing-registry-permission/","title":"Container image not pulled","tags":[],"description":"Wrong Container Image or Invalid Registry Permissions","content":"Problem Two of the most common problems are specifying the wrong container image or trying to use private images without providing registry credentials.\nNote: There is no observable difference in Pod status between a missing image and incorrect registry permissions. In either case, Kubernetes will report an ErrImagePull status for the Pods. For this reason, this article deals with both scenarios.\nExample Let\u0026rsquo;s see an example. We\u0026rsquo;ll create a pod named fail referencing a non-existent Docker image:\nkubectl run -i --tty fail --image=tutum/curl:1.123456 the command prompt doesn\u0026rsquo;t return and you can press ctrl+c\nError analysis We can then inspect our Pods and see that we have one Pod with a status of ErrImagePull or ImagePullBackOff.\n$ (minikube) kubectl get pods NAME READY STATUS RESTARTS AGE client-5b65b6c866-cs4ch 1/1 Running 1 1m fail-6667d7685d-7v6w8 0/1 ErrImagePull 0 \u0026lt;invalid\u0026gt; vuejs-578574b75f-5x98z 1/1 Running 0 1d $ (minikube) For some additional information, we can describe the failing Pod.\nkubectl describe pod fail-6667d7685d-7v6w8 As you can see in the events section, your image can\u0026rsquo;t be pulled\nName:\tfail-6667d7685d-7v6w8 Namespace:\tdefault Node:\tminikube/192.168.64.10 Start Time:\tWed, 22 Nov 2017 10:01:59 +0100 Labels:\tpod-template-hash=2223832418 run=fail Annotations:\tkubernetes.io/created-by={\u0026#34;kind\u0026#34;:\u0026#34;SerializedReference\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;reference\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;ReplicaSet\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;fail-6667d7685d\u0026#34;,\u0026#34;uid\u0026#34;:\u0026#34;cc4ccb3f-cf63-11e7-afca-4a7a1fa05b3f\u0026#34;,\u0026#34;a... . . . . Events: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 1m\t1m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned fail-6667d7685d-7v6w8 to minikube 1m\t1m\t1\tkubelet, minikube\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026#34;default-token-9fr6r\u0026#34; 1m\t6s\t4\tkubelet, minikube\tspec.containers{fail}\tNormal\tPulling\tpulling image \u0026#34;tutum/curl:1.123456\u0026#34; 1m\t5s\t4\tkubelet, minikube\tspec.containers{fail}\tWarning\tFailed\tFailed to pull image \u0026#34;tutum/curl:1.123456\u0026#34;: rpc error: code = Unknown desc = Error response from daemon: manifest for tutum/curl:1.123456 not found 1m\t\u0026lt;invalid\u0026gt;\t10\tkubelet, minikube\tWarning\tFailedSync\tError syncing pod 1m\t\u0026lt;invalid\u0026gt;\t6\tkubelet, minikube\tspec.containers{fail}\tNormal\tBackOff\tBack-off pulling image \u0026#34;tutum/curl:1.123456\u0026#34; Why couldn\u0026rsquo;t Kubernetes pull the image? There are three primary candidates besides network connectivity issues:\n The image tag is incorrect The image doesn\u0026rsquo;t exist Kubernetes doesn\u0026rsquo;t have permissions to pull that image  If you don\u0026rsquo;t notice a typo in your image tag, then it\u0026rsquo;s time to test using your local machine. I usually start by running docker pull on my local development machine with the exact same image tag. In this case, I would run docker pull tutum/curl:1.123456\nIf this succeeds, then it probably means that Kubernetes doesn\u0026rsquo;t have correct permissions to pull that image.\nAdd the docker registry user/pwd to your cluster\nkubectl create secret docker-registry dockersecret --docker-server=https://index.docker.io/v1/ --docker-username=\u0026lt;username\u0026gt; --docker-password=\u0026lt;password\u0026gt; --docker-email=\u0026lt;email\u0026gt; If the exact image tag fails, then I will test without an explicit image tag - docker pull tutum/curl - which will attempt to pull the latest tag. If this succeeds, then that means the originally specified tag doesn\u0026rsquo;t exist. Go to the Docker registry and check which tags are available for this image.\nIf docker pull tutum/curl (without an exact tag) fails, then we have a bigger problem - that image does not exist at all in our image registry.\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/applications/missing-registry-permission/","title":"Container image not pulled","tags":[],"description":"Wrong Container Image or Invalid Registry Permissions","content":"Problem Two of the most common problems are specifying the wrong container image or trying to use private images without providing registry credentials.\nNote: There is no observable difference in Pod status between a missing image and incorrect registry permissions. In either case, Kubernetes will report an ErrImagePull status for the Pods. For this reason, this article deals with both scenarios.\nExample Let\u0026rsquo;s see an example. We\u0026rsquo;ll create a pod named fail referencing a non-existent Docker image:\nkubectl run -i --tty fail --image=tutum/curl:1.123456 the command prompt doesn\u0026rsquo;t return and you can press ctrl+c\nError analysis We can then inspect our Pods and see that we have one Pod with a status of ErrImagePull or ImagePullBackOff.\n$ (minikube) kubectl get pods NAME READY STATUS RESTARTS AGE client-5b65b6c866-cs4ch 1/1 Running 1 1m fail-6667d7685d-7v6w8 0/1 ErrImagePull 0 \u0026lt;invalid\u0026gt; vuejs-578574b75f-5x98z 1/1 Running 0 1d $ (minikube) For some additional information, we can describe the failing Pod.\nkubectl describe pod fail-6667d7685d-7v6w8 As you can see in the events section, your image can\u0026rsquo;t be pulled\nName:\tfail-6667d7685d-7v6w8 Namespace:\tdefault Node:\tminikube/192.168.64.10 Start Time:\tWed, 22 Nov 2017 10:01:59 +0100 Labels:\tpod-template-hash=2223832418 run=fail Annotations:\tkubernetes.io/created-by={\u0026#34;kind\u0026#34;:\u0026#34;SerializedReference\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;reference\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;ReplicaSet\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;fail-6667d7685d\u0026#34;,\u0026#34;uid\u0026#34;:\u0026#34;cc4ccb3f-cf63-11e7-afca-4a7a1fa05b3f\u0026#34;,\u0026#34;a... . . . . Events: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 1m\t1m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned fail-6667d7685d-7v6w8 to minikube 1m\t1m\t1\tkubelet, minikube\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026#34;default-token-9fr6r\u0026#34; 1m\t6s\t4\tkubelet, minikube\tspec.containers{fail}\tNormal\tPulling\tpulling image \u0026#34;tutum/curl:1.123456\u0026#34; 1m\t5s\t4\tkubelet, minikube\tspec.containers{fail}\tWarning\tFailed\tFailed to pull image \u0026#34;tutum/curl:1.123456\u0026#34;: rpc error: code = Unknown desc = Error response from daemon: manifest for tutum/curl:1.123456 not found 1m\t\u0026lt;invalid\u0026gt;\t10\tkubelet, minikube\tWarning\tFailedSync\tError syncing pod 1m\t\u0026lt;invalid\u0026gt;\t6\tkubelet, minikube\tspec.containers{fail}\tNormal\tBackOff\tBack-off pulling image \u0026#34;tutum/curl:1.123456\u0026#34; Why couldn\u0026rsquo;t Kubernetes pull the image? There are three primary candidates besides network connectivity issues:\n The image tag is incorrect The image doesn\u0026rsquo;t exist Kubernetes doesn\u0026rsquo;t have permissions to pull that image  If you don\u0026rsquo;t notice a typo in your image tag, then it\u0026rsquo;s time to test using your local machine. I usually start by running docker pull on my local development machine with the exact same image tag. In this case, I would run docker pull tutum/curl:1.123456\nIf this succeeds, then it probably means that Kubernetes doesn\u0026rsquo;t have correct permissions to pull that image.\nAdd the docker registry user/pwd to your cluster\nkubectl create secret docker-registry dockersecret --docker-server=https://index.docker.io/v1/ --docker-username=\u0026lt;username\u0026gt; --docker-password=\u0026lt;password\u0026gt; --docker-email=\u0026lt;email\u0026gt; If the exact image tag fails, then I will test without an explicit image tag - docker pull tutum/curl - which will attempt to pull the latest tag. If this succeeds, then that means the originally specified tag doesn\u0026rsquo;t exist. Go to the Docker registry and check which tags are available for this image.\nIf docker pull tutum/curl (without an exact tag) fails, then we have a bigger problem - that image does not exist at all in our image registry.\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/applications/missing-registry-permission/","title":"Container image not pulled","tags":[],"description":"Wrong Container Image or Invalid Registry Permissions","content":"Problem Two of the most common problems are specifying the wrong container image or trying to use private images without providing registry credentials.\nNote: There is no observable difference in Pod status between a missing image and incorrect registry permissions. In either case, Kubernetes will report an ErrImagePull status for the Pods. For this reason, this article deals with both scenarios.\nExample Let\u0026rsquo;s see an example. We\u0026rsquo;ll create a pod named fail referencing a non-existent Docker image:\nkubectl run -i --tty fail --image=tutum/curl:1.123456 the command prompt doesn\u0026rsquo;t return and you can press ctrl+c\nError analysis We can then inspect our Pods and see that we have one Pod with a status of ErrImagePull or ImagePullBackOff.\n$ (minikube) kubectl get pods NAME READY STATUS RESTARTS AGE client-5b65b6c866-cs4ch 1/1 Running 1 1m fail-6667d7685d-7v6w8 0/1 ErrImagePull 0 \u0026lt;invalid\u0026gt; vuejs-578574b75f-5x98z 1/1 Running 0 1d $ (minikube) For some additional information, we can describe the failing Pod.\nkubectl describe pod fail-6667d7685d-7v6w8 As you can see in the events section, your image can\u0026rsquo;t be pulled\nName:\tfail-6667d7685d-7v6w8 Namespace:\tdefault Node:\tminikube/192.168.64.10 Start Time:\tWed, 22 Nov 2017 10:01:59 +0100 Labels:\tpod-template-hash=2223832418 run=fail Annotations:\tkubernetes.io/created-by={\u0026#34;kind\u0026#34;:\u0026#34;SerializedReference\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;reference\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;ReplicaSet\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;fail-6667d7685d\u0026#34;,\u0026#34;uid\u0026#34;:\u0026#34;cc4ccb3f-cf63-11e7-afca-4a7a1fa05b3f\u0026#34;,\u0026#34;a... . . . . Events: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 1m\t1m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned fail-6667d7685d-7v6w8 to minikube 1m\t1m\t1\tkubelet, minikube\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026#34;default-token-9fr6r\u0026#34; 1m\t6s\t4\tkubelet, minikube\tspec.containers{fail}\tNormal\tPulling\tpulling image \u0026#34;tutum/curl:1.123456\u0026#34; 1m\t5s\t4\tkubelet, minikube\tspec.containers{fail}\tWarning\tFailed\tFailed to pull image \u0026#34;tutum/curl:1.123456\u0026#34;: rpc error: code = Unknown desc = Error response from daemon: manifest for tutum/curl:1.123456 not found 1m\t\u0026lt;invalid\u0026gt;\t10\tkubelet, minikube\tWarning\tFailedSync\tError syncing pod 1m\t\u0026lt;invalid\u0026gt;\t6\tkubelet, minikube\tspec.containers{fail}\tNormal\tBackOff\tBack-off pulling image \u0026#34;tutum/curl:1.123456\u0026#34; Why couldn\u0026rsquo;t Kubernetes pull the image? There are three primary candidates besides network connectivity issues:\n The image tag is incorrect The image doesn\u0026rsquo;t exist Kubernetes doesn\u0026rsquo;t have permissions to pull that image  If you don\u0026rsquo;t notice a typo in your image tag, then it\u0026rsquo;s time to test using your local machine. I usually start by running docker pull on my local development machine with the exact same image tag. In this case, I would run docker pull tutum/curl:1.123456\nIf this succeeds, then it probably means that Kubernetes doesn\u0026rsquo;t have correct permissions to pull that image.\nAdd the docker registry user/pwd to your cluster\nkubectl create secret docker-registry dockersecret --docker-server=https://index.docker.io/v1/ --docker-username=\u0026lt;username\u0026gt; --docker-password=\u0026lt;password\u0026gt; --docker-email=\u0026lt;email\u0026gt; If the exact image tag fails, then I will test without an explicit image tag - docker pull tutum/curl - which will attempt to pull the latest tag. If this succeeds, then that means the originally specified tag doesn\u0026rsquo;t exist. Go to the Docker registry and check which tags are available for this image.\nIf docker pull tutum/curl (without an exact tag) fails, then we have a bigger problem - that image does not exist at all in our image registry.\n"},{"uri":"https://gardener.cloud/documentation/guides/applications/image-pull-policy/","title":"Container image not updating","tags":[],"description":"Updating Images in your cluster during development","content":"Preface A container image should use a fixed tag or the content hash of the image. It should not use the tags latest, head, canary, or other tags that are designed to be floating.\nProblem Many Kubernetes users have run into this problem. The story goes something like this:\n Deploy any image using an image tag (e.g. cp-enablement/awesomeapp:1.0) Fix a bug in awesomeapp Build a new image and push it with the same tag (cp-enablement/awesomeapp:1.0) Update your deployment Realize that the bug is still present Rinse and repeat steps 3 to 5 until you recognize this doesn\u0026rsquo;t work  The problem relates to how Kubernetes decides whether to do a docker pull when starting a container. Since we tagged our image as :1.0, the default pull policy is IfNotPresent. The Kubelet already has a local copy of cp-enablement/awesomeapp:1.0, hence it doesn\u0026rsquo;t attempt to do a docker pull. When the new Pods come up, they still use the old broken Docker image.\nThere are three ways to resolve this:\n Switch to using the tag :latest (DO NOT DO THIS!) Specify ImagePullPolicy: always (not recomended). Use unique tags (best practice)  Solution In the quest to automate myself out of a job, I created a bash script that runs anytime to create a new tag and push the build result to the registry.\n#!/usr/bin/env bash  # Set the docker image name and the corresponding repository # Ensure that you change them in the deployment.yml as well. # You must be logged in with docker login # # CHANGE THIS TO YOUR Docker.io SETTINGS # PROJECT=awesomeapp REPOSITORY=cp-enablement # exit if any subcommand or pipeline returns a non-zero status. set -e # set debug mode #set -x # build my nodeJS app # npm run build # get latest version IDs from the Docker.io registry and increment them # VERSION=$(curl https://registry.hub.docker.com/v1/repositories/$REPOSITORY/$PROJECT/tags | sed -e \u0026#39;s/[][]//g\u0026#39; -e \u0026#39;s/\u0026#34;//g\u0026#39; -e \u0026#39;s/ //g\u0026#39; | tr \u0026#39;}\u0026#39; \u0026#39;\\n\u0026#39; | awk -F: \u0026#39;{print $3}\u0026#39; | grep v| tail -n 1) VERSION=${VERSION:1} ((VERSION++)) VERSION=\u0026#34;v$VERSION\u0026#34; # build a new docker image # echo \u0026#39;\u0026gt;\u0026gt;\u0026gt; Building new image\u0026#39; # Due to a bug in Docker we need to analyse the log to find out if build passed (see https://github.com/dotcloud/docker/issues/1875) docker build --no-cache=true -t $REPOSITORY/$PROJECT:$VERSION . | tee /tmp/docker_build_result.log RESULT=$(cat /tmp/docker_build_result.log | tail -n 1) if [[ \u0026#34;$RESULT\u0026#34; != *Successfully* ]]; then exit -1 fi echo \u0026#39;\u0026gt;\u0026gt;\u0026gt; Push new image\u0026#39; docker push $REPOSITORY/$PROJECT:$VERSION "},{"uri":"https://gardener.cloud/v1.12.8/guides/applications/image-pull-policy/","title":"Container image not updating","tags":[],"description":"Updating Images in your cluster during development","content":"Preface A container image should use a fixed tag or the content hash of the image. It should not use the tags latest, head, canary, or other tags that are designed to be floating.\nProblem Many Kubernetes users have run into this problem. The story goes something like this:\n Deploy any image using an image tag (e.g. cp-enablement/awesomeapp:1.0) Fix a bug in awesomeapp Build a new image and push it with the same tag (cp-enablement/awesomeapp:1.0) Update your deployment Realize that the bug is still present Rinse and repeat steps 3 to 5 until you recognize this doesn\u0026rsquo;t work  The problem relates to how Kubernetes decides whether to do a docker pull when starting a container. Since we tagged our image as :1.0, the default pull policy is IfNotPresent. The Kubelet already has a local copy of cp-enablement/awesomeapp:1.0, hence it doesn\u0026rsquo;t attempt to do a docker pull. When the new Pods come up, they still use the old broken Docker image.\nThere are three ways to resolve this:\n Switch to using the tag :latest (DO NOT DO THIS!) Specify ImagePullPolicy: always (not recomended). Use unique tags (best practice)  Solution In the quest to automate myself out of a job, I created a bash script that runs anytime to create a new tag and push the build result to the registry.\n#!/usr/bin/env bash  # Set the docker image name and the corresponding repository # Ensure that you change them in the deployment.yml as well. # You must be logged in with docker login # # CHANGE THIS TO YOUR Docker.io SETTINGS # PROJECT=awesomeapp REPOSITORY=cp-enablement # exit if any subcommand or pipeline returns a non-zero status. set -e # set debug mode #set -x # build my nodeJS app # npm run build # get latest version IDs from the Docker.io registry and increment them # VERSION=$(curl https://registry.hub.docker.com/v1/repositories/$REPOSITORY/$PROJECT/tags | sed -e \u0026#39;s/[][]//g\u0026#39; -e \u0026#39;s/\u0026#34;//g\u0026#39; -e \u0026#39;s/ //g\u0026#39; | tr \u0026#39;}\u0026#39; \u0026#39;\\n\u0026#39; | awk -F: \u0026#39;{print $3}\u0026#39; | grep v| tail -n 1) VERSION=${VERSION:1} ((VERSION++)) VERSION=\u0026#34;v$VERSION\u0026#34; # build a new docker image # echo \u0026#39;\u0026gt;\u0026gt;\u0026gt; Building new image\u0026#39; # Due to a bug in Docker we need to analyse the log to find out if build passed (see https://github.com/dotcloud/docker/issues/1875) docker build --no-cache=true -t $REPOSITORY/$PROJECT:$VERSION . | tee /tmp/docker_build_result.log RESULT=$(cat /tmp/docker_build_result.log | tail -n 1) if [[ \u0026#34;$RESULT\u0026#34; != *Successfully* ]]; then exit -1 fi echo \u0026#39;\u0026gt;\u0026gt;\u0026gt; Push new image\u0026#39; docker push $REPOSITORY/$PROJECT:$VERSION "},{"uri":"https://gardener.cloud/v1.13.2/guides/applications/image-pull-policy/","title":"Container image not updating","tags":[],"description":"Updating Images in your cluster during development","content":"Preface A container image should use a fixed tag or the content hash of the image. It should not use the tags latest, head, canary, or other tags that are designed to be floating.\nProblem Many Kubernetes users have run into this problem. The story goes something like this:\n Deploy any image using an image tag (e.g. cp-enablement/awesomeapp:1.0) Fix a bug in awesomeapp Build a new image and push it with the same tag (cp-enablement/awesomeapp:1.0) Update your deployment Realize that the bug is still present Rinse and repeat steps 3 to 5 until you recognize this doesn\u0026rsquo;t work  The problem relates to how Kubernetes decides whether to do a docker pull when starting a container. Since we tagged our image as :1.0, the default pull policy is IfNotPresent. The Kubelet already has a local copy of cp-enablement/awesomeapp:1.0, hence it doesn\u0026rsquo;t attempt to do a docker pull. When the new Pods come up, they still use the old broken Docker image.\nThere are three ways to resolve this:\n Switch to using the tag :latest (DO NOT DO THIS!) Specify ImagePullPolicy: always (not recomended). Use unique tags (best practice)  Solution In the quest to automate myself out of a job, I created a bash script that runs anytime to create a new tag and push the build result to the registry.\n#!/usr/bin/env bash  # Set the docker image name and the corresponding repository # Ensure that you change them in the deployment.yml as well. # You must be logged in with docker login # # CHANGE THIS TO YOUR Docker.io SETTINGS # PROJECT=awesomeapp REPOSITORY=cp-enablement # exit if any subcommand or pipeline returns a non-zero status. set -e # set debug mode #set -x # build my nodeJS app # npm run build # get latest version IDs from the Docker.io registry and increment them # VERSION=$(curl https://registry.hub.docker.com/v1/repositories/$REPOSITORY/$PROJECT/tags | sed -e \u0026#39;s/[][]//g\u0026#39; -e \u0026#39;s/\u0026#34;//g\u0026#39; -e \u0026#39;s/ //g\u0026#39; | tr \u0026#39;}\u0026#39; \u0026#39;\\n\u0026#39; | awk -F: \u0026#39;{print $3}\u0026#39; | grep v| tail -n 1) VERSION=${VERSION:1} ((VERSION++)) VERSION=\u0026#34;v$VERSION\u0026#34; # build a new docker image # echo \u0026#39;\u0026gt;\u0026gt;\u0026gt; Building new image\u0026#39; # Due to a bug in Docker we need to analyse the log to find out if build passed (see https://github.com/dotcloud/docker/issues/1875) docker build --no-cache=true -t $REPOSITORY/$PROJECT:$VERSION . | tee /tmp/docker_build_result.log RESULT=$(cat /tmp/docker_build_result.log | tail -n 1) if [[ \u0026#34;$RESULT\u0026#34; != *Successfully* ]]; then exit -1 fi echo \u0026#39;\u0026gt;\u0026gt;\u0026gt; Push new image\u0026#39; docker push $REPOSITORY/$PROJECT:$VERSION "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/shoot-health-status-conditions/","title":"Contributing to shoot health status conditions","tags":[],"description":"","content":"Contributing to shoot health status conditions Gardener checks regularly (every minute by default) the health status of all shoot clusters. It categorizes its checks into four different types:\n APIServerAvailable: This type indicates whether the shoot\u0026rsquo;s kube-apiserver is available or not. ControlPlaneHealthy: This type indicates whether all the control plane components deployed to the shoot\u0026rsquo;s namespace in the seed do exist and are running fine. EveryNodeReady: This type indicates whether all Nodes and all Machine objects report healthiness. SystemComponentsHealthy: This type indicates whether all system components deployed to the kube-system namespace in the shoot do exist and are running fine.  Every Shoot resource has a status.conditions[] list that contains the mentioned types, together with a status (True/False) and a descriptive message/explanation of the status.\nMost extension controllers are deploying components and resources as part of their reconciliation flows into the seed or shoot cluster. A prominent example for this is the ControlPlane controller that usually deploys a cloud-controller-manager or CSI controllers as part of the shoot control plane. Now that the extensions deploy resources into the cluster, especially resources that are essential for the functionality of the cluster, they might want to contribute to Gardener\u0026rsquo;s checks mentioned above.\nWhat can extensions do to contribute to Gardener\u0026rsquo;s health checks? Every extension resource in Gardener\u0026rsquo;s extensions.gardener.cloud/v1alpha1 API group also has a status.conditions[] list (like the Shoot). Extension controllers can write conditions to the resource they are acting on and use a type that also exist in the shoot\u0026rsquo;s conditions. One exception is that APIServerAvailable can\u0026rsquo;t be used as the Gardener clearly can identify the status of this condition and it doesn\u0026rsquo;t make sense for extensions to try to contribute/modify it.\nAs an example for the ControlPlane controller let\u0026rsquo;s take a look at the following resource:\napiVersion:extensions.gardener.cloud/v1alpha1kind:ControlPlanemetadata:name:control-planenamespace:shoot--foo--barspec:...status:conditions:- type:ControlPlaneHealthystatus:\u0026#34;False\u0026#34;reason:DeploymentUnhealthymessage: \u0026#39;Deployment cloud-controller-manager is unhealthy:condition\u0026#34;Available\u0026#34;hasinvalid status False (expected True) due to MinimumReplicasUnavailable:Deploymentdoesnothaveminimumavailability.\u0026#39;lastUpdateTime:\u0026#34;2014-05-25T12:44:27Z\u0026#34;- type:ConfigComputedSuccessfullystatus:\u0026#34;True\u0026#34;reason:ConfigCreatedmessage:Thecloud-provider-confighasbeensuccessfullycomputed.lastUpdateTime:\u0026#34;2014-05-25T12:43:27Z\u0026#34;The extension controller has declared in its extension resource that one of the deployments it is responsible for is unhealthy. Also, it has written a second condition using a type that is unknown by Gardener.\nGardener will pick the list of conditions and recognize that the there is one with a type ControlPlaneHealthy. It will merge it with its own ControlPlaneHealthy condition and report it back to the Shoot's status:\napiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:labels:shoot.gardener.cloud/status:unhealthyname:some-shootnamespace:garden-corespec:status:conditions:- type:APIServerAvailablestatus:\u0026#34;True\u0026#34;reason:HealthzRequestSucceededmessage:APIserver/healthzendpointrespondedwithsuccessstatuscode.[response_time:31ms]lastUpdateTime:\u0026#34;2014-05-23T08:26:52Z\u0026#34;lastTransitionTime:\u0026#34;2014-05-25T12:45:13Z\u0026#34;- type:ControlPlaneHealthystatus:\u0026#34;False\u0026#34;reason:ControlPlaneUnhealthyReportmessage: \u0026#39;Deployment cloud-controller-manager is unhealthy:condition\u0026#34;Available\u0026#34;hasinvalid status False (expected True) due to MinimumReplicasUnavailable:Deploymentdoesnothaveminimumavailability.\u0026#39;lastUpdateTime:\u0026#34;2014-05-25T12:45:13Z\u0026#34;lastTransitionTime:\u0026#34;2014-05-25T12:45:13Z\u0026#34;...Hence, the only duty extensions have is to maintain the health status of their components in the extension resource they are managing. This can be accomplished using the health check library for extensions.\nError Codes The Gardener API includes some well-defined error codes, e.g., ERR_INFRA_UNAUTHORIZED, ERR_INFRA_DEPENDENCIES, etc. Extension may set these error codes in the .status.conditions[].codes[] list in case it makes sense. Gardener will pick them up and will similarly merge them into the .status.conditions[].codes[] list in the Shoot:\nstatus:conditions:- type:ControlPlaneHealthystatus:\u0026#34;False\u0026#34;reason:DeploymentUnhealthymessage: \u0026#39;Deployment cloud-controller-manager is unhealthy:condition\u0026#34;Available\u0026#34;hasinvalid status False (expected True) due to MinimumReplicasUnavailable:Deploymentdoesnothaveminimumavailability.\u0026#39;lastUpdateTime:\u0026#34;2014-05-25T12:44:27Z\u0026#34;codes:- ERR_INFRA_UNAUTHORIZED"},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/shoot-health-status-conditions/","title":"Contributing to shoot health status conditions","tags":[],"description":"","content":"Contributing to shoot health status conditions Gardener checks regularly (every minute by default) the health status of all shoot clusters. It categorizes its checks into four different types:\n APIServerAvailable: This type indicates whether the shoot\u0026rsquo;s kube-apiserver is available or not. ControlPlaneHealthy: This type indicates whether all the control plane components deployed to the shoot\u0026rsquo;s namespace in the seed do exist and are running fine. EveryNodeReady: This type indicates whether all Nodes and all Machine objects report healthiness. SystemComponentsHealthy: This type indicates whether all system components deployed to the kube-system namespace in the shoot do exist and are running fine.  Every Shoot resource has a status.conditions[] list that contains the mentioned types, together with a status (True/False) and a descriptive message/explanation of the status.\nMost extension controllers are deploying components and resources as part of their reconciliation flows into the seed or shoot cluster. A prominent example for this is the ControlPlane controller that usually deploys a cloud-controller-manager or CSI controllers as part of the shoot control plane. Now that the extensions deploy resources into the cluster, especially resources that are essential for the functionality of the cluster, they might want to contribute to Gardener\u0026rsquo;s checks mentioned above.\nWhat can extensions do to contribute to Gardener\u0026rsquo;s health checks? Every extension resource in Gardener\u0026rsquo;s extensions.gardener.cloud/v1alpha1 API group also has a status.conditions[] list (like the Shoot). Extension controllers can write conditions to the resource they are acting on and use a type that also exist in the shoot\u0026rsquo;s conditions. One exception is that APIServerAvailable can\u0026rsquo;t be used as the Gardener clearly can identify the status of this condition and it doesn\u0026rsquo;t make sense for extensions to try to contribute/modify it.\nAs an example for the ControlPlane controller let\u0026rsquo;s take a look at the following resource:\napiVersion:extensions.gardener.cloud/v1alpha1kind:ControlPlanemetadata:name:control-planenamespace:shoot--foo--barspec:...status:conditions:- type:ControlPlaneHealthystatus:\u0026#34;False\u0026#34;reason:DeploymentUnhealthymessage: \u0026#39;Deployment cloud-controller-manager is unhealthy:condition\u0026#34;Available\u0026#34;hasinvalid status False (expected True) due to MinimumReplicasUnavailable:Deploymentdoesnothaveminimumavailability.\u0026#39;lastUpdateTime:\u0026#34;2014-05-25T12:44:27Z\u0026#34;- type:ConfigComputedSuccessfullystatus:\u0026#34;True\u0026#34;reason:ConfigCreatedmessage:Thecloud-provider-confighasbeensuccessfullycomputed.lastUpdateTime:\u0026#34;2014-05-25T12:43:27Z\u0026#34;The extension controller has declared in its extension resource that one of the deployments it is responsible for is unhealthy. Also, it has written a second condition using a type that is unknown by Gardener.\nGardener will pick the list of conditions and recognize that the there is one with a type ControlPlaneHealthy. It will merge it with its own ControlPlaneHealthy condition and report it back to the Shoot's status:\napiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:labels:shoot.gardener.cloud/status:unhealthyname:some-shootnamespace:garden-corespec:status:conditions:- type:APIServerAvailablestatus:\u0026#34;True\u0026#34;reason:HealthzRequestSucceededmessage:APIserver/healthzendpointrespondedwithsuccessstatuscode.[response_time:31ms]lastUpdateTime:\u0026#34;2014-05-23T08:26:52Z\u0026#34;lastTransitionTime:\u0026#34;2014-05-25T12:45:13Z\u0026#34;- type:ControlPlaneHealthystatus:\u0026#34;False\u0026#34;reason:ControlPlaneUnhealthyReportmessage: \u0026#39;Deployment cloud-controller-manager is unhealthy:condition\u0026#34;Available\u0026#34;hasinvalid status False (expected True) due to MinimumReplicasUnavailable:Deploymentdoesnothaveminimumavailability.\u0026#39;lastUpdateTime:\u0026#34;2014-05-25T12:45:13Z\u0026#34;lastTransitionTime:\u0026#34;2014-05-25T12:45:13Z\u0026#34;...Hence, the only duty extensions have is to maintain the health status of their components in the extension resource they are managing. This can be accomplished using the health check library for extensions.\nError Codes The Gardener API includes some well-defined error codes, e.g., ERR_INFRA_UNAUTHORIZED, ERR_INFRA_DEPENDENCIES, etc. Extension may set these error codes in the .status.conditions[].codes[] list in case it makes sense. Gardener will pick them up and will similarly merge them into the .status.conditions[].codes[] list in the Shoot:\nstatus:conditions:- type:ControlPlaneHealthystatus:\u0026#34;False\u0026#34;reason:DeploymentUnhealthymessage: \u0026#39;Deployment cloud-controller-manager is unhealthy:condition\u0026#34;Available\u0026#34;hasinvalid status False (expected True) due to MinimumReplicasUnavailable:Deploymentdoesnothaveminimumavailability.\u0026#39;lastUpdateTime:\u0026#34;2014-05-25T12:44:27Z\u0026#34;codes:- ERR_INFRA_UNAUTHORIZED"},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/shoot-health-status-conditions/","title":"Contributing to shoot health status conditions","tags":[],"description":"","content":"Contributing to shoot health status conditions Gardener checks regularly (every minute by default) the health status of all shoot clusters. It categorizes its checks into four different types:\n APIServerAvailable: This type indicates whether the shoot\u0026rsquo;s kube-apiserver is available or not. ControlPlaneHealthy: This type indicates whether all the control plane components deployed to the shoot\u0026rsquo;s namespace in the seed do exist and are running fine. EveryNodeReady: This type indicates whether all Nodes and all Machine objects report healthiness. SystemComponentsHealthy: This type indicates whether all system components deployed to the kube-system namespace in the shoot do exist and are running fine.  Every Shoot resource has a status.conditions[] list that contains the mentioned types, together with a status (True/False) and a descriptive message/explanation of the status.\nMost extension controllers are deploying components and resources as part of their reconciliation flows into the seed or shoot cluster. A prominent example for this is the ControlPlane controller that usually deploys a cloud-controller-manager or CSI controllers as part of the shoot control plane. Now that the extensions deploy resources into the cluster, especially resources that are essential for the functionality of the cluster, they might want to contribute to Gardener\u0026rsquo;s checks mentioned above.\nWhat can extensions do to contribute to Gardener\u0026rsquo;s health checks? Every extension resource in Gardener\u0026rsquo;s extensions.gardener.cloud/v1alpha1 API group also has a status.conditions[] list (like the Shoot). Extension controllers can write conditions to the resource they are acting on and use a type that also exist in the shoot\u0026rsquo;s conditions. One exception is that APIServerAvailable can\u0026rsquo;t be used as the Gardener clearly can identify the status of this condition and it doesn\u0026rsquo;t make sense for extensions to try to contribute/modify it.\nAs an example for the ControlPlane controller let\u0026rsquo;s take a look at the following resource:\napiVersion:extensions.gardener.cloud/v1alpha1kind:ControlPlanemetadata:name:control-planenamespace:shoot--foo--barspec:...status:conditions:- type:ControlPlaneHealthystatus:\u0026#34;False\u0026#34;reason:DeploymentUnhealthymessage: \u0026#39;Deployment cloud-controller-manager is unhealthy:condition\u0026#34;Available\u0026#34;hasinvalid status False (expected True) due to MinimumReplicasUnavailable:Deploymentdoesnothaveminimumavailability.\u0026#39;lastUpdateTime:\u0026#34;2014-05-25T12:44:27Z\u0026#34;- type:ConfigComputedSuccessfullystatus:\u0026#34;True\u0026#34;reason:ConfigCreatedmessage:Thecloud-provider-confighasbeensuccessfullycomputed.lastUpdateTime:\u0026#34;2014-05-25T12:43:27Z\u0026#34;The extension controller has declared in its extension resource that one of the deployments it is responsible for is unhealthy. Also, it has written a second condition using a type that is unknown by Gardener.\nGardener will pick the list of conditions and recognize that the there is one with a type ControlPlaneHealthy. It will merge it with its own ControlPlaneHealthy condition and report it back to the Shoot's status:\napiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:labels:shoot.gardener.cloud/status:unhealthyname:some-shootnamespace:garden-corespec:status:conditions:- type:APIServerAvailablestatus:\u0026#34;True\u0026#34;reason:HealthzRequestSucceededmessage:APIserver/healthzendpointrespondedwithsuccessstatuscode.[response_time:31ms]lastUpdateTime:\u0026#34;2014-05-23T08:26:52Z\u0026#34;lastTransitionTime:\u0026#34;2014-05-25T12:45:13Z\u0026#34;- type:ControlPlaneHealthystatus:\u0026#34;False\u0026#34;reason:ControlPlaneUnhealthyReportmessage: \u0026#39;Deployment cloud-controller-manager is unhealthy:condition\u0026#34;Available\u0026#34;hasinvalid status False (expected True) due to MinimumReplicasUnavailable:Deploymentdoesnothaveminimumavailability.\u0026#39;lastUpdateTime:\u0026#34;2014-05-25T12:45:13Z\u0026#34;lastTransitionTime:\u0026#34;2014-05-25T12:45:13Z\u0026#34;...Hence, the only duty extensions have is to maintain the health status of their components in the extension resource they are managing. This can be accomplished using the health check library for extensions.\nError Codes The Gardener API includes some well-defined error codes, e.g., ERR_INFRA_UNAUTHORIZED, ERR_INFRA_DEPENDENCIES, etc. Extension may set these error codes in the .status.conditions[].codes[] list in case it makes sense. Gardener will pick them up and will similarly merge them into the .status.conditions[].codes[] list in the Shoot:\nstatus:conditions:- type:ControlPlaneHealthystatus:\u0026#34;False\u0026#34;reason:DeploymentUnhealthymessage: \u0026#39;Deployment cloud-controller-manager is unhealthy:condition\u0026#34;Available\u0026#34;hasinvalid status False (expected True) due to MinimumReplicasUnavailable:Deploymentdoesnothaveminimumavailability.\u0026#39;lastUpdateTime:\u0026#34;2014-05-25T12:44:27Z\u0026#34;codes:- ERR_INFRA_UNAUTHORIZED"},{"uri":"https://gardener.cloud/documentation/contribute/10_code/10-contribution_guide/","title":"Contribution Guide","tags":[],"description":"","content":"Contributing to Gardener Code of conduct All members of the Gardener community must abide by the CNCF Code of Conduct. Only by respecting each other can we develop a productive, collaborative community. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting gardener.opensource@sap.com and/or a Gardener project maintainer.\nContributing Gardener uses GitHub to manage reviews of pull requests.\n  If you are a new contributor see: Steps to Contribute\n  If you have a trivial fix or improvement, go ahead and create a pull request, addressing (with @...) a suitable maintainer of this repository (see CODEOWNERS of the repository you want to contribute to) in the description of the pull request.\n  If you plan to do something more involved, first discuss your ideas on our mailing list. This will avoid unnecessary work and surely give you and us a good deal of inspiration.\n  Relevant coding style guidelines are the Go Code Review Comments and the Formatting and style section of Peter Bourgon\u0026rsquo;s Go: Best Practices for Production Environments.\n  Steps to Contribute Should you wish to work on an issue, please claim it first by commenting on the GitHub issue that you want to work on it. This is to prevent duplicated efforts from contributors on the same issue.\nIf you have questions about one of the issues, with or without the tag, please comment on them and one of the maintainers will clarify it.\nWe kindly ask you to follow the Pull Request Checklist to ensure reviews can happen accordingly.\nContributing Code You are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions:\n Contributions must be licensed under the Apache 2.0 License You need to sign the Developer Certificate of Origin.  Contributing Documentation You are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions:\n Contributions must be licensed under the Creative Commons Attribution 4.0 International License You need to sign the Developer Certificate of Origin.  Developer Certificate of Origin Due to legal reasons, contributors will be asked to accept a Developer Certificate of Origin (DCO) before they submit the first pull request to this projects, this happens in an automated fashion during the submission process. We use the standard DCO text of the Linux Foundation.\nPull Request Checklist   Branch from the master branch and, if needed, rebase to the current master branch before submitting your pull request. If it doesn\u0026rsquo;t merge cleanly with master you may be asked to rebase your changes.\n  Commits should be as small as possible, while ensuring that each commit is correct independently (i.e., each commit should compile and pass tests).\n  Test your changes as thoroughly as possible before your commit them. Preferably, automate your test by unit / integration (e.g. Gardener integration testing) tests. If tested manually, provide information about the test scope in the PR description (e.g. Test passed: Upgrade K8s version from 1.14.5 to 1.15.2 on AWS, Azure, GCP, Alicloud, Openstack.).\n  Create Work In Progress [WIP] pull requests only if you need a clarification or an explicit review before you can continue your work item.\n  If your patch is not getting reviewed or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment, or you can ask for a review on our mailing list.\n  Post review:\n If a review requires you to change your commit(s), please test the changes again. Amend the affected commit(s) and force push onto your branch. Set respective comments in your GitHub review to resolved. Create a general PR comment to notify the reviewers that your amendments are ready for another round of review.    Issues and Planning We use GitHub issues to track bugs and enhancement requests. Please provide as much context as possible when you open an issue. The information you provide must be comprehensive enough to reproduce that issue for the assignee. Therefore, contributors may use but aren\u0026rsquo;t restricted to the issue template provided by the Gardener maintainers.\nZenHub is used for planning:\n Install the ZenHub Chrome plugin Login to ZenHub Open the Gardener ZenHub workspace  Security Release Process See Security Release Process\nCommunity Slack Channel #gardener, sign up here\nMailing List gardener@googlegroups.com\nThe mailing list is hosted through Google Groups. To receive the lists\u0026rsquo; emails, join the group, as you would any other Google Group.\nTwitter Follow @GardenerProject on Twitter. Please mention @GardenerProject in your own posts about Gardener.\nAccessing community documents In order to foster real time collaboration there are working documents and notes that are taken in Google Docs, and then transferred to this repository if appropriate.\nTo gain edit access for these documents, you must subscribe to the gardener mailing list, as these documents are shared automatically with anyone who subscribes to that list.\nWeekly Meeting We have a PUBLIC and RECORDED weekly meeting. We meet every Friday at 10:00 CET over Zoom. Find recordings in the Gardener Youtube channel. Let us know if you want to participate and live in a timezone where 10:00 CET is in the night, we can also schedule meetings on Thursday 17:00 CET.\nSee the meeting calendar on the web at calendar.google.com, or paste this iCal url into any iCal client.\nIf you have a topic you\u0026rsquo;d like to present or would like to see discussed, please propose a specific date on the Gardener Community Meeting Agenda. Find minutes in the same document. Please upload slides or other documents you presented to the Gardener Community Meeting folder. Subscribe to the gardener mailing list to get edit permissions.\n"},{"uri":"https://gardener.cloud/v1.12.8/contribute/10_code/10-contribution_guide/","title":"Contribution Guide","tags":[],"description":"","content":"Contributing to Gardener Code of conduct All members of the Gardener community must abide by the CNCF Code of Conduct. Only by respecting each other can we develop a productive, collaborative community. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting gardener.opensource@sap.com and/or a Gardener project maintainer.\nContributing Gardener uses GitHub to manage reviews of pull requests.\n  If you are a new contributor see: Steps to Contribute\n  If you have a trivial fix or improvement, go ahead and create a pull request, addressing (with @...) a suitable maintainer of this repository (see CODEOWNERS of the repository you want to contribute to) in the description of the pull request.\n  If you plan to do something more involved, first discuss your ideas on our mailing list. This will avoid unnecessary work and surely give you and us a good deal of inspiration.\n  Relevant coding style guidelines are the Go Code Review Comments and the Formatting and style section of Peter Bourgon\u0026rsquo;s Go: Best Practices for Production Environments.\n  Steps to Contribute Should you wish to work on an issue, please claim it first by commenting on the GitHub issue that you want to work on it. This is to prevent duplicated efforts from contributors on the same issue.\nIf you have questions about one of the issues, with or without the tag, please comment on them and one of the maintainers will clarify it.\nWe kindly ask you to follow the Pull Request Checklist to ensure reviews can happen accordingly.\nContributing Code You are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions:\n Contributions must be licensed under the Apache 2.0 License You need to sign the Developer Certificate of Origin.  Contributing Documentation You are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions:\n Contributions must be licensed under the Creative Commons Attribution 4.0 International License You need to sign the Developer Certificate of Origin.  Developer Certificate of Origin Due to legal reasons, contributors will be asked to accept a Developer Certificate of Origin (DCO) before they submit the first pull request to this projects, this happens in an automated fashion during the submission process. We use the standard DCO text of the Linux Foundation.\nPull Request Checklist   Branch from the master branch and, if needed, rebase to the current master branch before submitting your pull request. If it doesn\u0026rsquo;t merge cleanly with master you may be asked to rebase your changes.\n  Commits should be as small as possible, while ensuring that each commit is correct independently (i.e., each commit should compile and pass tests).\n  Test your changes as thoroughly as possible before your commit them. Preferably, automate your test by unit / integration (e.g. Gardener integration testing) tests. If tested manually, provide information about the test scope in the PR description (e.g. Test passed: Upgrade K8s version from 1.14.5 to 1.15.2 on AWS, Azure, GCP, Alicloud, Openstack.).\n  Create Work In Progress [WIP] pull requests only if you need a clarification or an explicit review before you can continue your work item.\n  If your patch is not getting reviewed or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment, or you can ask for a review on our mailing list.\n  Post review:\n If a review requires you to change your commit(s), please test the changes again. Amend the affected commit(s) and force push onto your branch. Set respective comments in your GitHub review to resolved. Create a general PR comment to notify the reviewers that your amendments are ready for another round of review.    Issues and Planning We use GitHub issues to track bugs and enhancement requests. Please provide as much context as possible when you open an issue. The information you provide must be comprehensive enough to reproduce that issue for the assignee. Therefore, contributors may use but aren\u0026rsquo;t restricted to the issue template provided by the Gardener maintainers.\nZenHub is used for planning:\n Install the ZenHub Chrome plugin Login to ZenHub Open the Gardener ZenHub workspace  Security Release Process See Security Release Process\nCommunity Slack Channel #gardener, sign up here\nMailing List gardener@googlegroups.com\nThe mailing list is hosted through Google Groups. To receive the lists\u0026rsquo; emails, join the group, as you would any other Google Group.\nTwitter Follow @GardenerProject on Twitter. Please mention @GardenerProject in your own posts about Gardener.\nAccessing community documents In order to foster real time collaboration there are working documents and notes that are taken in Google Docs, and then transferred to this repository if appropriate.\nTo gain edit access for these documents, you must subscribe to the gardener mailing list, as these documents are shared automatically with anyone who subscribes to that list.\nWeekly Meeting We have a PUBLIC and RECORDED weekly meeting. We meet every Friday at 10:00 CET over Zoom. Find recordings in the Gardener Youtube channel. Let us know if you want to participate and live in a timezone where 10:00 CET is in the night, we can also schedule meetings on Thursday 17:00 CET.\nSee the meeting calendar on the web at calendar.google.com, or paste this iCal url into any iCal client.\nIf you have a topic you\u0026rsquo;d like to present or would like to see discussed, please propose a specific date on the Gardener Community Meeting Agenda. Find minutes in the same document. Please upload slides or other documents you presented to the Gardener Community Meeting folder. Subscribe to the gardener mailing list to get edit permissions.\n"},{"uri":"https://gardener.cloud/v1.13.2/contribute/10_code/10-contribution_guide/","title":"Contribution Guide","tags":[],"description":"","content":"Contributing to Gardener Code of conduct All members of the Gardener community must abide by the CNCF Code of Conduct. Only by respecting each other can we develop a productive, collaborative community. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting gardener.opensource@sap.com and/or a Gardener project maintainer.\nContributing Gardener uses GitHub to manage reviews of pull requests.\n  If you are a new contributor see: Steps to Contribute\n  If you have a trivial fix or improvement, go ahead and create a pull request, addressing (with @...) a suitable maintainer of this repository (see CODEOWNERS of the repository you want to contribute to) in the description of the pull request.\n  If you plan to do something more involved, first discuss your ideas on our mailing list. This will avoid unnecessary work and surely give you and us a good deal of inspiration.\n  Relevant coding style guidelines are the Go Code Review Comments and the Formatting and style section of Peter Bourgon\u0026rsquo;s Go: Best Practices for Production Environments.\n  Steps to Contribute Should you wish to work on an issue, please claim it first by commenting on the GitHub issue that you want to work on it. This is to prevent duplicated efforts from contributors on the same issue.\nIf you have questions about one of the issues, with or without the tag, please comment on them and one of the maintainers will clarify it.\nWe kindly ask you to follow the Pull Request Checklist to ensure reviews can happen accordingly.\nContributing Code You are welcome to contribute code to Gardener in order to fix a bug or to implement a new feature.\nThe following rules govern code contributions:\n Contributions must be licensed under the Apache 2.0 License You need to sign the Developer Certificate of Origin.  Contributing Documentation You are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions:\n Contributions must be licensed under the Creative Commons Attribution 4.0 International License You need to sign the Developer Certificate of Origin.  Developer Certificate of Origin Due to legal reasons, contributors will be asked to accept a Developer Certificate of Origin (DCO) before they submit the first pull request to this projects, this happens in an automated fashion during the submission process. We use the standard DCO text of the Linux Foundation.\nPull Request Checklist   Branch from the master branch and, if needed, rebase to the current master branch before submitting your pull request. If it doesn\u0026rsquo;t merge cleanly with master you may be asked to rebase your changes.\n  Commits should be as small as possible, while ensuring that each commit is correct independently (i.e., each commit should compile and pass tests).\n  Test your changes as thoroughly as possible before your commit them. Preferably, automate your test by unit / integration (e.g. Gardener integration testing) tests. If tested manually, provide information about the test scope in the PR description (e.g. Test passed: Upgrade K8s version from 1.14.5 to 1.15.2 on AWS, Azure, GCP, Alicloud, Openstack.).\n  Create Work In Progress [WIP] pull requests only if you need a clarification or an explicit review before you can continue your work item.\n  If your patch is not getting reviewed or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment, or you can ask for a review on our mailing list.\n  Post review:\n If a review requires you to change your commit(s), please test the changes again. Amend the affected commit(s) and force push onto your branch. Set respective comments in your GitHub review to resolved. Create a general PR comment to notify the reviewers that your amendments are ready for another round of review.    Issues and Planning We use GitHub issues to track bugs and enhancement requests. Please provide as much context as possible when you open an issue. The information you provide must be comprehensive enough to reproduce that issue for the assignee. Therefore, contributors may use but aren\u0026rsquo;t restricted to the issue template provided by the Gardener maintainers.\nZenHub is used for planning:\n Install the ZenHub Chrome plugin Login to ZenHub Open the Gardener ZenHub workspace  Security Release Process See Security Release Process\nCommunity Slack Channel #gardener, sign up here\nMailing List gardener@googlegroups.com\nThe mailing list is hosted through Google Groups. To receive the lists\u0026rsquo; emails, join the group, as you would any other Google Group.\nTwitter Follow @GardenerProject on Twitter. Please mention @GardenerProject in your own posts about Gardener.\nAccessing community documents In order to foster real time collaboration there are working documents and notes that are taken in Google Docs, and then transferred to this repository if appropriate.\nTo gain edit access for these documents, you must subscribe to the gardener mailing list, as these documents are shared automatically with anyone who subscribes to that list.\nWeekly Meeting We have a PUBLIC and RECORDED weekly meeting. We meet every Friday at 10:00 CET over Zoom. Find recordings in the Gardener Youtube channel. Let us know if you want to participate and live in a timezone where 10:00 CET is in the night, we can also schedule meetings on Thursday 17:00 CET.\nSee the meeting calendar on the web at calendar.google.com, or paste this iCal url into any iCal client.\nIf you have a topic you\u0026rsquo;d like to present or would like to see discussed, please propose a specific date on the Gardener Community Meeting Agenda. Find minutes in the same document. Please upload slides or other documents you presented to the Gardener Community Meeting folder. Subscribe to the gardener mailing list to get edit permissions.\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/controlplane-webhooks/","title":"Controlplane customization webhooks","tags":[],"description":"","content":"Controlplane customization webhooks Gardener creates the Shoot controlplane in several steps of the Shoot flow. At different point of this flow, it:\n deploys standard controlplane components such as kube-apiserver, kube-controller-manager, and kube-scheduler by creating the corresponding deployments, services, and other resources in the Shoot namespace. initiates the deployment of custom controlplane components by ControlPlane controllers by creating a ControlPlane resource in the Shoot namespace.  In order to apply any provider-specific changes to the configuration provided by Gardener for the standard controlplane components, cloud extension providers can install mutating admission webhooks for the resources created by Gardener in the Shoot namespace.\nWhat needs to be implemented to support a new cloud provider? In order to support a new cloud provider you should install \u0026ldquo;controlplane\u0026rdquo; mutating webhooks for any of the following resources:\n Deployment with name kube-apiserver, kube-controller-manager, or kube-scheduler StatefulSet with name etcd-main or etcd-events Service with name kube-apiserver OperatingSystemConfig with any name and purpose reconcile  See Contract Specification for more details on the contract that Gardener and webhooks should adhere to regarding the content of the above resources.\nYou can install 3 different kinds of controlplane webhooks:\n Shoot, or controlplane webhooks apply changes needed by the Shoot cloud provider, for example the --cloud-provider command line flag of kube-apiserver and kube-controller-manager. Such webhooks should only operate on Shoot namespaces labeled with shoot.gardener.cloud/provider=\u0026lt;provider\u0026gt;. Seed, or controlplaneexposure webhooks apply changes needed by the Seed cloud provider, for example annotations on the kube-apiserver service to ensure cloud-specific load balancers are correctly provisioned for a service of type LoadBalancer. Such webhooks should only operate on Shoot namespaces labeled with seed.gardener.cloud/provider=\u0026lt;provider\u0026gt;.  The labels shoot.gardener.cloud/provider and shoot.gardener.cloud/provider are added by Gardener when it creates the Shoot namespace.\nContract Specification This section specifies the contract that Gardener and webhooks should adhere to in order to ensure smooth interoperability. Note that this contract can\u0026rsquo;t be specified formally and is therefore easy to violate, especially by Gardener. The Gardener team will nevertheless do its best to adhere to this contract in the future and to ensure via additional measures (tests, validations) that it\u0026rsquo;s not unintentionally broken. If it needs to be changed intentionally, this can only happen after proper communication has taken place to ensure that the affected provider webhooks could be adapted to work with the new version of the contract.\nNote: The contract described below may not necessarily be what Gardener does currently (as of May 2019). Rather, it reflects the target state after changes for Gardener extensibility have been introduced.\nkube-apiserver To deploy kube-apiserver, Gardener shall create a deployment and a service both named kube-apiserver in the Shoot namespace. They can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of the kube-apiserver deployment shall contain a container named kube-apiserver.\nThe command field of the kube-apiserver container shall contain the kube-apiserver command line. It shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n admission plugins (--enable-admission-plugins, --disable-admission-plugins) secure communications (--etcd-cafile, --etcd-certfile, --etcd-keyfile, \u0026hellip;) audit log (--audit-log-*) ports (--insecure-port, --secure-port)  The kube-apiserver command line shall not contain any provider-specific flags, such as:\n --cloud-provider --cloud-config  These flags can be added by webhooks if needed.\nThe kube-apiserver command line may contain a number of additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-apiserver behavior for the specific provider. Among the flags to be considered are:\n --endpoint-reconciler-type --advertise-address --feature-gates  Gardener may use SNI to expose the apiserver (APIServerSNI feature gate). In this case, Gardener shall label the kube-apiserver's Deployment with core.gardener.cloud/apiserver-exposure: gardener-managed label and expects that the --endpoint-reconciler-type and --advertise-address flags are not modified.\nThe --enable-admission-plugins flag may contain admission plugins that are not compatible with CSI plugins such as PersistentVolumeLabel. Webhooks should therefore ensure that such admission plugins are either explicitly enabled (if CSI plugins are not used) or disabled (otherwise).\nThe env field of the kube-apiserver container shall not contain any provider-specific environment variables (so it will be empty). If any provider-specific environment variables are needed, they should be added by webhooks.\nThe volumes field of the pod template of the kube-apiserver deployment, and respectively the volumeMounts field of the kube-apiserver container shall not contain any provider-specific Secret or ConfigMap resources. If such resources should be mounted as volumes, this should be done by webhooks.\nThe kube-apiserver Service may be of type LoadBalancer, but shall not contain any provider-specific annotations that may be needed to actually provision a load balancer resource in the Seed provider\u0026rsquo;s cloud. If any such annotations are needed, they should be added by webhooks (typically controlplaneexposure webhooks).\nThe kube-apiserver Service shall be of type ClusterIP, if Gardener is using SNI to expose the apiserver (APIServerSNI feature gate). In this case, Gardener shall label this Service with core.gardener.cloud/apiserver-exposure: gardener-managed label and expects that no mutations happen.\nkube-controller-manager To deploy kube-controller-manager, Gardener shall create a deployment named kube-controller-manager in the Shoot namespace. It can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of the kube-controller-manager deployment shall contain a container named kube-controller-manager.\nThe command field of the kube-controller-manager container shall contain the kube-controller-manager command line. It shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n --kubeconfig, --authentication-kubeconfig, --authorization-kubeconfig --leader-elect secure communications (--tls-cert-file, --tls-private-key-file, \u0026hellip;) cluster CIDR and identity (--cluster-cidr, --cluster-name) sync settings (--concurrent-deployment-syncs, --concurrent-replicaset-syncs) horizontal pod autoscaler (--horizontal-pod-autoscaler-*) ports (--port, --secure-port)  The kube-controller-manager command line shall not contain any provider-specific flags, such as:\n --cloud-provider --cloud-config --configure-cloud-routes --external-cloud-volume-plugin  These flags can be added by webhooks if needed.\nThe kube-controller-manager command line may contain a number of additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-controller-manager behavior for the specific provider. Among the flags to be considered are:\n --feature-gates  The env field of the kube-controller-manager container shall not contain any provider-specific environment variables (so it will be empty). If any provider-specific environment variables are needed, they should be added by webhooks.\nThe volumes field of the pod template of the kube-controller-manager deployment, and respectively the volumeMounts field of the kube-controller-manager container shall not contain any provider-specific Secret or ConfigMap resources. If such resources should be mounted as volumes, this should be done by webhooks.\nkube-scheduler To deploy kube-scheduler, Gardener shall create a deployment named kube-scheduler in the Shoot namespace. It can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of the kube-scheduler deployment shall contain a container named kube-scheduler.\nThe command field of the kube-scheduler container shall contain the kube-scheduler command line. It shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n --config --authentication-kubeconfig, --authorization-kubeconfig secure communications (--tls-cert-file, --tls-private-key-file, \u0026hellip;) ports (--port, --secure-port)  The kube-scheduler command line may contain additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-controller-manager behavior for the specific provider. Among the flags to be considered are:\n --feature-gates  The kube-scheduler command line can\u0026rsquo;t contain provider-specific flags, and it makes no sense to specify provider-specific environment variables or mount provider-specific Secret or ConfigMap resources as volumes.\netcd-main and etcd-events To deploy etcd, Gardener shall create 2 StatefulSets named etcd-main and etcd-events in the Shoot namespace. They can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of these 2 deployments shall contain a container named etcd. It shall not contain a sidecar container for etcd backups. If such a container is needed, it should be added by webhooks, together with any volumes it may need to mount.\nThe command field of the etcd container shall contain the etcd command line. It shall contain only provider-independent flags that should be ignored by webhooks. It can\u0026rsquo;t contain provider-specific flags, and it makes no sense to specify provider-specific environment variables or mount provider-specific Secret or ConfigMap resources as volumes.\nThe volumeClaimTemplates section of these 2 StatefulSets shall contain a template named etcd-main or etcd-events. This template shall use the default storage class. The corresponding claim is mounted into the etcd container at /var/etcd/data. If it is desirable to use a non-default storage class, this should be done by webhooks.\ncloud-controller-manager Gardener shall not deploy a cloud-controller-manager. If it is needed, it should be added by a ControlPlane controller\nCSI controllers Gardener shall not deploy a CSI controller. If it is needed, it should be added by a ControlPlane controller\nkubelet To specify the kubelet configuration, Gardener shall create a OperatingSystemConfig resource with any name and purpose reconcile in the Shoot namespace. It can therefore also be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener. Gardener may write multiple such resources with different type to the same Shoot namespaces if multiple OSs are used.\nThe OSC resource shall contain a unit named kubelet.service, containing the corresponding systemd unit configuration file. The [Service] section of this file shall contain a single ExecStart option having the kubelet command line as its value.\nThe OSC resource shall contain a file with path /var/lib/kubelet/config/kubelet, which contains a KubeletConfiguration resource in YAML format. Most of the flags that can be specified in the kubelet command line can alternatively be specified as options in this configuration as well.\nThe kubelet command line shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n --config --bootstrap-kubeconfig, --kubeconfig --network-plugin (and, if it equals cni, also --cni-bin-dir and --cni-conf-dir) --node-labels  The kubelet command line shall not contain any provider-specific flags, such as:\n --cloud-provider --cloud-config --provider-id  These flags can be added by webhooks if needed.\nThe kubelet command line / configuration may contain a number of additional provider-independent flags / options. In general, webhooks should ignore these unless they are known to interfere with the desired kubelet behavior for the specific provider. Among the flags / options to be considered are:\n --enable-controller-attach-detach (enableControllerAttachDetach) - should be set to true if CSI plugins are used, but in general can also be ignored since its default value is also true, and this should work both with and without CSI plugins. --feature-gates (featureGates) - should contain a list of specific feature gates if CSI plugins are used. If CSI plugins are not used, the corresponding feature gates can be ignored since enabling them should not harm in any way.  "},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/controlplane-webhooks/","title":"Controlplane customization webhooks","tags":[],"description":"","content":"Controlplane customization webhooks Gardener creates the Shoot controlplane in several steps of the Shoot flow. At different point of this flow, it:\n deploys standard controlplane components such as kube-apiserver, kube-controller-manager, and kube-scheduler by creating the corresponding deployments, services, and other resources in the Shoot namespace. initiates the deployment of custom controlplane components by ControlPlane controllers by creating a ControlPlane resource in the Shoot namespace.  In order to apply any provider-specific changes to the configuration provided by Gardener for the standard controlplane components, cloud extension providers can install mutating admission webhooks for the resources created by Gardener in the Shoot namespace.\nWhat needs to be implemented to support a new cloud provider? In order to support a new cloud provider you should install \u0026ldquo;controlplane\u0026rdquo; mutating webhooks for any of the following resources:\n Deployment with name kube-apiserver, kube-controller-manager, or kube-scheduler StatefulSet with name etcd-main or etcd-events Service with name kube-apiserver OperatingSystemConfig with any name and purpose reconcile  See Contract Specification for more details on the contract that Gardener and webhooks should adhere to regarding the content of the above resources.\nYou can install 3 different kinds of controlplane webhooks:\n Shoot, or controlplane webhooks apply changes needed by the Shoot cloud provider, for example the --cloud-provider command line flag of kube-apiserver and kube-controller-manager. Such webhooks should only operate on Shoot namespaces labeled with shoot.gardener.cloud/provider=\u0026lt;provider\u0026gt;. Seed, or controlplaneexposure webhooks apply changes needed by the Seed cloud provider, for example annotations on the kube-apiserver service to ensure cloud-specific load balancers are correctly provisioned for a service of type LoadBalancer. Such webhooks should only operate on Shoot namespaces labeled with seed.gardener.cloud/provider=\u0026lt;provider\u0026gt;.  The labels shoot.gardener.cloud/provider and shoot.gardener.cloud/provider are added by Gardener when it creates the Shoot namespace.\nContract Specification This section specifies the contract that Gardener and webhooks should adhere to in order to ensure smooth interoperability. Note that this contract can\u0026rsquo;t be specified formally and is therefore easy to violate, especially by Gardener. The Gardener team will nevertheless do its best to adhere to this contract in the future and to ensure via additional measures (tests, validations) that it\u0026rsquo;s not unintentionally broken. If it needs to be changed intentionally, this can only happen after proper communication has taken place to ensure that the affected provider webhooks could be adapted to work with the new version of the contract.\nNote: The contract described below may not necessarily be what Gardener does currently (as of May 2019). Rather, it reflects the target state after changes for Gardener extensibility have been introduced.\nkube-apiserver To deploy kube-apiserver, Gardener shall create a deployment and a service both named kube-apiserver in the Shoot namespace. They can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of the kube-apiserver deployment shall contain a container named kube-apiserver.\nThe command field of the kube-apiserver container shall contain the kube-apiserver command line. It shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n admission plugins (--enable-admission-plugins, --disable-admission-plugins) secure communications (--etcd-cafile, --etcd-certfile, --etcd-keyfile, \u0026hellip;) audit log (--audit-log-*) ports (--insecure-port, --secure-port)  The kube-apiserver command line shall not contain any provider-specific flags, such as:\n --cloud-provider --cloud-config  These flags can be added by webhooks if needed.\nThe kube-apiserver command line may contain a number of additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-apiserver behavior for the specific provider. Among the flags to be considered are:\n --endpoint-reconciler-type --advertise-address --feature-gates  Gardener may use SNI to expose the apiserver (APIServerSNI feature gate). In this case, Gardener shall label the kube-apiserver's Deployment with core.gardener.cloud/apiserver-exposure: gardener-managed label and expects that the --endpoint-reconciler-type and --advertise-address flags are not modified.\nThe --enable-admission-plugins flag may contain admission plugins that are not compatible with CSI plugins such as PersistentVolumeLabel. Webhooks should therefore ensure that such admission plugins are either explicitly enabled (if CSI plugins are not used) or disabled (otherwise).\nThe env field of the kube-apiserver container shall not contain any provider-specific environment variables (so it will be empty). If any provider-specific environment variables are needed, they should be added by webhooks.\nThe volumes field of the pod template of the kube-apiserver deployment, and respectively the volumeMounts field of the kube-apiserver container shall not contain any provider-specific Secret or ConfigMap resources. If such resources should be mounted as volumes, this should be done by webhooks.\nThe kube-apiserver Service may be of type LoadBalancer, but shall not contain any provider-specific annotations that may be needed to actually provision a load balancer resource in the Seed provider\u0026rsquo;s cloud. If any such annotations are needed, they should be added by webhooks (typically controlplaneexposure webhooks).\nThe kube-apiserver Service shall be of type ClusterIP, if Gardener is using SNI to expose the apiserver (APIServerSNI feature gate). In this case, Gardener shall label this Service with core.gardener.cloud/apiserver-exposure: gardener-managed label and expects that no mutations happen.\nkube-controller-manager To deploy kube-controller-manager, Gardener shall create a deployment named kube-controller-manager in the Shoot namespace. It can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of the kube-controller-manager deployment shall contain a container named kube-controller-manager.\nThe command field of the kube-controller-manager container shall contain the kube-controller-manager command line. It shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n --kubeconfig, --authentication-kubeconfig, --authorization-kubeconfig --leader-elect secure communications (--tls-cert-file, --tls-private-key-file, \u0026hellip;) cluster CIDR and identity (--cluster-cidr, --cluster-name) sync settings (--concurrent-deployment-syncs, --concurrent-replicaset-syncs) horizontal pod autoscaler (--horizontal-pod-autoscaler-*) ports (--port, --secure-port)  The kube-controller-manager command line shall not contain any provider-specific flags, such as:\n --cloud-provider --cloud-config --configure-cloud-routes --external-cloud-volume-plugin  These flags can be added by webhooks if needed.\nThe kube-controller-manager command line may contain a number of additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-controller-manager behavior for the specific provider. Among the flags to be considered are:\n --feature-gates  The env field of the kube-controller-manager container shall not contain any provider-specific environment variables (so it will be empty). If any provider-specific environment variables are needed, they should be added by webhooks.\nThe volumes field of the pod template of the kube-controller-manager deployment, and respectively the volumeMounts field of the kube-controller-manager container shall not contain any provider-specific Secret or ConfigMap resources. If such resources should be mounted as volumes, this should be done by webhooks.\nkube-scheduler To deploy kube-scheduler, Gardener shall create a deployment named kube-scheduler in the Shoot namespace. It can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of the kube-scheduler deployment shall contain a container named kube-scheduler.\nThe command field of the kube-scheduler container shall contain the kube-scheduler command line. It shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n --config --authentication-kubeconfig, --authorization-kubeconfig secure communications (--tls-cert-file, --tls-private-key-file, \u0026hellip;) ports (--port, --secure-port)  The kube-scheduler command line may contain additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-controller-manager behavior for the specific provider. Among the flags to be considered are:\n --feature-gates  The kube-scheduler command line can\u0026rsquo;t contain provider-specific flags, and it makes no sense to specify provider-specific environment variables or mount provider-specific Secret or ConfigMap resources as volumes.\netcd-main and etcd-events To deploy etcd, Gardener shall create 2 StatefulSets named etcd-main and etcd-events in the Shoot namespace. They can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of these 2 deployments shall contain a container named etcd. It shall not contain a sidecar container for etcd backups. If such a container is needed, it should be added by webhooks, together with any volumes it may need to mount.\nThe command field of the etcd container shall contain the etcd command line. It shall contain only provider-independent flags that should be ignored by webhooks. It can\u0026rsquo;t contain provider-specific flags, and it makes no sense to specify provider-specific environment variables or mount provider-specific Secret or ConfigMap resources as volumes.\nThe volumeClaimTemplates section of these 2 StatefulSets shall contain a template named etcd-main or etcd-events. This template shall use the default storage class. The corresponding claim is mounted into the etcd container at /var/etcd/data. If it is desirable to use a non-default storage class, this should be done by webhooks.\ncloud-controller-manager Gardener shall not deploy a cloud-controller-manager. If it is needed, it should be added by a ControlPlane controller\nCSI controllers Gardener shall not deploy a CSI controller. If it is needed, it should be added by a ControlPlane controller\nkubelet To specify the kubelet configuration, Gardener shall create a OperatingSystemConfig resource with any name and purpose reconcile in the Shoot namespace. It can therefore also be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener. Gardener may write multiple such resources with different type to the same Shoot namespaces if multiple OSs are used.\nThe OSC resource shall contain a unit named kubelet.service, containing the corresponding systemd unit configuration file. The [Service] section of this file shall contain a single ExecStart option having the kubelet command line as its value.\nThe OSC resource shall contain a file with path /var/lib/kubelet/config/kubelet, which contains a KubeletConfiguration resource in YAML format. Most of the flags that can be specified in the kubelet command line can alternatively be specified as options in this configuration as well.\nThe kubelet command line shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n --config --bootstrap-kubeconfig, --kubeconfig --network-plugin (and, if it equals cni, also --cni-bin-dir and --cni-conf-dir) --node-labels  The kubelet command line shall not contain any provider-specific flags, such as:\n --cloud-provider --cloud-config --provider-id  These flags can be added by webhooks if needed.\nThe kubelet command line / configuration may contain a number of additional provider-independent flags / options. In general, webhooks should ignore these unless they are known to interfere with the desired kubelet behavior for the specific provider. Among the flags / options to be considered are:\n --enable-controller-attach-detach (enableControllerAttachDetach) - should be set to true if CSI plugins are used, but in general can also be ignored since its default value is also true, and this should work both with and without CSI plugins. --feature-gates (featureGates) - should contain a list of specific feature gates if CSI plugins are used. If CSI plugins are not used, the corresponding feature gates can be ignored since enabling them should not harm in any way.  "},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/controlplane-webhooks/","title":"Controlplane customization webhooks","tags":[],"description":"","content":"Controlplane customization webhooks Gardener creates the Shoot controlplane in several steps of the Shoot flow. At different point of this flow, it:\n deploys standard controlplane components such as kube-apiserver, kube-controller-manager, and kube-scheduler by creating the corresponding deployments, services, and other resources in the Shoot namespace. initiates the deployment of custom controlplane components by ControlPlane controllers by creating a ControlPlane resource in the Shoot namespace.  In order to apply any provider-specific changes to the configuration provided by Gardener for the standard controlplane components, cloud extension providers can install mutating admission webhooks for the resources created by Gardener in the Shoot namespace.\nWhat needs to be implemented to support a new cloud provider? In order to support a new cloud provider you should install \u0026ldquo;controlplane\u0026rdquo; mutating webhooks for any of the following resources:\n Deployment with name kube-apiserver, kube-controller-manager, or kube-scheduler StatefulSet with name etcd-main or etcd-events Service with name kube-apiserver OperatingSystemConfig with any name and purpose reconcile  See Contract Specification for more details on the contract that Gardener and webhooks should adhere to regarding the content of the above resources.\nYou can install 3 different kinds of controlplane webhooks:\n Shoot, or controlplane webhooks apply changes needed by the Shoot cloud provider, for example the --cloud-provider command line flag of kube-apiserver and kube-controller-manager. Such webhooks should only operate on Shoot namespaces labeled with shoot.gardener.cloud/provider=\u0026lt;provider\u0026gt;. Seed, or controlplaneexposure webhooks apply changes needed by the Seed cloud provider, for example annotations on the kube-apiserver service to ensure cloud-specific load balancers are correctly provisioned for a service of type LoadBalancer. Such webhooks should only operate on Shoot namespaces labeled with seed.gardener.cloud/provider=\u0026lt;provider\u0026gt;.  The labels shoot.gardener.cloud/provider and shoot.gardener.cloud/provider are added by Gardener when it creates the Shoot namespace.\nContract Specification This section specifies the contract that Gardener and webhooks should adhere to in order to ensure smooth interoperability. Note that this contract can\u0026rsquo;t be specified formally and is therefore easy to violate, especially by Gardener. The Gardener team will nevertheless do its best to adhere to this contract in the future and to ensure via additional measures (tests, validations) that it\u0026rsquo;s not unintentionally broken. If it needs to be changed intentionally, this can only happen after proper communication has taken place to ensure that the affected provider webhooks could be adapted to work with the new version of the contract.\nNote: The contract described below may not necessarily be what Gardener does currently (as of May 2019). Rather, it reflects the target state after changes for Gardener extensibility have been introduced.\nkube-apiserver To deploy kube-apiserver, Gardener shall create a deployment and a service both named kube-apiserver in the Shoot namespace. They can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of the kube-apiserver deployment shall contain a container named kube-apiserver.\nThe command field of the kube-apiserver container shall contain the kube-apiserver command line. It shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n admission plugins (--enable-admission-plugins, --disable-admission-plugins) secure communications (--etcd-cafile, --etcd-certfile, --etcd-keyfile, \u0026hellip;) audit log (--audit-log-*) ports (--insecure-port, --secure-port)  The kube-apiserver command line shall not contain any provider-specific flags, such as:\n --cloud-provider --cloud-config  These flags can be added by webhooks if needed.\nThe kube-apiserver command line may contain a number of additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-apiserver behavior for the specific provider. Among the flags to be considered are:\n --endpoint-reconciler-type --advertise-address --feature-gates  Gardener may use SNI to expose the apiserver (APIServerSNI feature gate). In this case, Gardener shall label the kube-apiserver's Deployment with core.gardener.cloud/apiserver-exposure: gardener-managed label and expects that the --endpoint-reconciler-type and --advertise-address flags are not modified.\nThe --enable-admission-plugins flag may contain admission plugins that are not compatible with CSI plugins such as PersistentVolumeLabel. Webhooks should therefore ensure that such admission plugins are either explicitly enabled (if CSI plugins are not used) or disabled (otherwise).\nThe env field of the kube-apiserver container shall not contain any provider-specific environment variables (so it will be empty). If any provider-specific environment variables are needed, they should be added by webhooks.\nThe volumes field of the pod template of the kube-apiserver deployment, and respectively the volumeMounts field of the kube-apiserver container shall not contain any provider-specific Secret or ConfigMap resources. If such resources should be mounted as volumes, this should be done by webhooks.\nThe kube-apiserver Service may be of type LoadBalancer, but shall not contain any provider-specific annotations that may be needed to actually provision a load balancer resource in the Seed provider\u0026rsquo;s cloud. If any such annotations are needed, they should be added by webhooks (typically controlplaneexposure webhooks).\nThe kube-apiserver Service shall be of type ClusterIP, if Gardener is using SNI to expose the apiserver (APIServerSNI feature gate). In this case, Gardener shall label this Service with core.gardener.cloud/apiserver-exposure: gardener-managed label and expects that no mutations happen.\nkube-controller-manager To deploy kube-controller-manager, Gardener shall create a deployment named kube-controller-manager in the Shoot namespace. It can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of the kube-controller-manager deployment shall contain a container named kube-controller-manager.\nThe command field of the kube-controller-manager container shall contain the kube-controller-manager command line. It shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n --kubeconfig, --authentication-kubeconfig, --authorization-kubeconfig --leader-elect secure communications (--tls-cert-file, --tls-private-key-file, \u0026hellip;) cluster CIDR and identity (--cluster-cidr, --cluster-name) sync settings (--concurrent-deployment-syncs, --concurrent-replicaset-syncs) horizontal pod autoscaler (--horizontal-pod-autoscaler-*) ports (--port, --secure-port)  The kube-controller-manager command line shall not contain any provider-specific flags, such as:\n --cloud-provider --cloud-config --configure-cloud-routes --external-cloud-volume-plugin  These flags can be added by webhooks if needed.\nThe kube-controller-manager command line may contain a number of additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-controller-manager behavior for the specific provider. Among the flags to be considered are:\n --feature-gates  The env field of the kube-controller-manager container shall not contain any provider-specific environment variables (so it will be empty). If any provider-specific environment variables are needed, they should be added by webhooks.\nThe volumes field of the pod template of the kube-controller-manager deployment, and respectively the volumeMounts field of the kube-controller-manager container shall not contain any provider-specific Secret or ConfigMap resources. If such resources should be mounted as volumes, this should be done by webhooks.\nkube-scheduler To deploy kube-scheduler, Gardener shall create a deployment named kube-scheduler in the Shoot namespace. It can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of the kube-scheduler deployment shall contain a container named kube-scheduler.\nThe command field of the kube-scheduler container shall contain the kube-scheduler command line. It shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n --config --authentication-kubeconfig, --authorization-kubeconfig secure communications (--tls-cert-file, --tls-private-key-file, \u0026hellip;) ports (--port, --secure-port)  The kube-scheduler command line may contain additional provider-independent flags. In general, webhooks should ignore these unless they are known to interfere with the desired kube-controller-manager behavior for the specific provider. Among the flags to be considered are:\n --feature-gates  The kube-scheduler command line can\u0026rsquo;t contain provider-specific flags, and it makes no sense to specify provider-specific environment variables or mount provider-specific Secret or ConfigMap resources as volumes.\netcd-main and etcd-events To deploy etcd, Gardener shall create 2 StatefulSets named etcd-main and etcd-events in the Shoot namespace. They can be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener.\nThe pod template of these 2 deployments shall contain a container named etcd. It shall not contain a sidecar container for etcd backups. If such a container is needed, it should be added by webhooks, together with any volumes it may need to mount.\nThe command field of the etcd container shall contain the etcd command line. It shall contain only provider-independent flags that should be ignored by webhooks. It can\u0026rsquo;t contain provider-specific flags, and it makes no sense to specify provider-specific environment variables or mount provider-specific Secret or ConfigMap resources as volumes.\nThe volumeClaimTemplates section of these 2 StatefulSets shall contain a template named etcd-main or etcd-events. This template shall use the default storage class. The corresponding claim is mounted into the etcd container at /var/etcd/data. If it is desirable to use a non-default storage class, this should be done by webhooks.\ncloud-controller-manager Gardener shall not deploy a cloud-controller-manager. If it is needed, it should be added by a ControlPlane controller\nCSI controllers Gardener shall not deploy a CSI controller. If it is needed, it should be added by a ControlPlane controller\nkubelet To specify the kubelet configuration, Gardener shall create a OperatingSystemConfig resource with any name and purpose reconcile in the Shoot namespace. It can therefore also be mutated by webhooks to apply any provider-specific changes to the standard configuration provided by Gardener. Gardener may write multiple such resources with different type to the same Shoot namespaces if multiple OSs are used.\nThe OSC resource shall contain a unit named kubelet.service, containing the corresponding systemd unit configuration file. The [Service] section of this file shall contain a single ExecStart option having the kubelet command line as its value.\nThe OSC resource shall contain a file with path /var/lib/kubelet/config/kubelet, which contains a KubeletConfiguration resource in YAML format. Most of the flags that can be specified in the kubelet command line can alternatively be specified as options in this configuration as well.\nThe kubelet command line shall contain a number of provider-independent flags that should be ignored by webhooks, such as:\n --config --bootstrap-kubeconfig, --kubeconfig --network-plugin (and, if it equals cni, also --cni-bin-dir and --cni-conf-dir) --node-labels  The kubelet command line shall not contain any provider-specific flags, such as:\n --cloud-provider --cloud-config --provider-id  These flags can be added by webhooks if needed.\nThe kubelet command line / configuration may contain a number of additional provider-independent flags / options. In general, webhooks should ignore these unless they are known to interfere with the desired kubelet behavior for the specific provider. Among the flags / options to be considered are:\n --enable-controller-attach-detach (enableControllerAttachDetach) - should be set to true if CSI plugins are used, but in general can also be ignored since its default value is also true, and this should work both with and without CSI plugins. --feature-gates (featureGates) - should contain a list of specific feature gates if CSI plugins are used. If CSI plugins are not used, the corresponding feature gates can be ignored since enabling them should not harm in any way.  "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/controlplane/","title":"ControlPlane resource","tags":[],"description":"","content":"Contract: ControlPlane resource Most Kubernetes clusters require a cloud-controller-manager or CSI drivers in order to work properly. Before introducing the ControlPlane extension resource Gardener was having several different Helm charts for the cloud-controller-manager deployments for the various providers. Now, Gardener commissions an external, provider-specific controller to take over this task.\nWhich control plane resources are required? As mentioned in the controlplane customization webhooks document Gardener shall not deploy any cloud-controller-manager or any other provider-specific component. Instead, it creates a ControlPlane CRD that should be picked up by provider extensions. Its purpose is to trigger the deployment of such provider-specific components in the shoot namespace in the seed cluster.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:ControlPlanemetadata:name:control-planenamespace:shoot--foo--barspec:type:openstackregion:europe-west1secretRef:name:cloudprovidernamespace:shoot--foo--barproviderConfig:apiVersion:openstack.provider.extensions.gardener.cloud/v1alpha1kind:ControlPlaneConfigloadBalancerProvider:providerzone:eu-1acloudControllerManager:featureGates:CustomResourceValidation:trueinfrastructureProviderStatus:apiVersion:openstack.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusnetworks:floatingPool:id:vpc-1234subnets:- purpose:nodesid:subnetidThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used for the shoot cluster. However, the most important section is the .spec.providerConfig and the .spec.infrastructureProviderStatus. The first one contains an embedded declaration of the provider specific configuration for the control plane (that cannot be known by Gardener itself). You are responsible for designing how this configuration looks like. Gardener does not evaluate it but just copies this part from what has been provided by the end-user in the Shoot resource. The second one contains the output of the Infrastructure resource (that might be relevant for the CCM config).\nIn order to support a new control plane provider you need to write a controller that watches all ControlPlanes with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the Alicloud provider.\nThe control plane controller as part of the ControlPlane reconciliation, often deploys resources (e.g. pods/deployments) into the Shoot namespace in the Seed as part of its ControlPlane reconciliation loop. Because the namespace contains network policies that per default deny all ingress and egress traffic, the pods may need to have proper labels matching to the selectors of the network policies in order to allow the required network traffic. Otherwise, they won\u0026rsquo;t be allowed to talk to certain other components (e.g., the kube-apiserver of the shoot). Please see this document for more information.\nNon-provider specific information required for infrastructure creation Most providers might require further information that is not provider specific but already part of the shoot resource. One example for this is the GCP control plane controller which needs the Kubernetes version of the shoot cluster (because it already uses the in-tree Kubernetes cloud-controller-manager). As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information that is not part of the Infrastructure resource itself.\nReferences and additional resources  ControlPlane API (Golang specification) Exemplary implementation for the Alicloud provider  "},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/controlplane/","title":"ControlPlane resource","tags":[],"description":"","content":"Contract: ControlPlane resource Most Kubernetes clusters require a cloud-controller-manager or CSI drivers in order to work properly. Before introducing the ControlPlane extension resource Gardener was having several different Helm charts for the cloud-controller-manager deployments for the various providers. Now, Gardener commissions an external, provider-specific controller to take over this task.\nWhich control plane resources are required? As mentioned in the controlplane customization webhooks document Gardener shall not deploy any cloud-controller-manager or any other provider-specific component. Instead, it creates a ControlPlane CRD that should be picked up by provider extensions. Its purpose is to trigger the deployment of such provider-specific components in the shoot namespace in the seed cluster.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:ControlPlanemetadata:name:control-planenamespace:shoot--foo--barspec:type:openstackregion:europe-west1secretRef:name:cloudprovidernamespace:shoot--foo--barproviderConfig:apiVersion:openstack.provider.extensions.gardener.cloud/v1alpha1kind:ControlPlaneConfigloadBalancerProvider:providerzone:eu-1acloudControllerManager:featureGates:CustomResourceValidation:trueinfrastructureProviderStatus:apiVersion:openstack.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusnetworks:floatingPool:id:vpc-1234subnets:- purpose:nodesid:subnetidThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used for the shoot cluster. However, the most important section is the .spec.providerConfig and the .spec.infrastructureProviderStatus. The first one contains an embedded declaration of the provider specific configuration for the control plane (that cannot be known by Gardener itself). You are responsible for designing how this configuration looks like. Gardener does not evaluate it but just copies this part from what has been provided by the end-user in the Shoot resource. The second one contains the output of the Infrastructure resource (that might be relevant for the CCM config).\nIn order to support a new control plane provider you need to write a controller that watches all ControlPlanes with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the Alicloud provider.\nThe control plane controller as part of the ControlPlane reconciliation, often deploys resources (e.g. pods/deployments) into the Shoot namespace in the Seed as part of its ControlPlane reconciliation loop. Because the namespace contains network policies that per default deny all ingress and egress traffic, the pods may need to have proper labels matching to the selectors of the network policies in order to allow the required network traffic. Otherwise, they won\u0026rsquo;t be allowed to talk to certain other components (e.g., the kube-apiserver of the shoot). Please see this document for more information.\nNon-provider specific information required for infrastructure creation Most providers might require further information that is not provider specific but already part of the shoot resource. One example for this is the GCP control plane controller which needs the Kubernetes version of the shoot cluster (because it already uses the in-tree Kubernetes cloud-controller-manager). As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information that is not part of the Infrastructure resource itself.\nReferences and additional resources  ControlPlane API (Golang specification) Exemplary implementation for the Alicloud provider  "},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/controlplane/","title":"ControlPlane resource","tags":[],"description":"","content":"Contract: ControlPlane resource Most Kubernetes clusters require a cloud-controller-manager or CSI drivers in order to work properly. Before introducing the ControlPlane extension resource Gardener was having several different Helm charts for the cloud-controller-manager deployments for the various providers. Now, Gardener commissions an external, provider-specific controller to take over this task.\nWhich control plane resources are required? As mentioned in the controlplane customization webhooks document Gardener shall not deploy any cloud-controller-manager or any other provider-specific component. Instead, it creates a ControlPlane CRD that should be picked up by provider extensions. Its purpose is to trigger the deployment of such provider-specific components in the shoot namespace in the seed cluster.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:ControlPlanemetadata:name:control-planenamespace:shoot--foo--barspec:type:openstackregion:europe-west1secretRef:name:cloudprovidernamespace:shoot--foo--barproviderConfig:apiVersion:openstack.provider.extensions.gardener.cloud/v1alpha1kind:ControlPlaneConfigloadBalancerProvider:providerzone:eu-1acloudControllerManager:featureGates:CustomResourceValidation:trueinfrastructureProviderStatus:apiVersion:openstack.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusnetworks:floatingPool:id:vpc-1234subnets:- purpose:nodesid:subnetidThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used for the shoot cluster. However, the most important section is the .spec.providerConfig and the .spec.infrastructureProviderStatus. The first one contains an embedded declaration of the provider specific configuration for the control plane (that cannot be known by Gardener itself). You are responsible for designing how this configuration looks like. Gardener does not evaluate it but just copies this part from what has been provided by the end-user in the Shoot resource. The second one contains the output of the Infrastructure resource (that might be relevant for the CCM config).\nIn order to support a new control plane provider you need to write a controller that watches all ControlPlanes with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the Alicloud provider.\nThe control plane controller as part of the ControlPlane reconciliation, often deploys resources (e.g. pods/deployments) into the Shoot namespace in the Seed as part of its ControlPlane reconciliation loop. Because the namespace contains network policies that per default deny all ingress and egress traffic, the pods may need to have proper labels matching to the selectors of the network policies in order to allow the required network traffic. Otherwise, they won\u0026rsquo;t be allowed to talk to certain other components (e.g., the kube-apiserver of the shoot). Please see this document for more information.\nNon-provider specific information required for infrastructure creation Most providers might require further information that is not provider specific but already part of the shoot resource. One example for this is the GCP control plane controller which needs the Kubernetes version of the shoot cluster (because it already uses the in-tree Kubernetes cloud-controller-manager). As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information that is not part of the Infrastructure resource itself.\nReferences and additional resources  ControlPlane API (Golang specification) Exemplary implementation for the Alicloud provider  "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/controlplane-exposure/","title":"ControlPlane resource with purpose exposure","tags":[],"description":"","content":"Contract: ControlPlane resource with purpose exposure Some Kubernetes clusters require an additional deployments required by the seed cloud provider in order to work properly, e.g. AWS Load Balancer Readvertiser. Before using ControlPlane resources with purpose exposure Gardener was having different Helm charts for the deployments for the various providers. Now, Gardener commissions an external, provider-specific controller to take over this task.\nWhich control plane resources are required? As mentioned in the controlplane document Gardener shall not deploy any other provider-specific component. Instead, it creates a ControlPlane CRD with purpose exposure that should be picked up by provider extensions. Its purpose is to trigger the deployment of such provider-specific components in the shoot namespace in the seed cluster that are needed to expose the kube-apiserver.\nThe shoot cluster\u0026rsquo;s kube-apiserver are exposed via a Service of type LoadBalancer from the shoot provider (you may run the control plane of an Azure shoot in a GCP seed) it\u0026rsquo;s the seed provider extension controller that should act on the ControlPlane resources with purpose exposure.\nIf SNI is enabled, then the Service from above is of type ClusterIP and Gardner will not create ControlPlane resources with purpose exposure.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\napiVersion:extensions.gardener.cloud/v1alpha1kind:ControlPlanemetadata:name:control-plane-exposurenamespace:shoot--foo--barspec:type:awspurpose:exposureregion:europe-west1secretRef:name:cloudprovidernamespace:shoot--foo--barThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used for the shoot cluster. It is most likely not needed, however, still added for some potential corner cases. If you don\u0026rsquo;t need it then just ignore it. The .spec.region contains the region of the seed cluster.\nIn order to support a control plane provider with purpose exposure you need to write a controller or expand the existing controlplane controller that watches all ControlPlanes with .spec.type=\u0026lt;my-provider-name\u0026gt; and purpose exposure. You can take a look at the below referenced example implementation for the AWS provider.\nNon-provider specific information required for infrastructure creation Most providers might require further information that is not provider specific but already part of the shoot resource. As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information.\nReferences and additional resources  ControlPlane API (Golang specification) Exemplary implementation for the AWS provider AWS Load Balancer Readvertiser  "},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/controlplane-exposure/","title":"ControlPlane resource with purpose exposure","tags":[],"description":"","content":"Contract: ControlPlane resource with purpose exposure Some Kubernetes clusters require an additional deployments required by the seed cloud provider in order to work properly, e.g. AWS Load Balancer Readvertiser. Before using ControlPlane resources with purpose exposure Gardener was having different Helm charts for the deployments for the various providers. Now, Gardener commissions an external, provider-specific controller to take over this task.\nWhich control plane resources are required? As mentioned in the controlplane document Gardener shall not deploy any other provider-specific component. Instead, it creates a ControlPlane CRD with purpose exposure that should be picked up by provider extensions. Its purpose is to trigger the deployment of such provider-specific components in the shoot namespace in the seed cluster that are needed to expose the kube-apiserver.\nThe shoot cluster\u0026rsquo;s kube-apiserver are exposed via a Service of type LoadBalancer from the shoot provider (you may run the control plane of an Azure shoot in a GCP seed) it\u0026rsquo;s the seed provider extension controller that should act on the ControlPlane resources with purpose exposure.\nIf SNI is enabled, then the Service from above is of type ClusterIP and Gardner will not create ControlPlane resources with purpose exposure.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\napiVersion:extensions.gardener.cloud/v1alpha1kind:ControlPlanemetadata:name:control-plane-exposurenamespace:shoot--foo--barspec:type:awspurpose:exposureregion:europe-west1secretRef:name:cloudprovidernamespace:shoot--foo--barThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used for the shoot cluster. It is most likely not needed, however, still added for some potential corner cases. If you don\u0026rsquo;t need it then just ignore it. The .spec.region contains the region of the seed cluster.\nIn order to support a control plane provider with purpose exposure you need to write a controller or expand the existing controlplane controller that watches all ControlPlanes with .spec.type=\u0026lt;my-provider-name\u0026gt; and purpose exposure. You can take a look at the below referenced example implementation for the AWS provider.\nNon-provider specific information required for infrastructure creation Most providers might require further information that is not provider specific but already part of the shoot resource. As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information.\nReferences and additional resources  ControlPlane API (Golang specification) Exemplary implementation for the AWS provider AWS Load Balancer Readvertiser  "},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/controlplane-exposure/","title":"ControlPlane resource with purpose exposure","tags":[],"description":"","content":"Contract: ControlPlane resource with purpose exposure Some Kubernetes clusters require an additional deployments required by the seed cloud provider in order to work properly, e.g. AWS Load Balancer Readvertiser. Before using ControlPlane resources with purpose exposure Gardener was having different Helm charts for the deployments for the various providers. Now, Gardener commissions an external, provider-specific controller to take over this task.\nWhich control plane resources are required? As mentioned in the controlplane document Gardener shall not deploy any other provider-specific component. Instead, it creates a ControlPlane CRD with purpose exposure that should be picked up by provider extensions. Its purpose is to trigger the deployment of such provider-specific components in the shoot namespace in the seed cluster that are needed to expose the kube-apiserver.\nThe shoot cluster\u0026rsquo;s kube-apiserver are exposed via a Service of type LoadBalancer from the shoot provider (you may run the control plane of an Azure shoot in a GCP seed) it\u0026rsquo;s the seed provider extension controller that should act on the ControlPlane resources with purpose exposure.\nIf SNI is enabled, then the Service from above is of type ClusterIP and Gardner will not create ControlPlane resources with purpose exposure.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\napiVersion:extensions.gardener.cloud/v1alpha1kind:ControlPlanemetadata:name:control-plane-exposurenamespace:shoot--foo--barspec:type:awspurpose:exposureregion:europe-west1secretRef:name:cloudprovidernamespace:shoot--foo--barThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used for the shoot cluster. It is most likely not needed, however, still added for some potential corner cases. If you don\u0026rsquo;t need it then just ignore it. The .spec.region contains the region of the seed cluster.\nIn order to support a control plane provider with purpose exposure you need to write a controller or expand the existing controlplane controller that watches all ControlPlanes with .spec.type=\u0026lt;my-provider-name\u0026gt; and purpose exposure. You can take a look at the below referenced example implementation for the AWS provider.\nNon-provider specific information required for infrastructure creation Most providers might require further information that is not provider specific but already part of the shoot resource. As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information.\nReferences and additional resources  ControlPlane API (Golang specification) Exemplary implementation for the AWS provider AWS Load Balancer Readvertiser  "},{"uri":"https://gardener.cloud/documentation/references/core/","title":"Core","tags":[],"description":"","content":"Packages:\n  core.gardener.cloud/v1beta1   core.gardener.cloud/v1beta1  Package v1beta1 is a version of the API.\nResource Types:  BackupBucket  BackupEntry  CloudProfile  ControllerInstallation  ControllerRegistration  Plant  Project  Quota  SecretBinding  Seed  Shoot  BackupBucket   BackupBucket holds details about backup bucket\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  BackupBucket    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupBucketSpec     Specification of the Backup Bucket.\n     provider  BackupBucketProvider     Provider hold the details of cloud provider of the object store.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to BackupBucket resource.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupBucket for running controller.\n       status  BackupBucketStatus     Most recently observed status of the Backup Bucket.\n    BackupEntry   BackupEntry holds details about shoot backup.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  BackupEntry    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupEntrySpec     (Optional) Spec contains the specification of the Backup Entry.\n     bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupEntry for running controller.\n       status  BackupEntryStatus     (Optional) Status contains the most recently observed status of the Backup Entry.\n    CloudProfile   CloudProfile represents certain properties about a provider environment.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  CloudProfile    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  CloudProfileSpec     (Optional) Spec defines the provider environment properties.\n     caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every host machine of shoot cluster targeting this profile.\n    kubernetes  KubernetesSettings     Kubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []MachineType     MachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains provider-specific configuration for the profile.\n    regions  []Region     Regions contains constraints regarding allowed values for regions and zones.\n    seedSelector  SeedSelector     (Optional) SeedSelector contains an optional list of labels on Seed resources that marks those seeds whose shoots may use this provider profile. An empty list means that all seeds of the same provider type are supported. This is useful for environments that are of the same type (like openstack) but may have different \u0026ldquo;instances\u0026rdquo;/landscapes. Optionally a list of possible providers can be added to enable cross-provider scheduling. By default, the provider type of the seed must match the shoot\u0026rsquo;s provider.\n    type  string    Type is the name of the provider.\n    volumeTypes  []VolumeType     (Optional) VolumeTypes contains constraints regarding allowed values for volume types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n       ControllerInstallation   ControllerInstallation represents an installation request for an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  ControllerInstallation    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControllerInstallationSpec     Spec contains the specification of this installation.\n     registrationRef  Kubernetes core/v1.ObjectReference     RegistrationRef is used to reference a ControllerRegistration resources.\n    seedRef  Kubernetes core/v1.ObjectReference     SeedRef is used to reference a Seed resources.\n       status  ControllerInstallationStatus     Status contains the status of this installation.\n    ControllerRegistration   ControllerRegistration represents a registration of an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  ControllerRegistration    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControllerRegistrationSpec     Spec contains the specification of this registration.\n     resources  []ControllerResource     (Optional) Resources is a list of combinations of kinds (DNSProvider, Infrastructure, Generic, \u0026hellip;) and their actual types (aws-route53, gcp, auditlog, \u0026hellip;).\n    deployment  ControllerDeployment     (Optional) Deployment contains information for how this controller is deployed.\n       Plant      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Plant    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  PlantSpec     Spec contains the specification of this Plant.\n     secretRef  Kubernetes core/v1.LocalObjectReference     SecretRef is a reference to a Secret object containing the Kubeconfig of the external kubernetes clusters to be added to Gardener.\n    endpoints  []Endpoint     (Optional) Endpoints is the configuration plant endpoints\n       status  PlantStatus     Status contains the status of this Plant.\n    Project   Project holds certain properties about a Gardener project.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Project    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ProjectSpec     (Optional) Spec defines the project properties.\n     createdBy  Kubernetes rbac/v1.Subject     (Optional) CreatedBy is a subject representing a user name, an email address, or any other identifier of a user who created the project.\n    description  string    (Optional) Description is a human-readable description of what the project is used for.\n    owner  Kubernetes rbac/v1.Subject     (Optional) Owner is a subject representing a user name, an email address, or any other identifier of a user owning the project. IMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the owner role. The only way to change the owner will be by moving the owner role. In this API version the only way to change the owner is to use this field. TODO: Remove this field in favor of the owner role in v1.\n    purpose  string    (Optional) Purpose is a human-readable explanation of the project\u0026rsquo;s purpose.\n    members  []ProjectMember     (Optional) Members is a list of subjects representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    namespace  string    (Optional) Namespace is the name of the namespace that has been created for the Project object. A nil value means that Gardener will determine the name of the namespace.\n    tolerations  ProjectTolerations     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n       status  ProjectStatus     (Optional) Most recently observed status of the Project.\n    Quota      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Quota    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  QuotaSpec     (Optional) Spec defines the Quota constraints.\n     clusterLifetimeDays  int32    (Optional) ClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.\n    metrics  Kubernetes core/v1.ResourceList     Metrics is a list of resources which will be put under constraints.\n    scope  Kubernetes core/v1.ObjectReference     Scope is the scope of the Quota object, either \u0026lsquo;project\u0026rsquo; or \u0026lsquo;secret\u0026rsquo;.\n       SecretBinding      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  SecretBinding    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret object in the same or another namespace.\n    quotas  []Kubernetes core/v1.ObjectReference     (Optional) Quotas is a list of references to Quota objects in the same or another namespace.\n    Seed   Seed represents an installation request for an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Seed    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  SeedSpec     Spec contains the specification of this installation.\n     backup  SeedBackup     (Optional) Backup holds the object store configuration for the backups of shoot (currently only etcd). If it is not specified, then there won\u0026rsquo;t be any backups taken for shoots associated with this seed. If backup field is present in seed, then backups of the etcd from shoot control plane will be stored under the configured object store.\n    dns  SeedDNS     DNS contains DNS-relevant information about this seed cluster.\n    networks  SeedNetworks     Networks defines the pod, service and worker network of the Seed cluster.\n    provider  SeedProvider     Provider defines the provider type and region for this Seed cluster.\n    secretRef  Kubernetes core/v1.SecretReference     (Optional) SecretRef is a reference to a Secret object containing the Kubeconfig and the cloud provider credentials for the account the Seed cluster has been deployed to.\n    taints  []SeedTaint     (Optional) Taints describes taints on the seed.\n    volume  SeedVolume     (Optional) Volume contains settings for persistentvolumes created in the seed cluster.\n    settings  SeedSettings     (Optional) Settings contains certain settings for this seed cluster.\n    ingress  Ingress     (Optional) Ingress configures Ingress specific settings of the Seed cluster.\n       status  SeedStatus     Status contains the status of this installation.\n    Shoot      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Shoot    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ShootSpec     (Optional) Specification of the Shoot cluster.\n     addons  Addons     (Optional) Addons contains information about enabled/disabled addons and their configuration.\n    cloudProfileName  string    CloudProfileName is a name of a CloudProfile object.\n    dns  DNS     (Optional) DNS contains information about the DNS settings of the Shoot.\n    extensions  []Extension     (Optional) Extensions contain type and provider information for Shoot extensions.\n    hibernation  Hibernation     (Optional) Hibernation contains information whether the Shoot is suspended or not.\n    kubernetes  Kubernetes     Kubernetes contains the version and configuration settings of the control plane components.\n    networking  Networking     Networking contains information about cluster networking such as CNI Plugin type, CIDRs, \u0026hellip;etc.\n    maintenance  Maintenance     (Optional) Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n    monitoring  Monitoring     (Optional) Monitoring contains information about custom monitoring configurations for the shoot.\n    provider  Provider     Provider contains all provider-specific and provider-relevant information.\n    purpose  ShootPurpose     (Optional) Purpose is the purpose class for this cluster.\n    region  string    Region is a name of a region.\n    secretBindingName  string    SecretBindingName is the name of the a SecretBinding that has a reference to the provider secret. The credentials inside the provider secret will be used to create the shoot in the respective account.\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot.\n    seedSelector  SeedSelector     (Optional) SeedSelector is an optional selector which must match a seed\u0026rsquo;s labels for the shoot to be scheduled on that seed.\n    resources  []NamedResourceReference     (Optional) Resources holds a list of named resource references that can be referred to in extension configs by their names.\n    tolerations  []Toleration     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n       status  ShootStatus     (Optional) Most recently observed status of the Shoot cluster.\n    Addon   (Appears on: KubernetesDashboard, NginxIngress)  Addon allows enabling or disabling a specific addon and is used to derive from.\n   Field Description      enabled  bool    Enabled indicates whether the addon is enabled or not.\n    Addons   (Appears on: ShootSpec)  Addons is a collection of configuration for specific addons which are managed by the Gardener.\n   Field Description      kubernetesDashboard  KubernetesDashboard     (Optional) KubernetesDashboard holds configuration settings for the kubernetes dashboard addon.\n    nginxIngress  NginxIngress     (Optional) NginxIngress holds configuration settings for the nginx-ingress addon.\n    AdmissionPlugin   (Appears on: KubeAPIServerConfig)  AdmissionPlugin contains information about a specific admission plugin and its corresponding configuration.\n   Field Description      name  string    Name is the name of the plugin.\n    config  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) Config is the configuration of the plugin.\n    Alerting   (Appears on: Monitoring)  Alerting contains information about how alerting will be done (i.e. who will receive alerts and how).\n   Field Description      emailReceivers  []string    (Optional) MonitoringEmailReceivers is a list of recipients for alerts\n    AuditConfig   (Appears on: KubeAPIServerConfig)  AuditConfig contains settings for audit of the api server\n   Field Description      auditPolicy  AuditPolicy     (Optional) AuditPolicy contains configuration settings for audit policy of the kube-apiserver.\n    AuditPolicy   (Appears on: AuditConfig)  AuditPolicy contains audit policy for kube-apiserver\n   Field Description      configMapRef  Kubernetes core/v1.ObjectReference     (Optional) ConfigMapRef is a reference to a ConfigMap object in the same namespace, which contains the audit policy for the kube-apiserver.\n    AvailabilityZone   (Appears on: Region)  AvailabilityZone is an availability zone.\n   Field Description      name  string    Name is an an availability zone name.\n    unavailableMachineTypes  []string    (Optional) UnavailableMachineTypes is a list of machine type names that are not availability in this zone.\n    unavailableVolumeTypes  []string    (Optional) UnavailableVolumeTypes is a list of volume type names that are not availability in this zone.\n    BackupBucketProvider   (Appears on: BackupBucketSpec)  BackupBucketProvider holds the details of cloud provider of the object store.\n   Field Description      type  string    Type is the type of provider.\n    region  string    Region is the region of the bucket.\n    BackupBucketSpec   (Appears on: BackupBucket)  BackupBucketSpec is the specification of a Backup Bucket.\n   Field Description      provider  BackupBucketProvider     Provider hold the details of cloud provider of the object store.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to BackupBucket resource.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupBucket for running controller.\n    BackupBucketStatus   (Appears on: BackupBucket)  BackupBucketStatus holds the most recently observed status of the Backup Bucket.\n   Field Description      providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus is the configuration passed to BackupBucket resource.\n    lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the BackupBucket.\n    lastError  LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this BackupBucket. It corresponds to the BackupBucket\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    generatedSecretRef  Kubernetes core/v1.SecretReference     (Optional) GeneratedSecretRef is reference to the secret generated by backup bucket, which will have object store specific credentials.\n    BackupEntrySpec   (Appears on: BackupEntry)  BackupEntrySpec is the specification of a Backup Entry.\n   Field Description      bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupEntry for running controller.\n    BackupEntryStatus   (Appears on: BackupEntry)  BackupEntryStatus holds the most recently observed status of the Backup Entry.\n   Field Description      lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the BackupEntry.\n    lastError  LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this BackupEntry. It corresponds to the BackupEntry\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    CRI   (Appears on: MachineImageVersion, Worker)  CRI contains information about the Container Runtimes.\n   Field Description      name  CRIName     The name of the CRI library\n    containerRuntimes  []ContainerRuntime     (Optional) ContainerRuntimes is the list of the required container runtimes supported for a worker pool.\n    CRIName (string alias)\n  (Appears on: CRI)  CRIName is a type alias for the CRI name string.\nCloudInfo   (Appears on: ClusterInfo)  CloudInfo contains information about the cloud\n   Field Description      type  string    Type is the cloud type\n    region  string    Region is the cloud region\n    CloudProfileSpec   (Appears on: CloudProfile)  CloudProfileSpec is the specification of a CloudProfile. It must contain exactly one of its defined keys.\n   Field Description      caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every host machine of shoot cluster targeting this profile.\n    kubernetes  KubernetesSettings     Kubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []MachineType     MachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains provider-specific configuration for the profile.\n    regions  []Region     Regions contains constraints regarding allowed values for regions and zones.\n    seedSelector  SeedSelector     (Optional) SeedSelector contains an optional list of labels on Seed resources that marks those seeds whose shoots may use this provider profile. An empty list means that all seeds of the same provider type are supported. This is useful for environments that are of the same type (like openstack) but may have different \u0026ldquo;instances\u0026rdquo;/landscapes. Optionally a list of possible providers can be added to enable cross-provider scheduling. By default, the provider type of the seed must match the shoot\u0026rsquo;s provider.\n    type  string    Type is the name of the provider.\n    volumeTypes  []VolumeType     (Optional) VolumeTypes contains constraints regarding allowed values for volume types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    ClusterAutoscaler   (Appears on: Kubernetes)  ClusterAutoscaler contains the configuration flags for the Kubernetes cluster autoscaler.\n   Field Description      scaleDownDelayAfterAdd  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterAdd defines how long after scale up that scale down evaluation resumes (default: 1 hour).\n    scaleDownDelayAfterDelete  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterDelete how long after node deletion that scale down evaluation resumes, defaults to scanInterval (defaults to ScanInterval).\n    scaleDownDelayAfterFailure  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterFailure how long after scale down failure that scale down evaluation resumes (default: 3 mins).\n    scaleDownUnneededTime  Kubernetes meta/v1.Duration     (Optional) ScaleDownUnneededTime defines how long a node should be unneeded before it is eligible for scale down (default: 30 mins).\n    scaleDownUtilizationThreshold  float64    (Optional) ScaleDownUtilizationThreshold defines the threshold in % under which a node is being removed\n    scanInterval  Kubernetes meta/v1.Duration     (Optional) ScanInterval how often cluster is reevaluated for scale up or down (default: 10 secs).\n    ClusterInfo   (Appears on: PlantStatus)  ClusterInfo contains information about the Plant cluster\n   Field Description      cloud  CloudInfo     Cloud describes the cloud information\n    kubernetes  KubernetesInfo     Kubernetes describes kubernetes meta information (e.g., version)\n    Condition   (Appears on: ControllerInstallationStatus, PlantStatus, SeedStatus, ShootStatus)  Condition holds the information about the state of a resource.\n   Field Description      type  ConditionType     Type of the Shoot condition.\n    status  ConditionStatus     Status of the condition, one of True, False, Unknown.\n    lastTransitionTime  Kubernetes meta/v1.Time     Last time the condition transitioned from one status to another.\n    lastUpdateTime  Kubernetes meta/v1.Time     Last time the condition was updated.\n    reason  string    The reason for the condition\u0026rsquo;s last transition.\n    message  string    A human readable message indicating details about the transition.\n    codes  []ErrorCode     (Optional) Well-defined error codes in case the condition reports a problem.\n    ConditionStatus (string alias)\n  (Appears on: Condition)  ConditionStatus is the status of a condition.\nConditionType (string alias)\n  (Appears on: Condition)  ConditionType is a string alias.\nContainerRuntime   (Appears on: CRI)  ContainerRuntime contains information about worker\u0026rsquo;s available container runtime\n   Field Description      type  string    Type is the type of the Container Runtime.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to container runtime resource.\n    ControllerDeployment   (Appears on: ControllerRegistrationSpec)  ControllerDeployment contains information for how this controller is deployed.\n   Field Description      type  string    Type is the deployment type.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains type-specific configuration.\n    policy  ControllerDeploymentPolicy     (Optional) Policy controls how the controller is deployed. It defaults to \u0026lsquo;OnDemand\u0026rsquo;.\n    seedSelector  Kubernetes meta/v1.LabelSelector     (Optional) SeedSelector contains an optional label selector for seeds. Only if the labels match then this controller will be considered for a deployment. An empty list means that all seeds are selected.\n    ControllerDeploymentPolicy (string alias)\n  (Appears on: ControllerDeployment)  ControllerDeploymentPolicy is a string alias.\nControllerInstallationSpec   (Appears on: ControllerInstallation)  ControllerInstallationSpec is the specification of a ControllerInstallation.\n   Field Description      registrationRef  Kubernetes core/v1.ObjectReference     RegistrationRef is used to reference a ControllerRegistration resources.\n    seedRef  Kubernetes core/v1.ObjectReference     SeedRef is used to reference a Seed resources.\n    ControllerInstallationStatus   (Appears on: ControllerInstallation)  ControllerInstallationStatus is the status of a ControllerInstallation.\n   Field Description      conditions  []Condition     (Optional) Conditions represents the latest available observations of a ControllerInstallations\u0026rsquo;s current state.\n    providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus contains type-specific status.\n    ControllerRegistrationSpec   (Appears on: ControllerRegistration)  ControllerRegistrationSpec is the specification of a ControllerRegistration.\n   Field Description      resources  []ControllerResource     (Optional) Resources is a list of combinations of kinds (DNSProvider, Infrastructure, Generic, \u0026hellip;) and their actual types (aws-route53, gcp, auditlog, \u0026hellip;).\n    deployment  ControllerDeployment     (Optional) Deployment contains information for how this controller is deployed.\n    ControllerResource   (Appears on: ControllerRegistrationSpec)  ControllerResource is a combination of a kind (DNSProvider, Infrastructure, Generic, \u0026hellip;) and the actual type for this kind (aws-route53, gcp, auditlog, \u0026hellip;).\n   Field Description      kind  string    Kind is the resource kind, for example \u0026ldquo;OperatingSystemConfig\u0026rdquo;.\n    type  string    Type is the resource type, for example \u0026ldquo;coreos\u0026rdquo; or \u0026ldquo;ubuntu\u0026rdquo;.\n    globallyEnabled  bool    (Optional) GloballyEnabled determines if this ControllerResource is required by all Shoot clusters.\n    reconcileTimeout  Kubernetes meta/v1.Duration     (Optional) ReconcileTimeout defines how long Gardener should wait for the resource reconciliation.\n    primary  bool    (Optional) Primary determines if the controller backed by this ControllerRegistration is responsible for the extension resource\u0026rsquo;s lifecycle. This field defaults to true. There must be exactly one primary controller for this kind/type combination.\n    DNS   (Appears on: ShootSpec)  DNS holds information about the provider, the hosted zone id and the domain.\n   Field Description      domain  string    (Optional) Domain is the external available domain of the Shoot cluster. This domain will be written into the kubeconfig that is handed out to end-users. Once set it is immutable.\n    providers  []DNSProvider     (Optional) Providers is a list of DNS providers that shall be enabled for this shoot cluster. Only relevant if not a default domain is used.\n    DNSIncludeExclude   (Appears on: DNSProvider, SeedDNSProvider)     Field Description      include  []string    (Optional) Include is a list of resources that shall be included.\n    exclude  []string    (Optional) Exclude is a list of resources that shall be excluded.\n    DNSProvider   (Appears on: DNS)  DNSProvider contains information about a DNS provider.\n   Field Description      domains  DNSIncludeExclude     (Optional) Domains contains information about which domains shall be included/excluded for this provider.\n    primary  bool    (Optional) Primary indicates that this DNSProvider is used for shoot related domains.\n    secretName  string    (Optional) SecretName is a name of a secret containing credentials for the stated domain and the provider. When not specified, the Gardener will use the cloud provider credentials referenced by the Shoot and try to find respective credentials there (primary provider only). Specifying this field may override this behavior, i.e. forcing the Gardener to only look into the given secret.\n    type  string    (Optional) Type is the DNS provider type.\n    zones  DNSIncludeExclude     (Optional) Zones contains information about which hosted zones shall be included/excluded for this provider.\n    DataVolume   (Appears on: Worker)  DataVolume contains information about a data volume.\n   Field Description      name  string    Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    VolumeSize is the size of the volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    Endpoint   (Appears on: PlantSpec)  Endpoint is an endpoint for monitoring, logging and other services around the plant.\n   Field Description      name  string    Name is the name of the endpoint\n    url  string    URL is the url of the endpoint\n    purpose  string    Purpose is the purpose of the endpoint\n    ErrorCode (string alias)\n  (Appears on: Condition, LastError)  ErrorCode is a string alias.\nExpirableVersion   (Appears on: KubernetesSettings, MachineImageVersion)  ExpirableVersion contains a version and an expiration date.\n   Field Description      version  string    Version is the version identifier.\n    expirationDate  Kubernetes meta/v1.Time     (Optional) ExpirationDate defines the time at which this version expires.\n    classification  VersionClassification     (Optional) Classification defines the state of a version (preview, supported, deprecated)\n    Extension   (Appears on: ShootSpec)  Extension contains type and provider information for Shoot extensions.\n   Field Description      type  string    Type is the type of the extension resource.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to extension resource.\n    disabled  bool    (Optional) Disabled allows to disable extensions that were marked as \u0026lsquo;globally enabled\u0026rsquo; by Gardener administrators.\n    Gardener   (Appears on: SeedStatus, ShootStatus)  Gardener holds the information about the Gardener version that operated a resource.\n   Field Description      id  string    ID is the Docker container id of the Gardener which last acted on a resource.\n    name  string    Name is the hostname (pod name) of the Gardener which last acted on a resource.\n    version  string    Version is the version of the Gardener which last acted on a resource.\n    Hibernation   (Appears on: ShootSpec)  Hibernation contains information whether the Shoot is suspended or not.\n   Field Description      enabled  bool    (Optional) Enabled specifies whether the Shoot needs to be hibernated or not. If it is true, the Shoot\u0026rsquo;s desired state is to be hibernated. If it is false or nil, the Shoot\u0026rsquo;s desired state is to be awaken.\n    schedules  []HibernationSchedule     (Optional) Schedules determine the hibernation schedules.\n    HibernationSchedule   (Appears on: Hibernation)  HibernationSchedule determines the hibernation schedule of a Shoot. A Shoot will be regularly hibernated at each start time and will be woken up at each end time. Start or End can be omitted, though at least one of each has to be specified.\n   Field Description      start  string    (Optional) Start is a Cron spec at which time a Shoot will be hibernated.\n    end  string    (Optional) End is a Cron spec at which time a Shoot will be woken up.\n    location  string    (Optional) Location is the time location in which both start and and shall be evaluated.\n    HorizontalPodAutoscalerConfig   (Appears on: KubeControllerManagerConfig)  HorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      cpuInitializationPeriod  Kubernetes meta/v1.Duration     (Optional) The period after which a ready pod transition is considered to be the first.\n    downscaleDelay  Kubernetes meta/v1.Duration     (Optional) The period since last downscale, before another downscale can be performed in horizontal pod autoscaler.\n    downscaleStabilization  Kubernetes meta/v1.Duration     (Optional) The configurable window at which the controller will choose the highest recommendation for autoscaling.\n    initialReadinessDelay  Kubernetes meta/v1.Duration     (Optional) The configurable period at which the horizontal pod autoscaler considers a Pod not yet ready given that its unready and it has transitioned to unready during that time.\n    syncPeriod  Kubernetes meta/v1.Duration     (Optional) The period for syncing the number of pods in horizontal pod autoscaler.\n    tolerance  float64    (Optional) The minimum change (from 1.0) in the desired-to-actual metrics ratio for the horizontal pod autoscaler to consider scaling.\n    upscaleDelay  Kubernetes meta/v1.Duration     (Optional) The period since last upscale, before another upscale can be performed in horizontal pod autoscaler.\n    Ingress   (Appears on: SeedSpec)  Ingress configures the Ingress specific settings of the Seed cluster\n   Field Description      domain  string    Domain specifies the IngressDomain of the Seed cluster pointing to the ingress controller endpoint. It will be used to construct ingress URLs for system applications running in Shoot clusters. Once set this field is immutable.\n    controller  IngressController     Controller configures a Gardener managed Ingress Controller listening on the ingressDomain\n    IngressController   (Appears on: Ingress)  IngressController enables a Gardener managed Ingress Controller listening on the ingressDomain\n   Field Description      kind  string    Kind defines which kind of IngressController to use, for example nginx\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig specifies infrastructure specific configuration for the ingressController\n    KubeAPIServerConfig   (Appears on: Kubernetes)  KubeAPIServerConfig contains configuration settings for the kube-apiserver.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     admissionPlugins  []AdmissionPlugin     (Optional) AdmissionPlugins contains the list of user-defined admission plugins (additional to those managed by Gardener), and, if desired, the corresponding configuration.\n    apiAudiences  []string    (Optional) APIAudiences are the identifiers of the API. The service account token authenticator will validate that tokens used against the API are bound to at least one of these audiences. Defaults to [\u0026ldquo;kubernetes\u0026rdquo;].\n    auditConfig  AuditConfig     (Optional) AuditConfig contains configuration settings for the audit of the kube-apiserver.\n    enableBasicAuthentication  bool    (Optional) EnableBasicAuthentication defines whether basic authentication should be enabled for this cluster or not.\n    oidcConfig  OIDCConfig     (Optional) OIDCConfig contains configuration settings for the OIDC provider.\n    runtimeConfig  map[string]bool    (Optional) RuntimeConfig contains information about enabled or disabled APIs.\n    serviceAccountConfig  ServiceAccountConfig     (Optional) ServiceAccountConfig contains configuration settings for the service account handling of the kube-apiserver.\n    watchCacheSizes  WatchCacheSizes     (Optional) WatchCacheSizes contains configuration of the API server\u0026rsquo;s watch cache sizes. Configuring these flags might be useful for large-scale Shoot clusters with a lot of parallel update requests and a lot of watching controllers (e.g. large shooted Seed clusters). When the API server\u0026rsquo;s watch cache\u0026rsquo;s capacity is too small to cope with the amount of update requests and watchers for a particular resource, it might happen that controller watches are permanently stopped with too old resource version errors. Starting from kubernetes v1.19, the API server\u0026rsquo;s watch cache size is adapted dynamically and setting the watch cache size flags will have no effect, except when setting it to 0 (which disables the watch cache).\n    requests  KubeAPIServerRequests     (Optional) Requests contains configuration for request-specific settings for the kube-apiserver.\n    KubeAPIServerRequests   (Appears on: KubeAPIServerConfig)  KubeAPIServerRequests contains configuration for request-specific settings for the kube-apiserver.\n   Field Description      maxNonMutatingInflight  int32    (Optional) MaxNonMutatingInflight is the maximum number of non-mutating requests in flight at a given time. When the server exceeds this, it rejects requests.\n    maxMutatingInflight  int32    (Optional) MaxMutatingInflight is the maximum number of mutating requests in flight at a given time. When the server exceeds this, it rejects requests.\n    KubeControllerManagerConfig   (Appears on: Kubernetes)  KubeControllerManagerConfig contains configuration settings for the kube-controller-manager.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     horizontalPodAutoscaler  HorizontalPodAutoscalerConfig     (Optional) HorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager.\n    nodeCIDRMaskSize  int32    (Optional) NodeCIDRMaskSize defines the mask size for node cidr in cluster (default is 24)\n    podEvictionTimeout  Kubernetes meta/v1.Duration     (Optional) PodEvictionTimeout defines the grace period for deleting pods on failed nodes. Defaults to 2m.\n    KubeProxyConfig   (Appears on: Kubernetes)  KubeProxyConfig contains configuration settings for the kube-proxy.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     mode  ProxyMode     (Optional) Mode specifies which proxy mode to use. defaults to IPTables.\n    KubeSchedulerConfig   (Appears on: Kubernetes)  KubeSchedulerConfig contains configuration settings for the kube-scheduler.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     kubeMaxPDVols  string    (Optional) KubeMaxPDVols allows to configure the KUBE_MAX_PD_VOLS environment variable for the kube-scheduler. Please find more information here: https://kubernetes.io/docs/concepts/storage/storage-limits/#custom-limits Note that using this field is considered alpha-/experimental-level and is on your own risk. You should be aware of all the side-effects and consequences when changing it.\n    KubeletConfig   (Appears on: Kubernetes, WorkerKubernetes)  KubeletConfig contains configuration settings for the kubelet.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     cpuCFSQuota  bool    (Optional) CPUCFSQuota allows you to disable/enable CPU throttling for Pods.\n    cpuManagerPolicy  string    (Optional) CPUManagerPolicy allows to set alternative CPU management policies (default: none).\n    evictionHard  KubeletConfigEviction     (Optional) EvictionHard describes a set of eviction thresholds (e.g. memory.available   evictionMaxPodGracePeriod  int32    (Optional) EvictionMaxPodGracePeriod describes the maximum allowed grace period (in seconds) to use when terminating pods in response to a soft eviction threshold being met. Default: 90\n    evictionMinimumReclaim  KubeletConfigEvictionMinimumReclaim     (Optional) EvictionMinimumReclaim configures the amount of resources below the configured eviction threshold that the kubelet attempts to reclaim whenever the kubelet observes resource pressure. Default: 0 for each resource\n    evictionPressureTransitionPeriod  Kubernetes meta/v1.Duration     (Optional) EvictionPressureTransitionPeriod is the duration for which the kubelet has to wait before transitioning out of an eviction pressure condition. Default: 4m0s\n    evictionSoft  KubeletConfigEviction     (Optional) EvictionSoft describes a set of eviction thresholds (e.g. memory.available   evictionSoftGracePeriod  KubeletConfigEvictionSoftGracePeriod     (Optional) EvictionSoftGracePeriod describes a set of eviction grace periods (e.g. memory.available=1m30s) that correspond to how long a soft eviction threshold must hold before triggering a Pod eviction. Default: memory.available: 1m30s nodefs.available: 1m30s nodefs.inodesFree: 1m30s imagefs.available: 1m30s imagefs.inodesFree: 1m30s\n    maxPods  int32    (Optional) MaxPods is the maximum number of Pods that are allowed by the Kubelet. Default: 110\n    podPidsLimit  int64    (Optional) PodPIDsLimit is the maximum number of process IDs per pod allowed by the kubelet.\n    imagePullProgressDeadline  Kubernetes meta/v1.Duration     (Optional) ImagePullProgressDeadline describes the time limit under which if no pulling progress is made, the image pulling will be cancelled. Default: 1m\n    failSwapOn  bool    (Optional) FailSwapOn makes the Kubelet fail to start if swap is enabled on the node. (default true).\n    kubeReserved  KubeletConfigReserved     (Optional) KubeReserved is the configuration for resources reserved for kubernetes node components (mainly kubelet and container runtime). When updating these values, be aware that cgroup resizes may not succeed on active worker nodes. Look for the NodeAllocatableEnforced event to determine if the configuration was applied. Default: cpu=80m,memory=1Gi,pid=20k\n    systemReserved  KubeletConfigReserved     (Optional) SystemReserved is the configuration for resources reserved for system processes not managed by kubernetes (e.g. journald). When updating these values, be aware that cgroup resizes may not succeed on active worker nodes. Look for the NodeAllocatableEnforced event to determine if the configuration was applied.\n    KubeletConfigEviction   (Appears on: KubeletConfig)  KubeletConfigEviction contains kubelet eviction thresholds supporting either a resource.Quantity or a percentage based value.\n   Field Description      memoryAvailable  string    (Optional) MemoryAvailable is the threshold for the free memory on the host server.\n    imageFSAvailable  string    (Optional) ImageFSAvailable is the threshold for the free disk space in the imagefs filesystem (docker images and container writable layers).\n    imageFSInodesFree  string    (Optional) ImageFSInodesFree is the threshold for the available inodes in the imagefs filesystem.\n    nodeFSAvailable  string    (Optional) NodeFSAvailable is the threshold for the free disk space in the nodefs filesystem (docker volumes, logs, etc).\n    nodeFSInodesFree  string    (Optional) NodeFSInodesFree is the threshold for the available inodes in the nodefs filesystem.\n    KubeletConfigEvictionMinimumReclaim   (Appears on: KubeletConfig)  KubeletConfigEvictionMinimumReclaim contains configuration for the kubelet eviction minimum reclaim.\n   Field Description      memoryAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) MemoryAvailable is the threshold for the memory reclaim on the host server.\n    imageFSAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) ImageFSAvailable is the threshold for the disk space reclaim in the imagefs filesystem (docker images and container writable layers).\n    imageFSInodesFree  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) ImageFSInodesFree is the threshold for the inodes reclaim in the imagefs filesystem.\n    nodeFSAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) NodeFSAvailable is the threshold for the disk space reclaim in the nodefs filesystem (docker volumes, logs, etc).\n    nodeFSInodesFree  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) NodeFSInodesFree is the threshold for the inodes reclaim in the nodefs filesystem.\n    KubeletConfigEvictionSoftGracePeriod   (Appears on: KubeletConfig)  KubeletConfigEvictionSoftGracePeriod contains grace periods for kubelet eviction thresholds.\n   Field Description      memoryAvailable  Kubernetes meta/v1.Duration     (Optional) MemoryAvailable is the grace period for the MemoryAvailable eviction threshold.\n    imageFSAvailable  Kubernetes meta/v1.Duration     (Optional) ImageFSAvailable is the grace period for the ImageFSAvailable eviction threshold.\n    imageFSInodesFree  Kubernetes meta/v1.Duration     (Optional) ImageFSInodesFree is the grace period for the ImageFSInodesFree eviction threshold.\n    nodeFSAvailable  Kubernetes meta/v1.Duration     (Optional) NodeFSAvailable is the grace period for the NodeFSAvailable eviction threshold.\n    nodeFSInodesFree  Kubernetes meta/v1.Duration     (Optional) NodeFSInodesFree is the grace period for the NodeFSInodesFree eviction threshold.\n    KubeletConfigReserved   (Appears on: KubeletConfig)  KubeletConfigReserved contains reserved resources for daemons\n   Field Description      cpu  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) CPU is the reserved cpu.\n    memory  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) Memory is the reserved memory.\n    ephemeralStorage  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) EphemeralStorage is the reserved ephemeral-storage.\n    pid  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) PID is the reserved process-ids. To reserve PID, the SupportNodePidsLimit feature gate must be enabled in Kubernetes versions \u0026lt; 1.15.\n    Kubernetes   (Appears on: ShootSpec)  Kubernetes contains the version and configuration variables for the Shoot control plane.\n   Field Description      allowPrivilegedContainers  bool    (Optional) AllowPrivilegedContainers indicates whether privileged containers are allowed in the Shoot (default: true).\n    clusterAutoscaler  ClusterAutoscaler     (Optional) ClusterAutoscaler contains the configuration flags for the Kubernetes cluster autoscaler.\n    kubeAPIServer  KubeAPIServerConfig     (Optional) KubeAPIServer contains configuration settings for the kube-apiserver.\n    kubeControllerManager  KubeControllerManagerConfig     (Optional) KubeControllerManager contains configuration settings for the kube-controller-manager.\n    kubeScheduler  KubeSchedulerConfig     (Optional) KubeScheduler contains configuration settings for the kube-scheduler.\n    kubeProxy  KubeProxyConfig     (Optional) KubeProxy contains configuration settings for the kube-proxy.\n    kubelet  KubeletConfig     (Optional) Kubelet contains configuration settings for the kubelet.\n    version  string    Version is the semantic Kubernetes version to use for the Shoot cluster.\n    verticalPodAutoscaler  VerticalPodAutoscaler     (Optional) VerticalPodAutoscaler contains the configuration flags for the Kubernetes vertical pod autoscaler.\n    KubernetesConfig   (Appears on: KubeAPIServerConfig, KubeControllerManagerConfig, KubeProxyConfig, KubeSchedulerConfig, KubeletConfig)  KubernetesConfig contains common configuration fields for the control plane components.\n   Field Description      featureGates  map[string]bool    (Optional) FeatureGates contains information about enabled feature gates.\n    KubernetesDashboard   (Appears on: Addons)  KubernetesDashboard describes configuration values for the kubernetes-dashboard addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     authenticationMode  string    (Optional) AuthenticationMode defines the authentication mode for the kubernetes-dashboard.\n    KubernetesInfo   (Appears on: ClusterInfo)  KubernetesInfo contains the version and configuration variables for the Plant cluster.\n   Field Description      version  string    Version is the semantic Kubernetes version to use for the Plant cluster.\n    KubernetesSettings   (Appears on: CloudProfileSpec)  KubernetesSettings contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n   Field Description      versions  []ExpirableVersion     (Optional) Versions is the list of allowed Kubernetes versions with optional expiration dates for Shoot clusters.\n    LastError   (Appears on: BackupBucketStatus, BackupEntryStatus, ShootStatus)  LastError indicates the last occurred error for an operation on a resource.\n   Field Description      description  string    A human readable message indicating details about the last error.\n    taskID  string    (Optional) ID of the task which caused this last error\n    codes  []ErrorCode     (Optional) Well-defined error codes of the last error(s).\n    lastUpdateTime  Kubernetes meta/v1.Time     (Optional) Last time the error was reported\n    LastOperation   (Appears on: BackupBucketStatus, BackupEntryStatus, ShootStatus)  LastOperation indicates the type and the state of the last operation, along with a description message and a progress indicator.\n   Field Description      description  string    A human readable message indicating details about the last operation.\n    lastUpdateTime  Kubernetes meta/v1.Time     Last time the operation state transitioned from one to another.\n    progress  int32    The progress in percentage (0-100) of the last operation.\n    state  LastOperationState     Status of the last operation, one of Aborted, Processing, Succeeded, Error, Failed.\n    type  LastOperationType     Type of the last operation, one of Create, Reconcile, Delete.\n    LastOperationState (string alias)\n  (Appears on: LastOperation)  LastOperationState is a string alias.\nLastOperationType (string alias)\n  (Appears on: LastOperation)  LastOperationType is a string alias.\nMachine   (Appears on: Worker)  Machine contains information about the machine type and image.\n   Field Description      type  string    Type is the machine type of the worker group.\n    image  ShootMachineImage     (Optional) Image holds information about the machine image to use for all nodes of this pool. It will default to the latest version of the first image stated in the referenced CloudProfile if no value has been provided.\n    MachineControllerManagerSettings   (Appears on: Worker)  MachineControllerManagerSettings contains configurations for different worker-pools. Eg. MachineDrainTimeout, MachineHealthTimeout.\n   Field Description      machineDrainTimeout  Kubernetes meta/v1.Duration     (Optional) MachineDrainTimeout is the period after which machine is forcefully deleted.\n    machineHealthTimeout  Kubernetes meta/v1.Duration     (Optional) MachineHealthTimeout is the period after which machine is declared failed.\n    machineCreationTimeout  Kubernetes meta/v1.Duration     (Optional) MachineCreationTimeout is the period after which creation of the machine is declared failed.\n    maxEvictRetries  int32    (Optional) MaxEvictRetries are the number of eviction retries on a pod after which drain is declared failed, and forceful deletion is triggered.\n    nodeConditions  []string    (Optional) NodeConditions are the set of conditions if set to true for the period of MachineHealthTimeout, machine will be declared failed.\n    MachineImage   (Appears on: CloudProfileSpec)  MachineImage defines the name and multiple versions of the machine image in any environment.\n   Field Description      name  string    Name is the name of the image.\n    versions  []MachineImageVersion     Versions contains versions, expiration dates and container runtimes of the machine image\n    MachineImageVersion   (Appears on: MachineImage)  MachineImageVersion is an expirable version with list of supported container runtimes and interfaces\n   Field Description      ExpirableVersion  ExpirableVersion      (Members of ExpirableVersion are embedded into this type.)     cri  []CRI     (Optional) CRI list of supported container runtime and interfaces supported by this version\n    MachineType   (Appears on: CloudProfileSpec)  MachineType contains certain properties of a machine type.\n   Field Description      cpu  k8s.io/apimachinery/pkg/api/resource.Quantity     CPU is the number of CPUs for this machine type.\n    gpu  k8s.io/apimachinery/pkg/api/resource.Quantity     GPU is the number of GPUs for this machine type.\n    memory  k8s.io/apimachinery/pkg/api/resource.Quantity     Memory is the amount of memory for this machine type.\n    name  string    Name is the name of the machine type.\n    storage  MachineTypeStorage     (Optional) Storage is the amount of storage associated with the root volume of this machine type.\n    usable  bool    (Optional) Usable defines if the machine type can be used for shoot clusters.\n    MachineTypeStorage   (Appears on: MachineType)  MachineTypeStorage is the amount of storage associated with the root volume of this machine type.\n   Field Description      class  string    Class is the class of the storage type.\n    size  k8s.io/apimachinery/pkg/api/resource.Quantity     StorageSize is the storage size.\n    type  string    Type is the type of the storage.\n    Maintenance   (Appears on: ShootSpec)  Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n   Field Description      autoUpdate  MaintenanceAutoUpdate     (Optional) AutoUpdate contains information about which constraints should be automatically updated.\n    timeWindow  MaintenanceTimeWindow     (Optional) TimeWindow contains information about the time window for maintenance operations.\n    confineSpecUpdateRollout  bool    (Optional) ConfineSpecUpdateRollout prevents that changes/updates to the shoot specification will be rolled out immediately. Instead, they are rolled out during the shoot\u0026rsquo;s maintenance time window. There is one exception that will trigger an immediate roll out which is changes to the Spec.Hibernation.Enabled field.\n    MaintenanceAutoUpdate   (Appears on: Maintenance)  MaintenanceAutoUpdate contains information about which constraints should be automatically updated.\n   Field Description      kubernetesVersion  bool    KubernetesVersion indicates whether the patch Kubernetes version may be automatically updated (default: true).\n    machineImageVersion  bool    MachineImageVersion indicates whether the machine image version may be automatically updated (default: true).\n    MaintenanceTimeWindow   (Appears on: Maintenance)  MaintenanceTimeWindow contains information about the time window for maintenance operations.\n   Field Description      begin  string    Begin is the beginning of the time window in the format HHMMSS+ZONE, e.g. \u0026ldquo;220000+0100\u0026rdquo;. If not present, a random value will be computed.\n    end  string    End is the end of the time window in the format HHMMSS+ZONE, e.g. \u0026ldquo;220000+0100\u0026rdquo;. If not present, the value will be computed based on the \u0026ldquo;Begin\u0026rdquo; value.\n    Monitoring   (Appears on: ShootSpec)  Monitoring contains information about the monitoring configuration for the shoot.\n   Field Description      alerting  Alerting     (Optional) Alerting contains information about the alerting configuration for the shoot cluster.\n    NamedResourceReference   (Appears on: ShootSpec)  NamedResourceReference is a named reference to a resource.\n   Field Description      name  string    Name of the resource reference.\n    resourceRef  Kubernetes autoscaling/v1.CrossVersionObjectReference     ResourceRef is a reference to a resource.\n    Networking   (Appears on: ShootSpec)  Networking defines networking parameters for the shoot cluster.\n   Field Description      type  string    Type identifies the type of the networking plugin.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to network resource.\n    pods  string    (Optional) Pods is the CIDR of the pod network.\n    nodes  string    (Optional) Nodes is the CIDR of the entire node network.\n    services  string    (Optional) Services is the CIDR of the service network.\n    NginxIngress   (Appears on: Addons)  NginxIngress describes configuration values for the nginx-ingress addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     loadBalancerSourceRanges  []string    (Optional) LoadBalancerSourceRanges is list of allowed IP sources for NginxIngress\n    config  map[string]string    (Optional) Config contains custom configuration for the nginx-ingress-controller configuration. See https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/configmap.md#configuration-options\n    externalTrafficPolicy  Kubernetes core/v1.ServiceExternalTrafficPolicyType     (Optional) ExternalTrafficPolicy controls the .spec.externalTrafficPolicy value of the load balancer Service exposing the nginx-ingress. Defaults to Cluster.\n    OIDCConfig   (Appears on: KubeAPIServerConfig)  OIDCConfig contains configuration settings for the OIDC provider. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      caBundle  string    (Optional) If set, the OpenID server\u0026rsquo;s certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host\u0026rsquo;s root CA set will be used.\n    clientAuthentication  OpenIDConnectClientAuthentication     (Optional) ClientAuthentication can optionally contain client configuration used for kubeconfig generation.\n    clientID  string    (Optional) The client ID for the OpenID Connect client, must be set if oidc-issuer-url is set.\n    groupsClaim  string    (Optional) If provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This flag is experimental, please see the authentication documentation for further details.\n    groupsPrefix  string    (Optional) If provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies.\n    issuerURL  string    (Optional) The URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT).\n    requiredClaims  map[string]string    (Optional) ATTENTION: Only meaningful for Kubernetes \u0026gt;= 1.11 key=value pairs that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value.\n    signingAlgs  []string    (Optional) List of allowed JOSE asymmetric signing algorithms. JWTs with a \u0026lsquo;alg\u0026rsquo; header value not in this list will be rejected. Values are defined by RFC 7518 https://tools.ietf.org/html/rfc7518#section-3.1\n    usernameClaim  string    (Optional) The OpenID claim to use as the user name. Note that claims other than the default (\u0026lsquo;sub\u0026rsquo;) is not guaranteed to be unique and immutable. This flag is experimental, please see the authentication documentation for further details. (default \u0026ldquo;sub\u0026rdquo;)\n    usernamePrefix  string    (Optional) If provided, all usernames will be prefixed with this value. If not provided, username claims other than \u0026lsquo;email\u0026rsquo; are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value \u0026lsquo;-\u0026rsquo;.\n    OpenIDConnectClientAuthentication   (Appears on: OIDCConfig)  OpenIDConnectClientAuthentication contains configuration for OIDC clients.\n   Field Description      extraConfig  map[string]string    (Optional) Extra configuration added to kubeconfig\u0026rsquo;s auth-provider. Must not be any of idp-issuer-url, client-id, client-secret, idp-certificate-authority, idp-certificate-authority-data, id-token or refresh-token\n    secret  string    (Optional) The client Secret for the OpenID Connect client.\n    PlantSpec   (Appears on: Plant)  PlantSpec is the specification of a Plant.\n   Field Description      secretRef  Kubernetes core/v1.LocalObjectReference     SecretRef is a reference to a Secret object containing the Kubeconfig of the external kubernetes clusters to be added to Gardener.\n    endpoints  []Endpoint     (Optional) Endpoints is the configuration plant endpoints\n    PlantStatus   (Appears on: Plant)  PlantStatus is the status of a Plant.\n   Field Description      conditions  []Condition     (Optional) Conditions represents the latest available observations of a Plant\u0026rsquo;s current state.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Plant. It corresponds to the Plant\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    clusterInfo  ClusterInfo     ClusterInfo is additional computed information about the newly added cluster (Plant)\n    ProjectMember   (Appears on: ProjectSpec)  ProjectMember is a member of a project.\n   Field Description      Subject  Kubernetes rbac/v1.Subject      (Members of Subject are embedded into this type.) Subject is representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    role  string    Role represents the role of this member. IMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the roles list. TODO: Remove this field in favor of the owner role in v1.\n    roles  []string    (Optional) Roles represents the list of roles of this member.\n    ProjectPhase (string alias)\n  (Appears on: ProjectStatus)  ProjectPhase is a label for the condition of a project at the current time.\nProjectSpec   (Appears on: Project)  ProjectSpec is the specification of a Project.\n   Field Description      createdBy  Kubernetes rbac/v1.Subject     (Optional) CreatedBy is a subject representing a user name, an email address, or any other identifier of a user who created the project.\n    description  string    (Optional) Description is a human-readable description of what the project is used for.\n    owner  Kubernetes rbac/v1.Subject     (Optional) Owner is a subject representing a user name, an email address, or any other identifier of a user owning the project. IMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the owner role. The only way to change the owner will be by moving the owner role. In this API version the only way to change the owner is to use this field. TODO: Remove this field in favor of the owner role in v1.\n    purpose  string    (Optional) Purpose is a human-readable explanation of the project\u0026rsquo;s purpose.\n    members  []ProjectMember     (Optional) Members is a list of subjects representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    namespace  string    (Optional) Namespace is the name of the namespace that has been created for the Project object. A nil value means that Gardener will determine the name of the namespace.\n    tolerations  ProjectTolerations     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n    ProjectStatus   (Appears on: Project)  ProjectStatus holds the most recently observed status of the project.\n   Field Description      observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this project.\n    phase  ProjectPhase     Phase is the current phase of the project.\n    staleSinceTimestamp  Kubernetes meta/v1.Time     (Optional) StaleSinceTimestamp contains the timestamp when the project was first discovered to be stale/unused.\n    staleAutoDeleteTimestamp  Kubernetes meta/v1.Time     (Optional) StaleAutoDeleteTimestamp contains the timestamp when the project will be garbage-collected/automatically deleted because it\u0026rsquo;s stale/unused.\n    ProjectTolerations   (Appears on: ProjectSpec)  ProjectTolerations contains the tolerations for taints on seed clusters.\n   Field Description      defaults  []Toleration     (Optional) Defaults contains a list of tolerations that are added to the shoots in this project by default.\n    whitelist  []Toleration     (Optional) Whitelist contains a list of tolerations that are allowed to be added to the shoots in this project. Please note that this list may only be added by users having the spec-tolerations-whitelist verb for project resources.\n    Provider   (Appears on: ShootSpec)  Provider contains provider-specific information that are handed-over to the provider-specific extension controller.\n   Field Description      type  string    Type is the type of the provider.\n    controlPlaneConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ControlPlaneConfig contains the provider-specific control plane config blob. Please look up the concrete definition in the documentation of your provider extension.\n    infrastructureConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureConfig contains the provider-specific infrastructure config blob. Please look up the concrete definition in the documentation of your provider extension.\n    workers  []Worker     Workers is a list of worker groups.\n    ProxyMode (string alias)\n  (Appears on: KubeProxyConfig)  ProxyMode available in Linux platform: \u0026lsquo;userspace\u0026rsquo; (older, going to be EOL), \u0026lsquo;iptables\u0026rsquo; (newer, faster), \u0026lsquo;ipvs\u0026rsquo; (newest, better in performance and scalability). As of now only \u0026lsquo;iptables\u0026rsquo; and \u0026lsquo;ipvs\u0026rsquo; is supported by Gardener. In Linux platform, if the iptables proxy is selected, regardless of how, but the system\u0026rsquo;s kernel or iptables versions are insufficient, this always falls back to the userspace proxy. IPVS mode will be enabled when proxy mode is set to \u0026lsquo;ipvs\u0026rsquo;, and the fall back path is firstly iptables and then userspace.\nQuotaSpec   (Appears on: Quota)  QuotaSpec is the specification of a Quota.\n   Field Description      clusterLifetimeDays  int32    (Optional) ClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.\n    metrics  Kubernetes core/v1.ResourceList     Metrics is a list of resources which will be put under constraints.\n    scope  Kubernetes core/v1.ObjectReference     Scope is the scope of the Quota object, either \u0026lsquo;project\u0026rsquo; or \u0026lsquo;secret\u0026rsquo;.\n    Region   (Appears on: CloudProfileSpec)  Region contains certain properties of a region.\n   Field Description      name  string    Name is a region name.\n    zones  []AvailabilityZone     (Optional) Zones is a list of availability zones in this region.\n    labels  map[string]string    (Optional) Labels is an optional set of key-value pairs that contain certain administrator-controlled labels for this region. It can be used by Gardener administrators/operators to provide additional information about a region, e.g. wrt quality, reliability, access restrictions, etc.\n    ResourceWatchCacheSize   (Appears on: WatchCacheSizes)  ResourceWatchCacheSize contains configuration of the API server\u0026rsquo;s watch cache size for one specific resource.\n   Field Description      apiGroup  string    (Optional) APIGroup is the API group of the resource for which the watch cache size should be configured. An unset value is used to specify the legacy core API (e.g. for secrets).\n    resource  string    Resource is the name of the resource for which the watch cache size should be configured (in lowercase plural form, e.g. secrets).\n    size  int32    CacheSize specifies the watch cache size that should be configured for the specified resource.\n    SeedBackup   (Appears on: SeedSpec)  SeedBackup contains the object store configuration for backups for shoot (currently only etcd).\n   Field Description      provider  string    Provider is a provider name.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to BackupBucket resource.\n    region  string    (Optional) Region is a region name.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a Secret object containing the cloud provider credentials for the object store where backups should be stored. It should have enough privileges to manipulate the objects as well as buckets.\n    SeedDNS   (Appears on: SeedSpec)  SeedDNS contains DNS-relevant information about this seed cluster.\n   Field Description      ingressDomain  string    (Optional) IngressDomain is the domain of the Seed cluster pointing to the ingress controller endpoint. It will be used to construct ingress URLs for system applications running in Shoot clusters. Once set this field is immutable. This will be removed in the next API version and replaced by spec.ingress.domain.\n    provider  SeedDNSProvider     (Optional) Provider configures a DNSProvider\n    SeedDNSProvider   (Appears on: SeedDNS)  SeedDNSProvider configures a DNSProvider for Seeds\n   Field Description      type  string    Type describes the type of the dns-provider, for example aws-route53\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a Secret object containing cloud provider credentials used for registering external domains.\n    domains  DNSIncludeExclude     (Optional) Domains contains information about which domains shall be included/excluded for this provider.\n    zones  DNSIncludeExclude     (Optional) Zones contains information about which hosted zones shall be included/excluded for this provider.\n    SeedNetworks   (Appears on: SeedSpec)  SeedNetworks contains CIDRs for the pod, service and node networks of a Kubernetes cluster.\n   Field Description      nodes  string    (Optional) Nodes is the CIDR of the node network.\n    pods  string    Pods is the CIDR of the pod network.\n    services  string    Services is the CIDR of the service network.\n    shootDefaults  ShootNetworks     (Optional) ShootDefaults contains the default networks CIDRs for shoots.\n    blockCIDRs  []string    (Optional) BlockCIDRs is a list of network addresses that should be blocked for shoot control plane components running in the seed cluster.\n    SeedProvider   (Appears on: SeedSpec)  SeedProvider defines the provider type and region for this Seed cluster.\n   Field Description      type  string    Type is the name of the provider.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to Seed resource.\n    region  string    Region is a name of a region.\n    SeedSelector   (Appears on: CloudProfileSpec, ShootSpec)  SeedSelector contains constraints for selecting seed to be usable for shoots using a profile\n   Field Description      LabelSelector  Kubernetes meta/v1.LabelSelector      (Members of LabelSelector are embedded into this type.) (Optional) LabelSelector is optional and can be used to select seeds by their label settings\n    providerTypes  []string    (Optional) Providers is optional and can be used by restricting seeds by their provider type. \u0026lsquo;*\u0026rsquo; can be used to enable seeds regardless of their provider type.\n    SeedSettingExcessCapacityReservation   (Appears on: SeedSettings)  SeedSettingExcessCapacityReservation controls the excess capacity reservation for shoot control planes in the seed. When enabled then this is done via PodPriority and requires the Seed cluster to have Kubernetes version 1.11 or the PodPriority feature gate as well as the scheduling.k8s.io/v1alpha1 API group enabled.\n   Field Description      enabled  bool    Enabled controls whether the excess capacity reservation should be enabled.\n    SeedSettingLoadBalancerServices   (Appears on: SeedSettings)  SeedSettingLoadBalancerServices controls certain settings for services of type load balancer that are created in the seed.\n   Field Description      annotations  map[string]string    (Optional) Annotations is a map of annotations that will be injected/merged into every load balancer service object.\n    SeedSettingScheduling   (Appears on: SeedSettings)  SeedSettingScheduling controls settings for scheduling decisions for the seed.\n   Field Description      visible  bool    Visible controls whether the gardener-scheduler shall consider this seed when scheduling shoots. Invisible seeds are not considered by the scheduler.\n    SeedSettingShootDNS   (Appears on: SeedSettings)  SeedSettingShootDNS controls the shoot DNS settings for the seed.\n   Field Description      enabled  bool    Enabled controls whether the DNS for shoot clusters should be enabled. When disabled then all shoots using the seed won\u0026rsquo;t get any DNS providers, DNS records, and no DNS extension controller is required to be installed here. This is useful for environments where DNS is not required.\n    SeedSettingVerticalPodAutoscaler   (Appears on: SeedSettings)  SeedSettingVerticalPodAutoscaler controls certain settings for the vertical pod autoscaler components deployed in the seed.\n   Field Description      enabled  bool    Enabled controls whether the VPA components shall be deployed into the garden namespace in the seed cluster. It is enabled by default because Gardener heavily relies on a VPA being deployed. You should only disable this if your seed cluster already has another, manually/custom managed VPA deployment.\n    SeedSettings   (Appears on: SeedSpec)  SeedSettings contains certain settings for this seed cluster.\n   Field Description      excessCapacityReservation  SeedSettingExcessCapacityReservation     (Optional) ExcessCapacityReservation controls the excess capacity reservation for shoot control planes in the seed.\n    scheduling  SeedSettingScheduling     (Optional) Scheduling controls settings for scheduling decisions for the seed.\n    shootDNS  SeedSettingShootDNS     (Optional) ShootDNS controls the shoot DNS settings for the seed.\n    loadBalancerServices  SeedSettingLoadBalancerServices     (Optional) LoadBalancerServices controls certain settings for services of type load balancer that are created in the seed.\n    verticalPodAutoscaler  SeedSettingVerticalPodAutoscaler     (Optional) VerticalPodAutoscaler controls certain settings for the vertical pod autoscaler components deployed in the seed.\n    SeedSpec   (Appears on: Seed)  SeedSpec is the specification of a Seed.\n   Field Description      backup  SeedBackup     (Optional) Backup holds the object store configuration for the backups of shoot (currently only etcd). If it is not specified, then there won\u0026rsquo;t be any backups taken for shoots associated with this seed. If backup field is present in seed, then backups of the etcd from shoot control plane will be stored under the configured object store.\n    dns  SeedDNS     DNS contains DNS-relevant information about this seed cluster.\n    networks  SeedNetworks     Networks defines the pod, service and worker network of the Seed cluster.\n    provider  SeedProvider     Provider defines the provider type and region for this Seed cluster.\n    secretRef  Kubernetes core/v1.SecretReference     (Optional) SecretRef is a reference to a Secret object containing the Kubeconfig and the cloud provider credentials for the account the Seed cluster has been deployed to.\n    taints  []SeedTaint     (Optional) Taints describes taints on the seed.\n    volume  SeedVolume     (Optional) Volume contains settings for persistentvolumes created in the seed cluster.\n    settings  SeedSettings     (Optional) Settings contains certain settings for this seed cluster.\n    ingress  Ingress     (Optional) Ingress configures Ingress specific settings of the Seed cluster.\n    SeedStatus   (Appears on: Seed)  SeedStatus is the status of a Seed.\n   Field Description      gardener  Gardener     (Optional) Gardener holds information about the Gardener which last acted on the Shoot.\n    kubernetesVersion  string    (Optional) KubernetesVersion is the Kubernetes version of the seed cluster.\n    conditions  []Condition     (Optional) Conditions represents the latest available observations of a Seed\u0026rsquo;s current state.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Seed. It corresponds to the Seed\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    clusterIdentity  string    (Optional) ClusterIdentity is the identity of the Seed cluster\n    capacity  Kubernetes core/v1.ResourceList     (Optional) Capacity represents the total resources of a seed.\n    allocatable  Kubernetes core/v1.ResourceList     (Optional) Allocatable represents the resources of a seed that are available for scheduling. Defaults to Capacity.\n    SeedTaint   (Appears on: SeedSpec)  SeedTaint describes a taint on a seed.\n   Field Description      key  string    Key is the taint key to be applied to a seed.\n    value  string    (Optional) Value is the taint value corresponding to the taint key.\n    SeedVolume   (Appears on: SeedSpec)  SeedVolume contains settings for persistentvolumes created in the seed cluster.\n   Field Description      minimumSize  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) MinimumSize defines the minimum size that should be used for PVCs in the seed.\n    providers  []SeedVolumeProvider     (Optional) Providers is a list of storage class provisioner types for the seed.\n    SeedVolumeProvider   (Appears on: SeedVolume)  SeedVolumeProvider is a storage class provisioner type.\n   Field Description      purpose  string    Purpose is the purpose of this provider.\n    name  string    Name is the name of the storage class provisioner type.\n    ServiceAccountConfig   (Appears on: KubeAPIServerConfig)  ServiceAccountConfig is the kube-apiserver configuration for service accounts.\n   Field Description      issuer  string    (Optional) Issuer is the identifier of the service account token issuer. The issuer will assert this identifier in \u0026ldquo;iss\u0026rdquo; claim of issued tokens. This value is a string or URI. Defaults to URI of the API server.\n    signingKeySecretName  Kubernetes core/v1.LocalObjectReference     (Optional) SigningKeySecret is a reference to a secret that contains an optional private key of the service account token issuer. The issuer will sign issued ID tokens with this private key. Only useful if service account tokens are also issued by another external system.\n    ShootMachineImage   (Appears on: Machine)  ShootMachineImage defines the name and the version of the shoot\u0026rsquo;s machine image in any environment. Has to be defined in the respective CloudProfile.\n   Field Description      name  string    Name is the name of the image.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the shoot\u0026rsquo;s individual configuration passed to an extension resource.\n    version  string    (Optional) Version is the version of the shoot\u0026rsquo;s image. If version is not provided, it will be defaulted to the latest version from the CloudProfile.\n    ShootNetworks   (Appears on: SeedNetworks)  ShootNetworks contains the default networks CIDRs for shoots.\n   Field Description      pods  string    (Optional) Pods is the CIDR of the pod network.\n    services  string    (Optional) Services is the CIDR of the service network.\n    ShootPurpose (string alias)\n  (Appears on: ShootSpec)  ShootPurpose is a type alias for string.\nShootSpec   (Appears on: Shoot)  ShootSpec is the specification of a Shoot.\n   Field Description      addons  Addons     (Optional) Addons contains information about enabled/disabled addons and their configuration.\n    cloudProfileName  string    CloudProfileName is a name of a CloudProfile object.\n    dns  DNS     (Optional) DNS contains information about the DNS settings of the Shoot.\n    extensions  []Extension     (Optional) Extensions contain type and provider information for Shoot extensions.\n    hibernation  Hibernation     (Optional) Hibernation contains information whether the Shoot is suspended or not.\n    kubernetes  Kubernetes     Kubernetes contains the version and configuration settings of the control plane components.\n    networking  Networking     Networking contains information about cluster networking such as CNI Plugin type, CIDRs, \u0026hellip;etc.\n    maintenance  Maintenance     (Optional) Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n    monitoring  Monitoring     (Optional) Monitoring contains information about custom monitoring configurations for the shoot.\n    provider  Provider     Provider contains all provider-specific and provider-relevant information.\n    purpose  ShootPurpose     (Optional) Purpose is the purpose class for this cluster.\n    region  string    Region is a name of a region.\n    secretBindingName  string    SecretBindingName is the name of the a SecretBinding that has a reference to the provider secret. The credentials inside the provider secret will be used to create the shoot in the respective account.\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot.\n    seedSelector  SeedSelector     (Optional) SeedSelector is an optional selector which must match a seed\u0026rsquo;s labels for the shoot to be scheduled on that seed.\n    resources  []NamedResourceReference     (Optional) Resources holds a list of named resource references that can be referred to in extension configs by their names.\n    tolerations  []Toleration     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n    ShootStatus   (Appears on: Shoot)  ShootStatus holds the most recently observed status of the Shoot cluster.\n   Field Description      conditions  []Condition     (Optional) Conditions represents the latest available observations of a Shoots\u0026rsquo;s current state.\n    constraints  []Condition     (Optional) Constraints represents conditions of a Shoot\u0026rsquo;s current state that constraint some operations on it.\n    gardener  Gardener     Gardener holds information about the Gardener which last acted on the Shoot.\n    hibernated  bool    IsHibernated indicates whether the Shoot is currently hibernated.\n    lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the Shoot.\n    lastErrors  []LastError     (Optional) LastErrors holds information about the last occurred error(s) during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Shoot. It corresponds to the Shoot\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    retryCycleStartTime  Kubernetes meta/v1.Time     (Optional) RetryCycleStartTime is the start time of the last retry cycle (used to determine how often an operation must be retried until we give up).\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot. This value is only written after a successful create/reconcile operation. It will be used when control planes are moved between Seeds.\n    technicalID  string    TechnicalID is the name that is used for creating the Seed namespace, the infrastructure resources, and basically everything that is related to this particular Shoot.\n    uid  k8s.io/apimachinery/pkg/types.UID     UID is a unique identifier for the Shoot cluster to avoid portability between Kubernetes clusters. It is used to compute unique hashes.\n    clusterIdentity  string    (Optional) ClusterIdentity is the identity of the Shoot cluster\n    Toleration   (Appears on: ProjectTolerations, ShootSpec)  Toleration is a toleration for a seed taint.\n   Field Description      key  string    Key is the toleration key to be applied to a project or shoot.\n    value  string    (Optional) Value is the toleration value corresponding to the toleration key.\n    VersionClassification (string alias)\n  (Appears on: ExpirableVersion)  VersionClassification is the logical state of a version according to https://github.com/gardener/gardener/blob/master/docs/operations/versioning.md\nVerticalPodAutoscaler   (Appears on: Kubernetes)  VerticalPodAutoscaler contains the configuration flags for the Kubernetes vertical pod autoscaler.\n   Field Description      enabled  bool    Enabled specifies whether the Kubernetes VPA shall be enabled for the shoot cluster.\n    evictAfterOOMThreshold  Kubernetes meta/v1.Duration     (Optional) EvictAfterOOMThreshold defines the threshold that will lead to pod eviction in case it OOMed in less than the given threshold since its start and if it has only one container (default: 10m0s).\n    evictionRateBurst  int32    (Optional) EvictionRateBurst defines the burst of pods that can be evicted (default: 1)\n    evictionRateLimit  float64    (Optional) EvictionRateLimit defines the number of pods that can be evicted per second. A rate limit set to 0 or -1 will disable the rate limiter (default: -1).\n    evictionTolerance  float64    (Optional) EvictionTolerance defines the fraction of replica count that can be evicted for update in case more than one pod can be evicted (default: 0.5).\n    recommendationMarginFraction  float64    (Optional) RecommendationMarginFraction is the fraction of usage added as the safety margin to the recommended request (default: 0.15).\n    updaterInterval  Kubernetes meta/v1.Duration     (Optional) UpdaterInterval is the interval how often the updater should run (default: 1m0s).\n    recommenderInterval  Kubernetes meta/v1.Duration     (Optional) RecommenderInterval is the interval how often metrics should be fetched (default: 1m0s).\n    Volume   (Appears on: Worker)  Volume contains information about the volume type, size, and encryption.\n   Field Description      name  string    (Optional) Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    VolumeSize is the size of the volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    VolumeType   (Appears on: CloudProfileSpec)  VolumeType contains certain properties of a volume type.\n   Field Description      class  string    Class is the class of the volume type.\n    name  string    Name is the name of the volume type.\n    usable  bool    (Optional) Usable defines if the volume type can be used for shoot clusters.\n    WatchCacheSizes   (Appears on: KubeAPIServerConfig)  WatchCacheSizes contains configuration of the API server\u0026rsquo;s watch cache sizes.\n   Field Description      default  int32    (Optional) Default configures the default watch cache size of the kube-apiserver (flag --default-watch-cache-size, defaults to 100). See: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/\n    resources  []ResourceWatchCacheSize     (Optional) Resources configures the watch cache size of the kube-apiserver per resource (flag --watch-cache-sizes). See: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/\n    Worker   (Appears on: Provider)  Worker is the base definition of a worker group.\n   Field Description      annotations  map[string]string    (Optional) Annotations is a map of key/value pairs for annotations for all the Node objects in this worker pool.\n    caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every machine of this worker pool.\n    cri  CRI     (Optional) CRI contains configurations of CRI support of every machine in the worker pool\n    kubernetes  WorkerKubernetes     (Optional) Kubernetes contains configuration for Kubernetes components related to this worker pool.\n    labels  map[string]string    (Optional) Labels is a map of key/value pairs for labels for all the Node objects in this worker pool.\n    name  string    Name is the name of the worker group.\n    machine  Machine     Machine contains information about the machine type and image.\n    maximum  int32    Maximum is the maximum number of VMs to create.\n    minimum  int32    Minimum is the minimum number of VMs to create.\n    maxSurge  k8s.io/apimachinery/pkg/util/intstr.IntOrString     (Optional) MaxSurge is maximum number of VMs that are created during an update.\n    maxUnavailable  k8s.io/apimachinery/pkg/util/intstr.IntOrString     (Optional) MaxUnavailable is the maximum number of VMs that can be unavailable during an update.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the provider-specific configuration for this worker pool.\n    taints  []Kubernetes core/v1.Taint     (Optional) Taints is a list of taints for all the Node objects in this worker pool.\n    volume  Volume     (Optional) Volume contains information about the volume type and size.\n    dataVolumes  []DataVolume     (Optional) DataVolumes contains a list of additional worker volumes.\n    kubeletDataVolumeName  string    (Optional) KubeletDataVolumeName contains the name of a dataVolume that should be used for storing kubelet state.\n    zones  []string    (Optional) Zones is a list of availability zones that are used to evenly distribute this worker pool. Optional as not every provider may support availability zones.\n    systemComponents  WorkerSystemComponents     (Optional) SystemComponents contains configuration for system components related to this worker pool\n    machineControllerManager  MachineControllerManagerSettings     (Optional) MachineControllerManagerSettings contains configurations for different worker-pools. Eg. MachineDrainTimeout, MachineHealthTimeout.\n    WorkerKubernetes   (Appears on: Worker)  WorkerKubernetes contains configuration for Kubernetes components related to this worker pool.\n   Field Description      kubelet  KubeletConfig     (Optional) Kubelet contains configuration settings for all kubelets of this worker pool.\n    WorkerSystemComponents   (Appears on: Worker)  WorkerSystemComponents contains configuration for system components related to this worker pool\n   Field Description      allow  bool    Allow determines whether the pool should be allowed to host system components or not (defaults to true)\n      Generated with gen-crd-api-reference-docs \n"},{"uri":"https://gardener.cloud/v1.12.8/references/core/","title":"Core","tags":[],"description":"","content":"Packages:\n  core.gardener.cloud/v1beta1   core.gardener.cloud/v1beta1  Package v1beta1 is a version of the API.\nResource Types:  BackupBucket  BackupEntry  CloudProfile  ControllerInstallation  ControllerRegistration  Plant  Project  Quota  SecretBinding  Seed  Shoot  BackupBucket   BackupBucket holds details about backup bucket\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  BackupBucket    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupBucketSpec     Specification of the Backup Bucket.\n     provider  BackupBucketProvider     Provider hold the details of cloud provider of the object store.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to BackupBucket resource.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupBucket for running controller.\n       status  BackupBucketStatus     Most recently observed status of the Backup Bucket.\n    BackupEntry   BackupEntry holds details about shoot backup.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  BackupEntry    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupEntrySpec     (Optional) Spec contains the specification of the Backup Entry.\n     bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupEntry for running controller.\n       status  BackupEntryStatus     (Optional) Status contains the most recently observed status of the Backup Entry.\n    CloudProfile   CloudProfile represents certain properties about a provider environment.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  CloudProfile    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  CloudProfileSpec     (Optional) Spec defines the provider environment properties.\n     caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every host machine of shoot cluster targeting this profile.\n    kubernetes  KubernetesSettings     Kubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []MachineType     MachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains provider-specific configuration for the profile.\n    regions  []Region     Regions contains constraints regarding allowed values for regions and zones.\n    seedSelector  SeedSelector     (Optional) SeedSelector contains an optional list of labels on Seed resources that marks those seeds whose shoots may use this provider profile. An empty list means that all seeds of the same provider type are supported. This is useful for environments that are of the same type (like openstack) but may have different \u0026ldquo;instances\u0026rdquo;/landscapes. Optionally a list of possible providers can be added to enable cross-provider scheduling. By default, the provider type of the seed must match the shoot\u0026rsquo;s provider.\n    type  string    Type is the name of the provider.\n    volumeTypes  []VolumeType     (Optional) VolumeTypes contains constraints regarding allowed values for volume types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n       ControllerInstallation   ControllerInstallation represents an installation request for an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  ControllerInstallation    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControllerInstallationSpec     Spec contains the specification of this installation.\n     registrationRef  Kubernetes core/v1.ObjectReference     RegistrationRef is used to reference a ControllerRegistration resources.\n    seedRef  Kubernetes core/v1.ObjectReference     SeedRef is used to reference a Seed resources.\n       status  ControllerInstallationStatus     Status contains the status of this installation.\n    ControllerRegistration   ControllerRegistration represents a registration of an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  ControllerRegistration    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControllerRegistrationSpec     Spec contains the specification of this registration.\n     resources  []ControllerResource     (Optional) Resources is a list of combinations of kinds (DNSProvider, Infrastructure, Generic, \u0026hellip;) and their actual types (aws-route53, gcp, auditlog, \u0026hellip;).\n    deployment  ControllerDeployment     (Optional) Deployment contains information for how this controller is deployed.\n       Plant      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Plant    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  PlantSpec     Spec contains the specification of this Plant.\n     secretRef  Kubernetes core/v1.LocalObjectReference     SecretRef is a reference to a Secret object containing the Kubeconfig of the external kubernetes clusters to be added to Gardener.\n    endpoints  []Endpoint     (Optional) Endpoints is the configuration plant endpoints\n       status  PlantStatus     Status contains the status of this Plant.\n    Project   Project holds certain properties about a Gardener project.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Project    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ProjectSpec     (Optional) Spec defines the project properties.\n     createdBy  Kubernetes rbac/v1.Subject     (Optional) CreatedBy is a subject representing a user name, an email address, or any other identifier of a user who created the project.\n    description  string    (Optional) Description is a human-readable description of what the project is used for.\n    owner  Kubernetes rbac/v1.Subject     (Optional) Owner is a subject representing a user name, an email address, or any other identifier of a user owning the project. IMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the owner role. The only way to change the owner will be by moving the owner role. In this API version the only way to change the owner is to use this field. TODO: Remove this field in favor of the owner role in v1.\n    purpose  string    (Optional) Purpose is a human-readable explanation of the project\u0026rsquo;s purpose.\n    members  []ProjectMember     (Optional) Members is a list of subjects representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    namespace  string    (Optional) Namespace is the name of the namespace that has been created for the Project object. A nil value means that Gardener will determine the name of the namespace.\n    tolerations  ProjectTolerations     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n       status  ProjectStatus     (Optional) Most recently observed status of the Project.\n    Quota      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Quota    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  QuotaSpec     (Optional) Spec defines the Quota constraints.\n     clusterLifetimeDays  int32    (Optional) ClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.\n    metrics  Kubernetes core/v1.ResourceList     Metrics is a list of resources which will be put under constraints.\n    scope  Kubernetes core/v1.ObjectReference     Scope is the scope of the Quota object, either \u0026lsquo;project\u0026rsquo; or \u0026lsquo;secret\u0026rsquo;.\n       SecretBinding      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  SecretBinding    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret object in the same or another namespace.\n    quotas  []Kubernetes core/v1.ObjectReference     (Optional) Quotas is a list of references to Quota objects in the same or another namespace.\n    Seed   Seed represents an installation request for an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Seed    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  SeedSpec     Spec contains the specification of this installation.\n     backup  SeedBackup     (Optional) Backup holds the object store configuration for the backups of shoot (currently only etcd). If it is not specified, then there won\u0026rsquo;t be any backups taken for shoots associated with this seed. If backup field is present in seed, then backups of the etcd from shoot control plane will be stored under the configured object store.\n    dns  SeedDNS     DNS contains DNS-relevant information about this seed cluster.\n    networks  SeedNetworks     Networks defines the pod, service and worker network of the Seed cluster.\n    provider  SeedProvider     Provider defines the provider type and region for this Seed cluster.\n    secretRef  Kubernetes core/v1.SecretReference     (Optional) SecretRef is a reference to a Secret object containing the Kubeconfig and the cloud provider credentials for the account the Seed cluster has been deployed to.\n    taints  []SeedTaint     (Optional) Taints describes taints on the seed.\n    volume  SeedVolume     (Optional) Volume contains settings for persistentvolumes created in the seed cluster.\n    settings  SeedSettings     (Optional) Settings contains certain settings for this seed cluster.\n    ingress  Ingress     (Optional) Ingress configures Ingress specific settings of the Seed cluster.\n       status  SeedStatus     Status contains the status of this installation.\n    Shoot      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Shoot    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ShootSpec     (Optional) Specification of the Shoot cluster.\n     addons  Addons     (Optional) Addons contains information about enabled/disabled addons and their configuration.\n    cloudProfileName  string    CloudProfileName is a name of a CloudProfile object.\n    dns  DNS     (Optional) DNS contains information about the DNS settings of the Shoot.\n    extensions  []Extension     (Optional) Extensions contain type and provider information for Shoot extensions.\n    hibernation  Hibernation     (Optional) Hibernation contains information whether the Shoot is suspended or not.\n    kubernetes  Kubernetes     Kubernetes contains the version and configuration settings of the control plane components.\n    networking  Networking     Networking contains information about cluster networking such as CNI Plugin type, CIDRs, \u0026hellip;etc.\n    maintenance  Maintenance     (Optional) Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n    monitoring  Monitoring     (Optional) Monitoring contains information about custom monitoring configurations for the shoot.\n    provider  Provider     Provider contains all provider-specific and provider-relevant information.\n    purpose  ShootPurpose     (Optional) Purpose is the purpose class for this cluster.\n    region  string    Region is a name of a region.\n    secretBindingName  string    SecretBindingName is the name of the a SecretBinding that has a reference to the provider secret. The credentials inside the provider secret will be used to create the shoot in the respective account.\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot.\n    seedSelector  SeedSelector     (Optional) SeedSelector is an optional selector which must match a seed\u0026rsquo;s labels for the shoot to be scheduled on that seed.\n    resources  []NamedResourceReference     (Optional) Resources holds a list of named resource references that can be referred to in extension configs by their names.\n    tolerations  []Toleration     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n       status  ShootStatus     (Optional) Most recently observed status of the Shoot cluster.\n    Addon   (Appears on: KubernetesDashboard, NginxIngress)  Addon allows enabling or disabling a specific addon and is used to derive from.\n   Field Description      enabled  bool    Enabled indicates whether the addon is enabled or not.\n    Addons   (Appears on: ShootSpec)  Addons is a collection of configuration for specific addons which are managed by the Gardener.\n   Field Description      kubernetesDashboard  KubernetesDashboard     (Optional) KubernetesDashboard holds configuration settings for the kubernetes dashboard addon.\n    nginxIngress  NginxIngress     (Optional) NginxIngress holds configuration settings for the nginx-ingress addon.\n    AdmissionPlugin   (Appears on: KubeAPIServerConfig)  AdmissionPlugin contains information about a specific admission plugin and its corresponding configuration.\n   Field Description      name  string    Name is the name of the plugin.\n    config  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) Config is the configuration of the plugin.\n    Alerting   (Appears on: Monitoring)  Alerting contains information about how alerting will be done (i.e. who will receive alerts and how).\n   Field Description      emailReceivers  []string    (Optional) MonitoringEmailReceivers is a list of recipients for alerts\n    AuditConfig   (Appears on: KubeAPIServerConfig)  AuditConfig contains settings for audit of the api server\n   Field Description      auditPolicy  AuditPolicy     (Optional) AuditPolicy contains configuration settings for audit policy of the kube-apiserver.\n    AuditPolicy   (Appears on: AuditConfig)  AuditPolicy contains audit policy for kube-apiserver\n   Field Description      configMapRef  Kubernetes core/v1.ObjectReference     (Optional) ConfigMapRef is a reference to a ConfigMap object in the same namespace, which contains the audit policy for the kube-apiserver.\n    AvailabilityZone   (Appears on: Region)  AvailabilityZone is an availability zone.\n   Field Description      name  string    Name is an an availability zone name.\n    unavailableMachineTypes  []string    (Optional) UnavailableMachineTypes is a list of machine type names that are not availability in this zone.\n    unavailableVolumeTypes  []string    (Optional) UnavailableVolumeTypes is a list of volume type names that are not availability in this zone.\n    BackupBucketProvider   (Appears on: BackupBucketSpec)  BackupBucketProvider holds the details of cloud provider of the object store.\n   Field Description      type  string    Type is the type of provider.\n    region  string    Region is the region of the bucket.\n    BackupBucketSpec   (Appears on: BackupBucket)  BackupBucketSpec is the specification of a Backup Bucket.\n   Field Description      provider  BackupBucketProvider     Provider hold the details of cloud provider of the object store.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to BackupBucket resource.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupBucket for running controller.\n    BackupBucketStatus   (Appears on: BackupBucket)  BackupBucketStatus holds the most recently observed status of the Backup Bucket.\n   Field Description      providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus is the configuration passed to BackupBucket resource.\n    lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the BackupBucket.\n    lastError  LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this BackupBucket. It corresponds to the BackupBucket\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    generatedSecretRef  Kubernetes core/v1.SecretReference     (Optional) GeneratedSecretRef is reference to the secret generated by backup bucket, which will have object store specific credentials.\n    BackupEntrySpec   (Appears on: BackupEntry)  BackupEntrySpec is the specification of a Backup Entry.\n   Field Description      bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupEntry for running controller.\n    BackupEntryStatus   (Appears on: BackupEntry)  BackupEntryStatus holds the most recently observed status of the Backup Entry.\n   Field Description      lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the BackupEntry.\n    lastError  LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this BackupEntry. It corresponds to the BackupEntry\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    CRI   (Appears on: MachineImageVersion, Worker)  CRI contains information about the Container Runtimes.\n   Field Description      name  CRIName     The name of the CRI library\n    containerRuntimes  []ContainerRuntime     (Optional) ContainerRuntimes is the list of the required container runtimes supported for a worker pool.\n    CRIName (string alias)\n  (Appears on: CRI)  CRIName is a type alias for the CRI name string.\nCloudInfo   (Appears on: ClusterInfo)  CloudInfo contains information about the cloud\n   Field Description      type  string    Type is the cloud type\n    region  string    Region is the cloud region\n    CloudProfileSpec   (Appears on: CloudProfile)  CloudProfileSpec is the specification of a CloudProfile. It must contain exactly one of its defined keys.\n   Field Description      caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every host machine of shoot cluster targeting this profile.\n    kubernetes  KubernetesSettings     Kubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []MachineType     MachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains provider-specific configuration for the profile.\n    regions  []Region     Regions contains constraints regarding allowed values for regions and zones.\n    seedSelector  SeedSelector     (Optional) SeedSelector contains an optional list of labels on Seed resources that marks those seeds whose shoots may use this provider profile. An empty list means that all seeds of the same provider type are supported. This is useful for environments that are of the same type (like openstack) but may have different \u0026ldquo;instances\u0026rdquo;/landscapes. Optionally a list of possible providers can be added to enable cross-provider scheduling. By default, the provider type of the seed must match the shoot\u0026rsquo;s provider.\n    type  string    Type is the name of the provider.\n    volumeTypes  []VolumeType     (Optional) VolumeTypes contains constraints regarding allowed values for volume types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    ClusterAutoscaler   (Appears on: Kubernetes)  ClusterAutoscaler contains the configuration flags for the Kubernetes cluster autoscaler.\n   Field Description      scaleDownDelayAfterAdd  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterAdd defines how long after scale up that scale down evaluation resumes (default: 1 hour).\n    scaleDownDelayAfterDelete  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterDelete how long after node deletion that scale down evaluation resumes, defaults to scanInterval (defaults to ScanInterval).\n    scaleDownDelayAfterFailure  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterFailure how long after scale down failure that scale down evaluation resumes (default: 3 mins).\n    scaleDownUnneededTime  Kubernetes meta/v1.Duration     (Optional) ScaleDownUnneededTime defines how long a node should be unneeded before it is eligible for scale down (default: 30 mins).\n    scaleDownUtilizationThreshold  float64    (Optional) ScaleDownUtilizationThreshold defines the threshold in % under which a node is being removed\n    scanInterval  Kubernetes meta/v1.Duration     (Optional) ScanInterval how often cluster is reevaluated for scale up or down (default: 10 secs).\n    ClusterInfo   (Appears on: PlantStatus)  ClusterInfo contains information about the Plant cluster\n   Field Description      cloud  CloudInfo     Cloud describes the cloud information\n    kubernetes  KubernetesInfo     Kubernetes describes kubernetes meta information (e.g., version)\n    Condition   (Appears on: ControllerInstallationStatus, PlantStatus, SeedStatus, ShootStatus)  Condition holds the information about the state of a resource.\n   Field Description      type  ConditionType     Type of the Shoot condition.\n    status  ConditionStatus     Status of the condition, one of True, False, Unknown.\n    lastTransitionTime  Kubernetes meta/v1.Time     Last time the condition transitioned from one status to another.\n    lastUpdateTime  Kubernetes meta/v1.Time     Last time the condition was updated.\n    reason  string    The reason for the condition\u0026rsquo;s last transition.\n    message  string    A human readable message indicating details about the transition.\n    codes  []ErrorCode     (Optional) Well-defined error codes in case the condition reports a problem.\n    ConditionStatus (string alias)\n  (Appears on: Condition)  ConditionStatus is the status of a condition.\nConditionType (string alias)\n  (Appears on: Condition)  ConditionType is a string alias.\nContainerRuntime   (Appears on: CRI)  ContainerRuntime contains information about worker\u0026rsquo;s available container runtime\n   Field Description      type  string    Type is the type of the Container Runtime.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to container runtime resource.\n    ControllerDeployment   (Appears on: ControllerRegistrationSpec)  ControllerDeployment contains information for how this controller is deployed.\n   Field Description      type  string    Type is the deployment type.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains type-specific configuration.\n    policy  ControllerDeploymentPolicy     (Optional) Policy controls how the controller is deployed. It defaults to \u0026lsquo;OnDemand\u0026rsquo;.\n    seedSelector  Kubernetes meta/v1.LabelSelector     (Optional) SeedSelector contains an optional label selector for seeds. Only if the labels match then this controller will be considered for a deployment. An empty list means that all seeds are selected.\n    ControllerDeploymentPolicy (string alias)\n  (Appears on: ControllerDeployment)  ControllerDeploymentPolicy is a string alias.\nControllerInstallationSpec   (Appears on: ControllerInstallation)  ControllerInstallationSpec is the specification of a ControllerInstallation.\n   Field Description      registrationRef  Kubernetes core/v1.ObjectReference     RegistrationRef is used to reference a ControllerRegistration resources.\n    seedRef  Kubernetes core/v1.ObjectReference     SeedRef is used to reference a Seed resources.\n    ControllerInstallationStatus   (Appears on: ControllerInstallation)  ControllerInstallationStatus is the status of a ControllerInstallation.\n   Field Description      conditions  []Condition     (Optional) Conditions represents the latest available observations of a ControllerInstallations\u0026rsquo;s current state.\n    providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus contains type-specific status.\n    ControllerRegistrationSpec   (Appears on: ControllerRegistration)  ControllerRegistrationSpec is the specification of a ControllerRegistration.\n   Field Description      resources  []ControllerResource     (Optional) Resources is a list of combinations of kinds (DNSProvider, Infrastructure, Generic, \u0026hellip;) and their actual types (aws-route53, gcp, auditlog, \u0026hellip;).\n    deployment  ControllerDeployment     (Optional) Deployment contains information for how this controller is deployed.\n    ControllerResource   (Appears on: ControllerRegistrationSpec)  ControllerResource is a combination of a kind (DNSProvider, Infrastructure, Generic, \u0026hellip;) and the actual type for this kind (aws-route53, gcp, auditlog, \u0026hellip;).\n   Field Description      kind  string    Kind is the resource kind, for example \u0026ldquo;OperatingSystemConfig\u0026rdquo;.\n    type  string    Type is the resource type, for example \u0026ldquo;coreos\u0026rdquo; or \u0026ldquo;ubuntu\u0026rdquo;.\n    globallyEnabled  bool    (Optional) GloballyEnabled determines if this ControllerResource is required by all Shoot clusters.\n    reconcileTimeout  Kubernetes meta/v1.Duration     (Optional) ReconcileTimeout defines how long Gardener should wait for the resource reconciliation.\n    primary  bool    (Optional) Primary determines if the controller backed by this ControllerRegistration is responsible for the extension resource\u0026rsquo;s lifecycle. This field defaults to true. There must be exactly one primary controller for this kind/type combination.\n    DNS   (Appears on: ShootSpec)  DNS holds information about the provider, the hosted zone id and the domain.\n   Field Description      domain  string    (Optional) Domain is the external available domain of the Shoot cluster. This domain will be written into the kubeconfig that is handed out to end-users. Once set it is immutable.\n    providers  []DNSProvider     (Optional) Providers is a list of DNS providers that shall be enabled for this shoot cluster. Only relevant if not a default domain is used.\n    DNSIncludeExclude   (Appears on: DNSProvider, SeedDNSProvider)     Field Description      include  []string    (Optional) Include is a list of resources that shall be included.\n    exclude  []string    (Optional) Exclude is a list of resources that shall be excluded.\n    DNSProvider   (Appears on: DNS)  DNSProvider contains information about a DNS provider.\n   Field Description      domains  DNSIncludeExclude     (Optional) Domains contains information about which domains shall be included/excluded for this provider.\n    primary  bool    (Optional) Primary indicates that this DNSProvider is used for shoot related domains.\n    secretName  string    (Optional) SecretName is a name of a secret containing credentials for the stated domain and the provider. When not specified, the Gardener will use the cloud provider credentials referenced by the Shoot and try to find respective credentials there (primary provider only). Specifying this field may override this behavior, i.e. forcing the Gardener to only look into the given secret.\n    type  string    (Optional) Type is the DNS provider type.\n    zones  DNSIncludeExclude     (Optional) Zones contains information about which hosted zones shall be included/excluded for this provider.\n    DataVolume   (Appears on: Worker)  DataVolume contains information about a data volume.\n   Field Description      name  string    Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    VolumeSize is the size of the volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    Endpoint   (Appears on: PlantSpec)  Endpoint is an endpoint for monitoring, logging and other services around the plant.\n   Field Description      name  string    Name is the name of the endpoint\n    url  string    URL is the url of the endpoint\n    purpose  string    Purpose is the purpose of the endpoint\n    ErrorCode (string alias)\n  (Appears on: Condition, LastError)  ErrorCode is a string alias.\nExpirableVersion   (Appears on: KubernetesSettings, MachineImageVersion)  ExpirableVersion contains a version and an expiration date.\n   Field Description      version  string    Version is the version identifier.\n    expirationDate  Kubernetes meta/v1.Time     (Optional) ExpirationDate defines the time at which this version expires.\n    classification  VersionClassification     (Optional) Classification defines the state of a version (preview, supported, deprecated)\n    Extension   (Appears on: ShootSpec)  Extension contains type and provider information for Shoot extensions.\n   Field Description      type  string    Type is the type of the extension resource.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to extension resource.\n    disabled  bool    (Optional) Disabled allows to disable extensions that were marked as \u0026lsquo;globally enabled\u0026rsquo; by Gardener administrators.\n    Gardener   (Appears on: SeedStatus, ShootStatus)  Gardener holds the information about the Gardener version that operated a resource.\n   Field Description      id  string    ID is the Docker container id of the Gardener which last acted on a resource.\n    name  string    Name is the hostname (pod name) of the Gardener which last acted on a resource.\n    version  string    Version is the version of the Gardener which last acted on a resource.\n    Hibernation   (Appears on: ShootSpec)  Hibernation contains information whether the Shoot is suspended or not.\n   Field Description      enabled  bool    (Optional) Enabled specifies whether the Shoot needs to be hibernated or not. If it is true, the Shoot\u0026rsquo;s desired state is to be hibernated. If it is false or nil, the Shoot\u0026rsquo;s desired state is to be awaken.\n    schedules  []HibernationSchedule     (Optional) Schedules determine the hibernation schedules.\n    HibernationSchedule   (Appears on: Hibernation)  HibernationSchedule determines the hibernation schedule of a Shoot. A Shoot will be regularly hibernated at each start time and will be woken up at each end time. Start or End can be omitted, though at least one of each has to be specified.\n   Field Description      start  string    (Optional) Start is a Cron spec at which time a Shoot will be hibernated.\n    end  string    (Optional) End is a Cron spec at which time a Shoot will be woken up.\n    location  string    (Optional) Location is the time location in which both start and and shall be evaluated.\n    HorizontalPodAutoscalerConfig   (Appears on: KubeControllerManagerConfig)  HorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      cpuInitializationPeriod  Kubernetes meta/v1.Duration     (Optional) The period after which a ready pod transition is considered to be the first.\n    downscaleDelay  Kubernetes meta/v1.Duration     (Optional) The period since last downscale, before another downscale can be performed in horizontal pod autoscaler.\n    downscaleStabilization  Kubernetes meta/v1.Duration     (Optional) The configurable window at which the controller will choose the highest recommendation for autoscaling.\n    initialReadinessDelay  Kubernetes meta/v1.Duration     (Optional) The configurable period at which the horizontal pod autoscaler considers a Pod not yet ready given that its unready and it has transitioned to unready during that time.\n    syncPeriod  Kubernetes meta/v1.Duration     (Optional) The period for syncing the number of pods in horizontal pod autoscaler.\n    tolerance  float64    (Optional) The minimum change (from 1.0) in the desired-to-actual metrics ratio for the horizontal pod autoscaler to consider scaling.\n    upscaleDelay  Kubernetes meta/v1.Duration     (Optional) The period since last upscale, before another upscale can be performed in horizontal pod autoscaler.\n    Ingress   (Appears on: SeedSpec)  Ingress configures the Ingress specific settings of the Seed cluster\n   Field Description      domain  string    Domain specifies the IngressDomain of the Seed cluster pointing to the ingress controller endpoint. It will be used to construct ingress URLs for system applications running in Shoot clusters. Once set this field is immutable.\n    controller  IngressController     Controller configures a Gardener managed Ingress Controller listening on the ingressDomain\n    IngressController   (Appears on: Ingress)  IngressController enables a Gardener managed Ingress Controller listening on the ingressDomain\n   Field Description      kind  string    Kind defines which kind of IngressController to use, for example nginx\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig specifies infrastructure specific configuration for the ingressController\n    KubeAPIServerConfig   (Appears on: Kubernetes)  KubeAPIServerConfig contains configuration settings for the kube-apiserver.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     admissionPlugins  []AdmissionPlugin     (Optional) AdmissionPlugins contains the list of user-defined admission plugins (additional to those managed by Gardener), and, if desired, the corresponding configuration.\n    apiAudiences  []string    (Optional) APIAudiences are the identifiers of the API. The service account token authenticator will validate that tokens used against the API are bound to at least one of these audiences. Defaults to [\u0026ldquo;kubernetes\u0026rdquo;].\n    auditConfig  AuditConfig     (Optional) AuditConfig contains configuration settings for the audit of the kube-apiserver.\n    enableBasicAuthentication  bool    (Optional) EnableBasicAuthentication defines whether basic authentication should be enabled for this cluster or not.\n    oidcConfig  OIDCConfig     (Optional) OIDCConfig contains configuration settings for the OIDC provider.\n    runtimeConfig  map[string]bool    (Optional) RuntimeConfig contains information about enabled or disabled APIs.\n    serviceAccountConfig  ServiceAccountConfig     (Optional) ServiceAccountConfig contains configuration settings for the service account handling of the kube-apiserver.\n    watchCacheSizes  WatchCacheSizes     (Optional) WatchCacheSizes contains configuration of the API server\u0026rsquo;s watch cache sizes. Configuring these flags might be useful for large-scale Shoot clusters with a lot of parallel update requests and a lot of watching controllers (e.g. large shooted Seed clusters). When the API server\u0026rsquo;s watch cache\u0026rsquo;s capacity is too small to cope with the amount of update requests and watchers for a particular resource, it might happen that controller watches are permanently stopped with too old resource version errors. Starting from kubernetes v1.19, the API server\u0026rsquo;s watch cache size is adapted dynamically and setting the watch cache size flags will have no effect, except when setting it to 0 (which disables the watch cache).\n    requests  KubeAPIServerRequests     (Optional) Requests contains configuration for request-specific settings for the kube-apiserver.\n    KubeAPIServerRequests   (Appears on: KubeAPIServerConfig)  KubeAPIServerRequests contains configuration for request-specific settings for the kube-apiserver.\n   Field Description      maxNonMutatingInflight  int32    (Optional) MaxNonMutatingInflight is the maximum number of non-mutating requests in flight at a given time. When the server exceeds this, it rejects requests.\n    maxMutatingInflight  int32    (Optional) MaxMutatingInflight is the maximum number of mutating requests in flight at a given time. When the server exceeds this, it rejects requests.\n    KubeControllerManagerConfig   (Appears on: Kubernetes)  KubeControllerManagerConfig contains configuration settings for the kube-controller-manager.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     horizontalPodAutoscaler  HorizontalPodAutoscalerConfig     (Optional) HorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager.\n    nodeCIDRMaskSize  int32    (Optional) NodeCIDRMaskSize defines the mask size for node cidr in cluster (default is 24)\n    podEvictionTimeout  Kubernetes meta/v1.Duration     (Optional) PodEvictionTimeout defines the grace period for deleting pods on failed nodes. Defaults to 2m.\n    KubeProxyConfig   (Appears on: Kubernetes)  KubeProxyConfig contains configuration settings for the kube-proxy.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     mode  ProxyMode     (Optional) Mode specifies which proxy mode to use. defaults to IPTables.\n    KubeSchedulerConfig   (Appears on: Kubernetes)  KubeSchedulerConfig contains configuration settings for the kube-scheduler.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     kubeMaxPDVols  string    (Optional) KubeMaxPDVols allows to configure the KUBE_MAX_PD_VOLS environment variable for the kube-scheduler. Please find more information here: https://kubernetes.io/docs/concepts/storage/storage-limits/#custom-limits Note that using this field is considered alpha-/experimental-level and is on your own risk. You should be aware of all the side-effects and consequences when changing it.\n    KubeletConfig   (Appears on: Kubernetes, WorkerKubernetes)  KubeletConfig contains configuration settings for the kubelet.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     cpuCFSQuota  bool    (Optional) CPUCFSQuota allows you to disable/enable CPU throttling for Pods.\n    cpuManagerPolicy  string    (Optional) CPUManagerPolicy allows to set alternative CPU management policies (default: none).\n    evictionHard  KubeletConfigEviction     (Optional) EvictionHard describes a set of eviction thresholds (e.g. memory.available   evictionMaxPodGracePeriod  int32    (Optional) EvictionMaxPodGracePeriod describes the maximum allowed grace period (in seconds) to use when terminating pods in response to a soft eviction threshold being met. Default: 90\n    evictionMinimumReclaim  KubeletConfigEvictionMinimumReclaim     (Optional) EvictionMinimumReclaim configures the amount of resources below the configured eviction threshold that the kubelet attempts to reclaim whenever the kubelet observes resource pressure. Default: 0 for each resource\n    evictionPressureTransitionPeriod  Kubernetes meta/v1.Duration     (Optional) EvictionPressureTransitionPeriod is the duration for which the kubelet has to wait before transitioning out of an eviction pressure condition. Default: 4m0s\n    evictionSoft  KubeletConfigEviction     (Optional) EvictionSoft describes a set of eviction thresholds (e.g. memory.available   evictionSoftGracePeriod  KubeletConfigEvictionSoftGracePeriod     (Optional) EvictionSoftGracePeriod describes a set of eviction grace periods (e.g. memory.available=1m30s) that correspond to how long a soft eviction threshold must hold before triggering a Pod eviction. Default: memory.available: 1m30s nodefs.available: 1m30s nodefs.inodesFree: 1m30s imagefs.available: 1m30s imagefs.inodesFree: 1m30s\n    maxPods  int32    (Optional) MaxPods is the maximum number of Pods that are allowed by the Kubelet. Default: 110\n    podPidsLimit  int64    (Optional) PodPIDsLimit is the maximum number of process IDs per pod allowed by the kubelet.\n    imagePullProgressDeadline  Kubernetes meta/v1.Duration     (Optional) ImagePullProgressDeadline describes the time limit under which if no pulling progress is made, the image pulling will be cancelled. Default: 1m\n    failSwapOn  bool    (Optional) FailSwapOn makes the Kubelet fail to start if swap is enabled on the node. (default true).\n    kubeReserved  KubeletConfigReserved     (Optional) KubeReserved is the configuration for resources reserved for kubernetes node components (mainly kubelet and container runtime). When updating these values, be aware that cgroup resizes may not succeed on active worker nodes. Look for the NodeAllocatableEnforced event to determine if the configuration was applied. Default: cpu=80m,memory=1Gi,pid=20k\n    systemReserved  KubeletConfigReserved     (Optional) SystemReserved is the configuration for resources reserved for system processes not managed by kubernetes (e.g. journald). When updating these values, be aware that cgroup resizes may not succeed on active worker nodes. Look for the NodeAllocatableEnforced event to determine if the configuration was applied.\n    KubeletConfigEviction   (Appears on: KubeletConfig)  KubeletConfigEviction contains kubelet eviction thresholds supporting either a resource.Quantity or a percentage based value.\n   Field Description      memoryAvailable  string    (Optional) MemoryAvailable is the threshold for the free memory on the host server.\n    imageFSAvailable  string    (Optional) ImageFSAvailable is the threshold for the free disk space in the imagefs filesystem (docker images and container writable layers).\n    imageFSInodesFree  string    (Optional) ImageFSInodesFree is the threshold for the available inodes in the imagefs filesystem.\n    nodeFSAvailable  string    (Optional) NodeFSAvailable is the threshold for the free disk space in the nodefs filesystem (docker volumes, logs, etc).\n    nodeFSInodesFree  string    (Optional) NodeFSInodesFree is the threshold for the available inodes in the nodefs filesystem.\n    KubeletConfigEvictionMinimumReclaim   (Appears on: KubeletConfig)  KubeletConfigEvictionMinimumReclaim contains configuration for the kubelet eviction minimum reclaim.\n   Field Description      memoryAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) MemoryAvailable is the threshold for the memory reclaim on the host server.\n    imageFSAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) ImageFSAvailable is the threshold for the disk space reclaim in the imagefs filesystem (docker images and container writable layers).\n    imageFSInodesFree  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) ImageFSInodesFree is the threshold for the inodes reclaim in the imagefs filesystem.\n    nodeFSAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) NodeFSAvailable is the threshold for the disk space reclaim in the nodefs filesystem (docker volumes, logs, etc).\n    nodeFSInodesFree  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) NodeFSInodesFree is the threshold for the inodes reclaim in the nodefs filesystem.\n    KubeletConfigEvictionSoftGracePeriod   (Appears on: KubeletConfig)  KubeletConfigEvictionSoftGracePeriod contains grace periods for kubelet eviction thresholds.\n   Field Description      memoryAvailable  Kubernetes meta/v1.Duration     (Optional) MemoryAvailable is the grace period for the MemoryAvailable eviction threshold.\n    imageFSAvailable  Kubernetes meta/v1.Duration     (Optional) ImageFSAvailable is the grace period for the ImageFSAvailable eviction threshold.\n    imageFSInodesFree  Kubernetes meta/v1.Duration     (Optional) ImageFSInodesFree is the grace period for the ImageFSInodesFree eviction threshold.\n    nodeFSAvailable  Kubernetes meta/v1.Duration     (Optional) NodeFSAvailable is the grace period for the NodeFSAvailable eviction threshold.\n    nodeFSInodesFree  Kubernetes meta/v1.Duration     (Optional) NodeFSInodesFree is the grace period for the NodeFSInodesFree eviction threshold.\n    KubeletConfigReserved   (Appears on: KubeletConfig)  KubeletConfigReserved contains reserved resources for daemons\n   Field Description      cpu  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) CPU is the reserved cpu.\n    memory  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) Memory is the reserved memory.\n    ephemeralStorage  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) EphemeralStorage is the reserved ephemeral-storage.\n    pid  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) PID is the reserved process-ids. To reserve PID, the SupportNodePidsLimit feature gate must be enabled in Kubernetes versions \u0026lt; 1.15.\n    Kubernetes   (Appears on: ShootSpec)  Kubernetes contains the version and configuration variables for the Shoot control plane.\n   Field Description      allowPrivilegedContainers  bool    (Optional) AllowPrivilegedContainers indicates whether privileged containers are allowed in the Shoot (default: true).\n    clusterAutoscaler  ClusterAutoscaler     (Optional) ClusterAutoscaler contains the configuration flags for the Kubernetes cluster autoscaler.\n    kubeAPIServer  KubeAPIServerConfig     (Optional) KubeAPIServer contains configuration settings for the kube-apiserver.\n    kubeControllerManager  KubeControllerManagerConfig     (Optional) KubeControllerManager contains configuration settings for the kube-controller-manager.\n    kubeScheduler  KubeSchedulerConfig     (Optional) KubeScheduler contains configuration settings for the kube-scheduler.\n    kubeProxy  KubeProxyConfig     (Optional) KubeProxy contains configuration settings for the kube-proxy.\n    kubelet  KubeletConfig     (Optional) Kubelet contains configuration settings for the kubelet.\n    version  string    Version is the semantic Kubernetes version to use for the Shoot cluster.\n    verticalPodAutoscaler  VerticalPodAutoscaler     (Optional) VerticalPodAutoscaler contains the configuration flags for the Kubernetes vertical pod autoscaler.\n    KubernetesConfig   (Appears on: KubeAPIServerConfig, KubeControllerManagerConfig, KubeProxyConfig, KubeSchedulerConfig, KubeletConfig)  KubernetesConfig contains common configuration fields for the control plane components.\n   Field Description      featureGates  map[string]bool    (Optional) FeatureGates contains information about enabled feature gates.\n    KubernetesDashboard   (Appears on: Addons)  KubernetesDashboard describes configuration values for the kubernetes-dashboard addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     authenticationMode  string    (Optional) AuthenticationMode defines the authentication mode for the kubernetes-dashboard.\n    KubernetesInfo   (Appears on: ClusterInfo)  KubernetesInfo contains the version and configuration variables for the Plant cluster.\n   Field Description      version  string    Version is the semantic Kubernetes version to use for the Plant cluster.\n    KubernetesSettings   (Appears on: CloudProfileSpec)  KubernetesSettings contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n   Field Description      versions  []ExpirableVersion     (Optional) Versions is the list of allowed Kubernetes versions with optional expiration dates for Shoot clusters.\n    LastError   (Appears on: BackupBucketStatus, BackupEntryStatus, ShootStatus)  LastError indicates the last occurred error for an operation on a resource.\n   Field Description      description  string    A human readable message indicating details about the last error.\n    taskID  string    (Optional) ID of the task which caused this last error\n    codes  []ErrorCode     (Optional) Well-defined error codes of the last error(s).\n    lastUpdateTime  Kubernetes meta/v1.Time     (Optional) Last time the error was reported\n    LastOperation   (Appears on: BackupBucketStatus, BackupEntryStatus, ShootStatus)  LastOperation indicates the type and the state of the last operation, along with a description message and a progress indicator.\n   Field Description      description  string    A human readable message indicating details about the last operation.\n    lastUpdateTime  Kubernetes meta/v1.Time     Last time the operation state transitioned from one to another.\n    progress  int32    The progress in percentage (0-100) of the last operation.\n    state  LastOperationState     Status of the last operation, one of Aborted, Processing, Succeeded, Error, Failed.\n    type  LastOperationType     Type of the last operation, one of Create, Reconcile, Delete.\n    LastOperationState (string alias)\n  (Appears on: LastOperation)  LastOperationState is a string alias.\nLastOperationType (string alias)\n  (Appears on: LastOperation)  LastOperationType is a string alias.\nMachine   (Appears on: Worker)  Machine contains information about the machine type and image.\n   Field Description      type  string    Type is the machine type of the worker group.\n    image  ShootMachineImage     (Optional) Image holds information about the machine image to use for all nodes of this pool. It will default to the latest version of the first image stated in the referenced CloudProfile if no value has been provided.\n    MachineControllerManagerSettings   (Appears on: Worker)  MachineControllerManagerSettings contains configurations for different worker-pools. Eg. MachineDrainTimeout, MachineHealthTimeout.\n   Field Description      machineDrainTimeout  Kubernetes meta/v1.Duration     (Optional) MachineDrainTimeout is the period after which machine is forcefully deleted.\n    machineHealthTimeout  Kubernetes meta/v1.Duration     (Optional) MachineHealthTimeout is the period after which machine is declared failed.\n    machineCreationTimeout  Kubernetes meta/v1.Duration     (Optional) MachineCreationTimeout is the period after which creation of the machine is declared failed.\n    maxEvictRetries  int32    (Optional) MaxEvictRetries are the number of eviction retries on a pod after which drain is declared failed, and forceful deletion is triggered.\n    nodeConditions  []string    (Optional) NodeConditions are the set of conditions if set to true for the period of MachineHealthTimeout, machine will be declared failed.\n    MachineImage   (Appears on: CloudProfileSpec)  MachineImage defines the name and multiple versions of the machine image in any environment.\n   Field Description      name  string    Name is the name of the image.\n    versions  []MachineImageVersion     Versions contains versions, expiration dates and container runtimes of the machine image\n    MachineImageVersion   (Appears on: MachineImage)  MachineImageVersion is an expirable version with list of supported container runtimes and interfaces\n   Field Description      ExpirableVersion  ExpirableVersion      (Members of ExpirableVersion are embedded into this type.)     cri  []CRI     (Optional) CRI list of supported container runtime and interfaces supported by this version\n    MachineType   (Appears on: CloudProfileSpec)  MachineType contains certain properties of a machine type.\n   Field Description      cpu  k8s.io/apimachinery/pkg/api/resource.Quantity     CPU is the number of CPUs for this machine type.\n    gpu  k8s.io/apimachinery/pkg/api/resource.Quantity     GPU is the number of GPUs for this machine type.\n    memory  k8s.io/apimachinery/pkg/api/resource.Quantity     Memory is the amount of memory for this machine type.\n    name  string    Name is the name of the machine type.\n    storage  MachineTypeStorage     (Optional) Storage is the amount of storage associated with the root volume of this machine type.\n    usable  bool    (Optional) Usable defines if the machine type can be used for shoot clusters.\n    MachineTypeStorage   (Appears on: MachineType)  MachineTypeStorage is the amount of storage associated with the root volume of this machine type.\n   Field Description      class  string    Class is the class of the storage type.\n    size  k8s.io/apimachinery/pkg/api/resource.Quantity     StorageSize is the storage size.\n    type  string    Type is the type of the storage.\n    Maintenance   (Appears on: ShootSpec)  Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n   Field Description      autoUpdate  MaintenanceAutoUpdate     (Optional) AutoUpdate contains information about which constraints should be automatically updated.\n    timeWindow  MaintenanceTimeWindow     (Optional) TimeWindow contains information about the time window for maintenance operations.\n    confineSpecUpdateRollout  bool    (Optional) ConfineSpecUpdateRollout prevents that changes/updates to the shoot specification will be rolled out immediately. Instead, they are rolled out during the shoot\u0026rsquo;s maintenance time window. There is one exception that will trigger an immediate roll out which is changes to the Spec.Hibernation.Enabled field.\n    MaintenanceAutoUpdate   (Appears on: Maintenance)  MaintenanceAutoUpdate contains information about which constraints should be automatically updated.\n   Field Description      kubernetesVersion  bool    KubernetesVersion indicates whether the patch Kubernetes version may be automatically updated (default: true).\n    machineImageVersion  bool    MachineImageVersion indicates whether the machine image version may be automatically updated (default: true).\n    MaintenanceTimeWindow   (Appears on: Maintenance)  MaintenanceTimeWindow contains information about the time window for maintenance operations.\n   Field Description      begin  string    Begin is the beginning of the time window in the format HHMMSS+ZONE, e.g. \u0026ldquo;220000+0100\u0026rdquo;. If not present, a random value will be computed.\n    end  string    End is the end of the time window in the format HHMMSS+ZONE, e.g. \u0026ldquo;220000+0100\u0026rdquo;. If not present, the value will be computed based on the \u0026ldquo;Begin\u0026rdquo; value.\n    Monitoring   (Appears on: ShootSpec)  Monitoring contains information about the monitoring configuration for the shoot.\n   Field Description      alerting  Alerting     (Optional) Alerting contains information about the alerting configuration for the shoot cluster.\n    NamedResourceReference   (Appears on: ShootSpec)  NamedResourceReference is a named reference to a resource.\n   Field Description      name  string    Name of the resource reference.\n    resourceRef  Kubernetes autoscaling/v1.CrossVersionObjectReference     ResourceRef is a reference to a resource.\n    Networking   (Appears on: ShootSpec)  Networking defines networking parameters for the shoot cluster.\n   Field Description      type  string    Type identifies the type of the networking plugin.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to network resource.\n    pods  string    (Optional) Pods is the CIDR of the pod network.\n    nodes  string    (Optional) Nodes is the CIDR of the entire node network.\n    services  string    (Optional) Services is the CIDR of the service network.\n    NginxIngress   (Appears on: Addons)  NginxIngress describes configuration values for the nginx-ingress addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     loadBalancerSourceRanges  []string    (Optional) LoadBalancerSourceRanges is list of allowed IP sources for NginxIngress\n    config  map[string]string    (Optional) Config contains custom configuration for the nginx-ingress-controller configuration. See https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/configmap.md#configuration-options\n    externalTrafficPolicy  Kubernetes core/v1.ServiceExternalTrafficPolicyType     (Optional) ExternalTrafficPolicy controls the .spec.externalTrafficPolicy value of the load balancer Service exposing the nginx-ingress. Defaults to Cluster.\n    OIDCConfig   (Appears on: KubeAPIServerConfig)  OIDCConfig contains configuration settings for the OIDC provider. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      caBundle  string    (Optional) If set, the OpenID server\u0026rsquo;s certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host\u0026rsquo;s root CA set will be used.\n    clientAuthentication  OpenIDConnectClientAuthentication     (Optional) ClientAuthentication can optionally contain client configuration used for kubeconfig generation.\n    clientID  string    (Optional) The client ID for the OpenID Connect client, must be set if oidc-issuer-url is set.\n    groupsClaim  string    (Optional) If provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This flag is experimental, please see the authentication documentation for further details.\n    groupsPrefix  string    (Optional) If provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies.\n    issuerURL  string    (Optional) The URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT).\n    requiredClaims  map[string]string    (Optional) ATTENTION: Only meaningful for Kubernetes \u0026gt;= 1.11 key=value pairs that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value.\n    signingAlgs  []string    (Optional) List of allowed JOSE asymmetric signing algorithms. JWTs with a \u0026lsquo;alg\u0026rsquo; header value not in this list will be rejected. Values are defined by RFC 7518 https://tools.ietf.org/html/rfc7518#section-3.1\n    usernameClaim  string    (Optional) The OpenID claim to use as the user name. Note that claims other than the default (\u0026lsquo;sub\u0026rsquo;) is not guaranteed to be unique and immutable. This flag is experimental, please see the authentication documentation for further details. (default \u0026ldquo;sub\u0026rdquo;)\n    usernamePrefix  string    (Optional) If provided, all usernames will be prefixed with this value. If not provided, username claims other than \u0026lsquo;email\u0026rsquo; are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value \u0026lsquo;-\u0026rsquo;.\n    OpenIDConnectClientAuthentication   (Appears on: OIDCConfig)  OpenIDConnectClientAuthentication contains configuration for OIDC clients.\n   Field Description      extraConfig  map[string]string    (Optional) Extra configuration added to kubeconfig\u0026rsquo;s auth-provider. Must not be any of idp-issuer-url, client-id, client-secret, idp-certificate-authority, idp-certificate-authority-data, id-token or refresh-token\n    secret  string    (Optional) The client Secret for the OpenID Connect client.\n    PlantSpec   (Appears on: Plant)  PlantSpec is the specification of a Plant.\n   Field Description      secretRef  Kubernetes core/v1.LocalObjectReference     SecretRef is a reference to a Secret object containing the Kubeconfig of the external kubernetes clusters to be added to Gardener.\n    endpoints  []Endpoint     (Optional) Endpoints is the configuration plant endpoints\n    PlantStatus   (Appears on: Plant)  PlantStatus is the status of a Plant.\n   Field Description      conditions  []Condition     (Optional) Conditions represents the latest available observations of a Plant\u0026rsquo;s current state.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Plant. It corresponds to the Plant\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    clusterInfo  ClusterInfo     ClusterInfo is additional computed information about the newly added cluster (Plant)\n    ProjectMember   (Appears on: ProjectSpec)  ProjectMember is a member of a project.\n   Field Description      Subject  Kubernetes rbac/v1.Subject      (Members of Subject are embedded into this type.) Subject is representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    role  string    Role represents the role of this member. IMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the roles list. TODO: Remove this field in favor of the owner role in v1.\n    roles  []string    (Optional) Roles represents the list of roles of this member.\n    ProjectPhase (string alias)\n  (Appears on: ProjectStatus)  ProjectPhase is a label for the condition of a project at the current time.\nProjectSpec   (Appears on: Project)  ProjectSpec is the specification of a Project.\n   Field Description      createdBy  Kubernetes rbac/v1.Subject     (Optional) CreatedBy is a subject representing a user name, an email address, or any other identifier of a user who created the project.\n    description  string    (Optional) Description is a human-readable description of what the project is used for.\n    owner  Kubernetes rbac/v1.Subject     (Optional) Owner is a subject representing a user name, an email address, or any other identifier of a user owning the project. IMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the owner role. The only way to change the owner will be by moving the owner role. In this API version the only way to change the owner is to use this field. TODO: Remove this field in favor of the owner role in v1.\n    purpose  string    (Optional) Purpose is a human-readable explanation of the project\u0026rsquo;s purpose.\n    members  []ProjectMember     (Optional) Members is a list of subjects representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    namespace  string    (Optional) Namespace is the name of the namespace that has been created for the Project object. A nil value means that Gardener will determine the name of the namespace.\n    tolerations  ProjectTolerations     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n    ProjectStatus   (Appears on: Project)  ProjectStatus holds the most recently observed status of the project.\n   Field Description      observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this project.\n    phase  ProjectPhase     Phase is the current phase of the project.\n    staleSinceTimestamp  Kubernetes meta/v1.Time     (Optional) StaleSinceTimestamp contains the timestamp when the project was first discovered to be stale/unused.\n    staleAutoDeleteTimestamp  Kubernetes meta/v1.Time     (Optional) StaleAutoDeleteTimestamp contains the timestamp when the project will be garbage-collected/automatically deleted because it\u0026rsquo;s stale/unused.\n    ProjectTolerations   (Appears on: ProjectSpec)  ProjectTolerations contains the tolerations for taints on seed clusters.\n   Field Description      defaults  []Toleration     (Optional) Defaults contains a list of tolerations that are added to the shoots in this project by default.\n    whitelist  []Toleration     (Optional) Whitelist contains a list of tolerations that are allowed to be added to the shoots in this project. Please note that this list may only be added by users having the spec-tolerations-whitelist verb for project resources.\n    Provider   (Appears on: ShootSpec)  Provider contains provider-specific information that are handed-over to the provider-specific extension controller.\n   Field Description      type  string    Type is the type of the provider.\n    controlPlaneConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ControlPlaneConfig contains the provider-specific control plane config blob. Please look up the concrete definition in the documentation of your provider extension.\n    infrastructureConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureConfig contains the provider-specific infrastructure config blob. Please look up the concrete definition in the documentation of your provider extension.\n    workers  []Worker     Workers is a list of worker groups.\n    ProxyMode (string alias)\n  (Appears on: KubeProxyConfig)  ProxyMode available in Linux platform: \u0026lsquo;userspace\u0026rsquo; (older, going to be EOL), \u0026lsquo;iptables\u0026rsquo; (newer, faster), \u0026lsquo;ipvs\u0026rsquo; (newest, better in performance and scalability). As of now only \u0026lsquo;iptables\u0026rsquo; and \u0026lsquo;ipvs\u0026rsquo; is supported by Gardener. In Linux platform, if the iptables proxy is selected, regardless of how, but the system\u0026rsquo;s kernel or iptables versions are insufficient, this always falls back to the userspace proxy. IPVS mode will be enabled when proxy mode is set to \u0026lsquo;ipvs\u0026rsquo;, and the fall back path is firstly iptables and then userspace.\nQuotaSpec   (Appears on: Quota)  QuotaSpec is the specification of a Quota.\n   Field Description      clusterLifetimeDays  int32    (Optional) ClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.\n    metrics  Kubernetes core/v1.ResourceList     Metrics is a list of resources which will be put under constraints.\n    scope  Kubernetes core/v1.ObjectReference     Scope is the scope of the Quota object, either \u0026lsquo;project\u0026rsquo; or \u0026lsquo;secret\u0026rsquo;.\n    Region   (Appears on: CloudProfileSpec)  Region contains certain properties of a region.\n   Field Description      name  string    Name is a region name.\n    zones  []AvailabilityZone     (Optional) Zones is a list of availability zones in this region.\n    labels  map[string]string    (Optional) Labels is an optional set of key-value pairs that contain certain administrator-controlled labels for this region. It can be used by Gardener administrators/operators to provide additional information about a region, e.g. wrt quality, reliability, access restrictions, etc.\n    ResourceWatchCacheSize   (Appears on: WatchCacheSizes)  ResourceWatchCacheSize contains configuration of the API server\u0026rsquo;s watch cache size for one specific resource.\n   Field Description      apiGroup  string    (Optional) APIGroup is the API group of the resource for which the watch cache size should be configured. An unset value is used to specify the legacy core API (e.g. for secrets).\n    resource  string    Resource is the name of the resource for which the watch cache size should be configured (in lowercase plural form, e.g. secrets).\n    size  int32    CacheSize specifies the watch cache size that should be configured for the specified resource.\n    SeedBackup   (Appears on: SeedSpec)  SeedBackup contains the object store configuration for backups for shoot (currently only etcd).\n   Field Description      provider  string    Provider is a provider name.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to BackupBucket resource.\n    region  string    (Optional) Region is a region name.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a Secret object containing the cloud provider credentials for the object store where backups should be stored. It should have enough privileges to manipulate the objects as well as buckets.\n    SeedDNS   (Appears on: SeedSpec)  SeedDNS contains DNS-relevant information about this seed cluster.\n   Field Description      ingressDomain  string    (Optional) IngressDomain is the domain of the Seed cluster pointing to the ingress controller endpoint. It will be used to construct ingress URLs for system applications running in Shoot clusters. Once set this field is immutable. This will be removed in the next API version and replaced by spec.ingress.domain.\n    provider  SeedDNSProvider     (Optional) Provider configures a DNSProvider\n    SeedDNSProvider   (Appears on: SeedDNS)  SeedDNSProvider configures a DNSProvider for Seeds\n   Field Description      type  string    Type describes the type of the dns-provider, for example aws-route53\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a Secret object containing cloud provider credentials used for registering external domains.\n    domains  DNSIncludeExclude     (Optional) Domains contains information about which domains shall be included/excluded for this provider.\n    zones  DNSIncludeExclude     (Optional) Zones contains information about which hosted zones shall be included/excluded for this provider.\n    SeedNetworks   (Appears on: SeedSpec)  SeedNetworks contains CIDRs for the pod, service and node networks of a Kubernetes cluster.\n   Field Description      nodes  string    (Optional) Nodes is the CIDR of the node network.\n    pods  string    Pods is the CIDR of the pod network.\n    services  string    Services is the CIDR of the service network.\n    shootDefaults  ShootNetworks     (Optional) ShootDefaults contains the default networks CIDRs for shoots.\n    blockCIDRs  []string    (Optional) BlockCIDRs is a list of network addresses that should be blocked for shoot control plane components running in the seed cluster.\n    SeedProvider   (Appears on: SeedSpec)  SeedProvider defines the provider type and region for this Seed cluster.\n   Field Description      type  string    Type is the name of the provider.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to Seed resource.\n    region  string    Region is a name of a region.\n    SeedSelector   (Appears on: CloudProfileSpec, ShootSpec)  SeedSelector contains constraints for selecting seed to be usable for shoots using a profile\n   Field Description      LabelSelector  Kubernetes meta/v1.LabelSelector      (Members of LabelSelector are embedded into this type.) (Optional) LabelSelector is optional and can be used to select seeds by their label settings\n    providerTypes  []string    (Optional) Providers is optional and can be used by restricting seeds by their provider type. \u0026lsquo;*\u0026rsquo; can be used to enable seeds regardless of their provider type.\n    SeedSettingExcessCapacityReservation   (Appears on: SeedSettings)  SeedSettingExcessCapacityReservation controls the excess capacity reservation for shoot control planes in the seed. When enabled then this is done via PodPriority and requires the Seed cluster to have Kubernetes version 1.11 or the PodPriority feature gate as well as the scheduling.k8s.io/v1alpha1 API group enabled.\n   Field Description      enabled  bool    Enabled controls whether the excess capacity reservation should be enabled.\n    SeedSettingLoadBalancerServices   (Appears on: SeedSettings)  SeedSettingLoadBalancerServices controls certain settings for services of type load balancer that are created in the seed.\n   Field Description      annotations  map[string]string    (Optional) Annotations is a map of annotations that will be injected/merged into every load balancer service object.\n    SeedSettingScheduling   (Appears on: SeedSettings)  SeedSettingScheduling controls settings for scheduling decisions for the seed.\n   Field Description      visible  bool    Visible controls whether the gardener-scheduler shall consider this seed when scheduling shoots. Invisible seeds are not considered by the scheduler.\n    SeedSettingShootDNS   (Appears on: SeedSettings)  SeedSettingShootDNS controls the shoot DNS settings for the seed.\n   Field Description      enabled  bool    Enabled controls whether the DNS for shoot clusters should be enabled. When disabled then all shoots using the seed won\u0026rsquo;t get any DNS providers, DNS records, and no DNS extension controller is required to be installed here. This is useful for environments where DNS is not required.\n    SeedSettingVerticalPodAutoscaler   (Appears on: SeedSettings)  SeedSettingVerticalPodAutoscaler controls certain settings for the vertical pod autoscaler components deployed in the seed.\n   Field Description      enabled  bool    Enabled controls whether the VPA components shall be deployed into the garden namespace in the seed cluster. It is enabled by default because Gardener heavily relies on a VPA being deployed. You should only disable this if your seed cluster already has another, manually/custom managed VPA deployment.\n    SeedSettings   (Appears on: SeedSpec)  SeedSettings contains certain settings for this seed cluster.\n   Field Description      excessCapacityReservation  SeedSettingExcessCapacityReservation     (Optional) ExcessCapacityReservation controls the excess capacity reservation for shoot control planes in the seed.\n    scheduling  SeedSettingScheduling     (Optional) Scheduling controls settings for scheduling decisions for the seed.\n    shootDNS  SeedSettingShootDNS     (Optional) ShootDNS controls the shoot DNS settings for the seed.\n    loadBalancerServices  SeedSettingLoadBalancerServices     (Optional) LoadBalancerServices controls certain settings for services of type load balancer that are created in the seed.\n    verticalPodAutoscaler  SeedSettingVerticalPodAutoscaler     (Optional) VerticalPodAutoscaler controls certain settings for the vertical pod autoscaler components deployed in the seed.\n    SeedSpec   (Appears on: Seed)  SeedSpec is the specification of a Seed.\n   Field Description      backup  SeedBackup     (Optional) Backup holds the object store configuration for the backups of shoot (currently only etcd). If it is not specified, then there won\u0026rsquo;t be any backups taken for shoots associated with this seed. If backup field is present in seed, then backups of the etcd from shoot control plane will be stored under the configured object store.\n    dns  SeedDNS     DNS contains DNS-relevant information about this seed cluster.\n    networks  SeedNetworks     Networks defines the pod, service and worker network of the Seed cluster.\n    provider  SeedProvider     Provider defines the provider type and region for this Seed cluster.\n    secretRef  Kubernetes core/v1.SecretReference     (Optional) SecretRef is a reference to a Secret object containing the Kubeconfig and the cloud provider credentials for the account the Seed cluster has been deployed to.\n    taints  []SeedTaint     (Optional) Taints describes taints on the seed.\n    volume  SeedVolume     (Optional) Volume contains settings for persistentvolumes created in the seed cluster.\n    settings  SeedSettings     (Optional) Settings contains certain settings for this seed cluster.\n    ingress  Ingress     (Optional) Ingress configures Ingress specific settings of the Seed cluster.\n    SeedStatus   (Appears on: Seed)  SeedStatus is the status of a Seed.\n   Field Description      gardener  Gardener     (Optional) Gardener holds information about the Gardener which last acted on the Shoot.\n    kubernetesVersion  string    (Optional) KubernetesVersion is the Kubernetes version of the seed cluster.\n    conditions  []Condition     (Optional) Conditions represents the latest available observations of a Seed\u0026rsquo;s current state.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Seed. It corresponds to the Seed\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    clusterIdentity  string    (Optional) ClusterIdentity is the identity of the Seed cluster\n    capacity  Kubernetes core/v1.ResourceList     (Optional) Capacity represents the total resources of a seed.\n    allocatable  Kubernetes core/v1.ResourceList     (Optional) Allocatable represents the resources of a seed that are available for scheduling. Defaults to Capacity.\n    SeedTaint   (Appears on: SeedSpec)  SeedTaint describes a taint on a seed.\n   Field Description      key  string    Key is the taint key to be applied to a seed.\n    value  string    (Optional) Value is the taint value corresponding to the taint key.\n    SeedVolume   (Appears on: SeedSpec)  SeedVolume contains settings for persistentvolumes created in the seed cluster.\n   Field Description      minimumSize  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) MinimumSize defines the minimum size that should be used for PVCs in the seed.\n    providers  []SeedVolumeProvider     (Optional) Providers is a list of storage class provisioner types for the seed.\n    SeedVolumeProvider   (Appears on: SeedVolume)  SeedVolumeProvider is a storage class provisioner type.\n   Field Description      purpose  string    Purpose is the purpose of this provider.\n    name  string    Name is the name of the storage class provisioner type.\n    ServiceAccountConfig   (Appears on: KubeAPIServerConfig)  ServiceAccountConfig is the kube-apiserver configuration for service accounts.\n   Field Description      issuer  string    (Optional) Issuer is the identifier of the service account token issuer. The issuer will assert this identifier in \u0026ldquo;iss\u0026rdquo; claim of issued tokens. This value is a string or URI. Defaults to URI of the API server.\n    signingKeySecretName  Kubernetes core/v1.LocalObjectReference     (Optional) SigningKeySecret is a reference to a secret that contains an optional private key of the service account token issuer. The issuer will sign issued ID tokens with this private key. Only useful if service account tokens are also issued by another external system.\n    ShootMachineImage   (Appears on: Machine)  ShootMachineImage defines the name and the version of the shoot\u0026rsquo;s machine image in any environment. Has to be defined in the respective CloudProfile.\n   Field Description      name  string    Name is the name of the image.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the shoot\u0026rsquo;s individual configuration passed to an extension resource.\n    version  string    (Optional) Version is the version of the shoot\u0026rsquo;s image. If version is not provided, it will be defaulted to the latest version from the CloudProfile.\n    ShootNetworks   (Appears on: SeedNetworks)  ShootNetworks contains the default networks CIDRs for shoots.\n   Field Description      pods  string    (Optional) Pods is the CIDR of the pod network.\n    services  string    (Optional) Services is the CIDR of the service network.\n    ShootPurpose (string alias)\n  (Appears on: ShootSpec)  ShootPurpose is a type alias for string.\nShootSpec   (Appears on: Shoot)  ShootSpec is the specification of a Shoot.\n   Field Description      addons  Addons     (Optional) Addons contains information about enabled/disabled addons and their configuration.\n    cloudProfileName  string    CloudProfileName is a name of a CloudProfile object.\n    dns  DNS     (Optional) DNS contains information about the DNS settings of the Shoot.\n    extensions  []Extension     (Optional) Extensions contain type and provider information for Shoot extensions.\n    hibernation  Hibernation     (Optional) Hibernation contains information whether the Shoot is suspended or not.\n    kubernetes  Kubernetes     Kubernetes contains the version and configuration settings of the control plane components.\n    networking  Networking     Networking contains information about cluster networking such as CNI Plugin type, CIDRs, \u0026hellip;etc.\n    maintenance  Maintenance     (Optional) Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n    monitoring  Monitoring     (Optional) Monitoring contains information about custom monitoring configurations for the shoot.\n    provider  Provider     Provider contains all provider-specific and provider-relevant information.\n    purpose  ShootPurpose     (Optional) Purpose is the purpose class for this cluster.\n    region  string    Region is a name of a region.\n    secretBindingName  string    SecretBindingName is the name of the a SecretBinding that has a reference to the provider secret. The credentials inside the provider secret will be used to create the shoot in the respective account.\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot.\n    seedSelector  SeedSelector     (Optional) SeedSelector is an optional selector which must match a seed\u0026rsquo;s labels for the shoot to be scheduled on that seed.\n    resources  []NamedResourceReference     (Optional) Resources holds a list of named resource references that can be referred to in extension configs by their names.\n    tolerations  []Toleration     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n    ShootStatus   (Appears on: Shoot)  ShootStatus holds the most recently observed status of the Shoot cluster.\n   Field Description      conditions  []Condition     (Optional) Conditions represents the latest available observations of a Shoots\u0026rsquo;s current state.\n    constraints  []Condition     (Optional) Constraints represents conditions of a Shoot\u0026rsquo;s current state that constraint some operations on it.\n    gardener  Gardener     Gardener holds information about the Gardener which last acted on the Shoot.\n    hibernated  bool    IsHibernated indicates whether the Shoot is currently hibernated.\n    lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the Shoot.\n    lastErrors  []LastError     (Optional) LastErrors holds information about the last occurred error(s) during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Shoot. It corresponds to the Shoot\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    retryCycleStartTime  Kubernetes meta/v1.Time     (Optional) RetryCycleStartTime is the start time of the last retry cycle (used to determine how often an operation must be retried until we give up).\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot. This value is only written after a successful create/reconcile operation. It will be used when control planes are moved between Seeds.\n    technicalID  string    TechnicalID is the name that is used for creating the Seed namespace, the infrastructure resources, and basically everything that is related to this particular Shoot.\n    uid  k8s.io/apimachinery/pkg/types.UID     UID is a unique identifier for the Shoot cluster to avoid portability between Kubernetes clusters. It is used to compute unique hashes.\n    clusterIdentity  string    (Optional) ClusterIdentity is the identity of the Shoot cluster\n    Toleration   (Appears on: ProjectTolerations, ShootSpec)  Toleration is a toleration for a seed taint.\n   Field Description      key  string    Key is the toleration key to be applied to a project or shoot.\n    value  string    (Optional) Value is the toleration value corresponding to the toleration key.\n    VersionClassification (string alias)\n  (Appears on: ExpirableVersion)  VersionClassification is the logical state of a version according to https://github.com/gardener/gardener/blob/master/docs/operations/versioning.md\nVerticalPodAutoscaler   (Appears on: Kubernetes)  VerticalPodAutoscaler contains the configuration flags for the Kubernetes vertical pod autoscaler.\n   Field Description      enabled  bool    Enabled specifies whether the Kubernetes VPA shall be enabled for the shoot cluster.\n    evictAfterOOMThreshold  Kubernetes meta/v1.Duration     (Optional) EvictAfterOOMThreshold defines the threshold that will lead to pod eviction in case it OOMed in less than the given threshold since its start and if it has only one container (default: 10m0s).\n    evictionRateBurst  int32    (Optional) EvictionRateBurst defines the burst of pods that can be evicted (default: 1)\n    evictionRateLimit  float64    (Optional) EvictionRateLimit defines the number of pods that can be evicted per second. A rate limit set to 0 or -1 will disable the rate limiter (default: -1).\n    evictionTolerance  float64    (Optional) EvictionTolerance defines the fraction of replica count that can be evicted for update in case more than one pod can be evicted (default: 0.5).\n    recommendationMarginFraction  float64    (Optional) RecommendationMarginFraction is the fraction of usage added as the safety margin to the recommended request (default: 0.15).\n    updaterInterval  Kubernetes meta/v1.Duration     (Optional) UpdaterInterval is the interval how often the updater should run (default: 1m0s).\n    recommenderInterval  Kubernetes meta/v1.Duration     (Optional) RecommenderInterval is the interval how often metrics should be fetched (default: 1m0s).\n    Volume   (Appears on: Worker)  Volume contains information about the volume type, size, and encryption.\n   Field Description      name  string    (Optional) Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    VolumeSize is the size of the volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    VolumeType   (Appears on: CloudProfileSpec)  VolumeType contains certain properties of a volume type.\n   Field Description      class  string    Class is the class of the volume type.\n    name  string    Name is the name of the volume type.\n    usable  bool    (Optional) Usable defines if the volume type can be used for shoot clusters.\n    WatchCacheSizes   (Appears on: KubeAPIServerConfig)  WatchCacheSizes contains configuration of the API server\u0026rsquo;s watch cache sizes.\n   Field Description      default  int32    (Optional) Default configures the default watch cache size of the kube-apiserver (flag --default-watch-cache-size, defaults to 100). See: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/\n    resources  []ResourceWatchCacheSize     (Optional) Resources configures the watch cache size of the kube-apiserver per resource (flag --watch-cache-sizes). See: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/\n    Worker   (Appears on: Provider)  Worker is the base definition of a worker group.\n   Field Description      annotations  map[string]string    (Optional) Annotations is a map of key/value pairs for annotations for all the Node objects in this worker pool.\n    caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every machine of this worker pool.\n    cri  CRI     (Optional) CRI contains configurations of CRI support of every machine in the worker pool\n    kubernetes  WorkerKubernetes     (Optional) Kubernetes contains configuration for Kubernetes components related to this worker pool.\n    labels  map[string]string    (Optional) Labels is a map of key/value pairs for labels for all the Node objects in this worker pool.\n    name  string    Name is the name of the worker group.\n    machine  Machine     Machine contains information about the machine type and image.\n    maximum  int32    Maximum is the maximum number of VMs to create.\n    minimum  int32    Minimum is the minimum number of VMs to create.\n    maxSurge  k8s.io/apimachinery/pkg/util/intstr.IntOrString     (Optional) MaxSurge is maximum number of VMs that are created during an update.\n    maxUnavailable  k8s.io/apimachinery/pkg/util/intstr.IntOrString     (Optional) MaxUnavailable is the maximum number of VMs that can be unavailable during an update.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the provider-specific configuration for this worker pool.\n    taints  []Kubernetes core/v1.Taint     (Optional) Taints is a list of taints for all the Node objects in this worker pool.\n    volume  Volume     (Optional) Volume contains information about the volume type and size.\n    dataVolumes  []DataVolume     (Optional) DataVolumes contains a list of additional worker volumes.\n    kubeletDataVolumeName  string    (Optional) KubeletDataVolumeName contains the name of a dataVolume that should be used for storing kubelet state.\n    zones  []string    (Optional) Zones is a list of availability zones that are used to evenly distribute this worker pool. Optional as not every provider may support availability zones.\n    systemComponents  WorkerSystemComponents     (Optional) SystemComponents contains configuration for system components related to this worker pool\n    machineControllerManager  MachineControllerManagerSettings     (Optional) MachineControllerManagerSettings contains configurations for different worker-pools. Eg. MachineDrainTimeout, MachineHealthTimeout.\n    WorkerKubernetes   (Appears on: Worker)  WorkerKubernetes contains configuration for Kubernetes components related to this worker pool.\n   Field Description      kubelet  KubeletConfig     (Optional) Kubelet contains configuration settings for all kubelets of this worker pool.\n    WorkerSystemComponents   (Appears on: Worker)  WorkerSystemComponents contains configuration for system components related to this worker pool\n   Field Description      allow  bool    Allow determines whether the pool should be allowed to host system components or not (defaults to true)\n      Generated with gen-crd-api-reference-docs \n"},{"uri":"https://gardener.cloud/v1.13.2/references/core/","title":"Core","tags":[],"description":"","content":"Packages:\n  core.gardener.cloud/v1beta1   core.gardener.cloud/v1beta1  Package v1beta1 is a version of the API.\nResource Types:  BackupBucket  BackupEntry  CloudProfile  ControllerInstallation  ControllerRegistration  Plant  Project  Quota  SecretBinding  Seed  Shoot  BackupBucket   BackupBucket holds details about backup bucket\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  BackupBucket    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupBucketSpec     Specification of the Backup Bucket.\n     provider  BackupBucketProvider     Provider hold the details of cloud provider of the object store.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to BackupBucket resource.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupBucket for running controller.\n       status  BackupBucketStatus     Most recently observed status of the Backup Bucket.\n    BackupEntry   BackupEntry holds details about shoot backup.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  BackupEntry    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupEntrySpec     (Optional) Spec contains the specification of the Backup Entry.\n     bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupEntry for running controller.\n       status  BackupEntryStatus     (Optional) Status contains the most recently observed status of the Backup Entry.\n    CloudProfile   CloudProfile represents certain properties about a provider environment.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  CloudProfile    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  CloudProfileSpec     (Optional) Spec defines the provider environment properties.\n     caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every host machine of shoot cluster targeting this profile.\n    kubernetes  KubernetesSettings     Kubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []MachineType     MachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains provider-specific configuration for the profile.\n    regions  []Region     Regions contains constraints regarding allowed values for regions and zones.\n    seedSelector  SeedSelector     (Optional) SeedSelector contains an optional list of labels on Seed resources that marks those seeds whose shoots may use this provider profile. An empty list means that all seeds of the same provider type are supported. This is useful for environments that are of the same type (like openstack) but may have different \u0026ldquo;instances\u0026rdquo;/landscapes. Optionally a list of possible providers can be added to enable cross-provider scheduling. By default, the provider type of the seed must match the shoot\u0026rsquo;s provider.\n    type  string    Type is the name of the provider.\n    volumeTypes  []VolumeType     (Optional) VolumeTypes contains constraints regarding allowed values for volume types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n       ControllerInstallation   ControllerInstallation represents an installation request for an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  ControllerInstallation    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControllerInstallationSpec     Spec contains the specification of this installation.\n     registrationRef  Kubernetes core/v1.ObjectReference     RegistrationRef is used to reference a ControllerRegistration resources.\n    seedRef  Kubernetes core/v1.ObjectReference     SeedRef is used to reference a Seed resources.\n       status  ControllerInstallationStatus     Status contains the status of this installation.\n    ControllerRegistration   ControllerRegistration represents a registration of an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  ControllerRegistration    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControllerRegistrationSpec     Spec contains the specification of this registration.\n     resources  []ControllerResource     (Optional) Resources is a list of combinations of kinds (DNSProvider, Infrastructure, Generic, \u0026hellip;) and their actual types (aws-route53, gcp, auditlog, \u0026hellip;).\n    deployment  ControllerDeployment     (Optional) Deployment contains information for how this controller is deployed.\n       Plant      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Plant    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  PlantSpec     Spec contains the specification of this Plant.\n     secretRef  Kubernetes core/v1.LocalObjectReference     SecretRef is a reference to a Secret object containing the Kubeconfig of the external kubernetes clusters to be added to Gardener.\n    endpoints  []Endpoint     (Optional) Endpoints is the configuration plant endpoints\n       status  PlantStatus     Status contains the status of this Plant.\n    Project   Project holds certain properties about a Gardener project.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Project    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ProjectSpec     (Optional) Spec defines the project properties.\n     createdBy  Kubernetes rbac/v1.Subject     (Optional) CreatedBy is a subject representing a user name, an email address, or any other identifier of a user who created the project.\n    description  string    (Optional) Description is a human-readable description of what the project is used for.\n    owner  Kubernetes rbac/v1.Subject     (Optional) Owner is a subject representing a user name, an email address, or any other identifier of a user owning the project. IMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the owner role. The only way to change the owner will be by moving the owner role. In this API version the only way to change the owner is to use this field. TODO: Remove this field in favor of the owner role in v1.\n    purpose  string    (Optional) Purpose is a human-readable explanation of the project\u0026rsquo;s purpose.\n    members  []ProjectMember     (Optional) Members is a list of subjects representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    namespace  string    (Optional) Namespace is the name of the namespace that has been created for the Project object. A nil value means that Gardener will determine the name of the namespace.\n    tolerations  ProjectTolerations     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n       status  ProjectStatus     (Optional) Most recently observed status of the Project.\n    Quota      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Quota    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  QuotaSpec     (Optional) Spec defines the Quota constraints.\n     clusterLifetimeDays  int32    (Optional) ClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.\n    metrics  Kubernetes core/v1.ResourceList     Metrics is a list of resources which will be put under constraints.\n    scope  Kubernetes core/v1.ObjectReference     Scope is the scope of the Quota object, either \u0026lsquo;project\u0026rsquo; or \u0026lsquo;secret\u0026rsquo;.\n       SecretBinding      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  SecretBinding    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret object in the same or another namespace.\n    quotas  []Kubernetes core/v1.ObjectReference     (Optional) Quotas is a list of references to Quota objects in the same or another namespace.\n    Seed   Seed represents an installation request for an external controller.\n   Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Seed    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  SeedSpec     Spec contains the specification of this installation.\n     backup  SeedBackup     (Optional) Backup holds the object store configuration for the backups of shoot (currently only etcd). If it is not specified, then there won\u0026rsquo;t be any backups taken for shoots associated with this seed. If backup field is present in seed, then backups of the etcd from shoot control plane will be stored under the configured object store.\n    dns  SeedDNS     DNS contains DNS-relevant information about this seed cluster.\n    networks  SeedNetworks     Networks defines the pod, service and worker network of the Seed cluster.\n    provider  SeedProvider     Provider defines the provider type and region for this Seed cluster.\n    secretRef  Kubernetes core/v1.SecretReference     (Optional) SecretRef is a reference to a Secret object containing the Kubeconfig and the cloud provider credentials for the account the Seed cluster has been deployed to.\n    taints  []SeedTaint     (Optional) Taints describes taints on the seed.\n    volume  SeedVolume     (Optional) Volume contains settings for persistentvolumes created in the seed cluster.\n    settings  SeedSettings     (Optional) Settings contains certain settings for this seed cluster.\n    ingress  Ingress     (Optional) Ingress configures Ingress specific settings of the Seed cluster.\n       status  SeedStatus     Status contains the status of this installation.\n    Shoot      Field Description      apiVersion string   core.gardener.cloud/v1beta1      kind string  Shoot    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ShootSpec     (Optional) Specification of the Shoot cluster.\n     addons  Addons     (Optional) Addons contains information about enabled/disabled addons and their configuration.\n    cloudProfileName  string    CloudProfileName is a name of a CloudProfile object.\n    dns  DNS     (Optional) DNS contains information about the DNS settings of the Shoot.\n    extensions  []Extension     (Optional) Extensions contain type and provider information for Shoot extensions.\n    hibernation  Hibernation     (Optional) Hibernation contains information whether the Shoot is suspended or not.\n    kubernetes  Kubernetes     Kubernetes contains the version and configuration settings of the control plane components.\n    networking  Networking     Networking contains information about cluster networking such as CNI Plugin type, CIDRs, \u0026hellip;etc.\n    maintenance  Maintenance     (Optional) Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n    monitoring  Monitoring     (Optional) Monitoring contains information about custom monitoring configurations for the shoot.\n    provider  Provider     Provider contains all provider-specific and provider-relevant information.\n    purpose  ShootPurpose     (Optional) Purpose is the purpose class for this cluster.\n    region  string    Region is a name of a region.\n    secretBindingName  string    SecretBindingName is the name of the a SecretBinding that has a reference to the provider secret. The credentials inside the provider secret will be used to create the shoot in the respective account.\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot.\n    seedSelector  SeedSelector     (Optional) SeedSelector is an optional selector which must match a seed\u0026rsquo;s labels for the shoot to be scheduled on that seed.\n    resources  []NamedResourceReference     (Optional) Resources holds a list of named resource references that can be referred to in extension configs by their names.\n    tolerations  []Toleration     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n       status  ShootStatus     (Optional) Most recently observed status of the Shoot cluster.\n    Addon   (Appears on: KubernetesDashboard, NginxIngress)  Addon allows enabling or disabling a specific addon and is used to derive from.\n   Field Description      enabled  bool    Enabled indicates whether the addon is enabled or not.\n    Addons   (Appears on: ShootSpec)  Addons is a collection of configuration for specific addons which are managed by the Gardener.\n   Field Description      kubernetesDashboard  KubernetesDashboard     (Optional) KubernetesDashboard holds configuration settings for the kubernetes dashboard addon.\n    nginxIngress  NginxIngress     (Optional) NginxIngress holds configuration settings for the nginx-ingress addon.\n    AdmissionPlugin   (Appears on: KubeAPIServerConfig)  AdmissionPlugin contains information about a specific admission plugin and its corresponding configuration.\n   Field Description      name  string    Name is the name of the plugin.\n    config  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) Config is the configuration of the plugin.\n    Alerting   (Appears on: Monitoring)  Alerting contains information about how alerting will be done (i.e. who will receive alerts and how).\n   Field Description      emailReceivers  []string    (Optional) MonitoringEmailReceivers is a list of recipients for alerts\n    AuditConfig   (Appears on: KubeAPIServerConfig)  AuditConfig contains settings for audit of the api server\n   Field Description      auditPolicy  AuditPolicy     (Optional) AuditPolicy contains configuration settings for audit policy of the kube-apiserver.\n    AuditPolicy   (Appears on: AuditConfig)  AuditPolicy contains audit policy for kube-apiserver\n   Field Description      configMapRef  Kubernetes core/v1.ObjectReference     (Optional) ConfigMapRef is a reference to a ConfigMap object in the same namespace, which contains the audit policy for the kube-apiserver.\n    AvailabilityZone   (Appears on: Region)  AvailabilityZone is an availability zone.\n   Field Description      name  string    Name is an an availability zone name.\n    unavailableMachineTypes  []string    (Optional) UnavailableMachineTypes is a list of machine type names that are not availability in this zone.\n    unavailableVolumeTypes  []string    (Optional) UnavailableVolumeTypes is a list of volume type names that are not availability in this zone.\n    BackupBucketProvider   (Appears on: BackupBucketSpec)  BackupBucketProvider holds the details of cloud provider of the object store.\n   Field Description      type  string    Type is the type of provider.\n    region  string    Region is the region of the bucket.\n    BackupBucketSpec   (Appears on: BackupBucket)  BackupBucketSpec is the specification of a Backup Bucket.\n   Field Description      provider  BackupBucketProvider     Provider hold the details of cloud provider of the object store.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to BackupBucket resource.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupBucket for running controller.\n    BackupBucketStatus   (Appears on: BackupBucket)  BackupBucketStatus holds the most recently observed status of the Backup Bucket.\n   Field Description      providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus is the configuration passed to BackupBucket resource.\n    lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the BackupBucket.\n    lastError  LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this BackupBucket. It corresponds to the BackupBucket\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    generatedSecretRef  Kubernetes core/v1.SecretReference     (Optional) GeneratedSecretRef is reference to the secret generated by backup bucket, which will have object store specific credentials.\n    BackupEntrySpec   (Appears on: BackupEntry)  BackupEntrySpec is the specification of a Backup Entry.\n   Field Description      bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    seedName  string    (Optional) SeedName holds the name of the seed allocated to BackupEntry for running controller.\n    BackupEntryStatus   (Appears on: BackupEntry)  BackupEntryStatus holds the most recently observed status of the Backup Entry.\n   Field Description      lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the BackupEntry.\n    lastError  LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this BackupEntry. It corresponds to the BackupEntry\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    CRI   (Appears on: MachineImageVersion, Worker)  CRI contains information about the Container Runtimes.\n   Field Description      name  CRIName     The name of the CRI library\n    containerRuntimes  []ContainerRuntime     (Optional) ContainerRuntimes is the list of the required container runtimes supported for a worker pool.\n    CRIName (string alias)\n  (Appears on: CRI)  CRIName is a type alias for the CRI name string.\nCloudInfo   (Appears on: ClusterInfo)  CloudInfo contains information about the cloud\n   Field Description      type  string    Type is the cloud type\n    region  string    Region is the cloud region\n    CloudProfileSpec   (Appears on: CloudProfile)  CloudProfileSpec is the specification of a CloudProfile. It must contain exactly one of its defined keys.\n   Field Description      caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every host machine of shoot cluster targeting this profile.\n    kubernetes  KubernetesSettings     Kubernetes contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n    machineImages  []MachineImage     MachineImages contains constraints regarding allowed values for machine images in the Shoot specification.\n    machineTypes  []MachineType     MachineTypes contains constraints regarding allowed values for machine types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains provider-specific configuration for the profile.\n    regions  []Region     Regions contains constraints regarding allowed values for regions and zones.\n    seedSelector  SeedSelector     (Optional) SeedSelector contains an optional list of labels on Seed resources that marks those seeds whose shoots may use this provider profile. An empty list means that all seeds of the same provider type are supported. This is useful for environments that are of the same type (like openstack) but may have different \u0026ldquo;instances\u0026rdquo;/landscapes. Optionally a list of possible providers can be added to enable cross-provider scheduling. By default, the provider type of the seed must match the shoot\u0026rsquo;s provider.\n    type  string    Type is the name of the provider.\n    volumeTypes  []VolumeType     (Optional) VolumeTypes contains constraints regarding allowed values for volume types in the \u0026lsquo;workers\u0026rsquo; block in the Shoot specification.\n    ClusterAutoscaler   (Appears on: Kubernetes)  ClusterAutoscaler contains the configuration flags for the Kubernetes cluster autoscaler.\n   Field Description      scaleDownDelayAfterAdd  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterAdd defines how long after scale up that scale down evaluation resumes (default: 1 hour).\n    scaleDownDelayAfterDelete  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterDelete how long after node deletion that scale down evaluation resumes, defaults to scanInterval (defaults to ScanInterval).\n    scaleDownDelayAfterFailure  Kubernetes meta/v1.Duration     (Optional) ScaleDownDelayAfterFailure how long after scale down failure that scale down evaluation resumes (default: 3 mins).\n    scaleDownUnneededTime  Kubernetes meta/v1.Duration     (Optional) ScaleDownUnneededTime defines how long a node should be unneeded before it is eligible for scale down (default: 30 mins).\n    scaleDownUtilizationThreshold  float64    (Optional) ScaleDownUtilizationThreshold defines the threshold in % under which a node is being removed\n    scanInterval  Kubernetes meta/v1.Duration     (Optional) ScanInterval how often cluster is reevaluated for scale up or down (default: 10 secs).\n    ClusterInfo   (Appears on: PlantStatus)  ClusterInfo contains information about the Plant cluster\n   Field Description      cloud  CloudInfo     Cloud describes the cloud information\n    kubernetes  KubernetesInfo     Kubernetes describes kubernetes meta information (e.g., version)\n    Condition   (Appears on: ControllerInstallationStatus, PlantStatus, SeedStatus, ShootStatus)  Condition holds the information about the state of a resource.\n   Field Description      type  ConditionType     Type of the Shoot condition.\n    status  ConditionStatus     Status of the condition, one of True, False, Unknown.\n    lastTransitionTime  Kubernetes meta/v1.Time     Last time the condition transitioned from one status to another.\n    lastUpdateTime  Kubernetes meta/v1.Time     Last time the condition was updated.\n    reason  string    The reason for the condition\u0026rsquo;s last transition.\n    message  string    A human readable message indicating details about the transition.\n    codes  []ErrorCode     (Optional) Well-defined error codes in case the condition reports a problem.\n    ConditionStatus (string alias)\n  (Appears on: Condition)  ConditionStatus is the status of a condition.\nConditionType (string alias)\n  (Appears on: Condition)  ConditionType is a string alias.\nContainerRuntime   (Appears on: CRI)  ContainerRuntime contains information about worker\u0026rsquo;s available container runtime\n   Field Description      type  string    Type is the type of the Container Runtime.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to container runtime resource.\n    ControllerDeployment   (Appears on: ControllerRegistrationSpec)  ControllerDeployment contains information for how this controller is deployed.\n   Field Description      type  string    Type is the deployment type.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig contains type-specific configuration.\n    policy  ControllerDeploymentPolicy     (Optional) Policy controls how the controller is deployed. It defaults to \u0026lsquo;OnDemand\u0026rsquo;.\n    seedSelector  Kubernetes meta/v1.LabelSelector     (Optional) SeedSelector contains an optional label selector for seeds. Only if the labels match then this controller will be considered for a deployment. An empty list means that all seeds are selected.\n    ControllerDeploymentPolicy (string alias)\n  (Appears on: ControllerDeployment)  ControllerDeploymentPolicy is a string alias.\nControllerInstallationSpec   (Appears on: ControllerInstallation)  ControllerInstallationSpec is the specification of a ControllerInstallation.\n   Field Description      registrationRef  Kubernetes core/v1.ObjectReference     RegistrationRef is used to reference a ControllerRegistration resources.\n    seedRef  Kubernetes core/v1.ObjectReference     SeedRef is used to reference a Seed resources.\n    ControllerInstallationStatus   (Appears on: ControllerInstallation)  ControllerInstallationStatus is the status of a ControllerInstallation.\n   Field Description      conditions  []Condition     (Optional) Conditions represents the latest available observations of a ControllerInstallations\u0026rsquo;s current state.\n    providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus contains type-specific status.\n    ControllerRegistrationSpec   (Appears on: ControllerRegistration)  ControllerRegistrationSpec is the specification of a ControllerRegistration.\n   Field Description      resources  []ControllerResource     (Optional) Resources is a list of combinations of kinds (DNSProvider, Infrastructure, Generic, \u0026hellip;) and their actual types (aws-route53, gcp, auditlog, \u0026hellip;).\n    deployment  ControllerDeployment     (Optional) Deployment contains information for how this controller is deployed.\n    ControllerResource   (Appears on: ControllerRegistrationSpec)  ControllerResource is a combination of a kind (DNSProvider, Infrastructure, Generic, \u0026hellip;) and the actual type for this kind (aws-route53, gcp, auditlog, \u0026hellip;).\n   Field Description      kind  string    Kind is the resource kind, for example \u0026ldquo;OperatingSystemConfig\u0026rdquo;.\n    type  string    Type is the resource type, for example \u0026ldquo;coreos\u0026rdquo; or \u0026ldquo;ubuntu\u0026rdquo;.\n    globallyEnabled  bool    (Optional) GloballyEnabled determines if this ControllerResource is required by all Shoot clusters.\n    reconcileTimeout  Kubernetes meta/v1.Duration     (Optional) ReconcileTimeout defines how long Gardener should wait for the resource reconciliation.\n    primary  bool    (Optional) Primary determines if the controller backed by this ControllerRegistration is responsible for the extension resource\u0026rsquo;s lifecycle. This field defaults to true. There must be exactly one primary controller for this kind/type combination.\n    DNS   (Appears on: ShootSpec)  DNS holds information about the provider, the hosted zone id and the domain.\n   Field Description      domain  string    (Optional) Domain is the external available domain of the Shoot cluster. This domain will be written into the kubeconfig that is handed out to end-users. Once set it is immutable.\n    providers  []DNSProvider     (Optional) Providers is a list of DNS providers that shall be enabled for this shoot cluster. Only relevant if not a default domain is used.\n    DNSIncludeExclude   (Appears on: DNSProvider, SeedDNSProvider)     Field Description      include  []string    (Optional) Include is a list of resources that shall be included.\n    exclude  []string    (Optional) Exclude is a list of resources that shall be excluded.\n    DNSProvider   (Appears on: DNS)  DNSProvider contains information about a DNS provider.\n   Field Description      domains  DNSIncludeExclude     (Optional) Domains contains information about which domains shall be included/excluded for this provider.\n    primary  bool    (Optional) Primary indicates that this DNSProvider is used for shoot related domains.\n    secretName  string    (Optional) SecretName is a name of a secret containing credentials for the stated domain and the provider. When not specified, the Gardener will use the cloud provider credentials referenced by the Shoot and try to find respective credentials there (primary provider only). Specifying this field may override this behavior, i.e. forcing the Gardener to only look into the given secret.\n    type  string    (Optional) Type is the DNS provider type.\n    zones  DNSIncludeExclude     (Optional) Zones contains information about which hosted zones shall be included/excluded for this provider.\n    DataVolume   (Appears on: Worker)  DataVolume contains information about a data volume.\n   Field Description      name  string    Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    VolumeSize is the size of the volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    Endpoint   (Appears on: PlantSpec)  Endpoint is an endpoint for monitoring, logging and other services around the plant.\n   Field Description      name  string    Name is the name of the endpoint\n    url  string    URL is the url of the endpoint\n    purpose  string    Purpose is the purpose of the endpoint\n    ErrorCode (string alias)\n  (Appears on: Condition, LastError)  ErrorCode is a string alias.\nExpirableVersion   (Appears on: KubernetesSettings, MachineImageVersion)  ExpirableVersion contains a version and an expiration date.\n   Field Description      version  string    Version is the version identifier.\n    expirationDate  Kubernetes meta/v1.Time     (Optional) ExpirationDate defines the time at which this version expires.\n    classification  VersionClassification     (Optional) Classification defines the state of a version (preview, supported, deprecated)\n    Extension   (Appears on: ShootSpec)  Extension contains type and provider information for Shoot extensions.\n   Field Description      type  string    Type is the type of the extension resource.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to extension resource.\n    disabled  bool    (Optional) Disabled allows to disable extensions that were marked as \u0026lsquo;globally enabled\u0026rsquo; by Gardener administrators.\n    Gardener   (Appears on: SeedStatus, ShootStatus)  Gardener holds the information about the Gardener version that operated a resource.\n   Field Description      id  string    ID is the Docker container id of the Gardener which last acted on a resource.\n    name  string    Name is the hostname (pod name) of the Gardener which last acted on a resource.\n    version  string    Version is the version of the Gardener which last acted on a resource.\n    Hibernation   (Appears on: ShootSpec)  Hibernation contains information whether the Shoot is suspended or not.\n   Field Description      enabled  bool    (Optional) Enabled specifies whether the Shoot needs to be hibernated or not. If it is true, the Shoot\u0026rsquo;s desired state is to be hibernated. If it is false or nil, the Shoot\u0026rsquo;s desired state is to be awaken.\n    schedules  []HibernationSchedule     (Optional) Schedules determine the hibernation schedules.\n    HibernationSchedule   (Appears on: Hibernation)  HibernationSchedule determines the hibernation schedule of a Shoot. A Shoot will be regularly hibernated at each start time and will be woken up at each end time. Start or End can be omitted, though at least one of each has to be specified.\n   Field Description      start  string    (Optional) Start is a Cron spec at which time a Shoot will be hibernated.\n    end  string    (Optional) End is a Cron spec at which time a Shoot will be woken up.\n    location  string    (Optional) Location is the time location in which both start and and shall be evaluated.\n    HorizontalPodAutoscalerConfig   (Appears on: KubeControllerManagerConfig)  HorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      cpuInitializationPeriod  Kubernetes meta/v1.Duration     (Optional) The period after which a ready pod transition is considered to be the first.\n    downscaleDelay  Kubernetes meta/v1.Duration     (Optional) The period since last downscale, before another downscale can be performed in horizontal pod autoscaler.\n    downscaleStabilization  Kubernetes meta/v1.Duration     (Optional) The configurable window at which the controller will choose the highest recommendation for autoscaling.\n    initialReadinessDelay  Kubernetes meta/v1.Duration     (Optional) The configurable period at which the horizontal pod autoscaler considers a Pod not yet ready given that its unready and it has transitioned to unready during that time.\n    syncPeriod  Kubernetes meta/v1.Duration     (Optional) The period for syncing the number of pods in horizontal pod autoscaler.\n    tolerance  float64    (Optional) The minimum change (from 1.0) in the desired-to-actual metrics ratio for the horizontal pod autoscaler to consider scaling.\n    upscaleDelay  Kubernetes meta/v1.Duration     (Optional) The period since last upscale, before another upscale can be performed in horizontal pod autoscaler.\n    Ingress   (Appears on: SeedSpec)  Ingress configures the Ingress specific settings of the Seed cluster\n   Field Description      domain  string    Domain specifies the IngressDomain of the Seed cluster pointing to the ingress controller endpoint. It will be used to construct ingress URLs for system applications running in Shoot clusters. Once set this field is immutable.\n    controller  IngressController     Controller configures a Gardener managed Ingress Controller listening on the ingressDomain\n    IngressController   (Appears on: Ingress)  IngressController enables a Gardener managed Ingress Controller listening on the ingressDomain\n   Field Description      kind  string    Kind defines which kind of IngressController to use, for example nginx\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig specifies infrastructure specific configuration for the ingressController\n    KubeAPIServerConfig   (Appears on: Kubernetes)  KubeAPIServerConfig contains configuration settings for the kube-apiserver.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     admissionPlugins  []AdmissionPlugin     (Optional) AdmissionPlugins contains the list of user-defined admission plugins (additional to those managed by Gardener), and, if desired, the corresponding configuration.\n    apiAudiences  []string    (Optional) APIAudiences are the identifiers of the API. The service account token authenticator will validate that tokens used against the API are bound to at least one of these audiences. Defaults to [\u0026ldquo;kubernetes\u0026rdquo;].\n    auditConfig  AuditConfig     (Optional) AuditConfig contains configuration settings for the audit of the kube-apiserver.\n    enableBasicAuthentication  bool    (Optional) EnableBasicAuthentication defines whether basic authentication should be enabled for this cluster or not.\n    oidcConfig  OIDCConfig     (Optional) OIDCConfig contains configuration settings for the OIDC provider.\n    runtimeConfig  map[string]bool    (Optional) RuntimeConfig contains information about enabled or disabled APIs.\n    serviceAccountConfig  ServiceAccountConfig     (Optional) ServiceAccountConfig contains configuration settings for the service account handling of the kube-apiserver.\n    watchCacheSizes  WatchCacheSizes     (Optional) WatchCacheSizes contains configuration of the API server\u0026rsquo;s watch cache sizes. Configuring these flags might be useful for large-scale Shoot clusters with a lot of parallel update requests and a lot of watching controllers (e.g. large shooted Seed clusters). When the API server\u0026rsquo;s watch cache\u0026rsquo;s capacity is too small to cope with the amount of update requests and watchers for a particular resource, it might happen that controller watches are permanently stopped with too old resource version errors. Starting from kubernetes v1.19, the API server\u0026rsquo;s watch cache size is adapted dynamically and setting the watch cache size flags will have no effect, except when setting it to 0 (which disables the watch cache).\n    requests  KubeAPIServerRequests     (Optional) Requests contains configuration for request-specific settings for the kube-apiserver.\n    KubeAPIServerRequests   (Appears on: KubeAPIServerConfig)  KubeAPIServerRequests contains configuration for request-specific settings for the kube-apiserver.\n   Field Description      maxNonMutatingInflight  int32    (Optional) MaxNonMutatingInflight is the maximum number of non-mutating requests in flight at a given time. When the server exceeds this, it rejects requests.\n    maxMutatingInflight  int32    (Optional) MaxMutatingInflight is the maximum number of mutating requests in flight at a given time. When the server exceeds this, it rejects requests.\n    KubeControllerManagerConfig   (Appears on: Kubernetes)  KubeControllerManagerConfig contains configuration settings for the kube-controller-manager.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     horizontalPodAutoscaler  HorizontalPodAutoscalerConfig     (Optional) HorizontalPodAutoscalerConfig contains horizontal pod autoscaler configuration settings for the kube-controller-manager.\n    nodeCIDRMaskSize  int32    (Optional) NodeCIDRMaskSize defines the mask size for node cidr in cluster (default is 24)\n    podEvictionTimeout  Kubernetes meta/v1.Duration     (Optional) PodEvictionTimeout defines the grace period for deleting pods on failed nodes. Defaults to 2m.\n    KubeProxyConfig   (Appears on: Kubernetes)  KubeProxyConfig contains configuration settings for the kube-proxy.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     mode  ProxyMode     (Optional) Mode specifies which proxy mode to use. defaults to IPTables.\n    KubeSchedulerConfig   (Appears on: Kubernetes)  KubeSchedulerConfig contains configuration settings for the kube-scheduler.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     kubeMaxPDVols  string    (Optional) KubeMaxPDVols allows to configure the KUBE_MAX_PD_VOLS environment variable for the kube-scheduler. Please find more information here: https://kubernetes.io/docs/concepts/storage/storage-limits/#custom-limits Note that using this field is considered alpha-/experimental-level and is on your own risk. You should be aware of all the side-effects and consequences when changing it.\n    KubeletConfig   (Appears on: Kubernetes, WorkerKubernetes)  KubeletConfig contains configuration settings for the kubelet.\n   Field Description      KubernetesConfig  KubernetesConfig      (Members of KubernetesConfig are embedded into this type.)     cpuCFSQuota  bool    (Optional) CPUCFSQuota allows you to disable/enable CPU throttling for Pods.\n    cpuManagerPolicy  string    (Optional) CPUManagerPolicy allows to set alternative CPU management policies (default: none).\n    evictionHard  KubeletConfigEviction     (Optional) EvictionHard describes a set of eviction thresholds (e.g. memory.available   evictionMaxPodGracePeriod  int32    (Optional) EvictionMaxPodGracePeriod describes the maximum allowed grace period (in seconds) to use when terminating pods in response to a soft eviction threshold being met. Default: 90\n    evictionMinimumReclaim  KubeletConfigEvictionMinimumReclaim     (Optional) EvictionMinimumReclaim configures the amount of resources below the configured eviction threshold that the kubelet attempts to reclaim whenever the kubelet observes resource pressure. Default: 0 for each resource\n    evictionPressureTransitionPeriod  Kubernetes meta/v1.Duration     (Optional) EvictionPressureTransitionPeriod is the duration for which the kubelet has to wait before transitioning out of an eviction pressure condition. Default: 4m0s\n    evictionSoft  KubeletConfigEviction     (Optional) EvictionSoft describes a set of eviction thresholds (e.g. memory.available   evictionSoftGracePeriod  KubeletConfigEvictionSoftGracePeriod     (Optional) EvictionSoftGracePeriod describes a set of eviction grace periods (e.g. memory.available=1m30s) that correspond to how long a soft eviction threshold must hold before triggering a Pod eviction. Default: memory.available: 1m30s nodefs.available: 1m30s nodefs.inodesFree: 1m30s imagefs.available: 1m30s imagefs.inodesFree: 1m30s\n    maxPods  int32    (Optional) MaxPods is the maximum number of Pods that are allowed by the Kubelet. Default: 110\n    podPidsLimit  int64    (Optional) PodPIDsLimit is the maximum number of process IDs per pod allowed by the kubelet.\n    imagePullProgressDeadline  Kubernetes meta/v1.Duration     (Optional) ImagePullProgressDeadline describes the time limit under which if no pulling progress is made, the image pulling will be cancelled. Default: 1m\n    failSwapOn  bool    (Optional) FailSwapOn makes the Kubelet fail to start if swap is enabled on the node. (default true).\n    kubeReserved  KubeletConfigReserved     (Optional) KubeReserved is the configuration for resources reserved for kubernetes node components (mainly kubelet and container runtime). When updating these values, be aware that cgroup resizes may not succeed on active worker nodes. Look for the NodeAllocatableEnforced event to determine if the configuration was applied. Default: cpu=80m,memory=1Gi,pid=20k\n    systemReserved  KubeletConfigReserved     (Optional) SystemReserved is the configuration for resources reserved for system processes not managed by kubernetes (e.g. journald). When updating these values, be aware that cgroup resizes may not succeed on active worker nodes. Look for the NodeAllocatableEnforced event to determine if the configuration was applied.\n    KubeletConfigEviction   (Appears on: KubeletConfig)  KubeletConfigEviction contains kubelet eviction thresholds supporting either a resource.Quantity or a percentage based value.\n   Field Description      memoryAvailable  string    (Optional) MemoryAvailable is the threshold for the free memory on the host server.\n    imageFSAvailable  string    (Optional) ImageFSAvailable is the threshold for the free disk space in the imagefs filesystem (docker images and container writable layers).\n    imageFSInodesFree  string    (Optional) ImageFSInodesFree is the threshold for the available inodes in the imagefs filesystem.\n    nodeFSAvailable  string    (Optional) NodeFSAvailable is the threshold for the free disk space in the nodefs filesystem (docker volumes, logs, etc).\n    nodeFSInodesFree  string    (Optional) NodeFSInodesFree is the threshold for the available inodes in the nodefs filesystem.\n    KubeletConfigEvictionMinimumReclaim   (Appears on: KubeletConfig)  KubeletConfigEvictionMinimumReclaim contains configuration for the kubelet eviction minimum reclaim.\n   Field Description      memoryAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) MemoryAvailable is the threshold for the memory reclaim on the host server.\n    imageFSAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) ImageFSAvailable is the threshold for the disk space reclaim in the imagefs filesystem (docker images and container writable layers).\n    imageFSInodesFree  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) ImageFSInodesFree is the threshold for the inodes reclaim in the imagefs filesystem.\n    nodeFSAvailable  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) NodeFSAvailable is the threshold for the disk space reclaim in the nodefs filesystem (docker volumes, logs, etc).\n    nodeFSInodesFree  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) NodeFSInodesFree is the threshold for the inodes reclaim in the nodefs filesystem.\n    KubeletConfigEvictionSoftGracePeriod   (Appears on: KubeletConfig)  KubeletConfigEvictionSoftGracePeriod contains grace periods for kubelet eviction thresholds.\n   Field Description      memoryAvailable  Kubernetes meta/v1.Duration     (Optional) MemoryAvailable is the grace period for the MemoryAvailable eviction threshold.\n    imageFSAvailable  Kubernetes meta/v1.Duration     (Optional) ImageFSAvailable is the grace period for the ImageFSAvailable eviction threshold.\n    imageFSInodesFree  Kubernetes meta/v1.Duration     (Optional) ImageFSInodesFree is the grace period for the ImageFSInodesFree eviction threshold.\n    nodeFSAvailable  Kubernetes meta/v1.Duration     (Optional) NodeFSAvailable is the grace period for the NodeFSAvailable eviction threshold.\n    nodeFSInodesFree  Kubernetes meta/v1.Duration     (Optional) NodeFSInodesFree is the grace period for the NodeFSInodesFree eviction threshold.\n    KubeletConfigReserved   (Appears on: KubeletConfig)  KubeletConfigReserved contains reserved resources for daemons\n   Field Description      cpu  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) CPU is the reserved cpu.\n    memory  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) Memory is the reserved memory.\n    ephemeralStorage  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) EphemeralStorage is the reserved ephemeral-storage.\n    pid  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) PID is the reserved process-ids. To reserve PID, the SupportNodePidsLimit feature gate must be enabled in Kubernetes versions \u0026lt; 1.15.\n    Kubernetes   (Appears on: ShootSpec)  Kubernetes contains the version and configuration variables for the Shoot control plane.\n   Field Description      allowPrivilegedContainers  bool    (Optional) AllowPrivilegedContainers indicates whether privileged containers are allowed in the Shoot (default: true).\n    clusterAutoscaler  ClusterAutoscaler     (Optional) ClusterAutoscaler contains the configuration flags for the Kubernetes cluster autoscaler.\n    kubeAPIServer  KubeAPIServerConfig     (Optional) KubeAPIServer contains configuration settings for the kube-apiserver.\n    kubeControllerManager  KubeControllerManagerConfig     (Optional) KubeControllerManager contains configuration settings for the kube-controller-manager.\n    kubeScheduler  KubeSchedulerConfig     (Optional) KubeScheduler contains configuration settings for the kube-scheduler.\n    kubeProxy  KubeProxyConfig     (Optional) KubeProxy contains configuration settings for the kube-proxy.\n    kubelet  KubeletConfig     (Optional) Kubelet contains configuration settings for the kubelet.\n    version  string    Version is the semantic Kubernetes version to use for the Shoot cluster.\n    verticalPodAutoscaler  VerticalPodAutoscaler     (Optional) VerticalPodAutoscaler contains the configuration flags for the Kubernetes vertical pod autoscaler.\n    KubernetesConfig   (Appears on: KubeAPIServerConfig, KubeControllerManagerConfig, KubeProxyConfig, KubeSchedulerConfig, KubeletConfig)  KubernetesConfig contains common configuration fields for the control plane components.\n   Field Description      featureGates  map[string]bool    (Optional) FeatureGates contains information about enabled feature gates.\n    KubernetesDashboard   (Appears on: Addons)  KubernetesDashboard describes configuration values for the kubernetes-dashboard addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     authenticationMode  string    (Optional) AuthenticationMode defines the authentication mode for the kubernetes-dashboard.\n    KubernetesInfo   (Appears on: ClusterInfo)  KubernetesInfo contains the version and configuration variables for the Plant cluster.\n   Field Description      version  string    Version is the semantic Kubernetes version to use for the Plant cluster.\n    KubernetesSettings   (Appears on: CloudProfileSpec)  KubernetesSettings contains constraints regarding allowed values of the \u0026lsquo;kubernetes\u0026rsquo; block in the Shoot specification.\n   Field Description      versions  []ExpirableVersion     (Optional) Versions is the list of allowed Kubernetes versions with optional expiration dates for Shoot clusters.\n    LastError   (Appears on: BackupBucketStatus, BackupEntryStatus, ShootStatus)  LastError indicates the last occurred error for an operation on a resource.\n   Field Description      description  string    A human readable message indicating details about the last error.\n    taskID  string    (Optional) ID of the task which caused this last error\n    codes  []ErrorCode     (Optional) Well-defined error codes of the last error(s).\n    lastUpdateTime  Kubernetes meta/v1.Time     (Optional) Last time the error was reported\n    LastOperation   (Appears on: BackupBucketStatus, BackupEntryStatus, ShootStatus)  LastOperation indicates the type and the state of the last operation, along with a description message and a progress indicator.\n   Field Description      description  string    A human readable message indicating details about the last operation.\n    lastUpdateTime  Kubernetes meta/v1.Time     Last time the operation state transitioned from one to another.\n    progress  int32    The progress in percentage (0-100) of the last operation.\n    state  LastOperationState     Status of the last operation, one of Aborted, Processing, Succeeded, Error, Failed.\n    type  LastOperationType     Type of the last operation, one of Create, Reconcile, Delete.\n    LastOperationState (string alias)\n  (Appears on: LastOperation)  LastOperationState is a string alias.\nLastOperationType (string alias)\n  (Appears on: LastOperation)  LastOperationType is a string alias.\nMachine   (Appears on: Worker)  Machine contains information about the machine type and image.\n   Field Description      type  string    Type is the machine type of the worker group.\n    image  ShootMachineImage     (Optional) Image holds information about the machine image to use for all nodes of this pool. It will default to the latest version of the first image stated in the referenced CloudProfile if no value has been provided.\n    MachineControllerManagerSettings   (Appears on: Worker)  MachineControllerManagerSettings contains configurations for different worker-pools. Eg. MachineDrainTimeout, MachineHealthTimeout.\n   Field Description      machineDrainTimeout  Kubernetes meta/v1.Duration     (Optional) MachineDrainTimeout is the period after which machine is forcefully deleted.\n    machineHealthTimeout  Kubernetes meta/v1.Duration     (Optional) MachineHealthTimeout is the period after which machine is declared failed.\n    machineCreationTimeout  Kubernetes meta/v1.Duration     (Optional) MachineCreationTimeout is the period after which creation of the machine is declared failed.\n    maxEvictRetries  int32    (Optional) MaxEvictRetries are the number of eviction retries on a pod after which drain is declared failed, and forceful deletion is triggered.\n    nodeConditions  []string    (Optional) NodeConditions are the set of conditions if set to true for the period of MachineHealthTimeout, machine will be declared failed.\n    MachineImage   (Appears on: CloudProfileSpec)  MachineImage defines the name and multiple versions of the machine image in any environment.\n   Field Description      name  string    Name is the name of the image.\n    versions  []MachineImageVersion     Versions contains versions, expiration dates and container runtimes of the machine image\n    MachineImageVersion   (Appears on: MachineImage)  MachineImageVersion is an expirable version with list of supported container runtimes and interfaces\n   Field Description      ExpirableVersion  ExpirableVersion      (Members of ExpirableVersion are embedded into this type.)     cri  []CRI     (Optional) CRI list of supported container runtime and interfaces supported by this version\n    MachineType   (Appears on: CloudProfileSpec)  MachineType contains certain properties of a machine type.\n   Field Description      cpu  k8s.io/apimachinery/pkg/api/resource.Quantity     CPU is the number of CPUs for this machine type.\n    gpu  k8s.io/apimachinery/pkg/api/resource.Quantity     GPU is the number of GPUs for this machine type.\n    memory  k8s.io/apimachinery/pkg/api/resource.Quantity     Memory is the amount of memory for this machine type.\n    name  string    Name is the name of the machine type.\n    storage  MachineTypeStorage     (Optional) Storage is the amount of storage associated with the root volume of this machine type.\n    usable  bool    (Optional) Usable defines if the machine type can be used for shoot clusters.\n    MachineTypeStorage   (Appears on: MachineType)  MachineTypeStorage is the amount of storage associated with the root volume of this machine type.\n   Field Description      class  string    Class is the class of the storage type.\n    size  k8s.io/apimachinery/pkg/api/resource.Quantity     StorageSize is the storage size.\n    type  string    Type is the type of the storage.\n    Maintenance   (Appears on: ShootSpec)  Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n   Field Description      autoUpdate  MaintenanceAutoUpdate     (Optional) AutoUpdate contains information about which constraints should be automatically updated.\n    timeWindow  MaintenanceTimeWindow     (Optional) TimeWindow contains information about the time window for maintenance operations.\n    confineSpecUpdateRollout  bool    (Optional) ConfineSpecUpdateRollout prevents that changes/updates to the shoot specification will be rolled out immediately. Instead, they are rolled out during the shoot\u0026rsquo;s maintenance time window. There is one exception that will trigger an immediate roll out which is changes to the Spec.Hibernation.Enabled field.\n    MaintenanceAutoUpdate   (Appears on: Maintenance)  MaintenanceAutoUpdate contains information about which constraints should be automatically updated.\n   Field Description      kubernetesVersion  bool    KubernetesVersion indicates whether the patch Kubernetes version may be automatically updated (default: true).\n    machineImageVersion  bool    MachineImageVersion indicates whether the machine image version may be automatically updated (default: true).\n    MaintenanceTimeWindow   (Appears on: Maintenance)  MaintenanceTimeWindow contains information about the time window for maintenance operations.\n   Field Description      begin  string    Begin is the beginning of the time window in the format HHMMSS+ZONE, e.g. \u0026ldquo;220000+0100\u0026rdquo;. If not present, a random value will be computed.\n    end  string    End is the end of the time window in the format HHMMSS+ZONE, e.g. \u0026ldquo;220000+0100\u0026rdquo;. If not present, the value will be computed based on the \u0026ldquo;Begin\u0026rdquo; value.\n    Monitoring   (Appears on: ShootSpec)  Monitoring contains information about the monitoring configuration for the shoot.\n   Field Description      alerting  Alerting     (Optional) Alerting contains information about the alerting configuration for the shoot cluster.\n    NamedResourceReference   (Appears on: ShootSpec)  NamedResourceReference is a named reference to a resource.\n   Field Description      name  string    Name of the resource reference.\n    resourceRef  Kubernetes autoscaling/v1.CrossVersionObjectReference     ResourceRef is a reference to a resource.\n    Networking   (Appears on: ShootSpec)  Networking defines networking parameters for the shoot cluster.\n   Field Description      type  string    Type identifies the type of the networking plugin.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to network resource.\n    pods  string    (Optional) Pods is the CIDR of the pod network.\n    nodes  string    (Optional) Nodes is the CIDR of the entire node network.\n    services  string    (Optional) Services is the CIDR of the service network.\n    NginxIngress   (Appears on: Addons)  NginxIngress describes configuration values for the nginx-ingress addon.\n   Field Description      Addon  Addon      (Members of Addon are embedded into this type.)     loadBalancerSourceRanges  []string    (Optional) LoadBalancerSourceRanges is list of allowed IP sources for NginxIngress\n    config  map[string]string    (Optional) Config contains custom configuration for the nginx-ingress-controller configuration. See https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/configmap.md#configuration-options\n    externalTrafficPolicy  Kubernetes core/v1.ServiceExternalTrafficPolicyType     (Optional) ExternalTrafficPolicy controls the .spec.externalTrafficPolicy value of the load balancer Service exposing the nginx-ingress. Defaults to Cluster.\n    OIDCConfig   (Appears on: KubeAPIServerConfig)  OIDCConfig contains configuration settings for the OIDC provider. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      caBundle  string    (Optional) If set, the OpenID server\u0026rsquo;s certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host\u0026rsquo;s root CA set will be used.\n    clientAuthentication  OpenIDConnectClientAuthentication     (Optional) ClientAuthentication can optionally contain client configuration used for kubeconfig generation.\n    clientID  string    (Optional) The client ID for the OpenID Connect client, must be set if oidc-issuer-url is set.\n    groupsClaim  string    (Optional) If provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This flag is experimental, please see the authentication documentation for further details.\n    groupsPrefix  string    (Optional) If provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies.\n    issuerURL  string    (Optional) The URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT).\n    requiredClaims  map[string]string    (Optional) ATTENTION: Only meaningful for Kubernetes \u0026gt;= 1.11 key=value pairs that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value.\n    signingAlgs  []string    (Optional) List of allowed JOSE asymmetric signing algorithms. JWTs with a \u0026lsquo;alg\u0026rsquo; header value not in this list will be rejected. Values are defined by RFC 7518 https://tools.ietf.org/html/rfc7518#section-3.1\n    usernameClaim  string    (Optional) The OpenID claim to use as the user name. Note that claims other than the default (\u0026lsquo;sub\u0026rsquo;) is not guaranteed to be unique and immutable. This flag is experimental, please see the authentication documentation for further details. (default \u0026ldquo;sub\u0026rdquo;)\n    usernamePrefix  string    (Optional) If provided, all usernames will be prefixed with this value. If not provided, username claims other than \u0026lsquo;email\u0026rsquo; are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value \u0026lsquo;-\u0026rsquo;.\n    OpenIDConnectClientAuthentication   (Appears on: OIDCConfig)  OpenIDConnectClientAuthentication contains configuration for OIDC clients.\n   Field Description      extraConfig  map[string]string    (Optional) Extra configuration added to kubeconfig\u0026rsquo;s auth-provider. Must not be any of idp-issuer-url, client-id, client-secret, idp-certificate-authority, idp-certificate-authority-data, id-token or refresh-token\n    secret  string    (Optional) The client Secret for the OpenID Connect client.\n    PlantSpec   (Appears on: Plant)  PlantSpec is the specification of a Plant.\n   Field Description      secretRef  Kubernetes core/v1.LocalObjectReference     SecretRef is a reference to a Secret object containing the Kubeconfig of the external kubernetes clusters to be added to Gardener.\n    endpoints  []Endpoint     (Optional) Endpoints is the configuration plant endpoints\n    PlantStatus   (Appears on: Plant)  PlantStatus is the status of a Plant.\n   Field Description      conditions  []Condition     (Optional) Conditions represents the latest available observations of a Plant\u0026rsquo;s current state.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Plant. It corresponds to the Plant\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    clusterInfo  ClusterInfo     ClusterInfo is additional computed information about the newly added cluster (Plant)\n    ProjectMember   (Appears on: ProjectSpec)  ProjectMember is a member of a project.\n   Field Description      Subject  Kubernetes rbac/v1.Subject      (Members of Subject are embedded into this type.) Subject is representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    role  string    Role represents the role of this member. IMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the roles list. TODO: Remove this field in favor of the owner role in v1.\n    roles  []string    (Optional) Roles represents the list of roles of this member.\n    ProjectPhase (string alias)\n  (Appears on: ProjectStatus)  ProjectPhase is a label for the condition of a project at the current time.\nProjectSpec   (Appears on: Project)  ProjectSpec is the specification of a Project.\n   Field Description      createdBy  Kubernetes rbac/v1.Subject     (Optional) CreatedBy is a subject representing a user name, an email address, or any other identifier of a user who created the project.\n    description  string    (Optional) Description is a human-readable description of what the project is used for.\n    owner  Kubernetes rbac/v1.Subject     (Optional) Owner is a subject representing a user name, an email address, or any other identifier of a user owning the project. IMPORTANT: Be aware that this field will be removed in the v1 version of this API in favor of the owner role. The only way to change the owner will be by moving the owner role. In this API version the only way to change the owner is to use this field. TODO: Remove this field in favor of the owner role in v1.\n    purpose  string    (Optional) Purpose is a human-readable explanation of the project\u0026rsquo;s purpose.\n    members  []ProjectMember     (Optional) Members is a list of subjects representing a user name, an email address, or any other identifier of a user, group, or service account that has a certain role.\n    namespace  string    (Optional) Namespace is the name of the namespace that has been created for the Project object. A nil value means that Gardener will determine the name of the namespace.\n    tolerations  ProjectTolerations     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n    ProjectStatus   (Appears on: Project)  ProjectStatus holds the most recently observed status of the project.\n   Field Description      observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this project.\n    phase  ProjectPhase     Phase is the current phase of the project.\n    staleSinceTimestamp  Kubernetes meta/v1.Time     (Optional) StaleSinceTimestamp contains the timestamp when the project was first discovered to be stale/unused.\n    staleAutoDeleteTimestamp  Kubernetes meta/v1.Time     (Optional) StaleAutoDeleteTimestamp contains the timestamp when the project will be garbage-collected/automatically deleted because it\u0026rsquo;s stale/unused.\n    ProjectTolerations   (Appears on: ProjectSpec)  ProjectTolerations contains the tolerations for taints on seed clusters.\n   Field Description      defaults  []Toleration     (Optional) Defaults contains a list of tolerations that are added to the shoots in this project by default.\n    whitelist  []Toleration     (Optional) Whitelist contains a list of tolerations that are allowed to be added to the shoots in this project. Please note that this list may only be added by users having the spec-tolerations-whitelist verb for project resources.\n    Provider   (Appears on: ShootSpec)  Provider contains provider-specific information that are handed-over to the provider-specific extension controller.\n   Field Description      type  string    Type is the type of the provider.\n    controlPlaneConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ControlPlaneConfig contains the provider-specific control plane config blob. Please look up the concrete definition in the documentation of your provider extension.\n    infrastructureConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureConfig contains the provider-specific infrastructure config blob. Please look up the concrete definition in the documentation of your provider extension.\n    workers  []Worker     Workers is a list of worker groups.\n    ProxyMode (string alias)\n  (Appears on: KubeProxyConfig)  ProxyMode available in Linux platform: \u0026lsquo;userspace\u0026rsquo; (older, going to be EOL), \u0026lsquo;iptables\u0026rsquo; (newer, faster), \u0026lsquo;ipvs\u0026rsquo; (newest, better in performance and scalability). As of now only \u0026lsquo;iptables\u0026rsquo; and \u0026lsquo;ipvs\u0026rsquo; is supported by Gardener. In Linux platform, if the iptables proxy is selected, regardless of how, but the system\u0026rsquo;s kernel or iptables versions are insufficient, this always falls back to the userspace proxy. IPVS mode will be enabled when proxy mode is set to \u0026lsquo;ipvs\u0026rsquo;, and the fall back path is firstly iptables and then userspace.\nQuotaSpec   (Appears on: Quota)  QuotaSpec is the specification of a Quota.\n   Field Description      clusterLifetimeDays  int32    (Optional) ClusterLifetimeDays is the lifetime of a Shoot cluster in days before it will be terminated automatically.\n    metrics  Kubernetes core/v1.ResourceList     Metrics is a list of resources which will be put under constraints.\n    scope  Kubernetes core/v1.ObjectReference     Scope is the scope of the Quota object, either \u0026lsquo;project\u0026rsquo; or \u0026lsquo;secret\u0026rsquo;.\n    Region   (Appears on: CloudProfileSpec)  Region contains certain properties of a region.\n   Field Description      name  string    Name is a region name.\n    zones  []AvailabilityZone     (Optional) Zones is a list of availability zones in this region.\n    labels  map[string]string    (Optional) Labels is an optional set of key-value pairs that contain certain administrator-controlled labels for this region. It can be used by Gardener administrators/operators to provide additional information about a region, e.g. wrt quality, reliability, access restrictions, etc.\n    ResourceWatchCacheSize   (Appears on: WatchCacheSizes)  ResourceWatchCacheSize contains configuration of the API server\u0026rsquo;s watch cache size for one specific resource.\n   Field Description      apiGroup  string    (Optional) APIGroup is the API group of the resource for which the watch cache size should be configured. An unset value is used to specify the legacy core API (e.g. for secrets).\n    resource  string    Resource is the name of the resource for which the watch cache size should be configured (in lowercase plural form, e.g. secrets).\n    size  int32    CacheSize specifies the watch cache size that should be configured for the specified resource.\n    SeedBackup   (Appears on: SeedSpec)  SeedBackup contains the object store configuration for backups for shoot (currently only etcd).\n   Field Description      provider  string    Provider is a provider name.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to BackupBucket resource.\n    region  string    (Optional) Region is a region name.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a Secret object containing the cloud provider credentials for the object store where backups should be stored. It should have enough privileges to manipulate the objects as well as buckets.\n    SeedDNS   (Appears on: SeedSpec)  SeedDNS contains DNS-relevant information about this seed cluster.\n   Field Description      ingressDomain  string    (Optional) IngressDomain is the domain of the Seed cluster pointing to the ingress controller endpoint. It will be used to construct ingress URLs for system applications running in Shoot clusters. Once set this field is immutable. This will be removed in the next API version and replaced by spec.ingress.domain.\n    provider  SeedDNSProvider     (Optional) Provider configures a DNSProvider\n    SeedDNSProvider   (Appears on: SeedDNS)  SeedDNSProvider configures a DNSProvider for Seeds\n   Field Description      type  string    Type describes the type of the dns-provider, for example aws-route53\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a Secret object containing cloud provider credentials used for registering external domains.\n    domains  DNSIncludeExclude     (Optional) Domains contains information about which domains shall be included/excluded for this provider.\n    zones  DNSIncludeExclude     (Optional) Zones contains information about which hosted zones shall be included/excluded for this provider.\n    SeedNetworks   (Appears on: SeedSpec)  SeedNetworks contains CIDRs for the pod, service and node networks of a Kubernetes cluster.\n   Field Description      nodes  string    (Optional) Nodes is the CIDR of the node network.\n    pods  string    Pods is the CIDR of the pod network.\n    services  string    Services is the CIDR of the service network.\n    shootDefaults  ShootNetworks     (Optional) ShootDefaults contains the default networks CIDRs for shoots.\n    blockCIDRs  []string    (Optional) BlockCIDRs is a list of network addresses that should be blocked for shoot control plane components running in the seed cluster.\n    SeedProvider   (Appears on: SeedSpec)  SeedProvider defines the provider type and region for this Seed cluster.\n   Field Description      type  string    Type is the name of the provider.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the configuration passed to Seed resource.\n    region  string    Region is a name of a region.\n    SeedSelector   (Appears on: CloudProfileSpec, ShootSpec)  SeedSelector contains constraints for selecting seed to be usable for shoots using a profile\n   Field Description      LabelSelector  Kubernetes meta/v1.LabelSelector      (Members of LabelSelector are embedded into this type.) (Optional) LabelSelector is optional and can be used to select seeds by their label settings\n    providerTypes  []string    (Optional) Providers is optional and can be used by restricting seeds by their provider type. \u0026lsquo;*\u0026rsquo; can be used to enable seeds regardless of their provider type.\n    SeedSettingExcessCapacityReservation   (Appears on: SeedSettings)  SeedSettingExcessCapacityReservation controls the excess capacity reservation for shoot control planes in the seed. When enabled then this is done via PodPriority and requires the Seed cluster to have Kubernetes version 1.11 or the PodPriority feature gate as well as the scheduling.k8s.io/v1alpha1 API group enabled.\n   Field Description      enabled  bool    Enabled controls whether the excess capacity reservation should be enabled.\n    SeedSettingLoadBalancerServices   (Appears on: SeedSettings)  SeedSettingLoadBalancerServices controls certain settings for services of type load balancer that are created in the seed.\n   Field Description      annotations  map[string]string    (Optional) Annotations is a map of annotations that will be injected/merged into every load balancer service object.\n    SeedSettingScheduling   (Appears on: SeedSettings)  SeedSettingScheduling controls settings for scheduling decisions for the seed.\n   Field Description      visible  bool    Visible controls whether the gardener-scheduler shall consider this seed when scheduling shoots. Invisible seeds are not considered by the scheduler.\n    SeedSettingShootDNS   (Appears on: SeedSettings)  SeedSettingShootDNS controls the shoot DNS settings for the seed.\n   Field Description      enabled  bool    Enabled controls whether the DNS for shoot clusters should be enabled. When disabled then all shoots using the seed won\u0026rsquo;t get any DNS providers, DNS records, and no DNS extension controller is required to be installed here. This is useful for environments where DNS is not required.\n    SeedSettingVerticalPodAutoscaler   (Appears on: SeedSettings)  SeedSettingVerticalPodAutoscaler controls certain settings for the vertical pod autoscaler components deployed in the seed.\n   Field Description      enabled  bool    Enabled controls whether the VPA components shall be deployed into the garden namespace in the seed cluster. It is enabled by default because Gardener heavily relies on a VPA being deployed. You should only disable this if your seed cluster already has another, manually/custom managed VPA deployment.\n    SeedSettings   (Appears on: SeedSpec)  SeedSettings contains certain settings for this seed cluster.\n   Field Description      excessCapacityReservation  SeedSettingExcessCapacityReservation     (Optional) ExcessCapacityReservation controls the excess capacity reservation for shoot control planes in the seed.\n    scheduling  SeedSettingScheduling     (Optional) Scheduling controls settings for scheduling decisions for the seed.\n    shootDNS  SeedSettingShootDNS     (Optional) ShootDNS controls the shoot DNS settings for the seed.\n    loadBalancerServices  SeedSettingLoadBalancerServices     (Optional) LoadBalancerServices controls certain settings for services of type load balancer that are created in the seed.\n    verticalPodAutoscaler  SeedSettingVerticalPodAutoscaler     (Optional) VerticalPodAutoscaler controls certain settings for the vertical pod autoscaler components deployed in the seed.\n    SeedSpec   (Appears on: Seed)  SeedSpec is the specification of a Seed.\n   Field Description      backup  SeedBackup     (Optional) Backup holds the object store configuration for the backups of shoot (currently only etcd). If it is not specified, then there won\u0026rsquo;t be any backups taken for shoots associated with this seed. If backup field is present in seed, then backups of the etcd from shoot control plane will be stored under the configured object store.\n    dns  SeedDNS     DNS contains DNS-relevant information about this seed cluster.\n    networks  SeedNetworks     Networks defines the pod, service and worker network of the Seed cluster.\n    provider  SeedProvider     Provider defines the provider type and region for this Seed cluster.\n    secretRef  Kubernetes core/v1.SecretReference     (Optional) SecretRef is a reference to a Secret object containing the Kubeconfig and the cloud provider credentials for the account the Seed cluster has been deployed to.\n    taints  []SeedTaint     (Optional) Taints describes taints on the seed.\n    volume  SeedVolume     (Optional) Volume contains settings for persistentvolumes created in the seed cluster.\n    settings  SeedSettings     (Optional) Settings contains certain settings for this seed cluster.\n    ingress  Ingress     (Optional) Ingress configures Ingress specific settings of the Seed cluster.\n    SeedStatus   (Appears on: Seed)  SeedStatus is the status of a Seed.\n   Field Description      gardener  Gardener     (Optional) Gardener holds information about the Gardener which last acted on the Shoot.\n    kubernetesVersion  string    (Optional) KubernetesVersion is the Kubernetes version of the seed cluster.\n    conditions  []Condition     (Optional) Conditions represents the latest available observations of a Seed\u0026rsquo;s current state.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Seed. It corresponds to the Seed\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    clusterIdentity  string    (Optional) ClusterIdentity is the identity of the Seed cluster\n    capacity  Kubernetes core/v1.ResourceList     (Optional) Capacity represents the total resources of a seed.\n    allocatable  Kubernetes core/v1.ResourceList     (Optional) Allocatable represents the resources of a seed that are available for scheduling. Defaults to Capacity.\n    SeedTaint   (Appears on: SeedSpec)  SeedTaint describes a taint on a seed.\n   Field Description      key  string    Key is the taint key to be applied to a seed.\n    value  string    (Optional) Value is the taint value corresponding to the taint key.\n    SeedVolume   (Appears on: SeedSpec)  SeedVolume contains settings for persistentvolumes created in the seed cluster.\n   Field Description      minimumSize  k8s.io/apimachinery/pkg/api/resource.Quantity     (Optional) MinimumSize defines the minimum size that should be used for PVCs in the seed.\n    providers  []SeedVolumeProvider     (Optional) Providers is a list of storage class provisioner types for the seed.\n    SeedVolumeProvider   (Appears on: SeedVolume)  SeedVolumeProvider is a storage class provisioner type.\n   Field Description      purpose  string    Purpose is the purpose of this provider.\n    name  string    Name is the name of the storage class provisioner type.\n    ServiceAccountConfig   (Appears on: KubeAPIServerConfig)  ServiceAccountConfig is the kube-apiserver configuration for service accounts.\n   Field Description      issuer  string    (Optional) Issuer is the identifier of the service account token issuer. The issuer will assert this identifier in \u0026ldquo;iss\u0026rdquo; claim of issued tokens. This value is a string or URI. Defaults to URI of the API server.\n    signingKeySecretName  Kubernetes core/v1.LocalObjectReference     (Optional) SigningKeySecret is a reference to a secret that contains an optional private key of the service account token issuer. The issuer will sign issued ID tokens with this private key. Only useful if service account tokens are also issued by another external system.\n    ShootMachineImage   (Appears on: Machine)  ShootMachineImage defines the name and the version of the shoot\u0026rsquo;s machine image in any environment. Has to be defined in the respective CloudProfile.\n   Field Description      name  string    Name is the name of the image.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the shoot\u0026rsquo;s individual configuration passed to an extension resource.\n    version  string    (Optional) Version is the version of the shoot\u0026rsquo;s image. If version is not provided, it will be defaulted to the latest version from the CloudProfile.\n    ShootNetworks   (Appears on: SeedNetworks)  ShootNetworks contains the default networks CIDRs for shoots.\n   Field Description      pods  string    (Optional) Pods is the CIDR of the pod network.\n    services  string    (Optional) Services is the CIDR of the service network.\n    ShootPurpose (string alias)\n  (Appears on: ShootSpec)  ShootPurpose is a type alias for string.\nShootSpec   (Appears on: Shoot)  ShootSpec is the specification of a Shoot.\n   Field Description      addons  Addons     (Optional) Addons contains information about enabled/disabled addons and their configuration.\n    cloudProfileName  string    CloudProfileName is a name of a CloudProfile object.\n    dns  DNS     (Optional) DNS contains information about the DNS settings of the Shoot.\n    extensions  []Extension     (Optional) Extensions contain type and provider information for Shoot extensions.\n    hibernation  Hibernation     (Optional) Hibernation contains information whether the Shoot is suspended or not.\n    kubernetes  Kubernetes     Kubernetes contains the version and configuration settings of the control plane components.\n    networking  Networking     Networking contains information about cluster networking such as CNI Plugin type, CIDRs, \u0026hellip;etc.\n    maintenance  Maintenance     (Optional) Maintenance contains information about the time window for maintenance operations and which operations should be performed.\n    monitoring  Monitoring     (Optional) Monitoring contains information about custom monitoring configurations for the shoot.\n    provider  Provider     Provider contains all provider-specific and provider-relevant information.\n    purpose  ShootPurpose     (Optional) Purpose is the purpose class for this cluster.\n    region  string    Region is a name of a region.\n    secretBindingName  string    SecretBindingName is the name of the a SecretBinding that has a reference to the provider secret. The credentials inside the provider secret will be used to create the shoot in the respective account.\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot.\n    seedSelector  SeedSelector     (Optional) SeedSelector is an optional selector which must match a seed\u0026rsquo;s labels for the shoot to be scheduled on that seed.\n    resources  []NamedResourceReference     (Optional) Resources holds a list of named resource references that can be referred to in extension configs by their names.\n    tolerations  []Toleration     (Optional) Tolerations contains the tolerations for taints on seed clusters.\n    ShootStatus   (Appears on: Shoot)  ShootStatus holds the most recently observed status of the Shoot cluster.\n   Field Description      conditions  []Condition     (Optional) Conditions represents the latest available observations of a Shoots\u0026rsquo;s current state.\n    constraints  []Condition     (Optional) Constraints represents conditions of a Shoot\u0026rsquo;s current state that constraint some operations on it.\n    gardener  Gardener     Gardener holds information about the Gardener which last acted on the Shoot.\n    hibernated  bool    IsHibernated indicates whether the Shoot is currently hibernated.\n    lastOperation  LastOperation     (Optional) LastOperation holds information about the last operation on the Shoot.\n    lastErrors  []LastError     (Optional) LastErrors holds information about the last occurred error(s) during an operation.\n    observedGeneration  int64    (Optional) ObservedGeneration is the most recent generation observed for this Shoot. It corresponds to the Shoot\u0026rsquo;s generation, which is updated on mutation by the API Server.\n    retryCycleStartTime  Kubernetes meta/v1.Time     (Optional) RetryCycleStartTime is the start time of the last retry cycle (used to determine how often an operation must be retried until we give up).\n    seedName  string    (Optional) SeedName is the name of the seed cluster that runs the control plane of the Shoot. This value is only written after a successful create/reconcile operation. It will be used when control planes are moved between Seeds.\n    technicalID  string    TechnicalID is the name that is used for creating the Seed namespace, the infrastructure resources, and basically everything that is related to this particular Shoot.\n    uid  k8s.io/apimachinery/pkg/types.UID     UID is a unique identifier for the Shoot cluster to avoid portability between Kubernetes clusters. It is used to compute unique hashes.\n    clusterIdentity  string    (Optional) ClusterIdentity is the identity of the Shoot cluster\n    Toleration   (Appears on: ProjectTolerations, ShootSpec)  Toleration is a toleration for a seed taint.\n   Field Description      key  string    Key is the toleration key to be applied to a project or shoot.\n    value  string    (Optional) Value is the toleration value corresponding to the toleration key.\n    VersionClassification (string alias)\n  (Appears on: ExpirableVersion)  VersionClassification is the logical state of a version according to https://github.com/gardener/gardener/blob/master/docs/operations/versioning.md\nVerticalPodAutoscaler   (Appears on: Kubernetes)  VerticalPodAutoscaler contains the configuration flags for the Kubernetes vertical pod autoscaler.\n   Field Description      enabled  bool    Enabled specifies whether the Kubernetes VPA shall be enabled for the shoot cluster.\n    evictAfterOOMThreshold  Kubernetes meta/v1.Duration     (Optional) EvictAfterOOMThreshold defines the threshold that will lead to pod eviction in case it OOMed in less than the given threshold since its start and if it has only one container (default: 10m0s).\n    evictionRateBurst  int32    (Optional) EvictionRateBurst defines the burst of pods that can be evicted (default: 1)\n    evictionRateLimit  float64    (Optional) EvictionRateLimit defines the number of pods that can be evicted per second. A rate limit set to 0 or -1 will disable the rate limiter (default: -1).\n    evictionTolerance  float64    (Optional) EvictionTolerance defines the fraction of replica count that can be evicted for update in case more than one pod can be evicted (default: 0.5).\n    recommendationMarginFraction  float64    (Optional) RecommendationMarginFraction is the fraction of usage added as the safety margin to the recommended request (default: 0.15).\n    updaterInterval  Kubernetes meta/v1.Duration     (Optional) UpdaterInterval is the interval how often the updater should run (default: 1m0s).\n    recommenderInterval  Kubernetes meta/v1.Duration     (Optional) RecommenderInterval is the interval how often metrics should be fetched (default: 1m0s).\n    Volume   (Appears on: Worker)  Volume contains information about the volume type, size, and encryption.\n   Field Description      name  string    (Optional) Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    VolumeSize is the size of the volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    VolumeType   (Appears on: CloudProfileSpec)  VolumeType contains certain properties of a volume type.\n   Field Description      class  string    Class is the class of the volume type.\n    name  string    Name is the name of the volume type.\n    usable  bool    (Optional) Usable defines if the volume type can be used for shoot clusters.\n    WatchCacheSizes   (Appears on: KubeAPIServerConfig)  WatchCacheSizes contains configuration of the API server\u0026rsquo;s watch cache sizes.\n   Field Description      default  int32    (Optional) Default configures the default watch cache size of the kube-apiserver (flag --default-watch-cache-size, defaults to 100). See: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/\n    resources  []ResourceWatchCacheSize     (Optional) Resources configures the watch cache size of the kube-apiserver per resource (flag --watch-cache-sizes). See: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/\n    Worker   (Appears on: Provider)  Worker is the base definition of a worker group.\n   Field Description      annotations  map[string]string    (Optional) Annotations is a map of key/value pairs for annotations for all the Node objects in this worker pool.\n    caBundle  string    (Optional) CABundle is a certificate bundle which will be installed onto every machine of this worker pool.\n    cri  CRI     (Optional) CRI contains configurations of CRI support of every machine in the worker pool\n    kubernetes  WorkerKubernetes     (Optional) Kubernetes contains configuration for Kubernetes components related to this worker pool.\n    labels  map[string]string    (Optional) Labels is a map of key/value pairs for labels for all the Node objects in this worker pool.\n    name  string    Name is the name of the worker group.\n    machine  Machine     Machine contains information about the machine type and image.\n    maximum  int32    Maximum is the maximum number of VMs to create.\n    minimum  int32    Minimum is the minimum number of VMs to create.\n    maxSurge  k8s.io/apimachinery/pkg/util/intstr.IntOrString     (Optional) MaxSurge is maximum number of VMs that are created during an update.\n    maxUnavailable  k8s.io/apimachinery/pkg/util/intstr.IntOrString     (Optional) MaxUnavailable is the maximum number of VMs that can be unavailable during an update.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the provider-specific configuration for this worker pool.\n    taints  []Kubernetes core/v1.Taint     (Optional) Taints is a list of taints for all the Node objects in this worker pool.\n    volume  Volume     (Optional) Volume contains information about the volume type and size.\n    dataVolumes  []DataVolume     (Optional) DataVolumes contains a list of additional worker volumes.\n    kubeletDataVolumeName  string    (Optional) KubeletDataVolumeName contains the name of a dataVolume that should be used for storing kubelet state.\n    zones  []string    (Optional) Zones is a list of availability zones that are used to evenly distribute this worker pool. Optional as not every provider may support availability zones.\n    systemComponents  WorkerSystemComponents     (Optional) SystemComponents contains configuration for system components related to this worker pool\n    machineControllerManager  MachineControllerManagerSettings     (Optional) MachineControllerManagerSettings contains configurations for different worker-pools. Eg. MachineDrainTimeout, MachineHealthTimeout.\n    WorkerKubernetes   (Appears on: Worker)  WorkerKubernetes contains configuration for Kubernetes components related to this worker pool.\n   Field Description      kubelet  KubeletConfig     (Optional) Kubelet contains configuration settings for all kubelets of this worker pool.\n    WorkerSystemComponents   (Appears on: Worker)  WorkerSystemComponents contains configuration for system components related to this worker pool\n   Field Description      allow  bool    Allow determines whether the pool should be allowed to host system components or not (defaults to true)\n      Generated with gen-crd-api-reference-docs \n"},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/create-delete-shoot/","title":"Create / Delete a Shoot cluster","tags":[],"description":"","content":"Create a Shoot Cluster As you have already prepared an example Shoot manifest in the steps described in the development documentation, please open another Terminal pane/window with the KUBECONFIG environment variable pointing to the Garden development cluster and send the manifest to the Kubernetes API server:\n$ kubectl apply -f your-shoot-aws.yaml You should see that the Gardener has immediately picked up your manifest and started to deploy the Shoot cluster.\nIn order to investigate what is happening in the Seed cluster, please download its proper Kubeconfig yourself (see next paragraph). The namespace of the Shoot cluster in the Seed cluster will look like that: shoot-johndoe-johndoe-1, whereas the first johndoe is your namespace in the Garden cluster (also called \u0026ldquo;project\u0026rdquo;) and the johndoe-1 suffix is the actual name of the Shoot cluster.\nTo connect to the newly created Shoot cluster, you must download its Kubeconfig as well. Please connect to the proper Seed cluster, navigate to the Shoot namespace, and download the Kubeconfig from the kubecfg secret in that namespace.\nDelete a Shoot Cluster In order to delete your cluster, you have to set an annotation confirming the deletion first, and trigger the deletion after that. You can use the prepared delete shoot script which takes the Shoot name as first parameter. The namespace can be specified by the second parameter, but it is optional. If you don\u0026rsquo;t state it, it defaults to your namespace (the username you are logged in with to your machine).\n$ ./hack/usage/delete shoot johndoe-1 johndoe ( hack bash script can be found here https://github.com/gardener/gardener/blob/master/hack/usage/delete)\nConfigure a Shoot cluster alert receiver The receiver of the Shoot alerts can be configured from the .spec.monitoring.alerting.emailReceivers section in the Shoot specification. The value of the field has to be a list of valid mail addresses.\nThe alerting for the Shoot clusters is handled by the Prometheus Alertmanager. The Alertmanager will be deployed next to the control plane when the Shoot resource specifies .spec.monitoring.alerting.emailReceivers and if a SMTP secret exists.\nIf the field gets removed then the Alertmanager will be also removed during the next reconcilation of the cluster. The opposite is also valid if the field is added to an existing cluster.\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/administer_shoots/create-delete-shoot/","title":"Create / Delete a Shoot cluster","tags":[],"description":"","content":"Create a Shoot Cluster As you have already prepared an example Shoot manifest in the steps described in the development documentation, please open another Terminal pane/window with the KUBECONFIG environment variable pointing to the Garden development cluster and send the manifest to the Kubernetes API server:\n$ kubectl apply -f your-shoot-aws.yaml You should see that the Gardener has immediately picked up your manifest and started to deploy the Shoot cluster.\nIn order to investigate what is happening in the Seed cluster, please download its proper Kubeconfig yourself (see next paragraph). The namespace of the Shoot cluster in the Seed cluster will look like that: shoot-johndoe-johndoe-1, whereas the first johndoe is your namespace in the Garden cluster (also called \u0026ldquo;project\u0026rdquo;) and the johndoe-1 suffix is the actual name of the Shoot cluster.\nTo connect to the newly created Shoot cluster, you must download its Kubeconfig as well. Please connect to the proper Seed cluster, navigate to the Shoot namespace, and download the Kubeconfig from the kubecfg secret in that namespace.\nDelete a Shoot Cluster In order to delete your cluster, you have to set an annotation confirming the deletion first, and trigger the deletion after that. You can use the prepared delete shoot script which takes the Shoot name as first parameter. The namespace can be specified by the second parameter, but it is optional. If you don\u0026rsquo;t state it, it defaults to your namespace (the username you are logged in with to your machine).\n$ ./hack/usage/delete shoot johndoe-1 johndoe ( hack bash script can be found here https://github.com/gardener/gardener/blob/master/hack/usage/delete)\nConfigure a Shoot cluster alert receiver The receiver of the Shoot alerts can be configured from the .spec.monitoring.alerting.emailReceivers section in the Shoot specification. The value of the field has to be a list of valid mail addresses.\nThe alerting for the Shoot clusters is handled by the Prometheus Alertmanager. The Alertmanager will be deployed next to the control plane when the Shoot resource specifies .spec.monitoring.alerting.emailReceivers and if a SMTP secret exists.\nIf the field gets removed then the Alertmanager will be also removed during the next reconcilation of the cluster. The opposite is also valid if the field is added to an existing cluster.\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/administer_shoots/create-delete-shoot/","title":"Create / Delete a Shoot cluster","tags":[],"description":"","content":"Create a Shoot Cluster As you have already prepared an example Shoot manifest in the steps described in the development documentation, please open another Terminal pane/window with the KUBECONFIG environment variable pointing to the Garden development cluster and send the manifest to the Kubernetes API server:\n$ kubectl apply -f your-shoot-aws.yaml You should see that the Gardener has immediately picked up your manifest and started to deploy the Shoot cluster.\nIn order to investigate what is happening in the Seed cluster, please download its proper Kubeconfig yourself (see next paragraph). The namespace of the Shoot cluster in the Seed cluster will look like that: shoot-johndoe-johndoe-1, whereas the first johndoe is your namespace in the Garden cluster (also called \u0026ldquo;project\u0026rdquo;) and the johndoe-1 suffix is the actual name of the Shoot cluster.\nTo connect to the newly created Shoot cluster, you must download its Kubeconfig as well. Please connect to the proper Seed cluster, navigate to the Shoot namespace, and download the Kubeconfig from the kubecfg secret in that namespace.\nDelete a Shoot Cluster In order to delete your cluster, you have to set an annotation confirming the deletion first, and trigger the deletion after that. You can use the prepared delete shoot script which takes the Shoot name as first parameter. The namespace can be specified by the second parameter, but it is optional. If you don\u0026rsquo;t state it, it defaults to your namespace (the username you are logged in with to your machine).\n$ ./hack/usage/delete shoot johndoe-1 johndoe ( hack bash script can be found here https://github.com/gardener/gardener/blob/master/hack/usage/delete)\nConfigure a Shoot cluster alert receiver The receiver of the Shoot alerts can be configured from the .spec.monitoring.alerting.emailReceivers section in the Shoot specification. The value of the field has to be a list of valid mail addresses.\nThe alerting for the Shoot clusters is handled by the Prometheus Alertmanager. The Alertmanager will be deployed next to the control plane when the Shoot resource specifies .spec.monitoring.alerting.emailReceivers and if a SMTP secret exists.\nIf the field gets removed then the Alertmanager will be also removed during the next reconcilation of the cluster. The opposite is also valid if the field is added to an existing cluster.\n"},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/gardener_aws/","title":"Create a kubernetes cluster in AWS with Gardener","tags":[],"description":"","content":"Introduction Creating a Kubernetes cluster in an AWS Account is easy and the Gardener UI should be self-explanatory.\nGardener Create a new Project in Gardener Create new Project\nCopy policy from the Gardener AWS Create new policy Create new policy\nCreate a new technical user Create a new technical user\nsave the keys of the user, you will need them later on Gardener Add AWS Secret Create a new Cluster Create a new cluster\nCopy kubeconfig "},{"uri":"https://gardener.cloud/v1.12.8/guides/administer_shoots/gardener_aws/","title":"Create a kubernetes cluster in AWS with Gardener","tags":[],"description":"","content":"Introduction Creating a Kubernetes cluster in an AWS Account is easy and the Gardener UI should be self-explanatory.\nGardener Create a new Project in Gardener Create new Project\nCopy policy from the Gardener AWS Create new policy Create new policy\nCreate a new technical user Create a new technical user\nsave the keys of the user, you will need them later on Gardener Add AWS Secret Create a new Cluster Create a new cluster\nCopy kubeconfig "},{"uri":"https://gardener.cloud/v1.13.2/guides/administer_shoots/gardener_aws/","title":"Create a kubernetes cluster in AWS with Gardener","tags":[],"description":"","content":"Introduction Creating a Kubernetes cluster in an AWS Account is easy and the Gardener UI should be self-explanatory.\nGardener Create a new Project in Gardener Create new Project\nCopy policy from the Gardener AWS Create new policy Create new policy\nCreate a new technical user Create a new technical user\nsave the keys of the user, you will need them later on Gardener Add AWS Secret Create a new Cluster Create a new cluster\nCopy kubeconfig "},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/gardener_gcp/","title":"Create a kubernetes cluster on GCP with Gardener","tags":[],"description":"","content":"Introduction Creating a Kubernetes cluster in the GCP Account is easy and the Gardener UI should be self-explanatory.\nGardener Create a new Project in Gardener Create new Project\nCheck which roles are required by the Gardener GCP Create a new serviceaccount and assign roles Create a new serviceaccount\nCreate key for the serviceaccount Download the key of the serviceaccount as json save the keys of the user, you will need it later on\nEnable the Google compute API Enable the Google compute API Enable the Google IAM API Enable the Google IAM API Gardener Add GCP Secret Create a new Cluster Create a new cluster\nCopy kubeconfig "},{"uri":"https://gardener.cloud/v1.12.8/guides/administer_shoots/gardener_gcp/","title":"Create a kubernetes cluster on GCP with Gardener","tags":[],"description":"","content":"Introduction Creating a Kubernetes cluster in the GCP Account is easy and the Gardener UI should be self-explanatory.\nGardener Create a new Project in Gardener Create new Project\nCheck which roles are required by the Gardener GCP Create a new serviceaccount and assign roles Create a new serviceaccount\nCreate key for the serviceaccount Download the key of the serviceaccount as json save the keys of the user, you will need it later on\nEnable the Google compute API Enable the Google compute API Enable the Google IAM API Enable the Google IAM API Gardener Add GCP Secret Create a new Cluster Create a new cluster\nCopy kubeconfig "},{"uri":"https://gardener.cloud/v1.13.2/guides/administer_shoots/gardener_gcp/","title":"Create a kubernetes cluster on GCP with Gardener","tags":[],"description":"","content":"Introduction Creating a Kubernetes cluster in the GCP Account is easy and the Gardener UI should be self-explanatory.\nGardener Create a new Project in Gardener Create new Project\nCheck which roles are required by the Gardener GCP Create a new serviceaccount and assign roles Create a new serviceaccount\nCreate key for the serviceaccount Download the key of the serviceaccount as json save the keys of the user, you will need it later on\nEnable the Google compute API Enable the Google compute API Enable the Google IAM API Enable the Google IAM API Gardener Add GCP Secret Create a new Cluster Create a new cluster\nCopy kubeconfig "},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/create-shoot-into-existing-aws-vpc/","title":"Create a Shoot cluster into existing AWS VPC","tags":[],"description":"","content":"Create a Shoot cluster into existing AWS VPC Gardener can create a new VPC, or use an existing one for your Shoot cluster. Depending on your needs you may want to create Shoot(s) into already created VPC. The tutorial describes how to create a Shoot cluster into existing AWS VPC. The steps are identical for Alicloud, Azure, and GCP. Please note that the existing VPC must be in the same region like the shoot cluster that you want to deploy into the VPC.\nTL;DR If .spec.provider.infrastructureConfig.networks.vpc.cidr is specified, Gardener will create a new VPC with the given CIDR block and respectively will delete it on Shoot deletion.\nIf .spec.provider.infrastructureConfig.networks.vpc.id is specified, Gardener will use the existing VPC and respectively won\u0026rsquo;t delete it on Shoot deletion.\n It\u0026rsquo;s not recommended to create a Shoot cluster into VPC that is managed by Gardener (that is created for another Shoot cluster). In this case the deletion of the initial Shoot cluster will fail to delete the VPC because there will be resources attached to it.\nGardener won\u0026rsquo;t delete any manually created (unmanaged) resources in your cloud provider account.\n 1. Configure AWS CLI The aws configure command is a convenient way to setup your AWS CLI. It will prompt you for your credentials and settings which will be used in the following AWS CLI invocations.\n$ aws configure AWS Access Key ID [None]: \u0026lt;ACCESS_KEY_ID\u0026gt; AWS Secret Access Key [None]: \u0026lt;SECRET_ACCESS_KEY\u0026gt; Default region name [None]: \u0026lt;DEFAULT_REGION\u0026gt; Default output format [None]: \u0026lt;DEFAULT_OUTPUT_FORMAT\u0026gt; 2. Create VPC Create the VPC by running the following command:\n$ aws ec2 create-vpc --cidr-block \u0026lt;cidr-block\u0026gt; { \u0026#34;Vpc\u0026#34;: { \u0026#34;VpcId\u0026#34;: \u0026#34;vpc-ff7bbf86\u0026#34;, \u0026#34;InstanceTenancy\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Tags\u0026#34;: [], \u0026#34;CidrBlockAssociations\u0026#34;: [ { \u0026#34;AssociationId\u0026#34;: \u0026#34;vpc-cidr-assoc-6e42b505\u0026#34;, \u0026#34;CidrBlock\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;CidrBlockState\u0026#34;: { \u0026#34;State\u0026#34;: \u0026#34;associated\u0026#34; } } ], \u0026#34;Ipv6CidrBlockAssociationSet\u0026#34;: [], \u0026#34;State\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;DhcpOptionsId\u0026#34;: \u0026#34;dopt-38f7a057\u0026#34;, \u0026#34;CidrBlock\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;IsDefault\u0026#34;: false } } Gardener requires the VPC to have enabled DNS support, i.e the attributes enableDnsSupport and enableDnsHostnames must be set to true. enableDnsSupport attribute is enabled by default, enableDnsHostnames - not. Set the enableDnsHostnames attribute to true:\n$ aws ec2 modify-vpc-attribute --vpc-id vpc-ff7bbf86 --enable-dns-hostnames 3. Create Internet Gateway Gardener also requires that an internet gateway is attached to the VPC. You can create one using:\n$ aws ec2 create-internet-gateway { \u0026#34;InternetGateway\u0026#34;: { \u0026#34;Tags\u0026#34;: [], \u0026#34;InternetGatewayId\u0026#34;: \u0026#34;igw-c0a643a9\u0026#34;, \u0026#34;Attachments\u0026#34;: [] } } and attach it to the VPC using:\n$ aws ec2 attach-internet-gateway --internet-gateway-id igw-c0a643a9 --vpc-id vpc-ff7bbf86 4. Create the Shoot Prepare your Shoot manifest (you could check the example manifests). Please make sure that you choose the region in which you had created the VPC earlier (step 2). Also, put your VPC ID in the .spec.provider.infrastructureConfig.networks.vpc.id field:\nspec:region:\u0026lt;aws-region-of-vpc\u0026gt; provider:type:awsinfrastructureConfig:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:vpc:id:vpc-ff7bbf86# ...Apply your Shoot manifest.\n$ kubectl apply -f your-shoot-aws.yaml Ensure that the Shoot cluster is properly created.\n$ kubectl get shoot $SHOOT_NAME -n $SHOOT_NAMESPACE NAME CLOUDPROFILE VERSION SEED DOMAIN OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE \u0026lt;SHOOT_NAME\u0026gt; aws 1.15.0 aws \u0026lt;SHOOT_DOMAIN\u0026gt; Succeeded 100 True True True True 20m "},{"uri":"https://gardener.cloud/v1.12.8/guides/administer_shoots/create-shoot-into-existing-aws-vpc/","title":"Create a Shoot cluster into existing AWS VPC","tags":[],"description":"","content":"Create a Shoot cluster into existing AWS VPC Gardener can create a new VPC, or use an existing one for your Shoot cluster. Depending on your needs you may want to create Shoot(s) into already created VPC. The tutorial describes how to create a Shoot cluster into existing AWS VPC. The steps are identical for Alicloud, Azure, and GCP. Please note that the existing VPC must be in the same region like the shoot cluster that you want to deploy into the VPC.\nTL;DR If .spec.provider.infrastructureConfig.networks.vpc.cidr is specified, Gardener will create a new VPC with the given CIDR block and respectively will delete it on Shoot deletion.\nIf .spec.provider.infrastructureConfig.networks.vpc.id is specified, Gardener will use the existing VPC and respectively won\u0026rsquo;t delete it on Shoot deletion.\n It\u0026rsquo;s not recommended to create a Shoot cluster into VPC that is managed by Gardener (that is created for another Shoot cluster). In this case the deletion of the initial Shoot cluster will fail to delete the VPC because there will be resources attached to it.\nGardener won\u0026rsquo;t delete any manually created (unmanaged) resources in your cloud provider account.\n 1. Configure AWS CLI The aws configure command is a convenient way to setup your AWS CLI. It will prompt you for your credentials and settings which will be used in the following AWS CLI invocations.\n$ aws configure AWS Access Key ID [None]: \u0026lt;ACCESS_KEY_ID\u0026gt; AWS Secret Access Key [None]: \u0026lt;SECRET_ACCESS_KEY\u0026gt; Default region name [None]: \u0026lt;DEFAULT_REGION\u0026gt; Default output format [None]: \u0026lt;DEFAULT_OUTPUT_FORMAT\u0026gt; 2. Create VPC Create the VPC by running the following command:\n$ aws ec2 create-vpc --cidr-block \u0026lt;cidr-block\u0026gt; { \u0026#34;Vpc\u0026#34;: { \u0026#34;VpcId\u0026#34;: \u0026#34;vpc-ff7bbf86\u0026#34;, \u0026#34;InstanceTenancy\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Tags\u0026#34;: [], \u0026#34;CidrBlockAssociations\u0026#34;: [ { \u0026#34;AssociationId\u0026#34;: \u0026#34;vpc-cidr-assoc-6e42b505\u0026#34;, \u0026#34;CidrBlock\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;CidrBlockState\u0026#34;: { \u0026#34;State\u0026#34;: \u0026#34;associated\u0026#34; } } ], \u0026#34;Ipv6CidrBlockAssociationSet\u0026#34;: [], \u0026#34;State\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;DhcpOptionsId\u0026#34;: \u0026#34;dopt-38f7a057\u0026#34;, \u0026#34;CidrBlock\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;IsDefault\u0026#34;: false } } Gardener requires the VPC to have enabled DNS support, i.e the attributes enableDnsSupport and enableDnsHostnames must be set to true. enableDnsSupport attribute is enabled by default, enableDnsHostnames - not. Set the enableDnsHostnames attribute to true:\n$ aws ec2 modify-vpc-attribute --vpc-id vpc-ff7bbf86 --enable-dns-hostnames 3. Create Internet Gateway Gardener also requires that an internet gateway is attached to the VPC. You can create one using:\n$ aws ec2 create-internet-gateway { \u0026#34;InternetGateway\u0026#34;: { \u0026#34;Tags\u0026#34;: [], \u0026#34;InternetGatewayId\u0026#34;: \u0026#34;igw-c0a643a9\u0026#34;, \u0026#34;Attachments\u0026#34;: [] } } and attach it to the VPC using:\n$ aws ec2 attach-internet-gateway --internet-gateway-id igw-c0a643a9 --vpc-id vpc-ff7bbf86 4. Create the Shoot Prepare your Shoot manifest (you could check the example manifests). Please make sure that you choose the region in which you had created the VPC earlier (step 2). Also, put your VPC ID in the .spec.provider.infrastructureConfig.networks.vpc.id field:\nspec:region:\u0026lt;aws-region-of-vpc\u0026gt; provider:type:awsinfrastructureConfig:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:vpc:id:vpc-ff7bbf86# ...Apply your Shoot manifest.\n$ kubectl apply -f your-shoot-aws.yaml Ensure that the Shoot cluster is properly created.\n$ kubectl get shoot $SHOOT_NAME -n $SHOOT_NAMESPACE NAME CLOUDPROFILE VERSION SEED DOMAIN OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE \u0026lt;SHOOT_NAME\u0026gt; aws 1.15.0 aws \u0026lt;SHOOT_DOMAIN\u0026gt; Succeeded 100 True True True True 20m "},{"uri":"https://gardener.cloud/v1.13.2/guides/administer_shoots/create-shoot-into-existing-aws-vpc/","title":"Create a Shoot cluster into existing AWS VPC","tags":[],"description":"","content":"Create a Shoot cluster into existing AWS VPC Gardener can create a new VPC, or use an existing one for your Shoot cluster. Depending on your needs you may want to create Shoot(s) into already created VPC. The tutorial describes how to create a Shoot cluster into existing AWS VPC. The steps are identical for Alicloud, Azure, and GCP. Please note that the existing VPC must be in the same region like the shoot cluster that you want to deploy into the VPC.\nTL;DR If .spec.provider.infrastructureConfig.networks.vpc.cidr is specified, Gardener will create a new VPC with the given CIDR block and respectively will delete it on Shoot deletion.\nIf .spec.provider.infrastructureConfig.networks.vpc.id is specified, Gardener will use the existing VPC and respectively won\u0026rsquo;t delete it on Shoot deletion.\n It\u0026rsquo;s not recommended to create a Shoot cluster into VPC that is managed by Gardener (that is created for another Shoot cluster). In this case the deletion of the initial Shoot cluster will fail to delete the VPC because there will be resources attached to it.\nGardener won\u0026rsquo;t delete any manually created (unmanaged) resources in your cloud provider account.\n 1. Configure AWS CLI The aws configure command is a convenient way to setup your AWS CLI. It will prompt you for your credentials and settings which will be used in the following AWS CLI invocations.\n$ aws configure AWS Access Key ID [None]: \u0026lt;ACCESS_KEY_ID\u0026gt; AWS Secret Access Key [None]: \u0026lt;SECRET_ACCESS_KEY\u0026gt; Default region name [None]: \u0026lt;DEFAULT_REGION\u0026gt; Default output format [None]: \u0026lt;DEFAULT_OUTPUT_FORMAT\u0026gt; 2. Create VPC Create the VPC by running the following command:\n$ aws ec2 create-vpc --cidr-block \u0026lt;cidr-block\u0026gt; { \u0026#34;Vpc\u0026#34;: { \u0026#34;VpcId\u0026#34;: \u0026#34;vpc-ff7bbf86\u0026#34;, \u0026#34;InstanceTenancy\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Tags\u0026#34;: [], \u0026#34;CidrBlockAssociations\u0026#34;: [ { \u0026#34;AssociationId\u0026#34;: \u0026#34;vpc-cidr-assoc-6e42b505\u0026#34;, \u0026#34;CidrBlock\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;CidrBlockState\u0026#34;: { \u0026#34;State\u0026#34;: \u0026#34;associated\u0026#34; } } ], \u0026#34;Ipv6CidrBlockAssociationSet\u0026#34;: [], \u0026#34;State\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;DhcpOptionsId\u0026#34;: \u0026#34;dopt-38f7a057\u0026#34;, \u0026#34;CidrBlock\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;IsDefault\u0026#34;: false } } Gardener requires the VPC to have enabled DNS support, i.e the attributes enableDnsSupport and enableDnsHostnames must be set to true. enableDnsSupport attribute is enabled by default, enableDnsHostnames - not. Set the enableDnsHostnames attribute to true:\n$ aws ec2 modify-vpc-attribute --vpc-id vpc-ff7bbf86 --enable-dns-hostnames 3. Create Internet Gateway Gardener also requires that an internet gateway is attached to the VPC. You can create one using:\n$ aws ec2 create-internet-gateway { \u0026#34;InternetGateway\u0026#34;: { \u0026#34;Tags\u0026#34;: [], \u0026#34;InternetGatewayId\u0026#34;: \u0026#34;igw-c0a643a9\u0026#34;, \u0026#34;Attachments\u0026#34;: [] } } and attach it to the VPC using:\n$ aws ec2 attach-internet-gateway --internet-gateway-id igw-c0a643a9 --vpc-id vpc-ff7bbf86 4. Create the Shoot Prepare your Shoot manifest (you could check the example manifests). Please make sure that you choose the region in which you had created the VPC earlier (step 2). Also, put your VPC ID in the .spec.provider.infrastructureConfig.networks.vpc.id field:\nspec:region:\u0026lt;aws-region-of-vpc\u0026gt; provider:type:awsinfrastructureConfig:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:vpc:id:vpc-ff7bbf86# ...Apply your Shoot manifest.\n$ kubectl apply -f your-shoot-aws.yaml Ensure that the Shoot cluster is properly created.\n$ kubectl get shoot $SHOOT_NAME -n $SHOOT_NAMESPACE NAME CLOUDPROFILE VERSION SEED DOMAIN OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE \u0026lt;SHOOT_NAME\u0026gt; aws 1.15.0 aws \u0026lt;SHOOT_DOMAIN\u0026gt; Succeeded 100 True True True True 20m "},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/gardener-alibaba/","title":"Create Shoot Clusters in Alibaba Cloud","tags":[],"description":"","content":"Create Shoot Clusters in Alibaba Cloud Prerequisites  You need an Alibaba Cloud account. You have access to the Gardener dashboard and have permissions to create projects.  Procedure   Go to the Gardener dashboard and create a project.\n To be able to add shoot clusters to this project, you must first create a technical user on Alibaba cloud with sufficient permissions.\n   To copy the policy for Alibaba Cloud from the Gardener dashboard, choose Secrets, click on the help button ( ) for Alibaba Cloud secrets, and choose copy ( ).\n  To create a custom policy in Alibaba cloud, log on to your Alibaba account and choose RAM \u0026gt; Permissions \u0026gt; Policies. Paste policy that you copied from the Gardener dashboard to this custom policy.\n  In the Alibaba cloud console, choose RAM \u0026gt; Users and create a new technical user.\n After the user is created, AccessKeyId and AccessKeySecret are generated and displayed. Remember to save them. The AccessKey is used later to create secrets for Gardener.\n   Choose RAM \u0026gt; Permissions \u0026gt; Grants and assign the policy youve created before to the technical user.\n  On the Gardener dashboard, choose Secrets and then the plus sign in the Alibaba Cloud frame to add a new Alibaba Cloud secret.\n  Copy the AccessKeyId and AccessKeySecret you saved when you created the technical user on Alibaba Cloud Console.\n After successfully creating secrets for Alibaba Cloud, you can see them in the corresponding dropdown list whenever you create a shoot cluster on Alibaba cloud.\n   Choose Clusters and then the plus sign in the lower right.\n  On tab INFRASTRUCTURE, choose the secret you created before. The technical user related to the chosen Secret is used to create infrastructure resources on Alibaba Cloud.\n  Result You can now create shoot clusters on Alibaba cloud.\n The size of persistent volumes in your shoot cluster must at least be 20 GiB large. If you choose smaller sizes in your Kubernetes PV definition, the allocation of cloud disk space on Alibaba cloud fails.\n "},{"uri":"https://gardener.cloud/v1.12.8/guides/administer_shoots/gardener-alibaba/","title":"Create Shoot Clusters in Alibaba Cloud","tags":[],"description":"","content":"Create Shoot Clusters in Alibaba Cloud Prerequisites  You need an Alibaba Cloud account. You have access to the Gardener dashboard and have permissions to create projects.  Procedure   Go to the Gardener dashboard and create a project.\n To be able to add shoot clusters to this project, you must first create a technical user on Alibaba cloud with sufficient permissions.\n   To copy the policy for Alibaba Cloud from the Gardener dashboard, choose Secrets, click on the help button ( ) for Alibaba Cloud secrets, and choose copy ( ).\n  To create a custom policy in Alibaba cloud, log on to your Alibaba account and choose RAM \u0026gt; Permissions \u0026gt; Policies. Paste policy that you copied from the Gardener dashboard to this custom policy.\n  In the Alibaba cloud console, choose RAM \u0026gt; Users and create a new technical user.\n After the user is created, AccessKeyId and AccessKeySecret are generated and displayed. Remember to save them. The AccessKey is used later to create secrets for Gardener.\n   Choose RAM \u0026gt; Permissions \u0026gt; Grants and assign the policy youve created before to the technical user.\n  On the Gardener dashboard, choose Secrets and then the plus sign in the Alibaba Cloud frame to add a new Alibaba Cloud secret.\n  Copy the AccessKeyId and AccessKeySecret you saved when you created the technical user on Alibaba Cloud Console.\n After successfully creating secrets for Alibaba Cloud, you can see them in the corresponding dropdown list whenever you create a shoot cluster on Alibaba cloud.\n   Choose Clusters and then the plus sign in the lower right.\n  On tab INFRASTRUCTURE, choose the secret you created before. The technical user related to the chosen Secret is used to create infrastructure resources on Alibaba Cloud.\n  Result You can now create shoot clusters on Alibaba cloud.\n The size of persistent volumes in your shoot cluster must at least be 20 GiB large. If you choose smaller sizes in your Kubernetes PV definition, the allocation of cloud disk space on Alibaba cloud fails.\n "},{"uri":"https://gardener.cloud/v1.13.2/guides/administer_shoots/gardener-alibaba/","title":"Create Shoot Clusters in Alibaba Cloud","tags":[],"description":"","content":"Create Shoot Clusters in Alibaba Cloud Prerequisites  You need an Alibaba Cloud account. You have access to the Gardener dashboard and have permissions to create projects.  Procedure   Go to the Gardener dashboard and create a project.\n To be able to add shoot clusters to this project, you must first create a technical user on Alibaba cloud with sufficient permissions.\n   To copy the policy for Alibaba Cloud from the Gardener dashboard, choose Secrets, click on the help button ( ) for Alibaba Cloud secrets, and choose copy ( ).\n  To create a custom policy in Alibaba cloud, log on to your Alibaba account and choose RAM \u0026gt; Permissions \u0026gt; Policies. Paste policy that you copied from the Gardener dashboard to this custom policy.\n  In the Alibaba cloud console, choose RAM \u0026gt; Users and create a new technical user.\n After the user is created, AccessKeyId and AccessKeySecret are generated and displayed. Remember to save them. The AccessKey is used later to create secrets for Gardener.\n   Choose RAM \u0026gt; Permissions \u0026gt; Grants and assign the policy youve created before to the technical user.\n  On the Gardener dashboard, choose Secrets and then the plus sign in the Alibaba Cloud frame to add a new Alibaba Cloud secret.\n  Copy the AccessKeyId and AccessKeySecret you saved when you created the technical user on Alibaba Cloud Console.\n After successfully creating secrets for Alibaba Cloud, you can see them in the corresponding dropdown list whenever you create a shoot cluster on Alibaba cloud.\n   Choose Clusters and then the plus sign in the lower right.\n  On tab INFRASTRUCTURE, choose the secret you created before. The technical user related to the chosen Secret is used to create infrastructure resources on Alibaba Cloud.\n  Result You can now create shoot clusters on Alibaba cloud.\n The size of persistent volumes in your shoot cluster must at least be 20 GiB large. If you choose smaller sizes in your Kubernetes PV definition, the allocation of cloud disk space on Alibaba cloud fails.\n "},{"uri":"https://gardener.cloud/documentation/guides/applications/secure-seccomp/","title":"Custom Seccomp profile","tags":[],"description":"","content":"Custom Seccomp profile Context Seccomp (secure computing mode) is a security facility in the Linux kernel for restricting the set of system calls applications can make.\nStarting from Kubernetes v1.3.0 the Seccomp feature is in Alpha. To configure it on a Pod, the following annotations can be used:\n seccomp.security.alpha.kubernetes.io/pod: \u0026lt;seccomp-profile\u0026gt; where \u0026lt;seccomp-profile\u0026gt; is the seccomp profile to apply to all containers in a Pod. container.seccomp.security.alpha.kubernetes.io/\u0026lt;container-name\u0026gt;: \u0026lt;seccomp-profile\u0026gt; where \u0026lt;seccomp-profile\u0026gt; is the seccomp profile to apply to \u0026lt;container-name\u0026gt; in a Pod.  More details can be found in the PodSecurityPolicy documentation.\nInstallation of custom profile By default, kubelet loads custom Seccomp profiles from /var/lib/kubelet/seccomp/. There are two ways in which Seccomp profiles can be added to a Node:\n to be baked in the machine image to be added at runtime.  This guide focuses on creating those profiles via a DaemonSet.\nCreate a file called seccomp-profile.yaml with the following content:\napiVersion:v1kind:ConfigMapmetadata:name:seccomp-profilenamespace:kube-systemdata:my-profile.json:| {\u0026#34;defaultAction\u0026#34;: \u0026#34;SCMP_ACT_ALLOW\u0026#34;,\u0026#34;syscalls\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;chmod\u0026#34;,\u0026#34;action\u0026#34;: \u0026#34;SCMP_ACT_ERRNO\u0026#34;}]} The policy above is a very simple one and not siutable for complex applications. The default docker profile can be used a reference. Feel free to modify it to your needs.\n Apply the ConfigMap in your cluster:\n$ kubectl apply -f seccomp-profile.yaml configmap/seccomp-profile created The next steps is to create the DaemonSet seccomp installer. It\u0026rsquo;s going to copy the policy from above in /var/lib/kubelet/seccomp/my-profile.json.\nCreate a file called seccomp-installer.yaml with the following content:\napiVersion:apps/v1kind:DaemonSetmetadata:name:seccompnamespace:kube-systemlabels:security:seccompspec:selector:matchLabels:security:seccomptemplate:metadata:labels:security:seccompspec:initContainers:- name:installerimage:alpine:3.10.0command:[\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;cp -r -L /seccomp/*.json /host/seccomp/\u0026#34;]volumeMounts:- name:profilesmountPath:/seccomp- name:hostseccompmountPath:/host/seccompreadOnly:falsecontainers:- name:pauseimage:k8s.gcr.io/pause:3.1terminationGracePeriodSeconds:5volumes:- name:hostseccomphostPath:path:/var/lib/kubelet/seccomp- name:profilesconfigMap:name:seccomp-profileCreate the installer and wait until it\u0026rsquo;s ready on all Nodes:\n$ kubectl apply -f seccomp-installer.yaml daemonset.apps/seccomp-installer created $ kubectl -n kube-system get pods -l security=seccomp NAME READY STATUS RESTARTS AGE seccomp-installer-wjbxq 1/1 Running 0 21s Create a Pod using custom Seccomp profile Finally we want to create a profile which uses our new Seccomp profile my-profile.json.\nCreate a file called my-seccomp-pod.yaml with the following content:\napiVersion:v1kind:Podmetadata:name:seccomp-appnamespace:defaultannotations:seccomp.security.alpha.kubernetes.io/pod:\u0026#34;localhost/my-profile.json\u0026#34;# you can specify seccomp profile per container. If you add another profile you can configure# it for a specific container - \u0026#39;pause\u0026#39; in this case.# container.seccomp.security.alpha.kubernetes.io/pause: \u0026#34;localhost/some-other-profile.json\u0026#34;spec:containers:- name:pauseimage:k8s.gcr.io/pause:3.1Create the Pod and see that\u0026rsquo;s running:\n$ kubectl apply -f my-seccomp-pod.yaml pod/seccomp-app created $ kubectl get pod seccomp-app NAME READY STATUS RESTARTS AGE seccomp-app 1/1 Running 0 42s Throubleshooting If an invalid or not existing profile is used then the Pod will be stuck in ContainerCreating phase:\nbroken-seccomp-pod.yaml:\napiVersion:v1kind:Podmetadata:name:broken-seccompnamespace:defaultannotations:seccomp.security.alpha.kubernetes.io/pod:\u0026#34;localhost/not-existing-profile.json\u0026#34;spec:containers:- name:pauseimage:k8s.gcr.io/pause:3.1$ kubectl apply -f broken-seccomp-pod.yaml pod/broken-seccomp created $ kubectl get pod broken-seccomp NAME READY STATUS RESTARTS AGE broken-seccomp 1/1 ContainerCreating 0 2m $ kubectl describe pod broken-seccomp Name: broken-seccomp Namespace: default .... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18s default-scheduler Successfully assigned kube-system/broken-seccomp to docker-desktop Warning FailedCreatePodSandBox 4s (x2 over 18s) kubelet, docker-desktop Failed create pod sandbox: rpc error: code = Unknown desc = failed to make sandbox docker config for pod \u0026#34;broken-seccomp\u0026#34;: failed to generate sandbox security options for sandbox \u0026#34;broken-seccomp\u0026#34;: failed to generate seccomp security options for container: cannot load seccomp profile \u0026#34;/var/lib/kubelet/seccomp/not-existing-profile.json\u0026#34;: open /var/lib/kubelet/seccomp/not-existing-profile.json: no such file or directory Further reading  https://en.wikipedia.org/wiki/Seccomp https://docs.docker.com/engine/security/seccomp https://lwn.net/Articles/656307/ http://man7.org/conf/lpc2015/limiting_kernel_attack_surface_with_seccomp-LPC_2015-Kerrisk.pdf  "},{"uri":"https://gardener.cloud/v1.12.8/guides/applications/secure-seccomp/","title":"Custom Seccomp profile","tags":[],"description":"","content":"Custom Seccomp profile Context Seccomp (secure computing mode) is a security facility in the Linux kernel for restricting the set of system calls applications can make.\nStarting from Kubernetes v1.3.0 the Seccomp feature is in Alpha. To configure it on a Pod, the following annotations can be used:\n seccomp.security.alpha.kubernetes.io/pod: \u0026lt;seccomp-profile\u0026gt; where \u0026lt;seccomp-profile\u0026gt; is the seccomp profile to apply to all containers in a Pod. container.seccomp.security.alpha.kubernetes.io/\u0026lt;container-name\u0026gt;: \u0026lt;seccomp-profile\u0026gt; where \u0026lt;seccomp-profile\u0026gt; is the seccomp profile to apply to \u0026lt;container-name\u0026gt; in a Pod.  More details can be found in the PodSecurityPolicy documentation.\nInstallation of custom profile By default, kubelet loads custom Seccomp profiles from /var/lib/kubelet/seccomp/. There are two ways in which Seccomp profiles can be added to a Node:\n to be baked in the machine image to be added at runtime.  This guide focuses on creating those profiles via a DaemonSet.\nCreate a file called seccomp-profile.yaml with the following content:\napiVersion:v1kind:ConfigMapmetadata:name:seccomp-profilenamespace:kube-systemdata:my-profile.json:| {\u0026#34;defaultAction\u0026#34;: \u0026#34;SCMP_ACT_ALLOW\u0026#34;,\u0026#34;syscalls\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;chmod\u0026#34;,\u0026#34;action\u0026#34;: \u0026#34;SCMP_ACT_ERRNO\u0026#34;}]} The policy above is a very simple one and not siutable for complex applications. The default docker profile can be used a reference. Feel free to modify it to your needs.\n Apply the ConfigMap in your cluster:\n$ kubectl apply -f seccomp-profile.yaml configmap/seccomp-profile created The next steps is to create the DaemonSet seccomp installer. It\u0026rsquo;s going to copy the policy from above in /var/lib/kubelet/seccomp/my-profile.json.\nCreate a file called seccomp-installer.yaml with the following content:\napiVersion:apps/v1kind:DaemonSetmetadata:name:seccompnamespace:kube-systemlabels:security:seccompspec:selector:matchLabels:security:seccomptemplate:metadata:labels:security:seccompspec:initContainers:- name:installerimage:alpine:3.10.0command:[\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;cp -r -L /seccomp/*.json /host/seccomp/\u0026#34;]volumeMounts:- name:profilesmountPath:/seccomp- name:hostseccompmountPath:/host/seccompreadOnly:falsecontainers:- name:pauseimage:k8s.gcr.io/pause:3.1terminationGracePeriodSeconds:5volumes:- name:hostseccomphostPath:path:/var/lib/kubelet/seccomp- name:profilesconfigMap:name:seccomp-profileCreate the installer and wait until it\u0026rsquo;s ready on all Nodes:\n$ kubectl apply -f seccomp-installer.yaml daemonset.apps/seccomp-installer created $ kubectl -n kube-system get pods -l security=seccomp NAME READY STATUS RESTARTS AGE seccomp-installer-wjbxq 1/1 Running 0 21s Create a Pod using custom Seccomp profile Finally we want to create a profile which uses our new Seccomp profile my-profile.json.\nCreate a file called my-seccomp-pod.yaml with the following content:\napiVersion:v1kind:Podmetadata:name:seccomp-appnamespace:defaultannotations:seccomp.security.alpha.kubernetes.io/pod:\u0026#34;localhost/my-profile.json\u0026#34;# you can specify seccomp profile per container. If you add another profile you can configure# it for a specific container - \u0026#39;pause\u0026#39; in this case.# container.seccomp.security.alpha.kubernetes.io/pause: \u0026#34;localhost/some-other-profile.json\u0026#34;spec:containers:- name:pauseimage:k8s.gcr.io/pause:3.1Create the Pod and see that\u0026rsquo;s running:\n$ kubectl apply -f my-seccomp-pod.yaml pod/seccomp-app created $ kubectl get pod seccomp-app NAME READY STATUS RESTARTS AGE seccomp-app 1/1 Running 0 42s Throubleshooting If an invalid or not existing profile is used then the Pod will be stuck in ContainerCreating phase:\nbroken-seccomp-pod.yaml:\napiVersion:v1kind:Podmetadata:name:broken-seccompnamespace:defaultannotations:seccomp.security.alpha.kubernetes.io/pod:\u0026#34;localhost/not-existing-profile.json\u0026#34;spec:containers:- name:pauseimage:k8s.gcr.io/pause:3.1$ kubectl apply -f broken-seccomp-pod.yaml pod/broken-seccomp created $ kubectl get pod broken-seccomp NAME READY STATUS RESTARTS AGE broken-seccomp 1/1 ContainerCreating 0 2m $ kubectl describe pod broken-seccomp Name: broken-seccomp Namespace: default .... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18s default-scheduler Successfully assigned kube-system/broken-seccomp to docker-desktop Warning FailedCreatePodSandBox 4s (x2 over 18s) kubelet, docker-desktop Failed create pod sandbox: rpc error: code = Unknown desc = failed to make sandbox docker config for pod \u0026#34;broken-seccomp\u0026#34;: failed to generate sandbox security options for sandbox \u0026#34;broken-seccomp\u0026#34;: failed to generate seccomp security options for container: cannot load seccomp profile \u0026#34;/var/lib/kubelet/seccomp/not-existing-profile.json\u0026#34;: open /var/lib/kubelet/seccomp/not-existing-profile.json: no such file or directory Further reading  https://en.wikipedia.org/wiki/Seccomp https://docs.docker.com/engine/security/seccomp https://lwn.net/Articles/656307/ http://man7.org/conf/lpc2015/limiting_kernel_attack_surface_with_seccomp-LPC_2015-Kerrisk.pdf  "},{"uri":"https://gardener.cloud/v1.13.2/guides/applications/secure-seccomp/","title":"Custom Seccomp profile","tags":[],"description":"","content":"Custom Seccomp profile Context Seccomp (secure computing mode) is a security facility in the Linux kernel for restricting the set of system calls applications can make.\nStarting from Kubernetes v1.3.0 the Seccomp feature is in Alpha. To configure it on a Pod, the following annotations can be used:\n seccomp.security.alpha.kubernetes.io/pod: \u0026lt;seccomp-profile\u0026gt; where \u0026lt;seccomp-profile\u0026gt; is the seccomp profile to apply to all containers in a Pod. container.seccomp.security.alpha.kubernetes.io/\u0026lt;container-name\u0026gt;: \u0026lt;seccomp-profile\u0026gt; where \u0026lt;seccomp-profile\u0026gt; is the seccomp profile to apply to \u0026lt;container-name\u0026gt; in a Pod.  More details can be found in the PodSecurityPolicy documentation.\nInstallation of custom profile By default, kubelet loads custom Seccomp profiles from /var/lib/kubelet/seccomp/. There are two ways in which Seccomp profiles can be added to a Node:\n to be baked in the machine image to be added at runtime.  This guide focuses on creating those profiles via a DaemonSet.\nCreate a file called seccomp-profile.yaml with the following content:\napiVersion:v1kind:ConfigMapmetadata:name:seccomp-profilenamespace:kube-systemdata:my-profile.json:| {\u0026#34;defaultAction\u0026#34;: \u0026#34;SCMP_ACT_ALLOW\u0026#34;,\u0026#34;syscalls\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;chmod\u0026#34;,\u0026#34;action\u0026#34;: \u0026#34;SCMP_ACT_ERRNO\u0026#34;}]} The policy above is a very simple one and not siutable for complex applications. The default docker profile can be used a reference. Feel free to modify it to your needs.\n Apply the ConfigMap in your cluster:\n$ kubectl apply -f seccomp-profile.yaml configmap/seccomp-profile created The next steps is to create the DaemonSet seccomp installer. It\u0026rsquo;s going to copy the policy from above in /var/lib/kubelet/seccomp/my-profile.json.\nCreate a file called seccomp-installer.yaml with the following content:\napiVersion:apps/v1kind:DaemonSetmetadata:name:seccompnamespace:kube-systemlabels:security:seccompspec:selector:matchLabels:security:seccomptemplate:metadata:labels:security:seccompspec:initContainers:- name:installerimage:alpine:3.10.0command:[\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;cp -r -L /seccomp/*.json /host/seccomp/\u0026#34;]volumeMounts:- name:profilesmountPath:/seccomp- name:hostseccompmountPath:/host/seccompreadOnly:falsecontainers:- name:pauseimage:k8s.gcr.io/pause:3.1terminationGracePeriodSeconds:5volumes:- name:hostseccomphostPath:path:/var/lib/kubelet/seccomp- name:profilesconfigMap:name:seccomp-profileCreate the installer and wait until it\u0026rsquo;s ready on all Nodes:\n$ kubectl apply -f seccomp-installer.yaml daemonset.apps/seccomp-installer created $ kubectl -n kube-system get pods -l security=seccomp NAME READY STATUS RESTARTS AGE seccomp-installer-wjbxq 1/1 Running 0 21s Create a Pod using custom Seccomp profile Finally we want to create a profile which uses our new Seccomp profile my-profile.json.\nCreate a file called my-seccomp-pod.yaml with the following content:\napiVersion:v1kind:Podmetadata:name:seccomp-appnamespace:defaultannotations:seccomp.security.alpha.kubernetes.io/pod:\u0026#34;localhost/my-profile.json\u0026#34;# you can specify seccomp profile per container. If you add another profile you can configure# it for a specific container - \u0026#39;pause\u0026#39; in this case.# container.seccomp.security.alpha.kubernetes.io/pause: \u0026#34;localhost/some-other-profile.json\u0026#34;spec:containers:- name:pauseimage:k8s.gcr.io/pause:3.1Create the Pod and see that\u0026rsquo;s running:\n$ kubectl apply -f my-seccomp-pod.yaml pod/seccomp-app created $ kubectl get pod seccomp-app NAME READY STATUS RESTARTS AGE seccomp-app 1/1 Running 0 42s Throubleshooting If an invalid or not existing profile is used then the Pod will be stuck in ContainerCreating phase:\nbroken-seccomp-pod.yaml:\napiVersion:v1kind:Podmetadata:name:broken-seccompnamespace:defaultannotations:seccomp.security.alpha.kubernetes.io/pod:\u0026#34;localhost/not-existing-profile.json\u0026#34;spec:containers:- name:pauseimage:k8s.gcr.io/pause:3.1$ kubectl apply -f broken-seccomp-pod.yaml pod/broken-seccomp created $ kubectl get pod broken-seccomp NAME READY STATUS RESTARTS AGE broken-seccomp 1/1 ContainerCreating 0 2m $ kubectl describe pod broken-seccomp Name: broken-seccomp Namespace: default .... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18s default-scheduler Successfully assigned kube-system/broken-seccomp to docker-desktop Warning FailedCreatePodSandBox 4s (x2 over 18s) kubelet, docker-desktop Failed create pod sandbox: rpc error: code = Unknown desc = failed to make sandbox docker config for pod \u0026#34;broken-seccomp\u0026#34;: failed to generate sandbox security options for sandbox \u0026#34;broken-seccomp\u0026#34;: failed to generate seccomp security options for container: cannot load seccomp profile \u0026#34;/var/lib/kubelet/seccomp/not-existing-profile.json\u0026#34;: open /var/lib/kubelet/seccomp/not-existing-profile.json: no such file or directory Further reading  https://en.wikipedia.org/wiki/Seccomp https://docs.docker.com/engine/security/seccomp https://lwn.net/Articles/656307/ http://man7.org/conf/lpc2015/limiting_kernel_attack_surface_with_seccomp-LPC_2015-Kerrisk.pdf  "},{"uri":"https://gardener.cloud/components/dashboard/","title":"Dashboard","tags":[],"description":"","content":"Gardener Dashboard  \nDemo Development Setup Install Install all dependencies\nyarn Configuration KUBECONFIG If the dashboard is not running in the Garden Cluster you have to point the kubeconfig to Garden Cluster. This can be done in the default kubeconfig file in ${HOME}/.kube/config or by the KUBECONFIG environment variable.\nGARDENER_CONFIG The configuration file of the Gardener Dashboard can be specified as first command line argument or as environment variable GARDENER_CONFIG at the server process. If nothing is specified the default location is ${HOME}/.gardener/config.yaml.\nA local configuration example for minikube and dex could look like follows:\nport:3030logLevel:debuglogFormat:textapiServerUrl:https://minkube# garden cluster kube-apiserver urlsessionSecret:c2VjcmV0# symetric key used for encryptionoidc:issuer:https://minikube:32001client_id:dashboardclient_secret:c2VjcmV0# oauth client secretredirect_uri:http://localhost:8080/auth/callbackscope:\u0026#39;openid email profile groups audience:server:client_id:dashboard audience:server:client_id:kube-kubectl\u0026#39;clockTolerance:15frontend:dashboardUrl:pathname:/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy/defaultHibernationSchedule:evaluation:- start:0017**1,2,3,4,5development:- start:0017**1,2,3,4,5end:0008**1,2,3,4,5production:~Run locally (during development) Concurrently run the backend server (port 3030) and the frontend server (port 8080) both with hot reload enabled.\nyarn serve All request to /api, /auth and /config.json will be proxied by default to the backend server.\nBuild Build docker image locally.\nmake build Push Push docker image to Google Container Registry.\nmake push This command expects a valid gcloud configuration named gardener.\ngcloud config configurations describe gardener is_active: true name: gardener properties: core: account: john.doe@example.org project: johndoe-1008 People The following SAP developers contributed to this project until this initial contribution was published as open source.\n   contributor commits (%) +lines -lines first commit last commit     Holger Koser 313 (42%) 57878 18562 2017-07-13 2018-01-23   Andreas Herz 307 (41%) 13666 11099 2017-07-14 2017-10-27   Peter Sutter 99 (13%) 4838 3967 2017-11-07 2018-01-23   Gross, Lukas 31 (4%) 400 267 2018-01-10 2018-01-23    It is derived from the historical, internal gardener-ui repository at commit eeb623d60c86e6037c0e1dc2bdd9e54663bf41a8.\nLicense Apache License 2.0\nCopyright 2020 The Gardener Authors\n"},{"uri":"https://gardener.cloud/components/dashboard/","title":"Dashboard","tags":[],"description":"","content":"Gardener Dashboard  \nDemo Development Setup Install Install all dependencies\nyarn Configuration KUBECONFIG If the dashboard is not running in the Garden Cluster you have to point the kubeconfig to Garden Cluster. This can be done in the default kubeconfig file in ${HOME}/.kube/config or by the KUBECONFIG environment variable.\nGARDENER_CONFIG The configuration file of the Gardener Dashboard can be specified as first command line argument or as environment variable GARDENER_CONFIG at the server process. If nothing is specified the default location is ${HOME}/.gardener/config.yaml.\nA local configuration example for minikube and dex could look like follows:\nport:3030logLevel:debuglogFormat:textapiServerUrl:https://minkube# garden cluster kube-apiserver urlsessionSecret:c2VjcmV0# symetric key used for encryptionoidc:issuer:https://minikube:32001client_id:dashboardclient_secret:c2VjcmV0# oauth client secretredirect_uri:http://localhost:8080/auth/callbackscope:\u0026#39;openid email profile groups audience:server:client_id:dashboard audience:server:client_id:kube-kubectl\u0026#39;clockTolerance:15frontend:dashboardUrl:pathname:/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy/defaultHibernationSchedule:evaluation:- start:0017**1,2,3,4,5development:- start:0017**1,2,3,4,5end:0008**1,2,3,4,5production:~Run locally (during development) Concurrently run the backend server (port 3030) and the frontend server (port 8080) both with hot reload enabled.\nyarn serve All request to /api, /auth and /config.json will be proxied by default to the backend server.\nBuild Build docker image locally.\nmake build Push Push docker image to Google Container Registry.\nmake push This command expects a valid gcloud configuration named gardener.\ngcloud config configurations describe gardener is_active: true name: gardener properties: core: account: john.doe@example.org project: johndoe-1008 People The following SAP developers contributed to this project until this initial contribution was published as open source.\n   contributor commits (%) +lines -lines first commit last commit     Holger Koser 313 (42%) 57878 18562 2017-07-13 2018-01-23   Andreas Herz 307 (41%) 13666 11099 2017-07-14 2017-10-27   Peter Sutter 99 (13%) 4838 3967 2017-11-07 2018-01-23   Gross, Lukas 31 (4%) 400 267 2018-01-10 2018-01-23    It is derived from the historical, internal gardener-ui repository at commit eeb623d60c86e6037c0e1dc2bdd9e54663bf41a8.\nLicense Apache License 2.0\nCopyright 2020 The Gardener Authors\n"},{"uri":"https://gardener.cloud/components/dashboard/","title":"Dashboard","tags":[],"description":"","content":"Gardener Dashboard  \nDemo Development Setup Install Install all dependencies\nyarn Configuration KUBECONFIG If the dashboard is not running in the Garden Cluster you have to point the kubeconfig to Garden Cluster. This can be done in the default kubeconfig file in ${HOME}/.kube/config or by the KUBECONFIG environment variable.\nGARDENER_CONFIG The configuration file of the Gardener Dashboard can be specified as first command line argument or as environment variable GARDENER_CONFIG at the server process. If nothing is specified the default location is ${HOME}/.gardener/config.yaml.\nA local configuration example for minikube and dex could look like follows:\nport:3030logLevel:debuglogFormat:textapiServerUrl:https://minkube# garden cluster kube-apiserver urlsessionSecret:c2VjcmV0# symetric key used for encryptionoidc:issuer:https://minikube:32001client_id:dashboardclient_secret:c2VjcmV0# oauth client secretredirect_uri:http://localhost:8080/auth/callbackscope:\u0026#39;openid email profile groups audience:server:client_id:dashboard audience:server:client_id:kube-kubectl\u0026#39;clockTolerance:15frontend:dashboardUrl:pathname:/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy/defaultHibernationSchedule:evaluation:- start:0017**1,2,3,4,5development:- start:0017**1,2,3,4,5end:0008**1,2,3,4,5production:~Run locally (during development) Concurrently run the backend server (port 3030) and the frontend server (port 8080) both with hot reload enabled.\nyarn serve All request to /api, /auth and /config.json will be proxied by default to the backend server.\nBuild Build docker image locally.\nmake build Push Push docker image to Google Container Registry.\nmake push This command expects a valid gcloud configuration named gardener.\ngcloud config configurations describe gardener is_active: true name: gardener properties: core: account: john.doe@example.org project: johndoe-1008 People The following SAP developers contributed to this project until this initial contribution was published as open source.\n   contributor commits (%) +lines -lines first commit last commit     Holger Koser 313 (42%) 57878 18562 2017-07-13 2018-01-23   Andreas Herz 307 (41%) 13666 11099 2017-07-14 2017-10-27   Peter Sutter 99 (13%) 4838 3967 2017-11-07 2018-01-23   Gross, Lukas 31 (4%) 400 267 2018-01-10 2018-01-23    It is derived from the historical, internal gardener-ui repository at commit eeb623d60c86e6037c0e1dc2bdd9e54663bf41a8.\nLicense Apache License 2.0\nCopyright 2020 The Gardener Authors\n"},{"uri":"https://gardener.cloud/documentation/contribute/10_code/20_dependencies/","title":"Dependencies","tags":[],"description":"","content":"Testing We follow the BDD-style testing principles and are leveraging the Ginkgo framework along with Gomega as matcher library. In order to execute the existing tests, you can use\nmake test # runs tests make verify # runs static code checks and test There is an additional command for analyzing the code coverage of the tests. Ginkgo will generate standard Golang cover profiles which will be translated into a HTML file by the Go Cover Tool. Another command helps you to clean up the filesystem from the temporary cover profile files and the HTML report:\nmake test-cov open gardener.coverage.html make test-cov-clean sigs.k8s.io/controller-runtime env test Some of the integration tests in Gardener are using the sigs.k8s.io/controller-runtime/pkg/envtest package. It sets up a temporary control plane (etcd + kube-apiserver) against the integration tests can run. The test and test-cov rules in the Makefile prepare this env test automatically by downloading the respective binaries (if not yet present) and set the necessary environment variables.\nYou can also run go test or ginkgo without the test/test-cov rules. In this case you have to set the KUBEBUILDER_ASSETS environment variable to the path that contains the etcd + kube-apiserver binaries or you need to have the binaries pre-installed under /usr/local/kubebuilder/bin.\nDependency Management We are using go modules for depedency management. In order to add a new package dependency to the project, you can perform go get \u0026lt;PACKAGE\u0026gt;@\u0026lt;VERSION\u0026gt; or edit the go.mod file and append the package along with the version you want to use.\nUpdating Dependencies The Makefile contains a rule called revendor which performs go mod vendor and go mod tidy. go mod vendor resets the main module\u0026rsquo;s vendor directory to include all packages needed to build and test all the main module\u0026rsquo;s packages. It does not include test code for vendored packages. go mod tidy makes sure go.mod matches the source code in the module. It adds any missing modules necessary to build the current module\u0026rsquo;s packages and dependencies, and it removes unused modules that don\u0026rsquo;t provide any relevant packages.\nmake revendor The dependencies are installed into the vendor folder which should be added to the VCS.\n:warning: Make sure that you test the code after you have updated the dependencies!\n"},{"uri":"https://gardener.cloud/v1.12.8/contribute/10_code/20_dependencies/","title":"Dependencies","tags":[],"description":"","content":"Testing We follow the BDD-style testing principles and are leveraging the Ginkgo framework along with Gomega as matcher library. In order to execute the existing tests, you can use\nmake test # runs tests make verify # runs static code checks and test There is an additional command for analyzing the code coverage of the tests. Ginkgo will generate standard Golang cover profiles which will be translated into a HTML file by the Go Cover Tool. Another command helps you to clean up the filesystem from the temporary cover profile files and the HTML report:\nmake test-cov open gardener.coverage.html make test-cov-clean sigs.k8s.io/controller-runtime env test Some of the integration tests in Gardener are using the sigs.k8s.io/controller-runtime/pkg/envtest package. It sets up a temporary control plane (etcd + kube-apiserver) against the integration tests can run. The test and test-cov rules in the Makefile prepare this env test automatically by downloading the respective binaries (if not yet present) and set the necessary environment variables.\nYou can also run go test or ginkgo without the test/test-cov rules. In this case you have to set the KUBEBUILDER_ASSETS environment variable to the path that contains the etcd + kube-apiserver binaries or you need to have the binaries pre-installed under /usr/local/kubebuilder/bin.\nDependency Management We are using go modules for depedency management. In order to add a new package dependency to the project, you can perform go get \u0026lt;PACKAGE\u0026gt;@\u0026lt;VERSION\u0026gt; or edit the go.mod file and append the package along with the version you want to use.\nUpdating Dependencies The Makefile contains a rule called revendor which performs go mod vendor and go mod tidy. go mod vendor resets the main module\u0026rsquo;s vendor directory to include all packages needed to build and test all the main module\u0026rsquo;s packages. It does not include test code for vendored packages. go mod tidy makes sure go.mod matches the source code in the module. It adds any missing modules necessary to build the current module\u0026rsquo;s packages and dependencies, and it removes unused modules that don\u0026rsquo;t provide any relevant packages.\nmake revendor The dependencies are installed into the vendor folder which should be added to the VCS.\n:warning: Make sure that you test the code after you have updated the dependencies!\n"},{"uri":"https://gardener.cloud/v1.13.2/contribute/10_code/20_dependencies/","title":"Dependencies","tags":[],"description":"","content":"Testing We follow the BDD-style testing principles and are leveraging the Ginkgo framework along with Gomega as matcher library. In order to execute the existing tests, you can use\nmake test # runs tests make verify # runs static code checks and test There is an additional command for analyzing the code coverage of the tests. Ginkgo will generate standard Golang cover profiles which will be translated into a HTML file by the Go Cover Tool. Another command helps you to clean up the filesystem from the temporary cover profile files and the HTML report:\nmake test-cov open gardener.coverage.html make test-cov-clean sigs.k8s.io/controller-runtime env test Some of the integration tests in Gardener are using the sigs.k8s.io/controller-runtime/pkg/envtest package. It sets up a temporary control plane (etcd + kube-apiserver) against the integration tests can run. The test and test-cov rules in the Makefile prepare this env test automatically by downloading the respective binaries (if not yet present) and set the necessary environment variables.\nYou can also run go test or ginkgo without the test/test-cov rules. In this case you have to set the KUBEBUILDER_ASSETS environment variable to the path that contains the etcd + kube-apiserver binaries or you need to have the binaries pre-installed under /usr/local/kubebuilder/bin.\nDependency Management We are using go modules for depedency management. In order to add a new package dependency to the project, you can perform go get \u0026lt;PACKAGE\u0026gt;@\u0026lt;VERSION\u0026gt; or edit the go.mod file and append the package along with the version you want to use.\nUpdating Dependencies The Makefile contains a rule called revendor which performs go mod vendor and go mod tidy. go mod vendor resets the main module\u0026rsquo;s vendor directory to include all packages needed to build and test all the main module\u0026rsquo;s packages. It does not include test code for vendored packages. go mod tidy makes sure go.mod matches the source code in the module. It adds any missing modules necessary to build the current module\u0026rsquo;s packages and dependencies, and it removes unused modules that don\u0026rsquo;t provide any relevant packages.\nmake revendor The dependencies are installed into the vendor folder which should be added to the VCS.\n:warning: Make sure that you test the code after you have updated the dependencies!\n"},{"uri":"https://gardener.cloud/documentation/concepts/deployment/deploy_gardenlet_manually/","title":"Deploy a Gardenlet Manually","tags":[],"description":"","content":"Deploy a Gardenlet Manually Manually deploying a gardenlet is required in the following cases:\n  The Kubernetes cluster to be registered as a seed cluster has no public endpoint, because it is behind a firewall. The gardenlet must then be deployed into the cluster itself.\n  The Kubernetes cluster to be registered as a seed cluster is managed externally (the Kubernetes cluster is not a shoot cluster, so Automatic Deployment of Gardenlets cannot be used).\n  The gardenlet runs outside of the Kubernetes cluster that should be registered as a seed cluster. (The gardenlet is not restricted to run in the seed cluster or to be deployed into a Kubernetes cluster at all).\n   Once youve deployed a gardenlet manually, for example, behind a firewall, you can deploy new gardenlets automatically. The manually deployed gardenlet is then used as a template for the new gardenlets. More information: Automatic Deployment of Gardenlets.\n Prerequisites Kubernetes cluster that should be registered as a seed cluster   Verify that the cluster has a supported Kubernetes version.\n  Determine the nodes, pods, and services CIDR of the cluster. You need to configure this information in the Seed configuration. Gardener uses this information to check that the shoot cluster isnt created with overlapping CIDR ranges.\n  Every Seed cluster needs an Ingress controller which distributes external requests to internal components like grafana and prometheus. Gardener supports two approaches to achieve this:\n  a. Gardener managed Ingress controller and DNS records. For this configure the following lines in your Seed resource:\nspec:dns:provider:type:aws-route53secretRef:name:ingress-secretnamespace:gardeningress:domain:ingress.my-seed.example.comcontroller:kind:nginxproviderConfig:\u0026lt;some-optional-provider-specific-config-for-the-ingressController\u0026gt; Please note that if you set .spec.ingress then .spec.dns.ingressDomain must be nil.\nb. Self-managed DNS record and Ingress controller:\n:warning:\nThere should exist a DNS record *.ingress.\u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; where \u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; is the value of the .dns.ingressDomain field of a Seed cluster resource (or the respective Gardenlet configuration).\nThis is how it could be done for the Nginx ingress controller\nDeploy nginx into the kube-system namespace in the Kubernetes cluster that should be registered as a Seed.\nNginx will on most cloud providers create the service with type LoadBalancer with an external ip.\nNAME TYPE CLUSTER-IP EXTERNAL-IP nginx-ingress-controller LoadBalancer 10.0.15.46 34.200.30.30 Create a wildcard A record (e.g *.ingress.sweet-seed.. IN A 34.200.30.30) with your DNS provider and point it to the external ip of the ingress service. This ingress domain is later required to register the Seed cluster.\nPlease configure the ingress domain in the Seed specification as follows:\nspec:dns:ingressDomain:ingress.sweet-seed.\u0026lt;my-domain\u0026gt; Please note that if you set .spec.dns.ingressDomain then .spec.ingress must be nil.\nkubeconfig for the Seed Cluster The kubeconfig is required to deploy the gardenlet Helm chart to the seed cluster. This deployment requires admin privileges. The Helm chart contains a service account gardenlet that the gardenlet deployment uses by default to talk to the Seed API server.\n If the gardenlet isnt deployed in the seed cluster, the gardenlet can be configured to use a kubeconfig, which also requires full admin rights, from a mounted directory. The kubeconfig is specified in section seedClientConnection.kubeconfig of the Gardenlet configuration. This configuration option isnt used in the following,\nas this procedure only describes the recommended setup option where the gardenlet is running in the seed cluster itself.\n Procedure Overview   Prepare the garden cluster:\n Create a bootstrap token secret in the kube-system namespace of the garden cluster Create RBAC roles for the gardenlet to allow bootstrapping in the garden cluster    Prepare the gardenlet Helm chart.\n  Automatically register shoot cluster as a seed cluster.\n  Deploy the gardenlet\n  Check that the gardenlet is successfully deployed\n  Create a bootstrap token secret in the kube-system namespace of the garden cluster The gardenlet needs to talk to the Gardener API server residing in the garden cluster.\nThe gardenlet can be configured with an already existing garden cluster kubeconfig in one of the following ways:\n  Either by specifying gardenClientConnection.kubeconfig in the Gardenlet configuration or\n  by supplying the environment variable GARDEN_KUBECONFIG pointing to a mounted kubeconfig file).\n  The preferred way however, is to use the gardenlets ability to request a signed certificate for the garden cluster by leveraging Kubernetes Certificate Signing Requests. The gardenlet performs a TLS bootstrapping process that is similar to the Kubelet TLS Bootstrapping. Make sure that the API server of the garden cluster has bootstrap token authentication enabled.\nThe client credentials required for the gardenlets TLS bootstrapping process, need to be either token or certificate (OIDC isnt supported) and have permissions to create a Certificate Signing Request (CSR). Its recommended to use bootstrap tokens due to their desirable security properties (such as a limited token lifetime).\nTherefore, first create a bootstrap token secret for the garden cluster:\napiVersion:v1kind:Secretmetadata:# Name MUST be of form \u0026#34;bootstrap-token-\u0026lt;token id\u0026gt;\u0026#34;name:bootstrap-token-07401bnamespace:kube-system# Type MUST be \u0026#39;bootstrap.kubernetes.io/token\u0026#39;type:bootstrap.kubernetes.io/tokenstringData:# Human readable description. Optional.description:\u0026#34;Token to be used by the gardenlet for Seed `sweet-seed`.\u0026#34;# Token ID and secret. Required.token-id:07401b# 6 characterstoken-secret:f395accd246ae52d# 16 characters# Expiration. Optional.# expiration: 2017-03-10T03:22:11Z# Allowed usages.usage-bootstrap-authentication:\u0026#34;true\u0026#34;usage-bootstrap-signing:\u0026#34;true\u0026#34;When you later prepare the gardenlet Helm chart, a kubeconfig based on this token is shared with the gardenlet upon deployment.\nCreate RBAC roles for the gardenlet to allow bootstrapping in the garden cluster This step is only required if the gardenlet you deploy is the first gardenlet in the Gardener installation. Additionally, when using the control plane chart, the following resources are already contained in the Helm chart, that is, if you use it you can skip these steps as the needed RBAC roles already exist.\nThe gardenlet uses the configured bootstrap kubeconfig in gardenClientConnection.bootstrapKubeconfig to request a signed certificate for the user gardener.cloud:system:seed:\u0026lt;seed-name\u0026gt; in the group gardener.cloud:system:seeds.\nCreate a ClusterRole and ClusterRoleBinding that grant full admin permissions to authenticated gardenlets.\nCreate the following resources in the garden cluster:\n---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:gardener.cloud:system:seedsrules:- apiGroups:- \u0026#39;*\u0026#39;resources:- \u0026#39;*\u0026#39;verbs:- \u0026#39;*\u0026#39;---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:gardener.cloud:system:seedsroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:gardener.cloud:system:seedssubjects:- kind:Groupname:gardener.cloud:system:seedsapiGroup:rbac.authorization.k8s.ioPrepare the gardenlet Helm chart This section only describes the minimal configuration, using the global configuration values of the gardenlet Helm chart. For an overview over all values, see the configuration values. We refer to the global configuration values as gardenlet configuration in the remaining procedure.\n  Create a gardenlet configuration gardenlet-values.yaml based on this template.\n  Create a bootstrap kubeconfig based on the bootstrap token created in the garden cluster.\nReplace the \u0026lt;bootstrap-token\u0026gt; with token-id.token-secret (from our previous example: 07401b.f395accd246ae52d) from the bootstrap token secret.\napiVersion:v1kind:Configcurrent-context:gardenlet-bootstrap@defaultclusters:- cluster:certificate-authority-data:\u0026lt;ca-of-garden-cluster\u0026gt; server: https://\u0026lt;endpoint-of-garden-cluster\u0026gt;name:defaultcontexts:- context:cluster:defaultuser:gardenlet-bootstrapname:gardenlet-bootstrap@defaultusers:- name:gardenlet-bootstrapuser:token:\u0026lt;bootstrap-token\u0026gt;  In section gardenClientConnection.bootstrapKubeconfig of your gardenlet configuration, provide the bootstrap kubeconfig together with a name and namespace to the gardenlet Helm chart.\ngardenClientConnection:bootstrapKubeconfig:name:gardenlet-kubeconfig-bootstrapnamespace:gardenkubeconfig:| \u0026lt;bootstrap-kubeconfig\u0026gt; # will be base64 encoded by helmThe bootstrap kubeconfig is stored in the specified secret.\n  In section gardenClientConnection.kubeconfigSecret of your gardenlet configuration, define a name and a namespace where the gardenlet stores the real kubeconfig that it creates during the bootstrap process. If the secret doesn\u0026rsquo;t exist, the gardenlet creates it for you.\ngardenClientConnection:kubeconfigSecret:name:gardenlet-kubeconfignamespace:garden  Automatically register shoot cluster as a seed cluster A seed cluster can either be registered by manually creating the Seed resource or automatically by the gardenlet.\nThis functionality is useful for shooted seed clusters, as the gardenlet in the garden cluster deploys a copy of itself into the cluster with automatic registration of the Seed configured.\nHowever, it can also be used to have a streamlined seed cluster registration process when manually deploying the gardenlet.\n This procedure doesnt describe all the possible configurations for the Seed resource. For more information, see:\n Example Seed resource Configurable Seed settings.   Adjust the gardenlet component configuration   Supply the Seed resource in section seedConfig of your gardenlet configuration gardenlet-values.yaml.\n Note that with the seedConfig supplied, the gardenlet is only responsible to create and reconcile this one configured seed (in the previous example: sweet-seed). The gardenlet can also be configured to reconcile (but not create!) multiple Seeds based on a label selector, which is only recommended for a development setup.\n   Add the seedConfig to your gardenlet configuration gardenlet-values.yaml. The field seedConfig.spec.provider.type specifies the infrastructure provider type (for example, aws) of the seed cluster. For all supported infrastructure providers, see Known Extension Implementations.\n....seedConfig:metadata:name:sweet-seedspec:dns:ingressDomain:ingress.sweet-seed.\u0026lt;my-domain\u0026gt;# see prerequisitesnetworks:# see prerequisitesnodes:10.240.0.0/16pods:100.244.0.0/16services:100.32.0.0/13shootDefaults: # optional:non-overlappingdefaultCIDRsforshootclustersofthatSeedpods:100.96.0.0/11services:100.64.0.0/13provider:region:eu-west-1type:\u0026lt;provider\u0026gt;  Optional: Enable backup and restore The seed cluster can be set up with backup and restore for the main etcds of shoot clusters.\nGardener uses etcd-backup-restore that integrates with different storage providers to store the shoot cluster\u0026rsquo;s main etcd backups. Make sure to obtain client credentials that have sufficient permissions with the chosen storage provider.\nCreate a secret in the garden cluster with client credentials for the storage provider. The format of the secret is cloud provider specific and can be found in the repository of the respective Gardener extension. For example, the secret for AWS S3 can be found in the AWS provider extension (30-etcd-backup-secret.yaml).\napiVersion:v1kind:Secretmetadata:name:sweet-seed-backupnamespace:gardentype:Opaquedata:# client credentials format is provider specificConfigure the Seed resource in section seedConfig of your gardenlet configuration to use backup and restore:\n...seedConfig:metadata:name:sweet-seedspec:backup:provider:\u0026lt;provider\u0026gt; secretRef:name:sweet-seed-backupnamespace:gardenDeploy the gardenlet  The gardenlet doesnt have to run in the same Kubernetes cluster as the seed cluster its registering and reconciling, but it is in most cases advantageous to use in-cluster communication to talk to the Seed API server. Running a gardenlet outside of the cluster is mostly used for local development.\n The gardenlet-values.yaml looks something like this (with automatic Seed registration and backup for shoot clusters enabled):\nglobal:# Gardenlet configuration valuesgardenlet:enabled:true...\u0026lt;defaultconfig\u0026gt; ...config:gardenClientConnection:...bootstrapKubeconfig:name:gardenlet-bootstrap-kubeconfignamespace:gardenkubeconfig:| apiVersion: v1clusters:- cluster:certificate-authority-data:\u0026lt;dummy\u0026gt; server: \u0026lt;my-garden-cluster-endpoint\u0026gt;name:my-kubernetes-cluster....kubeconfigSecret:name:gardenlet-kubeconfignamespace:garden...\u0026lt;defaultconfig\u0026gt; ...seedConfig:metadata:name:sweet-seedspec:dns:ingressDomain:ingress.sweet-seed.\u0026lt;my-domain\u0026gt; networks:nodes:10.240.0.0/16pods:100.244.0.0/16services:100.32.0.0/13shootDefaults:pods:100.96.0.0/11services:100.64.0.0/13provider:region:eu-west-1type:\u0026lt;provider\u0026gt; backup:provider:\u0026lt;provider\u0026gt; secretRef:name:sweet-seed-backupnamespace:gardenDeploy the gardenlet Helm chart to the Kubernetes cluster.\nhelm install gardenlet charts/gardener/gardenlet \\  --namespace garden \\  -f gardenlet-values.yaml \\  --wait This helm chart creates:\n A service account gardenlet that the gardenlet can use to talk to the Seed API server. RBAC roles for the service account (full admin rights at the moment). The secret (garden/gardenlet-bootstrap-kubeconfig) containing the bootstrap kubeconfig. The gardenlet deployment in the garden namespace.  Check that the gardenlet is successfully deployed   Check that the gardenlets certificate bootstrap was successful.\nCheck if the secret gardenlet-kubeconfig in the namespace garden in the seed cluster is created and contains a kubeconfig with a valid certificate.\n  Get the kubeconfig from the created secret.\n$ kubectl -n garden get secret gardenlet-kubeconfig -o json | jq -r .data.kubeconfig | base64 -d   Test against the garden cluster and verify its working.\n  Extract the client-certificate-data from the user gardenlet.\n  View the certificate:\n$ openssl x509 -in ./gardenlet-cert -noout -text Check that the certificate is valid for a year (that is the lifetime of new certificates).\n    Check that the bootstrap secret gardenlet-bootstrap-kubeconfig has been deleted from the seed cluster in namespace garden.\n  Check that the seed cluster is registered and READY in the garden cluster.\nCheck that the seed cluster sweet-seed exists and all conditions indicate that its available. If so, the Gardenlet is sending regular heartbeats and the seed bootstrapping was successful.\nCheck that the conditions on the Seed resource look similar to the following:\n$ kubectl get seed sweet-seed -o json | jq .status.conditions [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2020-07-17T09:17:29Z\u0026#34;, \u0026#34;lastUpdateTime\u0026#34;: \u0026#34;2020-07-17T09:17:29Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Gardenlet is posting ready status.\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;GardenletReady\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GardenletReady\u0026#34; }, { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2020-07-17T09:17:49Z\u0026#34;, \u0026#34;lastUpdateTime\u0026#34;: \u0026#34;2020-07-17T09:53:17Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Seed cluster has been bootstrapped successfully.\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;BootstrappingSucceeded\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Bootstrapped\u0026#34; } ]   Related Links Issue #1724: Harden Gardenlet RBAC privileges.\nBackup and Restore.\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/deployment/deploy_gardenlet_manually/","title":"Deploy a Gardenlet Manually","tags":[],"description":"","content":"Deploy a Gardenlet Manually Manually deploying a gardenlet is required in the following cases:\n  The Kubernetes cluster to be registered as a seed cluster has no public endpoint, because it is behind a firewall. The gardenlet must then be deployed into the cluster itself.\n  The Kubernetes cluster to be registered as a seed cluster is managed externally (the Kubernetes cluster is not a shoot cluster, so Automatic Deployment of Gardenlets cannot be used).\n  The gardenlet runs outside of the Kubernetes cluster that should be registered as a seed cluster. (The gardenlet is not restricted to run in the seed cluster or to be deployed into a Kubernetes cluster at all).\n   Once youve deployed a gardenlet manually, for example, behind a firewall, you can deploy new gardenlets automatically. The manually deployed gardenlet is then used as a template for the new gardenlets. More information: Automatic Deployment of Gardenlets.\n Prerequisites Kubernetes cluster that should be registered as a seed cluster   Verify that the cluster has a supported Kubernetes version.\n  Determine the nodes, pods, and services CIDR of the cluster. You need to configure this information in the Seed configuration. Gardener uses this information to check that the shoot cluster isnt created with overlapping CIDR ranges.\n  Every Seed cluster needs an Ingress controller which distributes external requests to internal components like grafana and prometheus. Gardener supports two approaches to achieve this:\n  a. Gardener managed Ingress controller and DNS records. For this configure the following lines in your Seed resource:\nspec:dns:provider:type:aws-route53secretRef:name:ingress-secretnamespace:gardeningress:domain:ingress.my-seed.example.comcontroller:kind:nginxproviderConfig:\u0026lt;some-optional-provider-specific-config-for-the-ingressController\u0026gt; Please note that if you set .spec.ingress then .spec.dns.ingressDomain must be nil.\nb. Self-managed DNS record and Ingress controller:\n:warning:\nThere should exist a DNS record *.ingress.\u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; where \u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; is the value of the .dns.ingressDomain field of a Seed cluster resource (or the respective Gardenlet configuration).\nThis is how it could be done for the Nginx ingress controller\nDeploy nginx into the kube-system namespace in the Kubernetes cluster that should be registered as a Seed.\nNginx will on most cloud providers create the service with type LoadBalancer with an external ip.\nNAME TYPE CLUSTER-IP EXTERNAL-IP nginx-ingress-controller LoadBalancer 10.0.15.46 34.200.30.30 Create a wildcard A record (e.g *.ingress.sweet-seed.. IN A 34.200.30.30) with your DNS provider and point it to the external ip of the ingress service. This ingress domain is later required to register the Seed cluster.\nPlease configure the ingress domain in the Seed specification as follows:\nspec:dns:ingressDomain:ingress.sweet-seed.\u0026lt;my-domain\u0026gt; Please note that if you set .spec.dns.ingressDomain then .spec.ingress must be nil.\nkubeconfig for the Seed Cluster The kubeconfig is required to deploy the gardenlet Helm chart to the seed cluster. This deployment requires admin privileges. The Helm chart contains a service account gardenlet that the gardenlet deployment uses by default to talk to the Seed API server.\n If the gardenlet isnt deployed in the seed cluster, the gardenlet can be configured to use a kubeconfig, which also requires full admin rights, from a mounted directory. The kubeconfig is specified in section seedClientConnection.kubeconfig of the Gardenlet configuration. This configuration option isnt used in the following,\nas this procedure only describes the recommended setup option where the gardenlet is running in the seed cluster itself.\n Procedure Overview   Prepare the garden cluster:\n Create a bootstrap token secret in the kube-system namespace of the garden cluster Create RBAC roles for the gardenlet to allow bootstrapping in the garden cluster    Prepare the gardenlet Helm chart.\n  Automatically register shoot cluster as a seed cluster.\n  Deploy the gardenlet\n  Check that the gardenlet is successfully deployed\n  Create a bootstrap token secret in the kube-system namespace of the garden cluster The gardenlet needs to talk to the Gardener API server residing in the garden cluster.\nThe gardenlet can be configured with an already existing garden cluster kubeconfig in one of the following ways:\n  Either by specifying gardenClientConnection.kubeconfig in the Gardenlet configuration or\n  by supplying the environment variable GARDEN_KUBECONFIG pointing to a mounted kubeconfig file).\n  The preferred way however, is to use the gardenlets ability to request a signed certificate for the garden cluster by leveraging Kubernetes Certificate Signing Requests. The gardenlet performs a TLS bootstrapping process that is similar to the Kubelet TLS Bootstrapping. Make sure that the API server of the garden cluster has bootstrap token authentication enabled.\nThe client credentials required for the gardenlets TLS bootstrapping process, need to be either token or certificate (OIDC isnt supported) and have permissions to create a Certificate Signing Request (CSR). Its recommended to use bootstrap tokens due to their desirable security properties (such as a limited token lifetime).\nTherefore, first create a bootstrap token secret for the garden cluster:\napiVersion:v1kind:Secretmetadata:# Name MUST be of form \u0026#34;bootstrap-token-\u0026lt;token id\u0026gt;\u0026#34;name:bootstrap-token-07401bnamespace:kube-system# Type MUST be \u0026#39;bootstrap.kubernetes.io/token\u0026#39;type:bootstrap.kubernetes.io/tokenstringData:# Human readable description. Optional.description:\u0026#34;Token to be used by the gardenlet for Seed `sweet-seed`.\u0026#34;# Token ID and secret. Required.token-id:07401b# 6 characterstoken-secret:f395accd246ae52d# 16 characters# Expiration. Optional.# expiration: 2017-03-10T03:22:11Z# Allowed usages.usage-bootstrap-authentication:\u0026#34;true\u0026#34;usage-bootstrap-signing:\u0026#34;true\u0026#34;When you later prepare the gardenlet Helm chart, a kubeconfig based on this token is shared with the gardenlet upon deployment.\nCreate RBAC roles for the gardenlet to allow bootstrapping in the garden cluster This step is only required if the gardenlet you deploy is the first gardenlet in the Gardener installation. Additionally, when using the control plane chart, the following resources are already contained in the Helm chart, that is, if you use it you can skip these steps as the needed RBAC roles already exist.\nThe gardenlet uses the configured bootstrap kubeconfig in gardenClientConnection.bootstrapKubeconfig to request a signed certificate for the user gardener.cloud:system:seed:\u0026lt;seed-name\u0026gt; in the group gardener.cloud:system:seeds.\nCreate a ClusterRole and ClusterRoleBinding that grant full admin permissions to authenticated gardenlets.\nCreate the following resources in the garden cluster:\n---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:gardener.cloud:system:seedsrules:- apiGroups:- \u0026#39;*\u0026#39;resources:- \u0026#39;*\u0026#39;verbs:- \u0026#39;*\u0026#39;---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:gardener.cloud:system:seedsroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:gardener.cloud:system:seedssubjects:- kind:Groupname:gardener.cloud:system:seedsapiGroup:rbac.authorization.k8s.ioPrepare the gardenlet Helm chart This section only describes the minimal configuration, using the global configuration values of the gardenlet Helm chart. For an overview over all values, see the configuration values. We refer to the global configuration values as gardenlet configuration in the remaining procedure.\n  Create a gardenlet configuration gardenlet-values.yaml based on this template.\n  Create a bootstrap kubeconfig based on the bootstrap token created in the garden cluster.\nReplace the \u0026lt;bootstrap-token\u0026gt; with token-id.token-secret (from our previous example: 07401b.f395accd246ae52d) from the bootstrap token secret.\napiVersion:v1kind:Configcurrent-context:gardenlet-bootstrap@defaultclusters:- cluster:certificate-authority-data:\u0026lt;ca-of-garden-cluster\u0026gt; server: https://\u0026lt;endpoint-of-garden-cluster\u0026gt;name:defaultcontexts:- context:cluster:defaultuser:gardenlet-bootstrapname:gardenlet-bootstrap@defaultusers:- name:gardenlet-bootstrapuser:token:\u0026lt;bootstrap-token\u0026gt;  In section gardenClientConnection.bootstrapKubeconfig of your gardenlet configuration, provide the bootstrap kubeconfig together with a name and namespace to the gardenlet Helm chart.\ngardenClientConnection:bootstrapKubeconfig:name:gardenlet-kubeconfig-bootstrapnamespace:gardenkubeconfig:| \u0026lt;bootstrap-kubeconfig\u0026gt; # will be base64 encoded by helmThe bootstrap kubeconfig is stored in the specified secret.\n  In section gardenClientConnection.kubeconfigSecret of your gardenlet configuration, define a name and a namespace where the gardenlet stores the real kubeconfig that it creates during the bootstrap process. If the secret doesn\u0026rsquo;t exist, the gardenlet creates it for you.\ngardenClientConnection:kubeconfigSecret:name:gardenlet-kubeconfignamespace:garden  Automatically register shoot cluster as a seed cluster A seed cluster can either be registered by manually creating the Seed resource or automatically by the gardenlet.\nThis functionality is useful for shooted seed clusters, as the gardenlet in the garden cluster deploys a copy of itself into the cluster with automatic registration of the Seed configured.\nHowever, it can also be used to have a streamlined seed cluster registration process when manually deploying the gardenlet.\n This procedure doesnt describe all the possible configurations for the Seed resource. For more information, see:\n Example Seed resource Configurable Seed settings.   Adjust the gardenlet component configuration   Supply the Seed resource in section seedConfig of your gardenlet configuration gardenlet-values.yaml.\n Note that with the seedConfig supplied, the gardenlet is only responsible to create and reconcile this one configured seed (in the previous example: sweet-seed). The gardenlet can also be configured to reconcile (but not create!) multiple Seeds based on a label selector, which is only recommended for a development setup.\n   Add the seedConfig to your gardenlet configuration gardenlet-values.yaml. The field seedConfig.spec.provider.type specifies the infrastructure provider type (for example, aws) of the seed cluster. For all supported infrastructure providers, see Known Extension Implementations.\n....seedConfig:metadata:name:sweet-seedspec:dns:ingressDomain:ingress.sweet-seed.\u0026lt;my-domain\u0026gt;# see prerequisitesnetworks:# see prerequisitesnodes:10.240.0.0/16pods:100.244.0.0/16services:100.32.0.0/13shootDefaults: # optional:non-overlappingdefaultCIDRsforshootclustersofthatSeedpods:100.96.0.0/11services:100.64.0.0/13provider:region:eu-west-1type:\u0026lt;provider\u0026gt;  Optional: Enable backup and restore The seed cluster can be set up with backup and restore for the main etcds of shoot clusters.\nGardener uses etcd-backup-restore that integrates with different storage providers to store the shoot cluster\u0026rsquo;s main etcd backups. Make sure to obtain client credentials that have sufficient permissions with the chosen storage provider.\nCreate a secret in the garden cluster with client credentials for the storage provider. The format of the secret is cloud provider specific and can be found in the repository of the respective Gardener extension. For example, the secret for AWS S3 can be found in the AWS provider extension (30-etcd-backup-secret.yaml).\napiVersion:v1kind:Secretmetadata:name:sweet-seed-backupnamespace:gardentype:Opaquedata:# client credentials format is provider specificConfigure the Seed resource in section seedConfig of your gardenlet configuration to use backup and restore:\n...seedConfig:metadata:name:sweet-seedspec:backup:provider:\u0026lt;provider\u0026gt; secretRef:name:sweet-seed-backupnamespace:gardenDeploy the gardenlet  The gardenlet doesnt have to run in the same Kubernetes cluster as the seed cluster its registering and reconciling, but it is in most cases advantageous to use in-cluster communication to talk to the Seed API server. Running a gardenlet outside of the cluster is mostly used for local development.\n The gardenlet-values.yaml looks something like this (with automatic Seed registration and backup for shoot clusters enabled):\nglobal:# Gardenlet configuration valuesgardenlet:enabled:true...\u0026lt;defaultconfig\u0026gt; ...config:gardenClientConnection:...bootstrapKubeconfig:name:gardenlet-bootstrap-kubeconfignamespace:gardenkubeconfig:| apiVersion: v1clusters:- cluster:certificate-authority-data:\u0026lt;dummy\u0026gt; server: \u0026lt;my-garden-cluster-endpoint\u0026gt;name:my-kubernetes-cluster....kubeconfigSecret:name:gardenlet-kubeconfignamespace:garden...\u0026lt;defaultconfig\u0026gt; ...seedConfig:metadata:name:sweet-seedspec:dns:ingressDomain:ingress.sweet-seed.\u0026lt;my-domain\u0026gt; networks:nodes:10.240.0.0/16pods:100.244.0.0/16services:100.32.0.0/13shootDefaults:pods:100.96.0.0/11services:100.64.0.0/13provider:region:eu-west-1type:\u0026lt;provider\u0026gt; backup:provider:\u0026lt;provider\u0026gt; secretRef:name:sweet-seed-backupnamespace:gardenDeploy the gardenlet Helm chart to the Kubernetes cluster.\nhelm install gardenlet charts/gardener/gardenlet \\  --namespace garden \\  -f gardenlet-values.yaml \\  --wait This helm chart creates:\n A service account gardenlet that the gardenlet can use to talk to the Seed API server. RBAC roles for the service account (full admin rights at the moment). The secret (garden/gardenlet-bootstrap-kubeconfig) containing the bootstrap kubeconfig. The gardenlet deployment in the garden namespace.  Check that the gardenlet is successfully deployed   Check that the gardenlets certificate bootstrap was successful.\nCheck if the secret gardenlet-kubeconfig in the namespace garden in the seed cluster is created and contains a kubeconfig with a valid certificate.\n  Get the kubeconfig from the created secret.\n$ kubectl -n garden get secret gardenlet-kubeconfig -o json | jq -r .data.kubeconfig | base64 -d   Test against the garden cluster and verify its working.\n  Extract the client-certificate-data from the user gardenlet.\n  View the certificate:\n$ openssl x509 -in ./gardenlet-cert -noout -text Check that the certificate is valid for a year (that is the lifetime of new certificates).\n    Check that the bootstrap secret gardenlet-bootstrap-kubeconfig has been deleted from the seed cluster in namespace garden.\n  Check that the seed cluster is registered and READY in the garden cluster.\nCheck that the seed cluster sweet-seed exists and all conditions indicate that its available. If so, the Gardenlet is sending regular heartbeats and the seed bootstrapping was successful.\nCheck that the conditions on the Seed resource look similar to the following:\n$ kubectl get seed sweet-seed -o json | jq .status.conditions [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2020-07-17T09:17:29Z\u0026#34;, \u0026#34;lastUpdateTime\u0026#34;: \u0026#34;2020-07-17T09:17:29Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Gardenlet is posting ready status.\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;GardenletReady\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GardenletReady\u0026#34; }, { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2020-07-17T09:17:49Z\u0026#34;, \u0026#34;lastUpdateTime\u0026#34;: \u0026#34;2020-07-17T09:53:17Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Seed cluster has been bootstrapped successfully.\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;BootstrappingSucceeded\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Bootstrapped\u0026#34; } ]   Related Links Issue #1724: Harden Gardenlet RBAC privileges.\nBackup and Restore.\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/deployment/deploy_gardenlet_manually/","title":"Deploy a Gardenlet Manually","tags":[],"description":"","content":"Deploy a Gardenlet Manually Manually deploying a gardenlet is required in the following cases:\n  The Kubernetes cluster to be registered as a seed cluster has no public endpoint, because it is behind a firewall. The gardenlet must then be deployed into the cluster itself.\n  The Kubernetes cluster to be registered as a seed cluster is managed externally (the Kubernetes cluster is not a shoot cluster, so Automatic Deployment of Gardenlets cannot be used).\n  The gardenlet runs outside of the Kubernetes cluster that should be registered as a seed cluster. (The gardenlet is not restricted to run in the seed cluster or to be deployed into a Kubernetes cluster at all).\n   Once youve deployed a gardenlet manually, for example, behind a firewall, you can deploy new gardenlets automatically. The manually deployed gardenlet is then used as a template for the new gardenlets. More information: Automatic Deployment of Gardenlets.\n Prerequisites Kubernetes cluster that should be registered as a seed cluster   Verify that the cluster has a supported Kubernetes version.\n  Determine the nodes, pods, and services CIDR of the cluster. You need to configure this information in the Seed configuration. Gardener uses this information to check that the shoot cluster isnt created with overlapping CIDR ranges.\n  Every Seed cluster needs an Ingress controller which distributes external requests to internal components like grafana and prometheus. Gardener supports two approaches to achieve this:\n  a. Gardener managed Ingress controller and DNS records. For this configure the following lines in your Seed resource:\nspec:dns:provider:type:aws-route53secretRef:name:ingress-secretnamespace:gardeningress:domain:ingress.my-seed.example.comcontroller:kind:nginxproviderConfig:\u0026lt;some-optional-provider-specific-config-for-the-ingressController\u0026gt; Please note that if you set .spec.ingress then .spec.dns.ingressDomain must be nil.\nb. Self-managed DNS record and Ingress controller:\n:warning:\nThere should exist a DNS record *.ingress.\u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; where \u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; is the value of the .dns.ingressDomain field of a Seed cluster resource (or the respective Gardenlet configuration).\nThis is how it could be done for the Nginx ingress controller\nDeploy nginx into the kube-system namespace in the Kubernetes cluster that should be registered as a Seed.\nNginx will on most cloud providers create the service with type LoadBalancer with an external ip.\nNAME TYPE CLUSTER-IP EXTERNAL-IP nginx-ingress-controller LoadBalancer 10.0.15.46 34.200.30.30 Create a wildcard A record (e.g *.ingress.sweet-seed.. IN A 34.200.30.30) with your DNS provider and point it to the external ip of the ingress service. This ingress domain is later required to register the Seed cluster.\nPlease configure the ingress domain in the Seed specification as follows:\nspec:dns:ingressDomain:ingress.sweet-seed.\u0026lt;my-domain\u0026gt; Please note that if you set .spec.dns.ingressDomain then .spec.ingress must be nil.\nkubeconfig for the Seed Cluster The kubeconfig is required to deploy the gardenlet Helm chart to the seed cluster. This deployment requires admin privileges. The Helm chart contains a service account gardenlet that the gardenlet deployment uses by default to talk to the Seed API server.\n If the gardenlet isnt deployed in the seed cluster, the gardenlet can be configured to use a kubeconfig, which also requires full admin rights, from a mounted directory. The kubeconfig is specified in section seedClientConnection.kubeconfig of the Gardenlet configuration. This configuration option isnt used in the following,\nas this procedure only describes the recommended setup option where the gardenlet is running in the seed cluster itself.\n Procedure Overview   Prepare the garden cluster:\n Create a bootstrap token secret in the kube-system namespace of the garden cluster Create RBAC roles for the gardenlet to allow bootstrapping in the garden cluster    Prepare the gardenlet Helm chart.\n  Automatically register shoot cluster as a seed cluster.\n  Deploy the gardenlet\n  Check that the gardenlet is successfully deployed\n  Create a bootstrap token secret in the kube-system namespace of the garden cluster The gardenlet needs to talk to the Gardener API server residing in the garden cluster.\nThe gardenlet can be configured with an already existing garden cluster kubeconfig in one of the following ways:\n  Either by specifying gardenClientConnection.kubeconfig in the Gardenlet configuration or\n  by supplying the environment variable GARDEN_KUBECONFIG pointing to a mounted kubeconfig file).\n  The preferred way however, is to use the gardenlets ability to request a signed certificate for the garden cluster by leveraging Kubernetes Certificate Signing Requests. The gardenlet performs a TLS bootstrapping process that is similar to the Kubelet TLS Bootstrapping. Make sure that the API server of the garden cluster has bootstrap token authentication enabled.\nThe client credentials required for the gardenlets TLS bootstrapping process, need to be either token or certificate (OIDC isnt supported) and have permissions to create a Certificate Signing Request (CSR). Its recommended to use bootstrap tokens due to their desirable security properties (such as a limited token lifetime).\nTherefore, first create a bootstrap token secret for the garden cluster:\napiVersion:v1kind:Secretmetadata:# Name MUST be of form \u0026#34;bootstrap-token-\u0026lt;token id\u0026gt;\u0026#34;name:bootstrap-token-07401bnamespace:kube-system# Type MUST be \u0026#39;bootstrap.kubernetes.io/token\u0026#39;type:bootstrap.kubernetes.io/tokenstringData:# Human readable description. Optional.description:\u0026#34;Token to be used by the gardenlet for Seed `sweet-seed`.\u0026#34;# Token ID and secret. Required.token-id:07401b# 6 characterstoken-secret:f395accd246ae52d# 16 characters# Expiration. Optional.# expiration: 2017-03-10T03:22:11Z# Allowed usages.usage-bootstrap-authentication:\u0026#34;true\u0026#34;usage-bootstrap-signing:\u0026#34;true\u0026#34;When you later prepare the gardenlet Helm chart, a kubeconfig based on this token is shared with the gardenlet upon deployment.\nCreate RBAC roles for the gardenlet to allow bootstrapping in the garden cluster This step is only required if the gardenlet you deploy is the first gardenlet in the Gardener installation. Additionally, when using the control plane chart, the following resources are already contained in the Helm chart, that is, if you use it you can skip these steps as the needed RBAC roles already exist.\nThe gardenlet uses the configured bootstrap kubeconfig in gardenClientConnection.bootstrapKubeconfig to request a signed certificate for the user gardener.cloud:system:seed:\u0026lt;seed-name\u0026gt; in the group gardener.cloud:system:seeds.\nCreate a ClusterRole and ClusterRoleBinding that grant full admin permissions to authenticated gardenlets.\nCreate the following resources in the garden cluster:\n---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:gardener.cloud:system:seedsrules:- apiGroups:- \u0026#39;*\u0026#39;resources:- \u0026#39;*\u0026#39;verbs:- \u0026#39;*\u0026#39;---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:gardener.cloud:system:seedsroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:gardener.cloud:system:seedssubjects:- kind:Groupname:gardener.cloud:system:seedsapiGroup:rbac.authorization.k8s.ioPrepare the gardenlet Helm chart This section only describes the minimal configuration, using the global configuration values of the gardenlet Helm chart. For an overview over all values, see the configuration values. We refer to the global configuration values as gardenlet configuration in the remaining procedure.\n  Create a gardenlet configuration gardenlet-values.yaml based on this template.\n  Create a bootstrap kubeconfig based on the bootstrap token created in the garden cluster.\nReplace the \u0026lt;bootstrap-token\u0026gt; with token-id.token-secret (from our previous example: 07401b.f395accd246ae52d) from the bootstrap token secret.\napiVersion:v1kind:Configcurrent-context:gardenlet-bootstrap@defaultclusters:- cluster:certificate-authority-data:\u0026lt;ca-of-garden-cluster\u0026gt; server: https://\u0026lt;endpoint-of-garden-cluster\u0026gt;name:defaultcontexts:- context:cluster:defaultuser:gardenlet-bootstrapname:gardenlet-bootstrap@defaultusers:- name:gardenlet-bootstrapuser:token:\u0026lt;bootstrap-token\u0026gt;  In section gardenClientConnection.bootstrapKubeconfig of your gardenlet configuration, provide the bootstrap kubeconfig together with a name and namespace to the gardenlet Helm chart.\ngardenClientConnection:bootstrapKubeconfig:name:gardenlet-kubeconfig-bootstrapnamespace:gardenkubeconfig:| \u0026lt;bootstrap-kubeconfig\u0026gt; # will be base64 encoded by helmThe bootstrap kubeconfig is stored in the specified secret.\n  In section gardenClientConnection.kubeconfigSecret of your gardenlet configuration, define a name and a namespace where the gardenlet stores the real kubeconfig that it creates during the bootstrap process. If the secret doesn\u0026rsquo;t exist, the gardenlet creates it for you.\ngardenClientConnection:kubeconfigSecret:name:gardenlet-kubeconfignamespace:garden  Automatically register shoot cluster as a seed cluster A seed cluster can either be registered by manually creating the Seed resource or automatically by the gardenlet.\nThis functionality is useful for shooted seed clusters, as the gardenlet in the garden cluster deploys a copy of itself into the cluster with automatic registration of the Seed configured.\nHowever, it can also be used to have a streamlined seed cluster registration process when manually deploying the gardenlet.\n This procedure doesnt describe all the possible configurations for the Seed resource. For more information, see:\n Example Seed resource Configurable Seed settings.   Adjust the gardenlet component configuration   Supply the Seed resource in section seedConfig of your gardenlet configuration gardenlet-values.yaml.\n Note that with the seedConfig supplied, the gardenlet is only responsible to create and reconcile this one configured seed (in the previous example: sweet-seed). The gardenlet can also be configured to reconcile (but not create!) multiple Seeds based on a label selector, which is only recommended for a development setup.\n   Add the seedConfig to your gardenlet configuration gardenlet-values.yaml. The field seedConfig.spec.provider.type specifies the infrastructure provider type (for example, aws) of the seed cluster. For all supported infrastructure providers, see Known Extension Implementations.\n....seedConfig:metadata:name:sweet-seedspec:dns:ingressDomain:ingress.sweet-seed.\u0026lt;my-domain\u0026gt;# see prerequisitesnetworks:# see prerequisitesnodes:10.240.0.0/16pods:100.244.0.0/16services:100.32.0.0/13shootDefaults: # optional:non-overlappingdefaultCIDRsforshootclustersofthatSeedpods:100.96.0.0/11services:100.64.0.0/13provider:region:eu-west-1type:\u0026lt;provider\u0026gt;  Optional: Enable backup and restore The seed cluster can be set up with backup and restore for the main etcds of shoot clusters.\nGardener uses etcd-backup-restore that integrates with different storage providers to store the shoot cluster\u0026rsquo;s main etcd backups. Make sure to obtain client credentials that have sufficient permissions with the chosen storage provider.\nCreate a secret in the garden cluster with client credentials for the storage provider. The format of the secret is cloud provider specific and can be found in the repository of the respective Gardener extension. For example, the secret for AWS S3 can be found in the AWS provider extension (30-etcd-backup-secret.yaml).\napiVersion:v1kind:Secretmetadata:name:sweet-seed-backupnamespace:gardentype:Opaquedata:# client credentials format is provider specificConfigure the Seed resource in section seedConfig of your gardenlet configuration to use backup and restore:\n...seedConfig:metadata:name:sweet-seedspec:backup:provider:\u0026lt;provider\u0026gt; secretRef:name:sweet-seed-backupnamespace:gardenDeploy the gardenlet  The gardenlet doesnt have to run in the same Kubernetes cluster as the seed cluster its registering and reconciling, but it is in most cases advantageous to use in-cluster communication to talk to the Seed API server. Running a gardenlet outside of the cluster is mostly used for local development.\n The gardenlet-values.yaml looks something like this (with automatic Seed registration and backup for shoot clusters enabled):\nglobal:# Gardenlet configuration valuesgardenlet:enabled:true...\u0026lt;defaultconfig\u0026gt; ...config:gardenClientConnection:...bootstrapKubeconfig:name:gardenlet-bootstrap-kubeconfignamespace:gardenkubeconfig:| apiVersion: v1clusters:- cluster:certificate-authority-data:\u0026lt;dummy\u0026gt; server: \u0026lt;my-garden-cluster-endpoint\u0026gt;name:my-kubernetes-cluster....kubeconfigSecret:name:gardenlet-kubeconfignamespace:garden...\u0026lt;defaultconfig\u0026gt; ...seedConfig:metadata:name:sweet-seedspec:dns:ingressDomain:ingress.sweet-seed.\u0026lt;my-domain\u0026gt; networks:nodes:10.240.0.0/16pods:100.244.0.0/16services:100.32.0.0/13shootDefaults:pods:100.96.0.0/11services:100.64.0.0/13provider:region:eu-west-1type:\u0026lt;provider\u0026gt; backup:provider:\u0026lt;provider\u0026gt; secretRef:name:sweet-seed-backupnamespace:gardenDeploy the gardenlet Helm chart to the Kubernetes cluster.\nhelm install gardenlet charts/gardener/gardenlet \\  --namespace garden \\  -f gardenlet-values.yaml \\  --wait This helm chart creates:\n A service account gardenlet that the gardenlet can use to talk to the Seed API server. RBAC roles for the service account (full admin rights at the moment). The secret (garden/gardenlet-bootstrap-kubeconfig) containing the bootstrap kubeconfig. The gardenlet deployment in the garden namespace.  Check that the gardenlet is successfully deployed   Check that the gardenlets certificate bootstrap was successful.\nCheck if the secret gardenlet-kubeconfig in the namespace garden in the seed cluster is created and contains a kubeconfig with a valid certificate.\n  Get the kubeconfig from the created secret.\n$ kubectl -n garden get secret gardenlet-kubeconfig -o json | jq -r .data.kubeconfig | base64 -d   Test against the garden cluster and verify its working.\n  Extract the client-certificate-data from the user gardenlet.\n  View the certificate:\n$ openssl x509 -in ./gardenlet-cert -noout -text Check that the certificate is valid for a year (that is the lifetime of new certificates).\n    Check that the bootstrap secret gardenlet-bootstrap-kubeconfig has been deleted from the seed cluster in namespace garden.\n  Check that the seed cluster is registered and READY in the garden cluster.\nCheck that the seed cluster sweet-seed exists and all conditions indicate that its available. If so, the Gardenlet is sending regular heartbeats and the seed bootstrapping was successful.\nCheck that the conditions on the Seed resource look similar to the following:\n$ kubectl get seed sweet-seed -o json | jq .status.conditions [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2020-07-17T09:17:29Z\u0026#34;, \u0026#34;lastUpdateTime\u0026#34;: \u0026#34;2020-07-17T09:17:29Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Gardenlet is posting ready status.\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;GardenletReady\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;GardenletReady\u0026#34; }, { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2020-07-17T09:17:49Z\u0026#34;, \u0026#34;lastUpdateTime\u0026#34;: \u0026#34;2020-07-17T09:53:17Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Seed cluster has been bootstrapped successfully.\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;BootstrappingSucceeded\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Bootstrapped\u0026#34; } ]   Related Links Issue #1724: Harden Gardenlet RBAC privileges.\nBackup and Restore.\n"},{"uri":"https://gardener.cloud/documentation/contribute/10_code/27_deploy_into_cluster/","title":"Deploy into a Cluster","tags":[],"description":"","content":"Deploying the Gardener into a Kubernetes cluster Similar to Kubernetes, Gardener consists out of control plane components (Gardener API server, Gardener controller manager, Gardener scheduler), and an agent component (Gardenlet). The control plane is deployed in the so-called garden cluster while the agent is installed into every seed cluster. Please note that it is possible to use the garden cluster as seed cluster by simply deploying the Gardenlet into it.\nWe are providing Helm charts in order to manage the various resources of the components. Please always make sure that you use the Helm chart version that matches the Gardener version you want to deploy.\nDeploying the Gardener control plane (API server, controller manager, scheduler) The configuration values depict the various options to configure the different components. Please consult this document to get a detailed explanation of what can be configured for which component. Also note that all resources and deployments need to be created in the garden namespace (not overrideable).\nAfter preparing your values in a separate controlplane-values.yaml file, you can run the following command against your garden cluster:\nhelm install charts/gardener/controlplane \\  --namespace garden \\  --name gardener-controlplane \\  -f gardener-values.yaml \\  --wait Deploying Gardener extensions Gardener is an extensible system that does not contain the logic for provider-specific things like DNS management, cloud infrastructures, network plugins, operating system configs, and many more.\nYou have to install extension controllers for these parts. Please consult the documentation regarding extensions to get more information.\nDeploying the Gardener agent (Gardenlet) The Gardenlet requires a bootstrap token as well as a bootstrap kubeconfig in order to properly register itself with the Gardener control plane.\nThe configuration values depict the various options to configure it. Please consult this document to get a detailed explanation of what can be configured.\nPrepare your values in a separate gardenlet-values.yaml file:\n Create a bootstrap token secret in the kube-system namespace of the garden cluster (see this and this). Create a bootstrap kubeconfig containing this token:  apiVersion:v1kind:Configcurrent-context:gardenlet-bootstrap@defaultclusters:- cluster:certificate-authority-data:\u0026lt;ca-of-garden-cluster\u0026gt; server: https://\u0026lt;endpoint-of-garden-cluster\u0026gt;name:defaultcontexts:- context:cluster:defaultuser:gardenlet-bootstrapname:gardenlet-bootstrap@defaultusers:- name:gardenlet-bootstrapuser:token:\u0026lt;bootstrap-token\u0026gt;Provide this bootstrap kubeconfig together with a desired name and namespace to the Gardenlet Helm chart values here:  gardenClientConnection:bootstrapKubeconfig:name:gardenlet-kubeconfig-bootstrapnamespace:gardenkubeconfig:| \u0026lt;bootstrap-kubeconfig\u0026gt;Define a name and namespace where the Gardenlet shall store the real kubeconfig it creates during the bootstrap process here:  gardenClientConnection:kubeconfigSecret:name:gardenlet-kubeconfignamespace:gardenDefine either seedSelector or seedConfig (see this document  Now you are ready to deploy the Helm chart:\nhelm install charts/gardener/gardenlet \\  --namespace garden \\  --name gardenlet \\  -f gardenlet-values.yaml \\  --wait :warning: A current prerequisite of Kubernetes clusters that are used as seeds is to have a pre-deployed nginx-ingress-controller to make the Gardener work properly. Moreover, there should exist a DNS record *.ingress.\u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; where \u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; is the value of the .dns.ingressDomain field of a Seed cluster resource (or the respective Gardenlet configuration).\n"},{"uri":"https://gardener.cloud/v1.12.8/contribute/10_code/27_deploy_into_cluster/","title":"Deploy into a Cluster","tags":[],"description":"","content":"Deploying the Gardener into a Kubernetes cluster Similar to Kubernetes, Gardener consists out of control plane components (Gardener API server, Gardener controller manager, Gardener scheduler), and an agent component (Gardenlet). The control plane is deployed in the so-called garden cluster while the agent is installed into every seed cluster. Please note that it is possible to use the garden cluster as seed cluster by simply deploying the Gardenlet into it.\nWe are providing Helm charts in order to manage the various resources of the components. Please always make sure that you use the Helm chart version that matches the Gardener version you want to deploy.\nDeploying the Gardener control plane (API server, controller manager, scheduler) The configuration values depict the various options to configure the different components. Please consult this document to get a detailed explanation of what can be configured for which component. Also note that all resources and deployments need to be created in the garden namespace (not overrideable).\nAfter preparing your values in a separate controlplane-values.yaml file, you can run the following command against your garden cluster:\nhelm install charts/gardener/controlplane \\  --namespace garden \\  --name gardener-controlplane \\  -f gardener-values.yaml \\  --wait Deploying Gardener extensions Gardener is an extensible system that does not contain the logic for provider-specific things like DNS management, cloud infrastructures, network plugins, operating system configs, and many more.\nYou have to install extension controllers for these parts. Please consult the documentation regarding extensions to get more information.\nDeploying the Gardener agent (Gardenlet) The Gardenlet requires a bootstrap token as well as a bootstrap kubeconfig in order to properly register itself with the Gardener control plane.\nThe configuration values depict the various options to configure it. Please consult this document to get a detailed explanation of what can be configured.\nPrepare your values in a separate gardenlet-values.yaml file:\n Create a bootstrap token secret in the kube-system namespace of the garden cluster (see this and this). Create a bootstrap kubeconfig containing this token:  apiVersion:v1kind:Configcurrent-context:gardenlet-bootstrap@defaultclusters:- cluster:certificate-authority-data:\u0026lt;ca-of-garden-cluster\u0026gt; server: https://\u0026lt;endpoint-of-garden-cluster\u0026gt;name:defaultcontexts:- context:cluster:defaultuser:gardenlet-bootstrapname:gardenlet-bootstrap@defaultusers:- name:gardenlet-bootstrapuser:token:\u0026lt;bootstrap-token\u0026gt;Provide this bootstrap kubeconfig together with a desired name and namespace to the Gardenlet Helm chart values here:  gardenClientConnection:bootstrapKubeconfig:name:gardenlet-kubeconfig-bootstrapnamespace:gardenkubeconfig:| \u0026lt;bootstrap-kubeconfig\u0026gt;Define a name and namespace where the Gardenlet shall store the real kubeconfig it creates during the bootstrap process here:  gardenClientConnection:kubeconfigSecret:name:gardenlet-kubeconfignamespace:gardenDefine either seedSelector or seedConfig (see this document  Now you are ready to deploy the Helm chart:\nhelm install charts/gardener/gardenlet \\  --namespace garden \\  --name gardenlet \\  -f gardenlet-values.yaml \\  --wait :warning: A current prerequisite of Kubernetes clusters that are used as seeds is to have a pre-deployed nginx-ingress-controller to make the Gardener work properly. Moreover, there should exist a DNS record *.ingress.\u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; where \u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; is the value of the .dns.ingressDomain field of a Seed cluster resource (or the respective Gardenlet configuration).\n"},{"uri":"https://gardener.cloud/v1.13.2/contribute/10_code/27_deploy_into_cluster/","title":"Deploy into a Cluster","tags":[],"description":"","content":"Deploying the Gardener into a Kubernetes cluster Similar to Kubernetes, Gardener consists out of control plane components (Gardener API server, Gardener controller manager, Gardener scheduler), and an agent component (Gardenlet). The control plane is deployed in the so-called garden cluster while the agent is installed into every seed cluster. Please note that it is possible to use the garden cluster as seed cluster by simply deploying the Gardenlet into it.\nWe are providing Helm charts in order to manage the various resources of the components. Please always make sure that you use the Helm chart version that matches the Gardener version you want to deploy.\nDeploying the Gardener control plane (API server, controller manager, scheduler) The configuration values depict the various options to configure the different components. Please consult this document to get a detailed explanation of what can be configured for which component. Also note that all resources and deployments need to be created in the garden namespace (not overrideable).\nAfter preparing your values in a separate controlplane-values.yaml file, you can run the following command against your garden cluster:\nhelm install charts/gardener/controlplane \\  --namespace garden \\  --name gardener-controlplane \\  -f gardener-values.yaml \\  --wait Deploying Gardener extensions Gardener is an extensible system that does not contain the logic for provider-specific things like DNS management, cloud infrastructures, network plugins, operating system configs, and many more.\nYou have to install extension controllers for these parts. Please consult the documentation regarding extensions to get more information.\nDeploying the Gardener agent (Gardenlet) The Gardenlet requires a bootstrap token as well as a bootstrap kubeconfig in order to properly register itself with the Gardener control plane.\nThe configuration values depict the various options to configure it. Please consult this document to get a detailed explanation of what can be configured.\nPrepare your values in a separate gardenlet-values.yaml file:\n Create a bootstrap token secret in the kube-system namespace of the garden cluster (see this and this). Create a bootstrap kubeconfig containing this token:  apiVersion:v1kind:Configcurrent-context:gardenlet-bootstrap@defaultclusters:- cluster:certificate-authority-data:\u0026lt;ca-of-garden-cluster\u0026gt; server: https://\u0026lt;endpoint-of-garden-cluster\u0026gt;name:defaultcontexts:- context:cluster:defaultuser:gardenlet-bootstrapname:gardenlet-bootstrap@defaultusers:- name:gardenlet-bootstrapuser:token:\u0026lt;bootstrap-token\u0026gt;Provide this bootstrap kubeconfig together with a desired name and namespace to the Gardenlet Helm chart values here:  gardenClientConnection:bootstrapKubeconfig:name:gardenlet-kubeconfig-bootstrapnamespace:gardenkubeconfig:| \u0026lt;bootstrap-kubeconfig\u0026gt;Define a name and namespace where the Gardenlet shall store the real kubeconfig it creates during the bootstrap process here:  gardenClientConnection:kubeconfigSecret:name:gardenlet-kubeconfignamespace:gardenDefine either seedSelector or seedConfig (see this document  Now you are ready to deploy the Helm chart:\nhelm install charts/gardener/gardenlet \\  --namespace garden \\  --name gardenlet \\  -f gardenlet-values.yaml \\  --wait :warning: A current prerequisite of Kubernetes clusters that are used as seeds is to have a pre-deployed nginx-ingress-controller to make the Gardener work properly. Moreover, there should exist a DNS record *.ingress.\u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; where \u0026lt;SEED-CLUSTER-DOMAIN\u0026gt; is the value of the .dns.ingressDomain field of a Seed cluster resource (or the respective Gardenlet configuration).\n"},{"uri":"https://gardener.cloud/documentation/contribute/10_code/30_deploy_seed_into_aks/","title":"Deploy into AKS","tags":[],"description":"","content":"Deploying the previous Gardener versions and a Seed into an AKS cluster This document demonstrates how to install Gardener into an existing AKS cluster. We\u0026rsquo;ll use a single cluster to host both Gardener and a Seed to the same cluster for the sake of simplicity .\nPlease note that this document is to provide you an example installation and is not to be used in a production environment since there are some certificates hardcoded, non-HA and non-TLS-enabled etcd setup.\nHigh Level Overview In this example we\u0026rsquo;ll follow these steps to create a Seed cluster on AKS:\n Deploying the Gardener and a Seed into an AKS cluster High Level Overview Prerequisites  AWS credentials for Route 53 Hosted Zone Deploy AKS cluster  Initialize Helm on the Cluster Deploy stable/nginx-ingress chart to AKS Create wildcard DNS record for the ingress   Create Azure Service Principle to get Azure credentials Install gardenctl   Install Gardener  Create garden namespace Deploy etcd Deploy Gardener Helm Chart   Create a CloudProfile Define Seed cluster in Gardener  Create the Seed resource definition with its Secret   Create a Shoot cluster  Create a Project (namespace) for Shoots Create a SecretBinding and related Secret Create the Shoot resource  Cluster Resources After Shoot is Created Troubleshooting Shoot Creation Issues     Access Shoot cluster Delete Shoot cluster  Prerequisites Summary of prerequisites:\n An Azure AKS cluster with:  Helm initialized, an ingress controller deployed, a wildcard DNS record pointing the ingress, az command line client configured for Azure subscription,   An Azure service principle to provide Azure credentials to Gardener, A Route53 Hosted Zone and AWS account credentials with permissions on that Route53 Zone,  aws command line client configured for this account,   gardenctl command line client configured for the AKS cluster\u0026rsquo;s kubeconfig  Note: Gardener doesn\u0026rsquo;t have support for Azure DNS yet (see #494). So, we use a Route53 Hosted Zone even if we are deploying on Azure.\nAWS credentials for Route 53 Hosted Zone You need to provide credentials for AWS with permission to access Route53 Hosted Zone. In this example we\u0026rsquo;ll assume your domain for the Hosted Zone is .your.domain.here.\nHOSTED_ZONE_ID= # place your AWS Route53 hostedZoneID here Create an AWS user, define policy to allow permission for the Hosted Zone and note the hostedZoneID, accessKeyID and secretAccessKey for later use.\nDeploy AKS cluster Here you can find a summary for creating an AKS cluster, if you already have one, skip this step.\naz group create --name garden-1 --location eastus az aks create --resource-group garden-1 --name garden-1 \\ --kubernetes-version 1.11.5 \\ --node-count 2 --node-vm-size Standard_DS4_v2 \\ --generate-ssh-keys az aks get-credentials --resource-group garden-1 --name garden-1 --admin Initialize Helm on the Cluster Since RBAC is enabled by default we need to deploy helm with an RBAC config.\nkubectl apply -f https://raw.githubusercontent.com/Azure/helm-charts/master/docs/prerequisities/helm-rbac-config.yaml helm init --service-account tiller Deploy stable/nginx-ingress chart to AKS At the moment the Ingress resources created by the Gardener are expecting the nginx-ingress style annotations to work.\nhelm upgrade --install \\ --namespace kube-system \\ nginx-ingress stable/nginx-ingress Create wildcard DNS record for the ingress You need to pick a wildcard subdomain matching your Route53 Hosted Zone here. This ingress wildcard record is supposed to be part of the Seed cluster rather than Gardener cluster, in our example we\u0026rsquo;ll use *.seed-1.your.domain.here.\nAssuming you have the AWS cli for your Route53 Hosted Zone is configured on your local, here we\u0026rsquo;ll create the wildcard DNS record using the awless. You can also use the AWS console or any other tool of your choice to create the wildcard record:\nHOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) INGRESS_DOMAIN=\u0026#34;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; # Get LB IP address from `kubectl -n kube-system get svc shared-ingress-nginx-ingress-controller` LB_IP=$(kubectl -n kube-system get svc nginx-ingress-controller --template \u0026#39;{{(index .status.loadBalancer.ingress 0).ip}}\u0026#39;) awless create record \\ zone=$HOSTED_ZONE_ID \\ name=\u0026#34;*.$INGRESS_DOMAIN\u0026#34; \\ value=$LB_IP \\ type=A \\ ttl=300 Create Azure Service Principle to get Azure credentials We need client_id and client_secret to allow Gardener to reach Azure services, we can generate a pair by creating a Service Principle on Azure:\n$ az ad sp create-for-rbac --role=\u0026#34;Contributor\u0026#34; Retrying role assignment creation: 1/36 { \u0026#34;appId\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34;, #az_client_id \u0026#34;displayName\u0026#34;: \u0026#34;azure-cli-2018-05-23-16-15-49\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;http://azure-cli-2018-05-23-16-15-49\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34;, #az_client_secret \u0026#34;tenant\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34; #az_tenant_id } Let\u0026rsquo;s define some env variables for later use\nCLIENT_ID= # place your Azure Service Principal appId CLIENT_SECRET= # place your Azure Service Principal password here Install gardenctl In this example we\u0026rsquo;ll be using gardenctl to interact with Gardener. You can install gardenctl following instruction in its repo: https://github.com/gardener/gardenctl\nHere is a sample configuration for gardenctl:\n$ cat ~/.garden/config gardenClusters: - name: dev kubeConfig: ~/.kube/config Install Gardener Create garden namespace This is where we deploy Gardener components.\nkubectl apply -f example/00-namespace-garden.yaml Deploy etcd Since Gardener is an extension API Server, it can share the etcd backing native Kubernetes cluster\u0026rsquo;s API Server, and hence explicit etcd installation is optional. But in our case we have no access to the control plane components of the AKS cluster and we have to deploy our own etcd ourselves for Gardener. Lets deploy an etcd using the gardener/etcd-backup-restore project, which is also used by the Gardener for Shoot control plane.\n# pull the etcd-backup-restore git clone https://github.com/gardener/etcd-backup-restore.git # deploy etcd helm upgrade --install \\ --namespace garden \\ etcd etcd-backup-restore/chart \\ --set tls= Note: This etcd installation doesn\u0026rsquo;t provide HA. But etcd will be auto recovered by the Deployment. This could be sufficient for some deployments but may not be suitable for production usage. Also note that this etcd is not deployed with TLS enabled and doesn\u0026rsquo;t use certificates for authentication.\nCheck etcd pod\u0026rsquo;s health, it should have READY:2/2 and STATUS:Running:\n$ kubectl -n garden get pods NAME READY STATUS RESTARTS AGE etcd-for-test-0 2/2 Running 0 1m Deploy Gardener Helm Chart Check (current releases)[https://github.com/gardener/gardener/releases] and pick a suitable one to install.\nGARDENER_RELEASE=0.17.1 gardener-controller-manager will need to maintain some DNS records for Seed. So, you need to provide Route53 credentials in the values.yaml file:\n global.controller.internalDomain.hostedZoneID global.controller.internalDomain.domain: Here pick a subdomain for your Gardener to maintain DNS records for your Shoot clusters. This domain has to be within your Route53 Hosted Zone. e.g. garden-1.your.domain.here global.controller.internalDomain.credentials global.controller.internalDomain.secretAccessKey  HOSTED_ZONE_DOMAIN=$( aws route53 get-hosted-zone \\ --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} \\ --query \u0026#39;HostedZone.Name\u0026#39; \\ --output text) HOSTED_ZONE_DOMAIN=${HOSTED_ZONE_DOMAIN%%.} GARDENER_DOMAIN=\u0026#34;garden-1.${HOSTED_ZONE_DOMAIN}\u0026#34; ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) cat \u0026lt;\u0026lt;EOF \u0026gt; gardener-values.yaml global: apiserver: image: tag: ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} etcd: servers: http://etcd-for-test-client:2379 useSidecar: false controller: image: tag: ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} internalDomain: provider: aws-route53 hostedZoneID: ${HOSTED_ZONE_ID} domain: ${HOSTED_ZONE_DOMAIN} credentials: AWS_ACCESS_KEY_ID: ${ACCESS_KEY_ID} AWS_SECRET_ACCESS_KEY: ${SECRET_ACCESS_KEY} EOF After creating the gardener-values.yaml file, since chart definition in master branch can have breaking changes after the release, checkout the gardener tag for that release, and run:\ngit checkout ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} helm upgrade --install \\ --namespace garden \\ garden charts/gardener \\ -f charts/gardener/local-values.yaml \\ -f gardener-values.yaml Validate the Gardener is deployed:\nhelm status garden # Wait for `STATUS: DEPLOYED` kubectl -n garden get deploy,pod -l app=gardener # Better if you leave two terminals open in for below commands, and # keep an eye on whats going on behind the scenes as you create/delete # Gardener specific resources (Seed, CloudProfile, SecretBinding, Shoot). kubectl -n garden logs -f deployment/gardener-apiserver # confirm no issues kubectl -n garden logs -f deployment/gardener-controller-manager # confirm no issues, except some \u0026#34;Failed to list *v1beta1...\u0026#34; messages Note: This is not meant to be used in production. You may not want to use apiserver.insecureSkipTLSVerify=true, the hardcoded apiserver certificates, and insecure (non-tls enabled) etcd. But for the sake of keeping this example simple you can just keep those values as they are.\nCreate a CloudProfile We need to create a CloudProfile to be referred from the Shoot (example/30-cloudprofile-azure.yaml):\nkubectl apply -f example/30-cloudprofile-azure.yaml Validate that CloudProfile is created:\nkubectl describe -f example/30-cloudprofile-azure.yaml Define Seed cluster in Gardener In our setup we\u0026rsquo;ll use the cluster for Gardener also as a Seed, this saves us from creating a new Kubernetes cluster. But you can also create an explicit cluster for the Seed. Seed cluster can also be placed into any other cloud provider or on prem. But keep in mind that below steps may differ if you use a different cluster for seed.\nCurrently, a Seed cluster is just a Kubeconfig for the Gardener. The seed cluster could have been created by any tool, Gardener only cares about having a valid Kubeconfig to talk to its API.\nCreate the Seed resource definition with its Secret Lets start with the required seed secret first. Here we need to provide it\u0026rsquo;s cloud provider credentials and kubeconfig in the seed secret. Update example/40-secret-seed-azure.yaml and place the secrets for your environment:\n data.subscriptionID: you can learn this one with az account show data.tenantID: from az ad sp create-for-rbac output as you can see above data.clientID: from az ad sp create-for-rbac output as you can see above data.clientSecret: from az ad sp create-for-rbac output as you can see above data.kubeconfig: you can get this one with az aks get-credentials --resource-group garden-1 --name garden-1 -f - | base64)  Note: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r \u0026#39;.[] | select(.isDefault == true) | .id\u0026#39;) TENANT_ID=$(az account show -o tsv --query \u0026#39;tenantId\u0026#39;) KUBECONFIG_FOR_SEED_CLUSTER=$(az aks get-credentials --resource-group garden-1 --name garden-1 -f -) sed -i \\ -e \u0026#34;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-tenant)@$(echo \u0026#34;$TENANT_ID\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-client)@$(echo \u0026#34;${CLIENT_ID:?\u0026#34;CLIENT_ID is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(client-secret)@$(echo \u0026#34;${CLIENT_SECRET:?\u0026#34;CLIENT_SECRET is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(kubeconfig-for-seed-cluster)@$(echo \u0026#34;$KUBECONFIG_FOR_SEED_CLUSTER\u0026#34; | base64 -w 0)@\u0026#34; \\ example/40-secret-seed-azure.yaml After updating the fields, create the Seed secret:\nkubectl apply -f example/40-secret-seed-azure.yaml Before creating Seed, we need to update the example/50-seed-azure.yaml file and update:\n spec.networks: IP ranges used in your AKS cluster. spec.ingressDomain: Place here the wildcard domain you have for the ingress controller (we created this record in prerequisites). Gardener doesn\u0026rsquo;t create this DNS records but assumes its created ahead of time, Seed clusters are not provisioned by Gardener. spec.cloud.region: eastus (the region of the existing AKS cluster)  HOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) INGRESS_DOMAIN=\u0026#34;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; # discover AKS CIDRs NODE_CIDR=$(az network vnet list -g MC_garden-1_garden-1_eastus -o json | jq -r \u0026#39;.[] | .subnets[] | .addressPrefix\u0026#39;) POD_CIDR=$(kubectl -n kube-system get daemonset/kube-proxy -o yaml | grep cluster-cidr= | grep -v annotations | cut -d = -f2) SERVICE_CIDR=10.0.0.0/16 # This one is hardcoded for now, not easy to discover sed -i \\ -e \u0026#34;s/ingressDomain: dev.azure.seed.example.com/ingressDomain: $INGRESS_DOMAIN/\u0026#34; \\ -e \u0026#34;s/region: westeurope/region: eastus/\u0026#34; \\ -e \u0026#34;s@nodes: 10.240.0.0/16@nodes: $NODE_CIDR@\u0026#34; \\ -e \u0026#34;s@pods: 10.241.128.0/17@pods: $POD_CIDR@\u0026#34; \\ -e \u0026#34;s@services: 10.241.0.0/17@services: $SERVICE_CIDR@\u0026#34; \\ example/50-seed-azure.yaml Now we are ready to create the seed:\nkubectl apply -f example/50-seed-azure.yaml Check the logs in gardener-controller-manager and also wait for seed to be Ready: True. This means gardener-controller-manager is able to reach the Seed cluster with the credentials you provide.\n$ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ kubectl get seed azure NAME CLOUDPROFILE REGION INGRESS DOMAIN AVAILABLE AGE azure azure eastus seed-1.your.domain.here True 1m $ gardenctl ls seeds seeds: - seed: azure If something goes wrong verify that you provided right credentials, and base64 encoded strings of those in the secret. Also check the status field in the Seed resource and gardener-controller-manager logs:\n$ kubectl get seed azure -o json | jq .status { \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2018-05-31T14:56:49Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;all checks passed\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Passed\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Available\u0026#34; } ] } Create a Shoot cluster Create a Project (namespace) for Shoots In this step we create a namespace in Gardener cluster to keep Shoot resource definitions. A project in Gardener terminology is simply a namespace that holds group of Shoots, during this example we\u0026rsquo;ll deploy a single Shoot. (Mind the extra labels defined in example/00-namespace-garden-dev.yaml).\nkubectl apply -f example/05-project-dev.yaml You can check the projects via gardenctl:\n$ gardenctl target garden dev $ kubectl get project dev NAME NAMESPACE STATUS OWNER CREATOR AGE dev garden-dev Ready john.doe@example.com client 1m $ kubectl get ns garden-dev NAME STATUS AGE garden-dev Active 1m $ gardenctl ls projects projects: - project: garden-dev Create a SecretBinding and related Secret We\u0026rsquo;ll use same Azure credentials with example/40-secret-seed-azure.yaml, this is due to the fact that we use the same Azure Subscription for the Shoot and Seed clusters. Differently from the Seed secret, in this one we don\u0026rsquo;t need to provide kubeconfig since the Shoot cluster will be provisioned by Gardener, and we need to provide credentials for Route53 DNS records management.\nUpdate example/70-secret-cloudprovider-azure.yaml and place the secrets for your environment:\n data.subscriptionID: you can learn this one with az account show data.tenantID: from az ad sp create-for-rbac output as you can see above data.clientID: from az ad sp create-for-rbac output as you can see above data.clientSecret: from az ad sp create-for-rbac output as you can see above data.accessKeyID: You need to add this field for Route53 records to be updated. data.secretAccessKey: You need to add this field for Route53 records to be updated.  Note: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r \u0026#39;.[] | select(.isDefault == true) | .id\u0026#39;) TENANT_ID=$(az account show -o tsv --query \u0026#39;tenantId\u0026#39;) ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) sed -i \\ -e \u0026#34;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-tenant)@$(echo \u0026#34;$TENANT_ID\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-client)@$(echo \u0026#34;${CLIENT_ID:?\u0026#34;CLIENT_ID is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(client-secret)@$(echo \u0026#34;${CLIENT_SECRET:?\u0026#34;CLIENT_SECRET is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;\\$a\\ \\ accessKeyID: $(echo $ACCESS_KEY_ID | tr -d \u0026#39;\\n\u0026#39; | base64 )\u0026#34; \\ -e \u0026#34;\\$a\\ \\ secretAccessKey: $(echo $SECRET_ACCESS_KEY | tr -d \u0026#39;\\n\u0026#39; | base64 )\u0026#34; \\ example/70-secret-cloudprovider-azure.yaml After updating the fields, create the cloud provider secret:\nkubectl apply -f example/70-secret-cloudprovider-azure.yaml And create the SecretBinding resource to allow Gardener use that secret (example/80-secretbinding-cloudprovider-azure.yaml):\nsed -i \\ -e \u0026#39;s/# namespace: .*/ namespace: garden-dev/\u0026#39; \\ example/80-secretbinding-cloudprovider-azure.yaml kubectl apply -f example/80-secretbinding-cloudprovider-azure.yaml Check the logs in gardener-controller-manager, there should not be any problems reported.\nCreate the Shoot resource Update the fields in example/90-deprecated-shoot-azure.yaml:\n spec.cloud.region: eastus (this must match the seed cluster\u0026rsquo;s region) spec.dns.domain: This is used to specify the base domain for your api (and other in the future) endpoint(s). For example when johndoe-azure.garden-dev.your.domain.here is used as a value, then your apiserver is available at api.johndoe-azure.garden-dev.your.domain.here spec.dns.hostedZoneID: This field doesn\u0026rsquo;t exist in the example you need to add this field and place the Route53 Hosted Zone ID.  HOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) SHOOT_DOMAIN=\u0026#34;johndoe-azure.garden-dev.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; KUBE_LEGO_EMAIL=$(git config user.email) sed -i \\ -e \u0026#34;s/region: westeurope/region: eastus/\u0026#34; \\ -e \u0026#34;s/domain: johndoe-azure.garden-dev.example.com/domain: $SHOOT_DOMAIN/\u0026#34; \\ -e \u0026#34;/domain:/a\\ \\ \\ \\ hostedZoneID: $HOSTED_ZONE_ID\u0026#34; \\ -e \u0026#34;s/email: john.doe@example.com/email: $KUBE_LEGO_EMAIL/\u0026#34; \\ example/90-deprecated-shoot-azure.yaml And let\u0026rsquo;s create the Shoot resource:\nkubectl apply -f example/90-deprecated-shoot-azure.yaml After creating the Shoot resource, gardener-controller-manager will pick it up and start provisioning the Shoot cluster.\n$ kubectl get -f example/90-deprecated-shoot-azure.yaml NAME CLOUDPROFILE VERSION SEED DOMAIN OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE johndoe-azure azure 1.12.3 azure johndoe-azure.garden-dev.your.domain.here Processing 15 \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; 16s Follow the logs in your console with gardener-controller-manager, starting like below you\u0026rsquo;ll see plenty of Waiting and Executing, etc. logs and many tasks will keep repeating:\ntime=\u0026#34;2018-06-09T07:35:45Z\u0026#34; level=info msg=\u0026#34;[SHOOT RECONCILE] garden-dev/johndoe-azure\u0026#34; time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Starting flow Shoot cluster creation\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).botanist.Shoot.Components.DNS.External{Provider/Entry}.Deploy\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployNamespace\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployKubeAPIServerService\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployBackupNamespaceFromShoot\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:51Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:51Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).MoveBackupTerraformResources\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).WaitUntilKubeAPIServerServiceIsReady\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:56Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:57Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:57Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:01Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:02Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:02Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure ... At this stage you should be waiting for a while until the Shoot cluster is provisioned and initial resources are deployed.\nDuring the provisioning you can also check output of these commands to have a better understanding about what\u0026rsquo;s going on in the seed cluster:\n$ gardenctl ls shoots projects: - project: garden-dev shoots: - johndoe-azure $ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: johndoe-azure health: Unknown status: lastOperation: description: Executing DeployKubeAddonManager, ReconcileMachines. lastUpdateTime: 2018-06-09 08:40:20 +0100 IST progress: 74 state: Processing type: Create $ kubectl -n garden-dev get shoot johndoe-azure NAMESPACE NAME SEED DOMAIN VERSION CONTROL NODES SYSTEM LATEST garden-dev johndoe-azure azure johndoe-azure.garden-dev.your.domain.here 1.10.1 True True True Succeeded $ kubectl -n garden-dev describe shoot johndoe-azure ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Reconciling 1h gardener-controller-manager [BrXWiztO] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [rBFsfwU5] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [2HAbm45D] Reconciling Shoot cluster state Normal Reconciling 48m gardener-controller-manager [S1QA0ksz] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [lvcSKy1Q] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [MddMyk8W] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [XDAAWABd] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [6HYH9Psz] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [rhL38ym4] Reconciling Shoot cluster state Normal Reconciling 35m gardener-controller-manager [BOt4Nvso] Reconciling Shoot cluster state Normal Reconciling 35m gardener-controller-manager [JPtmXmxD] Reconciling Shoot cluster state Normal Reconciling 34m gardener-controller-manager [ldHsVA6G] Reconciling Shoot cluster state Normal Reconciled 31m gardener-controller-manager [ldHsVA6G] Reconciled Shoot cluster state Normal Reconciling 26m gardener-controller-manager [yBh2IBOF] Reconciling Shoot cluster state Normal Reconciled 24m gardener-controller-manager [yBh2IBOF] Reconciled Shoot cluster state Normal Reconciling 16m gardener-controller-manager [bqmFtHUA] Reconciling Shoot cluster state Normal Reconciled 14m gardener-controller-manager [bqmFtHUA] Reconciled Shoot cluster state Normal Reconciling 6m gardener-controller-manager [7QgHE5CH] Reconciling Shoot cluster state Normal Reconciled 3m gardener-controller-manager [7QgHE5CH] Reconciled Shoot cluster state Check Shoot cluster:\n$ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ gardenctl target project garden-dev $ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ gardenctl kubectl cluster-info Kubernetes master is running at https://api.johndoe-azure.garden-dev.your.domain.here CoreDNS is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubernetes-dashboard is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. Cluster Resources After Shoot is Created After the Shoot has been created the summary of the resources in the AKS cluster handled by Gardener will be something like this:\nnon-namespaced resources CloudProfile: azure Project: dev Namespace: garden-dev Seed: azure # cloud.profile:azure, cloud.region:eastus, secretRef.name:seed-azure, secretRef.namespace: garden Namespace: garden Secret: seed-azure # aks credentials, kubeconfig # No other resources with any kind handled by Gardener # Gardener components as well lives in this namespace Namespace: garden-dev # maps to \u0026#34;project:dev\u0026#34; in Gardener Secret: core-azure # credentials for aks + aws (for route53) SecretBinding: core-azure # secretRef.name:core-azure Shoot: johndoe-azure # seed:azure, secretBindingRef.name:core-azure Namespace: shoot--dev--johndoe-azure # These are automatically created once Shoot resource is created AzureMachineClass: shoot--dev--johndoe-azure-cpu-worker-8506a MachineDeployment: shoot--dev--johndoe-azure-cpu-worker MachineSet: shoot--dev--johndoe-azure-cpu-worker-849bbbf75 Machine: shoot--dev--johndoe-azure-cpu-worker-849bbbf75-b42vh BackupInfra: shoot--dev--johndoe-azure--c1b3b # seed:azure, shootUID: shoot.status.UID. # Many other resources created as part of shoot cluster, # but only above ones are handled by Gardener Namespace: backup--shoot--dev--johndoe-azure--c1b3b # Secrets and configMap having info related to backup infrastructure # are created by Gardener. Troubleshooting Shoot Creation Issues For any issue happening during Shoot provisioning, you can consult the gardener-controller-manager logs, or the state in the shoot resource, gardenctl also provides a command to check Shoot cluster states:\n# check gardener-controller-manager logs kubectl -n garden logs -f deployment/gardener-controller-manager # kubectl describe can provide you a human readable output of # same information in below gardenctl command. kubectl -n garden-dev describe shoot johndoe-azure # also try cheking the machine-controller-manager logs of the shoot kubectl logs -n shoot--dev--johndoe-azure deployment/machine-controller-manager With gardenctl:\n$ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: health: Ready status: johndoe-azure lastError: \u0026#34;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: \u0026#39;(*Botanist).Shoot.Components.DNS.External{Provider/Entry}.Destroy\u0026#39; returned \u0026#39;Terraform execution ... lastOperation: description: \u0026#34;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: \u0026#39;(*Botanist).Shoot.Components.DNS.External{Provider/Entry}.Destroy\u0026#39; returned \u0026#39;Terraform ... lastUpdateTime: 2018-06-03 09:48:00 +0100 IST progress: 100 state: Failed type: Reconcile Access Shoot cluster The gardenctl tool provides a convenient wrapper to operate on both cluster and cloud providers, here are some commands you can run\n# select target shoot cluster gardenctl ls gardens gardenctl target garden dev gardenctl ls projects gardenctl target shoot johndoe-azure # issue Azure client (az) commands on target shoot gardenctl az aks list # issue kubectl commands on target shoot gardenctl kubectl -- version --short # \u0026#39;--\u0026#39; is required if you want to # pass any args starting with \u0026#39;-\u0026#39; # open prometheus, alertmanager, grafana without having to find # the user/pass for each gardenctl show prometheus gardenctl show grafana gardenctl show alertmanager Easiest way to obtain kubeconfig of the shoot cluster:\n$ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ export KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ # From now on your local kubectl will be operating on target shoot $ kubectl cluster-info # will show your shoot cluster info $ unset KUBECONFIG # reset to your default kubectl The shoot cluster\u0026rsquo;s kubeconfig is being kept in a secret in the project namespace:\nkubectl -n shoot--dev--johndoe-azure get secret kubecfg -o jsonpath=\u0026#39;{.data.kubeconfig}\u0026#39; | base64 -D \u0026gt; /tmp/johndoe-azure-kubeconfig.yaml export KUBECONFIG=/tmp/johndoe-azure-kubeconfig.yaml Delete Shoot cluster Deleting a Shoot cluster is not straight forward, and this is to protect users from undesired/accidental cluster deletion. One has to place some special annotations to get a Shoot cluster removed. We use the hack/usage/delete script for this purpose.\nPlease refer to Creating / Deleting a Shoot cluster document for more details.\nhack/delete shoot johndoe-azure garden-dev "},{"uri":"https://gardener.cloud/v1.12.8/contribute/10_code/30_deploy_seed_into_aks/","title":"Deploy into AKS","tags":[],"description":"","content":"Deploying the previous Gardener versions and a Seed into an AKS cluster This document demonstrates how to install Gardener into an existing AKS cluster. We\u0026rsquo;ll use a single cluster to host both Gardener and a Seed to the same cluster for the sake of simplicity .\nPlease note that this document is to provide you an example installation and is not to be used in a production environment since there are some certificates hardcoded, non-HA and non-TLS-enabled etcd setup.\nHigh Level Overview In this example we\u0026rsquo;ll follow these steps to create a Seed cluster on AKS:\n Deploying the Gardener and a Seed into an AKS cluster High Level Overview Prerequisites  AWS credentials for Route 53 Hosted Zone Deploy AKS cluster  Initialize Helm on the Cluster Deploy stable/nginx-ingress chart to AKS Create wildcard DNS record for the ingress   Create Azure Service Principle to get Azure credentials Install gardenctl   Install Gardener  Create garden namespace Deploy etcd Deploy Gardener Helm Chart   Create a CloudProfile Define Seed cluster in Gardener  Create the Seed resource definition with its Secret   Create a Shoot cluster  Create a Project (namespace) for Shoots Create a SecretBinding and related Secret Create the Shoot resource  Cluster Resources After Shoot is Created Troubleshooting Shoot Creation Issues     Access Shoot cluster Delete Shoot cluster  Prerequisites Summary of prerequisites:\n An Azure AKS cluster with:  Helm initialized, an ingress controller deployed, a wildcard DNS record pointing the ingress, az command line client configured for Azure subscription,   An Azure service principle to provide Azure credentials to Gardener, A Route53 Hosted Zone and AWS account credentials with permissions on that Route53 Zone,  aws command line client configured for this account,   gardenctl command line client configured for the AKS cluster\u0026rsquo;s kubeconfig  Note: Gardener doesn\u0026rsquo;t have support for Azure DNS yet (see #494). So, we use a Route53 Hosted Zone even if we are deploying on Azure.\nAWS credentials for Route 53 Hosted Zone You need to provide credentials for AWS with permission to access Route53 Hosted Zone. In this example we\u0026rsquo;ll assume your domain for the Hosted Zone is .your.domain.here.\nHOSTED_ZONE_ID= # place your AWS Route53 hostedZoneID here Create an AWS user, define policy to allow permission for the Hosted Zone and note the hostedZoneID, accessKeyID and secretAccessKey for later use.\nDeploy AKS cluster Here you can find a summary for creating an AKS cluster, if you already have one, skip this step.\naz group create --name garden-1 --location eastus az aks create --resource-group garden-1 --name garden-1 \\ --kubernetes-version 1.11.5 \\ --node-count 2 --node-vm-size Standard_DS4_v2 \\ --generate-ssh-keys az aks get-credentials --resource-group garden-1 --name garden-1 --admin Initialize Helm on the Cluster Since RBAC is enabled by default we need to deploy helm with an RBAC config.\nkubectl apply -f https://raw.githubusercontent.com/Azure/helm-charts/master/docs/prerequisities/helm-rbac-config.yaml helm init --service-account tiller Deploy stable/nginx-ingress chart to AKS At the moment the Ingress resources created by the Gardener are expecting the nginx-ingress style annotations to work.\nhelm upgrade --install \\ --namespace kube-system \\ nginx-ingress stable/nginx-ingress Create wildcard DNS record for the ingress You need to pick a wildcard subdomain matching your Route53 Hosted Zone here. This ingress wildcard record is supposed to be part of the Seed cluster rather than Gardener cluster, in our example we\u0026rsquo;ll use *.seed-1.your.domain.here.\nAssuming you have the AWS cli for your Route53 Hosted Zone is configured on your local, here we\u0026rsquo;ll create the wildcard DNS record using the awless. You can also use the AWS console or any other tool of your choice to create the wildcard record:\nHOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) INGRESS_DOMAIN=\u0026#34;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; # Get LB IP address from `kubectl -n kube-system get svc shared-ingress-nginx-ingress-controller` LB_IP=$(kubectl -n kube-system get svc nginx-ingress-controller --template \u0026#39;{{(index .status.loadBalancer.ingress 0).ip}}\u0026#39;) awless create record \\ zone=$HOSTED_ZONE_ID \\ name=\u0026#34;*.$INGRESS_DOMAIN\u0026#34; \\ value=$LB_IP \\ type=A \\ ttl=300 Create Azure Service Principle to get Azure credentials We need client_id and client_secret to allow Gardener to reach Azure services, we can generate a pair by creating a Service Principle on Azure:\n$ az ad sp create-for-rbac --role=\u0026#34;Contributor\u0026#34; Retrying role assignment creation: 1/36 { \u0026#34;appId\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34;, #az_client_id \u0026#34;displayName\u0026#34;: \u0026#34;azure-cli-2018-05-23-16-15-49\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;http://azure-cli-2018-05-23-16-15-49\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34;, #az_client_secret \u0026#34;tenant\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34; #az_tenant_id } Let\u0026rsquo;s define some env variables for later use\nCLIENT_ID= # place your Azure Service Principal appId CLIENT_SECRET= # place your Azure Service Principal password here Install gardenctl In this example we\u0026rsquo;ll be using gardenctl to interact with Gardener. You can install gardenctl following instruction in its repo: https://github.com/gardener/gardenctl\nHere is a sample configuration for gardenctl:\n$ cat ~/.garden/config gardenClusters: - name: dev kubeConfig: ~/.kube/config Install Gardener Create garden namespace This is where we deploy Gardener components.\nkubectl apply -f example/00-namespace-garden.yaml Deploy etcd Since Gardener is an extension API Server, it can share the etcd backing native Kubernetes cluster\u0026rsquo;s API Server, and hence explicit etcd installation is optional. But in our case we have no access to the control plane components of the AKS cluster and we have to deploy our own etcd ourselves for Gardener. Lets deploy an etcd using the gardener/etcd-backup-restore project, which is also used by the Gardener for Shoot control plane.\n# pull the etcd-backup-restore git clone https://github.com/gardener/etcd-backup-restore.git # deploy etcd helm upgrade --install \\ --namespace garden \\ etcd etcd-backup-restore/chart \\ --set tls= Note: This etcd installation doesn\u0026rsquo;t provide HA. But etcd will be auto recovered by the Deployment. This could be sufficient for some deployments but may not be suitable for production usage. Also note that this etcd is not deployed with TLS enabled and doesn\u0026rsquo;t use certificates for authentication.\nCheck etcd pod\u0026rsquo;s health, it should have READY:2/2 and STATUS:Running:\n$ kubectl -n garden get pods NAME READY STATUS RESTARTS AGE etcd-for-test-0 2/2 Running 0 1m Deploy Gardener Helm Chart Check (current releases)[https://github.com/gardener/gardener/releases] and pick a suitable one to install.\nGARDENER_RELEASE=0.17.1 gardener-controller-manager will need to maintain some DNS records for Seed. So, you need to provide Route53 credentials in the values.yaml file:\n global.controller.internalDomain.hostedZoneID global.controller.internalDomain.domain: Here pick a subdomain for your Gardener to maintain DNS records for your Shoot clusters. This domain has to be within your Route53 Hosted Zone. e.g. garden-1.your.domain.here global.controller.internalDomain.credentials global.controller.internalDomain.secretAccessKey  HOSTED_ZONE_DOMAIN=$( aws route53 get-hosted-zone \\ --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} \\ --query \u0026#39;HostedZone.Name\u0026#39; \\ --output text) HOSTED_ZONE_DOMAIN=${HOSTED_ZONE_DOMAIN%%.} GARDENER_DOMAIN=\u0026#34;garden-1.${HOSTED_ZONE_DOMAIN}\u0026#34; ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) cat \u0026lt;\u0026lt;EOF \u0026gt; gardener-values.yaml global: apiserver: image: tag: ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} etcd: servers: http://etcd-for-test-client:2379 useSidecar: false controller: image: tag: ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} internalDomain: provider: aws-route53 hostedZoneID: ${HOSTED_ZONE_ID} domain: ${HOSTED_ZONE_DOMAIN} credentials: AWS_ACCESS_KEY_ID: ${ACCESS_KEY_ID} AWS_SECRET_ACCESS_KEY: ${SECRET_ACCESS_KEY} EOF After creating the gardener-values.yaml file, since chart definition in master branch can have breaking changes after the release, checkout the gardener tag for that release, and run:\ngit checkout ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} helm upgrade --install \\ --namespace garden \\ garden charts/gardener \\ -f charts/gardener/local-values.yaml \\ -f gardener-values.yaml Validate the Gardener is deployed:\nhelm status garden # Wait for `STATUS: DEPLOYED` kubectl -n garden get deploy,pod -l app=gardener # Better if you leave two terminals open in for below commands, and # keep an eye on whats going on behind the scenes as you create/delete # Gardener specific resources (Seed, CloudProfile, SecretBinding, Shoot). kubectl -n garden logs -f deployment/gardener-apiserver # confirm no issues kubectl -n garden logs -f deployment/gardener-controller-manager # confirm no issues, except some \u0026#34;Failed to list *v1beta1...\u0026#34; messages Note: This is not meant to be used in production. You may not want to use apiserver.insecureSkipTLSVerify=true, the hardcoded apiserver certificates, and insecure (non-tls enabled) etcd. But for the sake of keeping this example simple you can just keep those values as they are.\nCreate a CloudProfile We need to create a CloudProfile to be referred from the Shoot (example/30-cloudprofile-azure.yaml):\nkubectl apply -f example/30-cloudprofile-azure.yaml Validate that CloudProfile is created:\nkubectl describe -f example/30-cloudprofile-azure.yaml Define Seed cluster in Gardener In our setup we\u0026rsquo;ll use the cluster for Gardener also as a Seed, this saves us from creating a new Kubernetes cluster. But you can also create an explicit cluster for the Seed. Seed cluster can also be placed into any other cloud provider or on prem. But keep in mind that below steps may differ if you use a different cluster for seed.\nCurrently, a Seed cluster is just a Kubeconfig for the Gardener. The seed cluster could have been created by any tool, Gardener only cares about having a valid Kubeconfig to talk to its API.\nCreate the Seed resource definition with its Secret Lets start with the required seed secret first. Here we need to provide it\u0026rsquo;s cloud provider credentials and kubeconfig in the seed secret. Update example/40-secret-seed-azure.yaml and place the secrets for your environment:\n data.subscriptionID: you can learn this one with az account show data.tenantID: from az ad sp create-for-rbac output as you can see above data.clientID: from az ad sp create-for-rbac output as you can see above data.clientSecret: from az ad sp create-for-rbac output as you can see above data.kubeconfig: you can get this one with az aks get-credentials --resource-group garden-1 --name garden-1 -f - | base64)  Note: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r \u0026#39;.[] | select(.isDefault == true) | .id\u0026#39;) TENANT_ID=$(az account show -o tsv --query \u0026#39;tenantId\u0026#39;) KUBECONFIG_FOR_SEED_CLUSTER=$(az aks get-credentials --resource-group garden-1 --name garden-1 -f -) sed -i \\ -e \u0026#34;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-tenant)@$(echo \u0026#34;$TENANT_ID\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-client)@$(echo \u0026#34;${CLIENT_ID:?\u0026#34;CLIENT_ID is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(client-secret)@$(echo \u0026#34;${CLIENT_SECRET:?\u0026#34;CLIENT_SECRET is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(kubeconfig-for-seed-cluster)@$(echo \u0026#34;$KUBECONFIG_FOR_SEED_CLUSTER\u0026#34; | base64 -w 0)@\u0026#34; \\ example/40-secret-seed-azure.yaml After updating the fields, create the Seed secret:\nkubectl apply -f example/40-secret-seed-azure.yaml Before creating Seed, we need to update the example/50-seed-azure.yaml file and update:\n spec.networks: IP ranges used in your AKS cluster. spec.ingressDomain: Place here the wildcard domain you have for the ingress controller (we created this record in prerequisites). Gardener doesn\u0026rsquo;t create this DNS records but assumes its created ahead of time, Seed clusters are not provisioned by Gardener. spec.cloud.region: eastus (the region of the existing AKS cluster)  HOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) INGRESS_DOMAIN=\u0026#34;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; # discover AKS CIDRs NODE_CIDR=$(az network vnet list -g MC_garden-1_garden-1_eastus -o json | jq -r \u0026#39;.[] | .subnets[] | .addressPrefix\u0026#39;) POD_CIDR=$(kubectl -n kube-system get daemonset/kube-proxy -o yaml | grep cluster-cidr= | grep -v annotations | cut -d = -f2) SERVICE_CIDR=10.0.0.0/16 # This one is hardcoded for now, not easy to discover sed -i \\ -e \u0026#34;s/ingressDomain: dev.azure.seed.example.com/ingressDomain: $INGRESS_DOMAIN/\u0026#34; \\ -e \u0026#34;s/region: westeurope/region: eastus/\u0026#34; \\ -e \u0026#34;s@nodes: 10.240.0.0/16@nodes: $NODE_CIDR@\u0026#34; \\ -e \u0026#34;s@pods: 10.241.128.0/17@pods: $POD_CIDR@\u0026#34; \\ -e \u0026#34;s@services: 10.241.0.0/17@services: $SERVICE_CIDR@\u0026#34; \\ example/50-seed-azure.yaml Now we are ready to create the seed:\nkubectl apply -f example/50-seed-azure.yaml Check the logs in gardener-controller-manager and also wait for seed to be Ready: True. This means gardener-controller-manager is able to reach the Seed cluster with the credentials you provide.\n$ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ kubectl get seed azure NAME CLOUDPROFILE REGION INGRESS DOMAIN AVAILABLE AGE azure azure eastus seed-1.your.domain.here True 1m $ gardenctl ls seeds seeds: - seed: azure If something goes wrong verify that you provided right credentials, and base64 encoded strings of those in the secret. Also check the status field in the Seed resource and gardener-controller-manager logs:\n$ kubectl get seed azure -o json | jq .status { \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2018-05-31T14:56:49Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;all checks passed\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Passed\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Available\u0026#34; } ] } Create a Shoot cluster Create a Project (namespace) for Shoots In this step we create a namespace in Gardener cluster to keep Shoot resource definitions. A project in Gardener terminology is simply a namespace that holds group of Shoots, during this example we\u0026rsquo;ll deploy a single Shoot. (Mind the extra labels defined in example/00-namespace-garden-dev.yaml).\nkubectl apply -f example/05-project-dev.yaml You can check the projects via gardenctl:\n$ gardenctl target garden dev $ kubectl get project dev NAME NAMESPACE STATUS OWNER CREATOR AGE dev garden-dev Ready john.doe@example.com client 1m $ kubectl get ns garden-dev NAME STATUS AGE garden-dev Active 1m $ gardenctl ls projects projects: - project: garden-dev Create a SecretBinding and related Secret We\u0026rsquo;ll use same Azure credentials with example/40-secret-seed-azure.yaml, this is due to the fact that we use the same Azure Subscription for the Shoot and Seed clusters. Differently from the Seed secret, in this one we don\u0026rsquo;t need to provide kubeconfig since the Shoot cluster will be provisioned by Gardener, and we need to provide credentials for Route53 DNS records management.\nUpdate example/70-secret-cloudprovider-azure.yaml and place the secrets for your environment:\n data.subscriptionID: you can learn this one with az account show data.tenantID: from az ad sp create-for-rbac output as you can see above data.clientID: from az ad sp create-for-rbac output as you can see above data.clientSecret: from az ad sp create-for-rbac output as you can see above data.accessKeyID: You need to add this field for Route53 records to be updated. data.secretAccessKey: You need to add this field for Route53 records to be updated.  Note: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r \u0026#39;.[] | select(.isDefault == true) | .id\u0026#39;) TENANT_ID=$(az account show -o tsv --query \u0026#39;tenantId\u0026#39;) ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) sed -i \\ -e \u0026#34;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-tenant)@$(echo \u0026#34;$TENANT_ID\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-client)@$(echo \u0026#34;${CLIENT_ID:?\u0026#34;CLIENT_ID is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(client-secret)@$(echo \u0026#34;${CLIENT_SECRET:?\u0026#34;CLIENT_SECRET is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;\\$a\\ \\ accessKeyID: $(echo $ACCESS_KEY_ID | tr -d \u0026#39;\\n\u0026#39; | base64 )\u0026#34; \\ -e \u0026#34;\\$a\\ \\ secretAccessKey: $(echo $SECRET_ACCESS_KEY | tr -d \u0026#39;\\n\u0026#39; | base64 )\u0026#34; \\ example/70-secret-cloudprovider-azure.yaml After updating the fields, create the cloud provider secret:\nkubectl apply -f example/70-secret-cloudprovider-azure.yaml And create the SecretBinding resource to allow Gardener use that secret (example/80-secretbinding-cloudprovider-azure.yaml):\nsed -i \\ -e \u0026#39;s/# namespace: .*/ namespace: garden-dev/\u0026#39; \\ example/80-secretbinding-cloudprovider-azure.yaml kubectl apply -f example/80-secretbinding-cloudprovider-azure.yaml Check the logs in gardener-controller-manager, there should not be any problems reported.\nCreate the Shoot resource Update the fields in example/90-deprecated-shoot-azure.yaml:\n spec.cloud.region: eastus (this must match the seed cluster\u0026rsquo;s region) spec.dns.domain: This is used to specify the base domain for your api (and other in the future) endpoint(s). For example when johndoe-azure.garden-dev.your.domain.here is used as a value, then your apiserver is available at api.johndoe-azure.garden-dev.your.domain.here spec.dns.hostedZoneID: This field doesn\u0026rsquo;t exist in the example you need to add this field and place the Route53 Hosted Zone ID.  HOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) SHOOT_DOMAIN=\u0026#34;johndoe-azure.garden-dev.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; KUBE_LEGO_EMAIL=$(git config user.email) sed -i \\ -e \u0026#34;s/region: westeurope/region: eastus/\u0026#34; \\ -e \u0026#34;s/domain: johndoe-azure.garden-dev.example.com/domain: $SHOOT_DOMAIN/\u0026#34; \\ -e \u0026#34;/domain:/a\\ \\ \\ \\ hostedZoneID: $HOSTED_ZONE_ID\u0026#34; \\ -e \u0026#34;s/email: john.doe@example.com/email: $KUBE_LEGO_EMAIL/\u0026#34; \\ example/90-deprecated-shoot-azure.yaml And let\u0026rsquo;s create the Shoot resource:\nkubectl apply -f example/90-deprecated-shoot-azure.yaml After creating the Shoot resource, gardener-controller-manager will pick it up and start provisioning the Shoot cluster.\n$ kubectl get -f example/90-deprecated-shoot-azure.yaml NAME CLOUDPROFILE VERSION SEED DOMAIN OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE johndoe-azure azure 1.12.3 azure johndoe-azure.garden-dev.your.domain.here Processing 15 \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; 16s Follow the logs in your console with gardener-controller-manager, starting like below you\u0026rsquo;ll see plenty of Waiting and Executing, etc. logs and many tasks will keep repeating:\ntime=\u0026#34;2018-06-09T07:35:45Z\u0026#34; level=info msg=\u0026#34;[SHOOT RECONCILE] garden-dev/johndoe-azure\u0026#34; time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Starting flow Shoot cluster creation\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).botanist.Shoot.Components.DNS.External{Provider/Entry}.Deploy\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployNamespace\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployKubeAPIServerService\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployBackupNamespaceFromShoot\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:51Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:51Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).MoveBackupTerraformResources\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).WaitUntilKubeAPIServerServiceIsReady\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:56Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:57Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:57Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:01Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:02Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:02Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure ... At this stage you should be waiting for a while until the Shoot cluster is provisioned and initial resources are deployed.\nDuring the provisioning you can also check output of these commands to have a better understanding about what\u0026rsquo;s going on in the seed cluster:\n$ gardenctl ls shoots projects: - project: garden-dev shoots: - johndoe-azure $ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: johndoe-azure health: Unknown status: lastOperation: description: Executing DeployKubeAddonManager, ReconcileMachines. lastUpdateTime: 2018-06-09 08:40:20 +0100 IST progress: 74 state: Processing type: Create $ kubectl -n garden-dev get shoot johndoe-azure NAMESPACE NAME SEED DOMAIN VERSION CONTROL NODES SYSTEM LATEST garden-dev johndoe-azure azure johndoe-azure.garden-dev.your.domain.here 1.10.1 True True True Succeeded $ kubectl -n garden-dev describe shoot johndoe-azure ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Reconciling 1h gardener-controller-manager [BrXWiztO] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [rBFsfwU5] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [2HAbm45D] Reconciling Shoot cluster state Normal Reconciling 48m gardener-controller-manager [S1QA0ksz] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [lvcSKy1Q] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [MddMyk8W] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [XDAAWABd] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [6HYH9Psz] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [rhL38ym4] Reconciling Shoot cluster state Normal Reconciling 35m gardener-controller-manager [BOt4Nvso] Reconciling Shoot cluster state Normal Reconciling 35m gardener-controller-manager [JPtmXmxD] Reconciling Shoot cluster state Normal Reconciling 34m gardener-controller-manager [ldHsVA6G] Reconciling Shoot cluster state Normal Reconciled 31m gardener-controller-manager [ldHsVA6G] Reconciled Shoot cluster state Normal Reconciling 26m gardener-controller-manager [yBh2IBOF] Reconciling Shoot cluster state Normal Reconciled 24m gardener-controller-manager [yBh2IBOF] Reconciled Shoot cluster state Normal Reconciling 16m gardener-controller-manager [bqmFtHUA] Reconciling Shoot cluster state Normal Reconciled 14m gardener-controller-manager [bqmFtHUA] Reconciled Shoot cluster state Normal Reconciling 6m gardener-controller-manager [7QgHE5CH] Reconciling Shoot cluster state Normal Reconciled 3m gardener-controller-manager [7QgHE5CH] Reconciled Shoot cluster state Check Shoot cluster:\n$ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ gardenctl target project garden-dev $ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ gardenctl kubectl cluster-info Kubernetes master is running at https://api.johndoe-azure.garden-dev.your.domain.here CoreDNS is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubernetes-dashboard is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. Cluster Resources After Shoot is Created After the Shoot has been created the summary of the resources in the AKS cluster handled by Gardener will be something like this:\nnon-namespaced resources CloudProfile: azure Project: dev Namespace: garden-dev Seed: azure # cloud.profile:azure, cloud.region:eastus, secretRef.name:seed-azure, secretRef.namespace: garden Namespace: garden Secret: seed-azure # aks credentials, kubeconfig # No other resources with any kind handled by Gardener # Gardener components as well lives in this namespace Namespace: garden-dev # maps to \u0026#34;project:dev\u0026#34; in Gardener Secret: core-azure # credentials for aks + aws (for route53) SecretBinding: core-azure # secretRef.name:core-azure Shoot: johndoe-azure # seed:azure, secretBindingRef.name:core-azure Namespace: shoot--dev--johndoe-azure # These are automatically created once Shoot resource is created AzureMachineClass: shoot--dev--johndoe-azure-cpu-worker-8506a MachineDeployment: shoot--dev--johndoe-azure-cpu-worker MachineSet: shoot--dev--johndoe-azure-cpu-worker-849bbbf75 Machine: shoot--dev--johndoe-azure-cpu-worker-849bbbf75-b42vh BackupInfra: shoot--dev--johndoe-azure--c1b3b # seed:azure, shootUID: shoot.status.UID. # Many other resources created as part of shoot cluster, # but only above ones are handled by Gardener Namespace: backup--shoot--dev--johndoe-azure--c1b3b # Secrets and configMap having info related to backup infrastructure # are created by Gardener. Troubleshooting Shoot Creation Issues For any issue happening during Shoot provisioning, you can consult the gardener-controller-manager logs, or the state in the shoot resource, gardenctl also provides a command to check Shoot cluster states:\n# check gardener-controller-manager logs kubectl -n garden logs -f deployment/gardener-controller-manager # kubectl describe can provide you a human readable output of # same information in below gardenctl command. kubectl -n garden-dev describe shoot johndoe-azure # also try cheking the machine-controller-manager logs of the shoot kubectl logs -n shoot--dev--johndoe-azure deployment/machine-controller-manager With gardenctl:\n$ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: health: Ready status: johndoe-azure lastError: \u0026#34;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: \u0026#39;(*Botanist).Shoot.Components.DNS.External{Provider/Entry}.Destroy\u0026#39; returned \u0026#39;Terraform execution ... lastOperation: description: \u0026#34;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: \u0026#39;(*Botanist).Shoot.Components.DNS.External{Provider/Entry}.Destroy\u0026#39; returned \u0026#39;Terraform ... lastUpdateTime: 2018-06-03 09:48:00 +0100 IST progress: 100 state: Failed type: Reconcile Access Shoot cluster The gardenctl tool provides a convenient wrapper to operate on both cluster and cloud providers, here are some commands you can run\n# select target shoot cluster gardenctl ls gardens gardenctl target garden dev gardenctl ls projects gardenctl target shoot johndoe-azure # issue Azure client (az) commands on target shoot gardenctl az aks list # issue kubectl commands on target shoot gardenctl kubectl -- version --short # \u0026#39;--\u0026#39; is required if you want to # pass any args starting with \u0026#39;-\u0026#39; # open prometheus, alertmanager, grafana without having to find # the user/pass for each gardenctl show prometheus gardenctl show grafana gardenctl show alertmanager Easiest way to obtain kubeconfig of the shoot cluster:\n$ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ export KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ # From now on your local kubectl will be operating on target shoot $ kubectl cluster-info # will show your shoot cluster info $ unset KUBECONFIG # reset to your default kubectl The shoot cluster\u0026rsquo;s kubeconfig is being kept in a secret in the project namespace:\nkubectl -n shoot--dev--johndoe-azure get secret kubecfg -o jsonpath=\u0026#39;{.data.kubeconfig}\u0026#39; | base64 -D \u0026gt; /tmp/johndoe-azure-kubeconfig.yaml export KUBECONFIG=/tmp/johndoe-azure-kubeconfig.yaml Delete Shoot cluster Deleting a Shoot cluster is not straight forward, and this is to protect users from undesired/accidental cluster deletion. One has to place some special annotations to get a Shoot cluster removed. We use the hack/usage/delete script for this purpose.\nPlease refer to Creating / Deleting a Shoot cluster document for more details.\nhack/delete shoot johndoe-azure garden-dev "},{"uri":"https://gardener.cloud/v1.13.2/contribute/10_code/30_deploy_seed_into_aks/","title":"Deploy into AKS","tags":[],"description":"","content":"Deploying the previous Gardener versions and a Seed into an AKS cluster This document demonstrates how to install Gardener into an existing AKS cluster. We\u0026rsquo;ll use a single cluster to host both Gardener and a Seed to the same cluster for the sake of simplicity .\nPlease note that this document is to provide you an example installation and is not to be used in a production environment since there are some certificates hardcoded, non-HA and non-TLS-enabled etcd setup.\nHigh Level Overview In this example we\u0026rsquo;ll follow these steps to create a Seed cluster on AKS:\n Deploying the Gardener and a Seed into an AKS cluster High Level Overview Prerequisites  AWS credentials for Route 53 Hosted Zone Deploy AKS cluster  Initialize Helm on the Cluster Deploy stable/nginx-ingress chart to AKS Create wildcard DNS record for the ingress   Create Azure Service Principle to get Azure credentials Install gardenctl   Install Gardener  Create garden namespace Deploy etcd Deploy Gardener Helm Chart   Create a CloudProfile Define Seed cluster in Gardener  Create the Seed resource definition with its Secret   Create a Shoot cluster  Create a Project (namespace) for Shoots Create a SecretBinding and related Secret Create the Shoot resource  Cluster Resources After Shoot is Created Troubleshooting Shoot Creation Issues     Access Shoot cluster Delete Shoot cluster  Prerequisites Summary of prerequisites:\n An Azure AKS cluster with:  Helm initialized, an ingress controller deployed, a wildcard DNS record pointing the ingress, az command line client configured for Azure subscription,   An Azure service principle to provide Azure credentials to Gardener, A Route53 Hosted Zone and AWS account credentials with permissions on that Route53 Zone,  aws command line client configured for this account,   gardenctl command line client configured for the AKS cluster\u0026rsquo;s kubeconfig  Note: Gardener doesn\u0026rsquo;t have support for Azure DNS yet (see #494). So, we use a Route53 Hosted Zone even if we are deploying on Azure.\nAWS credentials for Route 53 Hosted Zone You need to provide credentials for AWS with permission to access Route53 Hosted Zone. In this example we\u0026rsquo;ll assume your domain for the Hosted Zone is .your.domain.here.\nHOSTED_ZONE_ID= # place your AWS Route53 hostedZoneID here Create an AWS user, define policy to allow permission for the Hosted Zone and note the hostedZoneID, accessKeyID and secretAccessKey for later use.\nDeploy AKS cluster Here you can find a summary for creating an AKS cluster, if you already have one, skip this step.\naz group create --name garden-1 --location eastus az aks create --resource-group garden-1 --name garden-1 \\ --kubernetes-version 1.11.5 \\ --node-count 2 --node-vm-size Standard_DS4_v2 \\ --generate-ssh-keys az aks get-credentials --resource-group garden-1 --name garden-1 --admin Initialize Helm on the Cluster Since RBAC is enabled by default we need to deploy helm with an RBAC config.\nkubectl apply -f https://raw.githubusercontent.com/Azure/helm-charts/master/docs/prerequisities/helm-rbac-config.yaml helm init --service-account tiller Deploy stable/nginx-ingress chart to AKS At the moment the Ingress resources created by the Gardener are expecting the nginx-ingress style annotations to work.\nhelm upgrade --install \\ --namespace kube-system \\ nginx-ingress stable/nginx-ingress Create wildcard DNS record for the ingress You need to pick a wildcard subdomain matching your Route53 Hosted Zone here. This ingress wildcard record is supposed to be part of the Seed cluster rather than Gardener cluster, in our example we\u0026rsquo;ll use *.seed-1.your.domain.here.\nAssuming you have the AWS cli for your Route53 Hosted Zone is configured on your local, here we\u0026rsquo;ll create the wildcard DNS record using the awless. You can also use the AWS console or any other tool of your choice to create the wildcard record:\nHOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) INGRESS_DOMAIN=\u0026#34;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; # Get LB IP address from `kubectl -n kube-system get svc shared-ingress-nginx-ingress-controller` LB_IP=$(kubectl -n kube-system get svc nginx-ingress-controller --template \u0026#39;{{(index .status.loadBalancer.ingress 0).ip}}\u0026#39;) awless create record \\ zone=$HOSTED_ZONE_ID \\ name=\u0026#34;*.$INGRESS_DOMAIN\u0026#34; \\ value=$LB_IP \\ type=A \\ ttl=300 Create Azure Service Principle to get Azure credentials We need client_id and client_secret to allow Gardener to reach Azure services, we can generate a pair by creating a Service Principle on Azure:\n$ az ad sp create-for-rbac --role=\u0026#34;Contributor\u0026#34; Retrying role assignment creation: 1/36 { \u0026#34;appId\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34;, #az_client_id \u0026#34;displayName\u0026#34;: \u0026#34;azure-cli-2018-05-23-16-15-49\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;http://azure-cli-2018-05-23-16-15-49\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34;, #az_client_secret \u0026#34;tenant\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34; #az_tenant_id } Let\u0026rsquo;s define some env variables for later use\nCLIENT_ID= # place your Azure Service Principal appId CLIENT_SECRET= # place your Azure Service Principal password here Install gardenctl In this example we\u0026rsquo;ll be using gardenctl to interact with Gardener. You can install gardenctl following instruction in its repo: https://github.com/gardener/gardenctl\nHere is a sample configuration for gardenctl:\n$ cat ~/.garden/config gardenClusters: - name: dev kubeConfig: ~/.kube/config Install Gardener Create garden namespace This is where we deploy Gardener components.\nkubectl apply -f example/00-namespace-garden.yaml Deploy etcd Since Gardener is an extension API Server, it can share the etcd backing native Kubernetes cluster\u0026rsquo;s API Server, and hence explicit etcd installation is optional. But in our case we have no access to the control plane components of the AKS cluster and we have to deploy our own etcd ourselves for Gardener. Lets deploy an etcd using the gardener/etcd-backup-restore project, which is also used by the Gardener for Shoot control plane.\n# pull the etcd-backup-restore git clone https://github.com/gardener/etcd-backup-restore.git # deploy etcd helm upgrade --install \\ --namespace garden \\ etcd etcd-backup-restore/chart \\ --set tls= Note: This etcd installation doesn\u0026rsquo;t provide HA. But etcd will be auto recovered by the Deployment. This could be sufficient for some deployments but may not be suitable for production usage. Also note that this etcd is not deployed with TLS enabled and doesn\u0026rsquo;t use certificates for authentication.\nCheck etcd pod\u0026rsquo;s health, it should have READY:2/2 and STATUS:Running:\n$ kubectl -n garden get pods NAME READY STATUS RESTARTS AGE etcd-for-test-0 2/2 Running 0 1m Deploy Gardener Helm Chart Check (current releases)[https://github.com/gardener/gardener/releases] and pick a suitable one to install.\nGARDENER_RELEASE=0.17.1 gardener-controller-manager will need to maintain some DNS records for Seed. So, you need to provide Route53 credentials in the values.yaml file:\n global.controller.internalDomain.hostedZoneID global.controller.internalDomain.domain: Here pick a subdomain for your Gardener to maintain DNS records for your Shoot clusters. This domain has to be within your Route53 Hosted Zone. e.g. garden-1.your.domain.here global.controller.internalDomain.credentials global.controller.internalDomain.secretAccessKey  HOSTED_ZONE_DOMAIN=$( aws route53 get-hosted-zone \\ --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} \\ --query \u0026#39;HostedZone.Name\u0026#39; \\ --output text) HOSTED_ZONE_DOMAIN=${HOSTED_ZONE_DOMAIN%%.} GARDENER_DOMAIN=\u0026#34;garden-1.${HOSTED_ZONE_DOMAIN}\u0026#34; ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) cat \u0026lt;\u0026lt;EOF \u0026gt; gardener-values.yaml global: apiserver: image: tag: ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} etcd: servers: http://etcd-for-test-client:2379 useSidecar: false controller: image: tag: ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} internalDomain: provider: aws-route53 hostedZoneID: ${HOSTED_ZONE_ID} domain: ${HOSTED_ZONE_DOMAIN} credentials: AWS_ACCESS_KEY_ID: ${ACCESS_KEY_ID} AWS_SECRET_ACCESS_KEY: ${SECRET_ACCESS_KEY} EOF After creating the gardener-values.yaml file, since chart definition in master branch can have breaking changes after the release, checkout the gardener tag for that release, and run:\ngit checkout ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} helm upgrade --install \\ --namespace garden \\ garden charts/gardener \\ -f charts/gardener/local-values.yaml \\ -f gardener-values.yaml Validate the Gardener is deployed:\nhelm status garden # Wait for `STATUS: DEPLOYED` kubectl -n garden get deploy,pod -l app=gardener # Better if you leave two terminals open in for below commands, and # keep an eye on whats going on behind the scenes as you create/delete # Gardener specific resources (Seed, CloudProfile, SecretBinding, Shoot). kubectl -n garden logs -f deployment/gardener-apiserver # confirm no issues kubectl -n garden logs -f deployment/gardener-controller-manager # confirm no issues, except some \u0026#34;Failed to list *v1beta1...\u0026#34; messages Note: This is not meant to be used in production. You may not want to use apiserver.insecureSkipTLSVerify=true, the hardcoded apiserver certificates, and insecure (non-tls enabled) etcd. But for the sake of keeping this example simple you can just keep those values as they are.\nCreate a CloudProfile We need to create a CloudProfile to be referred from the Shoot (example/30-cloudprofile-azure.yaml):\nkubectl apply -f example/30-cloudprofile-azure.yaml Validate that CloudProfile is created:\nkubectl describe -f example/30-cloudprofile-azure.yaml Define Seed cluster in Gardener In our setup we\u0026rsquo;ll use the cluster for Gardener also as a Seed, this saves us from creating a new Kubernetes cluster. But you can also create an explicit cluster for the Seed. Seed cluster can also be placed into any other cloud provider or on prem. But keep in mind that below steps may differ if you use a different cluster for seed.\nCurrently, a Seed cluster is just a Kubeconfig for the Gardener. The seed cluster could have been created by any tool, Gardener only cares about having a valid Kubeconfig to talk to its API.\nCreate the Seed resource definition with its Secret Lets start with the required seed secret first. Here we need to provide it\u0026rsquo;s cloud provider credentials and kubeconfig in the seed secret. Update example/40-secret-seed-azure.yaml and place the secrets for your environment:\n data.subscriptionID: you can learn this one with az account show data.tenantID: from az ad sp create-for-rbac output as you can see above data.clientID: from az ad sp create-for-rbac output as you can see above data.clientSecret: from az ad sp create-for-rbac output as you can see above data.kubeconfig: you can get this one with az aks get-credentials --resource-group garden-1 --name garden-1 -f - | base64)  Note: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r \u0026#39;.[] | select(.isDefault == true) | .id\u0026#39;) TENANT_ID=$(az account show -o tsv --query \u0026#39;tenantId\u0026#39;) KUBECONFIG_FOR_SEED_CLUSTER=$(az aks get-credentials --resource-group garden-1 --name garden-1 -f -) sed -i \\ -e \u0026#34;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-tenant)@$(echo \u0026#34;$TENANT_ID\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-client)@$(echo \u0026#34;${CLIENT_ID:?\u0026#34;CLIENT_ID is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(client-secret)@$(echo \u0026#34;${CLIENT_SECRET:?\u0026#34;CLIENT_SECRET is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(kubeconfig-for-seed-cluster)@$(echo \u0026#34;$KUBECONFIG_FOR_SEED_CLUSTER\u0026#34; | base64 -w 0)@\u0026#34; \\ example/40-secret-seed-azure.yaml After updating the fields, create the Seed secret:\nkubectl apply -f example/40-secret-seed-azure.yaml Before creating Seed, we need to update the example/50-seed-azure.yaml file and update:\n spec.networks: IP ranges used in your AKS cluster. spec.ingressDomain: Place here the wildcard domain you have for the ingress controller (we created this record in prerequisites). Gardener doesn\u0026rsquo;t create this DNS records but assumes its created ahead of time, Seed clusters are not provisioned by Gardener. spec.cloud.region: eastus (the region of the existing AKS cluster)  HOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) INGRESS_DOMAIN=\u0026#34;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; # discover AKS CIDRs NODE_CIDR=$(az network vnet list -g MC_garden-1_garden-1_eastus -o json | jq -r \u0026#39;.[] | .subnets[] | .addressPrefix\u0026#39;) POD_CIDR=$(kubectl -n kube-system get daemonset/kube-proxy -o yaml | grep cluster-cidr= | grep -v annotations | cut -d = -f2) SERVICE_CIDR=10.0.0.0/16 # This one is hardcoded for now, not easy to discover sed -i \\ -e \u0026#34;s/ingressDomain: dev.azure.seed.example.com/ingressDomain: $INGRESS_DOMAIN/\u0026#34; \\ -e \u0026#34;s/region: westeurope/region: eastus/\u0026#34; \\ -e \u0026#34;s@nodes: 10.240.0.0/16@nodes: $NODE_CIDR@\u0026#34; \\ -e \u0026#34;s@pods: 10.241.128.0/17@pods: $POD_CIDR@\u0026#34; \\ -e \u0026#34;s@services: 10.241.0.0/17@services: $SERVICE_CIDR@\u0026#34; \\ example/50-seed-azure.yaml Now we are ready to create the seed:\nkubectl apply -f example/50-seed-azure.yaml Check the logs in gardener-controller-manager and also wait for seed to be Ready: True. This means gardener-controller-manager is able to reach the Seed cluster with the credentials you provide.\n$ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ kubectl get seed azure NAME CLOUDPROFILE REGION INGRESS DOMAIN AVAILABLE AGE azure azure eastus seed-1.your.domain.here True 1m $ gardenctl ls seeds seeds: - seed: azure If something goes wrong verify that you provided right credentials, and base64 encoded strings of those in the secret. Also check the status field in the Seed resource and gardener-controller-manager logs:\n$ kubectl get seed azure -o json | jq .status { \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2018-05-31T14:56:49Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;all checks passed\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Passed\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Available\u0026#34; } ] } Create a Shoot cluster Create a Project (namespace) for Shoots In this step we create a namespace in Gardener cluster to keep Shoot resource definitions. A project in Gardener terminology is simply a namespace that holds group of Shoots, during this example we\u0026rsquo;ll deploy a single Shoot. (Mind the extra labels defined in example/00-namespace-garden-dev.yaml).\nkubectl apply -f example/05-project-dev.yaml You can check the projects via gardenctl:\n$ gardenctl target garden dev $ kubectl get project dev NAME NAMESPACE STATUS OWNER CREATOR AGE dev garden-dev Ready john.doe@example.com client 1m $ kubectl get ns garden-dev NAME STATUS AGE garden-dev Active 1m $ gardenctl ls projects projects: - project: garden-dev Create a SecretBinding and related Secret We\u0026rsquo;ll use same Azure credentials with example/40-secret-seed-azure.yaml, this is due to the fact that we use the same Azure Subscription for the Shoot and Seed clusters. Differently from the Seed secret, in this one we don\u0026rsquo;t need to provide kubeconfig since the Shoot cluster will be provisioned by Gardener, and we need to provide credentials for Route53 DNS records management.\nUpdate example/70-secret-cloudprovider-azure.yaml and place the secrets for your environment:\n data.subscriptionID: you can learn this one with az account show data.tenantID: from az ad sp create-for-rbac output as you can see above data.clientID: from az ad sp create-for-rbac output as you can see above data.clientSecret: from az ad sp create-for-rbac output as you can see above data.accessKeyID: You need to add this field for Route53 records to be updated. data.secretAccessKey: You need to add this field for Route53 records to be updated.  Note: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r \u0026#39;.[] | select(.isDefault == true) | .id\u0026#39;) TENANT_ID=$(az account show -o tsv --query \u0026#39;tenantId\u0026#39;) ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) sed -i \\ -e \u0026#34;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-tenant)@$(echo \u0026#34;$TENANT_ID\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-client)@$(echo \u0026#34;${CLIENT_ID:?\u0026#34;CLIENT_ID is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(client-secret)@$(echo \u0026#34;${CLIENT_SECRET:?\u0026#34;CLIENT_SECRET is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;\\$a\\ \\ accessKeyID: $(echo $ACCESS_KEY_ID | tr -d \u0026#39;\\n\u0026#39; | base64 )\u0026#34; \\ -e \u0026#34;\\$a\\ \\ secretAccessKey: $(echo $SECRET_ACCESS_KEY | tr -d \u0026#39;\\n\u0026#39; | base64 )\u0026#34; \\ example/70-secret-cloudprovider-azure.yaml After updating the fields, create the cloud provider secret:\nkubectl apply -f example/70-secret-cloudprovider-azure.yaml And create the SecretBinding resource to allow Gardener use that secret (example/80-secretbinding-cloudprovider-azure.yaml):\nsed -i \\ -e \u0026#39;s/# namespace: .*/ namespace: garden-dev/\u0026#39; \\ example/80-secretbinding-cloudprovider-azure.yaml kubectl apply -f example/80-secretbinding-cloudprovider-azure.yaml Check the logs in gardener-controller-manager, there should not be any problems reported.\nCreate the Shoot resource Update the fields in example/90-deprecated-shoot-azure.yaml:\n spec.cloud.region: eastus (this must match the seed cluster\u0026rsquo;s region) spec.dns.domain: This is used to specify the base domain for your api (and other in the future) endpoint(s). For example when johndoe-azure.garden-dev.your.domain.here is used as a value, then your apiserver is available at api.johndoe-azure.garden-dev.your.domain.here spec.dns.hostedZoneID: This field doesn\u0026rsquo;t exist in the example you need to add this field and place the Route53 Hosted Zone ID.  HOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) SHOOT_DOMAIN=\u0026#34;johndoe-azure.garden-dev.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; KUBE_LEGO_EMAIL=$(git config user.email) sed -i \\ -e \u0026#34;s/region: westeurope/region: eastus/\u0026#34; \\ -e \u0026#34;s/domain: johndoe-azure.garden-dev.example.com/domain: $SHOOT_DOMAIN/\u0026#34; \\ -e \u0026#34;/domain:/a\\ \\ \\ \\ hostedZoneID: $HOSTED_ZONE_ID\u0026#34; \\ -e \u0026#34;s/email: john.doe@example.com/email: $KUBE_LEGO_EMAIL/\u0026#34; \\ example/90-deprecated-shoot-azure.yaml And let\u0026rsquo;s create the Shoot resource:\nkubectl apply -f example/90-deprecated-shoot-azure.yaml After creating the Shoot resource, gardener-controller-manager will pick it up and start provisioning the Shoot cluster.\n$ kubectl get -f example/90-deprecated-shoot-azure.yaml NAME CLOUDPROFILE VERSION SEED DOMAIN OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE johndoe-azure azure 1.12.3 azure johndoe-azure.garden-dev.your.domain.here Processing 15 \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; 16s Follow the logs in your console with gardener-controller-manager, starting like below you\u0026rsquo;ll see plenty of Waiting and Executing, etc. logs and many tasks will keep repeating:\ntime=\u0026#34;2018-06-09T07:35:45Z\u0026#34; level=info msg=\u0026#34;[SHOOT RECONCILE] garden-dev/johndoe-azure\u0026#34; time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Starting flow Shoot cluster creation\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).botanist.Shoot.Components.DNS.External{Provider/Entry}.Deploy\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployNamespace\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployKubeAPIServerService\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployBackupNamespaceFromShoot\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:51Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:51Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).MoveBackupTerraformResources\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).WaitUntilKubeAPIServerServiceIsReady\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:56Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:57Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:57Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:01Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:02Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:02Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure ... At this stage you should be waiting for a while until the Shoot cluster is provisioned and initial resources are deployed.\nDuring the provisioning you can also check output of these commands to have a better understanding about what\u0026rsquo;s going on in the seed cluster:\n$ gardenctl ls shoots projects: - project: garden-dev shoots: - johndoe-azure $ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: johndoe-azure health: Unknown status: lastOperation: description: Executing DeployKubeAddonManager, ReconcileMachines. lastUpdateTime: 2018-06-09 08:40:20 +0100 IST progress: 74 state: Processing type: Create $ kubectl -n garden-dev get shoot johndoe-azure NAMESPACE NAME SEED DOMAIN VERSION CONTROL NODES SYSTEM LATEST garden-dev johndoe-azure azure johndoe-azure.garden-dev.your.domain.here 1.10.1 True True True Succeeded $ kubectl -n garden-dev describe shoot johndoe-azure ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Reconciling 1h gardener-controller-manager [BrXWiztO] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [rBFsfwU5] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [2HAbm45D] Reconciling Shoot cluster state Normal Reconciling 48m gardener-controller-manager [S1QA0ksz] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [lvcSKy1Q] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [MddMyk8W] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [XDAAWABd] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [6HYH9Psz] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [rhL38ym4] Reconciling Shoot cluster state Normal Reconciling 35m gardener-controller-manager [BOt4Nvso] Reconciling Shoot cluster state Normal Reconciling 35m gardener-controller-manager [JPtmXmxD] Reconciling Shoot cluster state Normal Reconciling 34m gardener-controller-manager [ldHsVA6G] Reconciling Shoot cluster state Normal Reconciled 31m gardener-controller-manager [ldHsVA6G] Reconciled Shoot cluster state Normal Reconciling 26m gardener-controller-manager [yBh2IBOF] Reconciling Shoot cluster state Normal Reconciled 24m gardener-controller-manager [yBh2IBOF] Reconciled Shoot cluster state Normal Reconciling 16m gardener-controller-manager [bqmFtHUA] Reconciling Shoot cluster state Normal Reconciled 14m gardener-controller-manager [bqmFtHUA] Reconciled Shoot cluster state Normal Reconciling 6m gardener-controller-manager [7QgHE5CH] Reconciling Shoot cluster state Normal Reconciled 3m gardener-controller-manager [7QgHE5CH] Reconciled Shoot cluster state Check Shoot cluster:\n$ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ gardenctl target project garden-dev $ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ gardenctl kubectl cluster-info Kubernetes master is running at https://api.johndoe-azure.garden-dev.your.domain.here CoreDNS is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubernetes-dashboard is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. Cluster Resources After Shoot is Created After the Shoot has been created the summary of the resources in the AKS cluster handled by Gardener will be something like this:\nnon-namespaced resources CloudProfile: azure Project: dev Namespace: garden-dev Seed: azure # cloud.profile:azure, cloud.region:eastus, secretRef.name:seed-azure, secretRef.namespace: garden Namespace: garden Secret: seed-azure # aks credentials, kubeconfig # No other resources with any kind handled by Gardener # Gardener components as well lives in this namespace Namespace: garden-dev # maps to \u0026#34;project:dev\u0026#34; in Gardener Secret: core-azure # credentials for aks + aws (for route53) SecretBinding: core-azure # secretRef.name:core-azure Shoot: johndoe-azure # seed:azure, secretBindingRef.name:core-azure Namespace: shoot--dev--johndoe-azure # These are automatically created once Shoot resource is created AzureMachineClass: shoot--dev--johndoe-azure-cpu-worker-8506a MachineDeployment: shoot--dev--johndoe-azure-cpu-worker MachineSet: shoot--dev--johndoe-azure-cpu-worker-849bbbf75 Machine: shoot--dev--johndoe-azure-cpu-worker-849bbbf75-b42vh BackupInfra: shoot--dev--johndoe-azure--c1b3b # seed:azure, shootUID: shoot.status.UID. # Many other resources created as part of shoot cluster, # but only above ones are handled by Gardener Namespace: backup--shoot--dev--johndoe-azure--c1b3b # Secrets and configMap having info related to backup infrastructure # are created by Gardener. Troubleshooting Shoot Creation Issues For any issue happening during Shoot provisioning, you can consult the gardener-controller-manager logs, or the state in the shoot resource, gardenctl also provides a command to check Shoot cluster states:\n# check gardener-controller-manager logs kubectl -n garden logs -f deployment/gardener-controller-manager # kubectl describe can provide you a human readable output of # same information in below gardenctl command. kubectl -n garden-dev describe shoot johndoe-azure # also try cheking the machine-controller-manager logs of the shoot kubectl logs -n shoot--dev--johndoe-azure deployment/machine-controller-manager With gardenctl:\n$ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: health: Ready status: johndoe-azure lastError: \u0026#34;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: \u0026#39;(*Botanist).Shoot.Components.DNS.External{Provider/Entry}.Destroy\u0026#39; returned \u0026#39;Terraform execution ... lastOperation: description: \u0026#34;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: \u0026#39;(*Botanist).Shoot.Components.DNS.External{Provider/Entry}.Destroy\u0026#39; returned \u0026#39;Terraform ... lastUpdateTime: 2018-06-03 09:48:00 +0100 IST progress: 100 state: Failed type: Reconcile Access Shoot cluster The gardenctl tool provides a convenient wrapper to operate on both cluster and cloud providers, here are some commands you can run\n# select target shoot cluster gardenctl ls gardens gardenctl target garden dev gardenctl ls projects gardenctl target shoot johndoe-azure # issue Azure client (az) commands on target shoot gardenctl az aks list # issue kubectl commands on target shoot gardenctl kubectl -- version --short # \u0026#39;--\u0026#39; is required if you want to # pass any args starting with \u0026#39;-\u0026#39; # open prometheus, alertmanager, grafana without having to find # the user/pass for each gardenctl show prometheus gardenctl show grafana gardenctl show alertmanager Easiest way to obtain kubeconfig of the shoot cluster:\n$ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ export KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ # From now on your local kubectl will be operating on target shoot $ kubectl cluster-info # will show your shoot cluster info $ unset KUBECONFIG # reset to your default kubectl The shoot cluster\u0026rsquo;s kubeconfig is being kept in a secret in the project namespace:\nkubectl -n shoot--dev--johndoe-azure get secret kubecfg -o jsonpath=\u0026#39;{.data.kubeconfig}\u0026#39; | base64 -D \u0026gt; /tmp/johndoe-azure-kubeconfig.yaml export KUBECONFIG=/tmp/johndoe-azure-kubeconfig.yaml Delete Shoot cluster Deleting a Shoot cluster is not straight forward, and this is to protect users from undesired/accidental cluster deletion. One has to place some special annotations to get a Shoot cluster removed. We use the hack/usage/delete script for this purpose.\nPlease refer to Creating / Deleting a Shoot cluster document for more details.\nhack/delete shoot johndoe-azure garden-dev "},{"uri":"https://gardener.cloud/documentation/concepts/deployment/deploy_gardenlet/","title":"Deploying Gardenlets","tags":[],"description":"","content":"Deploying Gardenlets Gardenlets act as decentral \u0026ldquo;agents\u0026rdquo; to manage shoot clusters of a seed cluster.\nTo support scaleability in an automated way, gardenlets are deployed automatically. However, you can still deploy gardenlets manually to be more flexible, for example, when shoot clusters that need to be managed by Gardener are behind a firewall. The gardenlet only requires network connectivity from the gardenlet to the Garden cluster (not the other way round), so it can be used to register Kubernetes clusters with no public endpoint.\nProcedure   First, an initial gardenlet needs to be deployed:\n Deploy it manually if you have special requirements. More information: Deploy a Gardenlet Manually Let the Gardener installer deploy it automatically otherwise. More information: Automatic Deployment of Gardenlets    To add additional seed clusters, it is recommended to use regular shoot clusters. The gardenlet automatically installs itself into these so-called shooted seed clusters.\n  "},{"uri":"https://gardener.cloud/v1.12.8/concepts/deployment/deploy_gardenlet/","title":"Deploying Gardenlets","tags":[],"description":"","content":"Deploying Gardenlets Gardenlets act as decentral \u0026ldquo;agents\u0026rdquo; to manage shoot clusters of a seed cluster.\nTo support scaleability in an automated way, gardenlets are deployed automatically. However, you can still deploy gardenlets manually to be more flexible, for example, when shoot clusters that need to be managed by Gardener are behind a firewall. The gardenlet only requires network connectivity from the gardenlet to the Garden cluster (not the other way round), so it can be used to register Kubernetes clusters with no public endpoint.\nProcedure   First, an initial gardenlet needs to be deployed:\n Deploy it manually if you have special requirements. More information: Deploy a Gardenlet Manually Let the Gardener installer deploy it automatically otherwise. More information: Automatic Deployment of Gardenlets    To add additional seed clusters, it is recommended to use regular shoot clusters. The gardenlet automatically installs itself into these so-called shooted seed clusters.\n  "},{"uri":"https://gardener.cloud/v1.13.2/concepts/deployment/deploy_gardenlet/","title":"Deploying Gardenlets","tags":[],"description":"","content":"Deploying Gardenlets Gardenlets act as decentral \u0026ldquo;agents\u0026rdquo; to manage shoot clusters of a seed cluster.\nTo support scaleability in an automated way, gardenlets are deployed automatically. However, you can still deploy gardenlets manually to be more flexible, for example, when shoot clusters that need to be managed by Gardener are behind a firewall. The gardenlet only requires network connectivity from the gardenlet to the Garden cluster (not the other way round), so it can be used to register Kubernetes clusters with no public endpoint.\nProcedure   First, an initial gardenlet needs to be deployed:\n Deploy it manually if you have special requirements. More information: Deploy a Gardenlet Manually Let the Gardener installer deploy it automatically otherwise. More information: Automatic Deployment of Gardenlets    To add additional seed clusters, it is recommended to use regular shoot clusters. The gardenlet automatically installs itself into these so-called shooted seed clusters.\n  "},{"uri":"https://gardener.cloud/documentation/concepts/deployment/setup_gardener/","title":"Deploying the Gardener into a Kubernetes cluster","tags":[],"description":"","content":"Deploying the Gardener into a Kubernetes cluster Similar to Kubernetes, Gardener consists out of control plane components (Gardener API server, Gardener controller manager, Gardener scheduler), and an agent component (Gardenlet). The control plane is deployed in the so-called garden cluster while the agent is installed into every seed cluster. Please note that it is possible to use the garden cluster as seed cluster by simply deploying the Gardenlet into it.\nWe are providing Helm charts in order to manage the various resources of the components. Please always make sure that you use the Helm chart version that matches the Gardener version you want to deploy.\nDeploying the Gardener control plane (API server, admission controller, controller manager, scheduler) The configuration values depict the various options to configure the different components. Please consult this document to get a detailed explanation of what can be configured for which component.\nAlso note that all resources and deployments need to be created in the garden namespace (not overrideable). If you enable the Gardener admission controller as part of you setup, please make sure the garden namespace is labelled with app: gardener. Otherwise, the backing service account for the admission controller Pod might not be created successfully. No action is necessary, if you deploy the garden namespace with the Gardener control plane Helm chart.\nAfter preparing your values in a separate controlplane-values.yaml file, you can run the following command against your garden cluster:\nhelm install charts/gardener/controlplane \\  --namespace garden \\  --name gardener-controlplane \\  -f gardener-values.yaml \\  --wait Deploying Gardener extensions Gardener is an extensible system that does not contain the logic for provider-specific things like DNS management, cloud infrastructures, network plugins, operating system configs, and many more.\nYou have to install extension controllers for these parts. Please consult the documentation regarding extensions to get more information.\nDeploying the Gardener agent (Gardenlet) Please refer to this document on how to deploy a Gardenlet.\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/deployment/setup_gardener/","title":"Deploying the Gardener into a Kubernetes cluster","tags":[],"description":"","content":"Deploying the Gardener into a Kubernetes cluster Similar to Kubernetes, Gardener consists out of control plane components (Gardener API server, Gardener controller manager, Gardener scheduler), and an agent component (Gardenlet). The control plane is deployed in the so-called garden cluster while the agent is installed into every seed cluster. Please note that it is possible to use the garden cluster as seed cluster by simply deploying the Gardenlet into it.\nWe are providing Helm charts in order to manage the various resources of the components. Please always make sure that you use the Helm chart version that matches the Gardener version you want to deploy.\nDeploying the Gardener control plane (API server, admission controller, controller manager, scheduler) The configuration values depict the various options to configure the different components. Please consult this document to get a detailed explanation of what can be configured for which component.\nAlso note that all resources and deployments need to be created in the garden namespace (not overrideable). If you enable the Gardener admission controller as part of you setup, please make sure the garden namespace is labelled with app: gardener. Otherwise, the backing service account for the admission controller Pod might not be created successfully. No action is necessary, if you deploy the garden namespace with the Gardener control plane Helm chart.\nAfter preparing your values in a separate controlplane-values.yaml file, you can run the following command against your garden cluster:\nhelm install charts/gardener/controlplane \\  --namespace garden \\  --name gardener-controlplane \\  -f gardener-values.yaml \\  --wait Deploying Gardener extensions Gardener is an extensible system that does not contain the logic for provider-specific things like DNS management, cloud infrastructures, network plugins, operating system configs, and many more.\nYou have to install extension controllers for these parts. Please consult the documentation regarding extensions to get more information.\nDeploying the Gardener agent (Gardenlet) Please refer to this document on how to deploy a Gardenlet.\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/deployment/setup_gardener/","title":"Deploying the Gardener into a Kubernetes cluster","tags":[],"description":"","content":"Deploying the Gardener into a Kubernetes cluster Similar to Kubernetes, Gardener consists out of control plane components (Gardener API server, Gardener controller manager, Gardener scheduler), and an agent component (Gardenlet). The control plane is deployed in the so-called garden cluster while the agent is installed into every seed cluster. Please note that it is possible to use the garden cluster as seed cluster by simply deploying the Gardenlet into it.\nWe are providing Helm charts in order to manage the various resources of the components. Please always make sure that you use the Helm chart version that matches the Gardener version you want to deploy.\nDeploying the Gardener control plane (API server, admission controller, controller manager, scheduler) The configuration values depict the various options to configure the different components. Please consult this document to get a detailed explanation of what can be configured for which component.\nAlso note that all resources and deployments need to be created in the garden namespace (not overrideable). If you enable the Gardener admission controller as part of you setup, please make sure the garden namespace is labelled with app: gardener. Otherwise, the backing service account for the admission controller Pod might not be created successfully. No action is necessary, if you deploy the garden namespace with the Gardener control plane Helm chart.\nAfter preparing your values in a separate controlplane-values.yaml file, you can run the following command against your garden cluster:\nhelm install charts/gardener/controlplane \\  --namespace garden \\  --name gardener-controlplane \\  -f gardener-values.yaml \\  --wait Deploying Gardener extensions Gardener is an extensible system that does not contain the logic for provider-specific things like DNS management, cloud infrastructures, network plugins, operating system configs, and many more.\nYou have to install extension controllers for these parts. Please consult the documentation regarding extensions to get more information.\nDeploying the Gardener agent (Gardenlet) Please refer to this document on how to deploy a Gardenlet.\n"},{"uri":"https://gardener.cloud/documentation/concepts/deployment/aks/","title":"Deploying the previous Gardener versions and a Seed into an AKS cluster","tags":[],"description":"","content":"Deploying the previous Gardener versions and a Seed into an AKS cluster This document demonstrates how to install Gardener into an existing AKS cluster. We\u0026rsquo;ll use a single cluster to host both Gardener and a Seed to the same cluster for the sake of simplicity .\nPlease note that this document is to provide you an example installation and is not to be used in a production environment since there are some certificates hardcoded, non-HA and non-TLS-enabled etcd setup.\nHigh Level Overview In this example we\u0026rsquo;ll follow these steps to create a Seed cluster on AKS:\n Deploying the Gardener and a Seed into an AKS cluster High Level Overview Prerequisites  AWS credentials for Route 53 Hosted Zone Deploy AKS cluster  Initialize Helm on the Cluster Deploy stable/nginx-ingress chart to AKS Create wildcard DNS record for the ingress   Create Azure Service Principle to get Azure credentials Install gardenctl   Install Gardener  Create garden namespace Deploy etcd Deploy Gardener Helm Chart   Create a CloudProfile Define Seed cluster in Gardener  Create the Seed resource definition with its Secret   Create a Shoot cluster  Create a Project (namespace) for Shoots Create a SecretBinding and related Secret Create the Shoot resource  Cluster Resources After Shoot is Created Troubleshooting Shoot Creation Issues     Access Shoot cluster Delete Shoot cluster  Prerequisites Summary of prerequisites:\n An Azure AKS cluster with:  Helm initialized, an ingress controller deployed, a wildcard DNS record pointing the ingress, az command line client configured for Azure subscription,   An Azure service principle to provide Azure credentials to Gardener, A Route53 Hosted Zone and AWS account credentials with permissions on that Route53 Zone,  aws command line client configured for this account,   gardenctl command line client configured for the AKS cluster\u0026rsquo;s kubeconfig  Note: Gardener doesn\u0026rsquo;t have support for Azure DNS yet (see #494). So, we use a Route53 Hosted Zone even if we are deploying on Azure.\nAWS credentials for Route 53 Hosted Zone You need to provide credentials for AWS with permission to access Route53 Hosted Zone. In this example we\u0026rsquo;ll assume your domain for the Hosted Zone is .your.domain.here.\nHOSTED_ZONE_ID= # place your AWS Route53 hostedZoneID here Create an AWS user, define policy to allow permission for the Hosted Zone and note the hostedZoneID, accessKeyID and secretAccessKey for later use.\nDeploy AKS cluster Here you can find a summary for creating an AKS cluster, if you already have one, skip this step.\naz group create --name garden-1 --location eastus az aks create --resource-group garden-1 --name garden-1 \\ --kubernetes-version 1.11.5 \\ --node-count 2 --node-vm-size Standard_DS4_v2 \\ --generate-ssh-keys az aks get-credentials --resource-group garden-1 --name garden-1 --admin Initialize Helm on the Cluster Since RBAC is enabled by default we need to deploy helm with an RBAC config.\nkubectl apply -f https://raw.githubusercontent.com/Azure/helm-charts/master/docs/prerequisities/helm-rbac-config.yaml helm init --service-account tiller Deploy stable/nginx-ingress chart to AKS At the moment the Ingress resources created by the Gardener are expecting the nginx-ingress style annotations to work.\nhelm upgrade --install \\ --namespace kube-system \\ nginx-ingress stable/nginx-ingress Create wildcard DNS record for the ingress You need to pick a wildcard subdomain matching your Route53 Hosted Zone here. This ingress wildcard record is supposed to be part of the Seed cluster rather than Gardener cluster, in our example we\u0026rsquo;ll use *.seed-1.your.domain.here.\nAssuming you have the AWS cli for your Route53 Hosted Zone is configured on your local, here we\u0026rsquo;ll create the wildcard DNS record using the awless. You can also use the AWS console or any other tool of your choice to create the wildcard record:\nHOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) INGRESS_DOMAIN=\u0026#34;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; # Get LB IP address from `kubectl -n kube-system get svc shared-ingress-nginx-ingress-controller` LB_IP=$(kubectl -n kube-system get svc nginx-ingress-controller --template \u0026#39;{{(index .status.loadBalancer.ingress 0).ip}}\u0026#39;) awless create record \\ zone=$HOSTED_ZONE_ID \\ name=\u0026#34;*.$INGRESS_DOMAIN\u0026#34; \\ value=$LB_IP \\ type=A \\ ttl=300 Create Azure Service Principle to get Azure credentials We need client_id and client_secret to allow Gardener to reach Azure services, we can generate a pair by creating a Service Principle on Azure:\n$ az ad sp create-for-rbac --role=\u0026#34;Contributor\u0026#34; Retrying role assignment creation: 1/36 { \u0026#34;appId\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34;, #az_client_id \u0026#34;displayName\u0026#34;: \u0026#34;azure-cli-2018-05-23-16-15-49\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;http://azure-cli-2018-05-23-16-15-49\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34;, #az_client_secret \u0026#34;tenant\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34; #az_tenant_id } Let\u0026rsquo;s define some env variables for later use\nCLIENT_ID= # place your Azure Service Principal appId CLIENT_SECRET= # place your Azure Service Principal password here Install gardenctl In this example we\u0026rsquo;ll be using gardenctl to interact with Gardener. You can install gardenctl following instruction in its repo: https://github.com/gardener/gardenctl\nHere is a sample configuration for gardenctl:\n$ cat ~/.garden/config gardenClusters: - name: dev kubeConfig: ~/.kube/config Install Gardener Create garden namespace This is where we deploy Gardener components.\nkubectl apply -f example/00-namespace-garden.yaml Deploy etcd Since Gardener is an extension API Server, it can share the etcd backing native Kubernetes cluster\u0026rsquo;s API Server, and hence explicit etcd installation is optional. But in our case we have no access to the control plane components of the AKS cluster and we have to deploy our own etcd ourselves for Gardener. Lets deploy an etcd using the gardener/etcd-backup-restore project, which is also used by the Gardener for Shoot control plane.\n# pull the etcd-backup-restore git clone https://github.com/gardener/etcd-backup-restore.git # deploy etcd helm upgrade --install \\ --namespace garden \\ etcd etcd-backup-restore/chart \\ --set tls= Note: This etcd installation doesn\u0026rsquo;t provide HA. But etcd will be auto recovered by the Deployment. This could be sufficient for some deployments but may not be suitable for production usage. Also note that this etcd is not deployed with TLS enabled and doesn\u0026rsquo;t use certificates for authentication.\nCheck etcd pod\u0026rsquo;s health, it should have READY:2/2 and STATUS:Running:\n$ kubectl -n garden get pods NAME READY STATUS RESTARTS AGE etcd-for-test-0 2/2 Running 0 1m Deploy Gardener Helm Chart Check current releases and pick a suitable one to install.\nGARDENER_RELEASE=0.17.1 gardener-controller-manager will need to maintain some DNS records for Seed. So, you need to provide Route53 credentials in the values.yaml file:\n global.controller.internalDomain.hostedZoneID global.controller.internalDomain.domain: Here pick a subdomain for your Gardener to maintain DNS records for your Shoot clusters. This domain has to be within your Route53 Hosted Zone. e.g. garden-1.your.domain.here global.controller.internalDomain.credentials global.controller.internalDomain.secretAccessKey  HOSTED_ZONE_DOMAIN=$( aws route53 get-hosted-zone \\ --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} \\ --query \u0026#39;HostedZone.Name\u0026#39; \\ --output text) HOSTED_ZONE_DOMAIN=${HOSTED_ZONE_DOMAIN%%.} GARDENER_DOMAIN=\u0026#34;garden-1.${HOSTED_ZONE_DOMAIN}\u0026#34; ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) cat \u0026lt;\u0026lt;EOF \u0026gt; gardener-values.yaml global: apiserver: image: tag: ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} etcd: servers: http://etcd-for-test-client:2379 useSidecar: false controller: image: tag: ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} internalDomain: provider: aws-route53 hostedZoneID: ${HOSTED_ZONE_ID} domain: ${HOSTED_ZONE_DOMAIN} credentials: AWS_ACCESS_KEY_ID: ${ACCESS_KEY_ID} AWS_SECRET_ACCESS_KEY: ${SECRET_ACCESS_KEY} EOF After creating the gardener-values.yaml file, since chart definition in master branch can have breaking changes after the release, checkout the gardener tag for that release, and run:\ngit checkout ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} helm upgrade --install \\ --namespace garden \\ garden charts/gardener \\ -f charts/gardener/local-values.yaml \\ -f gardener-values.yaml Validate the Gardener is deployed:\nhelm status garden # Wait for `STATUS: DEPLOYED` kubectl -n garden get deploy,pod -l app=gardener # Better if you leave two terminals open in for below commands, and # keep an eye on whats going on behind the scenes as you create/delete # Gardener specific resources (Seed, CloudProfile, SecretBinding, Shoot). kubectl -n garden logs -f deployment/gardener-apiserver # confirm no issues kubectl -n garden logs -f deployment/gardener-controller-manager # confirm no issues, except some \u0026#34;Failed to list *v1beta1...\u0026#34; messages Note: This is not meant to be used in production. You may not want to use apiserver.insecureSkipTLSVerify=true, the hardcoded apiserver certificates, and insecure (non-tls enabled) etcd. But for the sake of keeping this example simple you can just keep those values as they are.\nCreate a CloudProfile We need to create a CloudProfile to be referred from the Shoot (example/30-cloudprofile-azure.yaml):\nkubectl apply -f example/30-cloudprofile-azure.yaml Validate that CloudProfile is created:\nkubectl describe -f example/30-cloudprofile-azure.yaml Define Seed cluster in Gardener In our setup we\u0026rsquo;ll use the cluster for Gardener also as a Seed, this saves us from creating a new Kubernetes cluster. But you can also create an explicit cluster for the Seed. Seed cluster can also be placed into any other cloud provider or on prem. But keep in mind that below steps may differ if you use a different cluster for seed.\nCurrently, a Seed cluster is just a Kubeconfig for the Gardener. The seed cluster could have been created by any tool, Gardener only cares about having a valid Kubeconfig to talk to its API.\nCreate the Seed resource definition with its Secret Lets start with the required seed secret first. Here we need to provide it\u0026rsquo;s cloud provider credentials and kubeconfig in the seed secret. Update example/40-secret-seed-azure.yaml and place the secrets for your environment:\n data.subscriptionID: you can learn this one with az account show data.tenantID: from az ad sp create-for-rbac output as you can see above data.clientID: from az ad sp create-for-rbac output as you can see above data.clientSecret: from az ad sp create-for-rbac output as you can see above data.kubeconfig: you can get this one with az aks get-credentials --resource-group garden-1 --name garden-1 -f - | base64)  Note: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r \u0026#39;.[] | select(.isDefault == true) | .id\u0026#39;) TENANT_ID=$(az account show -o tsv --query \u0026#39;tenantId\u0026#39;) KUBECONFIG_FOR_SEED_CLUSTER=$(az aks get-credentials --resource-group garden-1 --name garden-1 -f -) sed -i \\ -e \u0026#34;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-tenant)@$(echo \u0026#34;$TENANT_ID\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-client)@$(echo \u0026#34;${CLIENT_ID:?\u0026#34;CLIENT_ID is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(client-secret)@$(echo \u0026#34;${CLIENT_SECRET:?\u0026#34;CLIENT_SECRET is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(kubeconfig-for-seed-cluster)@$(echo \u0026#34;$KUBECONFIG_FOR_SEED_CLUSTER\u0026#34; | base64 -w 0)@\u0026#34; \\ example/40-secret-seed-azure.yaml After updating the fields, create the Seed secret:\nkubectl apply -f example/40-secret-seed-azure.yaml Before creating Seed, we need to update the example/50-seed-azure.yaml file and update:\n spec.networks: IP ranges used in your AKS cluster. spec.ingressDomain: Place here the wildcard domain you have for the ingress controller (we created this record in prerequisites). Gardener doesn\u0026rsquo;t create this DNS records but assumes its created ahead of time, Seed clusters are not provisioned by Gardener. spec.cloud.region: eastus (the region of the existing AKS cluster)  HOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) INGRESS_DOMAIN=\u0026#34;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; # discover AKS CIDRs NODE_CIDR=$(az network vnet list -g MC_garden-1_garden-1_eastus -o json | jq -r \u0026#39;.[] | .subnets[] | .addressPrefix\u0026#39;) POD_CIDR=$(kubectl -n kube-system get daemonset/kube-proxy -o yaml | grep cluster-cidr= | grep -v annotations | cut -d = -f2) SERVICE_CIDR=10.0.0.0/16 # This one is hardcoded for now, not easy to discover sed -i \\ -e \u0026#34;s/ingressDomain: dev.azure.seed.example.com/ingressDomain: $INGRESS_DOMAIN/\u0026#34; \\ -e \u0026#34;s/region: westeurope/region: eastus/\u0026#34; \\ -e \u0026#34;s@nodes: 10.240.0.0/16@nodes: $NODE_CIDR@\u0026#34; \\ -e \u0026#34;s@pods: 10.241.128.0/17@pods: $POD_CIDR@\u0026#34; \\ -e \u0026#34;s@services: 10.241.0.0/17@services: $SERVICE_CIDR@\u0026#34; \\ example/50-seed-azure.yaml Now we are ready to create the seed:\nkubectl apply -f example/50-seed-azure.yaml Check the logs in gardener-controller-manager and also wait for seed to be Ready: True. This means gardener-controller-manager is able to reach the Seed cluster with the credentials you provide.\n$ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ kubectl get seed azure NAME CLOUDPROFILE REGION INGRESS DOMAIN AVAILABLE AGE azure azure eastus seed-1.your.domain.here True 1m $ gardenctl ls seeds seeds: - seed: azure If something goes wrong verify that you provided right credentials, and base64 encoded strings of those in the secret. Also check the status field in the Seed resource and gardener-controller-manager logs:\n$ kubectl get seed azure -o json | jq .status { \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2018-05-31T14:56:49Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;all checks passed\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Passed\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Available\u0026#34; } ] } Create a Shoot cluster Create a Project (namespace) for Shoots In this step we create a namespace in Gardener cluster to keep Shoot resource definitions. A project in Gardener terminology is simply a namespace that holds group of Shoots, during this example we\u0026rsquo;ll deploy a single Shoot. (Mind the extra labels defined in example/00-namespace-garden-dev.yaml).\nkubectl apply -f example/05-project-dev.yaml You can check the projects via gardenctl:\n$ gardenctl target garden dev $ kubectl get project dev NAME NAMESPACE STATUS OWNER CREATOR AGE dev garden-dev Ready john.doe@example.com client 1m $ kubectl get ns garden-dev NAME STATUS AGE garden-dev Active 1m $ gardenctl ls projects projects: - project: garden-dev Create a SecretBinding and related Secret We\u0026rsquo;ll use same Azure credentials with example/40-secret-seed-azure.yaml, this is due to the fact that we use the same Azure Subscription for the Shoot and Seed clusters. Differently from the Seed secret, in this one we don\u0026rsquo;t need to provide kubeconfig since the Shoot cluster will be provisioned by Gardener, and we need to provide credentials for Route53 DNS records management.\nUpdate example/70-secret-cloudprovider-azure.yaml and place the secrets for your environment:\n data.subscriptionID: you can learn this one with az account show data.tenantID: from az ad sp create-for-rbac output as you can see above data.clientID: from az ad sp create-for-rbac output as you can see above data.clientSecret: from az ad sp create-for-rbac output as you can see above data.accessKeyID: You need to add this field for Route53 records to be updated. data.secretAccessKey: You need to add this field for Route53 records to be updated.  Note: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r \u0026#39;.[] | select(.isDefault == true) | .id\u0026#39;) TENANT_ID=$(az account show -o tsv --query \u0026#39;tenantId\u0026#39;) ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) sed -i \\ -e \u0026#34;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-tenant)@$(echo \u0026#34;$TENANT_ID\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-client)@$(echo \u0026#34;${CLIENT_ID:?\u0026#34;CLIENT_ID is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(client-secret)@$(echo \u0026#34;${CLIENT_SECRET:?\u0026#34;CLIENT_SECRET is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;\\$a\\ \\ accessKeyID: $(echo $ACCESS_KEY_ID | tr -d \u0026#39;\\n\u0026#39; | base64 )\u0026#34; \\ -e \u0026#34;\\$a\\ \\ secretAccessKey: $(echo $SECRET_ACCESS_KEY | tr -d \u0026#39;\\n\u0026#39; | base64 )\u0026#34; \\ example/70-secret-cloudprovider-azure.yaml After updating the fields, create the cloud provider secret:\nkubectl apply -f example/70-secret-cloudprovider-azure.yaml And create the SecretBinding resource to allow Gardener use that secret (example/80-secretbinding-cloudprovider-azure.yaml):\nsed -i \\ -e \u0026#39;s/# namespace: .*/ namespace: garden-dev/\u0026#39; \\ example/80-secretbinding-cloudprovider-azure.yaml kubectl apply -f example/80-secretbinding-cloudprovider-azure.yaml Check the logs in gardener-controller-manager, there should not be any problems reported.\nCreate the Shoot resource Update the fields in example/90-deprecated-shoot-azure.yaml:\n spec.cloud.region: eastus (this must match the seed cluster\u0026rsquo;s region) spec.dns.domain: This is used to specify the base domain for your api (and other in the future) endpoint(s). For example when johndoe-azure.garden-dev.your.domain.here is used as a value, then your apiserver is available at api.johndoe-azure.garden-dev.your.domain.here spec.dns.hostedZoneID: This field doesn\u0026rsquo;t exist in the example you need to add this field and place the Route53 Hosted Zone ID.  HOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) SHOOT_DOMAIN=\u0026#34;johndoe-azure.garden-dev.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; KUBE_LEGO_EMAIL=$(git config user.email) sed -i \\ -e \u0026#34;s/region: westeurope/region: eastus/\u0026#34; \\ -e \u0026#34;s/domain: johndoe-azure.garden-dev.example.com/domain: $SHOOT_DOMAIN/\u0026#34; \\ -e \u0026#34;/domain:/a\\ \\ \\ \\ hostedZoneID: $HOSTED_ZONE_ID\u0026#34; \\ -e \u0026#34;s/email: john.doe@example.com/email: $KUBE_LEGO_EMAIL/\u0026#34; \\ example/90-deprecated-shoot-azure.yaml And let\u0026rsquo;s create the Shoot resource:\nkubectl apply -f example/90-deprecated-shoot-azure.yaml After creating the Shoot resource, gardener-controller-manager will pick it up and start provisioning the Shoot cluster.\n$ kubectl get -f example/90-deprecated-shoot-azure.yaml NAME CLOUDPROFILE VERSION SEED DOMAIN OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE johndoe-azure azure 1.12.3 azure johndoe-azure.garden-dev.your.domain.here Processing 15 \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; 16s Follow the logs in your console with gardener-controller-manager, starting like below you\u0026rsquo;ll see plenty of Waiting and Executing, etc. logs and many tasks will keep repeating:\ntime=\u0026#34;2018-06-09T07:35:45Z\u0026#34; level=info msg=\u0026#34;[SHOOT RECONCILE] garden-dev/johndoe-azure\u0026#34; time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Starting flow Shoot cluster creation\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).botanist.Shoot.Components.DNS.External{Provider/Entry}.Deploy\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployNamespace\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployKubeAPIServerService\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployBackupNamespaceFromShoot\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:51Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:51Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).MoveBackupTerraformResources\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).WaitUntilKubeAPIServerServiceIsReady\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:56Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:57Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:57Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:01Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:02Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:02Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure ... At this stage you should be waiting for a while until the Shoot cluster is provisioned and initial resources are deployed.\nDuring the provisioning you can also check output of these commands to have a better understanding about what\u0026rsquo;s going on in the seed cluster:\n$ gardenctl ls shoots projects: - project: garden-dev shoots: - johndoe-azure $ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: johndoe-azure health: Unknown status: lastOperation: description: Executing DeployKubeAddonManager, ReconcileMachines. lastUpdateTime: 2018-06-09 08:40:20 +0100 IST progress: 74 state: Processing type: Create $ kubectl -n garden-dev get shoot johndoe-azure NAMESPACE NAME SEED DOMAIN VERSION CONTROL NODES SYSTEM LATEST garden-dev johndoe-azure azure johndoe-azure.garden-dev.your.domain.here 1.10.1 True True True Succeeded $ kubectl -n garden-dev describe shoot johndoe-azure ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Reconciling 1h gardener-controller-manager [BrXWiztO] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [rBFsfwU5] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [2HAbm45D] Reconciling Shoot cluster state Normal Reconciling 48m gardener-controller-manager [S1QA0ksz] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [lvcSKy1Q] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [MddMyk8W] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [XDAAWABd] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [6HYH9Psz] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [rhL38ym4] Reconciling Shoot cluster state Normal Reconciling 35m gardener-controller-manager [BOt4Nvso] Reconciling Shoot cluster state Normal Reconciling 35m gardener-controller-manager [JPtmXmxD] Reconciling Shoot cluster state Normal Reconciling 34m gardener-controller-manager [ldHsVA6G] Reconciling Shoot cluster state Normal Reconciled 31m gardener-controller-manager [ldHsVA6G] Reconciled Shoot cluster state Normal Reconciling 26m gardener-controller-manager [yBh2IBOF] Reconciling Shoot cluster state Normal Reconciled 24m gardener-controller-manager [yBh2IBOF] Reconciled Shoot cluster state Normal Reconciling 16m gardener-controller-manager [bqmFtHUA] Reconciling Shoot cluster state Normal Reconciled 14m gardener-controller-manager [bqmFtHUA] Reconciled Shoot cluster state Normal Reconciling 6m gardener-controller-manager [7QgHE5CH] Reconciling Shoot cluster state Normal Reconciled 3m gardener-controller-manager [7QgHE5CH] Reconciled Shoot cluster state Check Shoot cluster:\n$ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ gardenctl target project garden-dev $ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ gardenctl kubectl cluster-info Kubernetes master is running at https://api.johndoe-azure.garden-dev.your.domain.here CoreDNS is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubernetes-dashboard is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. Cluster Resources After Shoot is Created After the Shoot has been created the summary of the resources in the AKS cluster handled by Gardener will be something like this:\nnon-namespaced resources CloudProfile: azure Project: dev Namespace: garden-dev Seed: azure # cloud.profile:azure, cloud.region:eastus, secretRef.name:seed-azure, secretRef.namespace: garden Namespace: garden Secret: seed-azure # aks credentials, kubeconfig # No other resources with any kind handled by Gardener # Gardener components as well lives in this namespace Namespace: garden-dev # maps to \u0026#34;project:dev\u0026#34; in Gardener Secret: core-azure # credentials for aks + aws (for route53) SecretBinding: core-azure # secretRef.name:core-azure Shoot: johndoe-azure # seed:azure, secretBindingRef.name:core-azure Namespace: shoot--dev--johndoe-azure # These are automatically created once Shoot resource is created AzureMachineClass: shoot--dev--johndoe-azure-cpu-worker-8506a MachineDeployment: shoot--dev--johndoe-azure-cpu-worker MachineSet: shoot--dev--johndoe-azure-cpu-worker-849bbbf75 Machine: shoot--dev--johndoe-azure-cpu-worker-849bbbf75-b42vh BackupInfra: shoot--dev--johndoe-azure--c1b3b # seed:azure, shootUID: shoot.status.UID. # Many other resources created as part of shoot cluster, # but only above ones are handled by Gardener Namespace: backup--shoot--dev--johndoe-azure--c1b3b # Secrets and configMap having info related to backup infrastructure # are created by Gardener. Troubleshooting Shoot Creation Issues For any issue happening during Shoot provisioning, you can consult the gardener-controller-manager logs, or the state in the shoot resource, gardenctl also provides a command to check Shoot cluster states:\n# check gardener-controller-manager logs kubectl -n garden logs -f deployment/gardener-controller-manager # kubectl describe can provide you a human readable output of # same information in below gardenctl command. kubectl -n garden-dev describe shoot johndoe-azure # also try cheking the machine-controller-manager logs of the shoot kubectl logs -n shoot--dev--johndoe-azure deployment/machine-controller-manager With gardenctl:\n$ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: health: Ready status: johndoe-azure lastError: \u0026#34;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: \u0026#39;(*Botanist).Shoot.Components.DNS.External{Provider/Entry}.Destroy\u0026#39; returned \u0026#39;Terraform execution ... lastOperation: description: \u0026#34;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: \u0026#39;(*Botanist).Shoot.Components.DNS.External{Provider/Entry}.Destroy\u0026#39; returned \u0026#39;Terraform ... lastUpdateTime: 2018-06-03 09:48:00 +0100 IST progress: 100 state: Failed type: Reconcile Access Shoot cluster The gardenctl tool provides a convenient wrapper to operate on both cluster and cloud providers, here are some commands you can run\n# select target shoot cluster gardenctl ls gardens gardenctl target garden dev gardenctl ls projects gardenctl target shoot johndoe-azure # issue Azure client (az) commands on target shoot gardenctl az aks list # issue kubectl commands on target shoot gardenctl kubectl -- version --short # \u0026#39;--\u0026#39; is required if you want to # pass any args starting with \u0026#39;-\u0026#39; # open prometheus, alertmanager, grafana without having to find # the user/pass for each gardenctl show prometheus gardenctl show grafana gardenctl show alertmanager Easiest way to obtain kubeconfig of the shoot cluster:\n$ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ export KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ # From now on your local kubectl will be operating on target shoot $ kubectl cluster-info # will show your shoot cluster info $ unset KUBECONFIG # reset to your default kubectl The shoot cluster\u0026rsquo;s kubeconfig is being kept in a secret in the project namespace:\nkubectl -n shoot--dev--johndoe-azure get secret kubecfg -o jsonpath=\u0026#39;{.data.kubeconfig}\u0026#39; | base64 -D \u0026gt; /tmp/johndoe-azure-kubeconfig.yaml export KUBECONFIG=/tmp/johndoe-azure-kubeconfig.yaml Delete Shoot cluster Deleting a Shoot cluster is not straight forward, and this is to protect users from undesired/accidental cluster deletion. One has to place some special annotations to get a Shoot cluster removed. We use the hack/usage/delete script for this purpose.\nPlease refer to Creating / Deleting a Shoot cluster document for more details.\nhack/delete shoot johndoe-azure garden-dev "},{"uri":"https://gardener.cloud/v1.12.8/concepts/deployment/aks/","title":"Deploying the previous Gardener versions and a Seed into an AKS cluster","tags":[],"description":"","content":"Deploying the previous Gardener versions and a Seed into an AKS cluster This document demonstrates how to install Gardener into an existing AKS cluster. We\u0026rsquo;ll use a single cluster to host both Gardener and a Seed to the same cluster for the sake of simplicity .\nPlease note that this document is to provide you an example installation and is not to be used in a production environment since there are some certificates hardcoded, non-HA and non-TLS-enabled etcd setup.\nHigh Level Overview In this example we\u0026rsquo;ll follow these steps to create a Seed cluster on AKS:\n Deploying the Gardener and a Seed into an AKS cluster High Level Overview Prerequisites  AWS credentials for Route 53 Hosted Zone Deploy AKS cluster  Initialize Helm on the Cluster Deploy stable/nginx-ingress chart to AKS Create wildcard DNS record for the ingress   Create Azure Service Principle to get Azure credentials Install gardenctl   Install Gardener  Create garden namespace Deploy etcd Deploy Gardener Helm Chart   Create a CloudProfile Define Seed cluster in Gardener  Create the Seed resource definition with its Secret   Create a Shoot cluster  Create a Project (namespace) for Shoots Create a SecretBinding and related Secret Create the Shoot resource  Cluster Resources After Shoot is Created Troubleshooting Shoot Creation Issues     Access Shoot cluster Delete Shoot cluster  Prerequisites Summary of prerequisites:\n An Azure AKS cluster with:  Helm initialized, an ingress controller deployed, a wildcard DNS record pointing the ingress, az command line client configured for Azure subscription,   An Azure service principle to provide Azure credentials to Gardener, A Route53 Hosted Zone and AWS account credentials with permissions on that Route53 Zone,  aws command line client configured for this account,   gardenctl command line client configured for the AKS cluster\u0026rsquo;s kubeconfig  Note: Gardener doesn\u0026rsquo;t have support for Azure DNS yet (see #494). So, we use a Route53 Hosted Zone even if we are deploying on Azure.\nAWS credentials for Route 53 Hosted Zone You need to provide credentials for AWS with permission to access Route53 Hosted Zone. In this example we\u0026rsquo;ll assume your domain for the Hosted Zone is .your.domain.here.\nHOSTED_ZONE_ID= # place your AWS Route53 hostedZoneID here Create an AWS user, define policy to allow permission for the Hosted Zone and note the hostedZoneID, accessKeyID and secretAccessKey for later use.\nDeploy AKS cluster Here you can find a summary for creating an AKS cluster, if you already have one, skip this step.\naz group create --name garden-1 --location eastus az aks create --resource-group garden-1 --name garden-1 \\ --kubernetes-version 1.11.5 \\ --node-count 2 --node-vm-size Standard_DS4_v2 \\ --generate-ssh-keys az aks get-credentials --resource-group garden-1 --name garden-1 --admin Initialize Helm on the Cluster Since RBAC is enabled by default we need to deploy helm with an RBAC config.\nkubectl apply -f https://raw.githubusercontent.com/Azure/helm-charts/master/docs/prerequisities/helm-rbac-config.yaml helm init --service-account tiller Deploy stable/nginx-ingress chart to AKS At the moment the Ingress resources created by the Gardener are expecting the nginx-ingress style annotations to work.\nhelm upgrade --install \\ --namespace kube-system \\ nginx-ingress stable/nginx-ingress Create wildcard DNS record for the ingress You need to pick a wildcard subdomain matching your Route53 Hosted Zone here. This ingress wildcard record is supposed to be part of the Seed cluster rather than Gardener cluster, in our example we\u0026rsquo;ll use *.seed-1.your.domain.here.\nAssuming you have the AWS cli for your Route53 Hosted Zone is configured on your local, here we\u0026rsquo;ll create the wildcard DNS record using the awless. You can also use the AWS console or any other tool of your choice to create the wildcard record:\nHOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) INGRESS_DOMAIN=\u0026#34;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; # Get LB IP address from `kubectl -n kube-system get svc shared-ingress-nginx-ingress-controller` LB_IP=$(kubectl -n kube-system get svc nginx-ingress-controller --template \u0026#39;{{(index .status.loadBalancer.ingress 0).ip}}\u0026#39;) awless create record \\ zone=$HOSTED_ZONE_ID \\ name=\u0026#34;*.$INGRESS_DOMAIN\u0026#34; \\ value=$LB_IP \\ type=A \\ ttl=300 Create Azure Service Principle to get Azure credentials We need client_id and client_secret to allow Gardener to reach Azure services, we can generate a pair by creating a Service Principle on Azure:\n$ az ad sp create-for-rbac --role=\u0026#34;Contributor\u0026#34; Retrying role assignment creation: 1/36 { \u0026#34;appId\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34;, #az_client_id \u0026#34;displayName\u0026#34;: \u0026#34;azure-cli-2018-05-23-16-15-49\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;http://azure-cli-2018-05-23-16-15-49\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34;, #az_client_secret \u0026#34;tenant\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34; #az_tenant_id } Let\u0026rsquo;s define some env variables for later use\nCLIENT_ID= # place your Azure Service Principal appId CLIENT_SECRET= # place your Azure Service Principal password here Install gardenctl In this example we\u0026rsquo;ll be using gardenctl to interact with Gardener. You can install gardenctl following instruction in its repo: https://github.com/gardener/gardenctl\nHere is a sample configuration for gardenctl:\n$ cat ~/.garden/config gardenClusters: - name: dev kubeConfig: ~/.kube/config Install Gardener Create garden namespace This is where we deploy Gardener components.\nkubectl apply -f example/00-namespace-garden.yaml Deploy etcd Since Gardener is an extension API Server, it can share the etcd backing native Kubernetes cluster\u0026rsquo;s API Server, and hence explicit etcd installation is optional. But in our case we have no access to the control plane components of the AKS cluster and we have to deploy our own etcd ourselves for Gardener. Lets deploy an etcd using the gardener/etcd-backup-restore project, which is also used by the Gardener for Shoot control plane.\n# pull the etcd-backup-restore git clone https://github.com/gardener/etcd-backup-restore.git # deploy etcd helm upgrade --install \\ --namespace garden \\ etcd etcd-backup-restore/chart \\ --set tls= Note: This etcd installation doesn\u0026rsquo;t provide HA. But etcd will be auto recovered by the Deployment. This could be sufficient for some deployments but may not be suitable for production usage. Also note that this etcd is not deployed with TLS enabled and doesn\u0026rsquo;t use certificates for authentication.\nCheck etcd pod\u0026rsquo;s health, it should have READY:2/2 and STATUS:Running:\n$ kubectl -n garden get pods NAME READY STATUS RESTARTS AGE etcd-for-test-0 2/2 Running 0 1m Deploy Gardener Helm Chart Check current releases and pick a suitable one to install.\nGARDENER_RELEASE=0.17.1 gardener-controller-manager will need to maintain some DNS records for Seed. So, you need to provide Route53 credentials in the values.yaml file:\n global.controller.internalDomain.hostedZoneID global.controller.internalDomain.domain: Here pick a subdomain for your Gardener to maintain DNS records for your Shoot clusters. This domain has to be within your Route53 Hosted Zone. e.g. garden-1.your.domain.here global.controller.internalDomain.credentials global.controller.internalDomain.secretAccessKey  HOSTED_ZONE_DOMAIN=$( aws route53 get-hosted-zone \\ --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} \\ --query \u0026#39;HostedZone.Name\u0026#39; \\ --output text) HOSTED_ZONE_DOMAIN=${HOSTED_ZONE_DOMAIN%%.} GARDENER_DOMAIN=\u0026#34;garden-1.${HOSTED_ZONE_DOMAIN}\u0026#34; ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) cat \u0026lt;\u0026lt;EOF \u0026gt; gardener-values.yaml global: apiserver: image: tag: ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} etcd: servers: http://etcd-for-test-client:2379 useSidecar: false controller: image: tag: ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} internalDomain: provider: aws-route53 hostedZoneID: ${HOSTED_ZONE_ID} domain: ${HOSTED_ZONE_DOMAIN} credentials: AWS_ACCESS_KEY_ID: ${ACCESS_KEY_ID} AWS_SECRET_ACCESS_KEY: ${SECRET_ACCESS_KEY} EOF After creating the gardener-values.yaml file, since chart definition in master branch can have breaking changes after the release, checkout the gardener tag for that release, and run:\ngit checkout ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} helm upgrade --install \\ --namespace garden \\ garden charts/gardener \\ -f charts/gardener/local-values.yaml \\ -f gardener-values.yaml Validate the Gardener is deployed:\nhelm status garden # Wait for `STATUS: DEPLOYED` kubectl -n garden get deploy,pod -l app=gardener # Better if you leave two terminals open in for below commands, and # keep an eye on whats going on behind the scenes as you create/delete # Gardener specific resources (Seed, CloudProfile, SecretBinding, Shoot). kubectl -n garden logs -f deployment/gardener-apiserver # confirm no issues kubectl -n garden logs -f deployment/gardener-controller-manager # confirm no issues, except some \u0026#34;Failed to list *v1beta1...\u0026#34; messages Note: This is not meant to be used in production. You may not want to use apiserver.insecureSkipTLSVerify=true, the hardcoded apiserver certificates, and insecure (non-tls enabled) etcd. But for the sake of keeping this example simple you can just keep those values as they are.\nCreate a CloudProfile We need to create a CloudProfile to be referred from the Shoot (example/30-cloudprofile-azure.yaml):\nkubectl apply -f example/30-cloudprofile-azure.yaml Validate that CloudProfile is created:\nkubectl describe -f example/30-cloudprofile-azure.yaml Define Seed cluster in Gardener In our setup we\u0026rsquo;ll use the cluster for Gardener also as a Seed, this saves us from creating a new Kubernetes cluster. But you can also create an explicit cluster for the Seed. Seed cluster can also be placed into any other cloud provider or on prem. But keep in mind that below steps may differ if you use a different cluster for seed.\nCurrently, a Seed cluster is just a Kubeconfig for the Gardener. The seed cluster could have been created by any tool, Gardener only cares about having a valid Kubeconfig to talk to its API.\nCreate the Seed resource definition with its Secret Lets start with the required seed secret first. Here we need to provide it\u0026rsquo;s cloud provider credentials and kubeconfig in the seed secret. Update example/40-secret-seed-azure.yaml and place the secrets for your environment:\n data.subscriptionID: you can learn this one with az account show data.tenantID: from az ad sp create-for-rbac output as you can see above data.clientID: from az ad sp create-for-rbac output as you can see above data.clientSecret: from az ad sp create-for-rbac output as you can see above data.kubeconfig: you can get this one with az aks get-credentials --resource-group garden-1 --name garden-1 -f - | base64)  Note: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r \u0026#39;.[] | select(.isDefault == true) | .id\u0026#39;) TENANT_ID=$(az account show -o tsv --query \u0026#39;tenantId\u0026#39;) KUBECONFIG_FOR_SEED_CLUSTER=$(az aks get-credentials --resource-group garden-1 --name garden-1 -f -) sed -i \\ -e \u0026#34;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-tenant)@$(echo \u0026#34;$TENANT_ID\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-client)@$(echo \u0026#34;${CLIENT_ID:?\u0026#34;CLIENT_ID is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(client-secret)@$(echo \u0026#34;${CLIENT_SECRET:?\u0026#34;CLIENT_SECRET is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(kubeconfig-for-seed-cluster)@$(echo \u0026#34;$KUBECONFIG_FOR_SEED_CLUSTER\u0026#34; | base64 -w 0)@\u0026#34; \\ example/40-secret-seed-azure.yaml After updating the fields, create the Seed secret:\nkubectl apply -f example/40-secret-seed-azure.yaml Before creating Seed, we need to update the example/50-seed-azure.yaml file and update:\n spec.networks: IP ranges used in your AKS cluster. spec.ingressDomain: Place here the wildcard domain you have for the ingress controller (we created this record in prerequisites). Gardener doesn\u0026rsquo;t create this DNS records but assumes its created ahead of time, Seed clusters are not provisioned by Gardener. spec.cloud.region: eastus (the region of the existing AKS cluster)  HOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) INGRESS_DOMAIN=\u0026#34;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; # discover AKS CIDRs NODE_CIDR=$(az network vnet list -g MC_garden-1_garden-1_eastus -o json | jq -r \u0026#39;.[] | .subnets[] | .addressPrefix\u0026#39;) POD_CIDR=$(kubectl -n kube-system get daemonset/kube-proxy -o yaml | grep cluster-cidr= | grep -v annotations | cut -d = -f2) SERVICE_CIDR=10.0.0.0/16 # This one is hardcoded for now, not easy to discover sed -i \\ -e \u0026#34;s/ingressDomain: dev.azure.seed.example.com/ingressDomain: $INGRESS_DOMAIN/\u0026#34; \\ -e \u0026#34;s/region: westeurope/region: eastus/\u0026#34; \\ -e \u0026#34;s@nodes: 10.240.0.0/16@nodes: $NODE_CIDR@\u0026#34; \\ -e \u0026#34;s@pods: 10.241.128.0/17@pods: $POD_CIDR@\u0026#34; \\ -e \u0026#34;s@services: 10.241.0.0/17@services: $SERVICE_CIDR@\u0026#34; \\ example/50-seed-azure.yaml Now we are ready to create the seed:\nkubectl apply -f example/50-seed-azure.yaml Check the logs in gardener-controller-manager and also wait for seed to be Ready: True. This means gardener-controller-manager is able to reach the Seed cluster with the credentials you provide.\n$ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ kubectl get seed azure NAME CLOUDPROFILE REGION INGRESS DOMAIN AVAILABLE AGE azure azure eastus seed-1.your.domain.here True 1m $ gardenctl ls seeds seeds: - seed: azure If something goes wrong verify that you provided right credentials, and base64 encoded strings of those in the secret. Also check the status field in the Seed resource and gardener-controller-manager logs:\n$ kubectl get seed azure -o json | jq .status { \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2018-05-31T14:56:49Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;all checks passed\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Passed\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Available\u0026#34; } ] } Create a Shoot cluster Create a Project (namespace) for Shoots In this step we create a namespace in Gardener cluster to keep Shoot resource definitions. A project in Gardener terminology is simply a namespace that holds group of Shoots, during this example we\u0026rsquo;ll deploy a single Shoot. (Mind the extra labels defined in example/00-namespace-garden-dev.yaml).\nkubectl apply -f example/05-project-dev.yaml You can check the projects via gardenctl:\n$ gardenctl target garden dev $ kubectl get project dev NAME NAMESPACE STATUS OWNER CREATOR AGE dev garden-dev Ready john.doe@example.com client 1m $ kubectl get ns garden-dev NAME STATUS AGE garden-dev Active 1m $ gardenctl ls projects projects: - project: garden-dev Create a SecretBinding and related Secret We\u0026rsquo;ll use same Azure credentials with example/40-secret-seed-azure.yaml, this is due to the fact that we use the same Azure Subscription for the Shoot and Seed clusters. Differently from the Seed secret, in this one we don\u0026rsquo;t need to provide kubeconfig since the Shoot cluster will be provisioned by Gardener, and we need to provide credentials for Route53 DNS records management.\nUpdate example/70-secret-cloudprovider-azure.yaml and place the secrets for your environment:\n data.subscriptionID: you can learn this one with az account show data.tenantID: from az ad sp create-for-rbac output as you can see above data.clientID: from az ad sp create-for-rbac output as you can see above data.clientSecret: from az ad sp create-for-rbac output as you can see above data.accessKeyID: You need to add this field for Route53 records to be updated. data.secretAccessKey: You need to add this field for Route53 records to be updated.  Note: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r \u0026#39;.[] | select(.isDefault == true) | .id\u0026#39;) TENANT_ID=$(az account show -o tsv --query \u0026#39;tenantId\u0026#39;) ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) sed -i \\ -e \u0026#34;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-tenant)@$(echo \u0026#34;$TENANT_ID\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-client)@$(echo \u0026#34;${CLIENT_ID:?\u0026#34;CLIENT_ID is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(client-secret)@$(echo \u0026#34;${CLIENT_SECRET:?\u0026#34;CLIENT_SECRET is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;\\$a\\ \\ accessKeyID: $(echo $ACCESS_KEY_ID | tr -d \u0026#39;\\n\u0026#39; | base64 )\u0026#34; \\ -e \u0026#34;\\$a\\ \\ secretAccessKey: $(echo $SECRET_ACCESS_KEY | tr -d \u0026#39;\\n\u0026#39; | base64 )\u0026#34; \\ example/70-secret-cloudprovider-azure.yaml After updating the fields, create the cloud provider secret:\nkubectl apply -f example/70-secret-cloudprovider-azure.yaml And create the SecretBinding resource to allow Gardener use that secret (example/80-secretbinding-cloudprovider-azure.yaml):\nsed -i \\ -e \u0026#39;s/# namespace: .*/ namespace: garden-dev/\u0026#39; \\ example/80-secretbinding-cloudprovider-azure.yaml kubectl apply -f example/80-secretbinding-cloudprovider-azure.yaml Check the logs in gardener-controller-manager, there should not be any problems reported.\nCreate the Shoot resource Update the fields in example/90-deprecated-shoot-azure.yaml:\n spec.cloud.region: eastus (this must match the seed cluster\u0026rsquo;s region) spec.dns.domain: This is used to specify the base domain for your api (and other in the future) endpoint(s). For example when johndoe-azure.garden-dev.your.domain.here is used as a value, then your apiserver is available at api.johndoe-azure.garden-dev.your.domain.here spec.dns.hostedZoneID: This field doesn\u0026rsquo;t exist in the example you need to add this field and place the Route53 Hosted Zone ID.  HOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) SHOOT_DOMAIN=\u0026#34;johndoe-azure.garden-dev.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; KUBE_LEGO_EMAIL=$(git config user.email) sed -i \\ -e \u0026#34;s/region: westeurope/region: eastus/\u0026#34; \\ -e \u0026#34;s/domain: johndoe-azure.garden-dev.example.com/domain: $SHOOT_DOMAIN/\u0026#34; \\ -e \u0026#34;/domain:/a\\ \\ \\ \\ hostedZoneID: $HOSTED_ZONE_ID\u0026#34; \\ -e \u0026#34;s/email: john.doe@example.com/email: $KUBE_LEGO_EMAIL/\u0026#34; \\ example/90-deprecated-shoot-azure.yaml And let\u0026rsquo;s create the Shoot resource:\nkubectl apply -f example/90-deprecated-shoot-azure.yaml After creating the Shoot resource, gardener-controller-manager will pick it up and start provisioning the Shoot cluster.\n$ kubectl get -f example/90-deprecated-shoot-azure.yaml NAME CLOUDPROFILE VERSION SEED DOMAIN OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE johndoe-azure azure 1.12.3 azure johndoe-azure.garden-dev.your.domain.here Processing 15 \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; 16s Follow the logs in your console with gardener-controller-manager, starting like below you\u0026rsquo;ll see plenty of Waiting and Executing, etc. logs and many tasks will keep repeating:\ntime=\u0026#34;2018-06-09T07:35:45Z\u0026#34; level=info msg=\u0026#34;[SHOOT RECONCILE] garden-dev/johndoe-azure\u0026#34; time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Starting flow Shoot cluster creation\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).botanist.Shoot.Components.DNS.External{Provider/Entry}.Deploy\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployNamespace\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployKubeAPIServerService\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployBackupNamespaceFromShoot\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:51Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:51Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).MoveBackupTerraformResources\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).WaitUntilKubeAPIServerServiceIsReady\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:56Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:57Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:57Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:01Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:02Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:02Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure ... At this stage you should be waiting for a while until the Shoot cluster is provisioned and initial resources are deployed.\nDuring the provisioning you can also check output of these commands to have a better understanding about what\u0026rsquo;s going on in the seed cluster:\n$ gardenctl ls shoots projects: - project: garden-dev shoots: - johndoe-azure $ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: johndoe-azure health: Unknown status: lastOperation: description: Executing DeployKubeAddonManager, ReconcileMachines. lastUpdateTime: 2018-06-09 08:40:20 +0100 IST progress: 74 state: Processing type: Create $ kubectl -n garden-dev get shoot johndoe-azure NAMESPACE NAME SEED DOMAIN VERSION CONTROL NODES SYSTEM LATEST garden-dev johndoe-azure azure johndoe-azure.garden-dev.your.domain.here 1.10.1 True True True Succeeded $ kubectl -n garden-dev describe shoot johndoe-azure ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Reconciling 1h gardener-controller-manager [BrXWiztO] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [rBFsfwU5] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [2HAbm45D] Reconciling Shoot cluster state Normal Reconciling 48m gardener-controller-manager [S1QA0ksz] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [lvcSKy1Q] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [MddMyk8W] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [XDAAWABd] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [6HYH9Psz] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [rhL38ym4] Reconciling Shoot cluster state Normal Reconciling 35m gardener-controller-manager [BOt4Nvso] Reconciling Shoot cluster state Normal Reconciling 35m gardener-controller-manager [JPtmXmxD] Reconciling Shoot cluster state Normal Reconciling 34m gardener-controller-manager [ldHsVA6G] Reconciling Shoot cluster state Normal Reconciled 31m gardener-controller-manager [ldHsVA6G] Reconciled Shoot cluster state Normal Reconciling 26m gardener-controller-manager [yBh2IBOF] Reconciling Shoot cluster state Normal Reconciled 24m gardener-controller-manager [yBh2IBOF] Reconciled Shoot cluster state Normal Reconciling 16m gardener-controller-manager [bqmFtHUA] Reconciling Shoot cluster state Normal Reconciled 14m gardener-controller-manager [bqmFtHUA] Reconciled Shoot cluster state Normal Reconciling 6m gardener-controller-manager [7QgHE5CH] Reconciling Shoot cluster state Normal Reconciled 3m gardener-controller-manager [7QgHE5CH] Reconciled Shoot cluster state Check Shoot cluster:\n$ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ gardenctl target project garden-dev $ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ gardenctl kubectl cluster-info Kubernetes master is running at https://api.johndoe-azure.garden-dev.your.domain.here CoreDNS is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubernetes-dashboard is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. Cluster Resources After Shoot is Created After the Shoot has been created the summary of the resources in the AKS cluster handled by Gardener will be something like this:\nnon-namespaced resources CloudProfile: azure Project: dev Namespace: garden-dev Seed: azure # cloud.profile:azure, cloud.region:eastus, secretRef.name:seed-azure, secretRef.namespace: garden Namespace: garden Secret: seed-azure # aks credentials, kubeconfig # No other resources with any kind handled by Gardener # Gardener components as well lives in this namespace Namespace: garden-dev # maps to \u0026#34;project:dev\u0026#34; in Gardener Secret: core-azure # credentials for aks + aws (for route53) SecretBinding: core-azure # secretRef.name:core-azure Shoot: johndoe-azure # seed:azure, secretBindingRef.name:core-azure Namespace: shoot--dev--johndoe-azure # These are automatically created once Shoot resource is created AzureMachineClass: shoot--dev--johndoe-azure-cpu-worker-8506a MachineDeployment: shoot--dev--johndoe-azure-cpu-worker MachineSet: shoot--dev--johndoe-azure-cpu-worker-849bbbf75 Machine: shoot--dev--johndoe-azure-cpu-worker-849bbbf75-b42vh BackupInfra: shoot--dev--johndoe-azure--c1b3b # seed:azure, shootUID: shoot.status.UID. # Many other resources created as part of shoot cluster, # but only above ones are handled by Gardener Namespace: backup--shoot--dev--johndoe-azure--c1b3b # Secrets and configMap having info related to backup infrastructure # are created by Gardener. Troubleshooting Shoot Creation Issues For any issue happening during Shoot provisioning, you can consult the gardener-controller-manager logs, or the state in the shoot resource, gardenctl also provides a command to check Shoot cluster states:\n# check gardener-controller-manager logs kubectl -n garden logs -f deployment/gardener-controller-manager # kubectl describe can provide you a human readable output of # same information in below gardenctl command. kubectl -n garden-dev describe shoot johndoe-azure # also try cheking the machine-controller-manager logs of the shoot kubectl logs -n shoot--dev--johndoe-azure deployment/machine-controller-manager With gardenctl:\n$ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: health: Ready status: johndoe-azure lastError: \u0026#34;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: \u0026#39;(*Botanist).Shoot.Components.DNS.External{Provider/Entry}.Destroy\u0026#39; returned \u0026#39;Terraform execution ... lastOperation: description: \u0026#34;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: \u0026#39;(*Botanist).Shoot.Components.DNS.External{Provider/Entry}.Destroy\u0026#39; returned \u0026#39;Terraform ... lastUpdateTime: 2018-06-03 09:48:00 +0100 IST progress: 100 state: Failed type: Reconcile Access Shoot cluster The gardenctl tool provides a convenient wrapper to operate on both cluster and cloud providers, here are some commands you can run\n# select target shoot cluster gardenctl ls gardens gardenctl target garden dev gardenctl ls projects gardenctl target shoot johndoe-azure # issue Azure client (az) commands on target shoot gardenctl az aks list # issue kubectl commands on target shoot gardenctl kubectl -- version --short # \u0026#39;--\u0026#39; is required if you want to # pass any args starting with \u0026#39;-\u0026#39; # open prometheus, alertmanager, grafana without having to find # the user/pass for each gardenctl show prometheus gardenctl show grafana gardenctl show alertmanager Easiest way to obtain kubeconfig of the shoot cluster:\n$ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ export KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ # From now on your local kubectl will be operating on target shoot $ kubectl cluster-info # will show your shoot cluster info $ unset KUBECONFIG # reset to your default kubectl The shoot cluster\u0026rsquo;s kubeconfig is being kept in a secret in the project namespace:\nkubectl -n shoot--dev--johndoe-azure get secret kubecfg -o jsonpath=\u0026#39;{.data.kubeconfig}\u0026#39; | base64 -D \u0026gt; /tmp/johndoe-azure-kubeconfig.yaml export KUBECONFIG=/tmp/johndoe-azure-kubeconfig.yaml Delete Shoot cluster Deleting a Shoot cluster is not straight forward, and this is to protect users from undesired/accidental cluster deletion. One has to place some special annotations to get a Shoot cluster removed. We use the hack/usage/delete script for this purpose.\nPlease refer to Creating / Deleting a Shoot cluster document for more details.\nhack/delete shoot johndoe-azure garden-dev "},{"uri":"https://gardener.cloud/v1.13.2/concepts/deployment/aks/","title":"Deploying the previous Gardener versions and a Seed into an AKS cluster","tags":[],"description":"","content":"Deploying the previous Gardener versions and a Seed into an AKS cluster This document demonstrates how to install Gardener into an existing AKS cluster. We\u0026rsquo;ll use a single cluster to host both Gardener and a Seed to the same cluster for the sake of simplicity .\nPlease note that this document is to provide you an example installation and is not to be used in a production environment since there are some certificates hardcoded, non-HA and non-TLS-enabled etcd setup.\nHigh Level Overview In this example we\u0026rsquo;ll follow these steps to create a Seed cluster on AKS:\n Deploying the Gardener and a Seed into an AKS cluster High Level Overview Prerequisites  AWS credentials for Route 53 Hosted Zone Deploy AKS cluster  Initialize Helm on the Cluster Deploy stable/nginx-ingress chart to AKS Create wildcard DNS record for the ingress   Create Azure Service Principle to get Azure credentials Install gardenctl   Install Gardener  Create garden namespace Deploy etcd Deploy Gardener Helm Chart   Create a CloudProfile Define Seed cluster in Gardener  Create the Seed resource definition with its Secret   Create a Shoot cluster  Create a Project (namespace) for Shoots Create a SecretBinding and related Secret Create the Shoot resource  Cluster Resources After Shoot is Created Troubleshooting Shoot Creation Issues     Access Shoot cluster Delete Shoot cluster  Prerequisites Summary of prerequisites:\n An Azure AKS cluster with:  Helm initialized, an ingress controller deployed, a wildcard DNS record pointing the ingress, az command line client configured for Azure subscription,   An Azure service principle to provide Azure credentials to Gardener, A Route53 Hosted Zone and AWS account credentials with permissions on that Route53 Zone,  aws command line client configured for this account,   gardenctl command line client configured for the AKS cluster\u0026rsquo;s kubeconfig  Note: Gardener doesn\u0026rsquo;t have support for Azure DNS yet (see #494). So, we use a Route53 Hosted Zone even if we are deploying on Azure.\nAWS credentials for Route 53 Hosted Zone You need to provide credentials for AWS with permission to access Route53 Hosted Zone. In this example we\u0026rsquo;ll assume your domain for the Hosted Zone is .your.domain.here.\nHOSTED_ZONE_ID= # place your AWS Route53 hostedZoneID here Create an AWS user, define policy to allow permission for the Hosted Zone and note the hostedZoneID, accessKeyID and secretAccessKey for later use.\nDeploy AKS cluster Here you can find a summary for creating an AKS cluster, if you already have one, skip this step.\naz group create --name garden-1 --location eastus az aks create --resource-group garden-1 --name garden-1 \\ --kubernetes-version 1.11.5 \\ --node-count 2 --node-vm-size Standard_DS4_v2 \\ --generate-ssh-keys az aks get-credentials --resource-group garden-1 --name garden-1 --admin Initialize Helm on the Cluster Since RBAC is enabled by default we need to deploy helm with an RBAC config.\nkubectl apply -f https://raw.githubusercontent.com/Azure/helm-charts/master/docs/prerequisities/helm-rbac-config.yaml helm init --service-account tiller Deploy stable/nginx-ingress chart to AKS At the moment the Ingress resources created by the Gardener are expecting the nginx-ingress style annotations to work.\nhelm upgrade --install \\ --namespace kube-system \\ nginx-ingress stable/nginx-ingress Create wildcard DNS record for the ingress You need to pick a wildcard subdomain matching your Route53 Hosted Zone here. This ingress wildcard record is supposed to be part of the Seed cluster rather than Gardener cluster, in our example we\u0026rsquo;ll use *.seed-1.your.domain.here.\nAssuming you have the AWS cli for your Route53 Hosted Zone is configured on your local, here we\u0026rsquo;ll create the wildcard DNS record using the awless. You can also use the AWS console or any other tool of your choice to create the wildcard record:\nHOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) INGRESS_DOMAIN=\u0026#34;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; # Get LB IP address from `kubectl -n kube-system get svc shared-ingress-nginx-ingress-controller` LB_IP=$(kubectl -n kube-system get svc nginx-ingress-controller --template \u0026#39;{{(index .status.loadBalancer.ingress 0).ip}}\u0026#39;) awless create record \\ zone=$HOSTED_ZONE_ID \\ name=\u0026#34;*.$INGRESS_DOMAIN\u0026#34; \\ value=$LB_IP \\ type=A \\ ttl=300 Create Azure Service Principle to get Azure credentials We need client_id and client_secret to allow Gardener to reach Azure services, we can generate a pair by creating a Service Principle on Azure:\n$ az ad sp create-for-rbac --role=\u0026#34;Contributor\u0026#34; Retrying role assignment creation: 1/36 { \u0026#34;appId\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34;, #az_client_id \u0026#34;displayName\u0026#34;: \u0026#34;azure-cli-2018-05-23-16-15-49\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;http://azure-cli-2018-05-23-16-15-49\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34;, #az_client_secret \u0026#34;tenant\u0026#34;: \u0026#34;xxxxxx-xxx-xxxx-xxx-xxxxx\u0026#34; #az_tenant_id } Let\u0026rsquo;s define some env variables for later use\nCLIENT_ID= # place your Azure Service Principal appId CLIENT_SECRET= # place your Azure Service Principal password here Install gardenctl In this example we\u0026rsquo;ll be using gardenctl to interact with Gardener. You can install gardenctl following instruction in its repo: https://github.com/gardener/gardenctl\nHere is a sample configuration for gardenctl:\n$ cat ~/.garden/config gardenClusters: - name: dev kubeConfig: ~/.kube/config Install Gardener Create garden namespace This is where we deploy Gardener components.\nkubectl apply -f example/00-namespace-garden.yaml Deploy etcd Since Gardener is an extension API Server, it can share the etcd backing native Kubernetes cluster\u0026rsquo;s API Server, and hence explicit etcd installation is optional. But in our case we have no access to the control plane components of the AKS cluster and we have to deploy our own etcd ourselves for Gardener. Lets deploy an etcd using the gardener/etcd-backup-restore project, which is also used by the Gardener for Shoot control plane.\n# pull the etcd-backup-restore git clone https://github.com/gardener/etcd-backup-restore.git # deploy etcd helm upgrade --install \\ --namespace garden \\ etcd etcd-backup-restore/chart \\ --set tls= Note: This etcd installation doesn\u0026rsquo;t provide HA. But etcd will be auto recovered by the Deployment. This could be sufficient for some deployments but may not be suitable for production usage. Also note that this etcd is not deployed with TLS enabled and doesn\u0026rsquo;t use certificates for authentication.\nCheck etcd pod\u0026rsquo;s health, it should have READY:2/2 and STATUS:Running:\n$ kubectl -n garden get pods NAME READY STATUS RESTARTS AGE etcd-for-test-0 2/2 Running 0 1m Deploy Gardener Helm Chart Check current releases and pick a suitable one to install.\nGARDENER_RELEASE=0.17.1 gardener-controller-manager will need to maintain some DNS records for Seed. So, you need to provide Route53 credentials in the values.yaml file:\n global.controller.internalDomain.hostedZoneID global.controller.internalDomain.domain: Here pick a subdomain for your Gardener to maintain DNS records for your Shoot clusters. This domain has to be within your Route53 Hosted Zone. e.g. garden-1.your.domain.here global.controller.internalDomain.credentials global.controller.internalDomain.secretAccessKey  HOSTED_ZONE_DOMAIN=$( aws route53 get-hosted-zone \\ --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} \\ --query \u0026#39;HostedZone.Name\u0026#39; \\ --output text) HOSTED_ZONE_DOMAIN=${HOSTED_ZONE_DOMAIN%%.} GARDENER_DOMAIN=\u0026#34;garden-1.${HOSTED_ZONE_DOMAIN}\u0026#34; ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) cat \u0026lt;\u0026lt;EOF \u0026gt; gardener-values.yaml global: apiserver: image: tag: ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} etcd: servers: http://etcd-for-test-client:2379 useSidecar: false controller: image: tag: ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} internalDomain: provider: aws-route53 hostedZoneID: ${HOSTED_ZONE_ID} domain: ${HOSTED_ZONE_DOMAIN} credentials: AWS_ACCESS_KEY_ID: ${ACCESS_KEY_ID} AWS_SECRET_ACCESS_KEY: ${SECRET_ACCESS_KEY} EOF After creating the gardener-values.yaml file, since chart definition in master branch can have breaking changes after the release, checkout the gardener tag for that release, and run:\ngit checkout ${GARDENER_RELEASE:?\u0026#34;GARDENER_RELEASE is missing\u0026#34;} helm upgrade --install \\ --namespace garden \\ garden charts/gardener \\ -f charts/gardener/local-values.yaml \\ -f gardener-values.yaml Validate the Gardener is deployed:\nhelm status garden # Wait for `STATUS: DEPLOYED` kubectl -n garden get deploy,pod -l app=gardener # Better if you leave two terminals open in for below commands, and # keep an eye on whats going on behind the scenes as you create/delete # Gardener specific resources (Seed, CloudProfile, SecretBinding, Shoot). kubectl -n garden logs -f deployment/gardener-apiserver # confirm no issues kubectl -n garden logs -f deployment/gardener-controller-manager # confirm no issues, except some \u0026#34;Failed to list *v1beta1...\u0026#34; messages Note: This is not meant to be used in production. You may not want to use apiserver.insecureSkipTLSVerify=true, the hardcoded apiserver certificates, and insecure (non-tls enabled) etcd. But for the sake of keeping this example simple you can just keep those values as they are.\nCreate a CloudProfile We need to create a CloudProfile to be referred from the Shoot (example/30-cloudprofile-azure.yaml):\nkubectl apply -f example/30-cloudprofile-azure.yaml Validate that CloudProfile is created:\nkubectl describe -f example/30-cloudprofile-azure.yaml Define Seed cluster in Gardener In our setup we\u0026rsquo;ll use the cluster for Gardener also as a Seed, this saves us from creating a new Kubernetes cluster. But you can also create an explicit cluster for the Seed. Seed cluster can also be placed into any other cloud provider or on prem. But keep in mind that below steps may differ if you use a different cluster for seed.\nCurrently, a Seed cluster is just a Kubeconfig for the Gardener. The seed cluster could have been created by any tool, Gardener only cares about having a valid Kubeconfig to talk to its API.\nCreate the Seed resource definition with its Secret Lets start with the required seed secret first. Here we need to provide it\u0026rsquo;s cloud provider credentials and kubeconfig in the seed secret. Update example/40-secret-seed-azure.yaml and place the secrets for your environment:\n data.subscriptionID: you can learn this one with az account show data.tenantID: from az ad sp create-for-rbac output as you can see above data.clientID: from az ad sp create-for-rbac output as you can see above data.clientSecret: from az ad sp create-for-rbac output as you can see above data.kubeconfig: you can get this one with az aks get-credentials --resource-group garden-1 --name garden-1 -f - | base64)  Note: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r \u0026#39;.[] | select(.isDefault == true) | .id\u0026#39;) TENANT_ID=$(az account show -o tsv --query \u0026#39;tenantId\u0026#39;) KUBECONFIG_FOR_SEED_CLUSTER=$(az aks get-credentials --resource-group garden-1 --name garden-1 -f -) sed -i \\ -e \u0026#34;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-tenant)@$(echo \u0026#34;$TENANT_ID\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-client)@$(echo \u0026#34;${CLIENT_ID:?\u0026#34;CLIENT_ID is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(client-secret)@$(echo \u0026#34;${CLIENT_SECRET:?\u0026#34;CLIENT_SECRET is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(kubeconfig-for-seed-cluster)@$(echo \u0026#34;$KUBECONFIG_FOR_SEED_CLUSTER\u0026#34; | base64 -w 0)@\u0026#34; \\ example/40-secret-seed-azure.yaml After updating the fields, create the Seed secret:\nkubectl apply -f example/40-secret-seed-azure.yaml Before creating Seed, we need to update the example/50-seed-azure.yaml file and update:\n spec.networks: IP ranges used in your AKS cluster. spec.ingressDomain: Place here the wildcard domain you have for the ingress controller (we created this record in prerequisites). Gardener doesn\u0026rsquo;t create this DNS records but assumes its created ahead of time, Seed clusters are not provisioned by Gardener. spec.cloud.region: eastus (the region of the existing AKS cluster)  HOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) INGRESS_DOMAIN=\u0026#34;seed-1.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; # discover AKS CIDRs NODE_CIDR=$(az network vnet list -g MC_garden-1_garden-1_eastus -o json | jq -r \u0026#39;.[] | .subnets[] | .addressPrefix\u0026#39;) POD_CIDR=$(kubectl -n kube-system get daemonset/kube-proxy -o yaml | grep cluster-cidr= | grep -v annotations | cut -d = -f2) SERVICE_CIDR=10.0.0.0/16 # This one is hardcoded for now, not easy to discover sed -i \\ -e \u0026#34;s/ingressDomain: dev.azure.seed.example.com/ingressDomain: $INGRESS_DOMAIN/\u0026#34; \\ -e \u0026#34;s/region: westeurope/region: eastus/\u0026#34; \\ -e \u0026#34;s@nodes: 10.240.0.0/16@nodes: $NODE_CIDR@\u0026#34; \\ -e \u0026#34;s@pods: 10.241.128.0/17@pods: $POD_CIDR@\u0026#34; \\ -e \u0026#34;s@services: 10.241.0.0/17@services: $SERVICE_CIDR@\u0026#34; \\ example/50-seed-azure.yaml Now we are ready to create the seed:\nkubectl apply -f example/50-seed-azure.yaml Check the logs in gardener-controller-manager and also wait for seed to be Ready: True. This means gardener-controller-manager is able to reach the Seed cluster with the credentials you provide.\n$ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ kubectl get seed azure NAME CLOUDPROFILE REGION INGRESS DOMAIN AVAILABLE AGE azure azure eastus seed-1.your.domain.here True 1m $ gardenctl ls seeds seeds: - seed: azure If something goes wrong verify that you provided right credentials, and base64 encoded strings of those in the secret. Also check the status field in the Seed resource and gardener-controller-manager logs:\n$ kubectl get seed azure -o json | jq .status { \u0026#34;conditions\u0026#34;: [ { \u0026#34;lastTransitionTime\u0026#34;: \u0026#34;2018-05-31T14:56:49Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;all checks passed\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Passed\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Available\u0026#34; } ] } Create a Shoot cluster Create a Project (namespace) for Shoots In this step we create a namespace in Gardener cluster to keep Shoot resource definitions. A project in Gardener terminology is simply a namespace that holds group of Shoots, during this example we\u0026rsquo;ll deploy a single Shoot. (Mind the extra labels defined in example/00-namespace-garden-dev.yaml).\nkubectl apply -f example/05-project-dev.yaml You can check the projects via gardenctl:\n$ gardenctl target garden dev $ kubectl get project dev NAME NAMESPACE STATUS OWNER CREATOR AGE dev garden-dev Ready john.doe@example.com client 1m $ kubectl get ns garden-dev NAME STATUS AGE garden-dev Active 1m $ gardenctl ls projects projects: - project: garden-dev Create a SecretBinding and related Secret We\u0026rsquo;ll use same Azure credentials with example/40-secret-seed-azure.yaml, this is due to the fact that we use the same Azure Subscription for the Shoot and Seed clusters. Differently from the Seed secret, in this one we don\u0026rsquo;t need to provide kubeconfig since the Shoot cluster will be provisioned by Gardener, and we need to provide credentials for Route53 DNS records management.\nUpdate example/70-secret-cloudprovider-azure.yaml and place the secrets for your environment:\n data.subscriptionID: you can learn this one with az account show data.tenantID: from az ad sp create-for-rbac output as you can see above data.clientID: from az ad sp create-for-rbac output as you can see above data.clientSecret: from az ad sp create-for-rbac output as you can see above data.accessKeyID: You need to add this field for Route53 records to be updated. data.secretAccessKey: You need to add this field for Route53 records to be updated.  Note: All of the above values must be base64 encoded. If you skip this it will hurt you later.\nSUBSCRIPTION_ID=$(az account list -o json | jq -r \u0026#39;.[] | select(.isDefault == true) | .id\u0026#39;) TENANT_ID=$(az account show -o tsv --query \u0026#39;tenantId\u0026#39;) ACCESS_KEY_ID=$(aws configure get aws_access_key_id) SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key) sed -i \\ -e \u0026#34;s@base64(uuid-of-subscription)@$(echo $SUBSCRIPTION_ID | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-tenant)@$(echo \u0026#34;$TENANT_ID\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(uuid-of-client)@$(echo \u0026#34;${CLIENT_ID:?\u0026#34;CLIENT_ID is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;s@base64(client-secret)@$(echo \u0026#34;${CLIENT_SECRET:?\u0026#34;CLIENT_SECRET is missing\u0026#34;}\u0026#34; | tr -d \u0026#39;\\n\u0026#39; | base64)@\u0026#34; \\ -e \u0026#34;\\$a\\ \\ accessKeyID: $(echo $ACCESS_KEY_ID | tr -d \u0026#39;\\n\u0026#39; | base64 )\u0026#34; \\ -e \u0026#34;\\$a\\ \\ secretAccessKey: $(echo $SECRET_ACCESS_KEY | tr -d \u0026#39;\\n\u0026#39; | base64 )\u0026#34; \\ example/70-secret-cloudprovider-azure.yaml After updating the fields, create the cloud provider secret:\nkubectl apply -f example/70-secret-cloudprovider-azure.yaml And create the SecretBinding resource to allow Gardener use that secret (example/80-secretbinding-cloudprovider-azure.yaml):\nsed -i \\ -e \u0026#39;s/# namespace: .*/ namespace: garden-dev/\u0026#39; \\ example/80-secretbinding-cloudprovider-azure.yaml kubectl apply -f example/80-secretbinding-cloudprovider-azure.yaml Check the logs in gardener-controller-manager, there should not be any problems reported.\nCreate the Shoot resource Update the fields in example/90-deprecated-shoot-azure.yaml:\n spec.cloud.region: eastus (this must match the seed cluster\u0026rsquo;s region) spec.dns.domain: This is used to specify the base domain for your api (and other in the future) endpoint(s). For example when johndoe-azure.garden-dev.your.domain.here is used as a value, then your apiserver is available at api.johndoe-azure.garden-dev.your.domain.here spec.dns.hostedZoneID: This field doesn\u0026rsquo;t exist in the example you need to add this field and place the Route53 Hosted Zone ID.  HOSTED_ZONE_DOMAIN=$(aws route53 get-hosted-zone --id /hostedzone/${HOSTED_ZONE_ID:?\u0026#34;HOSTED_ZONE_ID is missing\u0026#34;} --query \u0026#39;HostedZone.Name\u0026#39; --output text) SHOOT_DOMAIN=\u0026#34;johndoe-azure.garden-dev.${HOSTED_ZONE_DOMAIN%%.}\u0026#34; KUBE_LEGO_EMAIL=$(git config user.email) sed -i \\ -e \u0026#34;s/region: westeurope/region: eastus/\u0026#34; \\ -e \u0026#34;s/domain: johndoe-azure.garden-dev.example.com/domain: $SHOOT_DOMAIN/\u0026#34; \\ -e \u0026#34;/domain:/a\\ \\ \\ \\ hostedZoneID: $HOSTED_ZONE_ID\u0026#34; \\ -e \u0026#34;s/email: john.doe@example.com/email: $KUBE_LEGO_EMAIL/\u0026#34; \\ example/90-deprecated-shoot-azure.yaml And let\u0026rsquo;s create the Shoot resource:\nkubectl apply -f example/90-deprecated-shoot-azure.yaml After creating the Shoot resource, gardener-controller-manager will pick it up and start provisioning the Shoot cluster.\n$ kubectl get -f example/90-deprecated-shoot-azure.yaml NAME CLOUDPROFILE VERSION SEED DOMAIN OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE johndoe-azure azure 1.12.3 azure johndoe-azure.garden-dev.your.domain.here Processing 15 \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; 16s Follow the logs in your console with gardener-controller-manager, starting like below you\u0026rsquo;ll see plenty of Waiting and Executing, etc. logs and many tasks will keep repeating:\ntime=\u0026#34;2018-06-09T07:35:45Z\u0026#34; level=info msg=\u0026#34;[SHOOT RECONCILE] garden-dev/johndoe-azure\u0026#34; time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Starting flow Shoot cluster creation\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).botanist.Shoot.Components.DNS.External{Provider/Entry}.Deploy\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployNamespace\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployKubeAPIServerService\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).DeployBackupNamespaceFromShoot\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:46Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:51Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:51Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).MoveBackupTerraformResources\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Executing (*Botanist).WaitUntilKubeAPIServerServiceIsReady\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:52Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:56Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:57Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:35:57Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:01Z\u0026#34; level=info msg=\u0026#34;Waiting for Terraform validation Pod \u0026#39;johndoe-azure.external-dns.tf-pod-d8f66\u0026#39; to be completed...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:02Z\u0026#34; level=info msg=\u0026#34;Waiting until the kube-apiserver service is ready...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure time=\u0026#34;2018-06-09T07:36:02Z\u0026#34; level=info msg=\u0026#34;Waiting until the backup-infrastructure has been reconciled in the Garden cluster...\u0026#34; opid=VIBBBGFx shoot=garden-dev/johndoe-azure ... At this stage you should be waiting for a while until the Shoot cluster is provisioned and initial resources are deployed.\nDuring the provisioning you can also check output of these commands to have a better understanding about what\u0026rsquo;s going on in the seed cluster:\n$ gardenctl ls shoots projects: - project: garden-dev shoots: - johndoe-azure $ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: johndoe-azure health: Unknown status: lastOperation: description: Executing DeployKubeAddonManager, ReconcileMachines. lastUpdateTime: 2018-06-09 08:40:20 +0100 IST progress: 74 state: Processing type: Create $ kubectl -n garden-dev get shoot johndoe-azure NAMESPACE NAME SEED DOMAIN VERSION CONTROL NODES SYSTEM LATEST garden-dev johndoe-azure azure johndoe-azure.garden-dev.your.domain.here 1.10.1 True True True Succeeded $ kubectl -n garden-dev describe shoot johndoe-azure ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Reconciling 1h gardener-controller-manager [BrXWiztO] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [rBFsfwU5] Reconciling Shoot cluster state Normal Reconciling 59m gardener-controller-manager [2HAbm45D] Reconciling Shoot cluster state Normal Reconciling 48m gardener-controller-manager [S1QA0ksz] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [lvcSKy1Q] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [MddMyk8W] Reconciling Shoot cluster state Normal Reconciling 47m gardener-controller-manager [XDAAWABd] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [6HYH9Psz] Reconciling Shoot cluster state Normal Reconciling 46m gardener-controller-manager [rhL38ym4] Reconciling Shoot cluster state Normal Reconciling 35m gardener-controller-manager [BOt4Nvso] Reconciling Shoot cluster state Normal Reconciling 35m gardener-controller-manager [JPtmXmxD] Reconciling Shoot cluster state Normal Reconciling 34m gardener-controller-manager [ldHsVA6G] Reconciling Shoot cluster state Normal Reconciled 31m gardener-controller-manager [ldHsVA6G] Reconciled Shoot cluster state Normal Reconciling 26m gardener-controller-manager [yBh2IBOF] Reconciling Shoot cluster state Normal Reconciled 24m gardener-controller-manager [yBh2IBOF] Reconciled Shoot cluster state Normal Reconciling 16m gardener-controller-manager [bqmFtHUA] Reconciling Shoot cluster state Normal Reconciled 14m gardener-controller-manager [bqmFtHUA] Reconciled Shoot cluster state Normal Reconciling 6m gardener-controller-manager [7QgHE5CH] Reconciling Shoot cluster state Normal Reconciled 3m gardener-controller-manager [7QgHE5CH] Reconciled Shoot cluster state Check Shoot cluster:\n$ gardenctl target garden dev KUBECONFIG=/Users/user/.kube/config $ gardenctl target project garden-dev $ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ gardenctl kubectl cluster-info Kubernetes master is running at https://api.johndoe-azure.garden-dev.your.domain.here CoreDNS is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubernetes-dashboard is running at https://api.johndoe-azure.garden-dev.your.domain.here/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. Cluster Resources After Shoot is Created After the Shoot has been created the summary of the resources in the AKS cluster handled by Gardener will be something like this:\nnon-namespaced resources CloudProfile: azure Project: dev Namespace: garden-dev Seed: azure # cloud.profile:azure, cloud.region:eastus, secretRef.name:seed-azure, secretRef.namespace: garden Namespace: garden Secret: seed-azure # aks credentials, kubeconfig # No other resources with any kind handled by Gardener # Gardener components as well lives in this namespace Namespace: garden-dev # maps to \u0026#34;project:dev\u0026#34; in Gardener Secret: core-azure # credentials for aks + aws (for route53) SecretBinding: core-azure # secretRef.name:core-azure Shoot: johndoe-azure # seed:azure, secretBindingRef.name:core-azure Namespace: shoot--dev--johndoe-azure # These are automatically created once Shoot resource is created AzureMachineClass: shoot--dev--johndoe-azure-cpu-worker-8506a MachineDeployment: shoot--dev--johndoe-azure-cpu-worker MachineSet: shoot--dev--johndoe-azure-cpu-worker-849bbbf75 Machine: shoot--dev--johndoe-azure-cpu-worker-849bbbf75-b42vh BackupInfra: shoot--dev--johndoe-azure--c1b3b # seed:azure, shootUID: shoot.status.UID. # Many other resources created as part of shoot cluster, # but only above ones are handled by Gardener Namespace: backup--shoot--dev--johndoe-azure--c1b3b # Secrets and configMap having info related to backup infrastructure # are created by Gardener. Troubleshooting Shoot Creation Issues For any issue happening during Shoot provisioning, you can consult the gardener-controller-manager logs, or the state in the shoot resource, gardenctl also provides a command to check Shoot cluster states:\n# check gardener-controller-manager logs kubectl -n garden logs -f deployment/gardener-controller-manager # kubectl describe can provide you a human readable output of # same information in below gardenctl command. kubectl -n garden-dev describe shoot johndoe-azure # also try cheking the machine-controller-manager logs of the shoot kubectl logs -n shoot--dev--johndoe-azure deployment/machine-controller-manager With gardenctl:\n$ gardenctl ls issues issues: - project: garden-dev seed: azure shoot: health: Ready status: johndoe-azure lastError: \u0026#34;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: \u0026#39;(*Botanist).Shoot.Components.DNS.External{Provider/Entry}.Destroy\u0026#39; returned \u0026#39;Terraform execution ... lastOperation: description: \u0026#34;Failed to reconcile Shoot cluster state: Errors occurred during flow execution: \u0026#39;(*Botanist).Shoot.Components.DNS.External{Provider/Entry}.Destroy\u0026#39; returned \u0026#39;Terraform ... lastUpdateTime: 2018-06-03 09:48:00 +0100 IST progress: 100 state: Failed type: Reconcile Access Shoot cluster The gardenctl tool provides a convenient wrapper to operate on both cluster and cloud providers, here are some commands you can run\n# select target shoot cluster gardenctl ls gardens gardenctl target garden dev gardenctl ls projects gardenctl target shoot johndoe-azure # issue Azure client (az) commands on target shoot gardenctl az aks list # issue kubectl commands on target shoot gardenctl kubectl -- version --short # \u0026#39;--\u0026#39; is required if you want to # pass any args starting with \u0026#39;-\u0026#39; # open prometheus, alertmanager, grafana without having to find # the user/pass for each gardenctl show prometheus gardenctl show grafana gardenctl show alertmanager Easiest way to obtain kubeconfig of the shoot cluster:\n$ gardenctl target shoot johndoe-azure KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ export KUBECONFIG=/Users/user/.garden/cache/projects/garden-dev/johndoe-azure/kubeconfig.yaml $ # From now on your local kubectl will be operating on target shoot $ kubectl cluster-info # will show your shoot cluster info $ unset KUBECONFIG # reset to your default kubectl The shoot cluster\u0026rsquo;s kubeconfig is being kept in a secret in the project namespace:\nkubectl -n shoot--dev--johndoe-azure get secret kubecfg -o jsonpath=\u0026#39;{.data.kubeconfig}\u0026#39; | base64 -D \u0026gt; /tmp/johndoe-azure-kubeconfig.yaml export KUBECONFIG=/tmp/johndoe-azure-kubeconfig.yaml Delete Shoot cluster Deleting a Shoot cluster is not straight forward, and this is to protect users from undesired/accidental cluster deletion. One has to place some special annotations to get a Shoot cluster removed. We use the hack/usage/delete script for this purpose.\nPlease refer to Creating / Deleting a Shoot cluster document for more details.\nhack/delete shoot johndoe-azure garden-dev "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/dns/","title":"DNSProvider and DNSEntry resources","tags":[],"description":"","content":"Contract: DNSProvider and DNSEntry resources Every shoot cluster requires external DNS records that are publicly resolvable. The management of these DNS records requires provider-specific knowledge which is to be developed outside of the Gardener\u0026rsquo;s core repository.\nWhat does Gardener create DNS records for? Internal domain name Every shoot cluster\u0026rsquo;s kube-apiserver running in the seed is exposed via a load balancer that has a public endpoint (IP or hostname). This endpoint is used by end-users and also by system components (that are running in another network, e.g., the kubelet or kube-proxy) to talk to the cluster. In order to be robust against changes of this endpoint (e.g., caused due to re-creation of the load balancer or move of the control plane to another seed cluster) Gardener creates a so-called internal domain name for every shoot cluster. The internal domain name is a publicly resolvable DNS record that points to the load balancer of the kube-apiserver. Gardener uses this domain name in the kubeconfigs of all system components (instead of writing the load balancer endpoint directly into it. This way Gardener does not need to recreate all the kubeconfigs if the endpoint changes - it just needs to update the DNS record.\nExternal domain name The internal domain name is not configurable by end-users directly but dictated by the Gardener administrator. However, end-users usually prefer to have another DNS name, maybe even using their own domain sometimes to access their Kubernetes clusters. Gardener supports that by creating another DNS record, named external domain name, that actually points to the internal domain name. The kubeconfig handed out to end-users does contain this external domain name, i.e., users can access their clusters with the DNS name they like to.\nAs not every end-user has an own domain it is possible for Gardener administrators to configure so-called default domains. If configured, shoots that do not specify a domain explicitly get an external domain name based on a default domain (unless explicitly stated that this shoot should not get an external domain name (.spec.dns.provider=unmanaged).\nDomain name for ingress (deprecated) Gardener allows to deploy a nginx-ingress-controller into a shoot cluster (deprecated). This controller is exposed via a public load balancer (again, either IP or hostname). Gardener creates a wildcard DNS record pointing to this load balancer. Ingress resources can later use this wildcard DNS record to expose underlying applications.\nWhat needs to be implemented to support a new DNS provider? As part of the shoot flow Gardener will create two special resources in the seed cluster that need to be reconciled by an extension controller. The first resource (DNSProvider) is a declaration of a DNS provider (e.g., aws-route53, google-clouddns, \u0026hellip;) with a reference to a Secret object that contains the provider-specific credentials in order to talk to the provider\u0026rsquo;s API. It also allows to specify two lists of domains that shall be allowed or disallowed to be used for DNS entries:\n---apiVersion:v1kind:Secretmetadata:name:aws-credentialsnamespace:defaulttype:Opaquedata:# aws-route53 specific credentials here---apiVersion:dns.gardener.cloud/v1alpha1kind:DNSProvidermetadata:name:my-aws-accountnamespace:defaultspec:type:aws-route53secretRef:name:aws-credentialsdomains:include:- dev.my-fancy-domain.comexclude:- staging.my-fancy-domain.com- prod.my-fancy-domain.comWhen reconciling this resource the DNS controller has to read information about available DNS zones to figure out which domains can actually be supported by the provided credentials. Based on the constraints given in the DNSProvider resources .spec.domains.{include|exclude} fields it shall later only allow certain DNS entries. Gardener waits until the status indicates that the registration went well:\napiVersion:dns.gardener.cloud/v1alpha1kind:DNSProvider...status:state:Readymessage:everythingokOther possible states are Pending, Error, and Invalid. The DNS controller may provide an explanation of the .status.state in the .status.message field.\nNow Gardener may create DNSEntry objects that represent the ask to create an actual external DNS record:\n---apiVersion:dns.gardener.cloud/v1alpha1kind:DNSEntrymetadata:name:dnsnamespace:defaultspec:dnsName:apiserver.cluster1.dev.my-fancy-domain.comttl:600targets:- 8.8.8.8It has to be automatically determined whether the to-be-created DNS record is of type A or CNAME. The spec shall also allow the creation of TXT records, e.g.:\n---apiVersion:dns.gardener.cloud/v1alpha1kind:DNSEntrymetadata:name:dnsnamespace:defaultspec:dnsName:data.apiserver.cluster1.dev.my-fancy-domain.comttl:120text:| content for the DNS TXT recordThe status section of this resource looks similar like the DNSProvider's. Gardener is (as of today) only evaluating the .status.state and .status.message fields.\nReferences and additional resources  DNSProvider and DNSEntry API (Golang specification) external-dns-management project in Gardener\u0026rsquo;s GitHub organization  "},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/dns/","title":"DNSProvider and DNSEntry resources","tags":[],"description":"","content":"Contract: DNSProvider and DNSEntry resources Every shoot cluster requires external DNS records that are publicly resolvable. The management of these DNS records requires provider-specific knowledge which is to be developed outside of the Gardener\u0026rsquo;s core repository.\nWhat does Gardener create DNS records for? Internal domain name Every shoot cluster\u0026rsquo;s kube-apiserver running in the seed is exposed via a load balancer that has a public endpoint (IP or hostname). This endpoint is used by end-users and also by system components (that are running in another network, e.g., the kubelet or kube-proxy) to talk to the cluster. In order to be robust against changes of this endpoint (e.g., caused due to re-creation of the load balancer or move of the control plane to another seed cluster) Gardener creates a so-called internal domain name for every shoot cluster. The internal domain name is a publicly resolvable DNS record that points to the load balancer of the kube-apiserver. Gardener uses this domain name in the kubeconfigs of all system components (instead of writing the load balancer endpoint directly into it. This way Gardener does not need to recreate all the kubeconfigs if the endpoint changes - it just needs to update the DNS record.\nExternal domain name The internal domain name is not configurable by end-users directly but dictated by the Gardener administrator. However, end-users usually prefer to have another DNS name, maybe even using their own domain sometimes to access their Kubernetes clusters. Gardener supports that by creating another DNS record, named external domain name, that actually points to the internal domain name. The kubeconfig handed out to end-users does contain this external domain name, i.e., users can access their clusters with the DNS name they like to.\nAs not every end-user has an own domain it is possible for Gardener administrators to configure so-called default domains. If configured, shoots that do not specify a domain explicitly get an external domain name based on a default domain (unless explicitly stated that this shoot should not get an external domain name (.spec.dns.provider=unmanaged).\nDomain name for ingress (deprecated) Gardener allows to deploy a nginx-ingress-controller into a shoot cluster (deprecated). This controller is exposed via a public load balancer (again, either IP or hostname). Gardener creates a wildcard DNS record pointing to this load balancer. Ingress resources can later use this wildcard DNS record to expose underlying applications.\nWhat needs to be implemented to support a new DNS provider? As part of the shoot flow Gardener will create two special resources in the seed cluster that need to be reconciled by an extension controller. The first resource (DNSProvider) is a declaration of a DNS provider (e.g., aws-route53, google-clouddns, \u0026hellip;) with a reference to a Secret object that contains the provider-specific credentials in order to talk to the provider\u0026rsquo;s API. It also allows to specify two lists of domains that shall be allowed or disallowed to be used for DNS entries:\n---apiVersion:v1kind:Secretmetadata:name:aws-credentialsnamespace:defaulttype:Opaquedata:# aws-route53 specific credentials here---apiVersion:dns.gardener.cloud/v1alpha1kind:DNSProvidermetadata:name:my-aws-accountnamespace:defaultspec:type:aws-route53secretRef:name:aws-credentialsdomains:include:- dev.my-fancy-domain.comexclude:- staging.my-fancy-domain.com- prod.my-fancy-domain.comWhen reconciling this resource the DNS controller has to read information about available DNS zones to figure out which domains can actually be supported by the provided credentials. Based on the constraints given in the DNSProvider resources .spec.domains.{include|exclude} fields it shall later only allow certain DNS entries. Gardener waits until the status indicates that the registration went well:\napiVersion:dns.gardener.cloud/v1alpha1kind:DNSProvider...status:state:Readymessage:everythingokOther possible states are Pending, Error, and Invalid. The DNS controller may provide an explanation of the .status.state in the .status.message field.\nNow Gardener may create DNSEntry objects that represent the ask to create an actual external DNS record:\n---apiVersion:dns.gardener.cloud/v1alpha1kind:DNSEntrymetadata:name:dnsnamespace:defaultspec:dnsName:apiserver.cluster1.dev.my-fancy-domain.comttl:600targets:- 8.8.8.8It has to be automatically determined whether the to-be-created DNS record is of type A or CNAME. The spec shall also allow the creation of TXT records, e.g.:\n---apiVersion:dns.gardener.cloud/v1alpha1kind:DNSEntrymetadata:name:dnsnamespace:defaultspec:dnsName:data.apiserver.cluster1.dev.my-fancy-domain.comttl:120text:| content for the DNS TXT recordThe status section of this resource looks similar like the DNSProvider's. Gardener is (as of today) only evaluating the .status.state and .status.message fields.\nReferences and additional resources  DNSProvider and DNSEntry API (Golang specification) external-dns-management project in Gardener\u0026rsquo;s GitHub organization  "},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/dns/","title":"DNSProvider and DNSEntry resources","tags":[],"description":"","content":"Contract: DNSProvider and DNSEntry resources Every shoot cluster requires external DNS records that are publicly resolvable. The management of these DNS records requires provider-specific knowledge which is to be developed outside of the Gardener\u0026rsquo;s core repository.\nWhat does Gardener create DNS records for? Internal domain name Every shoot cluster\u0026rsquo;s kube-apiserver running in the seed is exposed via a load balancer that has a public endpoint (IP or hostname). This endpoint is used by end-users and also by system components (that are running in another network, e.g., the kubelet or kube-proxy) to talk to the cluster. In order to be robust against changes of this endpoint (e.g., caused due to re-creation of the load balancer or move of the control plane to another seed cluster) Gardener creates a so-called internal domain name for every shoot cluster. The internal domain name is a publicly resolvable DNS record that points to the load balancer of the kube-apiserver. Gardener uses this domain name in the kubeconfigs of all system components (instead of writing the load balancer endpoint directly into it. This way Gardener does not need to recreate all the kubeconfigs if the endpoint changes - it just needs to update the DNS record.\nExternal domain name The internal domain name is not configurable by end-users directly but dictated by the Gardener administrator. However, end-users usually prefer to have another DNS name, maybe even using their own domain sometimes to access their Kubernetes clusters. Gardener supports that by creating another DNS record, named external domain name, that actually points to the internal domain name. The kubeconfig handed out to end-users does contain this external domain name, i.e., users can access their clusters with the DNS name they like to.\nAs not every end-user has an own domain it is possible for Gardener administrators to configure so-called default domains. If configured, shoots that do not specify a domain explicitly get an external domain name based on a default domain (unless explicitly stated that this shoot should not get an external domain name (.spec.dns.provider=unmanaged).\nDomain name for ingress (deprecated) Gardener allows to deploy a nginx-ingress-controller into a shoot cluster (deprecated). This controller is exposed via a public load balancer (again, either IP or hostname). Gardener creates a wildcard DNS record pointing to this load balancer. Ingress resources can later use this wildcard DNS record to expose underlying applications.\nWhat needs to be implemented to support a new DNS provider? As part of the shoot flow Gardener will create two special resources in the seed cluster that need to be reconciled by an extension controller. The first resource (DNSProvider) is a declaration of a DNS provider (e.g., aws-route53, google-clouddns, \u0026hellip;) with a reference to a Secret object that contains the provider-specific credentials in order to talk to the provider\u0026rsquo;s API. It also allows to specify two lists of domains that shall be allowed or disallowed to be used for DNS entries:\n---apiVersion:v1kind:Secretmetadata:name:aws-credentialsnamespace:defaulttype:Opaquedata:# aws-route53 specific credentials here---apiVersion:dns.gardener.cloud/v1alpha1kind:DNSProvidermetadata:name:my-aws-accountnamespace:defaultspec:type:aws-route53secretRef:name:aws-credentialsdomains:include:- dev.my-fancy-domain.comexclude:- staging.my-fancy-domain.com- prod.my-fancy-domain.comWhen reconciling this resource the DNS controller has to read information about available DNS zones to figure out which domains can actually be supported by the provided credentials. Based on the constraints given in the DNSProvider resources .spec.domains.{include|exclude} fields it shall later only allow certain DNS entries. Gardener waits until the status indicates that the registration went well:\napiVersion:dns.gardener.cloud/v1alpha1kind:DNSProvider...status:state:Readymessage:everythingokOther possible states are Pending, Error, and Invalid. The DNS controller may provide an explanation of the .status.state in the .status.message field.\nNow Gardener may create DNSEntry objects that represent the ask to create an actual external DNS record:\n---apiVersion:dns.gardener.cloud/v1alpha1kind:DNSEntrymetadata:name:dnsnamespace:defaultspec:dnsName:apiserver.cluster1.dev.my-fancy-domain.comttl:600targets:- 8.8.8.8It has to be automatically determined whether the to-be-created DNS record is of type A or CNAME. The spec shall also allow the creation of TXT records, e.g.:\n---apiVersion:dns.gardener.cloud/v1alpha1kind:DNSEntrymetadata:name:dnsnamespace:defaultspec:dnsName:data.apiserver.cluster1.dev.my-fancy-domain.comttl:120text:| content for the DNS TXT recordThe status section of this resource looks similar like the DNSProvider's. Gardener is (as of today) only evaluating the .status.state and .status.message fields.\nReferences and additional resources  DNSProvider and DNSEntry API (Golang specification) external-dns-management project in Gardener\u0026rsquo;s GitHub organization  "},{"uri":"https://gardener.cloud/documentation/guides/applications/dockerfile_pitfall/","title":"Dockerfile pitfalls","tags":[],"description":"Common Dockerfile pitfalls","content":"Using latest tag for an image Many Dockerfiles use the FROM package:latest pattern at the top of their Dockerfiles to pull the latest image from a Docker registry.\nBad Dockerfile FROMalpineWhile simple, using the latest tag for an image means that your build can suddenly break if that image gets updated. This can lead to problems where everything builds fine locally (because your local cache thinks it is the latest) while a build server may fail, because some Pipelines makes a clean pull on every build. Additionally, troubleshooting can prove to be difficult, since the maintainer of the Dockerfile didn\u0026rsquo;t actually make any changes.\nGood Dockerfile A digest takes the place of the tag when pulling an image. This will ensure your Dockerfile remains immutable.\nFROMalpine@sha256:7043076348bf5040220df6ad703798fd8593a0918d06d3ce30c6c93be117e430Running apt/apk/yum update Running apt-get install is one of those things virtually every Debian-based Dockerfile will have to satiate some external package requirements your code needs to run. But, using apt-get as an example, comes with its own problems.\napt-get upgrade\nThis will update all your packages to their latests versions, which can be bad because it prevents your Dockerfile from creating consistent, immutable builds.\napt-get update in a different line than running your apt-get install command.\nRunning apt-get update as a single line entry will get cached by the build and won\u0026rsquo;t actually run every time you need to run apt-get install. Instead, make sure you run apt-get update in the same line with all the packages to ensure all are updated correctly.\nAvoid big container images Building small container image will reduce the time needed to start or restart pods. An image based on the popular Alpine Linux project is much smaller than most distribution based images (~5MB). For most popular languages and products, there are usually an official Alpine Linux image, e.g. golang, nodejs and postgres.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE postgres 9.6.9-alpine 6583932564f8 13 days ago 39.26 MB postgres 9.6 d92dad241eff 13 days ago 235.4 MB postgres 10.4-alpine 93797b0f31f4 13 days ago 39.56 MB In addition, for compiled languages such as Go or C++ which does not requires build time tooling during runtime, it is recommended to avoid build time tooling in the final images. With Docker\u0026rsquo;s support for multi-stages builds this can be easily achieved with minimal effort. Such an example can be found here.\nGoogle\u0026rsquo;s distroless image is also a good base image.\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/applications/dockerfile_pitfall/","title":"Dockerfile pitfalls","tags":[],"description":"Common Dockerfile pitfalls","content":"Using latest tag for an image Many Dockerfiles use the FROM package:latest pattern at the top of their Dockerfiles to pull the latest image from a Docker registry.\nBad Dockerfile FROMalpineWhile simple, using the latest tag for an image means that your build can suddenly break if that image gets updated. This can lead to problems where everything builds fine locally (because your local cache thinks it is the latest) while a build server may fail, because some Pipelines makes a clean pull on every build. Additionally, troubleshooting can prove to be difficult, since the maintainer of the Dockerfile didn\u0026rsquo;t actually make any changes.\nGood Dockerfile A digest takes the place of the tag when pulling an image. This will ensure your Dockerfile remains immutable.\nFROMalpine@sha256:7043076348bf5040220df6ad703798fd8593a0918d06d3ce30c6c93be117e430Running apt/apk/yum update Running apt-get install is one of those things virtually every Debian-based Dockerfile will have to satiate some external package requirements your code needs to run. But, using apt-get as an example, comes with its own problems.\napt-get upgrade\nThis will update all your packages to their latests versions, which can be bad because it prevents your Dockerfile from creating consistent, immutable builds.\napt-get update in a different line than running your apt-get install command.\nRunning apt-get update as a single line entry will get cached by the build and won\u0026rsquo;t actually run every time you need to run apt-get install. Instead, make sure you run apt-get update in the same line with all the packages to ensure all are updated correctly.\nAvoid big container images Building small container image will reduce the time needed to start or restart pods. An image based on the popular Alpine Linux project is much smaller than most distribution based images (~5MB). For most popular languages and products, there are usually an official Alpine Linux image, e.g. golang, nodejs and postgres.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE postgres 9.6.9-alpine 6583932564f8 13 days ago 39.26 MB postgres 9.6 d92dad241eff 13 days ago 235.4 MB postgres 10.4-alpine 93797b0f31f4 13 days ago 39.56 MB In addition, for compiled languages such as Go or C++ which does not requires build time tooling during runtime, it is recommended to avoid build time tooling in the final images. With Docker\u0026rsquo;s support for multi-stages builds this can be easily achieved with minimal effort. Such an example can be found here.\nGoogle\u0026rsquo;s distroless image is also a good base image.\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/applications/dockerfile_pitfall/","title":"Dockerfile pitfalls","tags":[],"description":"Common Dockerfile pitfalls","content":"Using latest tag for an image Many Dockerfiles use the FROM package:latest pattern at the top of their Dockerfiles to pull the latest image from a Docker registry.\nBad Dockerfile FROMalpineWhile simple, using the latest tag for an image means that your build can suddenly break if that image gets updated. This can lead to problems where everything builds fine locally (because your local cache thinks it is the latest) while a build server may fail, because some Pipelines makes a clean pull on every build. Additionally, troubleshooting can prove to be difficult, since the maintainer of the Dockerfile didn\u0026rsquo;t actually make any changes.\nGood Dockerfile A digest takes the place of the tag when pulling an image. This will ensure your Dockerfile remains immutable.\nFROMalpine@sha256:7043076348bf5040220df6ad703798fd8593a0918d06d3ce30c6c93be117e430Running apt/apk/yum update Running apt-get install is one of those things virtually every Debian-based Dockerfile will have to satiate some external package requirements your code needs to run. But, using apt-get as an example, comes with its own problems.\napt-get upgrade\nThis will update all your packages to their latests versions, which can be bad because it prevents your Dockerfile from creating consistent, immutable builds.\napt-get update in a different line than running your apt-get install command.\nRunning apt-get update as a single line entry will get cached by the build and won\u0026rsquo;t actually run every time you need to run apt-get install. Instead, make sure you run apt-get update in the same line with all the packages to ensure all are updated correctly.\nAvoid big container images Building small container image will reduce the time needed to start or restart pods. An image based on the popular Alpine Linux project is much smaller than most distribution based images (~5MB). For most popular languages and products, there are usually an official Alpine Linux image, e.g. golang, nodejs and postgres.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE postgres 9.6.9-alpine 6583932564f8 13 days ago 39.26 MB postgres 9.6 d92dad241eff 13 days ago 235.4 MB postgres 10.4-alpine 93797b0f31f4 13 days ago 39.56 MB In addition, for compiled languages such as Go or C++ which does not requires build time tooling during runtime, it is recommended to avoid build time tooling in the final images. With Docker\u0026rsquo;s support for multi-stages builds this can be easily achieved with minimal effort. Such an example can be found here.\nGoogle\u0026rsquo;s distroless image is also a good base image.\n"},{"uri":"https://gardener.cloud/contribute/docs/","title":"Documentation","tags":[],"description":"","content":"Contributing Documentation You are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions:\n Contributions must be licensed under the Creative Commons Attribution 4.0 International License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.  "},{"uri":"https://gardener.cloud/contribute/docs/","title":"Documentation","tags":[],"description":"","content":"Contributing Documentation You are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions:\n Contributions must be licensed under the Creative Commons Attribution 4.0 International License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.  "},{"uri":"https://gardener.cloud/contribute/docs/","title":"Documentation","tags":[],"description":"","content":"Contributing Documentation You are welcome to contribute documentation to Gardener.\nThe following rules govern documentation contributions:\n Contributions must be licensed under the Creative Commons Attribution 4.0 International License You need to sign the Contributor License Agreement. We are using CLA assistant providing a click-through workflow for accepting the CLA. For company contributors additionally the company needs to sign a corporate license agreement. See the following sections for details.  "},{"uri":"https://gardener.cloud/documentation/tutorials/dynamic-pvc/","title":"Dynamic Volume Provisioning","tags":[],"description":"Running a Postgres database on Kubernetes and dynamically provision and mount the storage volumes needed by the database","content":"Introduction The example shows how to run a postgres database on Kubernetes and how to dynamically provision and mount the storage volumes needed by the database\nRun postgres database Define the following Kubernetes resources in a yaml file\n PersistentVolumeClaim (PVC) Deployment  PersistentVolumeClaim apiVersion:v1kind:PersistentVolumeClaimmetadata:name:postgresdb-pvcspec:accessModes:- ReadWriteOnceresources:requests:storage:9GistorageClassName:\u0026#39;default\u0026#39;This defines a PVC using storage class default. Storage classes abstract from the underlying storage provider as well as other parameters, like disk-type (e.g.; solid-state vs standard disks).\nThe default storage class has annotation {\u0026ldquo;storageclass.kubernetes.io/is-default-class\u0026rdquo;:\u0026ldquo;true\u0026rdquo;}.\n$ kubectl describe sc default Name: default IsDefaultClass: Yes Annotations: kubectl.kubernetes.io/last-applied-configuration={\u0026#34;apiVersion\u0026#34;:\u0026#34;storage.k8s.io/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;StorageClass\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;true\u0026#34;},\u0026#34;labels\u0026#34;:{\u0026#34;addonmanager.kubernetes.io/mode\u0026#34;:\u0026#34;Exists\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;\u0026#34;},\u0026#34;parameters\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;gp2\u0026#34;},\u0026#34;provisioner\u0026#34;:\u0026#34;kubernetes.io/aws-ebs\u0026#34;} ,storageclass.kubernetes.io/is-default-class=true Provisioner: kubernetes.io/aws-ebs Parameters: type=gp2 AllowVolumeExpansion: \u0026lt;unset\u0026gt; MountOptions: \u0026lt;none\u0026gt; ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: \u0026lt;none\u0026gt; A Persistent Volume is automatically created when it is dynamically provisioned. In following example, the PVC is defined as \u0026ldquo;postgresdb-pvc\u0026rdquo;, and a corresponding PV \u0026ldquo;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb\u0026rdquo; is created and associated with pvc automatically.\n$ kubectl create -f .\\postgres_deployment.yaml persistentvolumeclaim \u0026#34;postgresdb-pvc\u0026#34; created $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 3s $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE postgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 8s Notice that the RECLAIM POLICY is Delete (default value), which is one of the two reclaim policies, the other one is Retain. (A third policy Recycle has been deprecated). In case of Delete, the PV is deleted automatically when the PVC is removed, and the data on the PVC will also be lost.\nOn the other hand, PV with Retain policy will not be deleted when the PVC is removed, and moved to Release status, so that data can be recovered by Administrators later.\nYou can use the kubectl patch command to change the reclaim policy as described here here or use kubectl edit pv \u0026lt;pv-name\u0026gt; to edit online as below:\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 44m # change the relcaim policy from \u0026#34;Delete\u0026#34; to \u0026#34;Retain\u0026#34; $ kubectl edit pv pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb persistentvolume \u0026#34;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb\u0026#34; edited # check the reclaim policy afterwards $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 45m Deployment Once a PVC is created, you can use it in your container via volumes.persistentVolumeClaim.claimName. In below example, pvc postgresdb-pvc is mounted as readable and writable, and in volumeMounts two paths in the container are mounted to subfolders in the volume.\napiVersion:apps/v1kind:Deploymentmetadata:name:postgresnamespace:defaultlabels:app:postgresannotations:deployment.kubernetes.io/revision:\u0026#34;1\u0026#34;spec:replicas:1strategy:type:RollingUpdaterollingUpdate:maxUnavailable:1maxSurge:1selector:matchLabels:app:postgrestemplate:metadata:name:postgreslabels:app:postgresspec:containers:- name:postgresimage:\u0026#34;cpettech.docker.repositories.sap.ondemand.com/jtrack_postgres:howto\u0026#34;env:- name:POSTGRES_USERvalue:postgres- name:POSTGRES_PASSWORDvalue:p5FVqfuJFrM42cVX9muQXxrC3r8S9yn0zqWnFR6xCoPqxqVQ- name:POSTGRES_INITDB_XLOGDIRvalue:\u0026#34;/var/log/postgresql/logs\u0026#34;ports:- containerPort:5432volumeMounts:- mountPath:/var/lib/postgresql/dataname:postgre-dbsubPath:data# https://github.com/kubernetes/website/pull/2292. Solve the issue of crashing initdb due to non-empty directory (i.e. lost+found)- mountPath:/var/log/postgresql/logsname:postgre-dbsubPath:logsvolumes:- name:postgre-dbpersistentVolumeClaim:claimName:postgresdb-pvcreadOnly:falseimagePullSecrets:- name:cpettechregistryTo check the mount points in the container:\n$ kubectl get po NAME READY STATUS RESTARTS AGE postgres-7f485fd768-c5jf9 1/1 Running 0 32m $ kubectl exec -it postgres-7f485fd768-c5jf9 bash root@postgres-7f485fd768-c5jf9:/# ls /var/lib/postgresql/data/ base pg_clog pg_dynshmem pg_ident.conf pg_multixact pg_replslot pg_snapshots pg_stat_tmp pg_tblspc PG_VERSION postgresql.auto.conf postmaster.opts global pg_commit_ts pg_hba.conf pg_logical pg_notify pg_serial pg_stat pg_subtrans pg_twophase pg_xlog postgresql.conf postmaster.pid root@postgres-7f485fd768-c5jf9:/# ls /var/log/postgresql/logs/ 000000010000000000000001 archive_status Deleting a PersistentVolumeClaim In case of \u0026ldquo;Delete\u0026rdquo; policy, deleting a PVC will also delete its associated PV. If \u0026ldquo;Retain\u0026rdquo; is the reclaim policy, the PV will change status from Bound to Released when PVC is deleted.\n# Check pvc and pv before deletion $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE postgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 50m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 50m # delete pvc $ kubectl delete pvc postgresdb-pvc persistentvolumeclaim \u0026#34;postgresdb-pvc\u0026#34; deleted # pv changed to status \u0026#34;Released\u0026#34; $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Released default/postgresdb-pvc default 51m "},{"uri":"https://gardener.cloud/v1.12.8/tutorials/dynamic-pvc/","title":"Dynamic Volume Provisioning","tags":[],"description":"Running a Postgres database on Kubernetes and dynamically provision and mount the storage volumes needed by the database","content":"Introduction The example shows how to run a postgres database on Kubernetes and how to dynamically provision and mount the storage volumes needed by the database\nRun postgres database Define the following Kubernetes resources in a yaml file\n PersistentVolumeClaim (PVC) Deployment  PersistentVolumeClaim apiVersion:v1kind:PersistentVolumeClaimmetadata:name:postgresdb-pvcspec:accessModes:- ReadWriteOnceresources:requests:storage:9GistorageClassName:\u0026#39;default\u0026#39;This defines a PVC using storage class default. Storage classes abstract from the underlying storage provider as well as other parameters, like disk-type (e.g.; solid-state vs standard disks).\nThe default storage class has annotation {\u0026ldquo;storageclass.kubernetes.io/is-default-class\u0026rdquo;:\u0026ldquo;true\u0026rdquo;}.\n$ kubectl describe sc default Name: default IsDefaultClass: Yes Annotations: kubectl.kubernetes.io/last-applied-configuration={\u0026#34;apiVersion\u0026#34;:\u0026#34;storage.k8s.io/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;StorageClass\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;true\u0026#34;},\u0026#34;labels\u0026#34;:{\u0026#34;addonmanager.kubernetes.io/mode\u0026#34;:\u0026#34;Exists\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;\u0026#34;},\u0026#34;parameters\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;gp2\u0026#34;},\u0026#34;provisioner\u0026#34;:\u0026#34;kubernetes.io/aws-ebs\u0026#34;} ,storageclass.kubernetes.io/is-default-class=true Provisioner: kubernetes.io/aws-ebs Parameters: type=gp2 AllowVolumeExpansion: \u0026lt;unset\u0026gt; MountOptions: \u0026lt;none\u0026gt; ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: \u0026lt;none\u0026gt; A Persistent Volume is automatically created when it is dynamically provisioned. In following example, the PVC is defined as \u0026ldquo;postgresdb-pvc\u0026rdquo;, and a corresponding PV \u0026ldquo;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb\u0026rdquo; is created and associated with pvc automatically.\n$ kubectl create -f .\\postgres_deployment.yaml persistentvolumeclaim \u0026#34;postgresdb-pvc\u0026#34; created $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 3s $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE postgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 8s Notice that the RECLAIM POLICY is Delete (default value), which is one of the two reclaim policies, the other one is Retain. (A third policy Recycle has been deprecated). In case of Delete, the PV is deleted automatically when the PVC is removed, and the data on the PVC will also be lost.\nOn the other hand, PV with Retain policy will not be deleted when the PVC is removed, and moved to Release status, so that data can be recovered by Administrators later.\nYou can use the kubectl patch command to change the reclaim policy as described here here or use kubectl edit pv \u0026lt;pv-name\u0026gt; to edit online as below:\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 44m # change the relcaim policy from \u0026#34;Delete\u0026#34; to \u0026#34;Retain\u0026#34; $ kubectl edit pv pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb persistentvolume \u0026#34;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb\u0026#34; edited # check the reclaim policy afterwards $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 45m Deployment Once a PVC is created, you can use it in your container via volumes.persistentVolumeClaim.claimName. In below example, pvc postgresdb-pvc is mounted as readable and writable, and in volumeMounts two paths in the container are mounted to subfolders in the volume.\napiVersion:apps/v1kind:Deploymentmetadata:name:postgresnamespace:defaultlabels:app:postgresannotations:deployment.kubernetes.io/revision:\u0026#34;1\u0026#34;spec:replicas:1strategy:type:RollingUpdaterollingUpdate:maxUnavailable:1maxSurge:1selector:matchLabels:app:postgrestemplate:metadata:name:postgreslabels:app:postgresspec:containers:- name:postgresimage:\u0026#34;cpettech.docker.repositories.sap.ondemand.com/jtrack_postgres:howto\u0026#34;env:- name:POSTGRES_USERvalue:postgres- name:POSTGRES_PASSWORDvalue:p5FVqfuJFrM42cVX9muQXxrC3r8S9yn0zqWnFR6xCoPqxqVQ- name:POSTGRES_INITDB_XLOGDIRvalue:\u0026#34;/var/log/postgresql/logs\u0026#34;ports:- containerPort:5432volumeMounts:- mountPath:/var/lib/postgresql/dataname:postgre-dbsubPath:data# https://github.com/kubernetes/website/pull/2292. Solve the issue of crashing initdb due to non-empty directory (i.e. lost+found)- mountPath:/var/log/postgresql/logsname:postgre-dbsubPath:logsvolumes:- name:postgre-dbpersistentVolumeClaim:claimName:postgresdb-pvcreadOnly:falseimagePullSecrets:- name:cpettechregistryTo check the mount points in the container:\n$ kubectl get po NAME READY STATUS RESTARTS AGE postgres-7f485fd768-c5jf9 1/1 Running 0 32m $ kubectl exec -it postgres-7f485fd768-c5jf9 bash root@postgres-7f485fd768-c5jf9:/# ls /var/lib/postgresql/data/ base pg_clog pg_dynshmem pg_ident.conf pg_multixact pg_replslot pg_snapshots pg_stat_tmp pg_tblspc PG_VERSION postgresql.auto.conf postmaster.opts global pg_commit_ts pg_hba.conf pg_logical pg_notify pg_serial pg_stat pg_subtrans pg_twophase pg_xlog postgresql.conf postmaster.pid root@postgres-7f485fd768-c5jf9:/# ls /var/log/postgresql/logs/ 000000010000000000000001 archive_status Deleting a PersistentVolumeClaim In case of \u0026ldquo;Delete\u0026rdquo; policy, deleting a PVC will also delete its associated PV. If \u0026ldquo;Retain\u0026rdquo; is the reclaim policy, the PV will change status from Bound to Released when PVC is deleted.\n# Check pvc and pv before deletion $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE postgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 50m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 50m # delete pvc $ kubectl delete pvc postgresdb-pvc persistentvolumeclaim \u0026#34;postgresdb-pvc\u0026#34; deleted # pv changed to status \u0026#34;Released\u0026#34; $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Released default/postgresdb-pvc default 51m "},{"uri":"https://gardener.cloud/v1.13.2/tutorials/dynamic-pvc/","title":"Dynamic Volume Provisioning","tags":[],"description":"Running a Postgres database on Kubernetes and dynamically provision and mount the storage volumes needed by the database","content":"Introduction The example shows how to run a postgres database on Kubernetes and how to dynamically provision and mount the storage volumes needed by the database\nRun postgres database Define the following Kubernetes resources in a yaml file\n PersistentVolumeClaim (PVC) Deployment  PersistentVolumeClaim apiVersion:v1kind:PersistentVolumeClaimmetadata:name:postgresdb-pvcspec:accessModes:- ReadWriteOnceresources:requests:storage:9GistorageClassName:\u0026#39;default\u0026#39;This defines a PVC using storage class default. Storage classes abstract from the underlying storage provider as well as other parameters, like disk-type (e.g.; solid-state vs standard disks).\nThe default storage class has annotation {\u0026ldquo;storageclass.kubernetes.io/is-default-class\u0026rdquo;:\u0026ldquo;true\u0026rdquo;}.\n$ kubectl describe sc default Name: default IsDefaultClass: Yes Annotations: kubectl.kubernetes.io/last-applied-configuration={\u0026#34;apiVersion\u0026#34;:\u0026#34;storage.k8s.io/v1beta1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;StorageClass\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;true\u0026#34;},\u0026#34;labels\u0026#34;:{\u0026#34;addonmanager.kubernetes.io/mode\u0026#34;:\u0026#34;Exists\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;\u0026#34;},\u0026#34;parameters\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;gp2\u0026#34;},\u0026#34;provisioner\u0026#34;:\u0026#34;kubernetes.io/aws-ebs\u0026#34;} ,storageclass.kubernetes.io/is-default-class=true Provisioner: kubernetes.io/aws-ebs Parameters: type=gp2 AllowVolumeExpansion: \u0026lt;unset\u0026gt; MountOptions: \u0026lt;none\u0026gt; ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: \u0026lt;none\u0026gt; A Persistent Volume is automatically created when it is dynamically provisioned. In following example, the PVC is defined as \u0026ldquo;postgresdb-pvc\u0026rdquo;, and a corresponding PV \u0026ldquo;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb\u0026rdquo; is created and associated with pvc automatically.\n$ kubectl create -f .\\postgres_deployment.yaml persistentvolumeclaim \u0026#34;postgresdb-pvc\u0026#34; created $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 3s $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE postgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 8s Notice that the RECLAIM POLICY is Delete (default value), which is one of the two reclaim policies, the other one is Retain. (A third policy Recycle has been deprecated). In case of Delete, the PV is deleted automatically when the PVC is removed, and the data on the PVC will also be lost.\nOn the other hand, PV with Retain policy will not be deleted when the PVC is removed, and moved to Release status, so that data can be recovered by Administrators later.\nYou can use the kubectl patch command to change the reclaim policy as described here here or use kubectl edit pv \u0026lt;pv-name\u0026gt; to edit online as below:\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Delete Bound default/postgresdb-pvc default 44m # change the relcaim policy from \u0026#34;Delete\u0026#34; to \u0026#34;Retain\u0026#34; $ kubectl edit pv pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb persistentvolume \u0026#34;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb\u0026#34; edited # check the reclaim policy afterwards $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 45m Deployment Once a PVC is created, you can use it in your container via volumes.persistentVolumeClaim.claimName. In below example, pvc postgresdb-pvc is mounted as readable and writable, and in volumeMounts two paths in the container are mounted to subfolders in the volume.\napiVersion:apps/v1kind:Deploymentmetadata:name:postgresnamespace:defaultlabels:app:postgresannotations:deployment.kubernetes.io/revision:\u0026#34;1\u0026#34;spec:replicas:1strategy:type:RollingUpdaterollingUpdate:maxUnavailable:1maxSurge:1selector:matchLabels:app:postgrestemplate:metadata:name:postgreslabels:app:postgresspec:containers:- name:postgresimage:\u0026#34;cpettech.docker.repositories.sap.ondemand.com/jtrack_postgres:howto\u0026#34;env:- name:POSTGRES_USERvalue:postgres- name:POSTGRES_PASSWORDvalue:p5FVqfuJFrM42cVX9muQXxrC3r8S9yn0zqWnFR6xCoPqxqVQ- name:POSTGRES_INITDB_XLOGDIRvalue:\u0026#34;/var/log/postgresql/logs\u0026#34;ports:- containerPort:5432volumeMounts:- mountPath:/var/lib/postgresql/dataname:postgre-dbsubPath:data# https://github.com/kubernetes/website/pull/2292. Solve the issue of crashing initdb due to non-empty directory (i.e. lost+found)- mountPath:/var/log/postgresql/logsname:postgre-dbsubPath:logsvolumes:- name:postgre-dbpersistentVolumeClaim:claimName:postgresdb-pvcreadOnly:falseimagePullSecrets:- name:cpettechregistryTo check the mount points in the container:\n$ kubectl get po NAME READY STATUS RESTARTS AGE postgres-7f485fd768-c5jf9 1/1 Running 0 32m $ kubectl exec -it postgres-7f485fd768-c5jf9 bash root@postgres-7f485fd768-c5jf9:/# ls /var/lib/postgresql/data/ base pg_clog pg_dynshmem pg_ident.conf pg_multixact pg_replslot pg_snapshots pg_stat_tmp pg_tblspc PG_VERSION postgresql.auto.conf postmaster.opts global pg_commit_ts pg_hba.conf pg_logical pg_notify pg_serial pg_stat pg_subtrans pg_twophase pg_xlog postgresql.conf postmaster.pid root@postgres-7f485fd768-c5jf9:/# ls /var/log/postgresql/logs/ 000000010000000000000001 archive_status Deleting a PersistentVolumeClaim In case of \u0026ldquo;Delete\u0026rdquo; policy, deleting a PVC will also delete its associated PV. If \u0026ldquo;Retain\u0026rdquo; is the reclaim policy, the PV will change status from Bound to Released when PVC is deleted.\n# Check pvc and pv before deletion $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE postgresdb-pvc Bound pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO default 50m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Bound default/postgresdb-pvc default 50m # delete pvc $ kubectl delete pvc postgresdb-pvc persistentvolumeclaim \u0026#34;postgresdb-pvc\u0026#34; deleted # pv changed to status \u0026#34;Released\u0026#34; $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb 9Gi RWO Retain Released default/postgresdb-pvc default 51m "},{"uri":"https://gardener.cloud/documentation/contribute/10_code/13_env/","title":"Environment","tags":[],"description":"","content":"Preparing the Setup Conceptually, all Gardener components are designated to run inside as a Pod inside a Kubernetes cluster. The API server extends the Kubernetes API via the user-aggregated API server concepts. However, if you want to develop it, you may want to work locally with the Gardener without building a Docker image and deploying it to a cluster each and every time. That means that the Gardener runs outside a Kubernetes cluster which requires providing a Kubeconfig in your local filesystem and point the Gardener to it when starting it (see below).\nFurther details could be found in\n Principles of Kubernetes, and its components Kubernetes Development Guide Architecture of Gardener  This setup is based on minikube, a Kubernetes cluster running on a single node. Docker for Desktop and kind are also supported.\nInstalling Golang environment Install latest version of Golang. For MacOS you could use Homebrew:\nbrew install golang For other OS, please check Go installation documentation.\nInstalling kubectl and helm As already mentioned in the introduction, the communication with the Gardener happens via the Kubernetes (Garden) cluster it is targeting. To interact with that cluster, you need to install kubectl. Please make sure that the version of kubectl is at least v1.11.x.\nOn MacOS run\nbrew install kubernetes-cli Please check the kubectl installation documentation for other OS.\nYou may also need to develop Helm charts or interact with Tiller using the Helm CLI:\nOn MacOS run\nbrew install kubernetes-helm On other OS please check the Helm installation documentation.\nInstalling git We use git as VCS which you need to install.\nOn MacOS run\nbrew install git On other OS, please check the Git installation documentation.\nInstalling openvpn We use OpenVPN to establish network connectivity from the control plane running in the Seed cluster to the Shoot\u0026rsquo;s worker nodes running in private networks. To harden the security we need to generate another secret to encrypt the network traffic (details). Please install the openvpn binary. On MacOS run\nbrew install openvpn export PATH=$(brew --prefix openvpn)/sbin:$PATH On other OS, please check the OpenVPN downloads page.\nInstalling Minikube You\u0026rsquo;ll need to have minikube installed and running.\n Note: Gardener is working only with self-contained kubeconfig files because of security issue. You can configure your minikube to create self-contained kubeconfig files via:\nminikube config set embed-certs true  Alternatively, you can also install Docker for Desktop and kind.\nIn case you want to use the \u0026ldquo;Docker for Mac Kubernetes\u0026rdquo; or if you want to build Docker images for the Gardener you have to install Docker itself. On MacOS, please use Docker for MacOS which can be downloaded here.\nOn other OS, please check the Docker installation documentation.\nInstalling iproute2 iproute2 provides a collection of utilities for network administration and configuration.\nOn MacOS run\nbrew install iproute2mac Installing yaml2json and jq go get -u github.com/bronze1man/yaml2json brew install jq [MacOS only] Install GNU core utilities When running on MacOS you have to install the GNU core utilities:\nbrew install coreutils gnu-sed This will create symbolic links for the GNU utilities with g prefix in /usr/local/bin, e.g., gsed or gbase64. To allow using them without the g prefix please put /usr/local/opt/coreutils/libexec/gnubin at the beginning of your PATH environment variable, e.g., export PATH=/usr/local/opt/coreutils/libexec/gnubin:$PATH.\n[Windows] WSL2 Apart from Linux distributions and MacOS, the local gardener setup can also run on the Windows Subsystem for Linux 2.\nWhile WSL1, plain docker for windows and various Linux distributions and local Kubernetes environments may be supported, this setup was verified with:\n WSL2 Docker Desktop WSL2 Engine Ubuntu 18.04 LTS on WSL2 Nodeless local garden (see below)  The Gardener repository and all the above-mentioned tools (git, golang, kubectl, \u0026hellip;) should be installed in your WSL2 distro, according to the distribution-specific Linux installation instructions.\n[Optional] Installing gcloud SDK In case you have to create a new release or a new hotfix of the Gardener you have to push the resulting Docker image into a Docker registry. Currently, we are using the Google Container Registry (this could change in the future). Please follow the official installation instructions from Google.\nLocal Gardener setup This setup is only meant to be used for developing purposes, which means that only the control plane of the Gardener cluster is running on your machine.\nGet the sources Clone the repository from GitHub.\ngit clone git@github.com:gardener/gardener.git cd gardener Start the Gardener :warning: Before you start developing, please ensure to comply with the following requirements:\n You have understood the principles of Kubernetes, and its components, what their purpose is and how they interact with each other. You have understood the architecture of Gardener, and what the various clusters are used for.  Start a local kubernetes cluster For the development of Gardener you need some kind of Kubernetes cluster, which can be used as a \u0026ldquo;garden\u0026rdquo; cluster. I.e. you need a Kubernetes API server on which you can register a APIService Gardener\u0026rsquo;s own Extension API Server.\nFor this you can use a standard tool from the community to setup a local cluster like minikube, kind or the Kubernetes Cluster feature in Docker for Desktop.\nHowever, if you develop and run Gardener\u0026rsquo;s components locally, you don\u0026rsquo;t actually a fully fledged Kubernetes Cluster, i.e. you don\u0026rsquo;t actually need to run Pods on it. If you want to use a more lightweight approach for development purposes, you can use the \u0026ldquo;nodeless Garden cluster setup\u0026rdquo; residing in hack/local-garden. This is the easiest way to get your Gardener development setup up and running.\nUsing the nodeless cluster setup\nSetting up a local nodeless Garden cluster is quite simple. The only prerequisite is a running docker daemon. Just use the provided Makefile rules to start your local Garden:\nmake local-garden-up [...] Starting gardener-dev kube-etcd cluster..! Starting gardener-dev kube-apiserver..! Starting gardener-dev kube-controller-manager..! Starting gardener-dev gardener-etcd cluster..! namespace/garden created clusterrole.rbac.authorization.k8s.io/gardener.cloud:admin created clusterrolebinding.rbac.authorization.k8s.io/front-proxy-client created [...] This will start all minimally required components of a Kubernetes cluster (etcd, kube-apiserver, kube-controller-manager) and an etcd Instance for the gardener-apiserver as Docker containers.\nTo tear down the local Garden cluster and remove the Docker containers, simply run:\nmake local-garden-down Using minikube\nAlternatively, spin up a cluster with minikube with this command:\nminikube start --embed-certs # `--embed-certs` can be omitted if minikube has already been set to create self-contained kubeconfig files.  minikube v1.8.2 on Darwin 10.15.3  Creating virtualbox VM (CPUs=2, Memory=2048MB, Disk=20000MB) ... [...]  Done! Thank you for using minikube! Prepare the Gardener Now, that you have started your local cluster, we can go ahead and register the Gardener API Server. Just point your KUBECONFIG environment variable to the local cluster you created in the previous step and run:\nmake dev-setup Found Minikube ... namespace/garden created namespace/garden-dev created deployment.apps/etcd created service/etcd created service/gardener-apiserver created service/gardener-controller-manager created endpoints/gardener-apiserver created endpoints/gardener-controller-manager created apiservice.apiregistration.k8s.io/v1alpha1.core.gardener.cloud created apiservice.apiregistration.k8s.io/v1beta1.core.gardener.cloud created validatingwebhookconfiguration.admissionregistration.k8s.io/gardener-controller-manager created Optionally, you can switch off the Logging feature gate of Gardenlet to save resources:\nsed -i -e \u0026#39;s/Logging: true/Logging: false/g\u0026#39; dev/20-componentconfig-gardenlet.yaml The Gardener exposes the API servers of Shoot clusters via Kubernetes services of type LoadBalancer. In order to establish stable endpoints (robust against changes of the load balancer address), it creates DNS records pointing to these load balancer addresses. They are used internally and by all cluster components to communicate. You need to have control over a domain (or subdomain) for which these records will be created. Please provide an internal domain secret (see this for an example) which contains credentials with the proper privileges. Further information can be found here.\nkubectl apply -f example/10-secret-internal-domain-unmanaged.yaml secret/internal-domain-unmanaged created Run the Gardener Next, run the Gardener API Server, the Gardener Controller Manager (optionally), the Gardener Scheduler (optionally), and the Gardenlet in different terminal windows/panes using rules in the Makefile.\nmake start-apiserver Found Minikube ... I0306 15:23:51.044421 74536 plugins.go:84] Registered admission plugin \u0026#34;ResourceReferenceManager\u0026#34; I0306 15:23:51.044523 74536 plugins.go:84] Registered admission plugin \u0026#34;DeletionConfirmation\u0026#34; [...] I0306 15:23:51.626836 74536 secure_serving.go:116] Serving securely on [::]:8443 [...] (Optional) Now you are ready to launch the Gardener Controller Manager.\nmake start-controller-manager time=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Starting Gardener controller manager...\u0026#34; time=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Feature Gates: \u0026#34; time=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Starting HTTP server on 0.0.0.0:2718\u0026#34; time=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Acquired leadership, starting controllers.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Starting HTTPS server on 0.0.0.0:2719\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Found internal domain secret internal-domain-unmanaged for domain nip.io.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Successfully bootstrapped the Garden cluster.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Gardener controller manager (version 1.0.0-dev) initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;ControllerRegistration controller initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;SecretBinding controller initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Project controller initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Quota controller initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;CloudProfile controller initialized.\u0026#34; [...] (Optional) Now you are ready to launch the Gardener Scheduler.\nmake start-scheduler time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Starting Gardener scheduler ...\u0026#34; time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Starting HTTP server on 0.0.0.0:10251\u0026#34; time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Acquired leadership, starting scheduler.\u0026#34; time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Gardener scheduler initialized (with Strategy: SameRegion)\u0026#34; time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Scheduler controller initialized.\u0026#34; [...] (Optional) Now you are ready to launch the Gardenlet.\nmake start-gardenlet time=\u0026#34;2019-11-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Starting Gardenlet...\u0026#34; time=\u0026#34;2019-11-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Feature Gates: HVPA=true, Logging=true\u0026#34; time=\u0026#34;2019-11-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Acquired leadership, starting controllers.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Found internal domain secret internal-domain-unmanaged for domain nip.io.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Gardenlet (version 1.0.0-dev) initialized.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;ControllerInstallation controller initialized.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Shoot controller initialized.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Seed controller initialized.\u0026#34; [...] :warning: The Gardenlet will handle all your seeds for this development scenario, although, for productive usage it is recommended to run it once per seed, see this document for more information. See the Appendix on how to configure the Seed clusters for the local development scenario.\nPlease checkout the Gardener Extensions Manager to install extension controllers - make sure that you install all of them required for your local development. Also, please refer to this document for further information about how extensions are registered in case you want to use other versions than the latest releases.\nThe Gardener should now be ready to operate on Shoot resources. You can use\nkubectl get shoots No resources found. to operate against your local running Gardener API Server.\n Note: It may take several seconds until the minikube cluster recognizes that the Gardener API server has been started and is available. No resources found is the expected result of our initial development setup.\n Limitations of local development setup You can run Gardener (API server, controller manager, scheduler, gardenlet) against any local Kubernetes cluster, however, your seed and shoot clusters must be deployed to a \u0026ldquo;real\u0026rdquo; provider. Currently, it is not possible to run Gardener entirely isolated from any cloud provider. We are planning to support a setup that can run completely locally (see this for details), however, it does not yet exist. This means that - after you have setup Gardener - you need to register an external seed cluster (e.g., one created in AWS). Only after that step you can start creating shoot clusters with your locally running Gardener.\nSome time ago, we had a local setup based on VirtualBox/Vagrant. However, as we have progressed with the Extensibility epic we noticed that this implementation/setup does no longer fit into how we envision external providers to be. Moreover, it hid too many things and came with a bunch of limitations, making the development scenario too \u0026ldquo;artificial\u0026rdquo;:\n No integration with machine-controller-manager. The Shoot API Server is exposed via a NodePort. In a cloud setup a LoadBalancer would be used. It was not possible to create Shoot clusters consisting of more than one worker node. Cluster auto-scaling therefore is not supported. It was not possible to create two or more Shoot clusters in parallel. The communication between the Seed and the Shoot Clusters uses VPN tunnel. In this setup tunnels are not needed since all components run on localhost.  Additional information To make sure that a specific Seed cluster will be chosen, specify the .spec.seedName field (see here for an example Shoot manifest).\nPlease take a look at the example manifests folder to see which resource objects you need to install into your Garden cluster.\nAppendix Configure Seed clusters for local development When using the Gardenlet in a local development scenario with make start-gardenlet then the Gardenlet component configuration is setup with a seed selector that targets all available Seed clusters. However, a Seed resource needs to be configured to allow being reconciled by a Gardenlet which such a configuration.\nWhen deploying the Gardenlet to reconcile only one Seed cluster (using component configuration .seedConfig), the Gardenlet either needs to be supplied with a kubeconfig for the particular Seed cluster, or acquires one via bootstrapping. Having said that, if the Gardenlet is configured to manage multiple Seed clusters based on a label selector, it needs to fetch the kubeconfig of each Seed cluster at runtime from somewhere. That is why the Seed resource needs to be configured with an additional secret reference that contains the kubeconfig of the Seed cluster.\nCreate a secret containing the base64 encoded kubeconfig of the Seed cluster (the scope of the permissions should be identical to the kubeconfig that the Gardenlet creates during bootstrapping - for now, cluster-admin privileges are recommended).\nCreate the secret in the Garden cluster:\napiVersion:v1kind:Secretmetadata:name:sweet-seednamespace:gardentype:Opaquedata:kubeconfig:\u0026lt;base64-seed-kubeconfig\u0026gt;.Adjust the Seed resource to reference the secret in spec.secretRef like so:\napiVersion:core.gardener.cloud/v1beta1kind:Seedmetadata:name:my-sweet-seedspec:...secretRef:name:sweet-seednamespace:garden"},{"uri":"https://gardener.cloud/v1.12.8/contribute/10_code/13_env/","title":"Environment","tags":[],"description":"","content":"Preparing the Setup Conceptually, all Gardener components are designated to run inside as a Pod inside a Kubernetes cluster. The API server extends the Kubernetes API via the user-aggregated API server concepts. However, if you want to develop it, you may want to work locally with the Gardener without building a Docker image and deploying it to a cluster each and every time. That means that the Gardener runs outside a Kubernetes cluster which requires providing a Kubeconfig in your local filesystem and point the Gardener to it when starting it (see below).\nFurther details could be found in\n Principles of Kubernetes, and its components Kubernetes Development Guide Architecture of Gardener  This setup is based on minikube, a Kubernetes cluster running on a single node. Docker for Desktop and kind are also supported.\nInstalling Golang environment Install latest version of Golang. For MacOS you could use Homebrew:\nbrew install golang For other OS, please check Go installation documentation.\nInstalling kubectl and helm As already mentioned in the introduction, the communication with the Gardener happens via the Kubernetes (Garden) cluster it is targeting. To interact with that cluster, you need to install kubectl. Please make sure that the version of kubectl is at least v1.11.x.\nOn MacOS run\nbrew install kubernetes-cli Please check the kubectl installation documentation for other OS.\nYou may also need to develop Helm charts or interact with Tiller using the Helm CLI:\nOn MacOS run\nbrew install kubernetes-helm On other OS please check the Helm installation documentation.\nInstalling git We use git as VCS which you need to install.\nOn MacOS run\nbrew install git On other OS, please check the Git installation documentation.\nInstalling openvpn We use OpenVPN to establish network connectivity from the control plane running in the Seed cluster to the Shoot\u0026rsquo;s worker nodes running in private networks. To harden the security we need to generate another secret to encrypt the network traffic (details). Please install the openvpn binary. On MacOS run\nbrew install openvpn export PATH=$(brew --prefix openvpn)/sbin:$PATH On other OS, please check the OpenVPN downloads page.\nInstalling Minikube You\u0026rsquo;ll need to have minikube installed and running.\n Note: Gardener is working only with self-contained kubeconfig files because of security issue. You can configure your minikube to create self-contained kubeconfig files via:\nminikube config set embed-certs true  Alternatively, you can also install Docker for Desktop and kind.\nIn case you want to use the \u0026ldquo;Docker for Mac Kubernetes\u0026rdquo; or if you want to build Docker images for the Gardener you have to install Docker itself. On MacOS, please use Docker for MacOS which can be downloaded here.\nOn other OS, please check the Docker installation documentation.\nInstalling iproute2 iproute2 provides a collection of utilities for network administration and configuration.\nOn MacOS run\nbrew install iproute2mac Installing yaml2json and jq go get -u github.com/bronze1man/yaml2json brew install jq [MacOS only] Install GNU core utilities When running on MacOS you have to install the GNU core utilities:\nbrew install coreutils gnu-sed This will create symbolic links for the GNU utilities with g prefix in /usr/local/bin, e.g., gsed or gbase64. To allow using them without the g prefix please put /usr/local/opt/coreutils/libexec/gnubin at the beginning of your PATH environment variable, e.g., export PATH=/usr/local/opt/coreutils/libexec/gnubin:$PATH.\n[Windows] WSL2 Apart from Linux distributions and MacOS, the local gardener setup can also run on the Windows Subsystem for Linux 2.\nWhile WSL1, plain docker for windows and various Linux distributions and local Kubernetes environments may be supported, this setup was verified with:\n WSL2 Docker Desktop WSL2 Engine Ubuntu 18.04 LTS on WSL2 Nodeless local garden (see below)  The Gardener repository and all the above-mentioned tools (git, golang, kubectl, \u0026hellip;) should be installed in your WSL2 distro, according to the distribution-specific Linux installation instructions.\n[Optional] Installing gcloud SDK In case you have to create a new release or a new hotfix of the Gardener you have to push the resulting Docker image into a Docker registry. Currently, we are using the Google Container Registry (this could change in the future). Please follow the official installation instructions from Google.\nLocal Gardener setup This setup is only meant to be used for developing purposes, which means that only the control plane of the Gardener cluster is running on your machine.\nGet the sources Clone the repository from GitHub.\ngit clone git@github.com:gardener/gardener.git cd gardener Start the Gardener :warning: Before you start developing, please ensure to comply with the following requirements:\n You have understood the principles of Kubernetes, and its components, what their purpose is and how they interact with each other. You have understood the architecture of Gardener, and what the various clusters are used for.  Start a local kubernetes cluster For the development of Gardener you need some kind of Kubernetes cluster, which can be used as a \u0026ldquo;garden\u0026rdquo; cluster. I.e. you need a Kubernetes API server on which you can register a APIService Gardener\u0026rsquo;s own Extension API Server.\nFor this you can use a standard tool from the community to setup a local cluster like minikube, kind or the Kubernetes Cluster feature in Docker for Desktop.\nHowever, if you develop and run Gardener\u0026rsquo;s components locally, you don\u0026rsquo;t actually a fully fledged Kubernetes Cluster, i.e. you don\u0026rsquo;t actually need to run Pods on it. If you want to use a more lightweight approach for development purposes, you can use the \u0026ldquo;nodeless Garden cluster setup\u0026rdquo; residing in hack/local-garden. This is the easiest way to get your Gardener development setup up and running.\nUsing the nodeless cluster setup\nSetting up a local nodeless Garden cluster is quite simple. The only prerequisite is a running docker daemon. Just use the provided Makefile rules to start your local Garden:\nmake local-garden-up [...] Starting gardener-dev kube-etcd cluster..! Starting gardener-dev kube-apiserver..! Starting gardener-dev kube-controller-manager..! Starting gardener-dev gardener-etcd cluster..! namespace/garden created clusterrole.rbac.authorization.k8s.io/gardener.cloud:admin created clusterrolebinding.rbac.authorization.k8s.io/front-proxy-client created [...] This will start all minimally required components of a Kubernetes cluster (etcd, kube-apiserver, kube-controller-manager) and an etcd Instance for the gardener-apiserver as Docker containers.\nTo tear down the local Garden cluster and remove the Docker containers, simply run:\nmake local-garden-down Using minikube\nAlternatively, spin up a cluster with minikube with this command:\nminikube start --embed-certs # `--embed-certs` can be omitted if minikube has already been set to create self-contained kubeconfig files.  minikube v1.8.2 on Darwin 10.15.3  Creating virtualbox VM (CPUs=2, Memory=2048MB, Disk=20000MB) ... [...]  Done! Thank you for using minikube! Prepare the Gardener Now, that you have started your local cluster, we can go ahead and register the Gardener API Server. Just point your KUBECONFIG environment variable to the local cluster you created in the previous step and run:\nmake dev-setup Found Minikube ... namespace/garden created namespace/garden-dev created deployment.apps/etcd created service/etcd created service/gardener-apiserver created service/gardener-controller-manager created endpoints/gardener-apiserver created endpoints/gardener-controller-manager created apiservice.apiregistration.k8s.io/v1alpha1.core.gardener.cloud created apiservice.apiregistration.k8s.io/v1beta1.core.gardener.cloud created validatingwebhookconfiguration.admissionregistration.k8s.io/gardener-controller-manager created Optionally, you can switch off the Logging feature gate of Gardenlet to save resources:\nsed -i -e \u0026#39;s/Logging: true/Logging: false/g\u0026#39; dev/20-componentconfig-gardenlet.yaml The Gardener exposes the API servers of Shoot clusters via Kubernetes services of type LoadBalancer. In order to establish stable endpoints (robust against changes of the load balancer address), it creates DNS records pointing to these load balancer addresses. They are used internally and by all cluster components to communicate. You need to have control over a domain (or subdomain) for which these records will be created. Please provide an internal domain secret (see this for an example) which contains credentials with the proper privileges. Further information can be found here.\nkubectl apply -f example/10-secret-internal-domain-unmanaged.yaml secret/internal-domain-unmanaged created Run the Gardener Next, run the Gardener API Server, the Gardener Controller Manager (optionally), the Gardener Scheduler (optionally), and the Gardenlet in different terminal windows/panes using rules in the Makefile.\nmake start-apiserver Found Minikube ... I0306 15:23:51.044421 74536 plugins.go:84] Registered admission plugin \u0026#34;ResourceReferenceManager\u0026#34; I0306 15:23:51.044523 74536 plugins.go:84] Registered admission plugin \u0026#34;DeletionConfirmation\u0026#34; [...] I0306 15:23:51.626836 74536 secure_serving.go:116] Serving securely on [::]:8443 [...] (Optional) Now you are ready to launch the Gardener Controller Manager.\nmake start-controller-manager time=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Starting Gardener controller manager...\u0026#34; time=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Feature Gates: \u0026#34; time=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Starting HTTP server on 0.0.0.0:2718\u0026#34; time=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Acquired leadership, starting controllers.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Starting HTTPS server on 0.0.0.0:2719\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Found internal domain secret internal-domain-unmanaged for domain nip.io.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Successfully bootstrapped the Garden cluster.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Gardener controller manager (version 1.0.0-dev) initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;ControllerRegistration controller initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;SecretBinding controller initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Project controller initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Quota controller initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;CloudProfile controller initialized.\u0026#34; [...] (Optional) Now you are ready to launch the Gardener Scheduler.\nmake start-scheduler time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Starting Gardener scheduler ...\u0026#34; time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Starting HTTP server on 0.0.0.0:10251\u0026#34; time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Acquired leadership, starting scheduler.\u0026#34; time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Gardener scheduler initialized (with Strategy: SameRegion)\u0026#34; time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Scheduler controller initialized.\u0026#34; [...] (Optional) Now you are ready to launch the Gardenlet.\nmake start-gardenlet time=\u0026#34;2019-11-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Starting Gardenlet...\u0026#34; time=\u0026#34;2019-11-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Feature Gates: HVPA=true, Logging=true\u0026#34; time=\u0026#34;2019-11-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Acquired leadership, starting controllers.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Found internal domain secret internal-domain-unmanaged for domain nip.io.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Gardenlet (version 1.0.0-dev) initialized.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;ControllerInstallation controller initialized.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Shoot controller initialized.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Seed controller initialized.\u0026#34; [...] :warning: The Gardenlet will handle all your seeds for this development scenario, although, for productive usage it is recommended to run it once per seed, see this document for more information. See the Appendix on how to configure the Seed clusters for the local development scenario.\nPlease checkout the Gardener Extensions Manager to install extension controllers - make sure that you install all of them required for your local development. Also, please refer to this document for further information about how extensions are registered in case you want to use other versions than the latest releases.\nThe Gardener should now be ready to operate on Shoot resources. You can use\nkubectl get shoots No resources found. to operate against your local running Gardener API Server.\n Note: It may take several seconds until the minikube cluster recognizes that the Gardener API server has been started and is available. No resources found is the expected result of our initial development setup.\n Limitations of local development setup You can run Gardener (API server, controller manager, scheduler, gardenlet) against any local Kubernetes cluster, however, your seed and shoot clusters must be deployed to a \u0026ldquo;real\u0026rdquo; provider. Currently, it is not possible to run Gardener entirely isolated from any cloud provider. We are planning to support a setup that can run completely locally (see this for details), however, it does not yet exist. This means that - after you have setup Gardener - you need to register an external seed cluster (e.g., one created in AWS). Only after that step you can start creating shoot clusters with your locally running Gardener.\nSome time ago, we had a local setup based on VirtualBox/Vagrant. However, as we have progressed with the Extensibility epic we noticed that this implementation/setup does no longer fit into how we envision external providers to be. Moreover, it hid too many things and came with a bunch of limitations, making the development scenario too \u0026ldquo;artificial\u0026rdquo;:\n No integration with machine-controller-manager. The Shoot API Server is exposed via a NodePort. In a cloud setup a LoadBalancer would be used. It was not possible to create Shoot clusters consisting of more than one worker node. Cluster auto-scaling therefore is not supported. It was not possible to create two or more Shoot clusters in parallel. The communication between the Seed and the Shoot Clusters uses VPN tunnel. In this setup tunnels are not needed since all components run on localhost.  Additional information To make sure that a specific Seed cluster will be chosen, specify the .spec.seedName field (see here for an example Shoot manifest).\nPlease take a look at the example manifests folder to see which resource objects you need to install into your Garden cluster.\nAppendix Configure Seed clusters for local development When using the Gardenlet in a local development scenario with make start-gardenlet then the Gardenlet component configuration is setup with a seed selector that targets all available Seed clusters. However, a Seed resource needs to be configured to allow being reconciled by a Gardenlet which such a configuration.\nWhen deploying the Gardenlet to reconcile only one Seed cluster (using component configuration .seedConfig), the Gardenlet either needs to be supplied with a kubeconfig for the particular Seed cluster, or acquires one via bootstrapping. Having said that, if the Gardenlet is configured to manage multiple Seed clusters based on a label selector, it needs to fetch the kubeconfig of each Seed cluster at runtime from somewhere. That is why the Seed resource needs to be configured with an additional secret reference that contains the kubeconfig of the Seed cluster.\nCreate a secret containing the base64 encoded kubeconfig of the Seed cluster (the scope of the permissions should be identical to the kubeconfig that the Gardenlet creates during bootstrapping - for now, cluster-admin privileges are recommended).\nCreate the secret in the Garden cluster:\napiVersion:v1kind:Secretmetadata:name:sweet-seednamespace:gardentype:Opaquedata:kubeconfig:\u0026lt;base64-seed-kubeconfig\u0026gt;.Adjust the Seed resource to reference the secret in spec.secretRef like so:\napiVersion:core.gardener.cloud/v1beta1kind:Seedmetadata:name:my-sweet-seedspec:...secretRef:name:sweet-seednamespace:garden"},{"uri":"https://gardener.cloud/v1.13.2/contribute/10_code/13_env/","title":"Environment","tags":[],"description":"","content":"Preparing the Setup Conceptually, all Gardener components are designated to run inside as a Pod inside a Kubernetes cluster. The API server extends the Kubernetes API via the user-aggregated API server concepts. However, if you want to develop it, you may want to work locally with the Gardener without building a Docker image and deploying it to a cluster each and every time. That means that the Gardener runs outside a Kubernetes cluster which requires providing a Kubeconfig in your local filesystem and point the Gardener to it when starting it (see below).\nFurther details could be found in\n Principles of Kubernetes, and its components Kubernetes Development Guide Architecture of Gardener  This setup is based on minikube, a Kubernetes cluster running on a single node. Docker for Desktop and kind are also supported.\nInstalling Golang environment Install latest version of Golang. For MacOS you could use Homebrew:\nbrew install golang For other OS, please check Go installation documentation.\nInstalling kubectl and helm As already mentioned in the introduction, the communication with the Gardener happens via the Kubernetes (Garden) cluster it is targeting. To interact with that cluster, you need to install kubectl. Please make sure that the version of kubectl is at least v1.11.x.\nOn MacOS run\nbrew install kubernetes-cli Please check the kubectl installation documentation for other OS.\nYou may also need to develop Helm charts or interact with Tiller using the Helm CLI:\nOn MacOS run\nbrew install kubernetes-helm On other OS please check the Helm installation documentation.\nInstalling git We use git as VCS which you need to install.\nOn MacOS run\nbrew install git On other OS, please check the Git installation documentation.\nInstalling openvpn We use OpenVPN to establish network connectivity from the control plane running in the Seed cluster to the Shoot\u0026rsquo;s worker nodes running in private networks. To harden the security we need to generate another secret to encrypt the network traffic (details). Please install the openvpn binary. On MacOS run\nbrew install openvpn export PATH=$(brew --prefix openvpn)/sbin:$PATH On other OS, please check the OpenVPN downloads page.\nInstalling Minikube You\u0026rsquo;ll need to have minikube installed and running.\n Note: Gardener is working only with self-contained kubeconfig files because of security issue. You can configure your minikube to create self-contained kubeconfig files via:\nminikube config set embed-certs true  Alternatively, you can also install Docker for Desktop and kind.\nIn case you want to use the \u0026ldquo;Docker for Mac Kubernetes\u0026rdquo; or if you want to build Docker images for the Gardener you have to install Docker itself. On MacOS, please use Docker for MacOS which can be downloaded here.\nOn other OS, please check the Docker installation documentation.\nInstalling iproute2 iproute2 provides a collection of utilities for network administration and configuration.\nOn MacOS run\nbrew install iproute2mac Installing yaml2json and jq go get -u github.com/bronze1man/yaml2json brew install jq [MacOS only] Install GNU core utilities When running on MacOS you have to install the GNU core utilities:\nbrew install coreutils gnu-sed This will create symbolic links for the GNU utilities with g prefix in /usr/local/bin, e.g., gsed or gbase64. To allow using them without the g prefix please put /usr/local/opt/coreutils/libexec/gnubin at the beginning of your PATH environment variable, e.g., export PATH=/usr/local/opt/coreutils/libexec/gnubin:$PATH.\n[Windows] WSL2 Apart from Linux distributions and MacOS, the local gardener setup can also run on the Windows Subsystem for Linux 2.\nWhile WSL1, plain docker for windows and various Linux distributions and local Kubernetes environments may be supported, this setup was verified with:\n WSL2 Docker Desktop WSL2 Engine Ubuntu 18.04 LTS on WSL2 Nodeless local garden (see below)  The Gardener repository and all the above-mentioned tools (git, golang, kubectl, \u0026hellip;) should be installed in your WSL2 distro, according to the distribution-specific Linux installation instructions.\n[Optional] Installing gcloud SDK In case you have to create a new release or a new hotfix of the Gardener you have to push the resulting Docker image into a Docker registry. Currently, we are using the Google Container Registry (this could change in the future). Please follow the official installation instructions from Google.\nLocal Gardener setup This setup is only meant to be used for developing purposes, which means that only the control plane of the Gardener cluster is running on your machine.\nGet the sources Clone the repository from GitHub.\ngit clone git@github.com:gardener/gardener.git cd gardener Start the Gardener :warning: Before you start developing, please ensure to comply with the following requirements:\n You have understood the principles of Kubernetes, and its components, what their purpose is and how they interact with each other. You have understood the architecture of Gardener, and what the various clusters are used for.  Start a local kubernetes cluster For the development of Gardener you need some kind of Kubernetes cluster, which can be used as a \u0026ldquo;garden\u0026rdquo; cluster. I.e. you need a Kubernetes API server on which you can register a APIService Gardener\u0026rsquo;s own Extension API Server.\nFor this you can use a standard tool from the community to setup a local cluster like minikube, kind or the Kubernetes Cluster feature in Docker for Desktop.\nHowever, if you develop and run Gardener\u0026rsquo;s components locally, you don\u0026rsquo;t actually a fully fledged Kubernetes Cluster, i.e. you don\u0026rsquo;t actually need to run Pods on it. If you want to use a more lightweight approach for development purposes, you can use the \u0026ldquo;nodeless Garden cluster setup\u0026rdquo; residing in hack/local-garden. This is the easiest way to get your Gardener development setup up and running.\nUsing the nodeless cluster setup\nSetting up a local nodeless Garden cluster is quite simple. The only prerequisite is a running docker daemon. Just use the provided Makefile rules to start your local Garden:\nmake local-garden-up [...] Starting gardener-dev kube-etcd cluster..! Starting gardener-dev kube-apiserver..! Starting gardener-dev kube-controller-manager..! Starting gardener-dev gardener-etcd cluster..! namespace/garden created clusterrole.rbac.authorization.k8s.io/gardener.cloud:admin created clusterrolebinding.rbac.authorization.k8s.io/front-proxy-client created [...] This will start all minimally required components of a Kubernetes cluster (etcd, kube-apiserver, kube-controller-manager) and an etcd Instance for the gardener-apiserver as Docker containers.\nTo tear down the local Garden cluster and remove the Docker containers, simply run:\nmake local-garden-down Using minikube\nAlternatively, spin up a cluster with minikube with this command:\nminikube start --embed-certs # `--embed-certs` can be omitted if minikube has already been set to create self-contained kubeconfig files.  minikube v1.8.2 on Darwin 10.15.3  Creating virtualbox VM (CPUs=2, Memory=2048MB, Disk=20000MB) ... [...]  Done! Thank you for using minikube! Prepare the Gardener Now, that you have started your local cluster, we can go ahead and register the Gardener API Server. Just point your KUBECONFIG environment variable to the local cluster you created in the previous step and run:\nmake dev-setup Found Minikube ... namespace/garden created namespace/garden-dev created deployment.apps/etcd created service/etcd created service/gardener-apiserver created service/gardener-controller-manager created endpoints/gardener-apiserver created endpoints/gardener-controller-manager created apiservice.apiregistration.k8s.io/v1alpha1.core.gardener.cloud created apiservice.apiregistration.k8s.io/v1beta1.core.gardener.cloud created validatingwebhookconfiguration.admissionregistration.k8s.io/gardener-controller-manager created Optionally, you can switch off the Logging feature gate of Gardenlet to save resources:\nsed -i -e \u0026#39;s/Logging: true/Logging: false/g\u0026#39; dev/20-componentconfig-gardenlet.yaml The Gardener exposes the API servers of Shoot clusters via Kubernetes services of type LoadBalancer. In order to establish stable endpoints (robust against changes of the load balancer address), it creates DNS records pointing to these load balancer addresses. They are used internally and by all cluster components to communicate. You need to have control over a domain (or subdomain) for which these records will be created. Please provide an internal domain secret (see this for an example) which contains credentials with the proper privileges. Further information can be found here.\nkubectl apply -f example/10-secret-internal-domain-unmanaged.yaml secret/internal-domain-unmanaged created Run the Gardener Next, run the Gardener API Server, the Gardener Controller Manager (optionally), the Gardener Scheduler (optionally), and the Gardenlet in different terminal windows/panes using rules in the Makefile.\nmake start-apiserver Found Minikube ... I0306 15:23:51.044421 74536 plugins.go:84] Registered admission plugin \u0026#34;ResourceReferenceManager\u0026#34; I0306 15:23:51.044523 74536 plugins.go:84] Registered admission plugin \u0026#34;DeletionConfirmation\u0026#34; [...] I0306 15:23:51.626836 74536 secure_serving.go:116] Serving securely on [::]:8443 [...] (Optional) Now you are ready to launch the Gardener Controller Manager.\nmake start-controller-manager time=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Starting Gardener controller manager...\u0026#34; time=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Feature Gates: \u0026#34; time=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Starting HTTP server on 0.0.0.0:2718\u0026#34; time=\u0026#34;2019-03-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Acquired leadership, starting controllers.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Starting HTTPS server on 0.0.0.0:2719\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Found internal domain secret internal-domain-unmanaged for domain nip.io.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Successfully bootstrapped the Garden cluster.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Gardener controller manager (version 1.0.0-dev) initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;ControllerRegistration controller initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;SecretBinding controller initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Project controller initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Quota controller initialized.\u0026#34; time=\u0026#34;2019-03-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;CloudProfile controller initialized.\u0026#34; [...] (Optional) Now you are ready to launch the Gardener Scheduler.\nmake start-scheduler time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Starting Gardener scheduler ...\u0026#34; time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Starting HTTP server on 0.0.0.0:10251\u0026#34; time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Acquired leadership, starting scheduler.\u0026#34; time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Gardener scheduler initialized (with Strategy: SameRegion)\u0026#34; time=\u0026#34;2019-05-02T16:31:50+02:00\u0026#34; level=info msg=\u0026#34;Scheduler controller initialized.\u0026#34; [...] (Optional) Now you are ready to launch the Gardenlet.\nmake start-gardenlet time=\u0026#34;2019-11-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Starting Gardenlet...\u0026#34; time=\u0026#34;2019-11-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Feature Gates: HVPA=true, Logging=true\u0026#34; time=\u0026#34;2019-11-06T15:24:17+02:00\u0026#34; level=info msg=\u0026#34;Acquired leadership, starting controllers.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Found internal domain secret internal-domain-unmanaged for domain nip.io.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Gardenlet (version 1.0.0-dev) initialized.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;ControllerInstallation controller initialized.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Shoot controller initialized.\u0026#34; time=\u0026#34;2019-11-06T15:24:18+02:00\u0026#34; level=info msg=\u0026#34;Seed controller initialized.\u0026#34; [...] :warning: The Gardenlet will handle all your seeds for this development scenario, although, for productive usage it is recommended to run it once per seed, see this document for more information. See the Appendix on how to configure the Seed clusters for the local development scenario.\nPlease checkout the Gardener Extensions Manager to install extension controllers - make sure that you install all of them required for your local development. Also, please refer to this document for further information about how extensions are registered in case you want to use other versions than the latest releases.\nThe Gardener should now be ready to operate on Shoot resources. You can use\nkubectl get shoots No resources found. to operate against your local running Gardener API Server.\n Note: It may take several seconds until the minikube cluster recognizes that the Gardener API server has been started and is available. No resources found is the expected result of our initial development setup.\n Limitations of local development setup You can run Gardener (API server, controller manager, scheduler, gardenlet) against any local Kubernetes cluster, however, your seed and shoot clusters must be deployed to a \u0026ldquo;real\u0026rdquo; provider. Currently, it is not possible to run Gardener entirely isolated from any cloud provider. We are planning to support a setup that can run completely locally (see this for details), however, it does not yet exist. This means that - after you have setup Gardener - you need to register an external seed cluster (e.g., one created in AWS). Only after that step you can start creating shoot clusters with your locally running Gardener.\nSome time ago, we had a local setup based on VirtualBox/Vagrant. However, as we have progressed with the Extensibility epic we noticed that this implementation/setup does no longer fit into how we envision external providers to be. Moreover, it hid too many things and came with a bunch of limitations, making the development scenario too \u0026ldquo;artificial\u0026rdquo;:\n No integration with machine-controller-manager. The Shoot API Server is exposed via a NodePort. In a cloud setup a LoadBalancer would be used. It was not possible to create Shoot clusters consisting of more than one worker node. Cluster auto-scaling therefore is not supported. It was not possible to create two or more Shoot clusters in parallel. The communication between the Seed and the Shoot Clusters uses VPN tunnel. In this setup tunnels are not needed since all components run on localhost.  Additional information To make sure that a specific Seed cluster will be chosen, specify the .spec.seedName field (see here for an example Shoot manifest).\nPlease take a look at the example manifests folder to see which resource objects you need to install into your Garden cluster.\nAppendix Configure Seed clusters for local development When using the Gardenlet in a local development scenario with make start-gardenlet then the Gardenlet component configuration is setup with a seed selector that targets all available Seed clusters. However, a Seed resource needs to be configured to allow being reconciled by a Gardenlet which such a configuration.\nWhen deploying the Gardenlet to reconcile only one Seed cluster (using component configuration .seedConfig), the Gardenlet either needs to be supplied with a kubeconfig for the particular Seed cluster, or acquires one via bootstrapping. Having said that, if the Gardenlet is configured to manage multiple Seed clusters based on a label selector, it needs to fetch the kubeconfig of each Seed cluster at runtime from somewhere. That is why the Seed resource needs to be configured with an additional secret reference that contains the kubeconfig of the Seed cluster.\nCreate a secret containing the base64 encoded kubeconfig of the Seed cluster (the scope of the permissions should be identical to the kubeconfig that the Gardenlet creates during bootstrapping - for now, cluster-admin privileges are recommended).\nCreate the secret in the Garden cluster:\napiVersion:v1kind:Secretmetadata:name:sweet-seednamespace:gardentype:Opaquedata:kubeconfig:\u0026lt;base64-seed-kubeconfig\u0026gt;.Adjust the Seed resource to reference the secret in spec.secretRef like so:\napiVersion:core.gardener.cloud/v1beta1kind:Seedmetadata:name:my-sweet-seedspec:...secretRef:name:sweet-seednamespace:garden"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/project-roles/","title":"Extending project roles","tags":[],"description":"","content":"Extending project roles The Project resource allows to specify a list of roles for every member (.spec.members[*].roles). There are a few standard roles defined by Gardener itself. Please consult this document for further information.\nHowever, extension controllers running in the garden cluster may also create CustomResourceDefinitions that project members might be able to CRUD. For this purpose Gardener also allows to specify extension roles.\nAn extension role is prefixed with extension:, e.g.\napiVersion:core.gardener.cloud/v1beta1kind:Projectmetadata:name:devspec:members:- apiGroup:rbac.authorization.k8s.iokind:Username:alice.doe@example.comrole:adminroles:- owner- extension:fooThe project controller will, for every extension role, create a ClusterRole with name name: gardener.cloud:extension:project:\u0026lt;projectName\u0026gt;:\u0026lt;roleName\u0026gt;, i.e., for above example: name: gardener.cloud:extension:project:dev:foo. This ClusterRole aggregates other ClusterRoles that are labeled with rbac.gardener.cloud/aggregate-to-extension-role=foo which might be created by extension controllers.\nExtension that might want to contribute to the core admin or viewer roles can use the labels rbac.gardener.cloud/aggregate-to-project-member=true or rbac.gardener.cloud/aggregate-to-project-viewer=true, respectively.\nPlease note that the names of the extension roles are restricted to 20 characters!\nMoreover, the project controller will also create a corresponding RoleBinding with the same name in the project namespace. It will automatically assign all members that are assigned to this extension role.\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/project-roles/","title":"Extending project roles","tags":[],"description":"","content":"Extending project roles The Project resource allows to specify a list of roles for every member (.spec.members[*].roles). There are a few standard roles defined by Gardener itself. Please consult this document for further information.\nHowever, extension controllers running in the garden cluster may also create CustomResourceDefinitions that project members might be able to CRUD. For this purpose Gardener also allows to specify extension roles.\nAn extension role is prefixed with extension:, e.g.\napiVersion:core.gardener.cloud/v1beta1kind:Projectmetadata:name:devspec:members:- apiGroup:rbac.authorization.k8s.iokind:Username:alice.doe@example.comrole:adminroles:- owner- extension:fooThe project controller will, for every extension role, create a ClusterRole with name name: gardener.cloud:extension:project:\u0026lt;projectName\u0026gt;:\u0026lt;roleName\u0026gt;, i.e., for above example: name: gardener.cloud:extension:project:dev:foo. This ClusterRole aggregates other ClusterRoles that are labeled with rbac.gardener.cloud/aggregate-to-extension-role=foo which might be created by extension controllers.\nExtension that might want to contribute to the core admin or viewer roles can use the labels rbac.gardener.cloud/aggregate-to-project-member=true or rbac.gardener.cloud/aggregate-to-project-viewer=true, respectively.\nPlease note that the names of the extension roles are restricted to 20 characters!\nMoreover, the project controller will also create a corresponding RoleBinding with the same name in the project namespace. It will automatically assign all members that are assigned to this extension role.\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/project-roles/","title":"Extending project roles","tags":[],"description":"","content":"Extending project roles The Project resource allows to specify a list of roles for every member (.spec.members[*].roles). There are a few standard roles defined by Gardener itself. Please consult this document for further information.\nHowever, extension controllers running in the garden cluster may also create CustomResourceDefinitions that project members might be able to CRUD. For this purpose Gardener also allows to specify extension roles.\nAn extension role is prefixed with extension:, e.g.\napiVersion:core.gardener.cloud/v1beta1kind:Projectmetadata:name:devspec:members:- apiGroup:rbac.authorization.k8s.iokind:Username:alice.doe@example.comrole:adminroles:- owner- extension:fooThe project controller will, for every extension role, create a ClusterRole with name name: gardener.cloud:extension:project:\u0026lt;projectName\u0026gt;:\u0026lt;roleName\u0026gt;, i.e., for above example: name: gardener.cloud:extension:project:dev:foo. This ClusterRole aggregates other ClusterRoles that are labeled with rbac.gardener.cloud/aggregate-to-extension-role=foo which might be created by extension controllers.\nExtension that might want to contribute to the core admin or viewer roles can use the labels rbac.gardener.cloud/aggregate-to-project-member=true or rbac.gardener.cloud/aggregate-to-project-viewer=true, respectively.\nPlease note that the names of the extension roles are restricted to 20 characters!\nMoreover, the project controller will also create a corresponding RoleBinding with the same name in the project namespace. It will automatically assign all members that are assigned to this extension role.\n"},{"uri":"https://gardener.cloud/documentation/concepts/monitoring/extending/","title":"Extending the Monitoring Stack","tags":[],"description":"","content":"Extending the Monitoring Stack This document provides instructions to extend the Shoot cluster monitoring stack by integrating new scrape targets, alerts and dashboards.\nPlease ensure that you have understood the basic principles of Prometheus and its ecosystem before you continue.\n:bangbang: The purpose of the monitoring stack is to observe the behaviour of the control plane and the system components deployed by Gardener onto the worker nodes. Monitoring of custom workloads running in the cluster is out of scope.\nOverview Each Shoot cluster comes with its own monitoring stack. The following components are deployed into the seed and shoot:\n Seed  Prometheus Grafana blackbox-exporter kube-state-metrics (Seed metrics) kube-state-metrics (Shoot metrics) Alertmanager (Optional)   Shoot  node-exporter(s) kube-state-metrics blackbox-exporter    In each Seed cluster there is a Prometheus in the garden namespace responsible for collecting metrics from the Seed kubelets and cAdvisors. These metrics are provided to each Shoot Prometheus via federation.\nThe alerts for all Shoot clusters hosted on a Seed are routed to a central Alertmanger running in the garden namespace of the Seed. The purpose of this central alertmanager is to forward all important alerts to the operators of the Gardener setup.\nThe Alertmanager in the Shoot namespace on the Seed is only responsible for forwarding alerts from its Shoot cluster to a cluster owner/cluster alert receiver via email. The Alertmanager is optional and the conditions for a deployment are already described here.\nAdding New Monitoring Targets After exploring the metrics which your component provides or adding new metrics, you should be aware which metrics are required to write the needed alerts and dashboards.\nPrometheus prefers a pull based metrics collection approach and therefore the targets to observe need to be defined upfront. The targets are defined in charts/seed-monitoring/charts/prometheus/templates/config.yaml. New scrape jobs can be added in the section scrape_configs. Detailed information how to configure scrape jobs and how to use the kubernetes service discovery are available in the Prometheus documentation.\nThe job_name of a scrape job should be the name of the component e.g. kube-apiserver or vpn. The collection interval should be the default of 30s. You do not need to specify this in the configuration.\nPlease do not ingest all metrics which are provided by a component. Rather collect only those metrics which are needed to define the alerts and dashboards (i.e. whitelist). This can be achieved by adding the following metric_relabel_configs statement to your scrape jobs (replace exampleComponent with component name).\n- job_name:example-component...metric_relabel_configs:{{include\u0026#34;prometheus.keep-metrics.metric-relabel-config\u0026#34;.Values.allowedMetrics.exampleComponent|indent6}}The whitelist for the metrics of your job can be maintained in charts/seed-monitoring/charts/prometheus/values.yaml in section allowedMetrics.exampleComponent (replace exampleComponent with component name). Check the following example:\nallowedMetrics:...exampleComponent:*metrics_name_1*metrics_name_2...Adding Alerts The alert definitons are located in charts/seed-monitoring/charts/prometheus/rules. There are two approaches for adding new alerts.\n Adding additional alerts for a component which already has a set of alerts. In this case you have to extend the existing rule file for the component. Adding alerts for a new component. In this case a new rule file with name scheme example-component.rules.yaml needs to be added. Add the new alert to alertInhibitionGraph.dot, add any required inhibition flows and render the new graph. To render the graph run:  dot -Tpng ./content/alertInhibitionGraph.dot -o ./content/alertInhibitionGraph.png  Create a test for the new alert. See Alert Tests.  Example alert:\ngroups:* name:example.rulesrules:* alert:ExampleAlertexpr:absent(up{job=\u0026#34;exampleJob\u0026#34;}==1)for:20mlabels:service:exampleseverity:critical# How severe is the alert? (blocker|critical|info|warning)type:shoot# For which topology is the alert relevant? (seed|shoot)visibility:all# Who should receive the alerts? (all|operator|owner)annotations:description:Alongerdescriptionoftheexamplealertthatshouldalsoexplaintheimpactofthealert.summary:Shortsummaryofanexamplealert.If the deployment of component is optional then the alert definitions needs to be added to charts/seed-monitoring/charts/prometheus/optional-rules instead. Furthermore the alerts for component need to be activatable in charts/seed-monitoring/charts/prometheus/values.yaml via rules.optional.example-component.enabled. The default should be true.\nBasic instruction how to define alert rules can be found in the Prometheus documentation.\nRouting tree The Alertmanager is grouping incoming alerts based on labels into buckets. Each bucket has its own configuration like alert receivers, initial delaying duration or resending frequency etc. You can find more information about Alertmanager routing in the Prometheus/Alertmanager documentation. The routing trees for the Alertmanagers deployed by Gardener are depicted below.\nCentral Seed Alertmanager\n main route (all alerts for all shoots on the seed will enter)  group by project and shoot name  group by visibility \u0026#34;all\u0026#34; and \u0026#34;operator\u0026#34;  group by severity \u0026#34;blocker\u0026#34;, \u0026#34;critical\u0026#34;, and \u0026#34;info\u0026#34;  route to Garden operators  group by severity \u0026#34;warning\u0026#34; (dropped)  group by visibility \u0026#34;owner\u0026#34; (dropped) Shoot Alertmanager\n main route (only alerts for one Shoot will enter)  group by visibility \u0026#34;all\u0026#34; and \u0026#34;owner\u0026#34;  group by severity \u0026#34;blocker\u0026#34;, \u0026#34;critical\u0026#34;, and \u0026#34;info\u0026#34;  route to cluster alert receiver  group by severity \u0026#34;warning\u0026#34; (dropped, will change soon  route to cluster alert receiver)  group by visibility \u0026#34;operator\u0026#34; (dropped) Alert Inhibition All alerts related to components running on the Shoot workers are inhibited in case of an issue with the vpn connection, because those components can\u0026rsquo;t be scraped anymore and Prometheus will fire alerts in consequence. The components running on the workers are probably healthy and the alerts are presumably false positives. The inhibition flow is shown in the figure below. If you add a new alert make sure to add it to the diagram.\nAlert Attributes Each alert rule definition has to contain the following annotations:\n summary: A short description of the issue. description: A detailed explanation of the issue with hints to the possible root causes and the impact assessment of the issue.  In addtion each alert must contain the following labels:\n type  shoot: Components running on the Shoot worker nodes in the kube-system namespace. seed: Components running on the Seed in the Shoot namespace as part of/next to the control plane.   service  Name of the component (in lowercase) e.g. kube-apiserver, alertmanager or vpn.   severity  blocker: All issues which make the cluster entirely unusable e.g. KubeAPIServerDown or KubeSchedulerDown critical: All issues which affect single functionalities/components but not affect the cluster in its core functionality e.g. VPNDown or KubeletDown. info: All issues that do not affect the cluster or its core functionality, but if this component is down we cannot determine if a blocker alert is firing. (i.e. A component with an info level severity is a dependency for a component with a blocker severity) warning: No current existing issue, rather a hint for situations which could lead to real issue in the close future e.g. HighLatencyApiServerToWorkers or ApiServerResponseSlow.    Alert Tests Execute the tests in $GARDENERHOME/.ci/test or if you want to only test the Prometheus alerts:\n# Install promtool go get -u github.com/prometheus/prometheus/cmd/promtool # Move to seed-monitoring/prometheus chart cd $GARDENERHOME/charts/seed-monitoring/charts/prometheus/ # Execute tests promtool test rules rules-tests/*test.yaml If you want to add alert tests:\n  Create a new file in rules-tests in the form \u0026lt;alert-group-name\u0026gt;.rules.test.yaml or if the alerts are for an existing component with existing tests, simply add the tests to the appropriate files.\n  Make sure that newly added tests succeed. See above.\n  Adding Grafana Dashboards The dashboard definition files are located in charts/seed-monitoring/charts/grafana/dashboards. Every dashboard needs its own file.\nIf you are adding a new component dashboard please also update the overview dashboard by adding a chart for its current up/down status and with a drill down option to the component dashboard.\nDashboard Structure The dashboards should be structured in the following way. The assignment of the component dashboards to the categories should be handled via dashboard tags.\n Kubernetes control plane components (Tag: control-plane)  All components which are part of the Kubernetes control plane e. g. Kube API Server, Kube Controller Manager, Kube Scheduler and Cloud Controller Manager ETCD + Backup/Restore Kubernetes Addon Manager   Node/Machine components (Tag: node/machine)  All metrics which are related to the behaviour/control of the Kubernetes nodes and kubelets Machine-Controller-Manager + Cluster Autoscaler   Networking components (Tag: network)  CoreDNS, KubeProxy, Calico, VPN, Nginx Ingress   Addon components (Tag: addon)  Cert Broker   Monitoring components (Tag: monitoring) Logging components (Tag: logging)  Mandatory Charts for Component Dashboards For each new component, its corresponding dashboard should contain the following charts in the first row, before adding custom charts for the component in the subsequent rows.\n Pod up/down status up{job=\u0026quot;example-component\u0026quot;} Pod/containers cpu utilization Pod/containers memorty consumption Pod/containers network i/o  These information is provided by the cAdvisor metrics. These metrics are already integrated. Please check the other dashboards for detailed information on how to query.\nChart Requirements Each chart needs to contain:\n a meaningful name a detailed description (for non trivial charts) appropriate x/y axis descriptions appropriate scaling levels for the x/y axis proper units for the x/y axis  Dashboard Parameters The following parameters should be added to all dashboards to ensure a homogeneous experience across all dashboards.\nDashboards have to \u0026hellip;\n contain a title which refers to the component name(s) contain a timezone statement which should be the browser time contain tags which express where the component is running (seed or shoot) and to which category the component belong (see dashboard structure) contain a version statement with a value of 1 be immutable  Example dashboard configuration\n{ \u0026#34;title\u0026#34;: \u0026#34;example-component\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;browser\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;seed\u0026#34;, \u0026#34;control-plane\u0026#34; ], \u0026#34;version\u0026#34;: 1, \u0026#34;editable\u0026#34;: \u0026#34;false\u0026#34; } Furthermore all dashboards should contain the following time options:\n{ \u0026#34;time\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;now-1h\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;now\u0026#34; }, \u0026#34;timepicker\u0026#34;: { \u0026#34;refresh_intervals\u0026#34;: [ \u0026#34;30s\u0026#34;, \u0026#34;1m\u0026#34;, \u0026#34;5m\u0026#34; ], \u0026#34;time_options\u0026#34;: [ \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;6h\u0026#34;, \u0026#34;12h\u0026#34;, \u0026#34;24h\u0026#34;, \u0026#34;2d\u0026#34;, \u0026#34;10d\u0026#34; ] } } "},{"uri":"https://gardener.cloud/v1.12.8/concepts/monitoring/extending/","title":"Extending the Monitoring Stack","tags":[],"description":"","content":"Extending the Monitoring Stack This document provides instructions to extend the Shoot cluster monitoring stack by integrating new scrape targets, alerts and dashboards.\nPlease ensure that you have understood the basic principles of Prometheus and its ecosystem before you continue.\n:bangbang: The purpose of the monitoring stack is to observe the behaviour of the control plane and the system components deployed by Gardener onto the worker nodes. Monitoring of custom workloads running in the cluster is out of scope.\nOverview Each Shoot cluster comes with its own monitoring stack. The following components are deployed into the seed and shoot:\n Seed  Prometheus Grafana blackbox-exporter kube-state-metrics (Seed metrics) kube-state-metrics (Shoot metrics) Alertmanager (Optional)   Shoot  node-exporter(s) kube-state-metrics blackbox-exporter    In each Seed cluster there is a Prometheus in the garden namespace responsible for collecting metrics from the Seed kubelets and cAdvisors. These metrics are provided to each Shoot Prometheus via federation.\nThe alerts for all Shoot clusters hosted on a Seed are routed to a central Alertmanger running in the garden namespace of the Seed. The purpose of this central alertmanager is to forward all important alerts to the operators of the Gardener setup.\nThe Alertmanager in the Shoot namespace on the Seed is only responsible for forwarding alerts from its Shoot cluster to a cluster owner/cluster alert receiver via email. The Alertmanager is optional and the conditions for a deployment are already described here.\nAdding New Monitoring Targets After exploring the metrics which your component provides or adding new metrics, you should be aware which metrics are required to write the needed alerts and dashboards.\nPrometheus prefers a pull based metrics collection approach and therefore the targets to observe need to be defined upfront. The targets are defined in charts/seed-monitoring/charts/prometheus/templates/config.yaml. New scrape jobs can be added in the section scrape_configs. Detailed information how to configure scrape jobs and how to use the kubernetes service discovery are available in the Prometheus documentation.\nThe job_name of a scrape job should be the name of the component e.g. kube-apiserver or vpn. The collection interval should be the default of 30s. You do not need to specify this in the configuration.\nPlease do not ingest all metrics which are provided by a component. Rather collect only those metrics which are needed to define the alerts and dashboards (i.e. whitelist). This can be achieved by adding the following metric_relabel_configs statement to your scrape jobs (replace exampleComponent with component name).\n- job_name:example-component...metric_relabel_configs:{{include\u0026#34;prometheus.keep-metrics.metric-relabel-config\u0026#34;.Values.allowedMetrics.exampleComponent|indent6}}The whitelist for the metrics of your job can be maintained in charts/seed-monitoring/charts/prometheus/values.yaml in section allowedMetrics.exampleComponent (replace exampleComponent with component name). Check the following example:\nallowedMetrics:...exampleComponent:*metrics_name_1*metrics_name_2...Adding Alerts The alert definitons are located in charts/seed-monitoring/charts/prometheus/rules. There are two approaches for adding new alerts.\n Adding additional alerts for a component which already has a set of alerts. In this case you have to extend the existing rule file for the component. Adding alerts for a new component. In this case a new rule file with name scheme example-component.rules.yaml needs to be added. Add the new alert to alertInhibitionGraph.dot, add any required inhibition flows and render the new graph. To render the graph run:  dot -Tpng ./content/alertInhibitionGraph.dot -o ./content/alertInhibitionGraph.png  Create a test for the new alert. See Alert Tests.  Example alert:\ngroups:* name:example.rulesrules:* alert:ExampleAlertexpr:absent(up{job=\u0026#34;exampleJob\u0026#34;}==1)for:20mlabels:service:exampleseverity:critical# How severe is the alert? (blocker|critical|info|warning)type:shoot# For which topology is the alert relevant? (seed|shoot)visibility:all# Who should receive the alerts? (all|operator|owner)annotations:description:Alongerdescriptionoftheexamplealertthatshouldalsoexplaintheimpactofthealert.summary:Shortsummaryofanexamplealert.If the deployment of component is optional then the alert definitions needs to be added to charts/seed-monitoring/charts/prometheus/optional-rules instead. Furthermore the alerts for component need to be activatable in charts/seed-monitoring/charts/prometheus/values.yaml via rules.optional.example-component.enabled. The default should be true.\nBasic instruction how to define alert rules can be found in the Prometheus documentation.\nRouting tree The Alertmanager is grouping incoming alerts based on labels into buckets. Each bucket has its own configuration like alert receivers, initial delaying duration or resending frequency etc. You can find more information about Alertmanager routing in the Prometheus/Alertmanager documentation. The routing trees for the Alertmanagers deployed by Gardener are depicted below.\nCentral Seed Alertmanager\n main route (all alerts for all shoots on the seed will enter)  group by project and shoot name  group by visibility \u0026#34;all\u0026#34; and \u0026#34;operator\u0026#34;  group by severity \u0026#34;blocker\u0026#34;, \u0026#34;critical\u0026#34;, and \u0026#34;info\u0026#34;  route to Garden operators  group by severity \u0026#34;warning\u0026#34; (dropped)  group by visibility \u0026#34;owner\u0026#34; (dropped) Shoot Alertmanager\n main route (only alerts for one Shoot will enter)  group by visibility \u0026#34;all\u0026#34; and \u0026#34;owner\u0026#34;  group by severity \u0026#34;blocker\u0026#34;, \u0026#34;critical\u0026#34;, and \u0026#34;info\u0026#34;  route to cluster alert receiver  group by severity \u0026#34;warning\u0026#34; (dropped, will change soon  route to cluster alert receiver)  group by visibility \u0026#34;operator\u0026#34; (dropped) Alert Inhibition All alerts related to components running on the Shoot workers are inhibited in case of an issue with the vpn connection, because those components can\u0026rsquo;t be scraped anymore and Prometheus will fire alerts in consequence. The components running on the workers are probably healthy and the alerts are presumably false positives. The inhibition flow is shown in the figure below. If you add a new alert make sure to add it to the diagram.\nAlert Attributes Each alert rule definition has to contain the following annotations:\n summary: A short description of the issue. description: A detailed explanation of the issue with hints to the possible root causes and the impact assessment of the issue.  In addtion each alert must contain the following labels:\n type  shoot: Components running on the Shoot worker nodes in the kube-system namespace. seed: Components running on the Seed in the Shoot namespace as part of/next to the control plane.   service  Name of the component (in lowercase) e.g. kube-apiserver, alertmanager or vpn.   severity  blocker: All issues which make the cluster entirely unusable e.g. KubeAPIServerDown or KubeSchedulerDown critical: All issues which affect single functionalities/components but not affect the cluster in its core functionality e.g. VPNDown or KubeletDown. info: All issues that do not affect the cluster or its core functionality, but if this component is down we cannot determine if a blocker alert is firing. (i.e. A component with an info level severity is a dependency for a component with a blocker severity) warning: No current existing issue, rather a hint for situations which could lead to real issue in the close future e.g. HighLatencyApiServerToWorkers or ApiServerResponseSlow.    Alert Tests Execute the tests in $GARDENERHOME/.ci/test or if you want to only test the Prometheus alerts:\n# Install promtool go get -u github.com/prometheus/prometheus/cmd/promtool # Move to seed-monitoring/prometheus chart cd $GARDENERHOME/charts/seed-monitoring/charts/prometheus/ # Execute tests promtool test rules rules-tests/*test.yaml If you want to add alert tests:\n  Create a new file in rules-tests in the form \u0026lt;alert-group-name\u0026gt;.rules.test.yaml or if the alerts are for an existing component with existing tests, simply add the tests to the appropriate files.\n  Make sure that newly added tests succeed. See above.\n  Adding Grafana Dashboards The dashboard definition files are located in charts/seed-monitoring/charts/grafana/dashboards. Every dashboard needs its own file.\nIf you are adding a new component dashboard please also update the overview dashboard by adding a chart for its current up/down status and with a drill down option to the component dashboard.\nDashboard Structure The dashboards should be structured in the following way. The assignment of the component dashboards to the categories should be handled via dashboard tags.\n Kubernetes control plane components (Tag: control-plane)  All components which are part of the Kubernetes control plane e. g. Kube API Server, Kube Controller Manager, Kube Scheduler and Cloud Controller Manager ETCD + Backup/Restore Kubernetes Addon Manager   Node/Machine components (Tag: node/machine)  All metrics which are related to the behaviour/control of the Kubernetes nodes and kubelets Machine-Controller-Manager + Cluster Autoscaler   Networking components (Tag: network)  CoreDNS, KubeProxy, Calico, VPN, Nginx Ingress   Addon components (Tag: addon)  Cert Broker   Monitoring components (Tag: monitoring) Logging components (Tag: logging)  Mandatory Charts for Component Dashboards For each new component, its corresponding dashboard should contain the following charts in the first row, before adding custom charts for the component in the subsequent rows.\n Pod up/down status up{job=\u0026quot;example-component\u0026quot;} Pod/containers cpu utilization Pod/containers memorty consumption Pod/containers network i/o  These information is provided by the cAdvisor metrics. These metrics are already integrated. Please check the other dashboards for detailed information on how to query.\nChart Requirements Each chart needs to contain:\n a meaningful name a detailed description (for non trivial charts) appropriate x/y axis descriptions appropriate scaling levels for the x/y axis proper units for the x/y axis  Dashboard Parameters The following parameters should be added to all dashboards to ensure a homogeneous experience across all dashboards.\nDashboards have to \u0026hellip;\n contain a title which refers to the component name(s) contain a timezone statement which should be the browser time contain tags which express where the component is running (seed or shoot) and to which category the component belong (see dashboard structure) contain a version statement with a value of 1 be immutable  Example dashboard configuration\n{ \u0026#34;title\u0026#34;: \u0026#34;example-component\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;browser\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;seed\u0026#34;, \u0026#34;control-plane\u0026#34; ], \u0026#34;version\u0026#34;: 1, \u0026#34;editable\u0026#34;: \u0026#34;false\u0026#34; } Furthermore all dashboards should contain the following time options:\n{ \u0026#34;time\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;now-1h\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;now\u0026#34; }, \u0026#34;timepicker\u0026#34;: { \u0026#34;refresh_intervals\u0026#34;: [ \u0026#34;30s\u0026#34;, \u0026#34;1m\u0026#34;, \u0026#34;5m\u0026#34; ], \u0026#34;time_options\u0026#34;: [ \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;6h\u0026#34;, \u0026#34;12h\u0026#34;, \u0026#34;24h\u0026#34;, \u0026#34;2d\u0026#34;, \u0026#34;10d\u0026#34; ] } } "},{"uri":"https://gardener.cloud/v1.13.2/concepts/monitoring/extending/","title":"Extending the Monitoring Stack","tags":[],"description":"","content":"Extending the Monitoring Stack This document provides instructions to extend the Shoot cluster monitoring stack by integrating new scrape targets, alerts and dashboards.\nPlease ensure that you have understood the basic principles of Prometheus and its ecosystem before you continue.\n:bangbang: The purpose of the monitoring stack is to observe the behaviour of the control plane and the system components deployed by Gardener onto the worker nodes. Monitoring of custom workloads running in the cluster is out of scope.\nOverview Each Shoot cluster comes with its own monitoring stack. The following components are deployed into the seed and shoot:\n Seed  Prometheus Grafana blackbox-exporter kube-state-metrics (Seed metrics) kube-state-metrics (Shoot metrics) Alertmanager (Optional)   Shoot  node-exporter(s) kube-state-metrics blackbox-exporter    In each Seed cluster there is a Prometheus in the garden namespace responsible for collecting metrics from the Seed kubelets and cAdvisors. These metrics are provided to each Shoot Prometheus via federation.\nThe alerts for all Shoot clusters hosted on a Seed are routed to a central Alertmanger running in the garden namespace of the Seed. The purpose of this central alertmanager is to forward all important alerts to the operators of the Gardener setup.\nThe Alertmanager in the Shoot namespace on the Seed is only responsible for forwarding alerts from its Shoot cluster to a cluster owner/cluster alert receiver via email. The Alertmanager is optional and the conditions for a deployment are already described here.\nAdding New Monitoring Targets After exploring the metrics which your component provides or adding new metrics, you should be aware which metrics are required to write the needed alerts and dashboards.\nPrometheus prefers a pull based metrics collection approach and therefore the targets to observe need to be defined upfront. The targets are defined in charts/seed-monitoring/charts/prometheus/templates/config.yaml. New scrape jobs can be added in the section scrape_configs. Detailed information how to configure scrape jobs and how to use the kubernetes service discovery are available in the Prometheus documentation.\nThe job_name of a scrape job should be the name of the component e.g. kube-apiserver or vpn. The collection interval should be the default of 30s. You do not need to specify this in the configuration.\nPlease do not ingest all metrics which are provided by a component. Rather collect only those metrics which are needed to define the alerts and dashboards (i.e. whitelist). This can be achieved by adding the following metric_relabel_configs statement to your scrape jobs (replace exampleComponent with component name).\n- job_name:example-component...metric_relabel_configs:{{include\u0026#34;prometheus.keep-metrics.metric-relabel-config\u0026#34;.Values.allowedMetrics.exampleComponent|indent6}}The whitelist for the metrics of your job can be maintained in charts/seed-monitoring/charts/prometheus/values.yaml in section allowedMetrics.exampleComponent (replace exampleComponent with component name). Check the following example:\nallowedMetrics:...exampleComponent:*metrics_name_1*metrics_name_2...Adding Alerts The alert definitons are located in charts/seed-monitoring/charts/prometheus/rules. There are two approaches for adding new alerts.\n Adding additional alerts for a component which already has a set of alerts. In this case you have to extend the existing rule file for the component. Adding alerts for a new component. In this case a new rule file with name scheme example-component.rules.yaml needs to be added. Add the new alert to alertInhibitionGraph.dot, add any required inhibition flows and render the new graph. To render the graph run:  dot -Tpng ./content/alertInhibitionGraph.dot -o ./content/alertInhibitionGraph.png  Create a test for the new alert. See Alert Tests.  Example alert:\ngroups:* name:example.rulesrules:* alert:ExampleAlertexpr:absent(up{job=\u0026#34;exampleJob\u0026#34;}==1)for:20mlabels:service:exampleseverity:critical# How severe is the alert? (blocker|critical|info|warning)type:shoot# For which topology is the alert relevant? (seed|shoot)visibility:all# Who should receive the alerts? (all|operator|owner)annotations:description:Alongerdescriptionoftheexamplealertthatshouldalsoexplaintheimpactofthealert.summary:Shortsummaryofanexamplealert.If the deployment of component is optional then the alert definitions needs to be added to charts/seed-monitoring/charts/prometheus/optional-rules instead. Furthermore the alerts for component need to be activatable in charts/seed-monitoring/charts/prometheus/values.yaml via rules.optional.example-component.enabled. The default should be true.\nBasic instruction how to define alert rules can be found in the Prometheus documentation.\nRouting tree The Alertmanager is grouping incoming alerts based on labels into buckets. Each bucket has its own configuration like alert receivers, initial delaying duration or resending frequency etc. You can find more information about Alertmanager routing in the Prometheus/Alertmanager documentation. The routing trees for the Alertmanagers deployed by Gardener are depicted below.\nCentral Seed Alertmanager\n main route (all alerts for all shoots on the seed will enter)  group by project and shoot name  group by visibility \u0026#34;all\u0026#34; and \u0026#34;operator\u0026#34;  group by severity \u0026#34;blocker\u0026#34;, \u0026#34;critical\u0026#34;, and \u0026#34;info\u0026#34;  route to Garden operators  group by severity \u0026#34;warning\u0026#34; (dropped)  group by visibility \u0026#34;owner\u0026#34; (dropped) Shoot Alertmanager\n main route (only alerts for one Shoot will enter)  group by visibility \u0026#34;all\u0026#34; and \u0026#34;owner\u0026#34;  group by severity \u0026#34;blocker\u0026#34;, \u0026#34;critical\u0026#34;, and \u0026#34;info\u0026#34;  route to cluster alert receiver  group by severity \u0026#34;warning\u0026#34; (dropped, will change soon  route to cluster alert receiver)  group by visibility \u0026#34;operator\u0026#34; (dropped) Alert Inhibition All alerts related to components running on the Shoot workers are inhibited in case of an issue with the vpn connection, because those components can\u0026rsquo;t be scraped anymore and Prometheus will fire alerts in consequence. The components running on the workers are probably healthy and the alerts are presumably false positives. The inhibition flow is shown in the figure below. If you add a new alert make sure to add it to the diagram.\nAlert Attributes Each alert rule definition has to contain the following annotations:\n summary: A short description of the issue. description: A detailed explanation of the issue with hints to the possible root causes and the impact assessment of the issue.  In addtion each alert must contain the following labels:\n type  shoot: Components running on the Shoot worker nodes in the kube-system namespace. seed: Components running on the Seed in the Shoot namespace as part of/next to the control plane.   service  Name of the component (in lowercase) e.g. kube-apiserver, alertmanager or vpn.   severity  blocker: All issues which make the cluster entirely unusable e.g. KubeAPIServerDown or KubeSchedulerDown critical: All issues which affect single functionalities/components but not affect the cluster in its core functionality e.g. VPNDown or KubeletDown. info: All issues that do not affect the cluster or its core functionality, but if this component is down we cannot determine if a blocker alert is firing. (i.e. A component with an info level severity is a dependency for a component with a blocker severity) warning: No current existing issue, rather a hint for situations which could lead to real issue in the close future e.g. HighLatencyApiServerToWorkers or ApiServerResponseSlow.    Alert Tests Execute the tests in $GARDENERHOME/.ci/test or if you want to only test the Prometheus alerts:\n# Install promtool go get -u github.com/prometheus/prometheus/cmd/promtool # Move to seed-monitoring/prometheus chart cd $GARDENERHOME/charts/seed-monitoring/charts/prometheus/ # Execute tests promtool test rules rules-tests/*test.yaml If you want to add alert tests:\n  Create a new file in rules-tests in the form \u0026lt;alert-group-name\u0026gt;.rules.test.yaml or if the alerts are for an existing component with existing tests, simply add the tests to the appropriate files.\n  Make sure that newly added tests succeed. See above.\n  Adding Grafana Dashboards The dashboard definition files are located in charts/seed-monitoring/charts/grafana/dashboards. Every dashboard needs its own file.\nIf you are adding a new component dashboard please also update the overview dashboard by adding a chart for its current up/down status and with a drill down option to the component dashboard.\nDashboard Structure The dashboards should be structured in the following way. The assignment of the component dashboards to the categories should be handled via dashboard tags.\n Kubernetes control plane components (Tag: control-plane)  All components which are part of the Kubernetes control plane e. g. Kube API Server, Kube Controller Manager, Kube Scheduler and Cloud Controller Manager ETCD + Backup/Restore Kubernetes Addon Manager   Node/Machine components (Tag: node/machine)  All metrics which are related to the behaviour/control of the Kubernetes nodes and kubelets Machine-Controller-Manager + Cluster Autoscaler   Networking components (Tag: network)  CoreDNS, KubeProxy, Calico, VPN, Nginx Ingress   Addon components (Tag: addon)  Cert Broker   Monitoring components (Tag: monitoring) Logging components (Tag: logging)  Mandatory Charts for Component Dashboards For each new component, its corresponding dashboard should contain the following charts in the first row, before adding custom charts for the component in the subsequent rows.\n Pod up/down status up{job=\u0026quot;example-component\u0026quot;} Pod/containers cpu utilization Pod/containers memorty consumption Pod/containers network i/o  These information is provided by the cAdvisor metrics. These metrics are already integrated. Please check the other dashboards for detailed information on how to query.\nChart Requirements Each chart needs to contain:\n a meaningful name a detailed description (for non trivial charts) appropriate x/y axis descriptions appropriate scaling levels for the x/y axis proper units for the x/y axis  Dashboard Parameters The following parameters should be added to all dashboards to ensure a homogeneous experience across all dashboards.\nDashboards have to \u0026hellip;\n contain a title which refers to the component name(s) contain a timezone statement which should be the browser time contain tags which express where the component is running (seed or shoot) and to which category the component belong (see dashboard structure) contain a version statement with a value of 1 be immutable  Example dashboard configuration\n{ \u0026#34;title\u0026#34;: \u0026#34;example-component\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;browser\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;seed\u0026#34;, \u0026#34;control-plane\u0026#34; ], \u0026#34;version\u0026#34;: 1, \u0026#34;editable\u0026#34;: \u0026#34;false\u0026#34; } Furthermore all dashboards should contain the following time options:\n{ \u0026#34;time\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;now-1h\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;now\u0026#34; }, \u0026#34;timepicker\u0026#34;: { \u0026#34;refresh_intervals\u0026#34;: [ \u0026#34;30s\u0026#34;, \u0026#34;1m\u0026#34;, \u0026#34;5m\u0026#34; ], \u0026#34;time_options\u0026#34;: [ \u0026#34;5m\u0026#34;, \u0026#34;15m\u0026#34;, \u0026#34;1h\u0026#34;, \u0026#34;6h\u0026#34;, \u0026#34;12h\u0026#34;, \u0026#34;24h\u0026#34;, \u0026#34;2d\u0026#34;, \u0026#34;10d\u0026#34; ] } } "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/overview/","title":"Extensibility overview","tags":[],"description":"","content":"Extensibility overview Initially, everything was developed in-tree in the Gardener project. All cloud providers and the configuration for all the supported operating systems were released together with the Gardener core itself. But as the project grew, it got more and more difficult to add new providers and maintain the existing code base. As a consequence and in order to become agile and flexible again, we proposed GEP-1 (Gardener Enhancement Proposal). The document describes an out-of-tree extension architecture that keeps the Gardener core logic independent of provider-specific knowledge (similar to what Kubernetes has achieved with out-of-tree cloud providers or with CSI volume plugins).\nBasic concepts Gardener keeps running in the \u0026ldquo;garden cluster\u0026rdquo; and implements the core logic of shoot cluster reconciliation/deletion. Extensions are Kubernetes controllers themselves (like Gardener) and run in the seed clusters. As usual, we try to use Kubernetes wherever applicable. We rely on Kubernetes extension concepts in order to enable extensibility for Gardener. The main ideas of GEP-1 are the following:\n  During the shoot reconciliation process Gardener will write CRDs into the seed cluster that are watched and managed by the extension controllers. They will reconcile (based on the .spec) and report whether everything went well or errors occurred in the CRD\u0026rsquo;s .status field.\n  Gardener keeps deploying the provider-independent control plane components (etcd, kube-apiserver, etc.). However, some of these components might still need little customization by providers, e.g., additional configuration, flags, etc. In this case, the extension controllers register webhooks in order to manipulate the manifests.\n  Example 1:\nGardener creates a new AWS shoot cluster and requires the preparation of infrastructure in order to proceed (networks, security groups, etc.). It writes the following CRD into the seed cluster:\napiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--core--aws-01spec:type:awsproviderConfig:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:vpc:cidr:10.250.0.0/16internal:- 10.250.112.0/22public:- 10.250.96.0/22workers:- 10.250.0.0/19zones:- eu-west-1adns:apiserver:api.aws-01.core.example.comregion:eu-west-1secretRef:name:my-aws-credentialssshPublicKey:| base64(key)Please note that the .spec.providerConfig is a raw blob and not evaluated or known in any way by Gardener. Instead, it was specified by the user (in the Shoot resource) and just \u0026ldquo;forwarded\u0026rdquo; to the extension controller. Only the AWS controller understands this configuration and will now start provisioning/reconciling the infrastructure. It reports in the .status field the result:\nstatus:observedGeneration:...state:...lastError:..lastOperation:...providerStatus:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusvpc:id:vpc-1234subnets:- id:subnet-acbd1234name:workerszone:eu-west-1securityGroups:- id:sg-xyz12345name:workersiam:nodesRoleARN:\u0026lt;some-arn\u0026gt; instanceProfileName: fooec2:keyName:barGardener waits until the .status.lastOperation/.status.lastError indicates that the operation reached a final state and either continuous with the next step or stops and reports the potential error. The extension-specific output in .status.providerStatus is - similar to .spec.providerConfig - not evaluated and simply forwarded to CRDs in subsequent steps.\nExample 2:\nGardener deploys the control plane components into the seed cluster, e.g. the kube-controller-manager deployment with the following flags:\napiVersion:apps/v1kind:Deployment...spec:template:spec:containers:- command:- /usr/local/bin/kube-controller-manager- --allocate-node-cidrs=true- --attach-detach-reconcile-sync-period=1m0s- --controllers=*,bootstrapsigner,tokencleaner- --cluster-cidr=100.96.0.0/11- --cluster-name=shoot--core--aws-01- --cluster-signing-cert-file=/srv/kubernetes/ca/ca.crt- --cluster-signing-key-file=/srv/kubernetes/ca/ca.key- --concurrent-deployment-syncs=10- --concurrent-replicaset-syncs=10...The AWS controller requires some additional flags in order to make the cluster functional. It needs to provide a Kubernetes cloud-config and also some cloud-specific flags. Consequently, it registers a MutatingWebhookConfiguration on Deployments and adds these flags to the container:\n- --cloud-provider=external- --external-cloud-volume-plugin=aws- --cloud-config=/etc/kubernetes/cloudprovider/cloudprovider.confOf course, it would have needed to create a ConfigMap containing the cloud config and to add the proper volume and volumeMounts to the manifest as well.\n(Please note for this special example: The Kubernetes community is also working on making the kube-controller-manager provider-independent. However, there will most probably be still components other than the kube-controller-manager which need to be adapted by extensions.)\nIf you are interested in writing an extension, or generally in digging deeper to find out the nitty-gritty details of the extension concepts please read GEP-1. We are truly looking forward to your feedback!\nCurrent status Meanwhile, the out-of-tree extension architecture of Gardener is in place and has been productively validated. We are tracking all internal and external extensions of Gardener in the repo: Gardener Extensions Library.\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/overview/","title":"Extensibility overview","tags":[],"description":"","content":"Extensibility overview Initially, everything was developed in-tree in the Gardener project. All cloud providers and the configuration for all the supported operating systems were released together with the Gardener core itself. But as the project grew, it got more and more difficult to add new providers and maintain the existing code base. As a consequence and in order to become agile and flexible again, we proposed GEP-1 (Gardener Enhancement Proposal). The document describes an out-of-tree extension architecture that keeps the Gardener core logic independent of provider-specific knowledge (similar to what Kubernetes has achieved with out-of-tree cloud providers or with CSI volume plugins).\nBasic concepts Gardener keeps running in the \u0026ldquo;garden cluster\u0026rdquo; and implements the core logic of shoot cluster reconciliation/deletion. Extensions are Kubernetes controllers themselves (like Gardener) and run in the seed clusters. As usual, we try to use Kubernetes wherever applicable. We rely on Kubernetes extension concepts in order to enable extensibility for Gardener. The main ideas of GEP-1 are the following:\n  During the shoot reconciliation process Gardener will write CRDs into the seed cluster that are watched and managed by the extension controllers. They will reconcile (based on the .spec) and report whether everything went well or errors occurred in the CRD\u0026rsquo;s .status field.\n  Gardener keeps deploying the provider-independent control plane components (etcd, kube-apiserver, etc.). However, some of these components might still need little customization by providers, e.g., additional configuration, flags, etc. In this case, the extension controllers register webhooks in order to manipulate the manifests.\n  Example 1:\nGardener creates a new AWS shoot cluster and requires the preparation of infrastructure in order to proceed (networks, security groups, etc.). It writes the following CRD into the seed cluster:\napiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--core--aws-01spec:type:awsproviderConfig:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:vpc:cidr:10.250.0.0/16internal:- 10.250.112.0/22public:- 10.250.96.0/22workers:- 10.250.0.0/19zones:- eu-west-1adns:apiserver:api.aws-01.core.example.comregion:eu-west-1secretRef:name:my-aws-credentialssshPublicKey:| base64(key)Please note that the .spec.providerConfig is a raw blob and not evaluated or known in any way by Gardener. Instead, it was specified by the user (in the Shoot resource) and just \u0026ldquo;forwarded\u0026rdquo; to the extension controller. Only the AWS controller understands this configuration and will now start provisioning/reconciling the infrastructure. It reports in the .status field the result:\nstatus:observedGeneration:...state:...lastError:..lastOperation:...providerStatus:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusvpc:id:vpc-1234subnets:- id:subnet-acbd1234name:workerszone:eu-west-1securityGroups:- id:sg-xyz12345name:workersiam:nodesRoleARN:\u0026lt;some-arn\u0026gt; instanceProfileName: fooec2:keyName:barGardener waits until the .status.lastOperation/.status.lastError indicates that the operation reached a final state and either continuous with the next step or stops and reports the potential error. The extension-specific output in .status.providerStatus is - similar to .spec.providerConfig - not evaluated and simply forwarded to CRDs in subsequent steps.\nExample 2:\nGardener deploys the control plane components into the seed cluster, e.g. the kube-controller-manager deployment with the following flags:\napiVersion:apps/v1kind:Deployment...spec:template:spec:containers:- command:- /usr/local/bin/kube-controller-manager- --allocate-node-cidrs=true- --attach-detach-reconcile-sync-period=1m0s- --controllers=*,bootstrapsigner,tokencleaner- --cluster-cidr=100.96.0.0/11- --cluster-name=shoot--core--aws-01- --cluster-signing-cert-file=/srv/kubernetes/ca/ca.crt- --cluster-signing-key-file=/srv/kubernetes/ca/ca.key- --concurrent-deployment-syncs=10- --concurrent-replicaset-syncs=10...The AWS controller requires some additional flags in order to make the cluster functional. It needs to provide a Kubernetes cloud-config and also some cloud-specific flags. Consequently, it registers a MutatingWebhookConfiguration on Deployments and adds these flags to the container:\n- --cloud-provider=external- --external-cloud-volume-plugin=aws- --cloud-config=/etc/kubernetes/cloudprovider/cloudprovider.confOf course, it would have needed to create a ConfigMap containing the cloud config and to add the proper volume and volumeMounts to the manifest as well.\n(Please note for this special example: The Kubernetes community is also working on making the kube-controller-manager provider-independent. However, there will most probably be still components other than the kube-controller-manager which need to be adapted by extensions.)\nIf you are interested in writing an extension, or generally in digging deeper to find out the nitty-gritty details of the extension concepts please read GEP-1. We are truly looking forward to your feedback!\nCurrent status Meanwhile, the out-of-tree extension architecture of Gardener is in place and has been productively validated. We are tracking all internal and external extensions of Gardener in the repo: Gardener Extensions Library.\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/overview/","title":"Extensibility overview","tags":[],"description":"","content":"Extensibility overview Initially, everything was developed in-tree in the Gardener project. All cloud providers and the configuration for all the supported operating systems were released together with the Gardener core itself. But as the project grew, it got more and more difficult to add new providers and maintain the existing code base. As a consequence and in order to become agile and flexible again, we proposed GEP-1 (Gardener Enhancement Proposal). The document describes an out-of-tree extension architecture that keeps the Gardener core logic independent of provider-specific knowledge (similar to what Kubernetes has achieved with out-of-tree cloud providers or with CSI volume plugins).\nBasic concepts Gardener keeps running in the \u0026ldquo;garden cluster\u0026rdquo; and implements the core logic of shoot cluster reconciliation/deletion. Extensions are Kubernetes controllers themselves (like Gardener) and run in the seed clusters. As usual, we try to use Kubernetes wherever applicable. We rely on Kubernetes extension concepts in order to enable extensibility for Gardener. The main ideas of GEP-1 are the following:\n  During the shoot reconciliation process Gardener will write CRDs into the seed cluster that are watched and managed by the extension controllers. They will reconcile (based on the .spec) and report whether everything went well or errors occurred in the CRD\u0026rsquo;s .status field.\n  Gardener keeps deploying the provider-independent control plane components (etcd, kube-apiserver, etc.). However, some of these components might still need little customization by providers, e.g., additional configuration, flags, etc. In this case, the extension controllers register webhooks in order to manipulate the manifests.\n  Example 1:\nGardener creates a new AWS shoot cluster and requires the preparation of infrastructure in order to proceed (networks, security groups, etc.). It writes the following CRD into the seed cluster:\napiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--core--aws-01spec:type:awsproviderConfig:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:vpc:cidr:10.250.0.0/16internal:- 10.250.112.0/22public:- 10.250.96.0/22workers:- 10.250.0.0/19zones:- eu-west-1adns:apiserver:api.aws-01.core.example.comregion:eu-west-1secretRef:name:my-aws-credentialssshPublicKey:| base64(key)Please note that the .spec.providerConfig is a raw blob and not evaluated or known in any way by Gardener. Instead, it was specified by the user (in the Shoot resource) and just \u0026ldquo;forwarded\u0026rdquo; to the extension controller. Only the AWS controller understands this configuration and will now start provisioning/reconciling the infrastructure. It reports in the .status field the result:\nstatus:observedGeneration:...state:...lastError:..lastOperation:...providerStatus:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusvpc:id:vpc-1234subnets:- id:subnet-acbd1234name:workerszone:eu-west-1securityGroups:- id:sg-xyz12345name:workersiam:nodesRoleARN:\u0026lt;some-arn\u0026gt; instanceProfileName: fooec2:keyName:barGardener waits until the .status.lastOperation/.status.lastError indicates that the operation reached a final state and either continuous with the next step or stops and reports the potential error. The extension-specific output in .status.providerStatus is - similar to .spec.providerConfig - not evaluated and simply forwarded to CRDs in subsequent steps.\nExample 2:\nGardener deploys the control plane components into the seed cluster, e.g. the kube-controller-manager deployment with the following flags:\napiVersion:apps/v1kind:Deployment...spec:template:spec:containers:- command:- /usr/local/bin/kube-controller-manager- --allocate-node-cidrs=true- --attach-detach-reconcile-sync-period=1m0s- --controllers=*,bootstrapsigner,tokencleaner- --cluster-cidr=100.96.0.0/11- --cluster-name=shoot--core--aws-01- --cluster-signing-cert-file=/srv/kubernetes/ca/ca.crt- --cluster-signing-key-file=/srv/kubernetes/ca/ca.key- --concurrent-deployment-syncs=10- --concurrent-replicaset-syncs=10...The AWS controller requires some additional flags in order to make the cluster functional. It needs to provide a Kubernetes cloud-config and also some cloud-specific flags. Consequently, it registers a MutatingWebhookConfiguration on Deployments and adds these flags to the container:\n- --cloud-provider=external- --external-cloud-volume-plugin=aws- --cloud-config=/etc/kubernetes/cloudprovider/cloudprovider.confOf course, it would have needed to create a ConfigMap containing the cloud config and to add the proper volume and volumeMounts to the manifest as well.\n(Please note for this special example: The Kubernetes community is also working on making the kube-controller-manager provider-independent. However, there will most probably be still components other than the kube-controller-manager which need to be adapted by extensions.)\nIf you are interested in writing an extension, or generally in digging deeper to find out the nitty-gritty details of the extension concepts please read GEP-1. We are truly looking forward to your feedback!\nCurrent status Meanwhile, the out-of-tree extension architecture of Gardener is in place and has been productively validated. We are tracking all internal and external extensions of Gardener in the repo: Gardener Extensions Library.\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/extension/","title":"Extension resource","tags":[],"description":"","content":"Contract: Extension resource Gardener defines common procedures which must be passed to create a functioning shoot cluster. Well known steps are represented by special resources like Infrastructure, OperatingSystemConfig or DNS. These resources are typically reconciled by dedicated controllers setting up the infrastructure on the hyperscaler or managing DNS entries, etc..\nBut, some requirements don\u0026rsquo;t match with those special resources or don\u0026rsquo;t depend on being proceeded at a specific step in the creation / deletion flow of the shoot. They require a more generic hook. Therefore, Gardener offers the Extension resource.\nWhat is required to register and support an Extension type? Gardener creates one Extension resource per registered extension type in ControllerRegistration per shoot.\napiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationmetadata:name:extension-examplespec:resources:- kind:Extensiontype:examplegloballyEnabled:trueIf spec.resources[].globallyEnabled is true then the Extension resources of the given type is created for every shoot cluster. Set to false, the Extension resource is only created if configured in the Shoot manifest.\nThe Extension resources are created in the shoot namespace of the seed cluster.\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Extensionmetadata:name:examplenamespace:shoot--foo--barspec:type:exampleproviderConfig:{}Your controller needs to reconcile extensions.extensions.gardener.cloud. Since there can exist multiple Extension resources per shoot, each one holds a spec.type field to let controllers check their responsibility (similar to all other extension resources of Gardener).\nProviderConfig It is possible to provide data in the Shoot resource which is copied to spec.providerConfig of the Extension resource.\n---apiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:name:barnamespace:garden-foospec:extensions:- type:exampleproviderConfig:foo:bar...results in\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Extensionmetadata:name:examplenamespace:shoot--foo--barspec:type:exampleproviderConfig:foo:barShoot reconciliation flow and Extension status Gardener creates Extension resources as part of the Shoot reconciliation. Moreover, it is guaranteed that the Cluster resource exists before the Extension resource is created.\nFor an Extension controller it is crucial to maintain the Extension's status correctly. At the end Gardener checks the status of each Extension and only reports a successful shoot reconciliation if the state of the last operation is Succeeded.\napiVersion:extensions.gardener.cloud/v1alpha1kind:Extensionmetadata:generation:1name:examplenamespace:shoot--foo--barspec:type:examplestatus:lastOperation:state:SucceededobservedGeneration:1"},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/extension/","title":"Extension resource","tags":[],"description":"","content":"Contract: Extension resource Gardener defines common procedures which must be passed to create a functioning shoot cluster. Well known steps are represented by special resources like Infrastructure, OperatingSystemConfig or DNS. These resources are typically reconciled by dedicated controllers setting up the infrastructure on the hyperscaler or managing DNS entries, etc..\nBut, some requirements don\u0026rsquo;t match with those special resources or don\u0026rsquo;t depend on being proceeded at a specific step in the creation / deletion flow of the shoot. They require a more generic hook. Therefore, Gardener offers the Extension resource.\nWhat is required to register and support an Extension type? Gardener creates one Extension resource per registered extension type in ControllerRegistration per shoot.\napiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationmetadata:name:extension-examplespec:resources:- kind:Extensiontype:examplegloballyEnabled:trueIf spec.resources[].globallyEnabled is true then the Extension resources of the given type is created for every shoot cluster. Set to false, the Extension resource is only created if configured in the Shoot manifest.\nThe Extension resources are created in the shoot namespace of the seed cluster.\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Extensionmetadata:name:examplenamespace:shoot--foo--barspec:type:exampleproviderConfig:{}Your controller needs to reconcile extensions.extensions.gardener.cloud. Since there can exist multiple Extension resources per shoot, each one holds a spec.type field to let controllers check their responsibility (similar to all other extension resources of Gardener).\nProviderConfig It is possible to provide data in the Shoot resource which is copied to spec.providerConfig of the Extension resource.\n---apiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:name:barnamespace:garden-foospec:extensions:- type:exampleproviderConfig:foo:bar...results in\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Extensionmetadata:name:examplenamespace:shoot--foo--barspec:type:exampleproviderConfig:foo:barShoot reconciliation flow and Extension status Gardener creates Extension resources as part of the Shoot reconciliation. Moreover, it is guaranteed that the Cluster resource exists before the Extension resource is created.\nFor an Extension controller it is crucial to maintain the Extension's status correctly. At the end Gardener checks the status of each Extension and only reports a successful shoot reconciliation if the state of the last operation is Succeeded.\napiVersion:extensions.gardener.cloud/v1alpha1kind:Extensionmetadata:generation:1name:examplenamespace:shoot--foo--barspec:type:examplestatus:lastOperation:state:SucceededobservedGeneration:1"},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/extension/","title":"Extension resource","tags":[],"description":"","content":"Contract: Extension resource Gardener defines common procedures which must be passed to create a functioning shoot cluster. Well known steps are represented by special resources like Infrastructure, OperatingSystemConfig or DNS. These resources are typically reconciled by dedicated controllers setting up the infrastructure on the hyperscaler or managing DNS entries, etc..\nBut, some requirements don\u0026rsquo;t match with those special resources or don\u0026rsquo;t depend on being proceeded at a specific step in the creation / deletion flow of the shoot. They require a more generic hook. Therefore, Gardener offers the Extension resource.\nWhat is required to register and support an Extension type? Gardener creates one Extension resource per registered extension type in ControllerRegistration per shoot.\napiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationmetadata:name:extension-examplespec:resources:- kind:Extensiontype:examplegloballyEnabled:trueIf spec.resources[].globallyEnabled is true then the Extension resources of the given type is created for every shoot cluster. Set to false, the Extension resource is only created if configured in the Shoot manifest.\nThe Extension resources are created in the shoot namespace of the seed cluster.\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Extensionmetadata:name:examplenamespace:shoot--foo--barspec:type:exampleproviderConfig:{}Your controller needs to reconcile extensions.extensions.gardener.cloud. Since there can exist multiple Extension resources per shoot, each one holds a spec.type field to let controllers check their responsibility (similar to all other extension resources of Gardener).\nProviderConfig It is possible to provide data in the Shoot resource which is copied to spec.providerConfig of the Extension resource.\n---apiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:name:barnamespace:garden-foospec:extensions:- type:exampleproviderConfig:foo:bar...results in\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Extensionmetadata:name:examplenamespace:shoot--foo--barspec:type:exampleproviderConfig:foo:barShoot reconciliation flow and Extension status Gardener creates Extension resources as part of the Shoot reconciliation. Moreover, it is guaranteed that the Cluster resource exists before the Extension resource is created.\nFor an Extension controller it is crucial to maintain the Extension's status correctly. At the end Gardener checks the status of each Extension and only reports a successful shoot reconciliation if the state of the last operation is Succeeded.\napiVersion:extensions.gardener.cloud/v1alpha1kind:Extensionmetadata:generation:1name:examplenamespace:shoot--foo--barspec:type:examplestatus:lastOperation:state:SucceededobservedGeneration:1"},{"uri":"https://gardener.cloud/documentation/references/extensions/","title":"Extensions","tags":[],"description":"","content":"Packages:\n  extensions.gardener.cloud/v1alpha1   extensions.gardener.cloud/v1alpha1  Package v1alpha1 is the v1alpha1 version of the API.\nResource Types:  BackupBucket  BackupEntry  Cluster  ContainerRuntime  ControlPlane  Extension  Infrastructure  Network  OperatingSystemConfig  Worker  BackupBucket   BackupBucket is a specification for backup bucket.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  BackupBucket    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupBucketSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this bucket.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n       status  BackupBucketStatus     (Optional)     BackupEntry   BackupEntry is a specification for backup Entry.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  BackupEntry    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupEntrySpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    backupBucketProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) BackupBucketProviderStatus contains the provider status that has been generated by the controller responsible for the BackupBucket resource.\n    region  string    Region is the region of this Entry.\n    bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n       status  BackupEntryStatus     (Optional)     Cluster   Cluster is a specification for a Cluster resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Cluster    metadata  Kubernetes meta/v1.ObjectMeta     Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ClusterSpec          cloudProfile  k8s.io/apimachinery/pkg/runtime.RawExtension     CloudProfile is a raw extension field that contains the cloudprofile resource referenced by the shoot that has to be reconciled.\n    seed  k8s.io/apimachinery/pkg/runtime.RawExtension     Seed is a raw extension field that contains the seed resource referenced by the shoot that has to be reconciled.\n    shoot  k8s.io/apimachinery/pkg/runtime.RawExtension     Shoot is a raw extension field that contains the shoot resource that has to be reconciled.\n       ContainerRuntime   ContainerRuntime is a specification for a container runtime resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  ContainerRuntime    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ContainerRuntimeSpec          binaryPath  string    BinaryPath is the Worker\u0026rsquo;s machine path where container runtime extensions should copy the binaries to.\n    workerPool  ContainerRuntimeWorkerPool     WorkerPool identifies the worker pool of the Shoot. For each worker pool and type, Gardener deploys a ContainerRuntime CRD.\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n       status  ContainerRuntimeStatus     (Optional)     ControlPlane   ControlPlane is a specification for a ControlPlane resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  ControlPlane    metadata  Kubernetes meta/v1.ObjectMeta     Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControlPlaneSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  Purpose     (Optional) Purpose contains the data if a cloud provider needs additional components in order to expose the control plane.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the region of this control plane.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n       status  ControlPlaneStatus     (Optional)     Extension   Extension is a specification for a Extension resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Extension    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ExtensionSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n       status  ExtensionStatus     (Optional)     Infrastructure   Infrastructure is a specification for cloud provider infrastructure.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Infrastructure    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  InfrastructureSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this infrastructure.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with this infrastructure.\n       status  InfrastructureStatus     (Optional)     Network   Network is the specification for cluster networking.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Network    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  NetworkSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    podCIDR  string    PodCIDR defines the CIDR that will be used for pods.\n    serviceCIDR  string    ServiceCIDR defines the CIDR that will be used for services.\n       status  NetworkStatus     (Optional)     OperatingSystemConfig   OperatingSystemConfig is a specification for a OperatingSystemConfig resource\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  OperatingSystemConfig    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  OperatingSystemConfigSpec          criConfig  CRIConfig     (Optional) CRI config is a structure contains configurations of the CRI library\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  OperatingSystemConfigPurpose     Purpose describes how the result of this OperatingSystemConfig is used by Gardener. Either it gets sent to the Worker extension controller to bootstrap a VM, or it is downloaded by the cloud-config-downloader script already running on a bootstrapped VM.\n    reloadConfigFilePath  string    (Optional) ReloadConfigFilePath is the path to the generated operating system configuration. If set, controllers are asked to use it when determining the .status.command of this resource. For example, if for CoreOS the reload-path might be \u0026ldquo;/var/lib/config\u0026rdquo;; then the controller shall set .status.command to \u0026ldquo;/usr/bin/coreos-cloudinit \u0026ndash;from-file=/var/lib/config\u0026rdquo;.\n    units  []Unit     (Optional) Units is a list of unit for the operating system configuration (usually, a systemd unit).\n    files  []File     (Optional) Files is a list of files that should get written to the host\u0026rsquo;s file system.\n       status  OperatingSystemConfigStatus     (Optional)     Worker   Worker is a specification for a Worker resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Worker    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  WorkerSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus is a raw extension field that contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the name of the region where the worker pool should be deployed to.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with these workers.\n    pools  []WorkerPool     Pools is a list of worker pools.\n       status  WorkerStatus     (Optional)     BackupBucketSpec   (Appears on: BackupBucket)  BackupBucketSpec is the spec for an BackupBucket resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this bucket.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    BackupBucketStatus   (Appears on: BackupBucket)  BackupBucketStatus is the status for an BackupBucket resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    generatedSecretRef  Kubernetes core/v1.SecretReference     (Optional) GeneratedSecretRef is reference to the secret generated by backup bucket, which will have object store specific credentials.\n    BackupEntrySpec   (Appears on: BackupEntry)  BackupEntrySpec is the spec for an BackupEntry resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    backupBucketProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) BackupBucketProviderStatus contains the provider status that has been generated by the controller responsible for the BackupBucket resource.\n    region  string    Region is the region of this Entry.\n    bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    BackupEntryStatus   (Appears on: BackupEntry)  BackupEntryStatus is the status for an BackupEntry resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    CRIConfig   (Appears on: OperatingSystemConfigSpec)  CRI config is a structure contains configurations of the CRI library\n   Field Description      name  CRIName     Name is a mandatory string containing the name of the CRI library.\n    CRIName (string alias)\n  (Appears on: CRIConfig)  CRIName is a type alias for the CRI name string.\nCloudConfig   (Appears on: OperatingSystemConfigStatus)  CloudConfig is a structure for containing the generated output for the given operating system config spec. It contains a reference to a secret as the result may contain confidential data.\n   Field Description      secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    ClusterSpec   (Appears on: Cluster)  ClusterSpec is the spec for a Cluster resource.\n   Field Description      cloudProfile  k8s.io/apimachinery/pkg/runtime.RawExtension     CloudProfile is a raw extension field that contains the cloudprofile resource referenced by the shoot that has to be reconciled.\n    seed  k8s.io/apimachinery/pkg/runtime.RawExtension     Seed is a raw extension field that contains the seed resource referenced by the shoot that has to be reconciled.\n    shoot  k8s.io/apimachinery/pkg/runtime.RawExtension     Shoot is a raw extension field that contains the shoot resource that has to be reconciled.\n    ContainerRuntimeSpec   (Appears on: ContainerRuntime)  ContainerRuntimeSpec is the spec for a ContainerRuntime resource.\n   Field Description      binaryPath  string    BinaryPath is the Worker\u0026rsquo;s machine path where container runtime extensions should copy the binaries to.\n    workerPool  ContainerRuntimeWorkerPool     WorkerPool identifies the worker pool of the Shoot. For each worker pool and type, Gardener deploys a ContainerRuntime CRD.\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    ContainerRuntimeStatus   (Appears on: ContainerRuntime)  ContainerRuntimeStatus is the status for a ContainerRuntime resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    ContainerRuntimeWorkerPool   (Appears on: ContainerRuntimeSpec)     Field Description      name  string    Name specifies the name of the worker pool the container runtime should be available for.\n    selector  Kubernetes meta/v1.LabelSelector     Selector is the label selector used by the extension to match the nodes belonging to the worker pool.\n    ControlPlaneSpec   (Appears on: ControlPlane)  ControlPlaneSpec is the spec of a ControlPlane resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  Purpose     (Optional) Purpose contains the data if a cloud provider needs additional components in order to expose the control plane.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the region of this control plane.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    ControlPlaneStatus   (Appears on: ControlPlane)  ControlPlaneStatus is the status of a ControlPlane resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    DataVolume   (Appears on: WorkerPool)  DataVolume contains information about a data volume.\n   Field Description      name  string    Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    Size is the of the root volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    DefaultSpec   (Appears on: BackupBucketSpec, BackupEntrySpec, ContainerRuntimeSpec, ControlPlaneSpec, ExtensionSpec, InfrastructureSpec, NetworkSpec, OperatingSystemConfigSpec, WorkerSpec)  DefaultSpec contains common status fields for every extension resource.\n   Field Description      type  string    Type contains the instance of the resource\u0026rsquo;s kind.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the provider specific configuration.\n    DefaultStatus   (Appears on: BackupBucketStatus, BackupEntryStatus, ContainerRuntimeStatus, ControlPlaneStatus, ExtensionStatus, InfrastructureStatus, NetworkStatus, OperatingSystemConfigStatus, WorkerStatus)  DefaultStatus contains common status fields for every extension resource.\n   Field Description      providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus contains provider-specific status.\n    conditions  []github.com/gardener/gardener/pkg/apis/core/v1beta1.Condition     (Optional) Conditions represents the latest available observations of a Seed\u0026rsquo;s current state.\n    lastError  github.com/gardener/gardener/pkg/apis/core/v1beta1.LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    lastOperation  github.com/gardener/gardener/pkg/apis/core/v1beta1.LastOperation     (Optional) LastOperation holds information about the last operation on the resource.\n    observedGeneration  int64    ObservedGeneration is the most recent generation observed for this resource.\n    state  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) State can be filled by the operating controller with what ever data it needs.\n    resources  []github.com/gardener/gardener/pkg/apis/core/v1beta1.NamedResourceReference     (Optional) Resources holds a list of named resource references that can be referred to in the state by their names.\n    DropIn   (Appears on: Unit)  DropIn is a drop-in configuration for a systemd unit.\n   Field Description      name  string    Name is the name of the drop-in.\n    content  string    Content is the content of the drop-in.\n    ExtensionSpec   (Appears on: Extension)  ExtensionSpec is the spec for a Extension resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    ExtensionStatus   (Appears on: Extension)  ExtensionStatus is the status for a Extension resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    File   (Appears on: OperatingSystemConfigSpec)  File is a file that should get written to the host\u0026rsquo;s file system. The content can either be inlined or referenced from a secret in the same namespace.\n   Field Description      path  string    Path is the path of the file system where the file should get written to.\n    permissions  int32    (Optional) Permissions describes with which permissions the file should get written to the file system. Should be defaulted to octal 0644.\n    content  FileContent     Content describe the file\u0026rsquo;s content.\n    FileContent   (Appears on: File)  FileContent can either reference a secret or contain inline configuration.\n   Field Description      secretRef  FileContentSecretRef     (Optional) SecretRef is a struct that contains information about the referenced secret.\n    inline  FileContentInline     (Optional) Inline is a struct that contains information about the inlined data.\n    FileContentInline   (Appears on: FileContent)  FileContentInline contains keys for inlining a file content\u0026rsquo;s data and encoding.\n   Field Description      encoding  string    Encoding is the file\u0026rsquo;s encoding (e.g. base64).\n    data  string    Data is the file\u0026rsquo;s data.\n    FileContentSecretRef   (Appears on: FileContent)  FileContentSecretRef contains keys for referencing a file content\u0026rsquo;s data from a secret in the same namespace.\n   Field Description      name  string    Name is the name of the secret.\n    dataKey  string    DataKey is the key in the secret\u0026rsquo;s .data field that should be read.\n    InfrastructureSpec   (Appears on: Infrastructure)  InfrastructureSpec is the spec for an Infrastructure resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this infrastructure.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with this infrastructure.\n    InfrastructureStatus   (Appears on: Infrastructure)  InfrastructureStatus is the status for an Infrastructure resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    nodesCIDR  string    (Optional) NodesCIDR is the CIDR of the node network that was optionally created by the acting extension controller. This might be needed in environments in which the CIDR for the network for the shoot worker node cannot be statically defined in the Shoot resource but must be computed dynamically.\n    MachineDeployment   (Appears on: WorkerStatus)  MachineDeployment is a created machine deployment.\n   Field Description      name  string    Name is the name of the MachineDeployment resource.\n    minimum  int32    Minimum is the minimum number for this machine deployment.\n    maximum  int32    Maximum is the maximum number for this machine deployment.\n    MachineImage   (Appears on: WorkerPool)  MachineImage contains logical information about the name and the version of the machie image that should be used. The logical information must be mapped to the provider-specific information (e.g., AMIs, \u0026hellip;) by the provider itself.\n   Field Description      name  string    Name is the logical name of the machine image.\n    version  string    Version is the version of the machine image.\n    NetworkSpec   (Appears on: Network)  NetworkSpec is the spec for an Network resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    podCIDR  string    PodCIDR defines the CIDR that will be used for pods.\n    serviceCIDR  string    ServiceCIDR defines the CIDR that will be used for services.\n    NetworkStatus   (Appears on: Network)  NetworkStatus is the status for an Network resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    Object   Object is an extension object resource.\nOperatingSystemConfigPurpose (string alias)\n  (Appears on: OperatingSystemConfigSpec)  OperatingSystemConfigPurpose is a string alias.\nOperatingSystemConfigSpec   (Appears on: OperatingSystemConfig)  OperatingSystemConfigSpec is the spec for a OperatingSystemConfig resource.\n   Field Description      criConfig  CRIConfig     (Optional) CRI config is a structure contains configurations of the CRI library\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  OperatingSystemConfigPurpose     Purpose describes how the result of this OperatingSystemConfig is used by Gardener. Either it gets sent to the Worker extension controller to bootstrap a VM, or it is downloaded by the cloud-config-downloader script already running on a bootstrapped VM.\n    reloadConfigFilePath  string    (Optional) ReloadConfigFilePath is the path to the generated operating system configuration. If set, controllers are asked to use it when determining the .status.command of this resource. For example, if for CoreOS the reload-path might be \u0026ldquo;/var/lib/config\u0026rdquo;; then the controller shall set .status.command to \u0026ldquo;/usr/bin/coreos-cloudinit \u0026ndash;from-file=/var/lib/config\u0026rdquo;.\n    units  []Unit     (Optional) Units is a list of unit for the operating system configuration (usually, a systemd unit).\n    files  []File     (Optional) Files is a list of files that should get written to the host\u0026rsquo;s file system.\n    OperatingSystemConfigStatus   (Appears on: OperatingSystemConfig)  OperatingSystemConfigStatus is the status for a OperatingSystemConfig resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    cloudConfig  CloudConfig     (Optional) CloudConfig is a structure for containing the generated output for the given operating system config spec. It contains a reference to a secret as the result may contain confidential data.\n    command  string    (Optional) Command is the command whose execution renews/reloads the cloud config on an existing VM, e.g. \u0026ldquo;/usr/bin/reload-cloud-config -from-file=\u0026rdquo;. The  is optionally provided by Gardener in the .spec.reloadConfigFilePath field.\n    units  []string    (Optional) Units is a list of systemd unit names that are part of the generated Cloud Config and shall be restarted when a new version has been downloaded.\n    Purpose (string alias)\n  (Appears on: ControlPlaneSpec)  Purpose is a string alias.\nSpec   Spec is the spec section of an Object.\nStatus   Status is the status of an Object.\nUnit   (Appears on: OperatingSystemConfigSpec)  Unit is a unit for the operating system configuration (usually, a systemd unit).\n   Field Description      name  string    Name is the name of a unit.\n    command  string    (Optional) Command is the unit\u0026rsquo;s command.\n    enable  bool    (Optional) Enable describes whether the unit is enabled or not.\n    content  string    (Optional) Content is the unit\u0026rsquo;s content.\n    dropIns  []DropIn     (Optional) DropIns is a list of drop-ins for this unit.\n    Volume   (Appears on: WorkerPool)  Volume contains information about the root disks that should be used for worker pools.\n   Field Description      name  string    (Optional) Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    Size is the of the root volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    WorkerPool   (Appears on: WorkerSpec)  WorkerPool is the definition of a specific worker pool.\n   Field Description      machineType  string    MachineType contains information about the machine type that should be used for this worker pool.\n    maximum  int32    Maximum is the maximum size of the worker pool.\n    maxSurge  k8s.io/apimachinery/pkg/util/intstr.IntOrString     MaxSurge is maximum number of VMs that are created during an update.\n    maxUnavailable  k8s.io/apimachinery/pkg/util/intstr.IntOrString     MaxUnavailable is the maximum number of VMs that can be unavailable during an update.\n    annotations  map[string]string    (Optional) Annotations is a map of key/value pairs for annotations for all the Node objects in this worker pool.\n    labels  map[string]string    (Optional) Labels is a map of key/value pairs for labels for all the Node objects in this worker pool.\n    taints  []Kubernetes core/v1.Taint     (Optional) Taints is a list of taints for all the Node objects in this worker pool.\n    machineImage  MachineImage     MachineImage contains logical information about the name and the version of the machie image that should be used. The logical information must be mapped to the provider-specific information (e.g., AMIs, \u0026hellip;) by the provider itself.\n    minimum  int32    Minimum is the minimum size of the worker pool.\n    name  string    Name is the name of this worker pool.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is a provider specific configuration for the worker pool.\n    userData  []byte    UserData is a base64-encoded string that contains the data that is sent to the provider\u0026rsquo;s APIs when a new machine/VM that is part of this worker pool shall be spawned.\n    volume  Volume     (Optional) Volume contains information about the root disks that should be used for this worker pool.\n    dataVolumes  []DataVolume     (Optional) DataVolumes contains a list of additional worker volumes.\n    kubeletDataVolumeName  string    (Optional) KubeletDataVolumeName contains the name of a dataVolume that should be used for storing kubelet state.\n    zones  []string    (Optional) Zones contains information about availability zones for this worker pool.\n    machineControllerManager  github.com/gardener/gardener/pkg/apis/core/v1beta1.MachineControllerManagerSettings     (Optional) MachineControllerManagerSettings contains configurations for different worker-pools. Eg. MachineDrainTimeout, MachineHealthTimeout.\n    WorkerSpec   (Appears on: Worker)  WorkerSpec is the spec for a Worker resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus is a raw extension field that contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the name of the region where the worker pool should be deployed to.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with these workers.\n    pools  []WorkerPool     Pools is a list of worker pools.\n    WorkerStatus   (Appears on: Worker)  WorkerStatus is the status for a Worker resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    machineDeployments  []MachineDeployment     MachineDeployments is a list of created machine deployments. It will be used to e.g. configure the cluster-autoscaler properly.\n      Generated with gen-crd-api-reference-docs \n"},{"uri":"https://gardener.cloud/v1.12.8/references/extensions/","title":"Extensions","tags":[],"description":"","content":"Packages:\n  extensions.gardener.cloud/v1alpha1   extensions.gardener.cloud/v1alpha1  Package v1alpha1 is the v1alpha1 version of the API.\nResource Types:  BackupBucket  BackupEntry  Cluster  ContainerRuntime  ControlPlane  Extension  Infrastructure  Network  OperatingSystemConfig  Worker  BackupBucket   BackupBucket is a specification for backup bucket.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  BackupBucket    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupBucketSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this bucket.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n       status  BackupBucketStatus     (Optional)     BackupEntry   BackupEntry is a specification for backup Entry.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  BackupEntry    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupEntrySpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    backupBucketProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) BackupBucketProviderStatus contains the provider status that has been generated by the controller responsible for the BackupBucket resource.\n    region  string    Region is the region of this Entry.\n    bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n       status  BackupEntryStatus     (Optional)     Cluster   Cluster is a specification for a Cluster resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Cluster    metadata  Kubernetes meta/v1.ObjectMeta     Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ClusterSpec          cloudProfile  k8s.io/apimachinery/pkg/runtime.RawExtension     CloudProfile is a raw extension field that contains the cloudprofile resource referenced by the shoot that has to be reconciled.\n    seed  k8s.io/apimachinery/pkg/runtime.RawExtension     Seed is a raw extension field that contains the seed resource referenced by the shoot that has to be reconciled.\n    shoot  k8s.io/apimachinery/pkg/runtime.RawExtension     Shoot is a raw extension field that contains the shoot resource that has to be reconciled.\n       ContainerRuntime   ContainerRuntime is a specification for a container runtime resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  ContainerRuntime    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ContainerRuntimeSpec          binaryPath  string    BinaryPath is the Worker\u0026rsquo;s machine path where container runtime extensions should copy the binaries to.\n    workerPool  ContainerRuntimeWorkerPool     WorkerPool identifies the worker pool of the Shoot. For each worker pool and type, Gardener deploys a ContainerRuntime CRD.\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n       status  ContainerRuntimeStatus     (Optional)     ControlPlane   ControlPlane is a specification for a ControlPlane resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  ControlPlane    metadata  Kubernetes meta/v1.ObjectMeta     Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControlPlaneSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  Purpose     (Optional) Purpose contains the data if a cloud provider needs additional components in order to expose the control plane.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the region of this control plane.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n       status  ControlPlaneStatus     (Optional)     Extension   Extension is a specification for a Extension resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Extension    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ExtensionSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n       status  ExtensionStatus     (Optional)     Infrastructure   Infrastructure is a specification for cloud provider infrastructure.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Infrastructure    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  InfrastructureSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this infrastructure.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with this infrastructure.\n       status  InfrastructureStatus     (Optional)     Network   Network is the specification for cluster networking.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Network    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  NetworkSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    podCIDR  string    PodCIDR defines the CIDR that will be used for pods.\n    serviceCIDR  string    ServiceCIDR defines the CIDR that will be used for services.\n       status  NetworkStatus     (Optional)     OperatingSystemConfig   OperatingSystemConfig is a specification for a OperatingSystemConfig resource\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  OperatingSystemConfig    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  OperatingSystemConfigSpec          criConfig  CRIConfig     (Optional) CRI config is a structure contains configurations of the CRI library\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  OperatingSystemConfigPurpose     Purpose describes how the result of this OperatingSystemConfig is used by Gardener. Either it gets sent to the Worker extension controller to bootstrap a VM, or it is downloaded by the cloud-config-downloader script already running on a bootstrapped VM.\n    reloadConfigFilePath  string    (Optional) ReloadConfigFilePath is the path to the generated operating system configuration. If set, controllers are asked to use it when determining the .status.command of this resource. For example, if for CoreOS the reload-path might be \u0026ldquo;/var/lib/config\u0026rdquo;; then the controller shall set .status.command to \u0026ldquo;/usr/bin/coreos-cloudinit \u0026ndash;from-file=/var/lib/config\u0026rdquo;.\n    units  []Unit     (Optional) Units is a list of unit for the operating system configuration (usually, a systemd unit).\n    files  []File     (Optional) Files is a list of files that should get written to the host\u0026rsquo;s file system.\n       status  OperatingSystemConfigStatus     (Optional)     Worker   Worker is a specification for a Worker resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Worker    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  WorkerSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus is a raw extension field that contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the name of the region where the worker pool should be deployed to.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with these workers.\n    pools  []WorkerPool     Pools is a list of worker pools.\n       status  WorkerStatus     (Optional)     BackupBucketSpec   (Appears on: BackupBucket)  BackupBucketSpec is the spec for an BackupBucket resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this bucket.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    BackupBucketStatus   (Appears on: BackupBucket)  BackupBucketStatus is the status for an BackupBucket resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    generatedSecretRef  Kubernetes core/v1.SecretReference     (Optional) GeneratedSecretRef is reference to the secret generated by backup bucket, which will have object store specific credentials.\n    BackupEntrySpec   (Appears on: BackupEntry)  BackupEntrySpec is the spec for an BackupEntry resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    backupBucketProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) BackupBucketProviderStatus contains the provider status that has been generated by the controller responsible for the BackupBucket resource.\n    region  string    Region is the region of this Entry.\n    bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    BackupEntryStatus   (Appears on: BackupEntry)  BackupEntryStatus is the status for an BackupEntry resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    CRIConfig   (Appears on: OperatingSystemConfigSpec)  CRI config is a structure contains configurations of the CRI library\n   Field Description      name  CRIName     Name is a mandatory string containing the name of the CRI library.\n    CRIName (string alias)\n  (Appears on: CRIConfig)  CRIName is a type alias for the CRI name string.\nCloudConfig   (Appears on: OperatingSystemConfigStatus)  CloudConfig is a structure for containing the generated output for the given operating system config spec. It contains a reference to a secret as the result may contain confidential data.\n   Field Description      secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    ClusterSpec   (Appears on: Cluster)  ClusterSpec is the spec for a Cluster resource.\n   Field Description      cloudProfile  k8s.io/apimachinery/pkg/runtime.RawExtension     CloudProfile is a raw extension field that contains the cloudprofile resource referenced by the shoot that has to be reconciled.\n    seed  k8s.io/apimachinery/pkg/runtime.RawExtension     Seed is a raw extension field that contains the seed resource referenced by the shoot that has to be reconciled.\n    shoot  k8s.io/apimachinery/pkg/runtime.RawExtension     Shoot is a raw extension field that contains the shoot resource that has to be reconciled.\n    ContainerRuntimeSpec   (Appears on: ContainerRuntime)  ContainerRuntimeSpec is the spec for a ContainerRuntime resource.\n   Field Description      binaryPath  string    BinaryPath is the Worker\u0026rsquo;s machine path where container runtime extensions should copy the binaries to.\n    workerPool  ContainerRuntimeWorkerPool     WorkerPool identifies the worker pool of the Shoot. For each worker pool and type, Gardener deploys a ContainerRuntime CRD.\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    ContainerRuntimeStatus   (Appears on: ContainerRuntime)  ContainerRuntimeStatus is the status for a ContainerRuntime resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    ContainerRuntimeWorkerPool   (Appears on: ContainerRuntimeSpec)     Field Description      name  string    Name specifies the name of the worker pool the container runtime should be available for.\n    selector  Kubernetes meta/v1.LabelSelector     Selector is the label selector used by the extension to match the nodes belonging to the worker pool.\n    ControlPlaneSpec   (Appears on: ControlPlane)  ControlPlaneSpec is the spec of a ControlPlane resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  Purpose     (Optional) Purpose contains the data if a cloud provider needs additional components in order to expose the control plane.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the region of this control plane.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    ControlPlaneStatus   (Appears on: ControlPlane)  ControlPlaneStatus is the status of a ControlPlane resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    DataVolume   (Appears on: WorkerPool)  DataVolume contains information about a data volume.\n   Field Description      name  string    Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    Size is the of the root volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    DefaultSpec   (Appears on: BackupBucketSpec, BackupEntrySpec, ContainerRuntimeSpec, ControlPlaneSpec, ExtensionSpec, InfrastructureSpec, NetworkSpec, OperatingSystemConfigSpec, WorkerSpec)  DefaultSpec contains common status fields for every extension resource.\n   Field Description      type  string    Type contains the instance of the resource\u0026rsquo;s kind.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the provider specific configuration.\n    DefaultStatus   (Appears on: BackupBucketStatus, BackupEntryStatus, ContainerRuntimeStatus, ControlPlaneStatus, ExtensionStatus, InfrastructureStatus, NetworkStatus, OperatingSystemConfigStatus, WorkerStatus)  DefaultStatus contains common status fields for every extension resource.\n   Field Description      providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus contains provider-specific status.\n    conditions  []github.com/gardener/gardener/pkg/apis/core/v1beta1.Condition     (Optional) Conditions represents the latest available observations of a Seed\u0026rsquo;s current state.\n    lastError  github.com/gardener/gardener/pkg/apis/core/v1beta1.LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    lastOperation  github.com/gardener/gardener/pkg/apis/core/v1beta1.LastOperation     (Optional) LastOperation holds information about the last operation on the resource.\n    observedGeneration  int64    ObservedGeneration is the most recent generation observed for this resource.\n    state  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) State can be filled by the operating controller with what ever data it needs.\n    resources  []github.com/gardener/gardener/pkg/apis/core/v1beta1.NamedResourceReference     (Optional) Resources holds a list of named resource references that can be referred to in the state by their names.\n    DropIn   (Appears on: Unit)  DropIn is a drop-in configuration for a systemd unit.\n   Field Description      name  string    Name is the name of the drop-in.\n    content  string    Content is the content of the drop-in.\n    ExtensionSpec   (Appears on: Extension)  ExtensionSpec is the spec for a Extension resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    ExtensionStatus   (Appears on: Extension)  ExtensionStatus is the status for a Extension resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    File   (Appears on: OperatingSystemConfigSpec)  File is a file that should get written to the host\u0026rsquo;s file system. The content can either be inlined or referenced from a secret in the same namespace.\n   Field Description      path  string    Path is the path of the file system where the file should get written to.\n    permissions  int32    (Optional) Permissions describes with which permissions the file should get written to the file system. Should be defaulted to octal 0644.\n    content  FileContent     Content describe the file\u0026rsquo;s content.\n    FileContent   (Appears on: File)  FileContent can either reference a secret or contain inline configuration.\n   Field Description      secretRef  FileContentSecretRef     (Optional) SecretRef is a struct that contains information about the referenced secret.\n    inline  FileContentInline     (Optional) Inline is a struct that contains information about the inlined data.\n    FileContentInline   (Appears on: FileContent)  FileContentInline contains keys for inlining a file content\u0026rsquo;s data and encoding.\n   Field Description      encoding  string    Encoding is the file\u0026rsquo;s encoding (e.g. base64).\n    data  string    Data is the file\u0026rsquo;s data.\n    FileContentSecretRef   (Appears on: FileContent)  FileContentSecretRef contains keys for referencing a file content\u0026rsquo;s data from a secret in the same namespace.\n   Field Description      name  string    Name is the name of the secret.\n    dataKey  string    DataKey is the key in the secret\u0026rsquo;s .data field that should be read.\n    InfrastructureSpec   (Appears on: Infrastructure)  InfrastructureSpec is the spec for an Infrastructure resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this infrastructure.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with this infrastructure.\n    InfrastructureStatus   (Appears on: Infrastructure)  InfrastructureStatus is the status for an Infrastructure resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    nodesCIDR  string    (Optional) NodesCIDR is the CIDR of the node network that was optionally created by the acting extension controller. This might be needed in environments in which the CIDR for the network for the shoot worker node cannot be statically defined in the Shoot resource but must be computed dynamically.\n    MachineDeployment   (Appears on: WorkerStatus)  MachineDeployment is a created machine deployment.\n   Field Description      name  string    Name is the name of the MachineDeployment resource.\n    minimum  int32    Minimum is the minimum number for this machine deployment.\n    maximum  int32    Maximum is the maximum number for this machine deployment.\n    MachineImage   (Appears on: WorkerPool)  MachineImage contains logical information about the name and the version of the machie image that should be used. The logical information must be mapped to the provider-specific information (e.g., AMIs, \u0026hellip;) by the provider itself.\n   Field Description      name  string    Name is the logical name of the machine image.\n    version  string    Version is the version of the machine image.\n    NetworkSpec   (Appears on: Network)  NetworkSpec is the spec for an Network resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    podCIDR  string    PodCIDR defines the CIDR that will be used for pods.\n    serviceCIDR  string    ServiceCIDR defines the CIDR that will be used for services.\n    NetworkStatus   (Appears on: Network)  NetworkStatus is the status for an Network resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    Object   Object is an extension object resource.\nOperatingSystemConfigPurpose (string alias)\n  (Appears on: OperatingSystemConfigSpec)  OperatingSystemConfigPurpose is a string alias.\nOperatingSystemConfigSpec   (Appears on: OperatingSystemConfig)  OperatingSystemConfigSpec is the spec for a OperatingSystemConfig resource.\n   Field Description      criConfig  CRIConfig     (Optional) CRI config is a structure contains configurations of the CRI library\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  OperatingSystemConfigPurpose     Purpose describes how the result of this OperatingSystemConfig is used by Gardener. Either it gets sent to the Worker extension controller to bootstrap a VM, or it is downloaded by the cloud-config-downloader script already running on a bootstrapped VM.\n    reloadConfigFilePath  string    (Optional) ReloadConfigFilePath is the path to the generated operating system configuration. If set, controllers are asked to use it when determining the .status.command of this resource. For example, if for CoreOS the reload-path might be \u0026ldquo;/var/lib/config\u0026rdquo;; then the controller shall set .status.command to \u0026ldquo;/usr/bin/coreos-cloudinit \u0026ndash;from-file=/var/lib/config\u0026rdquo;.\n    units  []Unit     (Optional) Units is a list of unit for the operating system configuration (usually, a systemd unit).\n    files  []File     (Optional) Files is a list of files that should get written to the host\u0026rsquo;s file system.\n    OperatingSystemConfigStatus   (Appears on: OperatingSystemConfig)  OperatingSystemConfigStatus is the status for a OperatingSystemConfig resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    cloudConfig  CloudConfig     (Optional) CloudConfig is a structure for containing the generated output for the given operating system config spec. It contains a reference to a secret as the result may contain confidential data.\n    command  string    (Optional) Command is the command whose execution renews/reloads the cloud config on an existing VM, e.g. \u0026ldquo;/usr/bin/reload-cloud-config -from-file=\u0026rdquo;. The  is optionally provided by Gardener in the .spec.reloadConfigFilePath field.\n    units  []string    (Optional) Units is a list of systemd unit names that are part of the generated Cloud Config and shall be restarted when a new version has been downloaded.\n    Purpose (string alias)\n  (Appears on: ControlPlaneSpec)  Purpose is a string alias.\nSpec   Spec is the spec section of an Object.\nStatus   Status is the status of an Object.\nUnit   (Appears on: OperatingSystemConfigSpec)  Unit is a unit for the operating system configuration (usually, a systemd unit).\n   Field Description      name  string    Name is the name of a unit.\n    command  string    (Optional) Command is the unit\u0026rsquo;s command.\n    enable  bool    (Optional) Enable describes whether the unit is enabled or not.\n    content  string    (Optional) Content is the unit\u0026rsquo;s content.\n    dropIns  []DropIn     (Optional) DropIns is a list of drop-ins for this unit.\n    Volume   (Appears on: WorkerPool)  Volume contains information about the root disks that should be used for worker pools.\n   Field Description      name  string    (Optional) Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    Size is the of the root volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    WorkerPool   (Appears on: WorkerSpec)  WorkerPool is the definition of a specific worker pool.\n   Field Description      machineType  string    MachineType contains information about the machine type that should be used for this worker pool.\n    maximum  int32    Maximum is the maximum size of the worker pool.\n    maxSurge  k8s.io/apimachinery/pkg/util/intstr.IntOrString     MaxSurge is maximum number of VMs that are created during an update.\n    maxUnavailable  k8s.io/apimachinery/pkg/util/intstr.IntOrString     MaxUnavailable is the maximum number of VMs that can be unavailable during an update.\n    annotations  map[string]string    (Optional) Annotations is a map of key/value pairs for annotations for all the Node objects in this worker pool.\n    labels  map[string]string    (Optional) Labels is a map of key/value pairs for labels for all the Node objects in this worker pool.\n    taints  []Kubernetes core/v1.Taint     (Optional) Taints is a list of taints for all the Node objects in this worker pool.\n    machineImage  MachineImage     MachineImage contains logical information about the name and the version of the machie image that should be used. The logical information must be mapped to the provider-specific information (e.g., AMIs, \u0026hellip;) by the provider itself.\n    minimum  int32    Minimum is the minimum size of the worker pool.\n    name  string    Name is the name of this worker pool.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is a provider specific configuration for the worker pool.\n    userData  []byte    UserData is a base64-encoded string that contains the data that is sent to the provider\u0026rsquo;s APIs when a new machine/VM that is part of this worker pool shall be spawned.\n    volume  Volume     (Optional) Volume contains information about the root disks that should be used for this worker pool.\n    dataVolumes  []DataVolume     (Optional) DataVolumes contains a list of additional worker volumes.\n    kubeletDataVolumeName  string    (Optional) KubeletDataVolumeName contains the name of a dataVolume that should be used for storing kubelet state.\n    zones  []string    (Optional) Zones contains information about availability zones for this worker pool.\n    machineControllerManager  github.com/gardener/gardener/pkg/apis/core/v1beta1.MachineControllerManagerSettings     (Optional) MachineControllerManagerSettings contains configurations for different worker-pools. Eg. MachineDrainTimeout, MachineHealthTimeout.\n    WorkerSpec   (Appears on: Worker)  WorkerSpec is the spec for a Worker resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus is a raw extension field that contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the name of the region where the worker pool should be deployed to.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with these workers.\n    pools  []WorkerPool     Pools is a list of worker pools.\n    WorkerStatus   (Appears on: Worker)  WorkerStatus is the status for a Worker resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    machineDeployments  []MachineDeployment     MachineDeployments is a list of created machine deployments. It will be used to e.g. configure the cluster-autoscaler properly.\n      Generated with gen-crd-api-reference-docs \n"},{"uri":"https://gardener.cloud/v1.13.2/references/extensions/","title":"Extensions","tags":[],"description":"","content":"Packages:\n  extensions.gardener.cloud/v1alpha1   extensions.gardener.cloud/v1alpha1  Package v1alpha1 is the v1alpha1 version of the API.\nResource Types:  BackupBucket  BackupEntry  Cluster  ContainerRuntime  ControlPlane  Extension  Infrastructure  Network  OperatingSystemConfig  Worker  BackupBucket   BackupBucket is a specification for backup bucket.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  BackupBucket    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupBucketSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this bucket.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n       status  BackupBucketStatus     (Optional)     BackupEntry   BackupEntry is a specification for backup Entry.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  BackupEntry    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  BackupEntrySpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    backupBucketProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) BackupBucketProviderStatus contains the provider status that has been generated by the controller responsible for the BackupBucket resource.\n    region  string    Region is the region of this Entry.\n    bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n       status  BackupEntryStatus     (Optional)     Cluster   Cluster is a specification for a Cluster resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Cluster    metadata  Kubernetes meta/v1.ObjectMeta     Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ClusterSpec          cloudProfile  k8s.io/apimachinery/pkg/runtime.RawExtension     CloudProfile is a raw extension field that contains the cloudprofile resource referenced by the shoot that has to be reconciled.\n    seed  k8s.io/apimachinery/pkg/runtime.RawExtension     Seed is a raw extension field that contains the seed resource referenced by the shoot that has to be reconciled.\n    shoot  k8s.io/apimachinery/pkg/runtime.RawExtension     Shoot is a raw extension field that contains the shoot resource that has to be reconciled.\n       ContainerRuntime   ContainerRuntime is a specification for a container runtime resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  ContainerRuntime    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ContainerRuntimeSpec          binaryPath  string    BinaryPath is the Worker\u0026rsquo;s machine path where container runtime extensions should copy the binaries to.\n    workerPool  ContainerRuntimeWorkerPool     WorkerPool identifies the worker pool of the Shoot. For each worker pool and type, Gardener deploys a ContainerRuntime CRD.\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n       status  ContainerRuntimeStatus     (Optional)     ControlPlane   ControlPlane is a specification for a ControlPlane resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  ControlPlane    metadata  Kubernetes meta/v1.ObjectMeta     Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ControlPlaneSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  Purpose     (Optional) Purpose contains the data if a cloud provider needs additional components in order to expose the control plane.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the region of this control plane.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n       status  ControlPlaneStatus     (Optional)     Extension   Extension is a specification for a Extension resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Extension    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  ExtensionSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n       status  ExtensionStatus     (Optional)     Infrastructure   Infrastructure is a specification for cloud provider infrastructure.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Infrastructure    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  InfrastructureSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this infrastructure.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with this infrastructure.\n       status  InfrastructureStatus     (Optional)     Network   Network is the specification for cluster networking.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Network    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  NetworkSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    podCIDR  string    PodCIDR defines the CIDR that will be used for pods.\n    serviceCIDR  string    ServiceCIDR defines the CIDR that will be used for services.\n       status  NetworkStatus     (Optional)     OperatingSystemConfig   OperatingSystemConfig is a specification for a OperatingSystemConfig resource\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  OperatingSystemConfig    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  OperatingSystemConfigSpec          criConfig  CRIConfig     (Optional) CRI config is a structure contains configurations of the CRI library\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  OperatingSystemConfigPurpose     Purpose describes how the result of this OperatingSystemConfig is used by Gardener. Either it gets sent to the Worker extension controller to bootstrap a VM, or it is downloaded by the cloud-config-downloader script already running on a bootstrapped VM.\n    reloadConfigFilePath  string    (Optional) ReloadConfigFilePath is the path to the generated operating system configuration. If set, controllers are asked to use it when determining the .status.command of this resource. For example, if for CoreOS the reload-path might be \u0026ldquo;/var/lib/config\u0026rdquo;; then the controller shall set .status.command to \u0026ldquo;/usr/bin/coreos-cloudinit \u0026ndash;from-file=/var/lib/config\u0026rdquo;.\n    units  []Unit     (Optional) Units is a list of unit for the operating system configuration (usually, a systemd unit).\n    files  []File     (Optional) Files is a list of files that should get written to the host\u0026rsquo;s file system.\n       status  OperatingSystemConfigStatus     (Optional)     Worker   Worker is a specification for a Worker resource.\n   Field Description      apiVersion string   extensions.gardener.cloud/v1alpha1      kind string  Worker    metadata  Kubernetes meta/v1.ObjectMeta     (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field.     spec  WorkerSpec          DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus is a raw extension field that contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the name of the region where the worker pool should be deployed to.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with these workers.\n    pools  []WorkerPool     Pools is a list of worker pools.\n       status  WorkerStatus     (Optional)     BackupBucketSpec   (Appears on: BackupBucket)  BackupBucketSpec is the spec for an BackupBucket resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this bucket.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    BackupBucketStatus   (Appears on: BackupBucket)  BackupBucketStatus is the status for an BackupBucket resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    generatedSecretRef  Kubernetes core/v1.SecretReference     (Optional) GeneratedSecretRef is reference to the secret generated by backup bucket, which will have object store specific credentials.\n    BackupEntrySpec   (Appears on: BackupEntry)  BackupEntrySpec is the spec for an BackupEntry resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    backupBucketProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) BackupBucketProviderStatus contains the provider status that has been generated by the controller responsible for the BackupBucket resource.\n    region  string    Region is the region of this Entry.\n    bucketName  string    BucketName is the name of backup bucket for this Backup Entry.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the credentials to access object store.\n    BackupEntryStatus   (Appears on: BackupEntry)  BackupEntryStatus is the status for an BackupEntry resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    CRIConfig   (Appears on: OperatingSystemConfigSpec)  CRI config is a structure contains configurations of the CRI library\n   Field Description      name  CRIName     Name is a mandatory string containing the name of the CRI library.\n    CRIName (string alias)\n  (Appears on: CRIConfig)  CRIName is a type alias for the CRI name string.\nCloudConfig   (Appears on: OperatingSystemConfigStatus)  CloudConfig is a structure for containing the generated output for the given operating system config spec. It contains a reference to a secret as the result may contain confidential data.\n   Field Description      secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    ClusterSpec   (Appears on: Cluster)  ClusterSpec is the spec for a Cluster resource.\n   Field Description      cloudProfile  k8s.io/apimachinery/pkg/runtime.RawExtension     CloudProfile is a raw extension field that contains the cloudprofile resource referenced by the shoot that has to be reconciled.\n    seed  k8s.io/apimachinery/pkg/runtime.RawExtension     Seed is a raw extension field that contains the seed resource referenced by the shoot that has to be reconciled.\n    shoot  k8s.io/apimachinery/pkg/runtime.RawExtension     Shoot is a raw extension field that contains the shoot resource that has to be reconciled.\n    ContainerRuntimeSpec   (Appears on: ContainerRuntime)  ContainerRuntimeSpec is the spec for a ContainerRuntime resource.\n   Field Description      binaryPath  string    BinaryPath is the Worker\u0026rsquo;s machine path where container runtime extensions should copy the binaries to.\n    workerPool  ContainerRuntimeWorkerPool     WorkerPool identifies the worker pool of the Shoot. For each worker pool and type, Gardener deploys a ContainerRuntime CRD.\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    ContainerRuntimeStatus   (Appears on: ContainerRuntime)  ContainerRuntimeStatus is the status for a ContainerRuntime resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    ContainerRuntimeWorkerPool   (Appears on: ContainerRuntimeSpec)     Field Description      name  string    Name specifies the name of the worker pool the container runtime should be available for.\n    selector  Kubernetes meta/v1.LabelSelector     Selector is the label selector used by the extension to match the nodes belonging to the worker pool.\n    ControlPlaneSpec   (Appears on: ControlPlane)  ControlPlaneSpec is the spec of a ControlPlane resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  Purpose     (Optional) Purpose contains the data if a cloud provider needs additional components in order to expose the control plane.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the region of this control plane.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    ControlPlaneStatus   (Appears on: ControlPlane)  ControlPlaneStatus is the status of a ControlPlane resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    DataVolume   (Appears on: WorkerPool)  DataVolume contains information about a data volume.\n   Field Description      name  string    Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    Size is the of the root volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    DefaultSpec   (Appears on: BackupBucketSpec, BackupEntrySpec, ContainerRuntimeSpec, ControlPlaneSpec, ExtensionSpec, InfrastructureSpec, NetworkSpec, OperatingSystemConfigSpec, WorkerSpec)  DefaultSpec contains common status fields for every extension resource.\n   Field Description      type  string    Type contains the instance of the resource\u0026rsquo;s kind.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is the provider specific configuration.\n    DefaultStatus   (Appears on: BackupBucketStatus, BackupEntryStatus, ContainerRuntimeStatus, ControlPlaneStatus, ExtensionStatus, InfrastructureStatus, NetworkStatus, OperatingSystemConfigStatus, WorkerStatus)  DefaultStatus contains common status fields for every extension resource.\n   Field Description      providerStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderStatus contains provider-specific status.\n    conditions  []github.com/gardener/gardener/pkg/apis/core/v1beta1.Condition     (Optional) Conditions represents the latest available observations of a Seed\u0026rsquo;s current state.\n    lastError  github.com/gardener/gardener/pkg/apis/core/v1beta1.LastError     (Optional) LastError holds information about the last occurred error during an operation.\n    lastOperation  github.com/gardener/gardener/pkg/apis/core/v1beta1.LastOperation     (Optional) LastOperation holds information about the last operation on the resource.\n    observedGeneration  int64    ObservedGeneration is the most recent generation observed for this resource.\n    state  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) State can be filled by the operating controller with what ever data it needs.\n    resources  []github.com/gardener/gardener/pkg/apis/core/v1beta1.NamedResourceReference     (Optional) Resources holds a list of named resource references that can be referred to in the state by their names.\n    DropIn   (Appears on: Unit)  DropIn is a drop-in configuration for a systemd unit.\n   Field Description      name  string    Name is the name of the drop-in.\n    content  string    Content is the content of the drop-in.\n    ExtensionSpec   (Appears on: Extension)  ExtensionSpec is the spec for a Extension resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    ExtensionStatus   (Appears on: Extension)  ExtensionStatus is the status for a Extension resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    File   (Appears on: OperatingSystemConfigSpec)  File is a file that should get written to the host\u0026rsquo;s file system. The content can either be inlined or referenced from a secret in the same namespace.\n   Field Description      path  string    Path is the path of the file system where the file should get written to.\n    permissions  int32    (Optional) Permissions describes with which permissions the file should get written to the file system. Should be defaulted to octal 0644.\n    content  FileContent     Content describe the file\u0026rsquo;s content.\n    FileContent   (Appears on: File)  FileContent can either reference a secret or contain inline configuration.\n   Field Description      secretRef  FileContentSecretRef     (Optional) SecretRef is a struct that contains information about the referenced secret.\n    inline  FileContentInline     (Optional) Inline is a struct that contains information about the inlined data.\n    FileContentInline   (Appears on: FileContent)  FileContentInline contains keys for inlining a file content\u0026rsquo;s data and encoding.\n   Field Description      encoding  string    Encoding is the file\u0026rsquo;s encoding (e.g. base64).\n    data  string    Data is the file\u0026rsquo;s data.\n    FileContentSecretRef   (Appears on: FileContent)  FileContentSecretRef contains keys for referencing a file content\u0026rsquo;s data from a secret in the same namespace.\n   Field Description      name  string    Name is the name of the secret.\n    dataKey  string    DataKey is the key in the secret\u0026rsquo;s .data field that should be read.\n    InfrastructureSpec   (Appears on: Infrastructure)  InfrastructureSpec is the spec for an Infrastructure resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    region  string    Region is the region of this infrastructure.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the actual result of the generated cloud config.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with this infrastructure.\n    InfrastructureStatus   (Appears on: Infrastructure)  InfrastructureStatus is the status for an Infrastructure resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    nodesCIDR  string    (Optional) NodesCIDR is the CIDR of the node network that was optionally created by the acting extension controller. This might be needed in environments in which the CIDR for the network for the shoot worker node cannot be statically defined in the Shoot resource but must be computed dynamically.\n    MachineDeployment   (Appears on: WorkerStatus)  MachineDeployment is a created machine deployment.\n   Field Description      name  string    Name is the name of the MachineDeployment resource.\n    minimum  int32    Minimum is the minimum number for this machine deployment.\n    maximum  int32    Maximum is the maximum number for this machine deployment.\n    MachineImage   (Appears on: WorkerPool)  MachineImage contains logical information about the name and the version of the machie image that should be used. The logical information must be mapped to the provider-specific information (e.g., AMIs, \u0026hellip;) by the provider itself.\n   Field Description      name  string    Name is the logical name of the machine image.\n    version  string    Version is the version of the machine image.\n    NetworkSpec   (Appears on: Network)  NetworkSpec is the spec for an Network resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    podCIDR  string    PodCIDR defines the CIDR that will be used for pods.\n    serviceCIDR  string    ServiceCIDR defines the CIDR that will be used for services.\n    NetworkStatus   (Appears on: Network)  NetworkStatus is the status for an Network resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    Object   Object is an extension object resource.\nOperatingSystemConfigPurpose (string alias)\n  (Appears on: OperatingSystemConfigSpec)  OperatingSystemConfigPurpose is a string alias.\nOperatingSystemConfigSpec   (Appears on: OperatingSystemConfig)  OperatingSystemConfigSpec is the spec for a OperatingSystemConfig resource.\n   Field Description      criConfig  CRIConfig     (Optional) CRI config is a structure contains configurations of the CRI library\n    DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    purpose  OperatingSystemConfigPurpose     Purpose describes how the result of this OperatingSystemConfig is used by Gardener. Either it gets sent to the Worker extension controller to bootstrap a VM, or it is downloaded by the cloud-config-downloader script already running on a bootstrapped VM.\n    reloadConfigFilePath  string    (Optional) ReloadConfigFilePath is the path to the generated operating system configuration. If set, controllers are asked to use it when determining the .status.command of this resource. For example, if for CoreOS the reload-path might be \u0026ldquo;/var/lib/config\u0026rdquo;; then the controller shall set .status.command to \u0026ldquo;/usr/bin/coreos-cloudinit \u0026ndash;from-file=/var/lib/config\u0026rdquo;.\n    units  []Unit     (Optional) Units is a list of unit for the operating system configuration (usually, a systemd unit).\n    files  []File     (Optional) Files is a list of files that should get written to the host\u0026rsquo;s file system.\n    OperatingSystemConfigStatus   (Appears on: OperatingSystemConfig)  OperatingSystemConfigStatus is the status for a OperatingSystemConfig resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    cloudConfig  CloudConfig     (Optional) CloudConfig is a structure for containing the generated output for the given operating system config spec. It contains a reference to a secret as the result may contain confidential data.\n    command  string    (Optional) Command is the command whose execution renews/reloads the cloud config on an existing VM, e.g. \u0026ldquo;/usr/bin/reload-cloud-config -from-file=\u0026rdquo;. The  is optionally provided by Gardener in the .spec.reloadConfigFilePath field.\n    units  []string    (Optional) Units is a list of systemd unit names that are part of the generated Cloud Config and shall be restarted when a new version has been downloaded.\n    Purpose (string alias)\n  (Appears on: ControlPlaneSpec)  Purpose is a string alias.\nSpec   Spec is the spec section of an Object.\nStatus   Status is the status of an Object.\nUnit   (Appears on: OperatingSystemConfigSpec)  Unit is a unit for the operating system configuration (usually, a systemd unit).\n   Field Description      name  string    Name is the name of a unit.\n    command  string    (Optional) Command is the unit\u0026rsquo;s command.\n    enable  bool    (Optional) Enable describes whether the unit is enabled or not.\n    content  string    (Optional) Content is the unit\u0026rsquo;s content.\n    dropIns  []DropIn     (Optional) DropIns is a list of drop-ins for this unit.\n    Volume   (Appears on: WorkerPool)  Volume contains information about the root disks that should be used for worker pools.\n   Field Description      name  string    (Optional) Name of the volume to make it referencable.\n    type  string    (Optional) Type is the type of the volume.\n    size  string    Size is the of the root volume.\n    encrypted  bool    (Optional) Encrypted determines if the volume should be encrypted.\n    WorkerPool   (Appears on: WorkerSpec)  WorkerPool is the definition of a specific worker pool.\n   Field Description      machineType  string    MachineType contains information about the machine type that should be used for this worker pool.\n    maximum  int32    Maximum is the maximum size of the worker pool.\n    maxSurge  k8s.io/apimachinery/pkg/util/intstr.IntOrString     MaxSurge is maximum number of VMs that are created during an update.\n    maxUnavailable  k8s.io/apimachinery/pkg/util/intstr.IntOrString     MaxUnavailable is the maximum number of VMs that can be unavailable during an update.\n    annotations  map[string]string    (Optional) Annotations is a map of key/value pairs for annotations for all the Node objects in this worker pool.\n    labels  map[string]string    (Optional) Labels is a map of key/value pairs for labels for all the Node objects in this worker pool.\n    taints  []Kubernetes core/v1.Taint     (Optional) Taints is a list of taints for all the Node objects in this worker pool.\n    machineImage  MachineImage     MachineImage contains logical information about the name and the version of the machie image that should be used. The logical information must be mapped to the provider-specific information (e.g., AMIs, \u0026hellip;) by the provider itself.\n    minimum  int32    Minimum is the minimum size of the worker pool.\n    name  string    Name is the name of this worker pool.\n    providerConfig  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) ProviderConfig is a provider specific configuration for the worker pool.\n    userData  []byte    UserData is a base64-encoded string that contains the data that is sent to the provider\u0026rsquo;s APIs when a new machine/VM that is part of this worker pool shall be spawned.\n    volume  Volume     (Optional) Volume contains information about the root disks that should be used for this worker pool.\n    dataVolumes  []DataVolume     (Optional) DataVolumes contains a list of additional worker volumes.\n    kubeletDataVolumeName  string    (Optional) KubeletDataVolumeName contains the name of a dataVolume that should be used for storing kubelet state.\n    zones  []string    (Optional) Zones contains information about availability zones for this worker pool.\n    machineControllerManager  github.com/gardener/gardener/pkg/apis/core/v1beta1.MachineControllerManagerSettings     (Optional) MachineControllerManagerSettings contains configurations for different worker-pools. Eg. MachineDrainTimeout, MachineHealthTimeout.\n    WorkerSpec   (Appears on: Worker)  WorkerSpec is the spec for a Worker resource.\n   Field Description      DefaultSpec  DefaultSpec      (Members of DefaultSpec are embedded into this type.) DefaultSpec is a structure containing common fields used by all extension resources.\n    infrastructureProviderStatus  k8s.io/apimachinery/pkg/runtime.RawExtension     (Optional) InfrastructureProviderStatus is a raw extension field that contains the provider status that has been generated by the controller responsible for the Infrastructure resource.\n    region  string    Region is the name of the region where the worker pool should be deployed to.\n    secretRef  Kubernetes core/v1.SecretReference     SecretRef is a reference to a secret that contains the cloud provider specific credentials.\n    sshPublicKey  []byte    (Optional) SSHPublicKey is the public SSH key that should be used with these workers.\n    pools  []WorkerPool     Pools is a list of worker pools.\n    WorkerStatus   (Appears on: Worker)  WorkerStatus is the status for a Worker resource.\n   Field Description      DefaultStatus  DefaultStatus      (Members of DefaultStatus are embedded into this type.) DefaultStatus is a structure containing common fields used by all extension resources.\n    machineDeployments  []MachineDeployment     MachineDeployments is a list of created machine deployments. It will be used to e.g. configure the cluster-autoscaler properly.\n      Generated with gen-crd-api-reference-docs \n"},{"uri":"https://gardener.cloud/documentation/tutorials/kube-featureflag/","title":"Feature Flags in Kubernetes Applications","tags":[],"description":"Implementing feature flags in Kubernetes application with labels and annotations","content":"Feature Flags in Kubernetes Applications Feature flags are used to change the behavior of a program at runtime without forcing a restart.\nAlthough they are essential in a native cloud environment, they cannot be implemented without significant effort on some platforms. Kubernetes has made this trivial. Here we will implement them through labels and annotations, but you can also implement them by connecting directly to the Kubernetes API Server.\nIn Kubernetes, labels are part of the identity of a resource and can be used through selectors. Annotations are similar, but do not participate in the identity of a resource and cannot be used to select resources. Nevertheless, they can still be used as feature flags to enable/disable application logic.\nPossible Use Cases  turn on/off a specific instance turn on/off profiling of a specific instance change the logging level, to capture detailed logs during a specific event change caching strategy at runtime change timeouts in production toggle on/off some special verification  How does this work Well use the Kubernetes downwardAPI ) to expose labels and annotations directly to our application. Well end up with two files (labels and annotations) in /etc/podinfo. First we add the downward api to spec.volumes. Note that it is possible to adding both labels and annotations into the same volume.\nHow to update/toggle the feature After the deployment of the demo application is done you can easily switch a feature in the application on or off. This is done very easily with kubectl by changing an annotation in the Pods.\nDeploy demo app/pod kubectl apply -f ./yaml/deployment.yaml Show the log kubectl logs featureflag-example -f Use business feature 2 kubectl annotate --overwrite pod featureflag-example businessFeature=implementation2 Use business feature 1 kubectl annotate --overwrite pod featureflag-example businessFeature=implementation1 Conclusion As you can see in the log of the Pod the application switches very fast between the implementations. Everything was controlled by annotations on the deployment or Pod. On the whole a very simple and maintainable solution to configure parts of the application without restarting the whole application.\nWrangling labels and annotations from the shell. # Add a label $ kubectl label pod my-pod-name a-label=foo # Show labels $ kubectl get pods --show-labels # If you only want to show specific labels, use -L=\u0026lt;label1\u0026gt;,\u0026lt;label2\u0026gt; # Update a label $ kubectl label pod my-pod-name a-label=bar --override # Delete a label .Remember the \u0026#34;-\u0026#34; at the end of the line. Required to remove a label $ kubectl label pod my-pod-name a-label- # Add an annotation $ kubectl annotatate pod my-pod-name an-annotation=foo # Show annotations $ kubectl describe pod my-pod-name # Update an annotation $ kubectl annotation pod my-pod-name an-annotation=foo --override # Delete an annotation. Remember the \u0026#34;-\u0026#34; at the end of the line. Required to remove a annotation $ kubectl annotation pod my-pod-name an-annotation- "},{"uri":"https://gardener.cloud/v1.12.8/tutorials/kube-featureflag/","title":"Feature Flags in Kubernetes Applications","tags":[],"description":"Implementing feature flags in Kubernetes application with labels and annotations","content":"Feature Flags in Kubernetes Applications Feature flags are used to change the behavior of a program at runtime without forcing a restart.\nAlthough they are essential in a native cloud environment, they cannot be implemented without significant effort on some platforms. Kubernetes has made this trivial. Here we will implement them through labels and annotations, but you can also implement them by connecting directly to the Kubernetes API Server.\nIn Kubernetes, labels are part of the identity of a resource and can be used through selectors. Annotations are similar, but do not participate in the identity of a resource and cannot be used to select resources. Nevertheless, they can still be used as feature flags to enable/disable application logic.\nPossible Use Cases  turn on/off a specific instance turn on/off profiling of a specific instance change the logging level, to capture detailed logs during a specific event change caching strategy at runtime change timeouts in production toggle on/off some special verification  How does this work Well use the Kubernetes downwardAPI ) to expose labels and annotations directly to our application. Well end up with two files (labels and annotations) in /etc/podinfo. First we add the downward api to spec.volumes. Note that it is possible to adding both labels and annotations into the same volume.\nHow to update/toggle the feature After the deployment of the demo application is done you can easily switch a feature in the application on or off. This is done very easily with kubectl by changing an annotation in the Pods.\nDeploy demo app/pod kubectl apply -f ./yaml/deployment.yaml Show the log kubectl logs featureflag-example -f Use business feature 2 kubectl annotate --overwrite pod featureflag-example businessFeature=implementation2 Use business feature 1 kubectl annotate --overwrite pod featureflag-example businessFeature=implementation1 Conclusion As you can see in the log of the Pod the application switches very fast between the implementations. Everything was controlled by annotations on the deployment or Pod. On the whole a very simple and maintainable solution to configure parts of the application without restarting the whole application.\nWrangling labels and annotations from the shell. # Add a label $ kubectl label pod my-pod-name a-label=foo # Show labels $ kubectl get pods --show-labels # If you only want to show specific labels, use -L=\u0026lt;label1\u0026gt;,\u0026lt;label2\u0026gt; # Update a label $ kubectl label pod my-pod-name a-label=bar --override # Delete a label .Remember the \u0026#34;-\u0026#34; at the end of the line. Required to remove a label $ kubectl label pod my-pod-name a-label- # Add an annotation $ kubectl annotatate pod my-pod-name an-annotation=foo # Show annotations $ kubectl describe pod my-pod-name # Update an annotation $ kubectl annotation pod my-pod-name an-annotation=foo --override # Delete an annotation. Remember the \u0026#34;-\u0026#34; at the end of the line. Required to remove a annotation $ kubectl annotation pod my-pod-name an-annotation- "},{"uri":"https://gardener.cloud/v1.13.2/tutorials/kube-featureflag/","title":"Feature Flags in Kubernetes Applications","tags":[],"description":"Implementing feature flags in Kubernetes application with labels and annotations","content":"Feature Flags in Kubernetes Applications Feature flags are used to change the behavior of a program at runtime without forcing a restart.\nAlthough they are essential in a native cloud environment, they cannot be implemented without significant effort on some platforms. Kubernetes has made this trivial. Here we will implement them through labels and annotations, but you can also implement them by connecting directly to the Kubernetes API Server.\nIn Kubernetes, labels are part of the identity of a resource and can be used through selectors. Annotations are similar, but do not participate in the identity of a resource and cannot be used to select resources. Nevertheless, they can still be used as feature flags to enable/disable application logic.\nPossible Use Cases  turn on/off a specific instance turn on/off profiling of a specific instance change the logging level, to capture detailed logs during a specific event change caching strategy at runtime change timeouts in production toggle on/off some special verification  How does this work Well use the Kubernetes downwardAPI ) to expose labels and annotations directly to our application. Well end up with two files (labels and annotations) in /etc/podinfo. First we add the downward api to spec.volumes. Note that it is possible to adding both labels and annotations into the same volume.\nHow to update/toggle the feature After the deployment of the demo application is done you can easily switch a feature in the application on or off. This is done very easily with kubectl by changing an annotation in the Pods.\nDeploy demo app/pod kubectl apply -f ./yaml/deployment.yaml Show the log kubectl logs featureflag-example -f Use business feature 2 kubectl annotate --overwrite pod featureflag-example businessFeature=implementation2 Use business feature 1 kubectl annotate --overwrite pod featureflag-example businessFeature=implementation1 Conclusion As you can see in the log of the Pod the application switches very fast between the implementations. Everything was controlled by annotations on the deployment or Pod. On the whole a very simple and maintainable solution to configure parts of the application without restarting the whole application.\nWrangling labels and annotations from the shell. # Add a label $ kubectl label pod my-pod-name a-label=foo # Show labels $ kubectl get pods --show-labels # If you only want to show specific labels, use -L=\u0026lt;label1\u0026gt;,\u0026lt;label2\u0026gt; # Update a label $ kubectl label pod my-pod-name a-label=bar --override # Delete a label .Remember the \u0026#34;-\u0026#34; at the end of the line. Required to remove a label $ kubectl label pod my-pod-name a-label- # Add an annotation $ kubectl annotatate pod my-pod-name an-annotation=foo # Show annotations $ kubectl describe pod my-pod-name # Update an annotation $ kubectl annotation pod my-pod-name an-annotation=foo --override # Delete an annotation. Remember the \u0026#34;-\u0026#34; at the end of the line. Required to remove a annotation $ kubectl annotation pod my-pod-name an-annotation- "},{"uri":"https://gardener.cloud/documentation/concepts/deployment/feature_gates/","title":"Feature Gates","tags":[],"description":"","content":"Feature Gates in Gardener This page contains an overview of the various feature gates an administrator can specify on different Gardener components.\nOverview Feature gates are a set of key=value pairs that describe Gardener features. You can turn these features on or off using the a component configuration file for a specific component.\nEach Gardener component lets you enable or disable a set of feature gates that are relevant to that component. For example this is the configuration of the gardenlet component.\nThe following tables are a summary of the feature gates that you can set on different Gardener components.\n The Since column contains the Gardener release when a feature is introduced or its release stage is changed. The Until column, if not empty, contains the last Gardener release in which you can still use a feature gate. If a feature is in the Alpha or Beta state, you can find the feature listed in the Alpha/Beta feature gate table. If a feature is stable you can find all stages for that feature listed in the Graduated/Deprecated feature gate table. The Graduated/Deprecated feature gate table also lists deprecated and withdrawn features.  Feature gates for Alpha or Beta features    Feature Default Stage Since Until     Logging false Alpha 0.13    HVPA false Alpha 0.31    HVPAForShootedSeed false Alpha 0.32    ManagedIstio false Alpha 1.5    APIServerSNI false Alpha 1.7    MountHostCADirectories false Alpha 1.11.0    SeedChange false Alpha 1.12.0    SeedKubeScheduler false Alpha 1.15.0     Using a feature A feature can be in Alpha, Beta or GA stage. An Alpha feature means:\n Disabled by default. Might be buggy. Enabling the feature may expose bugs. Support for feature may be dropped at any time without notice. The API may change in incompatible ways in a later software release without notice. Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support.  A Beta feature means:\n Enabled by default. The feature is well tested. Enabling the feature is considered safe. Support for the overall feature will not be dropped, though details may change. The schema and/or semantics of objects may change in incompatible ways in a subsequent beta or stable release. When this happens, we will provide instructions for migrating to the next version. This may require deleting, editing, and re-creating API objects. The editing process may require some thought. This may require downtime for applications that rely on the feature. Recommended for only non-critical uses because of potential for incompatible changes in subsequent releases.   Please do try Beta features and give feedback on them! After they exit beta, it may not be practical for us to make more changes.\n A General Availability (GA) feature is also referred to as a stable feature. It means:\n The feature is always enabled; you cannot disable it. The corresponding feature gate is no longer needed. Stable versions of features will appear in released software for many subsequent versions.  List of feature gates  Logging enables logging stack for Seed clusters. HVPA enables simultaneous horizontal and vertical scaling in Seed Clusters. HVPAForShootedSeed enables simultaneous horizontal and vertical scaling in shooted Seed clusters. ManagedIstio enables a Gardener-tailored Istio in each Seed cluster. Disable this feature if Istio is already installed in the cluster. Istio is not automatically removed if this feature is disabled. See the detailed documentation for more information. APIServerSNI enables only one LoadBalancer to be used for every Shoot cluster API server in a Seed. Enable this feature when ManagedIstio is enabled or Istio is manually deployed in Seed cluster. See GEP-8 for more details. MountHostCADirectories enables mounting common CA certificate directories in the Shoot API server pod that might be required for webhooks or OIDC. SeedChange enables updating the spec.seedName field during shoot validation from a non-empty value in order to trigger shoot control plane migration. SeedKubeScheduler adds custom kube-scheduler in gardener-kube-scheduler namespace. It schedules pods with scheduler name gardener-kube-scheduler on Nodes with higher resource utilization. It requires Seed cluster with kubernetes version 1.18 or higher.  "},{"uri":"https://gardener.cloud/v1.12.8/concepts/deployment/feature_gates/","title":"Feature Gates","tags":[],"description":"","content":"Feature Gates in Gardener This page contains an overview of the various feature gates an administrator can specify on different Gardener components.\nOverview Feature gates are a set of key=value pairs that describe Gardener features. You can turn these features on or off using the a component configuration file for a specific component.\nEach Gardener component lets you enable or disable a set of feature gates that are relevant to that component. For example this is the configuration of the gardenlet component.\nThe following tables are a summary of the feature gates that you can set on different Gardener components.\n The Since column contains the Gardener release when a feature is introduced or its release stage is changed. The Until column, if not empty, contains the last Gardener release in which you can still use a feature gate. If a feature is in the Alpha or Beta state, you can find the feature listed in the Alpha/Beta feature gate table. If a feature is stable you can find all stages for that feature listed in the Graduated/Deprecated feature gate table. The Graduated/Deprecated feature gate table also lists deprecated and withdrawn features.  Feature gates for Alpha or Beta features    Feature Default Stage Since Until     Logging false Alpha 0.13    HVPA false Alpha 0.31    HVPAForShootedSeed false Alpha 0.32    ManagedIstio false Alpha 1.5    APIServerSNI false Alpha 1.7    MountHostCADirectories false Alpha 1.11.0    SeedChange false Alpha 1.12.0    SeedKubeScheduler false Alpha 1.15.0     Using a feature A feature can be in Alpha, Beta or GA stage. An Alpha feature means:\n Disabled by default. Might be buggy. Enabling the feature may expose bugs. Support for feature may be dropped at any time without notice. The API may change in incompatible ways in a later software release without notice. Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support.  A Beta feature means:\n Enabled by default. The feature is well tested. Enabling the feature is considered safe. Support for the overall feature will not be dropped, though details may change. The schema and/or semantics of objects may change in incompatible ways in a subsequent beta or stable release. When this happens, we will provide instructions for migrating to the next version. This may require deleting, editing, and re-creating API objects. The editing process may require some thought. This may require downtime for applications that rely on the feature. Recommended for only non-critical uses because of potential for incompatible changes in subsequent releases.   Please do try Beta features and give feedback on them! After they exit beta, it may not be practical for us to make more changes.\n A General Availability (GA) feature is also referred to as a stable feature. It means:\n The feature is always enabled; you cannot disable it. The corresponding feature gate is no longer needed. Stable versions of features will appear in released software for many subsequent versions.  List of feature gates  Logging enables logging stack for Seed clusters. HVPA enables simultaneous horizontal and vertical scaling in Seed Clusters. HVPAForShootedSeed enables simultaneous horizontal and vertical scaling in shooted Seed clusters. ManagedIstio enables a Gardener-tailored Istio in each Seed cluster. Disable this feature if Istio is already installed in the cluster. Istio is not automatically removed if this feature is disabled. See the detailed documentation for more information. APIServerSNI enables only one LoadBalancer to be used for every Shoot cluster API server in a Seed. Enable this feature when ManagedIstio is enabled or Istio is manually deployed in Seed cluster. See GEP-8 for more details. MountHostCADirectories enables mounting common CA certificate directories in the Shoot API server pod that might be required for webhooks or OIDC. SeedChange enables updating the spec.seedName field during shoot validation from a non-empty value in order to trigger shoot control plane migration. SeedKubeScheduler adds custom kube-scheduler in gardener-kube-scheduler namespace. It schedules pods with scheduler name gardener-kube-scheduler on Nodes with higher resource utilization. It requires Seed cluster with kubernetes version 1.18 or higher.  "},{"uri":"https://gardener.cloud/v1.13.2/concepts/deployment/feature_gates/","title":"Feature Gates","tags":[],"description":"","content":"Feature Gates in Gardener This page contains an overview of the various feature gates an administrator can specify on different Gardener components.\nOverview Feature gates are a set of key=value pairs that describe Gardener features. You can turn these features on or off using the a component configuration file for a specific component.\nEach Gardener component lets you enable or disable a set of feature gates that are relevant to that component. For example this is the configuration of the gardenlet component.\nThe following tables are a summary of the feature gates that you can set on different Gardener components.\n The Since column contains the Gardener release when a feature is introduced or its release stage is changed. The Until column, if not empty, contains the last Gardener release in which you can still use a feature gate. If a feature is in the Alpha or Beta state, you can find the feature listed in the Alpha/Beta feature gate table. If a feature is stable you can find all stages for that feature listed in the Graduated/Deprecated feature gate table. The Graduated/Deprecated feature gate table also lists deprecated and withdrawn features.  Feature gates for Alpha or Beta features    Feature Default Stage Since Until     Logging false Alpha 0.13    HVPA false Alpha 0.31    HVPAForShootedSeed false Alpha 0.32    ManagedIstio false Alpha 1.5    APIServerSNI false Alpha 1.7    MountHostCADirectories false Alpha 1.11.0    SeedChange false Alpha 1.12.0    SeedKubeScheduler false Alpha 1.15.0     Using a feature A feature can be in Alpha, Beta or GA stage. An Alpha feature means:\n Disabled by default. Might be buggy. Enabling the feature may expose bugs. Support for feature may be dropped at any time without notice. The API may change in incompatible ways in a later software release without notice. Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support.  A Beta feature means:\n Enabled by default. The feature is well tested. Enabling the feature is considered safe. Support for the overall feature will not be dropped, though details may change. The schema and/or semantics of objects may change in incompatible ways in a subsequent beta or stable release. When this happens, we will provide instructions for migrating to the next version. This may require deleting, editing, and re-creating API objects. The editing process may require some thought. This may require downtime for applications that rely on the feature. Recommended for only non-critical uses because of potential for incompatible changes in subsequent releases.   Please do try Beta features and give feedback on them! After they exit beta, it may not be practical for us to make more changes.\n A General Availability (GA) feature is also referred to as a stable feature. It means:\n The feature is always enabled; you cannot disable it. The corresponding feature gate is no longer needed. Stable versions of features will appear in released software for many subsequent versions.  List of feature gates  Logging enables logging stack for Seed clusters. HVPA enables simultaneous horizontal and vertical scaling in Seed Clusters. HVPAForShootedSeed enables simultaneous horizontal and vertical scaling in shooted Seed clusters. ManagedIstio enables a Gardener-tailored Istio in each Seed cluster. Disable this feature if Istio is already installed in the cluster. Istio is not automatically removed if this feature is disabled. See the detailed documentation for more information. APIServerSNI enables only one LoadBalancer to be used for every Shoot cluster API server in a Seed. Enable this feature when ManagedIstio is enabled or Istio is manually deployed in Seed cluster. See GEP-8 for more details. MountHostCADirectories enables mounting common CA certificate directories in the Shoot API server pod that might be required for webhooks or OIDC. SeedChange enables updating the spec.seedName field during shoot validation from a non-empty value in order to trigger shoot control plane migration. SeedKubeScheduler adds custom kube-scheduler in gardener-kube-scheduler namespace. It schedules pods with scheduler name gardener-kube-scheduler on Nodes with higher resource utilization. It requires Seed cluster with kubernetes version 1.18 or higher.  "},{"uri":"https://gardener.cloud/components/gardenctl/","title":"gardenctl","tags":[],"description":"","content":"Gardenctl  \nWhat is gardenctl? gardenctl is a command-line client for administrative purposes for the Gardener. It facilitates the administration of one or many garden, seed and shoot clusters, e.g. to check for issues which occured in one of these clusters. Details about the concept behind the Gardener are described in the Gardener wiki.\nInstallation gardenctl is shipped for mac and linux in a binary format.\nOption 1: Install the latest release with Homebrew (macOS and Linux) as follows:\nbrew install gardener/tap/gardenctl Option 2: Manually download and install from gardenctl releases as follows:\n Download the latest release:  curl -LO https://github.com/gardener/gardenctl/releases/download/$(curl -s https://raw.githubusercontent.com/gardener/gardenctl/master/LATEST)/gardenctl-darwin-amd64 To download a specific version, replace the $(curl -s https://raw.githubusercontent.com/gardener/gardenctl/master/LATEST) portion of the command with the specific version.\nFor example, to download version 0.16.0 on macOS, type:\ncurl -LO https://github.com/gardener/gardenctl/releases/download/v0.16.0/gardenctl-darwin-amd64 Make the gardenctl binary executable.  chmod +x ./gardenctl-darwin-amd64 Move the binary in to your PATH.  sudo mv ./gardenctl-darwin-amd64 /usr/local/bin/gardenctl How to build it If no binary builds are available for your platform or architecture, you can build it from source, go get it or build the docker image from Dockerfile. Please keep in mind to use an up to date version of golang.\nPrerequisites To build gardenctl from sources you need to have a running Golang environment. Moreover, since gardenctl allows to execute kubectl as well as a running kubectl installation is recommended, but not required. Please check this description for further details.\nBuild gardenctl From source First, you need to clone the repository and build gardenctl.\ngit clone https://github.com/gardener/gardenctl.git cd gardenctl make build After successfully building gardenctl the executables are in the directory ~/go/src/github.com/gardener/gardenctl/bin/. Next, move the executable for your architecture to /usr/local/bin. In this case for darwin-amd64.\nsudo mv bin/darwin-amd64/gardenctl-darwin-amd64 /usr/local/bin/gardenctl gardenctl supports auto completion. This recommended feature is bound to gardenctl or the alias g. To configure it you can run:\nif you are using bash:\necho \u0026#34;source \u0026lt;(gardenctl completion bash)\u0026#34; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc if you are using zsh:\necho \u0026#34;source \u0026lt;(gardenctl completion zsh)\u0026#34; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc Via Dockerfile First clone the repository as described in the the build step \u0026ldquo;From source\u0026rdquo;. As next step add the garden \u0026ldquo;config\u0026rdquo; file and \u0026ldquo;clusters\u0026rdquo; folder with the corresponding kubeconfig files for the garden cluster. Then build the container image via docker build -t gardener/gardenctl:v1 . in the cloned repository and run a shell in the image with docker run -it gardener/gardenctl:v1 /bin/bash.\nConfigure gardenctl gardenctl requires a configuration file. The default location is in ~/.garden/config, but it can be overwritten with the environment variable GARDENCONFIG.\nHere an example file:\nemail:john.doe@example.comgithubURL:https://github.location.company.corpgardenClusters:- name:devkubeConfig:~/clusters/dev/kubeconfig.yamldashboardUrl:https://url_to_dashboardaccessRestrictions:- key:seed.gardener.cloud/eu-accessnotifyIf:truemsg:warningmsgoptions:- key:support.gardener.cloud/eu-access-for-cluster-addonsnotifyIf:truemsg:warningmsg- key:support.gardener.cloud/eu-access-for-cluster-nodesnotifyIf:truemsg:warningmsg- name:prodkubeConfig:~/clusters/prod/kubeconfig.yamlThe path to the kubeconfig files of a garden cluster can be relative by using the ~ (tilde) expansion or absolute.\ngardenctl caches some information, e.g. the garden project names. The location of this cache is per default $GARDENCTL_HOME/cache. If GARDENCTL_HOME is not set, ~/.garden is assumed.\ngardenctl supports multiple sessions. The session ID can be set via $GARDEN_SESSION_ID and the sessions are stored under $GARDENCTL_HOME/sessions.\ngardenctl makes it easy to get additional information of your IaaS provider by using the secrets stored in the corresponding projects in the Gardener. To use this functionality, the CLIs of the IaaS providers need to be available.\nPlease check the IaaS provider documentation for more details about their CLIs.\n aliyun aws az gcloud openstack  Moreover, gardenctl offers auto completion. To use it, the command\ngardenctl completion bash print on the standard output a completion script which can be sourced via\nsource \u0026lt;(gardenctl completion bash) Please keep in mind that the auto completion is bound to gardenctl or the alias g.\nUse gardenctl gardenctl requires the definition of a target, e.g. garden, project, seed or shoot. The following commands, e.g. gardenctl ls shoots uses the target definition as a context for getting the information.\nTargets represent a hierarchical structure of resources. On top, there is/are the garden/s. E.g. in case you setup a development and a production garden, you would have two entries in your ~/.garden/config. Via gardenctl ls gardens you get a list of the available gardens.\n gardenctl get target\nDisplays the current target gardenctl target [garden|project|seed|shoot]\nSet the target e.g. to a garden. It is as well possible to set the target directly to a element deeper in the hierarchy, e.g. to a shoot. gardenctl drop target\nDrop the deepest target.  Examples of basic usage:  List all seed cluster\ngardenctl ls seeds List all projects with shoot cluster\ngardenctl ls projects Target a seed cluster\ngardenctl target seed-gce-dev Target a project\ngardenctl target garden-vora Open prometheus ui for a targeted shoot-cluster\ngardenctl show prometheus Execute an aws command on a targeted aws shoot cluster\ngardenctl aws ec2 describe-instances or\ngardenctl aws ec2 describe-instances --no-cache without locally caching credentials Target a shoot directly and get all kube-dns pods in kube-system namespace\ngardenctl target myshoot\ngardenctl kubectl get pods -- -n kube-system -l k8s-app=kube-dns List all cluster with an issue\ngardenctl ls issues Drop an element from target stack\ngardenctl drop Open a shell to a cluster node\ngardenctl shell nodename Show logs from elasticsearch\ngardenctl logs etcd-main --elasticsearch Show last 100 logs from elasticsearch from the last 2 hours\ngardenctl logs etcd-main --elasticsearch --since=2h --tail=100 Show logs from seed nodes\ngardenctl target -g garden-name -s seed-name\ngardenctl logs tf infra shoot-name Show logs from shoot nodes\ngardenctl target -g garden-name -t shoot-name\ngardenctl logs api | scheduler | controller-manager | etcd-main -c etcd |etcd-main -c backup-restore | vpn-seed | vpn-shoot | machine-controller-manager | prometheus |grafana | cluster-autoscaler Show logs from garden nodes\ngardenctl target -g garden-name\ngardenctl logs gardener-apiserver | gardener-controller-manager SSH to shoot nodes (please unset any proxy env vars like HTTPS and HTTP before this command)\ngardenctl k get nodes\ngardenctl ssh node_name  Advanced usage based on JsonQuery The following examples are based on jq. The Json Query Playground offers a convenient environment to test the queries.\nBelow a list of examples:\n List the project name, shoot name and the state for all projects with issues  gardenctl ls issues -o json | jq \u0026#39;.issues[] | { project: .project, shoot: .shoot, state: .status.lastOperation.state }\u0026#39;  Print all issues of a single project e.g. garden-myproject  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.project==\u0026#34;garden-myproject\u0026#34;) then . else empty end\u0026#39;  Print all issues with error state \u0026ldquo;Error\u0026rdquo;  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state==\u0026#34;Error\u0026#34;) then . else empty end\u0026#39;  Print all issues with error state not equal \u0026ldquo;Succeded\u0026rdquo;  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state!=\u0026#34;Succeeded\u0026#34;) then . else empty end\u0026#39;  Print createdBy information (typically email addresses) of all shoots  gardenctl k get shoots -- -n garden-core -o json | jq -r \u0026#34;.items[].metadata | {email: .annotations.\\\u0026#34;garden.sapcloud.io/createdBy\\\u0026#34;, name: .name, namespace: .namespace}\u0026#34; Here a few on cluster analysis:\n Which states are there and how many clusters are in this state?  gardenctl ls issues -o json | jq \u0026#39;.issues | group_by( .status.lastOperation.state ) | .[] | {state:.[0].status.lastOperation.state, count:length}\u0026#39;  Get all clusters in state Failed  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state==\u0026#34;Failed\u0026#34;) then . else empty end\u0026#39; "},{"uri":"https://gardener.cloud/components/gardenctl/","title":"gardenctl","tags":[],"description":"","content":"Gardenctl  \nWhat is gardenctl? gardenctl is a command-line client for administrative purposes for the Gardener. It facilitates the administration of one or many garden, seed and shoot clusters, e.g. to check for issues which occured in one of these clusters. Details about the concept behind the Gardener are described in the Gardener wiki.\nInstallation gardenctl is shipped for mac and linux in a binary format.\nOption 1: Install the latest release with Homebrew (macOS and Linux) as follows:\nbrew install gardener/tap/gardenctl Option 2: Manually download and install from gardenctl releases as follows:\n Download the latest release:  curl -LO https://github.com/gardener/gardenctl/releases/download/$(curl -s https://raw.githubusercontent.com/gardener/gardenctl/master/LATEST)/gardenctl-darwin-amd64 To download a specific version, replace the $(curl -s https://raw.githubusercontent.com/gardener/gardenctl/master/LATEST) portion of the command with the specific version.\nFor example, to download version 0.16.0 on macOS, type:\ncurl -LO https://github.com/gardener/gardenctl/releases/download/v0.16.0/gardenctl-darwin-amd64 Make the gardenctl binary executable.  chmod +x ./gardenctl-darwin-amd64 Move the binary in to your PATH.  sudo mv ./gardenctl-darwin-amd64 /usr/local/bin/gardenctl How to build it If no binary builds are available for your platform or architecture, you can build it from source, go get it or build the docker image from Dockerfile. Please keep in mind to use an up to date version of golang.\nPrerequisites To build gardenctl from sources you need to have a running Golang environment. Moreover, since gardenctl allows to execute kubectl as well as a running kubectl installation is recommended, but not required. Please check this description for further details.\nBuild gardenctl From source First, you need to clone the repository and build gardenctl.\ngit clone https://github.com/gardener/gardenctl.git cd gardenctl make build After successfully building gardenctl the executables are in the directory ~/go/src/github.com/gardener/gardenctl/bin/. Next, move the executable for your architecture to /usr/local/bin. In this case for darwin-amd64.\nsudo mv bin/darwin-amd64/gardenctl-darwin-amd64 /usr/local/bin/gardenctl gardenctl supports auto completion. This recommended feature is bound to gardenctl or the alias g. To configure it you can run:\nif you are using bash:\necho \u0026#34;source \u0026lt;(gardenctl completion bash)\u0026#34; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc if you are using zsh:\necho \u0026#34;source \u0026lt;(gardenctl completion zsh)\u0026#34; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc Via Dockerfile First clone the repository as described in the the build step \u0026ldquo;From source\u0026rdquo;. As next step add the garden \u0026ldquo;config\u0026rdquo; file and \u0026ldquo;clusters\u0026rdquo; folder with the corresponding kubeconfig files for the garden cluster. Then build the container image via docker build -t gardener/gardenctl:v1 . in the cloned repository and run a shell in the image with docker run -it gardener/gardenctl:v1 /bin/bash.\nConfigure gardenctl gardenctl requires a configuration file. The default location is in ~/.garden/config, but it can be overwritten with the environment variable GARDENCONFIG.\nHere an example file:\nemail:john.doe@example.comgithubURL:https://github.location.company.corpgardenClusters:- name:devkubeConfig:~/clusters/dev/kubeconfig.yamldashboardUrl:https://url_to_dashboardaccessRestrictions:- key:seed.gardener.cloud/eu-accessnotifyIf:truemsg:warningmsgoptions:- key:support.gardener.cloud/eu-access-for-cluster-addonsnotifyIf:truemsg:warningmsg- key:support.gardener.cloud/eu-access-for-cluster-nodesnotifyIf:truemsg:warningmsg- name:prodkubeConfig:~/clusters/prod/kubeconfig.yamlThe path to the kubeconfig files of a garden cluster can be relative by using the ~ (tilde) expansion or absolute.\ngardenctl caches some information, e.g. the garden project names. The location of this cache is per default $GARDENCTL_HOME/cache. If GARDENCTL_HOME is not set, ~/.garden is assumed.\ngardenctl supports multiple sessions. The session ID can be set via $GARDEN_SESSION_ID and the sessions are stored under $GARDENCTL_HOME/sessions.\ngardenctl makes it easy to get additional information of your IaaS provider by using the secrets stored in the corresponding projects in the Gardener. To use this functionality, the CLIs of the IaaS providers need to be available.\nPlease check the IaaS provider documentation for more details about their CLIs.\n aliyun aws az gcloud openstack  Moreover, gardenctl offers auto completion. To use it, the command\ngardenctl completion bash print on the standard output a completion script which can be sourced via\nsource \u0026lt;(gardenctl completion bash) Please keep in mind that the auto completion is bound to gardenctl or the alias g.\nUse gardenctl gardenctl requires the definition of a target, e.g. garden, project, seed or shoot. The following commands, e.g. gardenctl ls shoots uses the target definition as a context for getting the information.\nTargets represent a hierarchical structure of resources. On top, there is/are the garden/s. E.g. in case you setup a development and a production garden, you would have two entries in your ~/.garden/config. Via gardenctl ls gardens you get a list of the available gardens.\n gardenctl get target\nDisplays the current target gardenctl target [garden|project|seed|shoot]\nSet the target e.g. to a garden. It is as well possible to set the target directly to a element deeper in the hierarchy, e.g. to a shoot. gardenctl drop target\nDrop the deepest target.  Examples of basic usage:  List all seed cluster\ngardenctl ls seeds List all projects with shoot cluster\ngardenctl ls projects Target a seed cluster\ngardenctl target seed-gce-dev Target a project\ngardenctl target garden-vora Open prometheus ui for a targeted shoot-cluster\ngardenctl show prometheus Execute an aws command on a targeted aws shoot cluster\ngardenctl aws ec2 describe-instances or\ngardenctl aws ec2 describe-instances --no-cache without locally caching credentials Target a shoot directly and get all kube-dns pods in kube-system namespace\ngardenctl target myshoot\ngardenctl kubectl get pods -- -n kube-system -l k8s-app=kube-dns List all cluster with an issue\ngardenctl ls issues Drop an element from target stack\ngardenctl drop Open a shell to a cluster node\ngardenctl shell nodename Show logs from elasticsearch\ngardenctl logs etcd-main --elasticsearch Show last 100 logs from elasticsearch from the last 2 hours\ngardenctl logs etcd-main --elasticsearch --since=2h --tail=100 Show logs from seed nodes\ngardenctl target -g garden-name -s seed-name\ngardenctl logs tf infra shoot-name Show logs from shoot nodes\ngardenctl target -g garden-name -t shoot-name\ngardenctl logs api | scheduler | controller-manager | etcd-main -c etcd |etcd-main -c backup-restore | vpn-seed | vpn-shoot | machine-controller-manager | prometheus |grafana | cluster-autoscaler Show logs from garden nodes\ngardenctl target -g garden-name\ngardenctl logs gardener-apiserver | gardener-controller-manager SSH to shoot nodes (please unset any proxy env vars like HTTPS and HTTP before this command)\ngardenctl k get nodes\ngardenctl ssh node_name  Advanced usage based on JsonQuery The following examples are based on jq. The Json Query Playground offers a convenient environment to test the queries.\nBelow a list of examples:\n List the project name, shoot name and the state for all projects with issues  gardenctl ls issues -o json | jq \u0026#39;.issues[] | { project: .project, shoot: .shoot, state: .status.lastOperation.state }\u0026#39;  Print all issues of a single project e.g. garden-myproject  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.project==\u0026#34;garden-myproject\u0026#34;) then . else empty end\u0026#39;  Print all issues with error state \u0026ldquo;Error\u0026rdquo;  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state==\u0026#34;Error\u0026#34;) then . else empty end\u0026#39;  Print all issues with error state not equal \u0026ldquo;Succeded\u0026rdquo;  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state!=\u0026#34;Succeeded\u0026#34;) then . else empty end\u0026#39;  Print createdBy information (typically email addresses) of all shoots  gardenctl k get shoots -- -n garden-core -o json | jq -r \u0026#34;.items[].metadata | {email: .annotations.\\\u0026#34;garden.sapcloud.io/createdBy\\\u0026#34;, name: .name, namespace: .namespace}\u0026#34; Here a few on cluster analysis:\n Which states are there and how many clusters are in this state?  gardenctl ls issues -o json | jq \u0026#39;.issues | group_by( .status.lastOperation.state ) | .[] | {state:.[0].status.lastOperation.state, count:length}\u0026#39;  Get all clusters in state Failed  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state==\u0026#34;Failed\u0026#34;) then . else empty end\u0026#39; "},{"uri":"https://gardener.cloud/components/gardenctl/","title":"gardenctl","tags":[],"description":"","content":"Gardenctl  \nWhat is gardenctl? gardenctl is a command-line client for administrative purposes for the Gardener. It facilitates the administration of one or many garden, seed and shoot clusters, e.g. to check for issues which occured in one of these clusters. Details about the concept behind the Gardener are described in the Gardener wiki.\nInstallation gardenctl is shipped for mac and linux in a binary format.\nOption 1: Install the latest release with Homebrew (macOS and Linux) as follows:\nbrew install gardener/tap/gardenctl Option 2: Manually download and install from gardenctl releases as follows:\n Download the latest release:  curl -LO https://github.com/gardener/gardenctl/releases/download/$(curl -s https://raw.githubusercontent.com/gardener/gardenctl/master/LATEST)/gardenctl-darwin-amd64 To download a specific version, replace the $(curl -s https://raw.githubusercontent.com/gardener/gardenctl/master/LATEST) portion of the command with the specific version.\nFor example, to download version 0.16.0 on macOS, type:\ncurl -LO https://github.com/gardener/gardenctl/releases/download/v0.16.0/gardenctl-darwin-amd64 Make the gardenctl binary executable.  chmod +x ./gardenctl-darwin-amd64 Move the binary in to your PATH.  sudo mv ./gardenctl-darwin-amd64 /usr/local/bin/gardenctl How to build it If no binary builds are available for your platform or architecture, you can build it from source, go get it or build the docker image from Dockerfile. Please keep in mind to use an up to date version of golang.\nPrerequisites To build gardenctl from sources you need to have a running Golang environment. Moreover, since gardenctl allows to execute kubectl as well as a running kubectl installation is recommended, but not required. Please check this description for further details.\nBuild gardenctl From source First, you need to clone the repository and build gardenctl.\ngit clone https://github.com/gardener/gardenctl.git cd gardenctl make build After successfully building gardenctl the executables are in the directory ~/go/src/github.com/gardener/gardenctl/bin/. Next, move the executable for your architecture to /usr/local/bin. In this case for darwin-amd64.\nsudo mv bin/darwin-amd64/gardenctl-darwin-amd64 /usr/local/bin/gardenctl gardenctl supports auto completion. This recommended feature is bound to gardenctl or the alias g. To configure it you can run:\nif you are using bash:\necho \u0026#34;source \u0026lt;(gardenctl completion bash)\u0026#34; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc if you are using zsh:\necho \u0026#34;source \u0026lt;(gardenctl completion zsh)\u0026#34; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc Via Dockerfile First clone the repository as described in the the build step \u0026ldquo;From source\u0026rdquo;. As next step add the garden \u0026ldquo;config\u0026rdquo; file and \u0026ldquo;clusters\u0026rdquo; folder with the corresponding kubeconfig files for the garden cluster. Then build the container image via docker build -t gardener/gardenctl:v1 . in the cloned repository and run a shell in the image with docker run -it gardener/gardenctl:v1 /bin/bash.\nConfigure gardenctl gardenctl requires a configuration file. The default location is in ~/.garden/config, but it can be overwritten with the environment variable GARDENCONFIG.\nHere an example file:\nemail:john.doe@example.comgithubURL:https://github.location.company.corpgardenClusters:- name:devkubeConfig:~/clusters/dev/kubeconfig.yamldashboardUrl:https://url_to_dashboardaccessRestrictions:- key:seed.gardener.cloud/eu-accessnotifyIf:truemsg:warningmsgoptions:- key:support.gardener.cloud/eu-access-for-cluster-addonsnotifyIf:truemsg:warningmsg- key:support.gardener.cloud/eu-access-for-cluster-nodesnotifyIf:truemsg:warningmsg- name:prodkubeConfig:~/clusters/prod/kubeconfig.yamlThe path to the kubeconfig files of a garden cluster can be relative by using the ~ (tilde) expansion or absolute.\ngardenctl caches some information, e.g. the garden project names. The location of this cache is per default $GARDENCTL_HOME/cache. If GARDENCTL_HOME is not set, ~/.garden is assumed.\ngardenctl supports multiple sessions. The session ID can be set via $GARDEN_SESSION_ID and the sessions are stored under $GARDENCTL_HOME/sessions.\ngardenctl makes it easy to get additional information of your IaaS provider by using the secrets stored in the corresponding projects in the Gardener. To use this functionality, the CLIs of the IaaS providers need to be available.\nPlease check the IaaS provider documentation for more details about their CLIs.\n aliyun aws az gcloud openstack  Moreover, gardenctl offers auto completion. To use it, the command\ngardenctl completion bash print on the standard output a completion script which can be sourced via\nsource \u0026lt;(gardenctl completion bash) Please keep in mind that the auto completion is bound to gardenctl or the alias g.\nUse gardenctl gardenctl requires the definition of a target, e.g. garden, project, seed or shoot. The following commands, e.g. gardenctl ls shoots uses the target definition as a context for getting the information.\nTargets represent a hierarchical structure of resources. On top, there is/are the garden/s. E.g. in case you setup a development and a production garden, you would have two entries in your ~/.garden/config. Via gardenctl ls gardens you get a list of the available gardens.\n gardenctl get target\nDisplays the current target gardenctl target [garden|project|seed|shoot]\nSet the target e.g. to a garden. It is as well possible to set the target directly to a element deeper in the hierarchy, e.g. to a shoot. gardenctl drop target\nDrop the deepest target.  Examples of basic usage:  List all seed cluster\ngardenctl ls seeds List all projects with shoot cluster\ngardenctl ls projects Target a seed cluster\ngardenctl target seed-gce-dev Target a project\ngardenctl target garden-vora Open prometheus ui for a targeted shoot-cluster\ngardenctl show prometheus Execute an aws command on a targeted aws shoot cluster\ngardenctl aws ec2 describe-instances or\ngardenctl aws ec2 describe-instances --no-cache without locally caching credentials Target a shoot directly and get all kube-dns pods in kube-system namespace\ngardenctl target myshoot\ngardenctl kubectl get pods -- -n kube-system -l k8s-app=kube-dns List all cluster with an issue\ngardenctl ls issues Drop an element from target stack\ngardenctl drop Open a shell to a cluster node\ngardenctl shell nodename Show logs from elasticsearch\ngardenctl logs etcd-main --elasticsearch Show last 100 logs from elasticsearch from the last 2 hours\ngardenctl logs etcd-main --elasticsearch --since=2h --tail=100 Show logs from seed nodes\ngardenctl target -g garden-name -s seed-name\ngardenctl logs tf infra shoot-name Show logs from shoot nodes\ngardenctl target -g garden-name -t shoot-name\ngardenctl logs api | scheduler | controller-manager | etcd-main -c etcd |etcd-main -c backup-restore | vpn-seed | vpn-shoot | machine-controller-manager | prometheus |grafana | cluster-autoscaler Show logs from garden nodes\ngardenctl target -g garden-name\ngardenctl logs gardener-apiserver | gardener-controller-manager SSH to shoot nodes (please unset any proxy env vars like HTTPS and HTTP before this command)\ngardenctl k get nodes\ngardenctl ssh node_name  Advanced usage based on JsonQuery The following examples are based on jq. The Json Query Playground offers a convenient environment to test the queries.\nBelow a list of examples:\n List the project name, shoot name and the state for all projects with issues  gardenctl ls issues -o json | jq \u0026#39;.issues[] | { project: .project, shoot: .shoot, state: .status.lastOperation.state }\u0026#39;  Print all issues of a single project e.g. garden-myproject  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.project==\u0026#34;garden-myproject\u0026#34;) then . else empty end\u0026#39;  Print all issues with error state \u0026ldquo;Error\u0026rdquo;  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state==\u0026#34;Error\u0026#34;) then . else empty end\u0026#39;  Print all issues with error state not equal \u0026ldquo;Succeded\u0026rdquo;  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state!=\u0026#34;Succeeded\u0026#34;) then . else empty end\u0026#39;  Print createdBy information (typically email addresses) of all shoots  gardenctl k get shoots -- -n garden-core -o json | jq -r \u0026#34;.items[].metadata | {email: .annotations.\\\u0026#34;garden.sapcloud.io/createdBy\\\u0026#34;, name: .name, namespace: .namespace}\u0026#34; Here a few on cluster analysis:\n Which states are there and how many clusters are in this state?  gardenctl ls issues -o json | jq \u0026#39;.issues | group_by( .status.lastOperation.state ) | .[] | {state:.[0].status.lastOperation.state, count:length}\u0026#39;  Get all clusters in state Failed  gardenctl ls issues -o json | jq \u0026#39;.issues[] | if (.status.lastOperation.state==\u0026#34;Failed\u0026#34;) then . else empty end\u0026#39; "},{"uri":"https://gardener.cloud/components/gardener/","title":"Gardener","tags":[],"description":"","content":"Gardener     \nGardener implements the automated management and operation of Kubernetes clusters as a service and provides a fully validated extensibility framework that can be adjusted to any programmatic cloud or infrastructure provider.\nGardener is 100% Kubernets-native and exposes its own Cluster API to create homogeneous clusters on all supported infrastructures. This API differs from SIG Cluster Lifecycle\u0026rsquo;s Cluster API that only harmonizes how to get to clusters, while Gardener\u0026rsquo;s Cluster API goes one step further and also harmonizes the make-up of the clusters themselves. That means, Gardener gives you homogeneous clusters with exactly the same bill of material, configuration and behavior on all supported infrastructures, which you can see further down below in the section on our K8s Conformance Test Coverage.\nIn 2020, SIG Cluster Lifecycle\u0026rsquo;s Cluster API made a huge step forward with v1alpha3 and the newly added support for declarative control plane management. This made it possible to integrate managed services like GKE or Gardener. We would be more than happy, if the community would be interested, to contribute a Gardener control plane provider. For more information on the relation between Gardener API and SIG Cluster Lifecycle\u0026rsquo;s Cluster API, please see here.\nGardener\u0026rsquo;s main principle is to leverage Kubernetes concepts for all of its tasks.\nIn essence, Gardener is an extension API server that comes along with a bundle of custom controllers. It introduces new API objects in an existing Kubernetes cluster (which is called garden cluster) in order to use them for the management of end-user Kubernetes clusters (which are called shoot clusters). These shoot clusters are described via declarative cluster specifications which are observed by the controllers. They will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.\nTo accomplish these tasks reliably and to offer a high quality of service, Gardener controls the main components of a Kubernetes cluster (etcd, API server, controller manager, scheduler). These so-called control plane components are hosted in Kubernetes clusters themselves (which are called seed clusters). This is the main difference compared to many other OSS cluster provisioning tools: The shoot clusters do not have dedicated master VMs. Instead, the control plane is deployed as a native Kubernetes workload into the seeds (the architecture is commonly referred to as kubeception or inception design). This does not only effectively reduce the total cost of ownership but also allows easier implementations for \u0026ldquo;day-2 operations\u0026rdquo; (like cluster updates or robustness) by relying on all the mature Kubernetes features and capabilities.\nGardener reuses the identical Kubernetes design to span a scalable multi-cloud and multi-cluster landscape. Such familiarity with known concepts has proven to quickly ease the initial learning curve and accelerate developer productivity:\n Kubernetes API Server = Gardener API Server Kubernetes Controller Manager = Gardener Controller Manager Kubernetes Scheduler = Gardener Scheduler Kubelet = Gardenlet Node = Seed cluster Pod = Shoot cluster  Please find more information regarding the concepts and a detailed description of the architecture in our Gardener Wiki and our blog posts on kubernetes.io: Gardener - the Kubernetes Botanist (17.5.2018) and Gardener Project Update (2.12.2019).\n K8s Conformance Test Coverage Conformance test results of latest stable Gardener release, transparently visible at the CNCF test grid:\n   Provider/K8s v1.20 v1.19 v1.18 v1.17 v1.16 v1.15 v1.14 v1.13 v1.12 v1.11 v1.10     AWS N/A     [2] [1] [1] [1] [1] [1]   Azure N/A     [2] [1] [1] [1] [1] [1]   GCP N/A     [2] [1] [1] [1] [1] [1]   OpenStack N/A     [2] [1] [1] [1] [1] [1]   Alicloud N/A     [2] N/A N/A N/A N/A N/A   Packet N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A   vSphere N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A    [1] Version is technically supported but no longer actively tested. Regressions will go unnoticed.\n[2] Conformance tests are still executed and validated, unfortunately no longer shown in TestGrid.\nBesides the conformance tests, over 400 additional e2e tests are executed on a daily basis. Get an overview of the test results at testgrid.\nStart using or developing the Gardener locally See our documentation in the /docs repository, please find the index here.\nSetting up your own Gardener landscape in the Cloud The quickest way to test drive Gardener is to install it virtually onto an existing Kubernetes cluster, just like you would install any other Kubernetes-ready application. Launch your automatic installer here\nWe also have a Gardener Helm Chart. Alternatively you can use our garden setup project to create a fully configured Gardener landscape which also includes our Gardener Dashboard.\nFeedback and Support Feedback and contributions are always welcome!\nAll channels for getting in touch or learning about our project are listed under the community section. We are cordially inviting interested parties to join our weekly meetings.\nPlease report bugs or suggestions about our Kubernetes clusters as such or the Gardener itself as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn More! Please find further resources about our project here:\n Our landing page gardener.cloud \u0026ldquo;Gardener Project Update\u0026rdquo; blog on kubernetes.io. \u0026ldquo;Gardener, the Kubernetes Botanist\u0026rdquo; blog on kubernetes.io SAP news article about \u0026ldquo;Project Gardener\u0026rdquo; Introduction movie: \u0026ldquo;Gardener - Planting the Seeds of Success in the Cloud\u0026rdquo; \u0026ldquo;Thinking Cloud Native\u0026rdquo; talk at EclipseCon 2018 Blog - \u0026ldquo;Showcase of Gardener at OSCON 2018\u0026rdquo;  "},{"uri":"https://gardener.cloud/components/gardener/","title":"Gardener","tags":[],"description":"","content":"Gardener     \nGardener implements the automated management and operation of Kubernetes clusters as a service and provides a fully validated extensibility framework that can be adjusted to any programmatic cloud or infrastructure provider.\nGardener is 100% Kubernets-native and exposes its own Cluster API to create homogeneous clusters on all supported infrastructures. This API differs from SIG Cluster Lifecycle\u0026rsquo;s Cluster API that only harmonizes how to get to clusters, while Gardener\u0026rsquo;s Cluster API goes one step further and also harmonizes the make-up of the clusters themselves. That means, Gardener gives you homogeneous clusters with exactly the same bill of material, configuration and behavior on all supported infrastructures, which you can see further down below in the section on our K8s Conformance Test Coverage.\nIn 2020, SIG Cluster Lifecycle\u0026rsquo;s Cluster API made a huge step forward with v1alpha3 and the newly added support for declarative control plane management. This made it possible to integrate managed services like GKE or Gardener. We would be more than happy, if the community would be interested, to contribute a Gardener control plane provider. For more information on the relation between Gardener API and SIG Cluster Lifecycle\u0026rsquo;s Cluster API, please see here.\nGardener\u0026rsquo;s main principle is to leverage Kubernetes concepts for all of its tasks.\nIn essence, Gardener is an extension API server that comes along with a bundle of custom controllers. It introduces new API objects in an existing Kubernetes cluster (which is called garden cluster) in order to use them for the management of end-user Kubernetes clusters (which are called shoot clusters). These shoot clusters are described via declarative cluster specifications which are observed by the controllers. They will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.\nTo accomplish these tasks reliably and to offer a high quality of service, Gardener controls the main components of a Kubernetes cluster (etcd, API server, controller manager, scheduler). These so-called control plane components are hosted in Kubernetes clusters themselves (which are called seed clusters). This is the main difference compared to many other OSS cluster provisioning tools: The shoot clusters do not have dedicated master VMs. Instead, the control plane is deployed as a native Kubernetes workload into the seeds (the architecture is commonly referred to as kubeception or inception design). This does not only effectively reduce the total cost of ownership but also allows easier implementations for \u0026ldquo;day-2 operations\u0026rdquo; (like cluster updates or robustness) by relying on all the mature Kubernetes features and capabilities.\nGardener reuses the identical Kubernetes design to span a scalable multi-cloud and multi-cluster landscape. Such familiarity with known concepts has proven to quickly ease the initial learning curve and accelerate developer productivity:\n Kubernetes API Server = Gardener API Server Kubernetes Controller Manager = Gardener Controller Manager Kubernetes Scheduler = Gardener Scheduler Kubelet = Gardenlet Node = Seed cluster Pod = Shoot cluster  Please find more information regarding the concepts and a detailed description of the architecture in our Gardener Wiki and our blog posts on kubernetes.io: Gardener - the Kubernetes Botanist (17.5.2018) and Gardener Project Update (2.12.2019).\n K8s Conformance Test Coverage Conformance test results of latest stable Gardener release, transparently visible at the CNCF test grid:\n   Provider/K8s v1.20 v1.19 v1.18 v1.17 v1.16 v1.15 v1.14 v1.13 v1.12 v1.11 v1.10     AWS N/A     [2] [1] [1] [1] [1] [1]   Azure N/A     [2] [1] [1] [1] [1] [1]   GCP N/A     [2] [1] [1] [1] [1] [1]   OpenStack N/A     [2] [1] [1] [1] [1] [1]   Alicloud N/A     [2] N/A N/A N/A N/A N/A   Packet N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A   vSphere N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A    [1] Version is technically supported but no longer actively tested. Regressions will go unnoticed.\n[2] Conformance tests are still executed and validated, unfortunately no longer shown in TestGrid.\nBesides the conformance tests, over 400 additional e2e tests are executed on a daily basis. Get an overview of the test results at testgrid.\nStart using or developing the Gardener locally See our documentation in the /docs repository, please find the index here.\nSetting up your own Gardener landscape in the Cloud The quickest way to test drive Gardener is to install it virtually onto an existing Kubernetes cluster, just like you would install any other Kubernetes-ready application. Launch your automatic installer here\nWe also have a Gardener Helm Chart. Alternatively you can use our garden setup project to create a fully configured Gardener landscape which also includes our Gardener Dashboard.\nFeedback and Support Feedback and contributions are always welcome!\nAll channels for getting in touch or learning about our project are listed under the community section. We are cordially inviting interested parties to join our weekly meetings.\nPlease report bugs or suggestions about our Kubernetes clusters as such or the Gardener itself as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn More! Please find further resources about our project here:\n Our landing page gardener.cloud \u0026ldquo;Gardener Project Update\u0026rdquo; blog on kubernetes.io. \u0026ldquo;Gardener, the Kubernetes Botanist\u0026rdquo; blog on kubernetes.io SAP news article about \u0026ldquo;Project Gardener\u0026rdquo; Introduction movie: \u0026ldquo;Gardener - Planting the Seeds of Success in the Cloud\u0026rdquo; \u0026ldquo;Thinking Cloud Native\u0026rdquo; talk at EclipseCon 2018 Blog - \u0026ldquo;Showcase of Gardener at OSCON 2018\u0026rdquo;  "},{"uri":"https://gardener.cloud/components/gardener/","title":"Gardener","tags":[],"description":"","content":"Gardener     \nGardener implements the automated management and operation of Kubernetes clusters as a service and provides a fully validated extensibility framework that can be adjusted to any programmatic cloud or infrastructure provider.\nGardener is 100% Kubernets-native and exposes its own Cluster API to create homogeneous clusters on all supported infrastructures. This API differs from SIG Cluster Lifecycle\u0026rsquo;s Cluster API that only harmonizes how to get to clusters, while Gardener\u0026rsquo;s Cluster API goes one step further and also harmonizes the make-up of the clusters themselves. That means, Gardener gives you homogeneous clusters with exactly the same bill of material, configuration and behavior on all supported infrastructures, which you can see further down below in the section on our K8s Conformance Test Coverage.\nIn 2020, SIG Cluster Lifecycle\u0026rsquo;s Cluster API made a huge step forward with v1alpha3 and the newly added support for declarative control plane management. This made it possible to integrate managed services like GKE or Gardener. We would be more than happy, if the community would be interested, to contribute a Gardener control plane provider. For more information on the relation between Gardener API and SIG Cluster Lifecycle\u0026rsquo;s Cluster API, please see here.\nGardener\u0026rsquo;s main principle is to leverage Kubernetes concepts for all of its tasks.\nIn essence, Gardener is an extension API server that comes along with a bundle of custom controllers. It introduces new API objects in an existing Kubernetes cluster (which is called garden cluster) in order to use them for the management of end-user Kubernetes clusters (which are called shoot clusters). These shoot clusters are described via declarative cluster specifications which are observed by the controllers. They will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.\nTo accomplish these tasks reliably and to offer a high quality of service, Gardener controls the main components of a Kubernetes cluster (etcd, API server, controller manager, scheduler). These so-called control plane components are hosted in Kubernetes clusters themselves (which are called seed clusters). This is the main difference compared to many other OSS cluster provisioning tools: The shoot clusters do not have dedicated master VMs. Instead, the control plane is deployed as a native Kubernetes workload into the seeds (the architecture is commonly referred to as kubeception or inception design). This does not only effectively reduce the total cost of ownership but also allows easier implementations for \u0026ldquo;day-2 operations\u0026rdquo; (like cluster updates or robustness) by relying on all the mature Kubernetes features and capabilities.\nGardener reuses the identical Kubernetes design to span a scalable multi-cloud and multi-cluster landscape. Such familiarity with known concepts has proven to quickly ease the initial learning curve and accelerate developer productivity:\n Kubernetes API Server = Gardener API Server Kubernetes Controller Manager = Gardener Controller Manager Kubernetes Scheduler = Gardener Scheduler Kubelet = Gardenlet Node = Seed cluster Pod = Shoot cluster  Please find more information regarding the concepts and a detailed description of the architecture in our Gardener Wiki and our blog posts on kubernetes.io: Gardener - the Kubernetes Botanist (17.5.2018) and Gardener Project Update (2.12.2019).\n K8s Conformance Test Coverage Conformance test results of latest stable Gardener release, transparently visible at the CNCF test grid:\n   Provider/K8s v1.20 v1.19 v1.18 v1.17 v1.16 v1.15 v1.14 v1.13 v1.12 v1.11 v1.10     AWS N/A     [2] [1] [1] [1] [1] [1]   Azure N/A     [2] [1] [1] [1] [1] [1]   GCP N/A     [2] [1] [1] [1] [1] [1]   OpenStack N/A     [2] [1] [1] [1] [1] [1]   Alicloud N/A     [2] N/A N/A N/A N/A N/A   Packet N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A   vSphere N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A    [1] Version is technically supported but no longer actively tested. Regressions will go unnoticed.\n[2] Conformance tests are still executed and validated, unfortunately no longer shown in TestGrid.\nBesides the conformance tests, over 400 additional e2e tests are executed on a daily basis. Get an overview of the test results at testgrid.\nStart using or developing the Gardener locally See our documentation in the /docs repository, please find the index here.\nSetting up your own Gardener landscape in the Cloud The quickest way to test drive Gardener is to install it virtually onto an existing Kubernetes cluster, just like you would install any other Kubernetes-ready application. Launch your automatic installer here\nWe also have a Gardener Helm Chart. Alternatively you can use our garden setup project to create a fully configured Gardener landscape which also includes our Gardener Dashboard.\nFeedback and Support Feedback and contributions are always welcome!\nAll channels for getting in touch or learning about our project are listed under the community section. We are cordially inviting interested parties to join our weekly meetings.\nPlease report bugs or suggestions about our Kubernetes clusters as such or the Gardener itself as GitHub issues or join our Slack channel #gardener (please invite yourself to the Kubernetes workspace here).\nLearn More! Please find further resources about our project here:\n Our landing page gardener.cloud \u0026ldquo;Gardener Project Update\u0026rdquo; blog on kubernetes.io. \u0026ldquo;Gardener, the Kubernetes Botanist\u0026rdquo; blog on kubernetes.io SAP news article about \u0026ldquo;Project Gardener\u0026rdquo; Introduction movie: \u0026ldquo;Gardener - Planting the Seeds of Success in the Cloud\u0026rdquo; \u0026ldquo;Thinking Cloud Native\u0026rdquo; talk at EclipseCon 2018 Blog - \u0026ldquo;Showcase of Gardener at OSCON 2018\u0026rdquo;  "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/containerruntime/","title":"Gardener Container Runtime Extension","tags":[],"description":"","content":"Gardener Container Runtime Extension At the lowest layers of a Kubernetes node is the software that, among other things, starts and stops containers. It is called Container Runtime. The most widely known container runtime is Docker, but it is not alone in this space. In fact, the container runtime space has been rapidly evolving.\nKubernetes supports different container runtimes using Container Runtime Interface (CRI)  a plugin interface which enables kubelet to use a wide variety of container runtimes.\nGardener supports creation of Worker machines using CRI, more information can be found here: CRI Support.\nMotivation Prior to the Container Runtime Extensibility concept, Gardener used Docker as the only container runtime to use in shoot worker machines. Because of the wide variety of different container runtimes offers multiple important features (for example enhanced security concepts) it is important to enable end users to use other container runtimes as well.\nThe ContainerRuntime Extension Resource Here is what a typical ContainerRuntime resource would look-like:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:ContainerRuntimemetadata:name:my-container-runtimespec:binaryPath:/var/bin/containerruntimestype:gvisorworkerPool:name:worker-ubuntuselector:matchLabels:worker.gardener.cloud/pool:worker-ubuntuGardener deploys one ContainerRuntime resource per worker pool per CRI. To exemplify this, consider a Shoot having two worker pools (worker-one, worker-two) using containerd as the CRI as well as gvisor and kata as enabled container runtimes. Gardener would deploy four ContainerRuntime resources. For worker-one: one ContainerRuntime for type gvisor and one for type kata. The same resource are being deployed for worker-two.\nSupporting a new Container Runtime Provider To add support for another container runtime (e.g., gvisor, kata-containers, etc.) a container runtime extension controller needs to be implemented. It should support Gardener\u0026rsquo;s supported CRI plugins.\nThe container runtime extension should install the necessary resources into the shoot cluster (e.g., RuntimeClasses), and it should copy the runtime binaries to the relevant worker machines in path: spec.binaryPath. Gardener labels the shoot nodes according to the CRI configured: worker.gardener.cloud/cri-name=\u0026lt;value\u0026gt; (e.g worker.gardener.cloud/cri-name=containerd) and multiple labels for each of the container runtimes configured for the shoot Worker machine: containerruntime.worker.gardener.cloud/\u0026lt;container-runtime-type-value\u0026gt;=true (e.g containerruntime.worker.gardener.cloud/gvisor=true). The way to install the binaries is by creating a daemon set which copies the binaries from an image in a docker registry to the relevant labeled Worker\u0026rsquo;s nodes (avoid downloading binaries from internet to also cater with isolated environments).\nFor additional reference, please have a look at the runtime-gvsior provider extension, which provides more information on how to configure the necessary charts as well as the actuators required to reconcile container runtime inside the Shoot cluster to the desired state.\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/containerruntime/","title":"Gardener Container Runtime Extension","tags":[],"description":"","content":"Gardener Container Runtime Extension At the lowest layers of a Kubernetes node is the software that, among other things, starts and stops containers. It is called Container Runtime. The most widely known container runtime is Docker, but it is not alone in this space. In fact, the container runtime space has been rapidly evolving.\nKubernetes supports different container runtimes using Container Runtime Interface (CRI)  a plugin interface which enables kubelet to use a wide variety of container runtimes.\nGardener supports creation of Worker machines using CRI, more information can be found here: CRI Support.\nMotivation Prior to the Container Runtime Extensibility concept, Gardener used Docker as the only container runtime to use in shoot worker machines. Because of the wide variety of different container runtimes offers multiple important features (for example enhanced security concepts) it is important to enable end users to use other container runtimes as well.\nThe ContainerRuntime Extension Resource Here is what a typical ContainerRuntime resource would look-like:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:ContainerRuntimemetadata:name:my-container-runtimespec:binaryPath:/var/bin/containerruntimestype:gvisorworkerPool:name:worker-ubuntuselector:matchLabels:worker.gardener.cloud/pool:worker-ubuntuGardener deploys one ContainerRuntime resource per worker pool per CRI. To exemplify this, consider a Shoot having two worker pools (worker-one, worker-two) using containerd as the CRI as well as gvisor and kata as enabled container runtimes. Gardener would deploy four ContainerRuntime resources. For worker-one: one ContainerRuntime for type gvisor and one for type kata. The same resource are being deployed for worker-two.\nSupporting a new Container Runtime Provider To add support for another container runtime (e.g., gvisor, kata-containers, etc.) a container runtime extension controller needs to be implemented. It should support Gardener\u0026rsquo;s supported CRI plugins.\nThe container runtime extension should install the necessary resources into the shoot cluster (e.g., RuntimeClasses), and it should copy the runtime binaries to the relevant worker machines in path: spec.binaryPath. Gardener labels the shoot nodes according to the CRI configured: worker.gardener.cloud/cri-name=\u0026lt;value\u0026gt; (e.g worker.gardener.cloud/cri-name=containerd) and multiple labels for each of the container runtimes configured for the shoot Worker machine: containerruntime.worker.gardener.cloud/\u0026lt;container-runtime-type-value\u0026gt;=true (e.g containerruntime.worker.gardener.cloud/gvisor=true). The way to install the binaries is by creating a daemon set which copies the binaries from an image in a docker registry to the relevant labeled Worker\u0026rsquo;s nodes (avoid downloading binaries from internet to also cater with isolated environments).\nFor additional reference, please have a look at the runtime-gvsior provider extension, which provides more information on how to configure the necessary charts as well as the actuators required to reconcile container runtime inside the Shoot cluster to the desired state.\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/containerruntime/","title":"Gardener Container Runtime Extension","tags":[],"description":"","content":"Gardener Container Runtime Extension At the lowest layers of a Kubernetes node is the software that, among other things, starts and stops containers. It is called Container Runtime. The most widely known container runtime is Docker, but it is not alone in this space. In fact, the container runtime space has been rapidly evolving.\nKubernetes supports different container runtimes using Container Runtime Interface (CRI)  a plugin interface which enables kubelet to use a wide variety of container runtimes.\nGardener supports creation of Worker machines using CRI, more information can be found here: CRI Support.\nMotivation Prior to the Container Runtime Extensibility concept, Gardener used Docker as the only container runtime to use in shoot worker machines. Because of the wide variety of different container runtimes offers multiple important features (for example enhanced security concepts) it is important to enable end users to use other container runtimes as well.\nThe ContainerRuntime Extension Resource Here is what a typical ContainerRuntime resource would look-like:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:ContainerRuntimemetadata:name:my-container-runtimespec:binaryPath:/var/bin/containerruntimestype:gvisorworkerPool:name:worker-ubuntuselector:matchLabels:worker.gardener.cloud/pool:worker-ubuntuGardener deploys one ContainerRuntime resource per worker pool per CRI. To exemplify this, consider a Shoot having two worker pools (worker-one, worker-two) using containerd as the CRI as well as gvisor and kata as enabled container runtimes. Gardener would deploy four ContainerRuntime resources. For worker-one: one ContainerRuntime for type gvisor and one for type kata. The same resource are being deployed for worker-two.\nSupporting a new Container Runtime Provider To add support for another container runtime (e.g., gvisor, kata-containers, etc.) a container runtime extension controller needs to be implemented. It should support Gardener\u0026rsquo;s supported CRI plugins.\nThe container runtime extension should install the necessary resources into the shoot cluster (e.g., RuntimeClasses), and it should copy the runtime binaries to the relevant worker machines in path: spec.binaryPath. Gardener labels the shoot nodes according to the CRI configured: worker.gardener.cloud/cri-name=\u0026lt;value\u0026gt; (e.g worker.gardener.cloud/cri-name=containerd) and multiple labels for each of the container runtimes configured for the shoot Worker machine: containerruntime.worker.gardener.cloud/\u0026lt;container-runtime-type-value\u0026gt;=true (e.g containerruntime.worker.gardener.cloud/gvisor=true). The way to install the binaries is by creating a daemon set which copies the binaries from an image in a docker registry to the relevant labeled Worker\u0026rsquo;s nodes (avoid downloading binaries from internet to also cater with isolated environments).\nFor additional reference, please have a look at the runtime-gvsior provider extension, which provides more information on how to configure the necessary charts as well as the actuators required to reconcile container runtime inside the Shoot cluster to the desired state.\n"},{"uri":"https://gardener.cloud/documentation/guides/gardener-cookies/","title":"Gardener Cookies","tags":[],"description":"","content":"Green Tea Matcha Cookies For a team event during the Christmas season we decided to completely reinterpret the topic cookies. :-)\nMatcha cookies have the delicate flavor and color of green tea. These soft, pillowy and chewy green tea cookies are perfect with tea. And of course they fit perfectly to our logo\nIngredients  1 stick butter, softened  cup of granulated sugar 1 cup + 2 tablespoons all-purpose flour 2 eggs 1 tablespoons culinary grade matcha powder 1 teaspoon baking powder Pinch of salt  Instructions  Cream together the butter and sugar in a large mixing bowl - it should be creamy colored and airy. A hand blender or stand mixer works well for this. This helps the cookie become fluffy and chewy. Gently incorporate the eggs to the butter mixture one at a time. In a separate bowl, sift together all the dry ingredients. Add the dry ingredients to the wet by adding a little at a time and folding or gently mixing the batter together. Keep going until you\u0026rsquo;ve incorporated all the remaining flour mixture. The dough should be a beautiful green color. Chill the dough for at least an hour - up to overnight. The longer the better! Preheat your oven to 325 F. Roll the dough into balls the size of ping pong balls and place them on a non-stick cookie sheet. Bake them for 12-15 minutes until the bottoms just start to become golden brown and the cookie no longer looks wet in the middle. Note: you can always bake them at 350 F for a less moist, fluffy cookie. It will bake faster by about 2-4 minutes 350 F so watch them closely. Remove and let cool on a rack and enjoy!  Note Make sure you get culinary grade matcha powder. You should be able to find this in Asian or natural grocers\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/gardener-cookies/","title":"Gardener Cookies","tags":[],"description":"","content":"Green Tea Matcha Cookies For a team event during the Christmas season we decided to completely reinterpret the topic cookies. :-)\nMatcha cookies have the delicate flavor and color of green tea. These soft, pillowy and chewy green tea cookies are perfect with tea. And of course they fit perfectly to our logo\nIngredients  1 stick butter, softened  cup of granulated sugar 1 cup + 2 tablespoons all-purpose flour 2 eggs 1 tablespoons culinary grade matcha powder 1 teaspoon baking powder Pinch of salt  Instructions  Cream together the butter and sugar in a large mixing bowl - it should be creamy colored and airy. A hand blender or stand mixer works well for this. This helps the cookie become fluffy and chewy. Gently incorporate the eggs to the butter mixture one at a time. In a separate bowl, sift together all the dry ingredients. Add the dry ingredients to the wet by adding a little at a time and folding or gently mixing the batter together. Keep going until you\u0026rsquo;ve incorporated all the remaining flour mixture. The dough should be a beautiful green color. Chill the dough for at least an hour - up to overnight. The longer the better! Preheat your oven to 325 F. Roll the dough into balls the size of ping pong balls and place them on a non-stick cookie sheet. Bake them for 12-15 minutes until the bottoms just start to become golden brown and the cookie no longer looks wet in the middle. Note: you can always bake them at 350 F for a less moist, fluffy cookie. It will bake faster by about 2-4 minutes 350 F so watch them closely. Remove and let cool on a rack and enjoy!  Note Make sure you get culinary grade matcha powder. You should be able to find this in Asian or natural grocers\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/gardener-cookies/","title":"Gardener Cookies","tags":[],"description":"","content":"Green Tea Matcha Cookies For a team event during the Christmas season we decided to completely reinterpret the topic cookies. :-)\nMatcha cookies have the delicate flavor and color of green tea. These soft, pillowy and chewy green tea cookies are perfect with tea. And of course they fit perfectly to our logo\nIngredients  1 stick butter, softened  cup of granulated sugar 1 cup + 2 tablespoons all-purpose flour 2 eggs 1 tablespoons culinary grade matcha powder 1 teaspoon baking powder Pinch of salt  Instructions  Cream together the butter and sugar in a large mixing bowl - it should be creamy colored and airy. A hand blender or stand mixer works well for this. This helps the cookie become fluffy and chewy. Gently incorporate the eggs to the butter mixture one at a time. In a separate bowl, sift together all the dry ingredients. Add the dry ingredients to the wet by adding a little at a time and folding or gently mixing the batter together. Keep going until you\u0026rsquo;ve incorporated all the remaining flour mixture. The dough should be a beautiful green color. Chill the dough for at least an hour - up to overnight. The longer the better! Preheat your oven to 325 F. Roll the dough into balls the size of ping pong balls and place them on a non-stick cookie sheet. Bake them for 12-15 minutes until the bottoms just start to become golden brown and the cookie no longer looks wet in the middle. Note: you can always bake them at 350 F for a less moist, fluffy cookie. It will bake faster by about 2-4 minutes 350 F so watch them closely. Remove and let cool on a rack and enjoy!  Note Make sure you get culinary grade matcha powder. You should be able to find this in Asian or natural grocers\n"},{"uri":"https://gardener.cloud/documentation/guides/install_gardener/setup/","title":"Gardener DNS Management for Shoots","tags":[],"description":"Configure DNS Management For Shoot Clusters","content":"Gardener DNS Management for Shoots Introduction Gardener allows Shoot clusters to request DNS names for Ingresses and Services out of the box. To support this the gardener must be installed with the shoot-dns-service extension. This extension uses the seed\u0026rsquo;s dns management infrastructure to maintain DNS names for shoot clusters. So, far only the external DNS domain of a shoot (already used for the kubernetes api server and ingress DNS names) can be used for managed DNS names.\n #body-inner blockquote { border: 0; padding: 10px; margin-top: 40px; margin-bottom: 40px; border-radius: 4px; background-color: rgba(0,0,0,0.05); box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23); position:relative; padding-left:60px; } #body-inner blockquote:before { content: \"!\"; font-weight: bold; position: absolute; top: 0; bottom: 0; left: 0; background-color: #00a273; color: white; vertical-align: middle; margin: auto; width: 36px; font-size: 30px; text-align: center; }  Configuration A general description for configuring the DNS management of the gardener can be found here.\nTo generally enable the DNS management for shoot objects the shoot-dns-service extension must be registered by providing an appropriate extension registration in the garden cluster.\nHere it is possible to decide whether the extension should be always available for all shoots or whether the extension must be separately enabled per shoot.\nIf the extension should be used for all shoots the registration must set the globallyEnabled flag to true.\nspec:resources:- kind:Extensiontype:shoot-dns-servicegloballyEnabled:trueProviding Base Domains usable for a Shoot So, far only the external DNS domain of a shoot already used for the kubernetes api server and ingress DNS names can be used for managed DNS names. This is either the shoot domain as subdomain of the default domain configured for the gardener installation or a dedicated domain with dedicated access credentials configured for a dedicated shoot via the shoot manifest.\nShoot Feature Gate If the shoot DNS feature is not globally enabled by default (depends on the extension registration on the garden cluster), it must be enabled per shoot.\nTo enable the feature for a shoot, the shoot manifest must explicitly add the shoot-dns-service extension.\n...spec:extensions:- type:shoot-dns-service..."},{"uri":"https://gardener.cloud/v1.12.8/guides/install_gardener/setup/","title":"Gardener DNS Management for Shoots","tags":[],"description":"Configure DNS Management For Shoot Clusters","content":"Gardener DNS Management for Shoots Introduction Gardener allows Shoot clusters to request DNS names for Ingresses and Services out of the box. To support this the gardener must be installed with the shoot-dns-service extension. This extension uses the seed\u0026rsquo;s dns management infrastructure to maintain DNS names for shoot clusters. So, far only the external DNS domain of a shoot (already used for the kubernetes api server and ingress DNS names) can be used for managed DNS names.\n #body-inner blockquote { border: 0; padding: 10px; margin-top: 40px; margin-bottom: 40px; border-radius: 4px; background-color: rgba(0,0,0,0.05); box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23); position:relative; padding-left:60px; } #body-inner blockquote:before { content: \"!\"; font-weight: bold; position: absolute; top: 0; bottom: 0; left: 0; background-color: #00a273; color: white; vertical-align: middle; margin: auto; width: 36px; font-size: 30px; text-align: center; }  Configuration A general description for configuring the DNS management of the gardener can be found here.\nTo generally enable the DNS management for shoot objects the shoot-dns-service extension must be registered by providing an appropriate extension registration in the garden cluster.\nHere it is possible to decide whether the extension should be always available for all shoots or whether the extension must be separately enabled per shoot.\nIf the extension should be used for all shoots the registration must set the globallyEnabled flag to true.\nspec:resources:- kind:Extensiontype:shoot-dns-servicegloballyEnabled:trueProviding Base Domains usable for a Shoot So, far only the external DNS domain of a shoot already used for the kubernetes api server and ingress DNS names can be used for managed DNS names. This is either the shoot domain as subdomain of the default domain configured for the gardener installation or a dedicated domain with dedicated access credentials configured for a dedicated shoot via the shoot manifest.\nShoot Feature Gate If the shoot DNS feature is not globally enabled by default (depends on the extension registration on the garden cluster), it must be enabled per shoot.\nTo enable the feature for a shoot, the shoot manifest must explicitly add the shoot-dns-service extension.\n...spec:extensions:- type:shoot-dns-service..."},{"uri":"https://gardener.cloud/v1.13.2/guides/install_gardener/setup/","title":"Gardener DNS Management for Shoots","tags":[],"description":"Configure DNS Management For Shoot Clusters","content":"Gardener DNS Management for Shoots Introduction Gardener allows Shoot clusters to request DNS names for Ingresses and Services out of the box. To support this the gardener must be installed with the shoot-dns-service extension. This extension uses the seed\u0026rsquo;s dns management infrastructure to maintain DNS names for shoot clusters. So, far only the external DNS domain of a shoot (already used for the kubernetes api server and ingress DNS names) can be used for managed DNS names.\n #body-inner blockquote { border: 0; padding: 10px; margin-top: 40px; margin-bottom: 40px; border-radius: 4px; background-color: rgba(0,0,0,0.05); box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23); position:relative; padding-left:60px; } #body-inner blockquote:before { content: \"!\"; font-weight: bold; position: absolute; top: 0; bottom: 0; left: 0; background-color: #00a273; color: white; vertical-align: middle; margin: auto; width: 36px; font-size: 30px; text-align: center; }  Configuration A general description for configuring the DNS management of the gardener can be found here.\nTo generally enable the DNS management for shoot objects the shoot-dns-service extension must be registered by providing an appropriate extension registration in the garden cluster.\nHere it is possible to decide whether the extension should be always available for all shoots or whether the extension must be separately enabled per shoot.\nIf the extension should be used for all shoots the registration must set the globallyEnabled flag to true.\nspec:resources:- kind:Extensiontype:shoot-dns-servicegloballyEnabled:trueProviding Base Domains usable for a Shoot So, far only the external DNS domain of a shoot already used for the kubernetes api server and ingress DNS names can be used for managed DNS names. This is either the shoot domain as subdomain of the default domain configured for the gardener installation or a dedicated domain with dedicated access credentials configured for a dedicated shoot via the shoot manifest.\nShoot Feature Gate If the shoot DNS feature is not globally enabled by default (depends on the extension registration on the garden cluster), it must be enabled per shoot.\nTo enable the feature for a shoot, the shoot manifest must explicitly add the shoot-dns-service extension.\n...spec:extensions:- type:shoot-dns-service..."},{"uri":"https://gardener.cloud/documentation/concepts/extensions/network/","title":"Gardener Network Extension","tags":[],"description":"","content":"Gardener Network Extension Gardener is an open-source project that provides a nested user model. Basically, there are two types of services provided by Gardener to its users:\n Managed: end-users only request a Kubernetes cluster (Clusters-as-a-Service) Hosted: operators utilize Gardener to provide their own managed version of Kubernetes (Cluster-Provisioner-as-a-service)  Whether an operator or an end-user, it makes sense to provide choice. For example, for an end-user it might make sense to choose a network-plugin that would support enforcing network policies (some plugins does not come with network-policy support by default). For operators however, choice only matters for delegation purposes i.e., when providing an own managed-service, it becomes important to also provide choice over which network-plugins to use.\nFurthermore, Gardener provisions clusters on different cloud-providers with different networking requirements. For example, Azure does not support Calico Networking [1], this leads to the introduction of manual exceptions in static add-on charts which is error prone and can lead to failures during upgrades.\nFinally, every provider is different, and thus the network always needs to adapt to the infrastructure needs to provide better performance. Consistency does not necessarily lie in the implementation but in the interface.\nMotivation Prior to the Network Extensibility concept, Gardener followed a mono network-plugin support model (i.e., Calico). Although this seemed to be the easier approach, it did not completely reflect the real use-case. The goal of the Gardener Network Extensions is to support different network plugins, therefore, the specification for the network resource won\u0026rsquo;t be fixed and will be customized based on the underlying network plugin.\nTo do so, a ProviderConfig field in the spec will be provided where each plugin will define. Below is an example for how to deploy Calico as the cluster network plugin.\nThe Network Extensions Resource Here is what a typical Network resource would look-like:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Networkmetadata:name:my-networkspec:podCIDR:100.244.0.0/16serviceCIDR:100.32.0.0/13type:calicoproviderConfig:apiVersion:calico.networking.extensions.gardener.cloud/v1alpha1kind:NetworkConfigbackend:birdipam:cidr:usePodCIDRtype:host-localThe above resources is divided into two parts (more information can be found here):\n global configuration (e.g., podCIDR, serviceCIDR, and type) provider specific config (e.g., for calico we can choose to configure a bird backend)   Note: certain cloud-provider extensions might have webhooks that would modify the network-resource to fit into their network specific context. As previously mentioned, Azure does not support IPIP, as a result, the Azure provider extension implements a webhook to mutate the backend and set it to None instead of bird.\n Supporting a new Network Extension Provider To add support for another networking provider (e.g., weave, Cilium, Flannel, etc.) a network extension controller needs to be implemented which would optionally have its own custom configuration specified in the spec.providerConfig in the Network resource. For example, if support for a network plugin named gardenet is required, the following Network resource would be created:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Networkmetadata:name:my-networkspec:podCIDR:100.244.0.0/16serviceCIDR:100.32.0.0/13type:gardenetproviderConfig:apiVersion:gardenet.networking.extensions.gardener.cloud/v1alpha1kind:NetworkConfiggardenetCustomConfigField:\u0026lt;value\u0026gt; ipam:cidr:usePodCIDRtype:host-localOnce applied, the presumably implemented Gardenet extension controller, would pick the configuration up, parse the providerConfig and create the necessary resources in the shoot.\nFor additional reference, please have a look at the networking-calico provider extension, which provides more information on how to configure the necessary charts as well as the actuators required to reconcile networking inside the Shoot cluster to the desired state.\nReferences [1] Azure support for Calico Networking\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/network/","title":"Gardener Network Extension","tags":[],"description":"","content":"Gardener Network Extension Gardener is an open-source project that provides a nested user model. Basically, there are two types of services provided by Gardener to its users:\n Managed: end-users only request a Kubernetes cluster (Clusters-as-a-Service) Hosted: operators utilize Gardener to provide their own managed version of Kubernetes (Cluster-Provisioner-as-a-service)  Whether an operator or an end-user, it makes sense to provide choice. For example, for an end-user it might make sense to choose a network-plugin that would support enforcing network policies (some plugins does not come with network-policy support by default). For operators however, choice only matters for delegation purposes i.e., when providing an own managed-service, it becomes important to also provide choice over which network-plugins to use.\nFurthermore, Gardener provisions clusters on different cloud-providers with different networking requirements. For example, Azure does not support Calico Networking [1], this leads to the introduction of manual exceptions in static add-on charts which is error prone and can lead to failures during upgrades.\nFinally, every provider is different, and thus the network always needs to adapt to the infrastructure needs to provide better performance. Consistency does not necessarily lie in the implementation but in the interface.\nMotivation Prior to the Network Extensibility concept, Gardener followed a mono network-plugin support model (i.e., Calico). Although this seemed to be the easier approach, it did not completely reflect the real use-case. The goal of the Gardener Network Extensions is to support different network plugins, therefore, the specification for the network resource won\u0026rsquo;t be fixed and will be customized based on the underlying network plugin.\nTo do so, a ProviderConfig field in the spec will be provided where each plugin will define. Below is an example for how to deploy Calico as the cluster network plugin.\nThe Network Extensions Resource Here is what a typical Network resource would look-like:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Networkmetadata:name:my-networkspec:podCIDR:100.244.0.0/16serviceCIDR:100.32.0.0/13type:calicoproviderConfig:apiVersion:calico.networking.extensions.gardener.cloud/v1alpha1kind:NetworkConfigbackend:birdipam:cidr:usePodCIDRtype:host-localThe above resources is divided into two parts (more information can be found here):\n global configuration (e.g., podCIDR, serviceCIDR, and type) provider specific config (e.g., for calico we can choose to configure a bird backend)   Note: certain cloud-provider extensions might have webhooks that would modify the network-resource to fit into their network specific context. As previously mentioned, Azure does not support IPIP, as a result, the Azure provider extension implements a webhook to mutate the backend and set it to None instead of bird.\n Supporting a new Network Extension Provider To add support for another networking provider (e.g., weave, Cilium, Flannel, etc.) a network extension controller needs to be implemented which would optionally have its own custom configuration specified in the spec.providerConfig in the Network resource. For example, if support for a network plugin named gardenet is required, the following Network resource would be created:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Networkmetadata:name:my-networkspec:podCIDR:100.244.0.0/16serviceCIDR:100.32.0.0/13type:gardenetproviderConfig:apiVersion:gardenet.networking.extensions.gardener.cloud/v1alpha1kind:NetworkConfiggardenetCustomConfigField:\u0026lt;value\u0026gt; ipam:cidr:usePodCIDRtype:host-localOnce applied, the presumably implemented Gardenet extension controller, would pick the configuration up, parse the providerConfig and create the necessary resources in the shoot.\nFor additional reference, please have a look at the networking-calico provider extension, which provides more information on how to configure the necessary charts as well as the actuators required to reconcile networking inside the Shoot cluster to the desired state.\nReferences [1] Azure support for Calico Networking\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/network/","title":"Gardener Network Extension","tags":[],"description":"","content":"Gardener Network Extension Gardener is an open-source project that provides a nested user model. Basically, there are two types of services provided by Gardener to its users:\n Managed: end-users only request a Kubernetes cluster (Clusters-as-a-Service) Hosted: operators utilize Gardener to provide their own managed version of Kubernetes (Cluster-Provisioner-as-a-service)  Whether an operator or an end-user, it makes sense to provide choice. For example, for an end-user it might make sense to choose a network-plugin that would support enforcing network policies (some plugins does not come with network-policy support by default). For operators however, choice only matters for delegation purposes i.e., when providing an own managed-service, it becomes important to also provide choice over which network-plugins to use.\nFurthermore, Gardener provisions clusters on different cloud-providers with different networking requirements. For example, Azure does not support Calico Networking [1], this leads to the introduction of manual exceptions in static add-on charts which is error prone and can lead to failures during upgrades.\nFinally, every provider is different, and thus the network always needs to adapt to the infrastructure needs to provide better performance. Consistency does not necessarily lie in the implementation but in the interface.\nMotivation Prior to the Network Extensibility concept, Gardener followed a mono network-plugin support model (i.e., Calico). Although this seemed to be the easier approach, it did not completely reflect the real use-case. The goal of the Gardener Network Extensions is to support different network plugins, therefore, the specification for the network resource won\u0026rsquo;t be fixed and will be customized based on the underlying network plugin.\nTo do so, a ProviderConfig field in the spec will be provided where each plugin will define. Below is an example for how to deploy Calico as the cluster network plugin.\nThe Network Extensions Resource Here is what a typical Network resource would look-like:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Networkmetadata:name:my-networkspec:podCIDR:100.244.0.0/16serviceCIDR:100.32.0.0/13type:calicoproviderConfig:apiVersion:calico.networking.extensions.gardener.cloud/v1alpha1kind:NetworkConfigbackend:birdipam:cidr:usePodCIDRtype:host-localThe above resources is divided into two parts (more information can be found here):\n global configuration (e.g., podCIDR, serviceCIDR, and type) provider specific config (e.g., for calico we can choose to configure a bird backend)   Note: certain cloud-provider extensions might have webhooks that would modify the network-resource to fit into their network specific context. As previously mentioned, Azure does not support IPIP, as a result, the Azure provider extension implements a webhook to mutate the backend and set it to None instead of bird.\n Supporting a new Network Extension Provider To add support for another networking provider (e.g., weave, Cilium, Flannel, etc.) a network extension controller needs to be implemented which would optionally have its own custom configuration specified in the spec.providerConfig in the Network resource. For example, if support for a network plugin named gardenet is required, the following Network resource would be created:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Networkmetadata:name:my-networkspec:podCIDR:100.244.0.0/16serviceCIDR:100.32.0.0/13type:gardenetproviderConfig:apiVersion:gardenet.networking.extensions.gardener.cloud/v1alpha1kind:NetworkConfiggardenetCustomConfigField:\u0026lt;value\u0026gt; ipam:cidr:usePodCIDRtype:host-localOnce applied, the presumably implemented Gardenet extension controller, would pick the configuration up, parse the providerConfig and create the necessary resources in the shoot.\nFor additional reference, please have a look at the networking-calico provider extension, which provides more information on how to configure the necessary charts as well as the actuators required to reconcile networking inside the Shoot cluster to the desired state.\nReferences [1] Azure support for Calico Networking\n"},{"uri":"https://gardener.cloud/documentation/tutorials/shoot_istio_dns_certs/","title":"Gardener yourself a Shoot with Istio, custom Domains, and Certificates","tags":[],"description":"","content":"As we ramp up more and more friends of Gardener, I thought it worthwile to explore and write a tutorial about how to simply\n create a Gardener managed Kubernetes Cluster (Shoot) via kubectl, install Istio as a preferred, production ready Ingress/Service Mesh (instead of the Nginx Ingress addon), attach your own custom domain to be managed by Gardener, combine everything with certificates from Let\u0026rsquo;s Encrypt.  Here are some pre-pointers that you will need to go deeper:\n CRUD Gardener Shoot DNS Management Certificate Management Tutorial Domain Names Tutorial Certificates  If you try my instructions and fail, then read the alternative title of this tutorial as \"Shoot yourself in foot with Gardener, custom Domains, Istio and Certificates\".\n First Things First Login to your Gardener landscape, setup a project with adequate infrastructure credentials and then navigate to your account. Note down the name of your secret. I chose the GCP infrastructure from the vast possible options that my Gardener provides me with, so i had named the secret as shoot-operator-gcp.\nFrom the Access widget (leave the default settings) download your personalized kubeconfig into ~/.kube/kubeconfig-garden-myproject. Follow the instructions to setup kubelogin: For convinience, let us set an alias command with\nalias kgarden=\u0026#34;kubectl --kubeconfig ~/.kube/kubeconfig-garden-myproject.yaml\u0026#34; kgarden now gives you all botanical powers and connects you directly with your Gardener.\nYou should now be able to run kgarden get shoots, automatically get an oidc token, and list already running clusters/shoots.\nPrepare your Custom Domain I am going to use Cloud Flare as programmatic DNS of my custom domain mydomain.io. Please follow detailed instructions from Cloud Flare on how to delegate your domain (the free account does not support delegating subdomains). Alternatively, AWS Route53 (and most others) support delegating subdomains.\nI needed to follow these instructions and created the following secret:\napiVersion:v1kind:Secretmetadata:name:cloudflare-mydomain-iotype:Opaquedata:CLOUDFLARE_API_TOKEN:useYOURownDAMITzNDU2Nzg5MDEyMzQ1Njc4OQ==Apply this secret into your project with kgarden create -f cloudflare-mydomain-io.yaml.\nOur External DNS Manager also supports Amazon Route53, Google CloudDNS, AliCloud DNS, Azure DNS, or OpenStack Designate. Check it out.\nPrepare Gardener Extensions I now need to prepare the Gardener extensions shoot-dns-service and shoot-cert-service and set the parameters accordingly.\nPlease note, that the availability of Gardener Extensions depends on how your administrator has configured the Gardener landscape. Please contact your Gardener administrator in case you experience any issues during activation.\n The following snipplet allows Gardener to manage my entire custom domain, whereas with the include: attribute I restrict all dynamic entries under the subdomain gsicdc.mydomain.io:\ndns:providers:- domains:include:- gsicdc.mydomain.ioprimary:falsesecretName:cloudflare-mydomain-iotype:cloudflare-dnsextensions:- type:shoot-dns-serviceThe next snipplet allows Gardener to manage certificates automatically from Let\u0026rsquo;s Encrypt on mydomain.io for me:\nextensions:- type:shoot-cert-serviceproviderConfig:apiVersion:service.cert.extensions.gardener.cloud/v1alpha1issuers:- email:me@mail.comname:mydomainserver:\u0026#39;https://acme-v02.api.letsencrypt.org/directory\u0026#39;- email:me@mail.comname:mydomain-stagingserver:\u0026#39;https://acme-staging-v02.api.letsencrypt.org/directory\u0026#39;Adjust the snipplets with your parameters (don't forget your email). And please use the mydomain-staging issuer while you are testing and learning. Otherwise, Let's Encrypt will rate limit your frequent requests and you can wait a week until you can continue.\n References for Let\u0026rsquo;s Encrypt:\n Rate limit Staging environment Challenge Types Wildcard Certificates  Create the Gardener Shoot Cluster Remember I chose to create the Shoot on GCP, so below is the simplest declarative shoot or cluster order document. Notice that I am referring to the infrastructure credentials with shoot-operator-gcp and I combined the above snipplets into the yaml file:\napiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:name:gsicdcspec:dns:providers:- domains:include:- gsicdc.mydomain.ioprimary:falsesecretName:cloudflare-mydomain-iotype:cloudflare-dnsextensions:- type:shoot-dns-service- type:shoot-cert-serviceproviderConfig:apiVersion:service.cert.extensions.gardener.cloud/v1alpha1issuers:- email:me@mail.comname:mydomainserver:\u0026#39;https://acme-v02.api.letsencrypt.org/directory\u0026#39;- email:me@mail.comname:mydomain-stagingserver:\u0026#39;https://acme-staging-v02.api.letsencrypt.org/directory\u0026#39;cloudProfileName:gcpkubernetes:allowPrivilegedContainers:trueversion:1.18.2maintenance:autoUpdate:kubernetesVersion:truemachineImageVersion:truenetworking:nodes:10.250.0.0/16pods:100.96.0.0/11services:100.64.0.0/13type:calicoprovider:controlPlaneConfig:apiVersion:gcp.provider.extensions.gardener.cloud/v1alpha1kind:ControlPlaneConfigzone:europe-west1-dinfrastructureConfig:apiVersion:gcp.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:workers:10.250.0.0/16type:gcpworkers:- machine:image:name:gardenlinuxversion:11.29.2type:n1-standard-2maxSurge:1maxUnavailable:0maximum:2minimum:1name:my-workerpoolvolume:size:50Gitype:pd-standardzones:- europe-west1-dpurpose:testingregion:europe-west1secretBindingName:shoot-operator-gcpCreate your cluster and wait for it to be ready (about 5 to 7min).\n$ kgarden create -f gsicdc.yaml shoot.core.gardener.cloud/gsicdc created $ kgarden get shoot gsicdc --watch NAME CLOUDPROFILE VERSION SEED DOMAIN HIBERNATION OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE gsicdc gcp 1.18.2 gcp gsicdc.myproject.shoot.devgarden.cloud Awake Processing 38 Progressing Progressing Unknown Unknown 83s ... gsicdc gcp 1.18.2 gcp gsicdc.myproject.shoot.devgarden.cloud Awake Succeeded 100 True True True False 6m7s Get access to your freshly baked cluster and set your KUBECONFIG:\n$ kgarden get secrets gsicdc.kubeconfig -o jsonpath={.data.kubeconfig} | base64 -d \u0026gt;kubeconfig-gsicdc.yaml $ export KUBECONFIG=$(pwd)/kubeconfig-gsicdc.yaml $ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 100.64.0.1 \u0026lt;none\u0026gt; 443/TCP 89m Install Istio Please follow the Istio installation instructions and download istioctl. If you are on a Mac, I recommend\n$ brew install istioctl I want to install Istio with a default profile and SDS enabled. Furthermore I pass the following annotations to the service object istio-ingressgateway in the istio-system namespace.\nannotations:cert.gardener.cloud/issuer:mydomain-stagingcert.gardener.cloud/secretname:wildcard-tlsdns.gardener.cloud/class:gardendns.gardener.cloud/dnsnames:\u0026#34;*.gsicdc.mydomain.io\u0026#34;dns.gardener.cloud/ttl:\u0026#34;120\u0026#34;With these annotations three things now happen automagically:\n The External DNS Manager, provided to you as a service (dns.gardener.cloud/class: garden), picks up the request and creates the wildcard DNS entry *.gsicdc.mydomain.io with a time to live of 120sec at your DNS provider. My provider Cloud Flare is very very quick (as opposed to some other services). You should be able to verify the entry with dig lovemygardener.gsicdc.mydomain.io within seconds. The Certificate Mangement picks up the request as well and initates a DNS01 protocol exchange with Let\u0026rsquo;s Encrypt; using the staging environment referred to with the issuer behind mydomain-staging. After aproximately 70sec (give and take) you will receive the wildcard certificate in the wildcard-tls secret in the namespace istio-system.  Notice, that the namespace for the certificate secret is often the cause of many troubeshooting sessions: the secret must reside in the same namespace of the gateway.\n Here is the istio-install script:\n$ export domainname=\u0026#34;*.gsicdc.mydomain.io\u0026#34; $ export issuer=\u0026#34;mydomain-staging\u0026#34; $ cat \u0026lt;\u0026lt;EOF | istioctl install -y -f - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: profile: default components: ingressGateways: - name: istio-ingressgateway enabled: true k8s: serviceAnnotations: cert.gardener.cloud/issuer: \u0026#34;${issuer}\u0026#34; cert.gardener.cloud/secretname: wildcard-tls dns.gardener.cloud/class: garden dns.gardener.cloud/dnsnames: \u0026#34;${domainname}\u0026#34; dns.gardener.cloud/ttl: \u0026#34;120\u0026#34; EOF Verify that setup is working and that DNS and certificates have been created/delivered:\n$ kubectl -n istio-system describe service istio-ingressgateway \u0026lt;snip\u0026gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal EnsuringLoadBalancer 58s service-controller Ensuring load balancer Normal reconcile 58s cert-controller-manager created certificate object istio-system/istio-ingressgateway-service-pwqdm Normal cert-annotation 58s cert-controller-manager wildcard-tls: cert request is pending Normal cert-annotation 54s cert-controller-manager wildcard-tls: certificate pending: certificate requested, preparing/waiting for successful DNS01 challenge Normal cert-annotation 28s cert-controller-manager wildcard-tls: certificate ready Normal EnsuredLoadBalancer 26s service-controller Ensured load balancer Normal reconcile 26s dns-controller-manager created dns entry object shoot--core--gsicdc/istio-ingressgateway-service-p9qqb Normal dns-annotation 26s dns-controller-manager *.gsicdc.mydomain.io: dns entry is pending Normal dns-annotation 21s (x3 over 21s) dns-controller-manager *.gsicdc.mydomain.io: dns entry active $ dig lovemygardener.gsicdc.mydomain.io ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; lovemygardener.gsicdc.mydomain.io \u0026lt;snip\u0026gt; ;; ANSWER SECTION: lovemygardener.gsicdc.mydomain.io. 120 IN A\t35.195.120.62 \u0026lt;snip\u0026gt; There you have it, the wildcard-tls certificate is ready and the *.gsicdc.mydomain.io dns entry is active. Traffic will be going your way.\nHandy tools to install Another set of fine tools to use are k14s\u0026rsquo;s kapp, k9s and HTTPie. While we are at it, let\u0026rsquo;s install them all. If you are on a Mac, I recommend:\n$ brew tap k14s/tap $ brew install ytt kbld kapp kwt imgpkg vendir $ brew install derailed/k9s/k9s $ brew install httpie Ingress to your service Networking is a central part of Kubernetes, but it can be challenging to understand exactly how it is expected to work. You should learn about Kubernetes networking, and first try to debug problems yourself. With a solid managed cluster from Gardener, it is always PEBCAK!\n Kubernetes Ingress is a subject that is evolving to much broader standard. Please watch Evolving the Kubernetes Ingress APIs to GA and Beyond for a good introduction. In this example, I did not want to use the Kubernetes Ingress compatibility option of Istio. Instead, I used VirtualService and Gateway from the Istio\u0026rsquo;s API group networking.istio.io/v1beta1 directly, and enabled istio-injection generically for the namespace.\nI use httpbin as service that I want to expose to the internet, or where my ingress should be routed to (depends on your point of view, I guess).\napiVersion:v1kind:Namespacemetadata:name:productionlabels:istio-injection:enabled---apiVersion:v1kind:Servicemetadata:name:httpbinnamespace:productionlabels:app:httpbinspec:ports:- name:httpport:8000targetPort:80selector:app:httpbin---apiVersion:apps/v1kind:Deploymentmetadata:name:httpbinnamespace:productionspec:replicas:1selector:matchLabels:app:httpbintemplate:metadata:labels:app:httpbinspec:containers:- image:docker.io/kennethreitz/httpbinimagePullPolicy:IfNotPresentname:httpbinports:- containerPort:80---apiVersion:networking.istio.io/v1beta1kind:Gatewaymetadata:name:httpbin-gwnamespace:productionspec:selector:istio:ingressgateway#! use istio default ingress gatewayservers:- port:number:80name:httpprotocol:HTTPtls:httpsRedirect:truehosts:- \u0026#34;httpbin.gsicdc.mydomain.io\u0026#34;- port:number:443name:httpsprotocol:HTTPStls:mode:SIMPLEcredentialName:wildcard-tlshosts:- \u0026#34;httpbin.gsicdc.mydomain.io\u0026#34;---apiVersion:networking.istio.io/v1beta1kind:VirtualServicemetadata:name:httpbin-vsnamespace:productionspec:hosts:- \u0026#34;httpbin.gsicdc.mydomain.io\u0026#34;gateways:- httpbin-gwhttp:- match:- uri:regex:/.*route:- destination:port:number:8000host:httpbin---Let us now deploy the whole package of Kubernetes primitives using kapp:\n$ kapp deploy -a httpbin -f httpbin-kapp.yaml Target cluster \u0026#39;https://api.gsicdc.myproject.shoot.devgarden.cloud\u0026#39; (nodes: shoot--myproject--gsicdc-my-workerpool-z1-6586c8f6cb-x24kh) Changes Namespace Name Kind Conds. Age Op Wait to Rs Ri (cluster) production Namespace - - create reconcile - - production httpbin Deployment - - create reconcile - - ^ httpbin Service - - create reconcile - - ^ httpbin-gw Gateway - - create reconcile - - ^ httpbin-vs VirtualService - - create reconcile - - Op: 5 create, 0 delete, 0 update, 0 noop Wait to: 5 reconcile, 0 delete, 0 noop Continue? [yN]: y 5:36:31PM: ---- applying 1 changes [0/5 done] ---- \u0026lt;snip\u0026gt; 5:37:00PM: ok: reconcile deployment/httpbin (apps/v1) namespace: production 5:37:00PM: ---- applying complete [5/5 done] ---- 5:37:00PM: ---- waiting complete [5/5 done] ---- Succeeded Let\u0026rsquo;s finaly test the service (Of course you can use the browser as well):\n$ http httpbin.gsicdc.mydomain.io HTTP/1.1 301 Moved Permanently content-length: 0 date: Wed, 13 May 2020 21:29:13 GMT location: https://httpbin.gsicdc.mydomain.io/ server: istio-envoy $ curl -k https://httpbin.gsicdc.mydomain.io/ip { \u0026#34;origin\u0026#34;: \u0026#34;10.250.0.2\u0026#34; } Quod erat demonstrandum. The proof of exchanging the issuer is now left to the reader.\nRemember that the certificate is actually not valid because it is issued from the Let's encrypt staging environment. Thus, we needed \"curl -k\" or \"http --verify no\".\n Hint: use the interactive k9s tool. Cleanup Remove the cloud native application:\n$ kapp ls Apps in namespace \u0026#39;default\u0026#39; Name Namespaces Lcs Lca httpbin (cluster),production true 17m $ kapp delete -a httpbin ... Continue? [yN]: y ... 11:47:47PM: ---- waiting complete [8/8 done] ---- Succeeded Remove Istio:\n$ istioctl x uninstall --purge clusterrole.rbac.authorization.k8s.io \u0026#34;prometheus-istio-system\u0026#34; deleted clusterrolebinding.rbac.authorization.k8s.io \u0026#34;prometheus-istio-system\u0026#34; deleted ... Delete your Shoot:\nkgarden annotate shoot gsicdc confirmation.gardener.cloud/deletion=true --overwrite kgarden delete shoot gsicdc --wait=false "},{"uri":"https://gardener.cloud/v1.12.8/tutorials/shoot_istio_dns_certs/","title":"Gardener yourself a Shoot with Istio, custom Domains, and Certificates","tags":[],"description":"","content":"As we ramp up more and more friends of Gardener, I thought it worthwile to explore and write a tutorial about how to simply\n create a Gardener managed Kubernetes Cluster (Shoot) via kubectl, install Istio as a preferred, production ready Ingress/Service Mesh (instead of the Nginx Ingress addon), attach your own custom domain to be managed by Gardener, combine everything with certificates from Let\u0026rsquo;s Encrypt.  Here are some pre-pointers that you will need to go deeper:\n CRUD Gardener Shoot DNS Management Certificate Management Tutorial Domain Names Tutorial Certificates  If you try my instructions and fail, then read the alternative title of this tutorial as \"Shoot yourself in foot with Gardener, custom Domains, Istio and Certificates\".\n First Things First Login to your Gardener landscape, setup a project with adequate infrastructure credentials and then navigate to your account. Note down the name of your secret. I chose the GCP infrastructure from the vast possible options that my Gardener provides me with, so i had named the secret as shoot-operator-gcp.\nFrom the Access widget (leave the default settings) download your personalized kubeconfig into ~/.kube/kubeconfig-garden-myproject. Follow the instructions to setup kubelogin: For convinience, let us set an alias command with\nalias kgarden=\u0026#34;kubectl --kubeconfig ~/.kube/kubeconfig-garden-myproject.yaml\u0026#34; kgarden now gives you all botanical powers and connects you directly with your Gardener.\nYou should now be able to run kgarden get shoots, automatically get an oidc token, and list already running clusters/shoots.\nPrepare your Custom Domain I am going to use Cloud Flare as programmatic DNS of my custom domain mydomain.io. Please follow detailed instructions from Cloud Flare on how to delegate your domain (the free account does not support delegating subdomains). Alternatively, AWS Route53 (and most others) support delegating subdomains.\nI needed to follow these instructions and created the following secret:\napiVersion:v1kind:Secretmetadata:name:cloudflare-mydomain-iotype:Opaquedata:CLOUDFLARE_API_TOKEN:useYOURownDAMITzNDU2Nzg5MDEyMzQ1Njc4OQ==Apply this secret into your project with kgarden create -f cloudflare-mydomain-io.yaml.\nOur External DNS Manager also supports Amazon Route53, Google CloudDNS, AliCloud DNS, Azure DNS, or OpenStack Designate. Check it out.\nPrepare Gardener Extensions I now need to prepare the Gardener extensions shoot-dns-service and shoot-cert-service and set the parameters accordingly.\nPlease note, that the availability of Gardener Extensions depends on how your administrator has configured the Gardener landscape. Please contact your Gardener administrator in case you experience any issues during activation.\n The following snipplet allows Gardener to manage my entire custom domain, whereas with the include: attribute I restrict all dynamic entries under the subdomain gsicdc.mydomain.io:\ndns:providers:- domains:include:- gsicdc.mydomain.ioprimary:falsesecretName:cloudflare-mydomain-iotype:cloudflare-dnsextensions:- type:shoot-dns-serviceThe next snipplet allows Gardener to manage certificates automatically from Let\u0026rsquo;s Encrypt on mydomain.io for me:\nextensions:- type:shoot-cert-serviceproviderConfig:apiVersion:service.cert.extensions.gardener.cloud/v1alpha1issuers:- email:me@mail.comname:mydomainserver:\u0026#39;https://acme-v02.api.letsencrypt.org/directory\u0026#39;- email:me@mail.comname:mydomain-stagingserver:\u0026#39;https://acme-staging-v02.api.letsencrypt.org/directory\u0026#39;Adjust the snipplets with your parameters (don't forget your email). And please use the mydomain-staging issuer while you are testing and learning. Otherwise, Let's Encrypt will rate limit your frequent requests and you can wait a week until you can continue.\n References for Let\u0026rsquo;s Encrypt:\n Rate limit Staging environment Challenge Types Wildcard Certificates  Create the Gardener Shoot Cluster Remember I chose to create the Shoot on GCP, so below is the simplest declarative shoot or cluster order document. Notice that I am referring to the infrastructure credentials with shoot-operator-gcp and I combined the above snipplets into the yaml file:\napiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:name:gsicdcspec:dns:providers:- domains:include:- gsicdc.mydomain.ioprimary:falsesecretName:cloudflare-mydomain-iotype:cloudflare-dnsextensions:- type:shoot-dns-service- type:shoot-cert-serviceproviderConfig:apiVersion:service.cert.extensions.gardener.cloud/v1alpha1issuers:- email:me@mail.comname:mydomainserver:\u0026#39;https://acme-v02.api.letsencrypt.org/directory\u0026#39;- email:me@mail.comname:mydomain-stagingserver:\u0026#39;https://acme-staging-v02.api.letsencrypt.org/directory\u0026#39;cloudProfileName:gcpkubernetes:allowPrivilegedContainers:trueversion:1.18.2maintenance:autoUpdate:kubernetesVersion:truemachineImageVersion:truenetworking:nodes:10.250.0.0/16pods:100.96.0.0/11services:100.64.0.0/13type:calicoprovider:controlPlaneConfig:apiVersion:gcp.provider.extensions.gardener.cloud/v1alpha1kind:ControlPlaneConfigzone:europe-west1-dinfrastructureConfig:apiVersion:gcp.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:workers:10.250.0.0/16type:gcpworkers:- machine:image:name:gardenlinuxversion:11.29.2type:n1-standard-2maxSurge:1maxUnavailable:0maximum:2minimum:1name:my-workerpoolvolume:size:50Gitype:pd-standardzones:- europe-west1-dpurpose:testingregion:europe-west1secretBindingName:shoot-operator-gcpCreate your cluster and wait for it to be ready (about 5 to 7min).\n$ kgarden create -f gsicdc.yaml shoot.core.gardener.cloud/gsicdc created $ kgarden get shoot gsicdc --watch NAME CLOUDPROFILE VERSION SEED DOMAIN HIBERNATION OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE gsicdc gcp 1.18.2 gcp gsicdc.myproject.shoot.devgarden.cloud Awake Processing 38 Progressing Progressing Unknown Unknown 83s ... gsicdc gcp 1.18.2 gcp gsicdc.myproject.shoot.devgarden.cloud Awake Succeeded 100 True True True False 6m7s Get access to your freshly baked cluster and set your KUBECONFIG:\n$ kgarden get secrets gsicdc.kubeconfig -o jsonpath={.data.kubeconfig} | base64 -d \u0026gt;kubeconfig-gsicdc.yaml $ export KUBECONFIG=$(pwd)/kubeconfig-gsicdc.yaml $ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 100.64.0.1 \u0026lt;none\u0026gt; 443/TCP 89m Install Istio Please follow the Istio installation instructions and download istioctl. If you are on a Mac, I recommend\n$ brew install istioctl I want to install Istio with a default profile and SDS enabled. Furthermore I pass the following annotations to the service object istio-ingressgateway in the istio-system namespace.\nannotations:cert.gardener.cloud/issuer:mydomain-stagingcert.gardener.cloud/secretname:wildcard-tlsdns.gardener.cloud/class:gardendns.gardener.cloud/dnsnames:\u0026#34;*.gsicdc.mydomain.io\u0026#34;dns.gardener.cloud/ttl:\u0026#34;120\u0026#34;With these annotations three things now happen automagically:\n The External DNS Manager, provided to you as a service (dns.gardener.cloud/class: garden), picks up the request and creates the wildcard DNS entry *.gsicdc.mydomain.io with a time to live of 120sec at your DNS provider. My provider Cloud Flare is very very quick (as opposed to some other services). You should be able to verify the entry with dig lovemygardener.gsicdc.mydomain.io within seconds. The Certificate Mangement picks up the request as well and initates a DNS01 protocol exchange with Let\u0026rsquo;s Encrypt; using the staging environment referred to with the issuer behind mydomain-staging. After aproximately 70sec (give and take) you will receive the wildcard certificate in the wildcard-tls secret in the namespace istio-system.  Notice, that the namespace for the certificate secret is often the cause of many troubeshooting sessions: the secret must reside in the same namespace of the gateway.\n Here is the istio-install script:\n$ export domainname=\u0026#34;*.gsicdc.mydomain.io\u0026#34; $ export issuer=\u0026#34;mydomain-staging\u0026#34; $ cat \u0026lt;\u0026lt;EOF | istioctl install -y -f - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: profile: default components: ingressGateways: - name: istio-ingressgateway enabled: true k8s: serviceAnnotations: cert.gardener.cloud/issuer: \u0026#34;${issuer}\u0026#34; cert.gardener.cloud/secretname: wildcard-tls dns.gardener.cloud/class: garden dns.gardener.cloud/dnsnames: \u0026#34;${domainname}\u0026#34; dns.gardener.cloud/ttl: \u0026#34;120\u0026#34; EOF Verify that setup is working and that DNS and certificates have been created/delivered:\n$ kubectl -n istio-system describe service istio-ingressgateway \u0026lt;snip\u0026gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal EnsuringLoadBalancer 58s service-controller Ensuring load balancer Normal reconcile 58s cert-controller-manager created certificate object istio-system/istio-ingressgateway-service-pwqdm Normal cert-annotation 58s cert-controller-manager wildcard-tls: cert request is pending Normal cert-annotation 54s cert-controller-manager wildcard-tls: certificate pending: certificate requested, preparing/waiting for successful DNS01 challenge Normal cert-annotation 28s cert-controller-manager wildcard-tls: certificate ready Normal EnsuredLoadBalancer 26s service-controller Ensured load balancer Normal reconcile 26s dns-controller-manager created dns entry object shoot--core--gsicdc/istio-ingressgateway-service-p9qqb Normal dns-annotation 26s dns-controller-manager *.gsicdc.mydomain.io: dns entry is pending Normal dns-annotation 21s (x3 over 21s) dns-controller-manager *.gsicdc.mydomain.io: dns entry active $ dig lovemygardener.gsicdc.mydomain.io ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; lovemygardener.gsicdc.mydomain.io \u0026lt;snip\u0026gt; ;; ANSWER SECTION: lovemygardener.gsicdc.mydomain.io. 120 IN A\t35.195.120.62 \u0026lt;snip\u0026gt; There you have it, the wildcard-tls certificate is ready and the *.gsicdc.mydomain.io dns entry is active. Traffic will be going your way.\nHandy tools to install Another set of fine tools to use are k14s\u0026rsquo;s kapp, k9s and HTTPie. While we are at it, let\u0026rsquo;s install them all. If you are on a Mac, I recommend:\n$ brew tap k14s/tap $ brew install ytt kbld kapp kwt imgpkg vendir $ brew install derailed/k9s/k9s $ brew install httpie Ingress to your service Networking is a central part of Kubernetes, but it can be challenging to understand exactly how it is expected to work. You should learn about Kubernetes networking, and first try to debug problems yourself. With a solid managed cluster from Gardener, it is always PEBCAK!\n Kubernetes Ingress is a subject that is evolving to much broader standard. Please watch Evolving the Kubernetes Ingress APIs to GA and Beyond for a good introduction. In this example, I did not want to use the Kubernetes Ingress compatibility option of Istio. Instead, I used VirtualService and Gateway from the Istio\u0026rsquo;s API group networking.istio.io/v1beta1 directly, and enabled istio-injection generically for the namespace.\nI use httpbin as service that I want to expose to the internet, or where my ingress should be routed to (depends on your point of view, I guess).\napiVersion:v1kind:Namespacemetadata:name:productionlabels:istio-injection:enabled---apiVersion:v1kind:Servicemetadata:name:httpbinnamespace:productionlabels:app:httpbinspec:ports:- name:httpport:8000targetPort:80selector:app:httpbin---apiVersion:apps/v1kind:Deploymentmetadata:name:httpbinnamespace:productionspec:replicas:1selector:matchLabels:app:httpbintemplate:metadata:labels:app:httpbinspec:containers:- image:docker.io/kennethreitz/httpbinimagePullPolicy:IfNotPresentname:httpbinports:- containerPort:80---apiVersion:networking.istio.io/v1beta1kind:Gatewaymetadata:name:httpbin-gwnamespace:productionspec:selector:istio:ingressgateway#! use istio default ingress gatewayservers:- port:number:80name:httpprotocol:HTTPtls:httpsRedirect:truehosts:- \u0026#34;httpbin.gsicdc.mydomain.io\u0026#34;- port:number:443name:httpsprotocol:HTTPStls:mode:SIMPLEcredentialName:wildcard-tlshosts:- \u0026#34;httpbin.gsicdc.mydomain.io\u0026#34;---apiVersion:networking.istio.io/v1beta1kind:VirtualServicemetadata:name:httpbin-vsnamespace:productionspec:hosts:- \u0026#34;httpbin.gsicdc.mydomain.io\u0026#34;gateways:- httpbin-gwhttp:- match:- uri:regex:/.*route:- destination:port:number:8000host:httpbin---Let us now deploy the whole package of Kubernetes primitives using kapp:\n$ kapp deploy -a httpbin -f httpbin-kapp.yaml Target cluster \u0026#39;https://api.gsicdc.myproject.shoot.devgarden.cloud\u0026#39; (nodes: shoot--myproject--gsicdc-my-workerpool-z1-6586c8f6cb-x24kh) Changes Namespace Name Kind Conds. Age Op Wait to Rs Ri (cluster) production Namespace - - create reconcile - - production httpbin Deployment - - create reconcile - - ^ httpbin Service - - create reconcile - - ^ httpbin-gw Gateway - - create reconcile - - ^ httpbin-vs VirtualService - - create reconcile - - Op: 5 create, 0 delete, 0 update, 0 noop Wait to: 5 reconcile, 0 delete, 0 noop Continue? [yN]: y 5:36:31PM: ---- applying 1 changes [0/5 done] ---- \u0026lt;snip\u0026gt; 5:37:00PM: ok: reconcile deployment/httpbin (apps/v1) namespace: production 5:37:00PM: ---- applying complete [5/5 done] ---- 5:37:00PM: ---- waiting complete [5/5 done] ---- Succeeded Let\u0026rsquo;s finaly test the service (Of course you can use the browser as well):\n$ http httpbin.gsicdc.mydomain.io HTTP/1.1 301 Moved Permanently content-length: 0 date: Wed, 13 May 2020 21:29:13 GMT location: https://httpbin.gsicdc.mydomain.io/ server: istio-envoy $ curl -k https://httpbin.gsicdc.mydomain.io/ip { \u0026#34;origin\u0026#34;: \u0026#34;10.250.0.2\u0026#34; } Quod erat demonstrandum. The proof of exchanging the issuer is now left to the reader.\nRemember that the certificate is actually not valid because it is issued from the Let's encrypt staging environment. Thus, we needed \"curl -k\" or \"http --verify no\".\n Hint: use the interactive k9s tool. Cleanup Remove the cloud native application:\n$ kapp ls Apps in namespace \u0026#39;default\u0026#39; Name Namespaces Lcs Lca httpbin (cluster),production true 17m $ kapp delete -a httpbin ... Continue? [yN]: y ... 11:47:47PM: ---- waiting complete [8/8 done] ---- Succeeded Remove Istio:\n$ istioctl x uninstall --purge clusterrole.rbac.authorization.k8s.io \u0026#34;prometheus-istio-system\u0026#34; deleted clusterrolebinding.rbac.authorization.k8s.io \u0026#34;prometheus-istio-system\u0026#34; deleted ... Delete your Shoot:\nkgarden annotate shoot gsicdc confirmation.gardener.cloud/deletion=true --overwrite kgarden delete shoot gsicdc --wait=false "},{"uri":"https://gardener.cloud/v1.13.2/tutorials/shoot_istio_dns_certs/","title":"Gardener yourself a Shoot with Istio, custom Domains, and Certificates","tags":[],"description":"","content":"As we ramp up more and more friends of Gardener, I thought it worthwile to explore and write a tutorial about how to simply\n create a Gardener managed Kubernetes Cluster (Shoot) via kubectl, install Istio as a preferred, production ready Ingress/Service Mesh (instead of the Nginx Ingress addon), attach your own custom domain to be managed by Gardener, combine everything with certificates from Let\u0026rsquo;s Encrypt.  Here are some pre-pointers that you will need to go deeper:\n CRUD Gardener Shoot DNS Management Certificate Management Tutorial Domain Names Tutorial Certificates  If you try my instructions and fail, then read the alternative title of this tutorial as \"Shoot yourself in foot with Gardener, custom Domains, Istio and Certificates\".\n First Things First Login to your Gardener landscape, setup a project with adequate infrastructure credentials and then navigate to your account. Note down the name of your secret. I chose the GCP infrastructure from the vast possible options that my Gardener provides me with, so i had named the secret as shoot-operator-gcp.\nFrom the Access widget (leave the default settings) download your personalized kubeconfig into ~/.kube/kubeconfig-garden-myproject. Follow the instructions to setup kubelogin: For convinience, let us set an alias command with\nalias kgarden=\u0026#34;kubectl --kubeconfig ~/.kube/kubeconfig-garden-myproject.yaml\u0026#34; kgarden now gives you all botanical powers and connects you directly with your Gardener.\nYou should now be able to run kgarden get shoots, automatically get an oidc token, and list already running clusters/shoots.\nPrepare your Custom Domain I am going to use Cloud Flare as programmatic DNS of my custom domain mydomain.io. Please follow detailed instructions from Cloud Flare on how to delegate your domain (the free account does not support delegating subdomains). Alternatively, AWS Route53 (and most others) support delegating subdomains.\nI needed to follow these instructions and created the following secret:\napiVersion:v1kind:Secretmetadata:name:cloudflare-mydomain-iotype:Opaquedata:CLOUDFLARE_API_TOKEN:useYOURownDAMITzNDU2Nzg5MDEyMzQ1Njc4OQ==Apply this secret into your project with kgarden create -f cloudflare-mydomain-io.yaml.\nOur External DNS Manager also supports Amazon Route53, Google CloudDNS, AliCloud DNS, Azure DNS, or OpenStack Designate. Check it out.\nPrepare Gardener Extensions I now need to prepare the Gardener extensions shoot-dns-service and shoot-cert-service and set the parameters accordingly.\nPlease note, that the availability of Gardener Extensions depends on how your administrator has configured the Gardener landscape. Please contact your Gardener administrator in case you experience any issues during activation.\n The following snipplet allows Gardener to manage my entire custom domain, whereas with the include: attribute I restrict all dynamic entries under the subdomain gsicdc.mydomain.io:\ndns:providers:- domains:include:- gsicdc.mydomain.ioprimary:falsesecretName:cloudflare-mydomain-iotype:cloudflare-dnsextensions:- type:shoot-dns-serviceThe next snipplet allows Gardener to manage certificates automatically from Let\u0026rsquo;s Encrypt on mydomain.io for me:\nextensions:- type:shoot-cert-serviceproviderConfig:apiVersion:service.cert.extensions.gardener.cloud/v1alpha1issuers:- email:me@mail.comname:mydomainserver:\u0026#39;https://acme-v02.api.letsencrypt.org/directory\u0026#39;- email:me@mail.comname:mydomain-stagingserver:\u0026#39;https://acme-staging-v02.api.letsencrypt.org/directory\u0026#39;Adjust the snipplets with your parameters (don't forget your email). And please use the mydomain-staging issuer while you are testing and learning. Otherwise, Let's Encrypt will rate limit your frequent requests and you can wait a week until you can continue.\n References for Let\u0026rsquo;s Encrypt:\n Rate limit Staging environment Challenge Types Wildcard Certificates  Create the Gardener Shoot Cluster Remember I chose to create the Shoot on GCP, so below is the simplest declarative shoot or cluster order document. Notice that I am referring to the infrastructure credentials with shoot-operator-gcp and I combined the above snipplets into the yaml file:\napiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:name:gsicdcspec:dns:providers:- domains:include:- gsicdc.mydomain.ioprimary:falsesecretName:cloudflare-mydomain-iotype:cloudflare-dnsextensions:- type:shoot-dns-service- type:shoot-cert-serviceproviderConfig:apiVersion:service.cert.extensions.gardener.cloud/v1alpha1issuers:- email:me@mail.comname:mydomainserver:\u0026#39;https://acme-v02.api.letsencrypt.org/directory\u0026#39;- email:me@mail.comname:mydomain-stagingserver:\u0026#39;https://acme-staging-v02.api.letsencrypt.org/directory\u0026#39;cloudProfileName:gcpkubernetes:allowPrivilegedContainers:trueversion:1.18.2maintenance:autoUpdate:kubernetesVersion:truemachineImageVersion:truenetworking:nodes:10.250.0.0/16pods:100.96.0.0/11services:100.64.0.0/13type:calicoprovider:controlPlaneConfig:apiVersion:gcp.provider.extensions.gardener.cloud/v1alpha1kind:ControlPlaneConfigzone:europe-west1-dinfrastructureConfig:apiVersion:gcp.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:workers:10.250.0.0/16type:gcpworkers:- machine:image:name:gardenlinuxversion:11.29.2type:n1-standard-2maxSurge:1maxUnavailable:0maximum:2minimum:1name:my-workerpoolvolume:size:50Gitype:pd-standardzones:- europe-west1-dpurpose:testingregion:europe-west1secretBindingName:shoot-operator-gcpCreate your cluster and wait for it to be ready (about 5 to 7min).\n$ kgarden create -f gsicdc.yaml shoot.core.gardener.cloud/gsicdc created $ kgarden get shoot gsicdc --watch NAME CLOUDPROFILE VERSION SEED DOMAIN HIBERNATION OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE gsicdc gcp 1.18.2 gcp gsicdc.myproject.shoot.devgarden.cloud Awake Processing 38 Progressing Progressing Unknown Unknown 83s ... gsicdc gcp 1.18.2 gcp gsicdc.myproject.shoot.devgarden.cloud Awake Succeeded 100 True True True False 6m7s Get access to your freshly baked cluster and set your KUBECONFIG:\n$ kgarden get secrets gsicdc.kubeconfig -o jsonpath={.data.kubeconfig} | base64 -d \u0026gt;kubeconfig-gsicdc.yaml $ export KUBECONFIG=$(pwd)/kubeconfig-gsicdc.yaml $ kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 100.64.0.1 \u0026lt;none\u0026gt; 443/TCP 89m Install Istio Please follow the Istio installation instructions and download istioctl. If you are on a Mac, I recommend\n$ brew install istioctl I want to install Istio with a default profile and SDS enabled. Furthermore I pass the following annotations to the service object istio-ingressgateway in the istio-system namespace.\nannotations:cert.gardener.cloud/issuer:mydomain-stagingcert.gardener.cloud/secretname:wildcard-tlsdns.gardener.cloud/class:gardendns.gardener.cloud/dnsnames:\u0026#34;*.gsicdc.mydomain.io\u0026#34;dns.gardener.cloud/ttl:\u0026#34;120\u0026#34;With these annotations three things now happen automagically:\n The External DNS Manager, provided to you as a service (dns.gardener.cloud/class: garden), picks up the request and creates the wildcard DNS entry *.gsicdc.mydomain.io with a time to live of 120sec at your DNS provider. My provider Cloud Flare is very very quick (as opposed to some other services). You should be able to verify the entry with dig lovemygardener.gsicdc.mydomain.io within seconds. The Certificate Mangement picks up the request as well and initates a DNS01 protocol exchange with Let\u0026rsquo;s Encrypt; using the staging environment referred to with the issuer behind mydomain-staging. After aproximately 70sec (give and take) you will receive the wildcard certificate in the wildcard-tls secret in the namespace istio-system.  Notice, that the namespace for the certificate secret is often the cause of many troubeshooting sessions: the secret must reside in the same namespace of the gateway.\n Here is the istio-install script:\n$ export domainname=\u0026#34;*.gsicdc.mydomain.io\u0026#34; $ export issuer=\u0026#34;mydomain-staging\u0026#34; $ cat \u0026lt;\u0026lt;EOF | istioctl install -y -f - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: profile: default components: ingressGateways: - name: istio-ingressgateway enabled: true k8s: serviceAnnotations: cert.gardener.cloud/issuer: \u0026#34;${issuer}\u0026#34; cert.gardener.cloud/secretname: wildcard-tls dns.gardener.cloud/class: garden dns.gardener.cloud/dnsnames: \u0026#34;${domainname}\u0026#34; dns.gardener.cloud/ttl: \u0026#34;120\u0026#34; EOF Verify that setup is working and that DNS and certificates have been created/delivered:\n$ kubectl -n istio-system describe service istio-ingressgateway \u0026lt;snip\u0026gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal EnsuringLoadBalancer 58s service-controller Ensuring load balancer Normal reconcile 58s cert-controller-manager created certificate object istio-system/istio-ingressgateway-service-pwqdm Normal cert-annotation 58s cert-controller-manager wildcard-tls: cert request is pending Normal cert-annotation 54s cert-controller-manager wildcard-tls: certificate pending: certificate requested, preparing/waiting for successful DNS01 challenge Normal cert-annotation 28s cert-controller-manager wildcard-tls: certificate ready Normal EnsuredLoadBalancer 26s service-controller Ensured load balancer Normal reconcile 26s dns-controller-manager created dns entry object shoot--core--gsicdc/istio-ingressgateway-service-p9qqb Normal dns-annotation 26s dns-controller-manager *.gsicdc.mydomain.io: dns entry is pending Normal dns-annotation 21s (x3 over 21s) dns-controller-manager *.gsicdc.mydomain.io: dns entry active $ dig lovemygardener.gsicdc.mydomain.io ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; lovemygardener.gsicdc.mydomain.io \u0026lt;snip\u0026gt; ;; ANSWER SECTION: lovemygardener.gsicdc.mydomain.io. 120 IN A\t35.195.120.62 \u0026lt;snip\u0026gt; There you have it, the wildcard-tls certificate is ready and the *.gsicdc.mydomain.io dns entry is active. Traffic will be going your way.\nHandy tools to install Another set of fine tools to use are k14s\u0026rsquo;s kapp, k9s and HTTPie. While we are at it, let\u0026rsquo;s install them all. If you are on a Mac, I recommend:\n$ brew tap k14s/tap $ brew install ytt kbld kapp kwt imgpkg vendir $ brew install derailed/k9s/k9s $ brew install httpie Ingress to your service Networking is a central part of Kubernetes, but it can be challenging to understand exactly how it is expected to work. You should learn about Kubernetes networking, and first try to debug problems yourself. With a solid managed cluster from Gardener, it is always PEBCAK!\n Kubernetes Ingress is a subject that is evolving to much broader standard. Please watch Evolving the Kubernetes Ingress APIs to GA and Beyond for a good introduction. In this example, I did not want to use the Kubernetes Ingress compatibility option of Istio. Instead, I used VirtualService and Gateway from the Istio\u0026rsquo;s API group networking.istio.io/v1beta1 directly, and enabled istio-injection generically for the namespace.\nI use httpbin as service that I want to expose to the internet, or where my ingress should be routed to (depends on your point of view, I guess).\napiVersion:v1kind:Namespacemetadata:name:productionlabels:istio-injection:enabled---apiVersion:v1kind:Servicemetadata:name:httpbinnamespace:productionlabels:app:httpbinspec:ports:- name:httpport:8000targetPort:80selector:app:httpbin---apiVersion:apps/v1kind:Deploymentmetadata:name:httpbinnamespace:productionspec:replicas:1selector:matchLabels:app:httpbintemplate:metadata:labels:app:httpbinspec:containers:- image:docker.io/kennethreitz/httpbinimagePullPolicy:IfNotPresentname:httpbinports:- containerPort:80---apiVersion:networking.istio.io/v1beta1kind:Gatewaymetadata:name:httpbin-gwnamespace:productionspec:selector:istio:ingressgateway#! use istio default ingress gatewayservers:- port:number:80name:httpprotocol:HTTPtls:httpsRedirect:truehosts:- \u0026#34;httpbin.gsicdc.mydomain.io\u0026#34;- port:number:443name:httpsprotocol:HTTPStls:mode:SIMPLEcredentialName:wildcard-tlshosts:- \u0026#34;httpbin.gsicdc.mydomain.io\u0026#34;---apiVersion:networking.istio.io/v1beta1kind:VirtualServicemetadata:name:httpbin-vsnamespace:productionspec:hosts:- \u0026#34;httpbin.gsicdc.mydomain.io\u0026#34;gateways:- httpbin-gwhttp:- match:- uri:regex:/.*route:- destination:port:number:8000host:httpbin---Let us now deploy the whole package of Kubernetes primitives using kapp:\n$ kapp deploy -a httpbin -f httpbin-kapp.yaml Target cluster \u0026#39;https://api.gsicdc.myproject.shoot.devgarden.cloud\u0026#39; (nodes: shoot--myproject--gsicdc-my-workerpool-z1-6586c8f6cb-x24kh) Changes Namespace Name Kind Conds. Age Op Wait to Rs Ri (cluster) production Namespace - - create reconcile - - production httpbin Deployment - - create reconcile - - ^ httpbin Service - - create reconcile - - ^ httpbin-gw Gateway - - create reconcile - - ^ httpbin-vs VirtualService - - create reconcile - - Op: 5 create, 0 delete, 0 update, 0 noop Wait to: 5 reconcile, 0 delete, 0 noop Continue? [yN]: y 5:36:31PM: ---- applying 1 changes [0/5 done] ---- \u0026lt;snip\u0026gt; 5:37:00PM: ok: reconcile deployment/httpbin (apps/v1) namespace: production 5:37:00PM: ---- applying complete [5/5 done] ---- 5:37:00PM: ---- waiting complete [5/5 done] ---- Succeeded Let\u0026rsquo;s finaly test the service (Of course you can use the browser as well):\n$ http httpbin.gsicdc.mydomain.io HTTP/1.1 301 Moved Permanently content-length: 0 date: Wed, 13 May 2020 21:29:13 GMT location: https://httpbin.gsicdc.mydomain.io/ server: istio-envoy $ curl -k https://httpbin.gsicdc.mydomain.io/ip { \u0026#34;origin\u0026#34;: \u0026#34;10.250.0.2\u0026#34; } Quod erat demonstrandum. The proof of exchanging the issuer is now left to the reader.\nRemember that the certificate is actually not valid because it is issued from the Let's encrypt staging environment. Thus, we needed \"curl -k\" or \"http --verify no\".\n Hint: use the interactive k9s tool. Cleanup Remove the cloud native application:\n$ kapp ls Apps in namespace \u0026#39;default\u0026#39; Name Namespaces Lcs Lca httpbin (cluster),production true 17m $ kapp delete -a httpbin ... Continue? [yN]: y ... 11:47:47PM: ---- waiting complete [8/8 done] ---- Succeeded Remove Istio:\n$ istioctl x uninstall --purge clusterrole.rbac.authorization.k8s.io \u0026#34;prometheus-istio-system\u0026#34; deleted clusterrolebinding.rbac.authorization.k8s.io \u0026#34;prometheus-istio-system\u0026#34; deleted ... Delete your Shoot:\nkgarden annotate shoot gsicdc confirmation.gardener.cloud/deletion=true --overwrite kgarden delete shoot gsicdc --wait=false "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/conventions/","title":"General conventions","tags":[],"description":"","content":"General conventions All the extensions that are registered to Gardener are deployed to the seed clusters (at the moment, every extension is installed to every seed cluster, however, in the future Gardener will be more smart to determine which extensions needs to be placed into which seed).\nSome of these extensions might need to create global resources in the seed (e.g., ClusterRoles), i.e., it\u0026rsquo;s important to have a naming scheme to avoid conflicts as it cannot be checked or validated upfront that two extensions don\u0026rsquo;t use the same names.\nConsequently, this page should help answering some general questions that might come up when it comes to developing an extension.\nIs there a naming scheme for (global) resources? As there is no formal process to validate non-existence of conflicts between two extensions please follow these naming schemes when creating resources (especially, when creating global resources, but it\u0026rsquo;s in general a good idea for most created resources):\nThe resource name should be prefixed with extensions.gardener.cloud:\u0026lt;extension-type\u0026gt;-\u0026lt;extension-name\u0026gt;:\u0026lt;resource-name\u0026gt;, for example:\n extensions.gardener.cloud:provider-aws:machine-controller-manager extensions.gardener.cloud:extension-certificate-service:cert-broker  How to create resources in the shoot cluster? Some extensions might not only create resources in the seed cluster itself but also in the shoot cluster. Usually, every extension comes with a ServiceAccount and the required RBAC permissions when it gets installed to the seed. However, there are no credentials for the shoot for every extension.\nGardener creates a kubeconfig for itself that it uses to interact with the shoot cluster. This kubeconfig is stored as a Secret with name gardener in the shoot namespace. Extension controllers may use this kubeconfig to interact with the shoot cluster if desired (it has full administrator privileges and no further RBAC rules are required). Instead, they could also create their own kubeconfig for every shoot (which, of course, is better for auditing reasons, but not yet enforced at this point in time).\nIf you need to deploy a non-DaemonSet resource you need to ensure that it only runs on nodes that are allowed to host system components and extensions. To do that you need to configure a nodeSelector as following:\nnodeSelector:worker.gardener.cloud/system-components:\u0026#34;true\u0026#34;How to create certificates/kubeconfigs for the shoot cluster? Gardener creates several certificate authorities (CA) that are used to create server/client certificates for various components. For example, the shoot\u0026rsquo;s etcd has its own CA, the kube-aggregator has its own CA as well, and both are different to the actual cluster\u0026rsquo;s CA.\nThese CAs are stored as Secrets in the shoot namespace (see this for the actual names). Extension controllers may use them to create further certificates/kubeconfigs for potential other components they need to deploy to the seed or shoot. These utility functions should help with the creation and management.\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/conventions/","title":"General conventions","tags":[],"description":"","content":"General conventions All the extensions that are registered to Gardener are deployed to the seed clusters (at the moment, every extension is installed to every seed cluster, however, in the future Gardener will be more smart to determine which extensions needs to be placed into which seed).\nSome of these extensions might need to create global resources in the seed (e.g., ClusterRoles), i.e., it\u0026rsquo;s important to have a naming scheme to avoid conflicts as it cannot be checked or validated upfront that two extensions don\u0026rsquo;t use the same names.\nConsequently, this page should help answering some general questions that might come up when it comes to developing an extension.\nIs there a naming scheme for (global) resources? As there is no formal process to validate non-existence of conflicts between two extensions please follow these naming schemes when creating resources (especially, when creating global resources, but it\u0026rsquo;s in general a good idea for most created resources):\nThe resource name should be prefixed with extensions.gardener.cloud:\u0026lt;extension-type\u0026gt;-\u0026lt;extension-name\u0026gt;:\u0026lt;resource-name\u0026gt;, for example:\n extensions.gardener.cloud:provider-aws:machine-controller-manager extensions.gardener.cloud:extension-certificate-service:cert-broker  How to create resources in the shoot cluster? Some extensions might not only create resources in the seed cluster itself but also in the shoot cluster. Usually, every extension comes with a ServiceAccount and the required RBAC permissions when it gets installed to the seed. However, there are no credentials for the shoot for every extension.\nGardener creates a kubeconfig for itself that it uses to interact with the shoot cluster. This kubeconfig is stored as a Secret with name gardener in the shoot namespace. Extension controllers may use this kubeconfig to interact with the shoot cluster if desired (it has full administrator privileges and no further RBAC rules are required). Instead, they could also create their own kubeconfig for every shoot (which, of course, is better for auditing reasons, but not yet enforced at this point in time).\nIf you need to deploy a non-DaemonSet resource you need to ensure that it only runs on nodes that are allowed to host system components and extensions. To do that you need to configure a nodeSelector as following:\nnodeSelector:worker.gardener.cloud/system-components:\u0026#34;true\u0026#34;How to create certificates/kubeconfigs for the shoot cluster? Gardener creates several certificate authorities (CA) that are used to create server/client certificates for various components. For example, the shoot\u0026rsquo;s etcd has its own CA, the kube-aggregator has its own CA as well, and both are different to the actual cluster\u0026rsquo;s CA.\nThese CAs are stored as Secrets in the shoot namespace (see this for the actual names). Extension controllers may use them to create further certificates/kubeconfigs for potential other components they need to deploy to the seed or shoot. These utility functions should help with the creation and management.\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/conventions/","title":"General conventions","tags":[],"description":"","content":"General conventions All the extensions that are registered to Gardener are deployed to the seed clusters (at the moment, every extension is installed to every seed cluster, however, in the future Gardener will be more smart to determine which extensions needs to be placed into which seed).\nSome of these extensions might need to create global resources in the seed (e.g., ClusterRoles), i.e., it\u0026rsquo;s important to have a naming scheme to avoid conflicts as it cannot be checked or validated upfront that two extensions don\u0026rsquo;t use the same names.\nConsequently, this page should help answering some general questions that might come up when it comes to developing an extension.\nIs there a naming scheme for (global) resources? As there is no formal process to validate non-existence of conflicts between two extensions please follow these naming schemes when creating resources (especially, when creating global resources, but it\u0026rsquo;s in general a good idea for most created resources):\nThe resource name should be prefixed with extensions.gardener.cloud:\u0026lt;extension-type\u0026gt;-\u0026lt;extension-name\u0026gt;:\u0026lt;resource-name\u0026gt;, for example:\n extensions.gardener.cloud:provider-aws:machine-controller-manager extensions.gardener.cloud:extension-certificate-service:cert-broker  How to create resources in the shoot cluster? Some extensions might not only create resources in the seed cluster itself but also in the shoot cluster. Usually, every extension comes with a ServiceAccount and the required RBAC permissions when it gets installed to the seed. However, there are no credentials for the shoot for every extension.\nGardener creates a kubeconfig for itself that it uses to interact with the shoot cluster. This kubeconfig is stored as a Secret with name gardener in the shoot namespace. Extension controllers may use this kubeconfig to interact with the shoot cluster if desired (it has full administrator privileges and no further RBAC rules are required). Instead, they could also create their own kubeconfig for every shoot (which, of course, is better for auditing reasons, but not yet enforced at this point in time).\nIf you need to deploy a non-DaemonSet resource you need to ensure that it only runs on nodes that are allowed to host system components and extensions. To do that you need to configure a nodeSelector as following:\nnodeSelector:worker.gardener.cloud/system-components:\u0026#34;true\u0026#34;How to create certificates/kubeconfigs for the shoot cluster? Gardener creates several certificate authorities (CA) that are used to create server/client certificates for various components. For example, the shoot\u0026rsquo;s etcd has its own CA, the kube-aggregator has its own CA as well, and both are different to the actual cluster\u0026rsquo;s CA.\nThese CAs are stored as Secrets in the shoot namespace (see this for the actual names). Extension controllers may use them to create further certificates/kubeconfigs for potential other components they need to deploy to the seed or shoot. These utility functions should help with the creation and management.\n"},{"uri":"https://gardener.cloud/documentation/guides/monitoring_and_troubleshooting/shell-to-node/","title":"Get a Shell to a Gardener Shoot Worker Node","tags":[],"description":"Describes the methods for getting shell access to worker nodes.","content":"Get a Shell to a Kubernetes Node To troubleshoot certain problems in a Kubernetes cluster, operators need access to the host of the Kubernetes node to troubleshoot problems. This can be required if a node misbehaves or fails to join the cluster in the first place.\nWith access to the host, it is for instance possible to check the kubelet logs and interact with common tools such as systemctland journalctl.\nThe first section of this guide explores options to get a shell to the node of a Gardener Kubernetes cluster. The options described in the second section do not rely on Kubernetes capabilities to get shell access to a node and thus can also be used if an instance failed to join the cluster.\nThis guide only covers how to get access to the host, but does not cover troubleshooting methods.\n Get a Shell to a Kubernetes Node Get a Shell to an operational cluster node  Gardener Dashboard gardenctl shell Gardener Ops Toolbelt Custom root pod   SSH access to a node that failed to join the cluster  Identifying the problematic instance gardenctl ssh SSH with manually created Bastion on AWS  Create the Bastion Security Group Create the bastion instance   Connecting to the target instance Cleanup    Get a Shell to an operational cluster node The following describes four different approaches to get a shell to an operational Shoot worker node. As a prerequisite to troubleshooting a Kubernetes node, the node must have joined the cluster successfully and be able to run a pod. All of the described approaches involve scheduling a pod with root permissions and mounting the root filesystem.\nGardener Dashboard Prerequisite: the terminal feature is configured for the Gardener dashboard.\nNavigate to the cluster overview page and find the Terminal in the Access tile.\nSelect the target Cluster (Garden, Seed / Control Plane, Shoot cluster) depending on the requirements and access rights (only certain users have access to the Seed Control Plane).\nTo open the terminal configuration, click on the top right-hand corner of the screen.\nSet the Terminal Runtime to \u0026ldquo;Privileged. Also specify the target node from the drop-down menu.\nThe dashboard then schedules a pod and opens a shell session to the node.\nTo get access to common binaries installed on the host, prefix the command with chroot /hostroot. Note that the path depends on where the root path is mounted in the container. In the default image used by the Dashboard, it is under /hostroot.\ngardenctl shell Prerequisite: kubectl and gardenctl are available and configured.\nFirst, target a Garden cluster containing all the Shoot definitions.\n$ gardenctl target garden \u0026lt;target-garden\u0026gt; Target an available Shoot by name. This sets up the context and configures the kubeconfig file of the Shoot cluster. Subsequent commands will execute in this context.\n$ gardenctl target shoot \u0026lt;target-shoot\u0026gt; Get the nodes of the Shoot cluster.\n$ gardenctl kubectl get nodes Pick a node name from the list above and get a root shell access to it.\n$ gardenctl shell \u0026lt;target-node\u0026gt; Gardener Ops Toolbelt Prerequisite: kubectl is available.\nThe Gardener ops-toolbelt can be used as a convenient way to deploy a root pod to a node. The pod uses an image that is bundled with a bunch of useful troubleshooting tools. This is also the same image that is used by default when using the Gardener Dashboard terminal feature as described in the previous section.\nThe easiest way to use the Gardener ops-toolbelt is to execute the ops-pod script in the hacks folder. To get root shell access to a node, execute the aforementioned script by supplying the target node name as an argument:\n$ \u0026lt;path-to-ops-toolbelt-repo\u0026gt;/hacks/ops-pod \u0026lt;target-node\u0026gt; Custom root pod Alternatively, a pod can be assigned to a target node and a shell can be opened via standard Kubernetes means. To enable root access to the node, the pod specification requires proper securityContext and volume properties.\nFor instance you can use the following pod manifest, after changing  with the name of the node you want this pod attached to:\napiVersion:v1kind:Podmetadata:name:privileged-podnamespace:defaultspec:nodeSelector:kubernetes.io/hostname:\u0026lt;target-node-name\u0026gt; containers:- name:busyboximage:busyboxstdin:truesecurityContext:privileged:truevolumeMounts:- name:host-root-volumemountPath:/hostreadOnly:truevolumes:- name:host-root-volumehostPath:path:/hostNetwork:truehostPID:truerestartPolicy:NeverSSH access to a node that failed to join the cluster This section explores two options that can be used to get SSH access to a node that failed to join the cluster. As it is not possible to schedule a pod on the node, the Kubernetes-based methods explored so far cannot be used in this scenario.\nAdditionally, Gardener typically provisions worker instances in a private subnet of the VPC, hence - there is no public IP address that could be used for direct SSH access.\nFor this scenario, cloud providers typically have extensive documentation (e.g AWS \u0026amp; GCP and in some cases tooling support). However, these approaches are mostly cloud provider specific, require interaction via their CLI and API or sometimes the installation of a cloud provider specific agent one the node.\nAlternatively, gardenctl can be used providing a cloud provider agnostic and out-of-the-box support to get ssh access to an instance in a private subnet. Currently gardenctl supports AWS, GCP, Openstack, Azure and Alibaba Cloud.\nIdentifying the problematic instance First, the problematic instance has to be identified. In Gardener, worker pools can be created in different cloud provider regions, zones and accounts.\nThe instance would typically show up as successfully started / running in the cloud provider dashboard or API and it is not immediately obvious which one has a problem. Instead, we can use the Gardener API / CRDs to obtain the faulty instance identifier in a cloud-agnostic way.\nGardener uses the Machine Controller Manager to create the Shoot worker nodes. For each worker node, the Machine Controller Manager creates a Machine CRD in the Shoot namespace in the respective Seed cluster. Usually the problematic instance can be identified as the respective Machine CRD has status pending.\nThe instance / node name can be obtained from the Machine .status field:\n$ kubectl get machine \u0026lt;machine-name\u0026gt; -o json | jq -r .status.node This is all the information needed to go ahead and use gardenctl ssh to get a shell to the node. In addition, the used cloud provider, the specific identifier of the instance and the instance region can be identified from the Machine CRD.\nGet the identifier of the instance via:\n$ kubectl get machine \u0026lt;machine-name\u0026gt; -o json | jq -r .spec.providerID // e.g aws:///eu-north-1/i-069733c435bdb4640 The identifier shows that the instance belongs to the cloud provider aws with the ec2 instance-id i-069733c435bdb4640 in region eu-north-1.\nTo get more information about the instance, check out the MachineClass (e.g AWSMachineClass) that is associated with each Machine CRD in the Shoot namespace of the Seed cluster. The AWSMachineClass contains the machine image (ami), machine-type, iam information, network-interfaces, subnets, security groups and attached volumes.\nOf course, the information can also be used to get the instance with the cloud provider CLI / API.\ngardenctl ssh Using the node name of the problematic instance, we can use the gardenctl ssh command to get SSH access to the cloud provider instance via an automatically set up bastion host. gardenctl takes care of spinning up the bastion instance, setting up the SSH keys, ports and security groups and opens a root shell on the target instance. After the SSH session has ended, gardenctl deletes the created cloud provider resources.\nUse the following commands:\nFirst, target a Garden cluster containing all the Shoot definitions.\n$ gardenctl target garden \u0026lt;target-garden\u0026gt; Target an available Shoot by name. This sets up the context, configures the kubeconfig file of the Shoot cluster and downloads the cloud provider credentials. Subsequent commands will execute in this context.\n$ gardenctl target shoot \u0026lt;target-shoot\u0026gt; This uses the cloud provider credentials to spin up the bastion and to open a shell on the target instance.\n$ gardenctl ssh \u0026lt;target-node\u0026gt; SSH with manually created Bastion on AWS In case you are not using gardenctl or want to control the bastion instance yourself, you can also manually set it up. The steps described here are generally the same as those used by gardenctl internally. Despite some cloud provider specifics they can be generalized to the following list:\n Open port 22 on the target instance. Create an instance / VM in a public subnet (bastion instance needs to have public ip address). Set-up security groups, roles and open port 22 for the bastion instance.  The following diagram shows an overview how the SSH access to the target instance works:\nThis guide demonstrates the setup of a bastion on AWS.\nPrerequisites:\n The AWS CLI is set up. Obtain target instance-id (see here). Obtain the VPC ID the Shoot resources are created in. This can be found in the Infrastructure CRD in the Shoot namespace in the Seed. Make sure that port 22 on the target instance is open (default for Gardener deployed instances).  Extract security group via  $ aws ec2 describe-instances --instance-ids \u0026lt;instance-id\u0026gt;  Check for rule that allows inbound connections on port 22:  $ aws ec2 describe-security-groups --group-ids=\u0026lt;security-group-id\u0026gt;  If not available, create the rule with the following comamnd:  $ aws ec2 authorize-security-group-ingress --group-id \u0026lt;security-group-id\u0026gt; --protocol tcp --port 22 --cidr 0.0.0.0/0   Create the Bastion Security Group   The common name of the security group is \u0026lt;shoot-name\u0026gt;-bsg. Create the security group:\n$ aws ec2 create-security-group --group-name \u0026lt;bastion-security-group-name\u0026gt; --description ssh-access --vpc-id \u0026lt;VPC-ID\u0026gt;   Optionally, create identifying tags for the security group:\n$ aws ec2 create-tags --resources \u0026lt;bastion-security-group-id\u0026gt; --tags Key=component,Value=\u0026lt;tag\u0026gt;   Create permission in the bastion security group that allows ssh access on port 22.\n$ aws ec2 authorize-security-group-ingress --group-id \u0026lt;bastion-security-group-id\u0026gt; --protocol tcp --port 22 --cidr 0.0.0.0/0   Create an IAM role for the bastion instance with the name \u0026lt;shoot-name\u0026gt;-bastions:\n$ aws iam create-role --role-name \u0026lt;shoot-name\u0026gt;-bastions The content should be:\n  { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeRegions\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] }   Create the instance profile with name \u0026lt;shoot-name\u0026gt;-bastions:\n$ aws iam create-instance-profile --instance-profile-name \u0026lt;name\u0026gt;   Add the created role to the instance profile:\n$ aws iam add-role-to-instance-profile --instance-profile-name \u0026lt;instance-profile-name\u0026gt; --role-name \u0026lt;role-name\u0026gt;   Create the bastion instance Next, in order to be able to ssh into the bastion instance, the instance has to be set up with a user with a public ssh key. Create a user gardener that has the same Gardener-generated public ssh key as the target instance.\n  First, we need to get the public part of the Shoot ssh-key. The ssh-key is stored in a secret in the the project namespace in the Garden cluster. The name is: \u0026lt;shoot-name\u0026gt;-ssh-publickey. Get the key via:\n$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\\\u0026#34;id_rsa.pub\\\u0026#34;   A script handed over as user-data to the bastion ec2 instance, can be used to create the gardener user and add the ssh-key. For your convenience, you can use the following script to generate the user-data.\n  #!/bin/bash -eu saveUserDataFile () { ssh_key=$1 cat \u0026gt; gardener-bastion-userdata.sh \u0026lt;\u0026lt;EOF #!/bin/bash -eu id gardener || useradd gardener -mU mkdir -p /home/gardener/.ssh echo \u0026#34;$ssh_key\u0026#34; \u0026gt; /home/gardener/.ssh/authorized_keys chown gardener:gardener /home/gardener/.ssh/authorized_keys echo \u0026#34;gardener ALL=(ALL) NOPASSWD:ALL\u0026#34; \u0026gt;/etc/sudoers.d/99-gardener-user EOF } if [ -p /dev/stdin ]; then read -r input cat | saveUserDataFile \u0026#34;$input\u0026#34; else pbpaste | saveUserDataFile \u0026#34;$input\u0026#34; fi   Use the script by handing-over the public ssh-key of the Shoot cluster:\n$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\\\u0026#34;id_rsa.pub\\\u0026#34; | ./generate-userdata.sh This generates a file called gardener-bastion-userdata.sh in the same directory containing the user-data.\n  The following information is needed to create the bastion instance:\nbastion-IAM-instance-profile-name\n Use the created instance profile with name \u0026lt;shoot-name\u0026gt;-bastions  image-id\n Possible use the same image-id as for the target instance (or any other image). Has cloud provider specific format (AWS: ami).  ssh-public-key-name\n This is the ssh key pair already created in the Shoot\u0026rsquo;s cloud provider account by Gardener during the Infrastructure CRD reconciliation. The name is usually: \u0026lt;shoot-name\u0026gt;-ssh-publickey  subnet-id\n Choose a subnet that is attached to an Internet Gateway and NAT Gateway (bastion instance must have a public IP). The Gardener created public subnet with the name \u0026lt;shoot-name\u0026gt;-public-utility-\u0026lt;xy\u0026gt; can be used. Please check the created subnets with the cloud provider.  bastion-security-group-id\n Use the id of the created bastion security group.  file-path-to-userdata\n  Use the filepath to user-data file generated in the previous step.\n  bastion-instance-name\n Optional to tag the instance. Usually \u0026lt;shoot-name\u0026gt;-bastions      Create the bastion instance via:\n  $ ec2 run-instances --iam-instance-profile Name=\u0026lt;bastion-IAM-instance-profile-name\u0026gt; --image-id \u0026lt;image-id\u0026gt; --count 1 --instance-type t3.nano --key-name \u0026lt;ssh-public-key-name\u0026gt; --security-group-ids \u0026lt;bastion-security-group-id\u0026gt; --subnet-id \u0026lt;subnet-id\u0026gt; --associate-public-ip-address --user-data \u0026lt;file-path-to-userdata\u0026gt; --tag-specifications ResourceType=instance,Tags=[{Key=Name,Value=\u0026lt;bastion-instance-name\u0026gt;},{Key=component,Value=\u0026lt;mytag\u0026gt;}] ResourceType=volume,Tags=[{Key=component,Value=\u0026lt;mytag\u0026gt;}]\u0026#34; Capture the instance-id from the reponse and wait until the ec2 instance is running and has a public ip address.\nConnecting to the target instance Save the private key of the ssh-key-pair in a temporary local file for later use.\n$ umask 077 $ kubectl get secret \u0026lt;shoot-name\u0026gt;.ssh-keypair -o json | jq -r .data.\\\u0026#34;id_rsa\\\u0026#34; | base64 -d \u0026gt; id_rsa.key Use the private ssh key to ssh into the bastion instance.\n$ ssh-i \u0026lt;path-to-private-key\u0026gt; gardener@\u0026lt;public-bastion-instance-ip\u0026gt;If that works, connect from your local terminal to the target instance via the bastion.\n$ ssh-i \u0026lt;path-to-private-key\u0026gt;-o ProxyCommand=\u0026#34;ssh -W %h:%p -i \u0026lt;private-key\u0026gt; -o IdentitiesOnly=yes -o StrictHostKeyChecking=no gardener@\u0026lt;public-ip-bastion\u0026gt;\u0026#34; gardener@\u0026lt;private-ip-target-instance\u0026gt; -o IdentitiesOnly=yes -o StrictHostKeyChecking=no Cleanup Do not forget to cleanup the created resources. Otherwise Gardener will eventually fail to delete the Shoot.\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/monitoring_and_troubleshooting/shell-to-node/","title":"Get a Shell to a Gardener Shoot Worker Node","tags":[],"description":"Describes the methods for getting shell access to worker nodes.","content":"Get a Shell to a Kubernetes Node To troubleshoot certain problems in a Kubernetes cluster, operators need access to the host of the Kubernetes node to troubleshoot problems. This can be required if a node misbehaves or fails to join the cluster in the first place.\nWith access to the host, it is for instance possible to check the kubelet logs and interact with common tools such as systemctland journalctl.\nThe first section of this guide explores options to get a shell to the node of a Gardener Kubernetes cluster. The options described in the second section do not rely on Kubernetes capabilities to get shell access to a node and thus can also be used if an instance failed to join the cluster.\nThis guide only covers how to get access to the host, but does not cover troubleshooting methods.\n Get a Shell to a Kubernetes Node Get a Shell to an operational cluster node  Gardener Dashboard gardenctl shell Gardener Ops Toolbelt Custom root pod   SSH access to a node that failed to join the cluster  Identifying the problematic instance gardenctl ssh SSH with manually created Bastion on AWS  Create the Bastion Security Group Create the bastion instance   Connecting to the target instance Cleanup    Get a Shell to an operational cluster node The following describes four different approaches to get a shell to an operational Shoot worker node. As a prerequisite to troubleshooting a Kubernetes node, the node must have joined the cluster successfully and be able to run a pod. All of the described approaches involve scheduling a pod with root permissions and mounting the root filesystem.\nGardener Dashboard Prerequisite: the terminal feature is configured for the Gardener dashboard.\nNavigate to the cluster overview page and find the Terminal in the Access tile.\nSelect the target Cluster (Garden, Seed / Control Plane, Shoot cluster) depending on the requirements and access rights (only certain users have access to the Seed Control Plane).\nTo open the terminal configuration, click on the top right-hand corner of the screen.\nSet the Terminal Runtime to \u0026ldquo;Privileged. Also specify the target node from the drop-down menu.\nThe dashboard then schedules a pod and opens a shell session to the node.\nTo get access to common binaries installed on the host, prefix the command with chroot /hostroot. Note that the path depends on where the root path is mounted in the container. In the default image used by the Dashboard, it is under /hostroot.\ngardenctl shell Prerequisite: kubectl and gardenctl are available and configured.\nFirst, target a Garden cluster containing all the Shoot definitions.\n$ gardenctl target garden \u0026lt;target-garden\u0026gt; Target an available Shoot by name. This sets up the context and configures the kubeconfig file of the Shoot cluster. Subsequent commands will execute in this context.\n$ gardenctl target shoot \u0026lt;target-shoot\u0026gt; Get the nodes of the Shoot cluster.\n$ gardenctl kubectl get nodes Pick a node name from the list above and get a root shell access to it.\n$ gardenctl shell \u0026lt;target-node\u0026gt; Gardener Ops Toolbelt Prerequisite: kubectl is available.\nThe Gardener ops-toolbelt can be used as a convenient way to deploy a root pod to a node. The pod uses an image that is bundled with a bunch of useful troubleshooting tools. This is also the same image that is used by default when using the Gardener Dashboard terminal feature as described in the previous section.\nThe easiest way to use the Gardener ops-toolbelt is to execute the ops-pod script in the hacks folder. To get root shell access to a node, execute the aforementioned script by supplying the target node name as an argument:\n$ \u0026lt;path-to-ops-toolbelt-repo\u0026gt;/hacks/ops-pod \u0026lt;target-node\u0026gt; Custom root pod Alternatively, a pod can be assigned to a target node and a shell can be opened via standard Kubernetes means. To enable root access to the node, the pod specification requires proper securityContext and volume properties.\nFor instance you can use the following pod manifest, after changing  with the name of the node you want this pod attached to:\napiVersion:v1kind:Podmetadata:name:privileged-podnamespace:defaultspec:nodeSelector:kubernetes.io/hostname:\u0026lt;target-node-name\u0026gt; containers:- name:busyboximage:busyboxstdin:truesecurityContext:privileged:truevolumeMounts:- name:host-root-volumemountPath:/hostreadOnly:truevolumes:- name:host-root-volumehostPath:path:/hostNetwork:truehostPID:truerestartPolicy:NeverSSH access to a node that failed to join the cluster This section explores two options that can be used to get SSH access to a node that failed to join the cluster. As it is not possible to schedule a pod on the node, the Kubernetes-based methods explored so far cannot be used in this scenario.\nAdditionally, Gardener typically provisions worker instances in a private subnet of the VPC, hence - there is no public IP address that could be used for direct SSH access.\nFor this scenario, cloud providers typically have extensive documentation (e.g AWS \u0026amp; GCP and in some cases tooling support). However, these approaches are mostly cloud provider specific, require interaction via their CLI and API or sometimes the installation of a cloud provider specific agent one the node.\nAlternatively, gardenctl can be used providing a cloud provider agnostic and out-of-the-box support to get ssh access to an instance in a private subnet. Currently gardenctl supports AWS, GCP, Openstack, Azure and Alibaba Cloud.\nIdentifying the problematic instance First, the problematic instance has to be identified. In Gardener, worker pools can be created in different cloud provider regions, zones and accounts.\nThe instance would typically show up as successfully started / running in the cloud provider dashboard or API and it is not immediately obvious which one has a problem. Instead, we can use the Gardener API / CRDs to obtain the faulty instance identifier in a cloud-agnostic way.\nGardener uses the Machine Controller Manager to create the Shoot worker nodes. For each worker node, the Machine Controller Manager creates a Machine CRD in the Shoot namespace in the respective Seed cluster. Usually the problematic instance can be identified as the respective Machine CRD has status pending.\nThe instance / node name can be obtained from the Machine .status field:\n$ kubectl get machine \u0026lt;machine-name\u0026gt; -o json | jq -r .status.node This is all the information needed to go ahead and use gardenctl ssh to get a shell to the node. In addition, the used cloud provider, the specific identifier of the instance and the instance region can be identified from the Machine CRD.\nGet the identifier of the instance via:\n$ kubectl get machine \u0026lt;machine-name\u0026gt; -o json | jq -r .spec.providerID // e.g aws:///eu-north-1/i-069733c435bdb4640 The identifier shows that the instance belongs to the cloud provider aws with the ec2 instance-id i-069733c435bdb4640 in region eu-north-1.\nTo get more information about the instance, check out the MachineClass (e.g AWSMachineClass) that is associated with each Machine CRD in the Shoot namespace of the Seed cluster. The AWSMachineClass contains the machine image (ami), machine-type, iam information, network-interfaces, subnets, security groups and attached volumes.\nOf course, the information can also be used to get the instance with the cloud provider CLI / API.\ngardenctl ssh Using the node name of the problematic instance, we can use the gardenctl ssh command to get SSH access to the cloud provider instance via an automatically set up bastion host. gardenctl takes care of spinning up the bastion instance, setting up the SSH keys, ports and security groups and opens a root shell on the target instance. After the SSH session has ended, gardenctl deletes the created cloud provider resources.\nUse the following commands:\nFirst, target a Garden cluster containing all the Shoot definitions.\n$ gardenctl target garden \u0026lt;target-garden\u0026gt; Target an available Shoot by name. This sets up the context, configures the kubeconfig file of the Shoot cluster and downloads the cloud provider credentials. Subsequent commands will execute in this context.\n$ gardenctl target shoot \u0026lt;target-shoot\u0026gt; This uses the cloud provider credentials to spin up the bastion and to open a shell on the target instance.\n$ gardenctl ssh \u0026lt;target-node\u0026gt; SSH with manually created Bastion on AWS In case you are not using gardenctl or want to control the bastion instance yourself, you can also manually set it up. The steps described here are generally the same as those used by gardenctl internally. Despite some cloud provider specifics they can be generalized to the following list:\n Open port 22 on the target instance. Create an instance / VM in a public subnet (bastion instance needs to have public ip address). Set-up security groups, roles and open port 22 for the bastion instance.  The following diagram shows an overview how the SSH access to the target instance works:\nThis guide demonstrates the setup of a bastion on AWS.\nPrerequisites:\n The AWS CLI is set up. Obtain target instance-id (see here). Obtain the VPC ID the Shoot resources are created in. This can be found in the Infrastructure CRD in the Shoot namespace in the Seed. Make sure that port 22 on the target instance is open (default for Gardener deployed instances).  Extract security group via  $ aws ec2 describe-instances --instance-ids \u0026lt;instance-id\u0026gt;  Check for rule that allows inbound connections on port 22:  $ aws ec2 describe-security-groups --group-ids=\u0026lt;security-group-id\u0026gt;  If not available, create the rule with the following comamnd:  $ aws ec2 authorize-security-group-ingress --group-id \u0026lt;security-group-id\u0026gt; --protocol tcp --port 22 --cidr 0.0.0.0/0   Create the Bastion Security Group   The common name of the security group is \u0026lt;shoot-name\u0026gt;-bsg. Create the security group:\n$ aws ec2 create-security-group --group-name \u0026lt;bastion-security-group-name\u0026gt; --description ssh-access --vpc-id \u0026lt;VPC-ID\u0026gt;   Optionally, create identifying tags for the security group:\n$ aws ec2 create-tags --resources \u0026lt;bastion-security-group-id\u0026gt; --tags Key=component,Value=\u0026lt;tag\u0026gt;   Create permission in the bastion security group that allows ssh access on port 22.\n$ aws ec2 authorize-security-group-ingress --group-id \u0026lt;bastion-security-group-id\u0026gt; --protocol tcp --port 22 --cidr 0.0.0.0/0   Create an IAM role for the bastion instance with the name \u0026lt;shoot-name\u0026gt;-bastions:\n$ aws iam create-role --role-name \u0026lt;shoot-name\u0026gt;-bastions The content should be:\n  { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeRegions\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] }   Create the instance profile with name \u0026lt;shoot-name\u0026gt;-bastions:\n$ aws iam create-instance-profile --instance-profile-name \u0026lt;name\u0026gt;   Add the created role to the instance profile:\n$ aws iam add-role-to-instance-profile --instance-profile-name \u0026lt;instance-profile-name\u0026gt; --role-name \u0026lt;role-name\u0026gt;   Create the bastion instance Next, in order to be able to ssh into the bastion instance, the instance has to be set up with a user with a public ssh key. Create a user gardener that has the same Gardener-generated public ssh key as the target instance.\n  First, we need to get the public part of the Shoot ssh-key. The ssh-key is stored in a secret in the the project namespace in the Garden cluster. The name is: \u0026lt;shoot-name\u0026gt;-ssh-publickey. Get the key via:\n$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\\\u0026#34;id_rsa.pub\\\u0026#34;   A script handed over as user-data to the bastion ec2 instance, can be used to create the gardener user and add the ssh-key. For your convenience, you can use the following script to generate the user-data.\n  #!/bin/bash -eu saveUserDataFile () { ssh_key=$1 cat \u0026gt; gardener-bastion-userdata.sh \u0026lt;\u0026lt;EOF #!/bin/bash -eu id gardener || useradd gardener -mU mkdir -p /home/gardener/.ssh echo \u0026#34;$ssh_key\u0026#34; \u0026gt; /home/gardener/.ssh/authorized_keys chown gardener:gardener /home/gardener/.ssh/authorized_keys echo \u0026#34;gardener ALL=(ALL) NOPASSWD:ALL\u0026#34; \u0026gt;/etc/sudoers.d/99-gardener-user EOF } if [ -p /dev/stdin ]; then read -r input cat | saveUserDataFile \u0026#34;$input\u0026#34; else pbpaste | saveUserDataFile \u0026#34;$input\u0026#34; fi   Use the script by handing-over the public ssh-key of the Shoot cluster:\n$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\\\u0026#34;id_rsa.pub\\\u0026#34; | ./generate-userdata.sh This generates a file called gardener-bastion-userdata.sh in the same directory containing the user-data.\n  The following information is needed to create the bastion instance:\nbastion-IAM-instance-profile-name\n Use the created instance profile with name \u0026lt;shoot-name\u0026gt;-bastions  image-id\n Possible use the same image-id as for the target instance (or any other image). Has cloud provider specific format (AWS: ami).  ssh-public-key-name\n This is the ssh key pair already created in the Shoot\u0026rsquo;s cloud provider account by Gardener during the Infrastructure CRD reconciliation. The name is usually: \u0026lt;shoot-name\u0026gt;-ssh-publickey  subnet-id\n Choose a subnet that is attached to an Internet Gateway and NAT Gateway (bastion instance must have a public IP). The Gardener created public subnet with the name \u0026lt;shoot-name\u0026gt;-public-utility-\u0026lt;xy\u0026gt; can be used. Please check the created subnets with the cloud provider.  bastion-security-group-id\n Use the id of the created bastion security group.  file-path-to-userdata\n  Use the filepath to user-data file generated in the previous step.\n  bastion-instance-name\n Optional to tag the instance. Usually \u0026lt;shoot-name\u0026gt;-bastions      Create the bastion instance via:\n  $ ec2 run-instances --iam-instance-profile Name=\u0026lt;bastion-IAM-instance-profile-name\u0026gt; --image-id \u0026lt;image-id\u0026gt; --count 1 --instance-type t3.nano --key-name \u0026lt;ssh-public-key-name\u0026gt; --security-group-ids \u0026lt;bastion-security-group-id\u0026gt; --subnet-id \u0026lt;subnet-id\u0026gt; --associate-public-ip-address --user-data \u0026lt;file-path-to-userdata\u0026gt; --tag-specifications ResourceType=instance,Tags=[{Key=Name,Value=\u0026lt;bastion-instance-name\u0026gt;},{Key=component,Value=\u0026lt;mytag\u0026gt;}] ResourceType=volume,Tags=[{Key=component,Value=\u0026lt;mytag\u0026gt;}]\u0026#34; Capture the instance-id from the reponse and wait until the ec2 instance is running and has a public ip address.\nConnecting to the target instance Save the private key of the ssh-key-pair in a temporary local file for later use.\n$ umask 077 $ kubectl get secret \u0026lt;shoot-name\u0026gt;.ssh-keypair -o json | jq -r .data.\\\u0026#34;id_rsa\\\u0026#34; | base64 -d \u0026gt; id_rsa.key Use the private ssh key to ssh into the bastion instance.\n$ ssh-i \u0026lt;path-to-private-key\u0026gt; gardener@\u0026lt;public-bastion-instance-ip\u0026gt;If that works, connect from your local terminal to the target instance via the bastion.\n$ ssh-i \u0026lt;path-to-private-key\u0026gt;-o ProxyCommand=\u0026#34;ssh -W %h:%p -i \u0026lt;private-key\u0026gt; -o IdentitiesOnly=yes -o StrictHostKeyChecking=no gardener@\u0026lt;public-ip-bastion\u0026gt;\u0026#34; gardener@\u0026lt;private-ip-target-instance\u0026gt; -o IdentitiesOnly=yes -o StrictHostKeyChecking=no Cleanup Do not forget to cleanup the created resources. Otherwise Gardener will eventually fail to delete the Shoot.\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/monitoring_and_troubleshooting/shell-to-node/","title":"Get a Shell to a Gardener Shoot Worker Node","tags":[],"description":"Describes the methods for getting shell access to worker nodes.","content":"Get a Shell to a Kubernetes Node To troubleshoot certain problems in a Kubernetes cluster, operators need access to the host of the Kubernetes node to troubleshoot problems. This can be required if a node misbehaves or fails to join the cluster in the first place.\nWith access to the host, it is for instance possible to check the kubelet logs and interact with common tools such as systemctland journalctl.\nThe first section of this guide explores options to get a shell to the node of a Gardener Kubernetes cluster. The options described in the second section do not rely on Kubernetes capabilities to get shell access to a node and thus can also be used if an instance failed to join the cluster.\nThis guide only covers how to get access to the host, but does not cover troubleshooting methods.\n Get a Shell to a Kubernetes Node Get a Shell to an operational cluster node  Gardener Dashboard gardenctl shell Gardener Ops Toolbelt Custom root pod   SSH access to a node that failed to join the cluster  Identifying the problematic instance gardenctl ssh SSH with manually created Bastion on AWS  Create the Bastion Security Group Create the bastion instance   Connecting to the target instance Cleanup    Get a Shell to an operational cluster node The following describes four different approaches to get a shell to an operational Shoot worker node. As a prerequisite to troubleshooting a Kubernetes node, the node must have joined the cluster successfully and be able to run a pod. All of the described approaches involve scheduling a pod with root permissions and mounting the root filesystem.\nGardener Dashboard Prerequisite: the terminal feature is configured for the Gardener dashboard.\nNavigate to the cluster overview page and find the Terminal in the Access tile.\nSelect the target Cluster (Garden, Seed / Control Plane, Shoot cluster) depending on the requirements and access rights (only certain users have access to the Seed Control Plane).\nTo open the terminal configuration, click on the top right-hand corner of the screen.\nSet the Terminal Runtime to \u0026ldquo;Privileged. Also specify the target node from the drop-down menu.\nThe dashboard then schedules a pod and opens a shell session to the node.\nTo get access to common binaries installed on the host, prefix the command with chroot /hostroot. Note that the path depends on where the root path is mounted in the container. In the default image used by the Dashboard, it is under /hostroot.\ngardenctl shell Prerequisite: kubectl and gardenctl are available and configured.\nFirst, target a Garden cluster containing all the Shoot definitions.\n$ gardenctl target garden \u0026lt;target-garden\u0026gt; Target an available Shoot by name. This sets up the context and configures the kubeconfig file of the Shoot cluster. Subsequent commands will execute in this context.\n$ gardenctl target shoot \u0026lt;target-shoot\u0026gt; Get the nodes of the Shoot cluster.\n$ gardenctl kubectl get nodes Pick a node name from the list above and get a root shell access to it.\n$ gardenctl shell \u0026lt;target-node\u0026gt; Gardener Ops Toolbelt Prerequisite: kubectl is available.\nThe Gardener ops-toolbelt can be used as a convenient way to deploy a root pod to a node. The pod uses an image that is bundled with a bunch of useful troubleshooting tools. This is also the same image that is used by default when using the Gardener Dashboard terminal feature as described in the previous section.\nThe easiest way to use the Gardener ops-toolbelt is to execute the ops-pod script in the hacks folder. To get root shell access to a node, execute the aforementioned script by supplying the target node name as an argument:\n$ \u0026lt;path-to-ops-toolbelt-repo\u0026gt;/hacks/ops-pod \u0026lt;target-node\u0026gt; Custom root pod Alternatively, a pod can be assigned to a target node and a shell can be opened via standard Kubernetes means. To enable root access to the node, the pod specification requires proper securityContext and volume properties.\nFor instance you can use the following pod manifest, after changing  with the name of the node you want this pod attached to:\napiVersion:v1kind:Podmetadata:name:privileged-podnamespace:defaultspec:nodeSelector:kubernetes.io/hostname:\u0026lt;target-node-name\u0026gt; containers:- name:busyboximage:busyboxstdin:truesecurityContext:privileged:truevolumeMounts:- name:host-root-volumemountPath:/hostreadOnly:truevolumes:- name:host-root-volumehostPath:path:/hostNetwork:truehostPID:truerestartPolicy:NeverSSH access to a node that failed to join the cluster This section explores two options that can be used to get SSH access to a node that failed to join the cluster. As it is not possible to schedule a pod on the node, the Kubernetes-based methods explored so far cannot be used in this scenario.\nAdditionally, Gardener typically provisions worker instances in a private subnet of the VPC, hence - there is no public IP address that could be used for direct SSH access.\nFor this scenario, cloud providers typically have extensive documentation (e.g AWS \u0026amp; GCP and in some cases tooling support). However, these approaches are mostly cloud provider specific, require interaction via their CLI and API or sometimes the installation of a cloud provider specific agent one the node.\nAlternatively, gardenctl can be used providing a cloud provider agnostic and out-of-the-box support to get ssh access to an instance in a private subnet. Currently gardenctl supports AWS, GCP, Openstack, Azure and Alibaba Cloud.\nIdentifying the problematic instance First, the problematic instance has to be identified. In Gardener, worker pools can be created in different cloud provider regions, zones and accounts.\nThe instance would typically show up as successfully started / running in the cloud provider dashboard or API and it is not immediately obvious which one has a problem. Instead, we can use the Gardener API / CRDs to obtain the faulty instance identifier in a cloud-agnostic way.\nGardener uses the Machine Controller Manager to create the Shoot worker nodes. For each worker node, the Machine Controller Manager creates a Machine CRD in the Shoot namespace in the respective Seed cluster. Usually the problematic instance can be identified as the respective Machine CRD has status pending.\nThe instance / node name can be obtained from the Machine .status field:\n$ kubectl get machine \u0026lt;machine-name\u0026gt; -o json | jq -r .status.node This is all the information needed to go ahead and use gardenctl ssh to get a shell to the node. In addition, the used cloud provider, the specific identifier of the instance and the instance region can be identified from the Machine CRD.\nGet the identifier of the instance via:\n$ kubectl get machine \u0026lt;machine-name\u0026gt; -o json | jq -r .spec.providerID // e.g aws:///eu-north-1/i-069733c435bdb4640 The identifier shows that the instance belongs to the cloud provider aws with the ec2 instance-id i-069733c435bdb4640 in region eu-north-1.\nTo get more information about the instance, check out the MachineClass (e.g AWSMachineClass) that is associated with each Machine CRD in the Shoot namespace of the Seed cluster. The AWSMachineClass contains the machine image (ami), machine-type, iam information, network-interfaces, subnets, security groups and attached volumes.\nOf course, the information can also be used to get the instance with the cloud provider CLI / API.\ngardenctl ssh Using the node name of the problematic instance, we can use the gardenctl ssh command to get SSH access to the cloud provider instance via an automatically set up bastion host. gardenctl takes care of spinning up the bastion instance, setting up the SSH keys, ports and security groups and opens a root shell on the target instance. After the SSH session has ended, gardenctl deletes the created cloud provider resources.\nUse the following commands:\nFirst, target a Garden cluster containing all the Shoot definitions.\n$ gardenctl target garden \u0026lt;target-garden\u0026gt; Target an available Shoot by name. This sets up the context, configures the kubeconfig file of the Shoot cluster and downloads the cloud provider credentials. Subsequent commands will execute in this context.\n$ gardenctl target shoot \u0026lt;target-shoot\u0026gt; This uses the cloud provider credentials to spin up the bastion and to open a shell on the target instance.\n$ gardenctl ssh \u0026lt;target-node\u0026gt; SSH with manually created Bastion on AWS In case you are not using gardenctl or want to control the bastion instance yourself, you can also manually set it up. The steps described here are generally the same as those used by gardenctl internally. Despite some cloud provider specifics they can be generalized to the following list:\n Open port 22 on the target instance. Create an instance / VM in a public subnet (bastion instance needs to have public ip address). Set-up security groups, roles and open port 22 for the bastion instance.  The following diagram shows an overview how the SSH access to the target instance works:\nThis guide demonstrates the setup of a bastion on AWS.\nPrerequisites:\n The AWS CLI is set up. Obtain target instance-id (see here). Obtain the VPC ID the Shoot resources are created in. This can be found in the Infrastructure CRD in the Shoot namespace in the Seed. Make sure that port 22 on the target instance is open (default for Gardener deployed instances).  Extract security group via  $ aws ec2 describe-instances --instance-ids \u0026lt;instance-id\u0026gt;  Check for rule that allows inbound connections on port 22:  $ aws ec2 describe-security-groups --group-ids=\u0026lt;security-group-id\u0026gt;  If not available, create the rule with the following comamnd:  $ aws ec2 authorize-security-group-ingress --group-id \u0026lt;security-group-id\u0026gt; --protocol tcp --port 22 --cidr 0.0.0.0/0   Create the Bastion Security Group   The common name of the security group is \u0026lt;shoot-name\u0026gt;-bsg. Create the security group:\n$ aws ec2 create-security-group --group-name \u0026lt;bastion-security-group-name\u0026gt; --description ssh-access --vpc-id \u0026lt;VPC-ID\u0026gt;   Optionally, create identifying tags for the security group:\n$ aws ec2 create-tags --resources \u0026lt;bastion-security-group-id\u0026gt; --tags Key=component,Value=\u0026lt;tag\u0026gt;   Create permission in the bastion security group that allows ssh access on port 22.\n$ aws ec2 authorize-security-group-ingress --group-id \u0026lt;bastion-security-group-id\u0026gt; --protocol tcp --port 22 --cidr 0.0.0.0/0   Create an IAM role for the bastion instance with the name \u0026lt;shoot-name\u0026gt;-bastions:\n$ aws iam create-role --role-name \u0026lt;shoot-name\u0026gt;-bastions The content should be:\n  { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeRegions\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] }   Create the instance profile with name \u0026lt;shoot-name\u0026gt;-bastions:\n$ aws iam create-instance-profile --instance-profile-name \u0026lt;name\u0026gt;   Add the created role to the instance profile:\n$ aws iam add-role-to-instance-profile --instance-profile-name \u0026lt;instance-profile-name\u0026gt; --role-name \u0026lt;role-name\u0026gt;   Create the bastion instance Next, in order to be able to ssh into the bastion instance, the instance has to be set up with a user with a public ssh key. Create a user gardener that has the same Gardener-generated public ssh key as the target instance.\n  First, we need to get the public part of the Shoot ssh-key. The ssh-key is stored in a secret in the the project namespace in the Garden cluster. The name is: \u0026lt;shoot-name\u0026gt;-ssh-publickey. Get the key via:\n$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\\\u0026#34;id_rsa.pub\\\u0026#34;   A script handed over as user-data to the bastion ec2 instance, can be used to create the gardener user and add the ssh-key. For your convenience, you can use the following script to generate the user-data.\n  #!/bin/bash -eu saveUserDataFile () { ssh_key=$1 cat \u0026gt; gardener-bastion-userdata.sh \u0026lt;\u0026lt;EOF #!/bin/bash -eu id gardener || useradd gardener -mU mkdir -p /home/gardener/.ssh echo \u0026#34;$ssh_key\u0026#34; \u0026gt; /home/gardener/.ssh/authorized_keys chown gardener:gardener /home/gardener/.ssh/authorized_keys echo \u0026#34;gardener ALL=(ALL) NOPASSWD:ALL\u0026#34; \u0026gt;/etc/sudoers.d/99-gardener-user EOF } if [ -p /dev/stdin ]; then read -r input cat | saveUserDataFile \u0026#34;$input\u0026#34; else pbpaste | saveUserDataFile \u0026#34;$input\u0026#34; fi   Use the script by handing-over the public ssh-key of the Shoot cluster:\n$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\\\u0026#34;id_rsa.pub\\\u0026#34; | ./generate-userdata.sh This generates a file called gardener-bastion-userdata.sh in the same directory containing the user-data.\n  The following information is needed to create the bastion instance:\nbastion-IAM-instance-profile-name\n Use the created instance profile with name \u0026lt;shoot-name\u0026gt;-bastions  image-id\n Possible use the same image-id as for the target instance (or any other image). Has cloud provider specific format (AWS: ami).  ssh-public-key-name\n This is the ssh key pair already created in the Shoot\u0026rsquo;s cloud provider account by Gardener during the Infrastructure CRD reconciliation. The name is usually: \u0026lt;shoot-name\u0026gt;-ssh-publickey  subnet-id\n Choose a subnet that is attached to an Internet Gateway and NAT Gateway (bastion instance must have a public IP). The Gardener created public subnet with the name \u0026lt;shoot-name\u0026gt;-public-utility-\u0026lt;xy\u0026gt; can be used. Please check the created subnets with the cloud provider.  bastion-security-group-id\n Use the id of the created bastion security group.  file-path-to-userdata\n  Use the filepath to user-data file generated in the previous step.\n  bastion-instance-name\n Optional to tag the instance. Usually \u0026lt;shoot-name\u0026gt;-bastions      Create the bastion instance via:\n  $ ec2 run-instances --iam-instance-profile Name=\u0026lt;bastion-IAM-instance-profile-name\u0026gt; --image-id \u0026lt;image-id\u0026gt; --count 1 --instance-type t3.nano --key-name \u0026lt;ssh-public-key-name\u0026gt; --security-group-ids \u0026lt;bastion-security-group-id\u0026gt; --subnet-id \u0026lt;subnet-id\u0026gt; --associate-public-ip-address --user-data \u0026lt;file-path-to-userdata\u0026gt; --tag-specifications ResourceType=instance,Tags=[{Key=Name,Value=\u0026lt;bastion-instance-name\u0026gt;},{Key=component,Value=\u0026lt;mytag\u0026gt;}] ResourceType=volume,Tags=[{Key=component,Value=\u0026lt;mytag\u0026gt;}]\u0026#34; Capture the instance-id from the reponse and wait until the ec2 instance is running and has a public ip address.\nConnecting to the target instance Save the private key of the ssh-key-pair in a temporary local file for later use.\n$ umask 077 $ kubectl get secret \u0026lt;shoot-name\u0026gt;.ssh-keypair -o json | jq -r .data.\\\u0026#34;id_rsa\\\u0026#34; | base64 -d \u0026gt; id_rsa.key Use the private ssh key to ssh into the bastion instance.\n$ ssh-i \u0026lt;path-to-private-key\u0026gt; gardener@\u0026lt;public-bastion-instance-ip\u0026gt;If that works, connect from your local terminal to the target instance via the bastion.\n$ ssh-i \u0026lt;path-to-private-key\u0026gt;-o ProxyCommand=\u0026#34;ssh -W %h:%p -i \u0026lt;private-key\u0026gt; -o IdentitiesOnly=yes -o StrictHostKeyChecking=no gardener@\u0026lt;public-ip-bastion\u0026gt;\u0026#34; gardener@\u0026lt;private-ip-target-instance\u0026gt; -o IdentitiesOnly=yes -o StrictHostKeyChecking=no Cleanup Do not forget to cleanup the created resources. Otherwise Gardener will eventually fail to delete the Shoot.\n"},{"uri":"https://gardener.cloud/documentation/tutorials/gpu/","title":"GPU Enabled Cluster","tags":[],"description":"Setting up a GPU Enabled Cluster for Deep Learning","content":"Intro Be aware, that the following sections might be opinionated. Kubernetes, and the GPU support in particular, are rapidly evolving, which means that this guide is likely to be outdated sometime soon. For this reason, contributions are highly appreciated to update this guide.\nCreate a Cluster First thing first, lets create a k8s cluster with GPU accelerated nodes. In this example we will use AWS p2.xlarge EC2 instance because it\u0026rsquo;s the cheapest available option at the moment. Use such cheap instances for learning to limit your resource costs. This costs around 1/hour per GPU\nInstall NVidia Driver as Daemonset apiVersion:apps/v1kind:DaemonSetmetadata:name:nvidia-driver-installernamespace:kube-systemlabels:k8s-app:nvidia-driver-installerspec:selector:matchLabels:name:nvidia-driver-installerk8s-app:nvidia-driver-installertemplate:metadata:labels:name:nvidia-driver-installerk8s-app:nvidia-driver-installerspec:hostPID:trueinitContainers:- image:squat/modulus:4a1799e7aa0143bcbb70d354bab3e419b1f54972name:modulusargs:- compile- nvidia- \u0026#34;410.104\u0026#34;securityContext:privileged:trueenv:- name:MODULUS_CHROOTvalue:\u0026#34;true\u0026#34;- name:MODULUS_INSTALLvalue:\u0026#34;true\u0026#34;- name:MODULUS_INSTALL_DIRvalue:/opt/drivers- name:MODULUS_CACHE_DIRvalue:/opt/modulus/cache- name:MODULUS_LD_ROOTvalue:/root- name:IGNORE_MISSING_MODULE_SYMVERSvalue:\u0026#34;1\u0026#34;volumeMounts:- name:etc-coreosmountPath:/etc/coreosreadOnly:true- name:usr-share-coreosmountPath:/usr/share/coreosreadOnly:true- name:ld-rootmountPath:/root- name:module-cachemountPath:/opt/modulus/cache- name:module-install-dir-basemountPath:/opt/drivers- name:devmountPath:/devcontainers:- image:\u0026#34;gcr.io/google-containers/pause:3.1\u0026#34;name:pausetolerations:- key:\u0026#34;nvidia.com/gpu\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;operator:\u0026#34;Exists\u0026#34;volumes:- name:etc-coreoshostPath:path:/etc/coreos- name:usr-share-coreoshostPath:path:/usr/share/coreos- name:ld-roothostPath:path:/- name:module-cachehostPath:path:/opt/modulus/cache- name:devhostPath:path:/dev- name:module-install-dir-basehostPath:path:/opt/driversInstall Device Plugin apiVersion:apps/v1kind:DaemonSetmetadata:name:nvidia-gpu-device-pluginnamespace:kube-systemlabels:k8s-app:nvidia-gpu-device-plugin#addonmanager.kubernetes.io/mode: Reconcilespec:selector:matchLabels:k8s-app:nvidia-gpu-device-plugintemplate:metadata:labels:k8s-app:nvidia-gpu-device-pluginannotations:scheduler.alpha.kubernetes.io/critical-pod:\u0026#39;\u0026#39;spec:priorityClassName:system-node-criticalvolumes:- name:device-pluginhostPath:path:/var/lib/kubelet/device-plugins- name:devhostPath:path:/devcontainers:- image:\u0026#34;k8s.gcr.io/nvidia-gpu-device-plugin@sha256:08509a36233c5096bb273a492251a9a5ca28558ab36d74007ca2a9d3f0b61e1d\u0026#34;command:[\u0026#34;/usr/bin/nvidia-gpu-device-plugin\u0026#34;,\u0026#34;-logtostderr\u0026#34;,\u0026#34;-host-path=/opt/drivers/nvidia\u0026#34;]name:nvidia-gpu-device-pluginresources:requests:cpu:50mmemory:10Milimits:cpu:50mmemory:10MisecurityContext:privileged:truevolumeMounts:- name:device-pluginmountPath:/device-plugin- name:devmountPath:/devupdateStrategy:type:RollingUpdateTest To run an example training on a GPU node, start first a base image with Tensorflow with GPU support \u0026amp; Keras\napiVersion:apps/v1kind:Deploymentmetadata:name:deeplearning-workbenchnamespace:defaultspec:replicas:1selector:matchLabels:app:deeplearning-workbenchtemplate:metadata:labels:app:deeplearning-workbenchspec:containers:- name:deeplearning-workbenchimage:afritzler/deeplearning-workbenchresources:limits:nvidia.com/gpu:1tolerations:- key:\u0026#34;nvidia.com/gpu\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;operator:\u0026#34;Exists\u0026#34;Note: the tolerations section above is not required if you deploy the ExtendedResourceToleration admission controller to your cluster. You can do this in the kubernetes section of your Gardener cluster shoot.yaml as follows:\nkubernetes: kubeAPIServer: admissionPlugins: - name: ExtendedResourceToleration Now exec into the container and start an example Keras training\nkubectl exec -it deeplearning-workbench-8676458f5d-p4d2v -- /bin/bash cd /keras/example python imdb_cnn.py Acknowledgments \u0026amp; References  Andreas Fritzler from the Gardener Core team for the R\u0026amp;D and providing this setup. Build and install NVIDIA driver on CoreOS Nvidia Device Plugin  "},{"uri":"https://gardener.cloud/v1.12.8/tutorials/gpu/","title":"GPU Enabled Cluster","tags":[],"description":"Setting up a GPU Enabled Cluster for Deep Learning","content":"Intro Be aware, that the following sections might be opinionated. Kubernetes, and the GPU support in particular, are rapidly evolving, which means that this guide is likely to be outdated sometime soon. For this reason, contributions are highly appreciated to update this guide.\nCreate a Cluster First thing first, lets create a k8s cluster with GPU accelerated nodes. In this example we will use AWS p2.xlarge EC2 instance because it\u0026rsquo;s the cheapest available option at the moment. Use such cheap instances for learning to limit your resource costs. This costs around 1/hour per GPU\nInstall NVidia Driver as Daemonset apiVersion:apps/v1kind:DaemonSetmetadata:name:nvidia-driver-installernamespace:kube-systemlabels:k8s-app:nvidia-driver-installerspec:selector:matchLabels:name:nvidia-driver-installerk8s-app:nvidia-driver-installertemplate:metadata:labels:name:nvidia-driver-installerk8s-app:nvidia-driver-installerspec:hostPID:trueinitContainers:- image:squat/modulus:4a1799e7aa0143bcbb70d354bab3e419b1f54972name:modulusargs:- compile- nvidia- \u0026#34;410.104\u0026#34;securityContext:privileged:trueenv:- name:MODULUS_CHROOTvalue:\u0026#34;true\u0026#34;- name:MODULUS_INSTALLvalue:\u0026#34;true\u0026#34;- name:MODULUS_INSTALL_DIRvalue:/opt/drivers- name:MODULUS_CACHE_DIRvalue:/opt/modulus/cache- name:MODULUS_LD_ROOTvalue:/root- name:IGNORE_MISSING_MODULE_SYMVERSvalue:\u0026#34;1\u0026#34;volumeMounts:- name:etc-coreosmountPath:/etc/coreosreadOnly:true- name:usr-share-coreosmountPath:/usr/share/coreosreadOnly:true- name:ld-rootmountPath:/root- name:module-cachemountPath:/opt/modulus/cache- name:module-install-dir-basemountPath:/opt/drivers- name:devmountPath:/devcontainers:- image:\u0026#34;gcr.io/google-containers/pause:3.1\u0026#34;name:pausetolerations:- key:\u0026#34;nvidia.com/gpu\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;operator:\u0026#34;Exists\u0026#34;volumes:- name:etc-coreoshostPath:path:/etc/coreos- name:usr-share-coreoshostPath:path:/usr/share/coreos- name:ld-roothostPath:path:/- name:module-cachehostPath:path:/opt/modulus/cache- name:devhostPath:path:/dev- name:module-install-dir-basehostPath:path:/opt/driversInstall Device Plugin apiVersion:apps/v1kind:DaemonSetmetadata:name:nvidia-gpu-device-pluginnamespace:kube-systemlabels:k8s-app:nvidia-gpu-device-plugin#addonmanager.kubernetes.io/mode: Reconcilespec:selector:matchLabels:k8s-app:nvidia-gpu-device-plugintemplate:metadata:labels:k8s-app:nvidia-gpu-device-pluginannotations:scheduler.alpha.kubernetes.io/critical-pod:\u0026#39;\u0026#39;spec:priorityClassName:system-node-criticalvolumes:- name:device-pluginhostPath:path:/var/lib/kubelet/device-plugins- name:devhostPath:path:/devcontainers:- image:\u0026#34;k8s.gcr.io/nvidia-gpu-device-plugin@sha256:08509a36233c5096bb273a492251a9a5ca28558ab36d74007ca2a9d3f0b61e1d\u0026#34;command:[\u0026#34;/usr/bin/nvidia-gpu-device-plugin\u0026#34;,\u0026#34;-logtostderr\u0026#34;,\u0026#34;-host-path=/opt/drivers/nvidia\u0026#34;]name:nvidia-gpu-device-pluginresources:requests:cpu:50mmemory:10Milimits:cpu:50mmemory:10MisecurityContext:privileged:truevolumeMounts:- name:device-pluginmountPath:/device-plugin- name:devmountPath:/devupdateStrategy:type:RollingUpdateTest To run an example training on a GPU node, start first a base image with Tensorflow with GPU support \u0026amp; Keras\napiVersion:apps/v1kind:Deploymentmetadata:name:deeplearning-workbenchnamespace:defaultspec:replicas:1selector:matchLabels:app:deeplearning-workbenchtemplate:metadata:labels:app:deeplearning-workbenchspec:containers:- name:deeplearning-workbenchimage:afritzler/deeplearning-workbenchresources:limits:nvidia.com/gpu:1tolerations:- key:\u0026#34;nvidia.com/gpu\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;operator:\u0026#34;Exists\u0026#34;Note: the tolerations section above is not required if you deploy the ExtendedResourceToleration admission controller to your cluster. You can do this in the kubernetes section of your Gardener cluster shoot.yaml as follows:\nkubernetes: kubeAPIServer: admissionPlugins: - name: ExtendedResourceToleration Now exec into the container and start an example Keras training\nkubectl exec -it deeplearning-workbench-8676458f5d-p4d2v -- /bin/bash cd /keras/example python imdb_cnn.py Acknowledgments \u0026amp; References  Andreas Fritzler from the Gardener Core team for the R\u0026amp;D and providing this setup. Build and install NVIDIA driver on CoreOS Nvidia Device Plugin  "},{"uri":"https://gardener.cloud/v1.13.2/tutorials/gpu/","title":"GPU Enabled Cluster","tags":[],"description":"Setting up a GPU Enabled Cluster for Deep Learning","content":"Intro Be aware, that the following sections might be opinionated. Kubernetes, and the GPU support in particular, are rapidly evolving, which means that this guide is likely to be outdated sometime soon. For this reason, contributions are highly appreciated to update this guide.\nCreate a Cluster First thing first, lets create a k8s cluster with GPU accelerated nodes. In this example we will use AWS p2.xlarge EC2 instance because it\u0026rsquo;s the cheapest available option at the moment. Use such cheap instances for learning to limit your resource costs. This costs around 1/hour per GPU\nInstall NVidia Driver as Daemonset apiVersion:apps/v1kind:DaemonSetmetadata:name:nvidia-driver-installernamespace:kube-systemlabels:k8s-app:nvidia-driver-installerspec:selector:matchLabels:name:nvidia-driver-installerk8s-app:nvidia-driver-installertemplate:metadata:labels:name:nvidia-driver-installerk8s-app:nvidia-driver-installerspec:hostPID:trueinitContainers:- image:squat/modulus:4a1799e7aa0143bcbb70d354bab3e419b1f54972name:modulusargs:- compile- nvidia- \u0026#34;410.104\u0026#34;securityContext:privileged:trueenv:- name:MODULUS_CHROOTvalue:\u0026#34;true\u0026#34;- name:MODULUS_INSTALLvalue:\u0026#34;true\u0026#34;- name:MODULUS_INSTALL_DIRvalue:/opt/drivers- name:MODULUS_CACHE_DIRvalue:/opt/modulus/cache- name:MODULUS_LD_ROOTvalue:/root- name:IGNORE_MISSING_MODULE_SYMVERSvalue:\u0026#34;1\u0026#34;volumeMounts:- name:etc-coreosmountPath:/etc/coreosreadOnly:true- name:usr-share-coreosmountPath:/usr/share/coreosreadOnly:true- name:ld-rootmountPath:/root- name:module-cachemountPath:/opt/modulus/cache- name:module-install-dir-basemountPath:/opt/drivers- name:devmountPath:/devcontainers:- image:\u0026#34;gcr.io/google-containers/pause:3.1\u0026#34;name:pausetolerations:- key:\u0026#34;nvidia.com/gpu\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;operator:\u0026#34;Exists\u0026#34;volumes:- name:etc-coreoshostPath:path:/etc/coreos- name:usr-share-coreoshostPath:path:/usr/share/coreos- name:ld-roothostPath:path:/- name:module-cachehostPath:path:/opt/modulus/cache- name:devhostPath:path:/dev- name:module-install-dir-basehostPath:path:/opt/driversInstall Device Plugin apiVersion:apps/v1kind:DaemonSetmetadata:name:nvidia-gpu-device-pluginnamespace:kube-systemlabels:k8s-app:nvidia-gpu-device-plugin#addonmanager.kubernetes.io/mode: Reconcilespec:selector:matchLabels:k8s-app:nvidia-gpu-device-plugintemplate:metadata:labels:k8s-app:nvidia-gpu-device-pluginannotations:scheduler.alpha.kubernetes.io/critical-pod:\u0026#39;\u0026#39;spec:priorityClassName:system-node-criticalvolumes:- name:device-pluginhostPath:path:/var/lib/kubelet/device-plugins- name:devhostPath:path:/devcontainers:- image:\u0026#34;k8s.gcr.io/nvidia-gpu-device-plugin@sha256:08509a36233c5096bb273a492251a9a5ca28558ab36d74007ca2a9d3f0b61e1d\u0026#34;command:[\u0026#34;/usr/bin/nvidia-gpu-device-plugin\u0026#34;,\u0026#34;-logtostderr\u0026#34;,\u0026#34;-host-path=/opt/drivers/nvidia\u0026#34;]name:nvidia-gpu-device-pluginresources:requests:cpu:50mmemory:10Milimits:cpu:50mmemory:10MisecurityContext:privileged:truevolumeMounts:- name:device-pluginmountPath:/device-plugin- name:devmountPath:/devupdateStrategy:type:RollingUpdateTest To run an example training on a GPU node, start first a base image with Tensorflow with GPU support \u0026amp; Keras\napiVersion:apps/v1kind:Deploymentmetadata:name:deeplearning-workbenchnamespace:defaultspec:replicas:1selector:matchLabels:app:deeplearning-workbenchtemplate:metadata:labels:app:deeplearning-workbenchspec:containers:- name:deeplearning-workbenchimage:afritzler/deeplearning-workbenchresources:limits:nvidia.com/gpu:1tolerations:- key:\u0026#34;nvidia.com/gpu\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;operator:\u0026#34;Exists\u0026#34;Note: the tolerations section above is not required if you deploy the ExtendedResourceToleration admission controller to your cluster. You can do this in the kubernetes section of your Gardener cluster shoot.yaml as follows:\nkubernetes: kubeAPIServer: admissionPlugins: - name: ExtendedResourceToleration Now exec into the container and start an example Keras training\nkubectl exec -it deeplearning-workbench-8676458f5d-p4d2v -- /bin/bash cd /keras/example python imdb_cnn.py Acknowledgments \u0026amp; References  Andreas Fritzler from the Gardener Core team for the R\u0026amp;D and providing this setup. Build and install NVIDIA driver on CoreOS Nvidia Device Plugin  "},{"uri":"https://gardener.cloud/documentation/guides/install_gardener/secure-setup/","title":"Hardening the Gardener Community Setup","tags":[],"description":"","content":"Hardening the Gardener Community Setup Context Gardener stakeholders in the Open Source community usually use the Gardener Setup Scripts, to create a Garden cluster based on Kubernetes v1.9 which then can be used to create Shoot clusters based on Kubernetes v1.10, v1.11 and v1.12. Shoot clusters can play the following roles in a Gardener landscape:\n Seed cluster Shoot cluster  As Alban Crequy from Kinvolk has recommended in his recent Gardener blog Auditing Kubernetes for Secure Setup the Gardener Team at SAP has applied several means to harden the Gardener landscapes at SAP.\nRecommendations Mitigation for Gardener CVE-2018-2475 The following recommendations describe how you can harden your Gardener Community Setup by adding a Seed cluster hardened with network policies.\n Use the Gardener Setup Scripts to create a Garden cluster in a dedicated IaaS account Create a Shoot cluster in a different IaaS account As a precaution you should not deploy the Kubernetes dashboard on this Shoot cluster Register this newly created Shoot cluster as a Seed cluster in the Gardener End user Shoot clusters can then be created using this newly created Seed cluster (which in turn is a Shoot cluster).  A tutorial on how to create a shooted seed cluster can be found here.\nThe rational behind this activity is, that Calico network policies harden this Seed cluster but the community installer uses Flannel which does not offer these features for the Garden cluster.\nWhen you have added a hardened Seed cluster you are expected not be vulnerable to the Gardener CVE-2018-2475 anymore.\nMitigation for Kubernetes CVE-2018-1002105 In addition when you follow the recommendations in the recent Gardener Security Announcement you are expected not be vulnerable to the Kubernetes CVE-2018-1002105 with your hardened Gardener Community Setup.\nAlternative Approach For this alternative approach there is no Gardener blog available, it is not part of the Gardener Setup Scripts, but it was tested by the Gardener Team at SAP. Use GKE to host a Garden cluster based on Kubernetes v1.10, v1.11 and v1.12 (without the Kubernetes dashboard) in a dedicated GCP account. If you do this by your own, please ensure that the network policies are turned on, which might not be the case by default. Then you can apply the security configuration which Alban Crequy from Kinvolk has recommended in his blog directly in the Garden cluster and create Shoot clusters from there in a different IaaS account.\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/install_gardener/secure-setup/","title":"Hardening the Gardener Community Setup","tags":[],"description":"","content":"Hardening the Gardener Community Setup Context Gardener stakeholders in the Open Source community usually use the Gardener Setup Scripts, to create a Garden cluster based on Kubernetes v1.9 which then can be used to create Shoot clusters based on Kubernetes v1.10, v1.11 and v1.12. Shoot clusters can play the following roles in a Gardener landscape:\n Seed cluster Shoot cluster  As Alban Crequy from Kinvolk has recommended in his recent Gardener blog Auditing Kubernetes for Secure Setup the Gardener Team at SAP has applied several means to harden the Gardener landscapes at SAP.\nRecommendations Mitigation for Gardener CVE-2018-2475 The following recommendations describe how you can harden your Gardener Community Setup by adding a Seed cluster hardened with network policies.\n Use the Gardener Setup Scripts to create a Garden cluster in a dedicated IaaS account Create a Shoot cluster in a different IaaS account As a precaution you should not deploy the Kubernetes dashboard on this Shoot cluster Register this newly created Shoot cluster as a Seed cluster in the Gardener End user Shoot clusters can then be created using this newly created Seed cluster (which in turn is a Shoot cluster).  A tutorial on how to create a shooted seed cluster can be found here.\nThe rational behind this activity is, that Calico network policies harden this Seed cluster but the community installer uses Flannel which does not offer these features for the Garden cluster.\nWhen you have added a hardened Seed cluster you are expected not be vulnerable to the Gardener CVE-2018-2475 anymore.\nMitigation for Kubernetes CVE-2018-1002105 In addition when you follow the recommendations in the recent Gardener Security Announcement you are expected not be vulnerable to the Kubernetes CVE-2018-1002105 with your hardened Gardener Community Setup.\nAlternative Approach For this alternative approach there is no Gardener blog available, it is not part of the Gardener Setup Scripts, but it was tested by the Gardener Team at SAP. Use GKE to host a Garden cluster based on Kubernetes v1.10, v1.11 and v1.12 (without the Kubernetes dashboard) in a dedicated GCP account. If you do this by your own, please ensure that the network policies are turned on, which might not be the case by default. Then you can apply the security configuration which Alban Crequy from Kinvolk has recommended in his blog directly in the Garden cluster and create Shoot clusters from there in a different IaaS account.\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/install_gardener/secure-setup/","title":"Hardening the Gardener Community Setup","tags":[],"description":"","content":"Hardening the Gardener Community Setup Context Gardener stakeholders in the Open Source community usually use the Gardener Setup Scripts, to create a Garden cluster based on Kubernetes v1.9 which then can be used to create Shoot clusters based on Kubernetes v1.10, v1.11 and v1.12. Shoot clusters can play the following roles in a Gardener landscape:\n Seed cluster Shoot cluster  As Alban Crequy from Kinvolk has recommended in his recent Gardener blog Auditing Kubernetes for Secure Setup the Gardener Team at SAP has applied several means to harden the Gardener landscapes at SAP.\nRecommendations Mitigation for Gardener CVE-2018-2475 The following recommendations describe how you can harden your Gardener Community Setup by adding a Seed cluster hardened with network policies.\n Use the Gardener Setup Scripts to create a Garden cluster in a dedicated IaaS account Create a Shoot cluster in a different IaaS account As a precaution you should not deploy the Kubernetes dashboard on this Shoot cluster Register this newly created Shoot cluster as a Seed cluster in the Gardener End user Shoot clusters can then be created using this newly created Seed cluster (which in turn is a Shoot cluster).  A tutorial on how to create a shooted seed cluster can be found here.\nThe rational behind this activity is, that Calico network policies harden this Seed cluster but the community installer uses Flannel which does not offer these features for the Garden cluster.\nWhen you have added a hardened Seed cluster you are expected not be vulnerable to the Gardener CVE-2018-2475 anymore.\nMitigation for Kubernetes CVE-2018-1002105 In addition when you follow the recommendations in the recent Gardener Security Announcement you are expected not be vulnerable to the Kubernetes CVE-2018-1002105 with your hardened Gardener Community Setup.\nAlternative Approach For this alternative approach there is no Gardener blog available, it is not part of the Gardener Setup Scripts, but it was tested by the Gardener Team at SAP. Use GKE to host a Garden cluster based on Kubernetes v1.10, v1.11 and v1.12 (without the Kubernetes dashboard) in a dedicated GCP account. If you do this by your own, please ensure that the network policies are turned on, which might not be the case by default. Then you can apply the security configuration which Alban Crequy from Kinvolk has recommended in his blog directly in the Garden cluster and create Shoot clusters from there in a different IaaS account.\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/healthcheck-library/","title":"Health Check Library","tags":[],"description":"","content":"Health Check Library Goal Typically an extension reconciles a specific resource (Custom Resource Definitions (CRDs)) and creates/modifies resources in the cluster (via helm, managed resources, kubectl, \u0026hellip;). We call these API Objects \u0026lsquo;dependent objects\u0026rsquo; - as they are bound to the lifecycle of the extension.\nThe goal of this library is to enable extensions to setup health checks for their \u0026lsquo;dependent objects\u0026rsquo; with minimal effort.\nUsage The library provides a generic controller with the ability to register any resource that satisfies the extension object interface. An example is the Worker CRD.\nHealth check functions for commonly used dependent objects can be reused and registered with the controller, such as:\n Deployment DaemonSet StatefulSet ManagedResource (Gardener specific)  See below example taken from the provider-aws.\nhealth.DefaultRegisterExtensionForHealthCheck( aws.Type, extensionsv1alpha1.SchemeGroupVersion.WithKind(extensionsv1alpha1.WorkerResource), func() runtime.Object { return \u0026amp;extensionsv1alpha1.Worker{} }, mgr, // controller runtime manager  opts, // options for the health check controller  nil, // custom predicates  map[extensionshealthcheckcontroller.HealthCheck]string{ general.CheckManagedResource(genericactuator.McmShootResourceName): string(gardencorev1beta1.ShootSystemComponentsHealthy), general.CheckSeedDeployment(aws.MachineControllerManagerName): string(gardencorev1beta1.ShootEveryNodeReady), worker.SufficientNodesAvailable(): string(gardencorev1beta1.ShootEveryNodeReady), }) This creates a health check controller that reconciles the extensions.gardener.cloud/v1alpha1.Worker resource with the spec.type \u0026lsquo;aws\u0026rsquo;. Three health check functions are registered that are executed during reconciliation. Each health check is mapped to a single HealthConditionType that results in conditions with the same condition.type (see below). To contribute to the Shoot\u0026rsquo;s health, the following can be used: SystemComponentsHealthy, EveryNodeReady, ControlPlaneHealthy. The Gardener/Gardenlet checks each extension for conditions matching these types. However extensions are free to choose any HealthConditionType. More information can be found here.\nA health check has to satisfy below interface. You can find implementation examples here.\ntype HealthCheck interface { // Check is the function that executes the actual health check  Check(context.Context, types.NamespacedName) (*SingleCheckResult, error) // InjectSeedClient injects the seed client  InjectSeedClient(client.Client) // InjectShootClient injects the shoot client  InjectShootClient(client.Client) // SetLoggerSuffix injects the logger  SetLoggerSuffix(string, string) // DeepCopy clones the healthCheck  DeepCopy() HealthCheck } The health check controller regularly (default: 30s) reconciles the extension resource and executes the registered health checks for the dependent objects. As a result, the controller writes condition(s) to the status of the extension containing the health check result. In our example, two checks are mapped to ShootEveryNodeReady and one to ShootSystemComponentsHealthy, leading to conditions with two distinct HealthConditionTypes (condition.type)\nstatus:conditions:- lastTransitionTime:\u0026#34;20XX-10-28T08:17:21Z\u0026#34;lastUpdateTime:\u0026#34;20XX-11-28T08:17:21Z\u0026#34;message:(1/1)Healthcheckssuccessfulreason:HealthCheckSuccessfulstatus:\u0026#34;True\u0026#34;type:SystemComponentsHealthy- lastTransitionTime:\u0026#34;20XX-10-28T08:17:21Z\u0026#34;lastUpdateTime:\u0026#34;20XX-11-28T08:17:21Z\u0026#34;message:(2/2)Healthcheckssuccessfulreason:HealthCheckSuccessfulstatus:\u0026#34;True\u0026#34;type:EveryNodeReadyPlease note that there are four statuses: True, False, Unknown, and Progressing.\n True should be used for successful health checks. False should be used for unsuccessful/failing health checks. Unknown should be used when there was an error trying to determine the health status. Progressing should be used to indicate that the health status did not succeed but for expected reasons (e.g., a cluster scale up/down could make the standard health check fail because something is wrong with the Machines, however, it\u0026rsquo;s actually an expected situation and known to be completed within a few minutes.)  Health checks that report Progressing should also provide a timeout after which this \u0026ldquo;progressing situation\u0026rdquo; is expected to be completed. The health check library will automatically transition the status to False if the timeout was exceeded.\nAdditional Considerations It is up to the extension to decide how to conduct health checks, though it is recommended to make use of the build-in health check functionality of managed-resources for trivial checks. By deploying the depending resources via managed resources, the gardener resource manager conducts basic checks for different API objects out-of-the-box (e.g Deployments, DaemonSets, \u0026hellip;) - and writes health conditions. In turn, the library contains a health check function to gather the health information from managed resources.\nMore sophisticated health checks should be implemented by the extension controller itself (implementing the HealthCheck interface).\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/healthcheck-library/","title":"Health Check Library","tags":[],"description":"","content":"Health Check Library Goal Typically an extension reconciles a specific resource (Custom Resource Definitions (CRDs)) and creates/modifies resources in the cluster (via helm, managed resources, kubectl, \u0026hellip;). We call these API Objects \u0026lsquo;dependent objects\u0026rsquo; - as they are bound to the lifecycle of the extension.\nThe goal of this library is to enable extensions to setup health checks for their \u0026lsquo;dependent objects\u0026rsquo; with minimal effort.\nUsage The library provides a generic controller with the ability to register any resource that satisfies the extension object interface. An example is the Worker CRD.\nHealth check functions for commonly used dependent objects can be reused and registered with the controller, such as:\n Deployment DaemonSet StatefulSet ManagedResource (Gardener specific)  See below example taken from the provider-aws.\nhealth.DefaultRegisterExtensionForHealthCheck( aws.Type, extensionsv1alpha1.SchemeGroupVersion.WithKind(extensionsv1alpha1.WorkerResource), func() runtime.Object { return \u0026amp;extensionsv1alpha1.Worker{} }, mgr, // controller runtime manager  opts, // options for the health check controller  nil, // custom predicates  map[extensionshealthcheckcontroller.HealthCheck]string{ general.CheckManagedResource(genericactuator.McmShootResourceName): string(gardencorev1beta1.ShootSystemComponentsHealthy), general.CheckSeedDeployment(aws.MachineControllerManagerName): string(gardencorev1beta1.ShootEveryNodeReady), worker.SufficientNodesAvailable(): string(gardencorev1beta1.ShootEveryNodeReady), }) This creates a health check controller that reconciles the extensions.gardener.cloud/v1alpha1.Worker resource with the spec.type \u0026lsquo;aws\u0026rsquo;. Three health check functions are registered that are executed during reconciliation. Each health check is mapped to a single HealthConditionType that results in conditions with the same condition.type (see below). To contribute to the Shoot\u0026rsquo;s health, the following can be used: SystemComponentsHealthy, EveryNodeReady, ControlPlaneHealthy. The Gardener/Gardenlet checks each extension for conditions matching these types. However extensions are free to choose any HealthConditionType. More information can be found here.\nA health check has to satisfy below interface. You can find implementation examples here.\ntype HealthCheck interface { // Check is the function that executes the actual health check  Check(context.Context, types.NamespacedName) (*SingleCheckResult, error) // InjectSeedClient injects the seed client  InjectSeedClient(client.Client) // InjectShootClient injects the shoot client  InjectShootClient(client.Client) // SetLoggerSuffix injects the logger  SetLoggerSuffix(string, string) // DeepCopy clones the healthCheck  DeepCopy() HealthCheck } The health check controller regularly (default: 30s) reconciles the extension resource and executes the registered health checks for the dependent objects. As a result, the controller writes condition(s) to the status of the extension containing the health check result. In our example, two checks are mapped to ShootEveryNodeReady and one to ShootSystemComponentsHealthy, leading to conditions with two distinct HealthConditionTypes (condition.type)\nstatus:conditions:- lastTransitionTime:\u0026#34;20XX-10-28T08:17:21Z\u0026#34;lastUpdateTime:\u0026#34;20XX-11-28T08:17:21Z\u0026#34;message:(1/1)Healthcheckssuccessfulreason:HealthCheckSuccessfulstatus:\u0026#34;True\u0026#34;type:SystemComponentsHealthy- lastTransitionTime:\u0026#34;20XX-10-28T08:17:21Z\u0026#34;lastUpdateTime:\u0026#34;20XX-11-28T08:17:21Z\u0026#34;message:(2/2)Healthcheckssuccessfulreason:HealthCheckSuccessfulstatus:\u0026#34;True\u0026#34;type:EveryNodeReadyPlease note that there are four statuses: True, False, Unknown, and Progressing.\n True should be used for successful health checks. False should be used for unsuccessful/failing health checks. Unknown should be used when there was an error trying to determine the health status. Progressing should be used to indicate that the health status did not succeed but for expected reasons (e.g., a cluster scale up/down could make the standard health check fail because something is wrong with the Machines, however, it\u0026rsquo;s actually an expected situation and known to be completed within a few minutes.)  Health checks that report Progressing should also provide a timeout after which this \u0026ldquo;progressing situation\u0026rdquo; is expected to be completed. The health check library will automatically transition the status to False if the timeout was exceeded.\nAdditional Considerations It is up to the extension to decide how to conduct health checks, though it is recommended to make use of the build-in health check functionality of managed-resources for trivial checks. By deploying the depending resources via managed resources, the gardener resource manager conducts basic checks for different API objects out-of-the-box (e.g Deployments, DaemonSets, \u0026hellip;) - and writes health conditions. In turn, the library contains a health check function to gather the health information from managed resources.\nMore sophisticated health checks should be implemented by the extension controller itself (implementing the HealthCheck interface).\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/healthcheck-library/","title":"Health Check Library","tags":[],"description":"","content":"Health Check Library Goal Typically an extension reconciles a specific resource (Custom Resource Definitions (CRDs)) and creates/modifies resources in the cluster (via helm, managed resources, kubectl, \u0026hellip;). We call these API Objects \u0026lsquo;dependent objects\u0026rsquo; - as they are bound to the lifecycle of the extension.\nThe goal of this library is to enable extensions to setup health checks for their \u0026lsquo;dependent objects\u0026rsquo; with minimal effort.\nUsage The library provides a generic controller with the ability to register any resource that satisfies the extension object interface. An example is the Worker CRD.\nHealth check functions for commonly used dependent objects can be reused and registered with the controller, such as:\n Deployment DaemonSet StatefulSet ManagedResource (Gardener specific)  See below example taken from the provider-aws.\nhealth.DefaultRegisterExtensionForHealthCheck( aws.Type, extensionsv1alpha1.SchemeGroupVersion.WithKind(extensionsv1alpha1.WorkerResource), func() runtime.Object { return \u0026amp;extensionsv1alpha1.Worker{} }, mgr, // controller runtime manager  opts, // options for the health check controller  nil, // custom predicates  map[extensionshealthcheckcontroller.HealthCheck]string{ general.CheckManagedResource(genericactuator.McmShootResourceName): string(gardencorev1beta1.ShootSystemComponentsHealthy), general.CheckSeedDeployment(aws.MachineControllerManagerName): string(gardencorev1beta1.ShootEveryNodeReady), worker.SufficientNodesAvailable(): string(gardencorev1beta1.ShootEveryNodeReady), }) This creates a health check controller that reconciles the extensions.gardener.cloud/v1alpha1.Worker resource with the spec.type \u0026lsquo;aws\u0026rsquo;. Three health check functions are registered that are executed during reconciliation. Each health check is mapped to a single HealthConditionType that results in conditions with the same condition.type (see below). To contribute to the Shoot\u0026rsquo;s health, the following can be used: SystemComponentsHealthy, EveryNodeReady, ControlPlaneHealthy. The Gardener/Gardenlet checks each extension for conditions matching these types. However extensions are free to choose any HealthConditionType. More information can be found here.\nA health check has to satisfy below interface. You can find implementation examples here.\ntype HealthCheck interface { // Check is the function that executes the actual health check  Check(context.Context, types.NamespacedName) (*SingleCheckResult, error) // InjectSeedClient injects the seed client  InjectSeedClient(client.Client) // InjectShootClient injects the shoot client  InjectShootClient(client.Client) // SetLoggerSuffix injects the logger  SetLoggerSuffix(string, string) // DeepCopy clones the healthCheck  DeepCopy() HealthCheck } The health check controller regularly (default: 30s) reconciles the extension resource and executes the registered health checks for the dependent objects. As a result, the controller writes condition(s) to the status of the extension containing the health check result. In our example, two checks are mapped to ShootEveryNodeReady and one to ShootSystemComponentsHealthy, leading to conditions with two distinct HealthConditionTypes (condition.type)\nstatus:conditions:- lastTransitionTime:\u0026#34;20XX-10-28T08:17:21Z\u0026#34;lastUpdateTime:\u0026#34;20XX-11-28T08:17:21Z\u0026#34;message:(1/1)Healthcheckssuccessfulreason:HealthCheckSuccessfulstatus:\u0026#34;True\u0026#34;type:SystemComponentsHealthy- lastTransitionTime:\u0026#34;20XX-10-28T08:17:21Z\u0026#34;lastUpdateTime:\u0026#34;20XX-11-28T08:17:21Z\u0026#34;message:(2/2)Healthcheckssuccessfulreason:HealthCheckSuccessfulstatus:\u0026#34;True\u0026#34;type:EveryNodeReadyPlease note that there are four statuses: True, False, Unknown, and Progressing.\n True should be used for successful health checks. False should be used for unsuccessful/failing health checks. Unknown should be used when there was an error trying to determine the health status. Progressing should be used to indicate that the health status did not succeed but for expected reasons (e.g., a cluster scale up/down could make the standard health check fail because something is wrong with the Machines, however, it\u0026rsquo;s actually an expected situation and known to be completed within a few minutes.)  Health checks that report Progressing should also provide a timeout after which this \u0026ldquo;progressing situation\u0026rdquo; is expected to be completed. The health check library will automatically transition the status to False if the timeout was exceeded.\nAdditional Considerations It is up to the extension to decide how to conduct health checks, though it is recommended to make use of the build-in health check functionality of managed-resources for trivial checks. By deploying the depending resources via managed resources, the gardener resource manager conducts basic checks for different API objects out-of-the-box (e.g Deployments, DaemonSets, \u0026hellip;) - and writes health conditions. In turn, the library contains a health check function to gather the health information from managed resources.\nMore sophisticated health checks should be implemented by the extension controller itself (implementing the HealthCheck interface).\n"},{"uri":"https://gardener.cloud/documentation/guides/monitoring_and_troubleshooting/debug-a-pod/","title":"How to debug a pod","tags":[],"description":"Your pod doesn&#39;t run as expected. Are there any log files? Where? How could I debug a pod?","content":"Introduction Kubernetes offers powerful options to get more details about startup or runtime failures of pods as e.g. described in Application Introspection and Debugging or Debug Pods and Replication Controllers.\nIn order to identify pods with potential issus you could e.g. run kubectl get pods --all-namespaces | grep -iv Running  to filter out the pods which are not in the state Running. One of frequent error state is CrashLoopBackOff, which tells that a pod crashes right after the start. Kubernetes then tries to restart the pod, but often the pod startup fails again.\nHere is a short list of possible reasons which might lead to a pod crash:\n error during image pull caused by e.g. wrong/missing secrets or wrong/missing image the app runs in an error state caused e.g. by missing environmental variables (ConfigMaps) or secrets liveness probe failed too high resource consumption (memory and/or CPU) or too strict quota settings persistent volumes can\u0026rsquo;t be created/mounted the container image is not updated  Basically, the commands kubectl logs ... and kubectl describe ... with additional parameters are used to get more detailed information. By calling e.g. kubectl logs --help you get more detailed information about the command and its parameters.\nIn the next sections you\u0026rsquo;ll find some basic approaches to get some ideas what went wrong.\nRemarks:\n Even if the pods seem to be running as the status Running indicates, a high counter of the Restarts shows potential problems There is as well an interactive Tutorial Troubleshooting with Kubectl available which explains basic debugging activities The examples below are deployed into the namespace default. In case you want to change it use the optional parameter --namespace \u0026lt;your-namespace\u0026gt; to select the target namespace. They require Kubernetes release  1.8.  Prerequisites Your deployment was successful (no logical/syntactical errors in the manifest files), but the pod(s) aren\u0026rsquo;t running.\nError caused by wrong image name You run kubectl describe pod \u0026lt;your-pod\u0026gt; \u0026lt;your-namespace\u0026gt; to get detailed information about the pod startup.\nIn the Events section, you get an error message like Failed to pull image ... and Reason: Failed. The pod is in state ImagePullBackOff.\nThe example below is based on demo in Kubernetes documentation. In all examples the default namespace is used.\nFirst, cleanup with\nkubectl delete pod termination-demo Next, create a resource based on the yaml content below\napiVersion:v1kind:Podmetadata:name:termination-demospec:containers:- name:termination-demo-containerimage:debianncommand:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log\u0026#34;]kubectl describe pod termination-demo lists the following content in the Event section\nEvents: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 2m\t2m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned termination-demo to ip-10-250-17-112.eu-west-1.compute.internal 2m\t2m\t1\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026#34;default-token-sgccm\u0026#34; 2m\t1m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulling\tpulling image \u0026#34;debiann\u0026#34; 2m\t1m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tWarning\tFailed\tFailed to pull image \u0026#34;debiann\u0026#34;: rpc error: code = Unknown desc = Error: image library/debiann:latest not found 2m\t54s\t10\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tWarning\tFailedSync\tError syncing pod 2m\t54s\t6\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tBackOff\tBack-off pulling image \u0026#34;debiann\u0026#34; The error message with Reason: Failed tells that there is an error during pulling the image. A closer look at the image name indicates a misspelling.\nApp runs in an error state caused by missing ConfigMaps or Secrets This example illustrates the behavior in case of the app expecting environment variables but the corresponding Kubernetes artifacts are missing.\nFirst, cleanup with\nkubectldeletedeploymenttermination-demokubectldeleteconfigmapsapp-envNext, deploy this manifest\napiVersion:apps/v1kind:Deploymentmetadata:name:termination-demolabels:app:termination-demospec:replicas:1selector:matchLabels:app:termination-demotemplate:metadata:labels:app:termination-demospec:containers:- name:termination-demo-containerimage:debiancommand:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;sed \\\u0026#34;s/foo/bar/\\\u0026#34; \u0026lt; $MYFILE\u0026#34;]Now, the command kubectl get pods lists the pod termination-demo-xxx in the state Error or CrashLoopBackOff. The command kubectl describe pod termination-demo-xxx tells that there is no error during startup but gives no clue about what caused the crash.\nEvents: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 19m\t19m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned termination-demo-5fb484867d-xz2x9 to ip-10-250-17-112.eu-west-1.compute.internal 19m\t19m\t1\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026#34;default-token-sgccm\u0026#34; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulling\tpulling image \u0026#34;debian\u0026#34; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulled\tSuccessfully pulled image \u0026#34;debian\u0026#34; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tCreated\tCreated container 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tStarted\tStarted container 19m\t14m\t24\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tWarning\tBackOff\tBack-off restarting failed container 19m\t4m\t69\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tWarning\tFailedSync\tError syncing pod The command kubectl get logs termination-demo-xxx gives access to the output, the application writes on stderr and stdout. In this case, you get an output like\n/bin/sh: 1: cannot open : No such file So you need to have a closer look at the application. In this case the environmental variable MYFILEis missing. To fix this issue you could e.g. add a ConfigMap to your deployment as it is shown in the manifest listed below.\napiVersion:v1kind:ConfigMapmetadata:name:app-envdata:MYFILE:\u0026#34;/etc/profile\u0026#34;---apiVersion:apps/v1kind:Deploymentmetadata:name:termination-demolabels:app:termination-demospec:replicas:1selector:matchLabels:app:termination-demotemplate:metadata:labels:app:termination-demospec:containers:- name:termination-demo-containerimage:debiancommand:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;sed \\\u0026#34;s/foo/bar/\\\u0026#34; \u0026lt; $MYFILE\u0026#34;]envFrom:- configMapRef:name:app-envNote that once you fix the error and re-run the scenario, you might still see the pod in CrashLoopBackOff status. It is because the container finishes the command sed ... and runs to completion. In order to keep the container in Running status, a long running task is required, e.g.\napiVersion:v1kind:ConfigMapmetadata:name:app-envdata:MYFILE:\u0026#34;/etc/profile\u0026#34;SLEEP:\u0026#34;5\u0026#34;---apiVersion:apps/v1kind:Deploymentmetadata:name:termination-demolabels:app:termination-demospec:replicas:1selector:matchLabels:app:termination-demotemplate:metadata:labels:app:termination-demospec:containers:- name:termination-demo-containerimage:debiancommand:[\u0026#34;/bin/sh\u0026#34;]# args: [\u0026#34;-c\u0026#34;, \u0026#34;sed \\\u0026#34;s/foo/bar/\\\u0026#34; \u0026lt; $MYFILE\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;while true; do sleep $SLEEP; echo sleeping; done;\u0026#34;]envFrom:- configMapRef:name:app-envToo high resource consumption or too strict quota settings You can optionally specify the amount of memory and/or CPU your container gets during runtime. In case these settings are missing, the default requests settings are taken: CPU: 0m (in Milli CPU) and RAM: 0Gi which indicate no other limits than the ones of the node(s) itself. Find more details in Configure Default Memory Requests and Limits for a Namespace,\nIn case your application needs more resources, Kubernetes distinguishes between requests and limit settings: requests specify the guaranteed amount of resource, whereas limit tells Kubernetes the maximum amount of resource the container might need. Mathematically both settings could be described by the relation 0 \u0026lt;= requests \u0026lt;= limit. For both settings you need to consider the total amount of resources the available nodes provide. For a detailed description of the concept see Resource Quality of Service in Kubernetes.\nUse kubectl describe nodes to get a first overview of the resource consumption of your cluster. Of special interest are the figures indicating the amount of CPU and Memory Requests at the bottom of the output.\nThe next example demonstrates what happens in case the CPU request is too high in order to be managed by your cluster.\nFirst, cleanup with\nkubectldeletedeploymenttermination-demokubectldeleteconfigmapsapp-envNext, adapt the cpu in the yaml below to be slightly higher than the remaining cpu resources in your cluster and deploy this manifest. In this example 600m (milli CPUs) are requested in a Kubernetes system with a single 2 Core worker node which results in an error message.\napiVersion:apps/v1kind:Deploymentmetadata:name:termination-demolabels:app:termination-demospec:replicas:1selector:matchLabels:app:termination-demotemplate:metadata:labels:app:termination-demospec:containers:- name:termination-demo-containerimage:debiancommand:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log\u0026#34;]resources:requests:cpu:\u0026#34;600m\u0026#34;The command kubectl get pods lists the pod termination-demo-xxx in the state Pending. More details on why this happens could be found by using the command kubectl describe pod termination-demo-xxx:\n$ kubectl describe po termination-demo-fdb7bb7d9-mzvfw Name: termination-demo-fdb7bb7d9-mzvfw Namespace: default ... Containers: termination-demo-container: Image: debian Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Command: /bin/sh Args: -c sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log Requests: cpu: 6 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-t549m (ro) Conditions: Type Status PodScheduled False Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 9s (x7 over 40s) default-scheduler 0/2 nodes are available: 2 Insufficient cpu. More details in\n Managing Compute Resources for Containters Resource Quality of Service in Kubernetes  Remark:\n This example works similarly when specifying a too high request for memory In case you configured an autoscaler range when creating your Kubernetes cluster another worker node will be started automatically if you didn\u0026rsquo;t reach the maximum number of worker nodes If your app is running out of memory (the memory settings are too small), you typically find OOMKilled (Out Of Memory) message in the Events section fo the kubectl describe pod ... output  Why was the container image not updated? You applied a fix in your app, created a new container image and pushed it into your container repository. After redeploying your Kubernetes manifests you expected to get the updated app, but still the same bug is in the new deployment present.\nThis behavior is related to how Kubernetes decides whether to pull a new docker image or to use the cached one.\nIn case you didn\u0026rsquo;t change the image tag, the default image policy IfNotPresent tells Kubernetes to use the cached image (see Images).\nAs a best practice you should not use the tag latest and change the image tag whenever you changed anything in your image (see Configuration Best Practices).\nFind more details in FAQ Container Image not updating\nLinks  Application Introspection and Debugging Debug Pods and Replication Controllers Logging Architecture Configure Default Memory Requests and Limits for a Namespace Managing Compute Resources for Containters Resource Quality of Service in Kubernetes Interactive Tutorial Troubleshooting with Kubectl Images Kubernetes Best Practises  "},{"uri":"https://gardener.cloud/v1.12.8/guides/monitoring_and_troubleshooting/debug-a-pod/","title":"How to debug a pod","tags":[],"description":"Your pod doesn&#39;t run as expected. Are there any log files? Where? How could I debug a pod?","content":"Introduction Kubernetes offers powerful options to get more details about startup or runtime failures of pods as e.g. described in Application Introspection and Debugging or Debug Pods and Replication Controllers.\nIn order to identify pods with potential issus you could e.g. run kubectl get pods --all-namespaces | grep -iv Running  to filter out the pods which are not in the state Running. One of frequent error state is CrashLoopBackOff, which tells that a pod crashes right after the start. Kubernetes then tries to restart the pod, but often the pod startup fails again.\nHere is a short list of possible reasons which might lead to a pod crash:\n error during image pull caused by e.g. wrong/missing secrets or wrong/missing image the app runs in an error state caused e.g. by missing environmental variables (ConfigMaps) or secrets liveness probe failed too high resource consumption (memory and/or CPU) or too strict quota settings persistent volumes can\u0026rsquo;t be created/mounted the container image is not updated  Basically, the commands kubectl logs ... and kubectl describe ... with additional parameters are used to get more detailed information. By calling e.g. kubectl logs --help you get more detailed information about the command and its parameters.\nIn the next sections you\u0026rsquo;ll find some basic approaches to get some ideas what went wrong.\nRemarks:\n Even if the pods seem to be running as the status Running indicates, a high counter of the Restarts shows potential problems There is as well an interactive Tutorial Troubleshooting with Kubectl available which explains basic debugging activities The examples below are deployed into the namespace default. In case you want to change it use the optional parameter --namespace \u0026lt;your-namespace\u0026gt; to select the target namespace. They require Kubernetes release  1.8.  Prerequisites Your deployment was successful (no logical/syntactical errors in the manifest files), but the pod(s) aren\u0026rsquo;t running.\nError caused by wrong image name You run kubectl describe pod \u0026lt;your-pod\u0026gt; \u0026lt;your-namespace\u0026gt; to get detailed information about the pod startup.\nIn the Events section, you get an error message like Failed to pull image ... and Reason: Failed. The pod is in state ImagePullBackOff.\nThe example below is based on demo in Kubernetes documentation. In all examples the default namespace is used.\nFirst, cleanup with\nkubectl delete pod termination-demo Next, create a resource based on the yaml content below\napiVersion:v1kind:Podmetadata:name:termination-demospec:containers:- name:termination-demo-containerimage:debianncommand:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log\u0026#34;]kubectl describe pod termination-demo lists the following content in the Event section\nEvents: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 2m\t2m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned termination-demo to ip-10-250-17-112.eu-west-1.compute.internal 2m\t2m\t1\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026#34;default-token-sgccm\u0026#34; 2m\t1m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulling\tpulling image \u0026#34;debiann\u0026#34; 2m\t1m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tWarning\tFailed\tFailed to pull image \u0026#34;debiann\u0026#34;: rpc error: code = Unknown desc = Error: image library/debiann:latest not found 2m\t54s\t10\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tWarning\tFailedSync\tError syncing pod 2m\t54s\t6\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tBackOff\tBack-off pulling image \u0026#34;debiann\u0026#34; The error message with Reason: Failed tells that there is an error during pulling the image. A closer look at the image name indicates a misspelling.\nApp runs in an error state caused by missing ConfigMaps or Secrets This example illustrates the behavior in case of the app expecting environment variables but the corresponding Kubernetes artifacts are missing.\nFirst, cleanup with\nkubectldeletedeploymenttermination-demokubectldeleteconfigmapsapp-envNext, deploy this manifest\napiVersion:apps/v1kind:Deploymentmetadata:name:termination-demolabels:app:termination-demospec:replicas:1selector:matchLabels:app:termination-demotemplate:metadata:labels:app:termination-demospec:containers:- name:termination-demo-containerimage:debiancommand:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;sed \\\u0026#34;s/foo/bar/\\\u0026#34; \u0026lt; $MYFILE\u0026#34;]Now, the command kubectl get pods lists the pod termination-demo-xxx in the state Error or CrashLoopBackOff. The command kubectl describe pod termination-demo-xxx tells that there is no error during startup but gives no clue about what caused the crash.\nEvents: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 19m\t19m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned termination-demo-5fb484867d-xz2x9 to ip-10-250-17-112.eu-west-1.compute.internal 19m\t19m\t1\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026#34;default-token-sgccm\u0026#34; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulling\tpulling image \u0026#34;debian\u0026#34; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulled\tSuccessfully pulled image \u0026#34;debian\u0026#34; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tCreated\tCreated container 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tStarted\tStarted container 19m\t14m\t24\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tWarning\tBackOff\tBack-off restarting failed container 19m\t4m\t69\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tWarning\tFailedSync\tError syncing pod The command kubectl get logs termination-demo-xxx gives access to the output, the application writes on stderr and stdout. In this case, you get an output like\n/bin/sh: 1: cannot open : No such file So you need to have a closer look at the application. In this case the environmental variable MYFILEis missing. To fix this issue you could e.g. add a ConfigMap to your deployment as it is shown in the manifest listed below.\napiVersion:v1kind:ConfigMapmetadata:name:app-envdata:MYFILE:\u0026#34;/etc/profile\u0026#34;---apiVersion:apps/v1kind:Deploymentmetadata:name:termination-demolabels:app:termination-demospec:replicas:1selector:matchLabels:app:termination-demotemplate:metadata:labels:app:termination-demospec:containers:- name:termination-demo-containerimage:debiancommand:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;sed \\\u0026#34;s/foo/bar/\\\u0026#34; \u0026lt; $MYFILE\u0026#34;]envFrom:- configMapRef:name:app-envNote that once you fix the error and re-run the scenario, you might still see the pod in CrashLoopBackOff status. It is because the container finishes the command sed ... and runs to completion. In order to keep the container in Running status, a long running task is required, e.g.\napiVersion:v1kind:ConfigMapmetadata:name:app-envdata:MYFILE:\u0026#34;/etc/profile\u0026#34;SLEEP:\u0026#34;5\u0026#34;---apiVersion:apps/v1kind:Deploymentmetadata:name:termination-demolabels:app:termination-demospec:replicas:1selector:matchLabels:app:termination-demotemplate:metadata:labels:app:termination-demospec:containers:- name:termination-demo-containerimage:debiancommand:[\u0026#34;/bin/sh\u0026#34;]# args: [\u0026#34;-c\u0026#34;, \u0026#34;sed \\\u0026#34;s/foo/bar/\\\u0026#34; \u0026lt; $MYFILE\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;while true; do sleep $SLEEP; echo sleeping; done;\u0026#34;]envFrom:- configMapRef:name:app-envToo high resource consumption or too strict quota settings You can optionally specify the amount of memory and/or CPU your container gets during runtime. In case these settings are missing, the default requests settings are taken: CPU: 0m (in Milli CPU) and RAM: 0Gi which indicate no other limits than the ones of the node(s) itself. Find more details in Configure Default Memory Requests and Limits for a Namespace,\nIn case your application needs more resources, Kubernetes distinguishes between requests and limit settings: requests specify the guaranteed amount of resource, whereas limit tells Kubernetes the maximum amount of resource the container might need. Mathematically both settings could be described by the relation 0 \u0026lt;= requests \u0026lt;= limit. For both settings you need to consider the total amount of resources the available nodes provide. For a detailed description of the concept see Resource Quality of Service in Kubernetes.\nUse kubectl describe nodes to get a first overview of the resource consumption of your cluster. Of special interest are the figures indicating the amount of CPU and Memory Requests at the bottom of the output.\nThe next example demonstrates what happens in case the CPU request is too high in order to be managed by your cluster.\nFirst, cleanup with\nkubectldeletedeploymenttermination-demokubectldeleteconfigmapsapp-envNext, adapt the cpu in the yaml below to be slightly higher than the remaining cpu resources in your cluster and deploy this manifest. In this example 600m (milli CPUs) are requested in a Kubernetes system with a single 2 Core worker node which results in an error message.\napiVersion:apps/v1kind:Deploymentmetadata:name:termination-demolabels:app:termination-demospec:replicas:1selector:matchLabels:app:termination-demotemplate:metadata:labels:app:termination-demospec:containers:- name:termination-demo-containerimage:debiancommand:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log\u0026#34;]resources:requests:cpu:\u0026#34;600m\u0026#34;The command kubectl get pods lists the pod termination-demo-xxx in the state Pending. More details on why this happens could be found by using the command kubectl describe pod termination-demo-xxx:\n$ kubectl describe po termination-demo-fdb7bb7d9-mzvfw Name: termination-demo-fdb7bb7d9-mzvfw Namespace: default ... Containers: termination-demo-container: Image: debian Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Command: /bin/sh Args: -c sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log Requests: cpu: 6 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-t549m (ro) Conditions: Type Status PodScheduled False Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 9s (x7 over 40s) default-scheduler 0/2 nodes are available: 2 Insufficient cpu. More details in\n Managing Compute Resources for Containters Resource Quality of Service in Kubernetes  Remark:\n This example works similarly when specifying a too high request for memory In case you configured an autoscaler range when creating your Kubernetes cluster another worker node will be started automatically if you didn\u0026rsquo;t reach the maximum number of worker nodes If your app is running out of memory (the memory settings are too small), you typically find OOMKilled (Out Of Memory) message in the Events section fo the kubectl describe pod ... output  Why was the container image not updated? You applied a fix in your app, created a new container image and pushed it into your container repository. After redeploying your Kubernetes manifests you expected to get the updated app, but still the same bug is in the new deployment present.\nThis behavior is related to how Kubernetes decides whether to pull a new docker image or to use the cached one.\nIn case you didn\u0026rsquo;t change the image tag, the default image policy IfNotPresent tells Kubernetes to use the cached image (see Images).\nAs a best practice you should not use the tag latest and change the image tag whenever you changed anything in your image (see Configuration Best Practices).\nFind more details in FAQ Container Image not updating\nLinks  Application Introspection and Debugging Debug Pods and Replication Controllers Logging Architecture Configure Default Memory Requests and Limits for a Namespace Managing Compute Resources for Containters Resource Quality of Service in Kubernetes Interactive Tutorial Troubleshooting with Kubectl Images Kubernetes Best Practises  "},{"uri":"https://gardener.cloud/v1.13.2/guides/monitoring_and_troubleshooting/debug-a-pod/","title":"How to debug a pod","tags":[],"description":"Your pod doesn&#39;t run as expected. Are there any log files? Where? How could I debug a pod?","content":"Introduction Kubernetes offers powerful options to get more details about startup or runtime failures of pods as e.g. described in Application Introspection and Debugging or Debug Pods and Replication Controllers.\nIn order to identify pods with potential issus you could e.g. run kubectl get pods --all-namespaces | grep -iv Running  to filter out the pods which are not in the state Running. One of frequent error state is CrashLoopBackOff, which tells that a pod crashes right after the start. Kubernetes then tries to restart the pod, but often the pod startup fails again.\nHere is a short list of possible reasons which might lead to a pod crash:\n error during image pull caused by e.g. wrong/missing secrets or wrong/missing image the app runs in an error state caused e.g. by missing environmental variables (ConfigMaps) or secrets liveness probe failed too high resource consumption (memory and/or CPU) or too strict quota settings persistent volumes can\u0026rsquo;t be created/mounted the container image is not updated  Basically, the commands kubectl logs ... and kubectl describe ... with additional parameters are used to get more detailed information. By calling e.g. kubectl logs --help you get more detailed information about the command and its parameters.\nIn the next sections you\u0026rsquo;ll find some basic approaches to get some ideas what went wrong.\nRemarks:\n Even if the pods seem to be running as the status Running indicates, a high counter of the Restarts shows potential problems There is as well an interactive Tutorial Troubleshooting with Kubectl available which explains basic debugging activities The examples below are deployed into the namespace default. In case you want to change it use the optional parameter --namespace \u0026lt;your-namespace\u0026gt; to select the target namespace. They require Kubernetes release  1.8.  Prerequisites Your deployment was successful (no logical/syntactical errors in the manifest files), but the pod(s) aren\u0026rsquo;t running.\nError caused by wrong image name You run kubectl describe pod \u0026lt;your-pod\u0026gt; \u0026lt;your-namespace\u0026gt; to get detailed information about the pod startup.\nIn the Events section, you get an error message like Failed to pull image ... and Reason: Failed. The pod is in state ImagePullBackOff.\nThe example below is based on demo in Kubernetes documentation. In all examples the default namespace is used.\nFirst, cleanup with\nkubectl delete pod termination-demo Next, create a resource based on the yaml content below\napiVersion:v1kind:Podmetadata:name:termination-demospec:containers:- name:termination-demo-containerimage:debianncommand:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log\u0026#34;]kubectl describe pod termination-demo lists the following content in the Event section\nEvents: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 2m\t2m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned termination-demo to ip-10-250-17-112.eu-west-1.compute.internal 2m\t2m\t1\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026#34;default-token-sgccm\u0026#34; 2m\t1m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulling\tpulling image \u0026#34;debiann\u0026#34; 2m\t1m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tWarning\tFailed\tFailed to pull image \u0026#34;debiann\u0026#34;: rpc error: code = Unknown desc = Error: image library/debiann:latest not found 2m\t54s\t10\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tWarning\tFailedSync\tError syncing pod 2m\t54s\t6\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tBackOff\tBack-off pulling image \u0026#34;debiann\u0026#34; The error message with Reason: Failed tells that there is an error during pulling the image. A closer look at the image name indicates a misspelling.\nApp runs in an error state caused by missing ConfigMaps or Secrets This example illustrates the behavior in case of the app expecting environment variables but the corresponding Kubernetes artifacts are missing.\nFirst, cleanup with\nkubectldeletedeploymenttermination-demokubectldeleteconfigmapsapp-envNext, deploy this manifest\napiVersion:apps/v1kind:Deploymentmetadata:name:termination-demolabels:app:termination-demospec:replicas:1selector:matchLabels:app:termination-demotemplate:metadata:labels:app:termination-demospec:containers:- name:termination-demo-containerimage:debiancommand:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;sed \\\u0026#34;s/foo/bar/\\\u0026#34; \u0026lt; $MYFILE\u0026#34;]Now, the command kubectl get pods lists the pod termination-demo-xxx in the state Error or CrashLoopBackOff. The command kubectl describe pod termination-demo-xxx tells that there is no error during startup but gives no clue about what caused the crash.\nEvents: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReason\tMessage ---------\t--------\t-----\t----\t-------------\t--------\t------\t------- 19m\t19m\t1\tdefault-scheduler\tNormal\tScheduled\tSuccessfully assigned termination-demo-5fb484867d-xz2x9 to ip-10-250-17-112.eu-west-1.compute.internal 19m\t19m\t1\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tNormal\tSuccessfulMountVolume\tMountVolume.SetUp succeeded for volume \u0026#34;default-token-sgccm\u0026#34; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulling\tpulling image \u0026#34;debian\u0026#34; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tPulled\tSuccessfully pulled image \u0026#34;debian\u0026#34; 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tCreated\tCreated container 19m\t19m\t4\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tNormal\tStarted\tStarted container 19m\t14m\t24\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tspec.containers{termination-demo-container}\tWarning\tBackOff\tBack-off restarting failed container 19m\t4m\t69\tkubelet, ip-10-250-17-112.eu-west-1.compute.internal\tWarning\tFailedSync\tError syncing pod The command kubectl get logs termination-demo-xxx gives access to the output, the application writes on stderr and stdout. In this case, you get an output like\n/bin/sh: 1: cannot open : No such file So you need to have a closer look at the application. In this case the environmental variable MYFILEis missing. To fix this issue you could e.g. add a ConfigMap to your deployment as it is shown in the manifest listed below.\napiVersion:v1kind:ConfigMapmetadata:name:app-envdata:MYFILE:\u0026#34;/etc/profile\u0026#34;---apiVersion:apps/v1kind:Deploymentmetadata:name:termination-demolabels:app:termination-demospec:replicas:1selector:matchLabels:app:termination-demotemplate:metadata:labels:app:termination-demospec:containers:- name:termination-demo-containerimage:debiancommand:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;sed \\\u0026#34;s/foo/bar/\\\u0026#34; \u0026lt; $MYFILE\u0026#34;]envFrom:- configMapRef:name:app-envNote that once you fix the error and re-run the scenario, you might still see the pod in CrashLoopBackOff status. It is because the container finishes the command sed ... and runs to completion. In order to keep the container in Running status, a long running task is required, e.g.\napiVersion:v1kind:ConfigMapmetadata:name:app-envdata:MYFILE:\u0026#34;/etc/profile\u0026#34;SLEEP:\u0026#34;5\u0026#34;---apiVersion:apps/v1kind:Deploymentmetadata:name:termination-demolabels:app:termination-demospec:replicas:1selector:matchLabels:app:termination-demotemplate:metadata:labels:app:termination-demospec:containers:- name:termination-demo-containerimage:debiancommand:[\u0026#34;/bin/sh\u0026#34;]# args: [\u0026#34;-c\u0026#34;, \u0026#34;sed \\\u0026#34;s/foo/bar/\\\u0026#34; \u0026lt; $MYFILE\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;while true; do sleep $SLEEP; echo sleeping; done;\u0026#34;]envFrom:- configMapRef:name:app-envToo high resource consumption or too strict quota settings You can optionally specify the amount of memory and/or CPU your container gets during runtime. In case these settings are missing, the default requests settings are taken: CPU: 0m (in Milli CPU) and RAM: 0Gi which indicate no other limits than the ones of the node(s) itself. Find more details in Configure Default Memory Requests and Limits for a Namespace,\nIn case your application needs more resources, Kubernetes distinguishes between requests and limit settings: requests specify the guaranteed amount of resource, whereas limit tells Kubernetes the maximum amount of resource the container might need. Mathematically both settings could be described by the relation 0 \u0026lt;= requests \u0026lt;= limit. For both settings you need to consider the total amount of resources the available nodes provide. For a detailed description of the concept see Resource Quality of Service in Kubernetes.\nUse kubectl describe nodes to get a first overview of the resource consumption of your cluster. Of special interest are the figures indicating the amount of CPU and Memory Requests at the bottom of the output.\nThe next example demonstrates what happens in case the CPU request is too high in order to be managed by your cluster.\nFirst, cleanup with\nkubectldeletedeploymenttermination-demokubectldeleteconfigmapsapp-envNext, adapt the cpu in the yaml below to be slightly higher than the remaining cpu resources in your cluster and deploy this manifest. In this example 600m (milli CPUs) are requested in a Kubernetes system with a single 2 Core worker node which results in an error message.\napiVersion:apps/v1kind:Deploymentmetadata:name:termination-demolabels:app:termination-demospec:replicas:1selector:matchLabels:app:termination-demotemplate:metadata:labels:app:termination-demospec:containers:- name:termination-demo-containerimage:debiancommand:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log\u0026#34;]resources:requests:cpu:\u0026#34;600m\u0026#34;The command kubectl get pods lists the pod termination-demo-xxx in the state Pending. More details on why this happens could be found by using the command kubectl describe pod termination-demo-xxx:\n$ kubectl describe po termination-demo-fdb7bb7d9-mzvfw Name: termination-demo-fdb7bb7d9-mzvfw Namespace: default ... Containers: termination-demo-container: Image: debian Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Command: /bin/sh Args: -c sleep 10 \u0026amp;\u0026amp; echo Sleep expired \u0026gt; /dev/termination-log Requests: cpu: 6 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-t549m (ro) Conditions: Type Status PodScheduled False Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 9s (x7 over 40s) default-scheduler 0/2 nodes are available: 2 Insufficient cpu. More details in\n Managing Compute Resources for Containters Resource Quality of Service in Kubernetes  Remark:\n This example works similarly when specifying a too high request for memory In case you configured an autoscaler range when creating your Kubernetes cluster another worker node will be started automatically if you didn\u0026rsquo;t reach the maximum number of worker nodes If your app is running out of memory (the memory settings are too small), you typically find OOMKilled (Out Of Memory) message in the Events section fo the kubectl describe pod ... output  Why was the container image not updated? You applied a fix in your app, created a new container image and pushed it into your container repository. After redeploying your Kubernetes manifests you expected to get the updated app, but still the same bug is in the new deployment present.\nThis behavior is related to how Kubernetes decides whether to pull a new docker image or to use the cached one.\nIn case you didn\u0026rsquo;t change the image tag, the default image policy IfNotPresent tells Kubernetes to use the cached image (see Images).\nAs a best practice you should not use the tag latest and change the image tag whenever you changed anything in your image (see Configuration Best Practices).\nFind more details in FAQ Container Image not updating\nLinks  Application Introspection and Debugging Debug Pods and Replication Controllers Logging Architecture Configure Default Memory Requests and Limits for a Namespace Managing Compute Resources for Containters Resource Quality of Service in Kubernetes Interactive Tutorial Troubleshooting with Kubectl Images Kubernetes Best Practises  "},{"uri":"https://gardener.cloud/documentation/guides/applications/https/","title":"HTTPS with self Signed Certificate","tags":[],"description":"","content":"Configuring ingress with front-end TLS It is alyways recommended to enable encryption for services to prevent traffic interception and man-in-the-middle attacks - even in DEV environments.\nYou should configure front-end Transport Layer Security (TLS) so that the ingress controller can secure access to a service from the client to the load balancer by using HTTPS.\nWe will use basic procedure here. If your configuration requires advanced security options, please refer to official CloudFlare\u0026rsquo;s cfssl documentation.\nBefore you begin At first, you need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using Gardener\nInstall CFSSL The first step in securing Docker and Kubernetes is to set up a PKI infrastructure for managing TLS certificates.\nInitialize a CA Before we can generate any certs we need to initialize a CA.\nmkdir cfssl cd cfssl cfssl print-defaults config \u0026gt; ca-config.json cfssl print-defaults csr \u0026gt; ca-csr.json Configure CA options Now we can configure signing options inside ca-config.json config file. Default options contain following preconfigured fields:\n profiles: www with server auth (TLS Web Server Authentication) X509 V3 extension and client with client auth (TLS Web Client Authentication) X509 V3 extension. expiry: with 8760h default value (or 365 days)  For compliance let\u0026rsquo;s edit the ca-config.json file and rename www profile into server\nEdit the ca-csr.json to your needs. See example below. Keep in mind that the hosts entries must match all your ingress entries.\nexample ca-csr.json\n{ \u0026#34;CN\u0026#34;: \u0026#34;Gardener Self Signed CA\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;, \u0026#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;San Francisco\u0026#34; } ] } And generate CA with defined options:\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca - You\u0026rsquo;ll get following files:\n ca-key.pem ca.csr ca.pem  Note: Please keep ca-key.pem file in safe. This key allows to create any kind of certificates within your CA.\nGenerate server certificate cfssl print-defaults csr \u0026gt; server.json Most important values for server certificate are Common Name (CN) and hosts. We have to substitute them, for example:\n{ \u0026#34;CN\u0026#34;: \u0026#34;Gardener Self Signed CA\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;, \u0026#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;San Francisco\u0026#34; } ] } Now we are ready to generate server certificate and private key:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server You\u0026rsquo;ll get following files:\n server-key.pem server.csr server.pem  Configure Kubernetes ingress with TLS To configure front-end TLS, you need to create a TLS certificate (already done above), create a Kubernetes secret, update applicable .yaml files, apply your .yaml file changes, regenerate ingress controllers, and visit the application.\nCreate Kubernetes secret kubectl create secret tls tls-secret --key ./server-key.pem --cert server.pem Create Service / Ingress now you can referenc ethe TLS secret within your ingress definition\nexample ingress definition\napiVersion:v1kind:Servicemetadata:labels:app:node-servername:node-svcnamespace:defaultspec:type:NodePortports:- port:8080selector:app:node-server---apiVersion:extensions/v1beta1kind:Ingressmetadata:name:node-ingressspec:tls:- hosts:- ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.comsecretName:tls-secretrules:- host:ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:node-svcservicePort:8080"},{"uri":"https://gardener.cloud/documentation/guides/applications/https/","title":"HTTPS with self Signed Certificate","tags":[],"description":"","content":"Configuring ingress with front-end TLS It is always recommended to enable encryption for services to prevent traffic interception and man-in-the-middle attacks - even in DEV environments.\nYou should configure front-end Transport Layer Security (TLS) so that the ingress controller can secure access to a service from the client to the load balancer by using HTTPS.\nWe will use basic procedure here. If your configuration requires advanced security options, please refer to official CloudFlare\u0026rsquo;s cfssl documentation.\nBefore you begin At first, you need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using Gardener\nInstall CFSSL The first step in securing Docker and Kubernetes is to set up a PKI infrastructure for managing TLS certificates.\nInitialize a CA Before we can generate any certs we need to initialize a CA.\nmkdir cfssl cd cfssl cfssl print-defaults config \u0026gt; ca-config.json cfssl print-defaults csr \u0026gt; ca-csr.json Configure CA options Now we can configure signing options inside ca-config.json config file. Default options contain following preconfigured fields:\n profiles: www with server auth (TLS Web Server Authentication) X509 V3 extension and client with client auth (TLS Web Client Authentication) X509 V3 extension. expiry: with 8760h default value (or 365 days)  For compliance let\u0026rsquo;s edit the ca-config.json file and rename www profile into server\nEdit the ca-csr.json to your needs. See example below. Keep in mind that the hosts entries must match all your ingress entries.\nexample ca-csr.json\n{ \u0026#34;CN\u0026#34;: \u0026#34;Gardener Self Signed CA\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;, \u0026#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;San Francisco\u0026#34; } ] } And generate CA with defined options:\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca - You\u0026rsquo;ll get following files:\n ca-key.pem ca.csr ca.pem  Note: Please keep ca-key.pem file in safe. This key allows to create any kind of certificates within your CA.\nGenerate server certificate cfssl print-defaults csr \u0026gt; server.json Most important values for server certificate are Common Name (CN) and hosts. We have to substitute them, for example:\n{ \u0026#34;CN\u0026#34;: \u0026#34;Gardener Self Signed CA\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;, \u0026#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;San Francisco\u0026#34; } ] } Now we are ready to generate server certificate and private key:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server You\u0026rsquo;ll get following files:\n server-key.pem server.csr server.pem  Configure Kubernetes ingress with TLS To configure front-end TLS, you need to create a TLS certificate (already done above), create a Kubernetes secret, update applicable .yaml files, apply your .yaml file changes, regenerate ingress controllers, and visit the application.\nCreate Kubernetes secret kubectl create secret tls tls-secret --key ./server-key.pem --cert server.pem Create Service / Ingress now you can referenc ethe TLS secret within your ingress definition\nexample ingress definition\napiVersion:v1kind:Servicemetadata:labels:app:node-servername:node-svcnamespace:defaultspec:type:NodePortports:- port:8080selector:app:node-server---apiVersion:extensions/v1beta1kind:Ingressmetadata:name:node-ingressspec:tls:- hosts:- ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.comsecretName:tls-secretrules:- host:ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:node-svcservicePort:8080"},{"uri":"https://gardener.cloud/v1.12.8/guides/applications/https/","title":"HTTPS with self Signed Certificate","tags":[],"description":"","content":"Configuring ingress with front-end TLS It is alyways recommended to enable encryption for services to prevent traffic interception and man-in-the-middle attacks - even in DEV environments.\nYou should configure front-end Transport Layer Security (TLS) so that the ingress controller can secure access to a service from the client to the load balancer by using HTTPS.\nWe will use basic procedure here. If your configuration requires advanced security options, please refer to official CloudFlare\u0026rsquo;s cfssl documentation.\nBefore you begin At first, you need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using Gardener\nInstall CFSSL The first step in securing Docker and Kubernetes is to set up a PKI infrastructure for managing TLS certificates.\nInitialize a CA Before we can generate any certs we need to initialize a CA.\nmkdir cfssl cd cfssl cfssl print-defaults config \u0026gt; ca-config.json cfssl print-defaults csr \u0026gt; ca-csr.json Configure CA options Now we can configure signing options inside ca-config.json config file. Default options contain following preconfigured fields:\n profiles: www with server auth (TLS Web Server Authentication) X509 V3 extension and client with client auth (TLS Web Client Authentication) X509 V3 extension. expiry: with 8760h default value (or 365 days)  For compliance let\u0026rsquo;s edit the ca-config.json file and rename www profile into server\nEdit the ca-csr.json to your needs. See example below. Keep in mind that the hosts entries must match all your ingress entries.\nexample ca-csr.json\n{ \u0026#34;CN\u0026#34;: \u0026#34;Gardener Self Signed CA\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;, \u0026#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;San Francisco\u0026#34; } ] } And generate CA with defined options:\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca - You\u0026rsquo;ll get following files:\n ca-key.pem ca.csr ca.pem  Note: Please keep ca-key.pem file in safe. This key allows to create any kind of certificates within your CA.\nGenerate server certificate cfssl print-defaults csr \u0026gt; server.json Most important values for server certificate are Common Name (CN) and hosts. We have to substitute them, for example:\n{ \u0026#34;CN\u0026#34;: \u0026#34;Gardener Self Signed CA\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;, \u0026#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;San Francisco\u0026#34; } ] } Now we are ready to generate server certificate and private key:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server You\u0026rsquo;ll get following files:\n server-key.pem server.csr server.pem  Configure Kubernetes ingress with TLS To configure front-end TLS, you need to create a TLS certificate (already done above), create a Kubernetes secret, update applicable .yaml files, apply your .yaml file changes, regenerate ingress controllers, and visit the application.\nCreate Kubernetes secret kubectl create secret tls tls-secret --key ./server-key.pem --cert server.pem Create Service / Ingress now you can referenc ethe TLS secret within your ingress definition\nexample ingress definition\napiVersion:v1kind:Servicemetadata:labels:app:node-servername:node-svcnamespace:defaultspec:type:NodePortports:- port:8080selector:app:node-server---apiVersion:extensions/v1beta1kind:Ingressmetadata:name:node-ingressspec:tls:- hosts:- ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.comsecretName:tls-secretrules:- host:ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:node-svcservicePort:8080"},{"uri":"https://gardener.cloud/v1.12.8/guides/applications/https/","title":"HTTPS with self Signed Certificate","tags":[],"description":"","content":"Configuring ingress with front-end TLS It is always recommended to enable encryption for services to prevent traffic interception and man-in-the-middle attacks - even in DEV environments.\nYou should configure front-end Transport Layer Security (TLS) so that the ingress controller can secure access to a service from the client to the load balancer by using HTTPS.\nWe will use basic procedure here. If your configuration requires advanced security options, please refer to official CloudFlare\u0026rsquo;s cfssl documentation.\nBefore you begin At first, you need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using Gardener\nInstall CFSSL The first step in securing Docker and Kubernetes is to set up a PKI infrastructure for managing TLS certificates.\nInitialize a CA Before we can generate any certs we need to initialize a CA.\nmkdir cfssl cd cfssl cfssl print-defaults config \u0026gt; ca-config.json cfssl print-defaults csr \u0026gt; ca-csr.json Configure CA options Now we can configure signing options inside ca-config.json config file. Default options contain following preconfigured fields:\n profiles: www with server auth (TLS Web Server Authentication) X509 V3 extension and client with client auth (TLS Web Client Authentication) X509 V3 extension. expiry: with 8760h default value (or 365 days)  For compliance let\u0026rsquo;s edit the ca-config.json file and rename www profile into server\nEdit the ca-csr.json to your needs. See example below. Keep in mind that the hosts entries must match all your ingress entries.\nexample ca-csr.json\n{ \u0026#34;CN\u0026#34;: \u0026#34;Gardener Self Signed CA\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;, \u0026#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;San Francisco\u0026#34; } ] } And generate CA with defined options:\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca - You\u0026rsquo;ll get following files:\n ca-key.pem ca.csr ca.pem  Note: Please keep ca-key.pem file in safe. This key allows to create any kind of certificates within your CA.\nGenerate server certificate cfssl print-defaults csr \u0026gt; server.json Most important values for server certificate are Common Name (CN) and hosts. We have to substitute them, for example:\n{ \u0026#34;CN\u0026#34;: \u0026#34;Gardener Self Signed CA\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;, \u0026#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;San Francisco\u0026#34; } ] } Now we are ready to generate server certificate and private key:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server You\u0026rsquo;ll get following files:\n server-key.pem server.csr server.pem  Configure Kubernetes ingress with TLS To configure front-end TLS, you need to create a TLS certificate (already done above), create a Kubernetes secret, update applicable .yaml files, apply your .yaml file changes, regenerate ingress controllers, and visit the application.\nCreate Kubernetes secret kubectl create secret tls tls-secret --key ./server-key.pem --cert server.pem Create Service / Ingress now you can referenc ethe TLS secret within your ingress definition\nexample ingress definition\napiVersion:v1kind:Servicemetadata:labels:app:node-servername:node-svcnamespace:defaultspec:type:NodePortports:- port:8080selector:app:node-server---apiVersion:extensions/v1beta1kind:Ingressmetadata:name:node-ingressspec:tls:- hosts:- ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.comsecretName:tls-secretrules:- host:ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:node-svcservicePort:8080"},{"uri":"https://gardener.cloud/v1.13.2/guides/applications/https/","title":"HTTPS with self Signed Certificate","tags":[],"description":"","content":"Configuring ingress with front-end TLS It is alyways recommended to enable encryption for services to prevent traffic interception and man-in-the-middle attacks - even in DEV environments.\nYou should configure front-end Transport Layer Security (TLS) so that the ingress controller can secure access to a service from the client to the load balancer by using HTTPS.\nWe will use basic procedure here. If your configuration requires advanced security options, please refer to official CloudFlare\u0026rsquo;s cfssl documentation.\nBefore you begin At first, you need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using Gardener\nInstall CFSSL The first step in securing Docker and Kubernetes is to set up a PKI infrastructure for managing TLS certificates.\nInitialize a CA Before we can generate any certs we need to initialize a CA.\nmkdir cfssl cd cfssl cfssl print-defaults config \u0026gt; ca-config.json cfssl print-defaults csr \u0026gt; ca-csr.json Configure CA options Now we can configure signing options inside ca-config.json config file. Default options contain following preconfigured fields:\n profiles: www with server auth (TLS Web Server Authentication) X509 V3 extension and client with client auth (TLS Web Client Authentication) X509 V3 extension. expiry: with 8760h default value (or 365 days)  For compliance let\u0026rsquo;s edit the ca-config.json file and rename www profile into server\nEdit the ca-csr.json to your needs. See example below. Keep in mind that the hosts entries must match all your ingress entries.\nexample ca-csr.json\n{ \u0026#34;CN\u0026#34;: \u0026#34;Gardener Self Signed CA\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;, \u0026#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;San Francisco\u0026#34; } ] } And generate CA with defined options:\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca - You\u0026rsquo;ll get following files:\n ca-key.pem ca.csr ca.pem  Note: Please keep ca-key.pem file in safe. This key allows to create any kind of certificates within your CA.\nGenerate server certificate cfssl print-defaults csr \u0026gt; server.json Most important values for server certificate are Common Name (CN) and hosts. We have to substitute them, for example:\n{ \u0026#34;CN\u0026#34;: \u0026#34;Gardener Self Signed CA\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;, \u0026#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;San Francisco\u0026#34; } ] } Now we are ready to generate server certificate and private key:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server You\u0026rsquo;ll get following files:\n server-key.pem server.csr server.pem  Configure Kubernetes ingress with TLS To configure front-end TLS, you need to create a TLS certificate (already done above), create a Kubernetes secret, update applicable .yaml files, apply your .yaml file changes, regenerate ingress controllers, and visit the application.\nCreate Kubernetes secret kubectl create secret tls tls-secret --key ./server-key.pem --cert server.pem Create Service / Ingress now you can referenc ethe TLS secret within your ingress definition\nexample ingress definition\napiVersion:v1kind:Servicemetadata:labels:app:node-servername:node-svcnamespace:defaultspec:type:NodePortports:- port:8080selector:app:node-server---apiVersion:extensions/v1beta1kind:Ingressmetadata:name:node-ingressspec:tls:- hosts:- ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.comsecretName:tls-secretrules:- host:ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:node-svcservicePort:8080"},{"uri":"https://gardener.cloud/v1.13.2/guides/applications/https/","title":"HTTPS with self Signed Certificate","tags":[],"description":"","content":"Configuring ingress with front-end TLS It is always recommended to enable encryption for services to prevent traffic interception and man-in-the-middle attacks - even in DEV environments.\nYou should configure front-end Transport Layer Security (TLS) so that the ingress controller can secure access to a service from the client to the load balancer by using HTTPS.\nWe will use basic procedure here. If your configuration requires advanced security options, please refer to official CloudFlare\u0026rsquo;s cfssl documentation.\nBefore you begin At first, you need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using Gardener\nInstall CFSSL The first step in securing Docker and Kubernetes is to set up a PKI infrastructure for managing TLS certificates.\nInitialize a CA Before we can generate any certs we need to initialize a CA.\nmkdir cfssl cd cfssl cfssl print-defaults config \u0026gt; ca-config.json cfssl print-defaults csr \u0026gt; ca-csr.json Configure CA options Now we can configure signing options inside ca-config.json config file. Default options contain following preconfigured fields:\n profiles: www with server auth (TLS Web Server Authentication) X509 V3 extension and client with client auth (TLS Web Client Authentication) X509 V3 extension. expiry: with 8760h default value (or 365 days)  For compliance let\u0026rsquo;s edit the ca-config.json file and rename www profile into server\nEdit the ca-csr.json to your needs. See example below. Keep in mind that the hosts entries must match all your ingress entries.\nexample ca-csr.json\n{ \u0026#34;CN\u0026#34;: \u0026#34;Gardener Self Signed CA\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;, \u0026#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;San Francisco\u0026#34; } ] } And generate CA with defined options:\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca - You\u0026rsquo;ll get following files:\n ca-key.pem ca.csr ca.pem  Note: Please keep ca-key.pem file in safe. This key allows to create any kind of certificates within your CA.\nGenerate server certificate cfssl print-defaults csr \u0026gt; server.json Most important values for server certificate are Common Name (CN) and hosts. We have to substitute them, for example:\n{ \u0026#34;CN\u0026#34;: \u0026#34;Gardener Self Signed CA\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34;, \u0026#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com\u0026#34; ], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;San Francisco\u0026#34; } ] } Now we are ready to generate server certificate and private key:\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server You\u0026rsquo;ll get following files:\n server-key.pem server.csr server.pem  Configure Kubernetes ingress with TLS To configure front-end TLS, you need to create a TLS certificate (already done above), create a Kubernetes secret, update applicable .yaml files, apply your .yaml file changes, regenerate ingress controllers, and visit the application.\nCreate Kubernetes secret kubectl create secret tls tls-secret --key ./server-key.pem --cert server.pem Create Service / Ingress now you can referenc ethe TLS secret within your ingress definition\nexample ingress definition\napiVersion:v1kind:Servicemetadata:labels:app:node-servername:node-svcnamespace:defaultspec:type:NodePortports:- port:8080selector:app:node-server---apiVersion:extensions/v1beta1kind:Ingressmetadata:name:node-ingressspec:tls:- hosts:- ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.comsecretName:tls-secretrules:- host:ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:node-svcservicePort:8080"},{"uri":"https://gardener.cloud/documentation/concepts/deployment/image_vector/","title":"Image Vector","tags":[],"description":"","content":"Image Vector The Gardenlet is deploying several different container images into the seed and the shoot clusters. The image repositories and tags are defined in a central image vector file. Obviously, the image versions defined there must fit together with the deployment manifests (e.g., some command-line flags do only exist in certain versions).\nExample images:- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:gcr.io/google_containers/pause-amd64tag:\u0026#34;3.0\u0026#34;version:1.11.x- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:gcr.io/google_containers/pause-amd64tag:\u0026#34;3.1\u0026#34;version:\u0026#34;\u0026gt;= 1.12\u0026#34;...That means that the Gardenlet will use the pause-container in with tag 3.0 for all seed/shoot clusters with Kubernetes version 1.11.x, and tag 3.1 for all clusters with Kubernetes \u0026gt;= 1.12.\nOverwrite image vector In some environment it is not possible to use these \u0026ldquo;pre-defined\u0026rdquo; images that come with a Gardener release. A prominent example for that is Alicloud in China which does not allow access to Google\u0026rsquo;s GCR. In these cases you might want to overwrite certain images, e.g., point the pause-container to a different registry.\n:warning: If you specify an image that does not fit to the resource manifest then the seed/shoot reconciliation might fail.\nIn order to overwrite the images you must provide a similar file to Gardenlet:\nimages:- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:my-custom-image-registry/pause-amd64tag:\u0026#34;3.0\u0026#34;version:1.11.x- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:my-custom-image-registry/pause-amd64tag:\u0026#34;3.1\u0026#34;version:\u0026#34;\u0026gt;= 1.12\u0026#34;...During deployment of the gardenlet create a ConfigMap containing the above content and mount it as a volume into the gardenlet pod. Next, specify the environment variable IMAGEVECTOR_OVERWRITE whose value must be the path to the file you just mounted:\napiVersion:v1kind:ConfigMapmetadata:name:gardenlet-images-overwritenamespace:gardendata:images_overwrite.yaml:| images:- ...---apiVersion:apps/v1kind:Deploymentmetadata:name:gardenletnamespace:gardenspec:template:...spec:containers:- name:gardenletenv:- name:IMAGEVECTOR_OVERWRITEvalue:/charts-overwrite/images_overwrite.yamlvolumeMounts:- name:gardenlet-images-overwritemountPath:/charts-overwrite...volumes:- name:gardenlet-images-overwriteconfigMap:name:gardenlet-images-overwrite...Image vectors for dependent components The gardenlet is deploying a lot of different components that might deploy other images themselves. These components might use an image vector as well. Operators might want to customize the image locations for these transitive images as well, hence, they might need to specify an image vector overwrite for the components directly deployed by Gardener.\nIt is possible to specify the IMAGEVECTOR_OVERWRITE_COMPONENTS environment variable to the gardenlet that points to a file with the following content:\ncomponents:- name:etcd-druidimageVectorOverwrite:| images:- name:etcdtag:v1.2.3repository:etcd/etcd...The gardenlet will, if supported by the directly deployed component (etcd-druid in this example), inject the given imageVectorOverwrite into the Deployment manifest. The respective component is responsible for using the overwritten images instead of its defaults.\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/deployment/image_vector/","title":"Image Vector","tags":[],"description":"","content":"Image Vector The Gardenlet is deploying several different container images into the seed and the shoot clusters. The image repositories and tags are defined in a central image vector file. Obviously, the image versions defined there must fit together with the deployment manifests (e.g., some command-line flags do only exist in certain versions).\nExample images:- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:gcr.io/google_containers/pause-amd64tag:\u0026#34;3.0\u0026#34;version:1.11.x- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:gcr.io/google_containers/pause-amd64tag:\u0026#34;3.1\u0026#34;version:\u0026#34;\u0026gt;= 1.12\u0026#34;...That means that the Gardenlet will use the pause-container in with tag 3.0 for all seed/shoot clusters with Kubernetes version 1.11.x, and tag 3.1 for all clusters with Kubernetes \u0026gt;= 1.12.\nOverwrite image vector In some environment it is not possible to use these \u0026ldquo;pre-defined\u0026rdquo; images that come with a Gardener release. A prominent example for that is Alicloud in China which does not allow access to Google\u0026rsquo;s GCR. In these cases you might want to overwrite certain images, e.g., point the pause-container to a different registry.\n:warning: If you specify an image that does not fit to the resource manifest then the seed/shoot reconciliation might fail.\nIn order to overwrite the images you must provide a similar file to Gardenlet:\nimages:- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:my-custom-image-registry/pause-amd64tag:\u0026#34;3.0\u0026#34;version:1.11.x- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:my-custom-image-registry/pause-amd64tag:\u0026#34;3.1\u0026#34;version:\u0026#34;\u0026gt;= 1.12\u0026#34;...During deployment of the gardenlet create a ConfigMap containing the above content and mount it as a volume into the gardenlet pod. Next, specify the environment variable IMAGEVECTOR_OVERWRITE whose value must be the path to the file you just mounted:\napiVersion:v1kind:ConfigMapmetadata:name:gardenlet-images-overwritenamespace:gardendata:images_overwrite.yaml:| images:- ...---apiVersion:apps/v1kind:Deploymentmetadata:name:gardenletnamespace:gardenspec:template:...spec:containers:- name:gardenletenv:- name:IMAGEVECTOR_OVERWRITEvalue:/charts-overwrite/images_overwrite.yamlvolumeMounts:- name:gardenlet-images-overwritemountPath:/charts-overwrite...volumes:- name:gardenlet-images-overwriteconfigMap:name:gardenlet-images-overwrite...Image vectors for dependent components The gardenlet is deploying a lot of different components that might deploy other images themselves. These components might use an image vector as well. Operators might want to customize the image locations for these transitive images as well, hence, they might need to specify an image vector overwrite for the components directly deployed by Gardener.\nIt is possible to specify the IMAGEVECTOR_OVERWRITE_COMPONENTS environment variable to the gardenlet that points to a file with the following content:\ncomponents:- name:etcd-druidimageVectorOverwrite:| images:- name:etcdtag:v1.2.3repository:etcd/etcd...The gardenlet will, if supported by the directly deployed component (etcd-druid in this example), inject the given imageVectorOverwrite into the Deployment manifest. The respective component is responsible for using the overwritten images instead of its defaults.\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/deployment/image_vector/","title":"Image Vector","tags":[],"description":"","content":"Image Vector The Gardenlet is deploying several different container images into the seed and the shoot clusters. The image repositories and tags are defined in a central image vector file. Obviously, the image versions defined there must fit together with the deployment manifests (e.g., some command-line flags do only exist in certain versions).\nExample images:- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:gcr.io/google_containers/pause-amd64tag:\u0026#34;3.0\u0026#34;version:1.11.x- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:gcr.io/google_containers/pause-amd64tag:\u0026#34;3.1\u0026#34;version:\u0026#34;\u0026gt;= 1.12\u0026#34;...That means that the Gardenlet will use the pause-container in with tag 3.0 for all seed/shoot clusters with Kubernetes version 1.11.x, and tag 3.1 for all clusters with Kubernetes \u0026gt;= 1.12.\nOverwrite image vector In some environment it is not possible to use these \u0026ldquo;pre-defined\u0026rdquo; images that come with a Gardener release. A prominent example for that is Alicloud in China which does not allow access to Google\u0026rsquo;s GCR. In these cases you might want to overwrite certain images, e.g., point the pause-container to a different registry.\n:warning: If you specify an image that does not fit to the resource manifest then the seed/shoot reconciliation might fail.\nIn order to overwrite the images you must provide a similar file to Gardenlet:\nimages:- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:my-custom-image-registry/pause-amd64tag:\u0026#34;3.0\u0026#34;version:1.11.x- name:pause-containersourceRepository:github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfilerepository:my-custom-image-registry/pause-amd64tag:\u0026#34;3.1\u0026#34;version:\u0026#34;\u0026gt;= 1.12\u0026#34;...During deployment of the gardenlet create a ConfigMap containing the above content and mount it as a volume into the gardenlet pod. Next, specify the environment variable IMAGEVECTOR_OVERWRITE whose value must be the path to the file you just mounted:\napiVersion:v1kind:ConfigMapmetadata:name:gardenlet-images-overwritenamespace:gardendata:images_overwrite.yaml:| images:- ...---apiVersion:apps/v1kind:Deploymentmetadata:name:gardenletnamespace:gardenspec:template:...spec:containers:- name:gardenletenv:- name:IMAGEVECTOR_OVERWRITEvalue:/charts-overwrite/images_overwrite.yamlvolumeMounts:- name:gardenlet-images-overwritemountPath:/charts-overwrite...volumes:- name:gardenlet-images-overwriteconfigMap:name:gardenlet-images-overwrite...Image vectors for dependent components The gardenlet is deploying a lot of different components that might deploy other images themselves. These components might use an image vector as well. Operators might want to customize the image locations for these transitive images as well, hence, they might need to specify an image vector overwrite for the components directly deployed by Gardener.\nIt is possible to specify the IMAGEVECTOR_OVERWRITE_COMPONENTS environment variable to the gardenlet that points to a file with the following content:\ncomponents:- name:etcd-druidimageVectorOverwrite:| images:- name:etcdtag:v1.2.3repository:etcd/etcd...The gardenlet will, if supported by the directly deployed component (etcd-druid in this example), inject the given imageVectorOverwrite into the Deployment manifest. The respective component is responsible for using the overwritten images instead of its defaults.\n"},{"uri":"https://gardener.cloud/documentation/concepts/core-components/api-server/apiserver_admission_plugins/","title":"In-Tree Admission Plugins","tags":[],"description":"","content":"Admission Plugins Similar to the kube-apiserver, the gardener-apiserver comes with a few in-tree managed admission plugins. If you want to get an overview of the what and why of admission plugins then this document might be a good start.\nThis document lists all existing admission plugins with a short explanation of what it is responsible for.\nClusterOpenIDConnectPreset, OpenIDConnectPreset (both enabled by default)\nThese admission controllers react on CREATE operations for Shoots. If the Shoot does not specify any OIDC configuration (.spec.kubernetes.kubeAPIServer.oidcConfig=nil) then it tries to find a matching ClusterOpenIDConnectPreset or OpenIDConnectPreset, respectively. If there are multiples that match then the one with the highest weight \u0026ldquo;wins\u0026rdquo;. In this case, the admission controller will default the OIDC configuration in the Shoot.\nControllerRegistrationResources (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for ControllerRegistrations. It validates that there exists only one ControllerRegistration in the system that is primarily responsible for a given kind/type resource combination. This prevents misconfiguration by the Gardener administrator/operator.\nCustomVerbAuthorizer (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Projects. It validates whether the user is bound to a RBAC role with the modify-spec-tolerations-whitelist verb in case the user tries to change the .spec.tolerations.whitelist field of the respective Project resource. Usually, regular project members are not bound to this custom verb, allowing the Gardener administrator to manage certain toleration whitelists on Project basis.\nDeletionConfirmation (enabled by default)\nThis admission controller reacts on DELETE operations for Projects and Shoots. It validates that the respective resource is annotated with a deletion confirmation annotation, namely confirmation.gardener.cloud/deletion=true. Only if this annotation is present it allows the DELETE operation to pass. This prevents users from accidental/undesired deletions.\nExtensionValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for BackupEntrys, BackupBuckets, Seeds, and Shoots. For all the various extension types in the specifications of these objects, it validates whether there exists a ControllerRegistration in the system that is primarily responsible for the stated extension type(s). This prevents misconfigurations that would otherwise allow users to create such resources with extension types that don\u0026rsquo;t exist in the cluster, effectively leading to failing reconciliation loops.\nPlantValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Plants. It sets the gardener.cloud/created-by annotation for newly created Plant resources. Also, it prevents creating new Plant resources in Projects that are already have a deletion timestamp.\nResourceQuota (enabled by default)\nThis admission controller enables object count ResourceQuotas for Gardener resources, e.g. Shoots, SecretBindings, Projects, etc..\n :warning: In addition to this admission plugin, the ResourceQuota controller must be enabled for the Kube-Controller-Manager of your Garden cluster.\n ResourceReferenceManager (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for CloudProfiles, Projects, SecretBindings, Seeds, and Shoots. Generally, it checks whether referred resources stated in the specifications of these objects exist in the system (e.g., if a referenced Secret exists). However, it also has some special behaviours for certain resources:\n CloudProfiles: It rejects removing Kubernetes or machine image versions if there is at least one Shoot that refers to them. Projects: It sets the .spec.createdBy field for newly created Project resources, and defaults the .spec.owner field in case it is empty (to the same value of .spec.createdBy). Seeds: It rejects changing the .spec.settings.shootDNS.enabled value if there is at least one Shoot that refers to this seed. Shoots: It sets the gardener.cloud/created-by=\u0026lt;username\u0026gt; annotation for newly created Shoot resources.  SeedValidator (enabled by default)\nThis admission controller reacts on DELETE operations for Seeds. Rejects the deletion if Shoot(s) reference the seed cluster.\nShootDNS (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It tries to assign a default domain to the Shoot if it gets scheduled to a seed that enables DNS for shoots (.spec.settings.shootDNS.enabled=true). It also validates that the DNS configuration (.spec.dns) is not set if the seed disables DNS for shoots.\nShootQuotaValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It validates the resource consumption declared in the specification against applicable Quota resources. Only if the applicable Quota resources admit the configured resources in the Shoot then it allows the request. Applicable Quotas are referred in the SecretBinding that is used by the Shoot.\nShootStateDeletionValidator (enabled by default)\nThis admission controller reacts on DELETE operations for ShootStates. It prevents the deletion of the respective ShootState resource in case the corresponding Shoot resource does still exist in the system. This prevents losing the shoot\u0026rsquo;s data required to recover it / migrate its control plane to a new seed cluster.\nShootTolerationRestriction (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It validates the .spec.tolerations used in Shoots against the whitelist of its Project, or against the whitelist configured in the admission controller\u0026rsquo;s configuration, respectively. Additionally, it defaults the .spec.tolerations in Shoots with those configured in its Project, and those configured in the admission controller\u0026rsquo;s configuration, respectively.\nShootValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It validates certain configurations in the specification against the referred CloudProfile (e.g., machine images, machine types, used Kubernetes version, \u0026hellip;). Generally, it performs validations that cannot be handled by the static API validation due to their dynamic nature (e.g., when something needs to be checked against referred resources). Additionally, it takes over certain defaulting tasks (e.g., default machine image for worker pools).\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/core-components/api-server/apiserver_admission_plugins/","title":"In-Tree Admission Plugins","tags":[],"description":"","content":"Admission Plugins Similar to the kube-apiserver, the gardener-apiserver comes with a few in-tree managed admission plugins. If you want to get an overview of the what and why of admission plugins then this document might be a good start.\nThis document lists all existing admission plugins with a short explanation of what it is responsible for.\nClusterOpenIDConnectPreset, OpenIDConnectPreset (both enabled by default)\nThese admission controllers react on CREATE operations for Shoots. If the Shoot does not specify any OIDC configuration (.spec.kubernetes.kubeAPIServer.oidcConfig=nil) then it tries to find a matching ClusterOpenIDConnectPreset or OpenIDConnectPreset, respectively. If there are multiples that match then the one with the highest weight \u0026ldquo;wins\u0026rdquo;. In this case, the admission controller will default the OIDC configuration in the Shoot.\nControllerRegistrationResources (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for ControllerRegistrations. It validates that there exists only one ControllerRegistration in the system that is primarily responsible for a given kind/type resource combination. This prevents misconfiguration by the Gardener administrator/operator.\nCustomVerbAuthorizer (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Projects. It validates whether the user is bound to a RBAC role with the modify-spec-tolerations-whitelist verb in case the user tries to change the .spec.tolerations.whitelist field of the respective Project resource. Usually, regular project members are not bound to this custom verb, allowing the Gardener administrator to manage certain toleration whitelists on Project basis.\nDeletionConfirmation (enabled by default)\nThis admission controller reacts on DELETE operations for Projects and Shoots. It validates that the respective resource is annotated with a deletion confirmation annotation, namely confirmation.gardener.cloud/deletion=true. Only if this annotation is present it allows the DELETE operation to pass. This prevents users from accidental/undesired deletions.\nExtensionValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for BackupEntrys, BackupBuckets, Seeds, and Shoots. For all the various extension types in the specifications of these objects, it validates whether there exists a ControllerRegistration in the system that is primarily responsible for the stated extension type(s). This prevents misconfigurations that would otherwise allow users to create such resources with extension types that don\u0026rsquo;t exist in the cluster, effectively leading to failing reconciliation loops.\nPlantValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Plants. It sets the gardener.cloud/created-by annotation for newly created Plant resources. Also, it prevents creating new Plant resources in Projects that are already have a deletion timestamp.\nResourceQuota (enabled by default)\nThis admission controller enables object count ResourceQuotas for Gardener resources, e.g. Shoots, SecretBindings, Projects, etc..\n :warning: In addition to this admission plugin, the ResourceQuota controller must be enabled for the Kube-Controller-Manager of your Garden cluster.\n ResourceReferenceManager (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for CloudProfiles, Projects, SecretBindings, Seeds, and Shoots. Generally, it checks whether referred resources stated in the specifications of these objects exist in the system (e.g., if a referenced Secret exists). However, it also has some special behaviours for certain resources:\n CloudProfiles: It rejects removing Kubernetes or machine image versions if there is at least one Shoot that refers to them. Projects: It sets the .spec.createdBy field for newly created Project resources, and defaults the .spec.owner field in case it is empty (to the same value of .spec.createdBy). Seeds: It rejects changing the .spec.settings.shootDNS.enabled value if there is at least one Shoot that refers to this seed. Shoots: It sets the gardener.cloud/created-by=\u0026lt;username\u0026gt; annotation for newly created Shoot resources.  SeedValidator (enabled by default)\nThis admission controller reacts on DELETE operations for Seeds. Rejects the deletion if Shoot(s) reference the seed cluster.\nShootDNS (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It tries to assign a default domain to the Shoot if it gets scheduled to a seed that enables DNS for shoots (.spec.settings.shootDNS.enabled=true). It also validates that the DNS configuration (.spec.dns) is not set if the seed disables DNS for shoots.\nShootQuotaValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It validates the resource consumption declared in the specification against applicable Quota resources. Only if the applicable Quota resources admit the configured resources in the Shoot then it allows the request. Applicable Quotas are referred in the SecretBinding that is used by the Shoot.\nShootStateDeletionValidator (enabled by default)\nThis admission controller reacts on DELETE operations for ShootStates. It prevents the deletion of the respective ShootState resource in case the corresponding Shoot resource does still exist in the system. This prevents losing the shoot\u0026rsquo;s data required to recover it / migrate its control plane to a new seed cluster.\nShootTolerationRestriction (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It validates the .spec.tolerations used in Shoots against the whitelist of its Project, or against the whitelist configured in the admission controller\u0026rsquo;s configuration, respectively. Additionally, it defaults the .spec.tolerations in Shoots with those configured in its Project, and those configured in the admission controller\u0026rsquo;s configuration, respectively.\nShootValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It validates certain configurations in the specification against the referred CloudProfile (e.g., machine images, machine types, used Kubernetes version, \u0026hellip;). Generally, it performs validations that cannot be handled by the static API validation due to their dynamic nature (e.g., when something needs to be checked against referred resources). Additionally, it takes over certain defaulting tasks (e.g., default machine image for worker pools).\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/core-components/api-server/apiserver_admission_plugins/","title":"In-Tree Admission Plugins","tags":[],"description":"","content":"Admission Plugins Similar to the kube-apiserver, the gardener-apiserver comes with a few in-tree managed admission plugins. If you want to get an overview of the what and why of admission plugins then this document might be a good start.\nThis document lists all existing admission plugins with a short explanation of what it is responsible for.\nClusterOpenIDConnectPreset, OpenIDConnectPreset (both enabled by default)\nThese admission controllers react on CREATE operations for Shoots. If the Shoot does not specify any OIDC configuration (.spec.kubernetes.kubeAPIServer.oidcConfig=nil) then it tries to find a matching ClusterOpenIDConnectPreset or OpenIDConnectPreset, respectively. If there are multiples that match then the one with the highest weight \u0026ldquo;wins\u0026rdquo;. In this case, the admission controller will default the OIDC configuration in the Shoot.\nControllerRegistrationResources (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for ControllerRegistrations. It validates that there exists only one ControllerRegistration in the system that is primarily responsible for a given kind/type resource combination. This prevents misconfiguration by the Gardener administrator/operator.\nCustomVerbAuthorizer (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Projects. It validates whether the user is bound to a RBAC role with the modify-spec-tolerations-whitelist verb in case the user tries to change the .spec.tolerations.whitelist field of the respective Project resource. Usually, regular project members are not bound to this custom verb, allowing the Gardener administrator to manage certain toleration whitelists on Project basis.\nDeletionConfirmation (enabled by default)\nThis admission controller reacts on DELETE operations for Projects and Shoots. It validates that the respective resource is annotated with a deletion confirmation annotation, namely confirmation.gardener.cloud/deletion=true. Only if this annotation is present it allows the DELETE operation to pass. This prevents users from accidental/undesired deletions.\nExtensionValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for BackupEntrys, BackupBuckets, Seeds, and Shoots. For all the various extension types in the specifications of these objects, it validates whether there exists a ControllerRegistration in the system that is primarily responsible for the stated extension type(s). This prevents misconfigurations that would otherwise allow users to create such resources with extension types that don\u0026rsquo;t exist in the cluster, effectively leading to failing reconciliation loops.\nPlantValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Plants. It sets the gardener.cloud/created-by annotation for newly created Plant resources. Also, it prevents creating new Plant resources in Projects that are already have a deletion timestamp.\nResourceQuota (enabled by default)\nThis admission controller enables object count ResourceQuotas for Gardener resources, e.g. Shoots, SecretBindings, Projects, etc..\n :warning: In addition to this admission plugin, the ResourceQuota controller must be enabled for the Kube-Controller-Manager of your Garden cluster.\n ResourceReferenceManager (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for CloudProfiles, Projects, SecretBindings, Seeds, and Shoots. Generally, it checks whether referred resources stated in the specifications of these objects exist in the system (e.g., if a referenced Secret exists). However, it also has some special behaviours for certain resources:\n CloudProfiles: It rejects removing Kubernetes or machine image versions if there is at least one Shoot that refers to them. Projects: It sets the .spec.createdBy field for newly created Project resources, and defaults the .spec.owner field in case it is empty (to the same value of .spec.createdBy). Seeds: It rejects changing the .spec.settings.shootDNS.enabled value if there is at least one Shoot that refers to this seed. Shoots: It sets the gardener.cloud/created-by=\u0026lt;username\u0026gt; annotation for newly created Shoot resources.  SeedValidator (enabled by default)\nThis admission controller reacts on DELETE operations for Seeds. Rejects the deletion if Shoot(s) reference the seed cluster.\nShootDNS (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It tries to assign a default domain to the Shoot if it gets scheduled to a seed that enables DNS for shoots (.spec.settings.shootDNS.enabled=true). It also validates that the DNS configuration (.spec.dns) is not set if the seed disables DNS for shoots.\nShootQuotaValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It validates the resource consumption declared in the specification against applicable Quota resources. Only if the applicable Quota resources admit the configured resources in the Shoot then it allows the request. Applicable Quotas are referred in the SecretBinding that is used by the Shoot.\nShootStateDeletionValidator (enabled by default)\nThis admission controller reacts on DELETE operations for ShootStates. It prevents the deletion of the respective ShootState resource in case the corresponding Shoot resource does still exist in the system. This prevents losing the shoot\u0026rsquo;s data required to recover it / migrate its control plane to a new seed cluster.\nShootTolerationRestriction (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It validates the .spec.tolerations used in Shoots against the whitelist of its Project, or against the whitelist configured in the admission controller\u0026rsquo;s configuration, respectively. Additionally, it defaults the .spec.tolerations in Shoots with those configured in its Project, and those configured in the admission controller\u0026rsquo;s configuration, respectively.\nShootValidator (enabled by default)\nThis admission controller reacts on CREATE and UPDATE operations for Shoots. It validates certain configurations in the specification against the referred CloudProfile (e.g., machine images, machine types, used Kubernetes version, \u0026hellip;). Generally, it performs validations that cannot be handled by the static API validation due to their dynamic nature (e.g., when something needs to be checked against referred resources). Additionally, it takes over certain defaulting tasks (e.g., default machine image for worker pools).\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/infrastructure/","title":"Infrastructure resource","tags":[],"description":"","content":"Contract: Infrastructure resource Every Kubernetes cluster requires some low-level infrastructure to be setup in order to work properly. Examples for that are networks, routing entries, security groups, IAM roles, etc. Before introducing the Infrastructure extension resource Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see here). Now, Gardener commissions an external, provider-specific controller to take over this task.\nWhich infrastructure resources are required? Unfortunately, there is no general answer to this question as it is highly provider specific. Consider the above mentioned resources, i.e. VPC, subnets, route tables, security groups, IAM roles, SSH key pairs. Most of the resources are required in order to create VMs (the shoot cluster worker nodes), load balancers, and volumes.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--foo--barspec:type:azureregion:eu-west-1secretRef:name:cloudprovidernamespace:shoot--foo--barproviderConfig:apiVersion:azure.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfigresourceGroup:name:mygroupnetworks:vnet:# specify either \u0026#39;name\u0026#39; or \u0026#39;cidr\u0026#39;# name: my-vnetcidr:10.250.0.0/16workers:10.250.0.0/19The .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. However, the most important section is the .spec.providerConfig. It contains an embedded declaration of the provider specific configuration for the infrastructure (that cannot be known by Gardener itself). You are responsible for designing how this configuration looks like. Gardener does not evaluate it but just copies this part from what has been provided by the end-user in the Shoot resource.\nAfter your controller has created the required resources in your provider\u0026rsquo;s infrastructure it needs to generate an output that can be used by other controllers in subsequent steps. An example for that is the Worker extension resource controller. It is responsible for creating virtual machines (shoot worker nodes) in this prepared infrastructure. Everything that it needs to know in order to do that (e.g., the network IDs, security group names, etc. (again: provider-specific)) needs to be provided as output in the Infrastructure resource:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--foo--barspec:...status:lastOperation:...providerStatus:apiVersion:azure.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusresourceGroup:name:mygroupnetworks:vnet:name:my-vnetsubnets:- purpose:nodesname:my-subnetavailabilitySets:- purpose:nodesid:av-set-idname:av-set-namerouteTables:- purpose:nodesname:route-table-namesecurityGroups:- purpose:nodesname:sec-group-nameIn order to support a new infrastructure provider you need to write a controller that watches all Infrastructures with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the Azure provider.\nDynamic nodes network for shoot clusters Some environments do not allow end-users to statically define a CIDR for the network that shall be used for the shoot worker nodes. In these cases it is possible for the extension controllers to dynamically provision a network for the nodes (as part of their reconciliation loops), and to provide the CIDR in the status of the Infrastructure resource:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--foo--barspec:...status:lastOperation:...providerStatus:...nodesCIDR:10.250.0.0/16Gardener will pick this nodesCIDR and use it to configure the VPN components to establish network connectivity between the control plane and the worker nodes. If the Shoot resource already specifies a nodes CIDR in .spec.networking.nodes and the extension controller provides also a value in .status.nodesCIDR in the Infrastructure resource then the latter one will always be considered with higher priority by Gardener.\nNon-provider specific information required for infrastructure creation Some providers might require further information that is not provider specific but already part of the shoot resource. One example for this is the GCP infrastructure controller which needs the pod and the service network of the cluster in order to prepare and configure the infrastructure correctly. As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information that is not part of the Infrastructure resource itself.\nReferences and additional resources  Infrastructure API (Golang specification) Exemplary implementation for the Azure provider  "},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/infrastructure/","title":"Infrastructure resource","tags":[],"description":"","content":"Contract: Infrastructure resource Every Kubernetes cluster requires some low-level infrastructure to be setup in order to work properly. Examples for that are networks, routing entries, security groups, IAM roles, etc. Before introducing the Infrastructure extension resource Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see here). Now, Gardener commissions an external, provider-specific controller to take over this task.\nWhich infrastructure resources are required? Unfortunately, there is no general answer to this question as it is highly provider specific. Consider the above mentioned resources, i.e. VPC, subnets, route tables, security groups, IAM roles, SSH key pairs. Most of the resources are required in order to create VMs (the shoot cluster worker nodes), load balancers, and volumes.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--foo--barspec:type:azureregion:eu-west-1secretRef:name:cloudprovidernamespace:shoot--foo--barproviderConfig:apiVersion:azure.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfigresourceGroup:name:mygroupnetworks:vnet:# specify either \u0026#39;name\u0026#39; or \u0026#39;cidr\u0026#39;# name: my-vnetcidr:10.250.0.0/16workers:10.250.0.0/19The .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. However, the most important section is the .spec.providerConfig. It contains an embedded declaration of the provider specific configuration for the infrastructure (that cannot be known by Gardener itself). You are responsible for designing how this configuration looks like. Gardener does not evaluate it but just copies this part from what has been provided by the end-user in the Shoot resource.\nAfter your controller has created the required resources in your provider\u0026rsquo;s infrastructure it needs to generate an output that can be used by other controllers in subsequent steps. An example for that is the Worker extension resource controller. It is responsible for creating virtual machines (shoot worker nodes) in this prepared infrastructure. Everything that it needs to know in order to do that (e.g., the network IDs, security group names, etc. (again: provider-specific)) needs to be provided as output in the Infrastructure resource:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--foo--barspec:...status:lastOperation:...providerStatus:apiVersion:azure.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusresourceGroup:name:mygroupnetworks:vnet:name:my-vnetsubnets:- purpose:nodesname:my-subnetavailabilitySets:- purpose:nodesid:av-set-idname:av-set-namerouteTables:- purpose:nodesname:route-table-namesecurityGroups:- purpose:nodesname:sec-group-nameIn order to support a new infrastructure provider you need to write a controller that watches all Infrastructures with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the Azure provider.\nDynamic nodes network for shoot clusters Some environments do not allow end-users to statically define a CIDR for the network that shall be used for the shoot worker nodes. In these cases it is possible for the extension controllers to dynamically provision a network for the nodes (as part of their reconciliation loops), and to provide the CIDR in the status of the Infrastructure resource:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--foo--barspec:...status:lastOperation:...providerStatus:...nodesCIDR:10.250.0.0/16Gardener will pick this nodesCIDR and use it to configure the VPN components to establish network connectivity between the control plane and the worker nodes. If the Shoot resource already specifies a nodes CIDR in .spec.networking.nodes and the extension controller provides also a value in .status.nodesCIDR in the Infrastructure resource then the latter one will always be considered with higher priority by Gardener.\nNon-provider specific information required for infrastructure creation Some providers might require further information that is not provider specific but already part of the shoot resource. One example for this is the GCP infrastructure controller which needs the pod and the service network of the cluster in order to prepare and configure the infrastructure correctly. As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information that is not part of the Infrastructure resource itself.\nReferences and additional resources  Infrastructure API (Golang specification) Exemplary implementation for the Azure provider  "},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/infrastructure/","title":"Infrastructure resource","tags":[],"description":"","content":"Contract: Infrastructure resource Every Kubernetes cluster requires some low-level infrastructure to be setup in order to work properly. Examples for that are networks, routing entries, security groups, IAM roles, etc. Before introducing the Infrastructure extension resource Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see here). Now, Gardener commissions an external, provider-specific controller to take over this task.\nWhich infrastructure resources are required? Unfortunately, there is no general answer to this question as it is highly provider specific. Consider the above mentioned resources, i.e. VPC, subnets, route tables, security groups, IAM roles, SSH key pairs. Most of the resources are required in order to create VMs (the shoot cluster worker nodes), load balancers, and volumes.\nWhat needs to be implemented to support a new infrastructure provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--foo--barspec:type:azureregion:eu-west-1secretRef:name:cloudprovidernamespace:shoot--foo--barproviderConfig:apiVersion:azure.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureConfigresourceGroup:name:mygroupnetworks:vnet:# specify either \u0026#39;name\u0026#39; or \u0026#39;cidr\u0026#39;# name: my-vnetcidr:10.250.0.0/16workers:10.250.0.0/19The .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. However, the most important section is the .spec.providerConfig. It contains an embedded declaration of the provider specific configuration for the infrastructure (that cannot be known by Gardener itself). You are responsible for designing how this configuration looks like. Gardener does not evaluate it but just copies this part from what has been provided by the end-user in the Shoot resource.\nAfter your controller has created the required resources in your provider\u0026rsquo;s infrastructure it needs to generate an output that can be used by other controllers in subsequent steps. An example for that is the Worker extension resource controller. It is responsible for creating virtual machines (shoot worker nodes) in this prepared infrastructure. Everything that it needs to know in order to do that (e.g., the network IDs, security group names, etc. (again: provider-specific)) needs to be provided as output in the Infrastructure resource:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--foo--barspec:...status:lastOperation:...providerStatus:apiVersion:azure.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusresourceGroup:name:mygroupnetworks:vnet:name:my-vnetsubnets:- purpose:nodesname:my-subnetavailabilitySets:- purpose:nodesid:av-set-idname:av-set-namerouteTables:- purpose:nodesname:route-table-namesecurityGroups:- purpose:nodesname:sec-group-nameIn order to support a new infrastructure provider you need to write a controller that watches all Infrastructures with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the Azure provider.\nDynamic nodes network for shoot clusters Some environments do not allow end-users to statically define a CIDR for the network that shall be used for the shoot worker nodes. In these cases it is possible for the extension controllers to dynamically provision a network for the nodes (as part of their reconciliation loops), and to provide the CIDR in the status of the Infrastructure resource:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Infrastructuremetadata:name:infrastructurenamespace:shoot--foo--barspec:...status:lastOperation:...providerStatus:...nodesCIDR:10.250.0.0/16Gardener will pick this nodesCIDR and use it to configure the VPN components to establish network connectivity between the control plane and the worker nodes. If the Shoot resource already specifies a nodes CIDR in .spec.networking.nodes and the extension controller provides also a value in .status.nodesCIDR in the Infrastructure resource then the latter one will always be considered with higher priority by Gardener.\nNon-provider specific information required for infrastructure creation Some providers might require further information that is not provider specific but already part of the shoot resource. One example for this is the GCP infrastructure controller which needs the pod and the service network of the cluster in order to prepare and configure the infrastructure correctly. As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information that is not part of the Infrastructure resource itself.\nReferences and additional resources  Infrastructure API (Golang specification) Exemplary implementation for the Azure provider  "},{"uri":"https://gardener.cloud/documentation/tutorials/knative-install/","title":"Install Knative in Gardener clusters","tags":[],"description":"A walkthrough the steps for installing Knative in Gardener shoot clusters.","content":"This guide walks you through the installation of the latest version of Knative using pre-built images on a Gardener created cluster environment. To set up your own Gardener, see the documentation or have a look at the landscape-setup-template project. To learn more about this open source project, read the blog on kubernetes.io.\nBefore you begin Knative requires a Kubernetes cluster v1.15 or newer.\nInstall and configure kubectl   If you already have kubectl CLI, run kubectl version --short to check the version. You need v1.10 or newer. If your kubectl is older, follow the next step to install a newer version.\n  Install the kubectl CLI.\n  Access Gardener   Create a project in the Gardener dashboard. This will essentially create a Kubernetes namespace with the name garden-\u0026lt;my-project\u0026gt;.\n  Configure access to your Gardener project using a kubeconfig. If you are not the Gardener Administrator already, you can create a technical user in the Gardener dashboard: go to the \u0026ldquo;Members\u0026rdquo; section and add a service account. You can then download the kubeconfig for your project. You can skip this step if you create your cluster using the user interface; it is only needed for programmatic access, make sure you set export KUBECONFIG=garden-my-project.yaml in your shell.   Creating a Kubernetes cluster You can create your cluster using kubectl cli by providing a cluster specification yaml file. You can find an example for GCP here. Make sure the namespace matches that of your project. Then just apply the prepared so-called \u0026ldquo;shoot\u0026rdquo; cluster crd with kubectl:\nkubectl apply --filename my-cluster.yaml The easier alternative is to create the cluster following the cluster creation wizard in the Gardener dashboard: Configure kubectl for your cluster You can now download the kubeconfig for your freshly created cluster in the Gardener dashboard or via cli as follows:\nkubectl --namespace shoot--my-project--my-cluster get secret kubecfg --output jsonpath={.data.kubeconfig} | base64 --decode \u0026gt; my-cluster.yaml This kubeconfig file has full administrators access to you cluster. For the rest of this guide be sure you have export KUBECONFIG=my-cluster.yaml set.\nInstalling Istio Knative depends on Istio. If your cloud platform offers a managed Istio installation, we recommend installing Istio that way, unless you need the ability to customize your installation.\nOtherwise, see the Installing Istio for Knative guide to install Istio.\nYou must install Istio on your Kubernetes cluster before continuing with these instructions to install Knative.\nInstalling cluster-local-gateway for serving cluster-internal traffic If you installed Istio, you can install a cluster-local-gateway within your Knative cluster so that you can serve cluster-internal traffic. If you want to configure your revisions to use routes that are visible only within your cluster, install and use the cluster-local-gateway.\nInstalling Knative The following commands install all available Knative components as well as the standard set of observability plugins. To customize your Knative installation, see Performing a Custom Knative Installation.\n  If you are upgrading from Knative 0.3.x: Update your domain and static IP address to be associated with the LoadBalancer istio-ingressgateway instead of knative-ingressgateway. Then run the following to clean up leftover resources:\nkubectl delete svc knative-ingressgateway -n istio-system kubectl delete deploy knative-ingressgateway -n istio-system If you have the Knative Eventing Sources component installed, you will also need to delete the following resource before upgrading:\nkubectl delete statefulset/controller-manager -n knative-sources While the deletion of this resource during the upgrade process will not prevent modifications to Eventing Source resources, those changes will not be completed until the upgrade process finishes.\n  To install Knative, first install the CRDs by running the kubectl apply command once with the -l knative.dev/crd-install=true flag. This prevents race conditions during the install, which cause intermittent errors:\nkubectl apply --selector knative.dev/crd-install=true \\ --filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml \\ --filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml \\ --filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml   To complete the install of Knative and its dependencies, run the kubectl apply command again, this time without the --selector flag, to complete the install of Knative and its dependencies:\nkubectl apply --filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml \\ --filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml \\ --filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml   Monitor the Knative components until all of the components show a STATUS of Running:\nkubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing kubectl get pods --namespace knative-monitoring   Set your custom domain  Fetch the external IP or CNAME of the knative-ingressgateway  kubectl --namespace istio-system get service knative-ingressgateway NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE knative-ingressgateway LoadBalancer 100.70.219.81 35.233.41.212 80:32380/TCP,443:32390/TCP,32400:32400/TCP 4d Create a wildcard DNS entry in your custom domain to point to above IP or CNAME  *.knative.\u0026lt;my domain\u0026gt; == A 35.233.41.212 # or CNAME if you are on AWS *.knative.\u0026lt;my domain\u0026gt; == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Adapt your knative config-domain (set your domain in the data field)  kubectl --namespace knative-serving get configmaps config-domain --output yaml apiVersion: v1 data: knative.\u0026lt;my domain\u0026gt;: \u0026#34;\u0026#34; kind: ConfigMap name: config-domain namespace: knative-serving What\u0026rsquo;s next Now that your cluster has Knative installed, you can see what Knative has to offer.\nTo deploy your first app with the Getting Started with Knative App Deployment guide.\nGet started with Knative Eventing by walking through one of the Eventing Samples.\nInstall Cert-Manager if you want to use the automatic TLS cert provisioning feature.\nCleaning up Use the Gardener dashboard to delete your cluster, or execute the following with kubectl pointing to your garden-my-project.yaml kubeconfig:\nkubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project annotate shoot my-cluster confirmation.gardener.cloud/deletion=true kubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project delete shoot my-cluster "},{"uri":"https://gardener.cloud/v1.12.8/tutorials/knative-install/","title":"Install Knative in Gardener clusters","tags":[],"description":"A walkthrough the steps for installing Knative in Gardener shoot clusters.","content":"This guide walks you through the installation of the latest version of Knative using pre-built images on a Gardener created cluster environment. To set up your own Gardener, see the documentation or have a look at the landscape-setup-template project. To learn more about this open source project, read the blog on kubernetes.io.\nBefore you begin Knative requires a Kubernetes cluster v1.15 or newer.\nInstall and configure kubectl   If you already have kubectl CLI, run kubectl version --short to check the version. You need v1.10 or newer. If your kubectl is older, follow the next step to install a newer version.\n  Install the kubectl CLI.\n  Access Gardener   Create a project in the Gardener dashboard. This will essentially create a Kubernetes namespace with the name garden-\u0026lt;my-project\u0026gt;.\n  Configure access to your Gardener project using a kubeconfig. If you are not the Gardener Administrator already, you can create a technical user in the Gardener dashboard: go to the \u0026ldquo;Members\u0026rdquo; section and add a service account. You can then download the kubeconfig for your project. You can skip this step if you create your cluster using the user interface; it is only needed for programmatic access, make sure you set export KUBECONFIG=garden-my-project.yaml in your shell.   Creating a Kubernetes cluster You can create your cluster using kubectl cli by providing a cluster specification yaml file. You can find an example for GCP here. Make sure the namespace matches that of your project. Then just apply the prepared so-called \u0026ldquo;shoot\u0026rdquo; cluster crd with kubectl:\nkubectl apply --filename my-cluster.yaml The easier alternative is to create the cluster following the cluster creation wizard in the Gardener dashboard: Configure kubectl for your cluster You can now download the kubeconfig for your freshly created cluster in the Gardener dashboard or via cli as follows:\nkubectl --namespace shoot--my-project--my-cluster get secret kubecfg --output jsonpath={.data.kubeconfig} | base64 --decode \u0026gt; my-cluster.yaml This kubeconfig file has full administrators access to you cluster. For the rest of this guide be sure you have export KUBECONFIG=my-cluster.yaml set.\nInstalling Istio Knative depends on Istio. If your cloud platform offers a managed Istio installation, we recommend installing Istio that way, unless you need the ability to customize your installation.\nOtherwise, see the Installing Istio for Knative guide to install Istio.\nYou must install Istio on your Kubernetes cluster before continuing with these instructions to install Knative.\nInstalling cluster-local-gateway for serving cluster-internal traffic If you installed Istio, you can install a cluster-local-gateway within your Knative cluster so that you can serve cluster-internal traffic. If you want to configure your revisions to use routes that are visible only within your cluster, install and use the cluster-local-gateway.\nInstalling Knative The following commands install all available Knative components as well as the standard set of observability plugins. To customize your Knative installation, see Performing a Custom Knative Installation.\n  If you are upgrading from Knative 0.3.x: Update your domain and static IP address to be associated with the LoadBalancer istio-ingressgateway instead of knative-ingressgateway. Then run the following to clean up leftover resources:\nkubectl delete svc knative-ingressgateway -n istio-system kubectl delete deploy knative-ingressgateway -n istio-system If you have the Knative Eventing Sources component installed, you will also need to delete the following resource before upgrading:\nkubectl delete statefulset/controller-manager -n knative-sources While the deletion of this resource during the upgrade process will not prevent modifications to Eventing Source resources, those changes will not be completed until the upgrade process finishes.\n  To install Knative, first install the CRDs by running the kubectl apply command once with the -l knative.dev/crd-install=true flag. This prevents race conditions during the install, which cause intermittent errors:\nkubectl apply --selector knative.dev/crd-install=true \\ --filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml \\ --filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml \\ --filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml   To complete the install of Knative and its dependencies, run the kubectl apply command again, this time without the --selector flag, to complete the install of Knative and its dependencies:\nkubectl apply --filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml \\ --filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml \\ --filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml   Monitor the Knative components until all of the components show a STATUS of Running:\nkubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing kubectl get pods --namespace knative-monitoring   Set your custom domain  Fetch the external IP or CNAME of the knative-ingressgateway  kubectl --namespace istio-system get service knative-ingressgateway NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE knative-ingressgateway LoadBalancer 100.70.219.81 35.233.41.212 80:32380/TCP,443:32390/TCP,32400:32400/TCP 4d Create a wildcard DNS entry in your custom domain to point to above IP or CNAME  *.knative.\u0026lt;my domain\u0026gt; == A 35.233.41.212 # or CNAME if you are on AWS *.knative.\u0026lt;my domain\u0026gt; == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Adapt your knative config-domain (set your domain in the data field)  kubectl --namespace knative-serving get configmaps config-domain --output yaml apiVersion: v1 data: knative.\u0026lt;my domain\u0026gt;: \u0026#34;\u0026#34; kind: ConfigMap name: config-domain namespace: knative-serving What\u0026rsquo;s next Now that your cluster has Knative installed, you can see what Knative has to offer.\nTo deploy your first app with the Getting Started with Knative App Deployment guide.\nGet started with Knative Eventing by walking through one of the Eventing Samples.\nInstall Cert-Manager if you want to use the automatic TLS cert provisioning feature.\nCleaning up Use the Gardener dashboard to delete your cluster, or execute the following with kubectl pointing to your garden-my-project.yaml kubeconfig:\nkubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project annotate shoot my-cluster confirmation.gardener.cloud/deletion=true kubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project delete shoot my-cluster "},{"uri":"https://gardener.cloud/v1.13.2/tutorials/knative-install/","title":"Install Knative in Gardener clusters","tags":[],"description":"A walkthrough the steps for installing Knative in Gardener shoot clusters.","content":"This guide walks you through the installation of the latest version of Knative using pre-built images on a Gardener created cluster environment. To set up your own Gardener, see the documentation or have a look at the landscape-setup-template project. To learn more about this open source project, read the blog on kubernetes.io.\nBefore you begin Knative requires a Kubernetes cluster v1.15 or newer.\nInstall and configure kubectl   If you already have kubectl CLI, run kubectl version --short to check the version. You need v1.10 or newer. If your kubectl is older, follow the next step to install a newer version.\n  Install the kubectl CLI.\n  Access Gardener   Create a project in the Gardener dashboard. This will essentially create a Kubernetes namespace with the name garden-\u0026lt;my-project\u0026gt;.\n  Configure access to your Gardener project using a kubeconfig. If you are not the Gardener Administrator already, you can create a technical user in the Gardener dashboard: go to the \u0026ldquo;Members\u0026rdquo; section and add a service account. You can then download the kubeconfig for your project. You can skip this step if you create your cluster using the user interface; it is only needed for programmatic access, make sure you set export KUBECONFIG=garden-my-project.yaml in your shell.   Creating a Kubernetes cluster You can create your cluster using kubectl cli by providing a cluster specification yaml file. You can find an example for GCP here. Make sure the namespace matches that of your project. Then just apply the prepared so-called \u0026ldquo;shoot\u0026rdquo; cluster crd with kubectl:\nkubectl apply --filename my-cluster.yaml The easier alternative is to create the cluster following the cluster creation wizard in the Gardener dashboard: Configure kubectl for your cluster You can now download the kubeconfig for your freshly created cluster in the Gardener dashboard or via cli as follows:\nkubectl --namespace shoot--my-project--my-cluster get secret kubecfg --output jsonpath={.data.kubeconfig} | base64 --decode \u0026gt; my-cluster.yaml This kubeconfig file has full administrators access to you cluster. For the rest of this guide be sure you have export KUBECONFIG=my-cluster.yaml set.\nInstalling Istio Knative depends on Istio. If your cloud platform offers a managed Istio installation, we recommend installing Istio that way, unless you need the ability to customize your installation.\nOtherwise, see the Installing Istio for Knative guide to install Istio.\nYou must install Istio on your Kubernetes cluster before continuing with these instructions to install Knative.\nInstalling cluster-local-gateway for serving cluster-internal traffic If you installed Istio, you can install a cluster-local-gateway within your Knative cluster so that you can serve cluster-internal traffic. If you want to configure your revisions to use routes that are visible only within your cluster, install and use the cluster-local-gateway.\nInstalling Knative The following commands install all available Knative components as well as the standard set of observability plugins. To customize your Knative installation, see Performing a Custom Knative Installation.\n  If you are upgrading from Knative 0.3.x: Update your domain and static IP address to be associated with the LoadBalancer istio-ingressgateway instead of knative-ingressgateway. Then run the following to clean up leftover resources:\nkubectl delete svc knative-ingressgateway -n istio-system kubectl delete deploy knative-ingressgateway -n istio-system If you have the Knative Eventing Sources component installed, you will also need to delete the following resource before upgrading:\nkubectl delete statefulset/controller-manager -n knative-sources While the deletion of this resource during the upgrade process will not prevent modifications to Eventing Source resources, those changes will not be completed until the upgrade process finishes.\n  To install Knative, first install the CRDs by running the kubectl apply command once with the -l knative.dev/crd-install=true flag. This prevents race conditions during the install, which cause intermittent errors:\nkubectl apply --selector knative.dev/crd-install=true \\ --filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml \\ --filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml \\ --filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml   To complete the install of Knative and its dependencies, run the kubectl apply command again, this time without the --selector flag, to complete the install of Knative and its dependencies:\nkubectl apply --filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml \\ --filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml \\ --filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml   Monitor the Knative components until all of the components show a STATUS of Running:\nkubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing kubectl get pods --namespace knative-monitoring   Set your custom domain  Fetch the external IP or CNAME of the knative-ingressgateway  kubectl --namespace istio-system get service knative-ingressgateway NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE knative-ingressgateway LoadBalancer 100.70.219.81 35.233.41.212 80:32380/TCP,443:32390/TCP,32400:32400/TCP 4d Create a wildcard DNS entry in your custom domain to point to above IP or CNAME  *.knative.\u0026lt;my domain\u0026gt; == A 35.233.41.212 # or CNAME if you are on AWS *.knative.\u0026lt;my domain\u0026gt; == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Adapt your knative config-domain (set your domain in the data field)  kubectl --namespace knative-serving get configmaps config-domain --output yaml apiVersion: v1 data: knative.\u0026lt;my domain\u0026gt;: \u0026#34;\u0026#34; kind: ConfigMap name: config-domain namespace: knative-serving What\u0026rsquo;s next Now that your cluster has Knative installed, you can see what Knative has to offer.\nTo deploy your first app with the Getting Started with Knative App Deployment guide.\nGet started with Knative Eventing by walking through one of the Eventing Samples.\nInstall Cert-Manager if you want to use the automatic TLS cert provisioning feature.\nCleaning up Use the Gardener dashboard to delete your cluster, or execute the following with kubectl pointing to your garden-my-project.yaml kubeconfig:\nkubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project annotate shoot my-cluster confirmation.gardener.cloud/deletion=true kubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project delete shoot my-cluster "},{"uri":"https://gardener.cloud/documentation/guides/applications/content_trust/","title":"Integrity and Immutability","tags":[],"description":"Ensure that you get always the right image","content":"Introduction When transferring data among networked systems, trust is a central concern. In particular, when communicating over an untrusted medium such as the internet, it is critical to ensure the integrity and immutability of all the data a system operates on. Especially if you use Docker Engine to push and pull images (data) to a public registry.\nThis immutability offers me a guarantee that any and all containers that I instantiate will be absolutely identical at inception. Surprise surprise, deterministic operations.\nA Lesson in Deterministic Ops Docker Tags are about as reliable and disposable as this guy down here.\nSeems simple enough. You have probably already deployed hundreds of YAML\u0026rsquo;s or started endless count of Docker container.\ndocker run --name mynginx1 -P -d nginx:1.13.9 or\napiVersion:apps/v1kind:Deploymentmetadata:name:rss-sitespec:replicas:1selector:matchLabels:app:webtemplate:metadata:labels:app:webspec:containers:- name:front-endimage:nginx:1.13.9ports:- containerPort:80But Tags are mutable and humans are prone to error. Not a good combination. Here well dig into why the use of tags can be dangerous and how to deploy your containers across a pipeline and across environments, you guessed it, with determinism in mind.\nI want to ensure that whether its today or 5 years from now, that specific deployment uses the very same image that I defined. Any updates or newer versions of an image should be executed as a new deployment. The solution: digest\nA digest takes the place of the tag when pulling an image, for example, to pull the above image by digest, run the following command:\ndocker run --name mynginx1 -P -d nginx@sha256:4771d09578c7c6a65299e110b3ee1c0a2592f5ea2618d23e4ffe7a4cab1ce5de You can now make sure that the same image is always loaded at every deployment. It doesn\u0026rsquo;t matter if the TAG of the image has been changed or not. This solves the problem of repeatability.\nContent Trust However, theres an additionally hidden danger. It is possible for an attacker to replace a server image with another one infected with malware.\nDocker Content trust gives you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel.\nPrior to version 1.8, Docker didnt have a way to verify the authenticity of a server image. But in v1.8, a new feature called Docker Content Trust was introduced to automatically sign and verify the signature of a publisher.\nSo, as soon as a server image is downloaded, it is cross-checked with the signature of the publisher to see if someone tampered with it in any way. This solves the problem of trust.\nIn addition you should scan all images for known vulnerabilities, this can fill another book\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/applications/content_trust/","title":"Integrity and Immutability","tags":[],"description":"Ensure that you get always the right image","content":"Introduction When transferring data among networked systems, trust is a central concern. In particular, when communicating over an untrusted medium such as the internet, it is critical to ensure the integrity and immutability of all the data a system operates on. Especially if you use Docker Engine to push and pull images (data) to a public registry.\nThis immutability offers me a guarantee that any and all containers that I instantiate will be absolutely identical at inception. Surprise surprise, deterministic operations.\nA Lesson in Deterministic Ops Docker Tags are about as reliable and disposable as this guy down here.\nSeems simple enough. You have probably already deployed hundreds of YAML\u0026rsquo;s or started endless count of Docker container.\ndocker run --name mynginx1 -P -d nginx:1.13.9 or\napiVersion:apps/v1kind:Deploymentmetadata:name:rss-sitespec:replicas:1selector:matchLabels:app:webtemplate:metadata:labels:app:webspec:containers:- name:front-endimage:nginx:1.13.9ports:- containerPort:80But Tags are mutable and humans are prone to error. Not a good combination. Here well dig into why the use of tags can be dangerous and how to deploy your containers across a pipeline and across environments, you guessed it, with determinism in mind.\nI want to ensure that whether its today or 5 years from now, that specific deployment uses the very same image that I defined. Any updates or newer versions of an image should be executed as a new deployment. The solution: digest\nA digest takes the place of the tag when pulling an image, for example, to pull the above image by digest, run the following command:\ndocker run --name mynginx1 -P -d nginx@sha256:4771d09578c7c6a65299e110b3ee1c0a2592f5ea2618d23e4ffe7a4cab1ce5de You can now make sure that the same image is always loaded at every deployment. It doesn\u0026rsquo;t matter if the TAG of the image has been changed or not. This solves the problem of repeatability.\nContent Trust However, theres an additionally hidden danger. It is possible for an attacker to replace a server image with another one infected with malware.\nDocker Content trust gives you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel.\nPrior to version 1.8, Docker didnt have a way to verify the authenticity of a server image. But in v1.8, a new feature called Docker Content Trust was introduced to automatically sign and verify the signature of a publisher.\nSo, as soon as a server image is downloaded, it is cross-checked with the signature of the publisher to see if someone tampered with it in any way. This solves the problem of trust.\nIn addition you should scan all images for known vulnerabilities, this can fill another book\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/applications/content_trust/","title":"Integrity and Immutability","tags":[],"description":"Ensure that you get always the right image","content":"Introduction When transferring data among networked systems, trust is a central concern. In particular, when communicating over an untrusted medium such as the internet, it is critical to ensure the integrity and immutability of all the data a system operates on. Especially if you use Docker Engine to push and pull images (data) to a public registry.\nThis immutability offers me a guarantee that any and all containers that I instantiate will be absolutely identical at inception. Surprise surprise, deterministic operations.\nA Lesson in Deterministic Ops Docker Tags are about as reliable and disposable as this guy down here.\nSeems simple enough. You have probably already deployed hundreds of YAML\u0026rsquo;s or started endless count of Docker container.\ndocker run --name mynginx1 -P -d nginx:1.13.9 or\napiVersion:apps/v1kind:Deploymentmetadata:name:rss-sitespec:replicas:1selector:matchLabels:app:webtemplate:metadata:labels:app:webspec:containers:- name:front-endimage:nginx:1.13.9ports:- containerPort:80But Tags are mutable and humans are prone to error. Not a good combination. Here well dig into why the use of tags can be dangerous and how to deploy your containers across a pipeline and across environments, you guessed it, with determinism in mind.\nI want to ensure that whether its today or 5 years from now, that specific deployment uses the very same image that I defined. Any updates or newer versions of an image should be executed as a new deployment. The solution: digest\nA digest takes the place of the tag when pulling an image, for example, to pull the above image by digest, run the following command:\ndocker run --name mynginx1 -P -d nginx@sha256:4771d09578c7c6a65299e110b3ee1c0a2592f5ea2618d23e4ffe7a4cab1ce5de You can now make sure that the same image is always loaded at every deployment. It doesn\u0026rsquo;t matter if the TAG of the image has been changed or not. This solves the problem of repeatability.\nContent Trust However, theres an additionally hidden danger. It is possible for an attacker to replace a server image with another one infected with malware.\nDocker Content trust gives you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel.\nPrior to version 1.8, Docker didnt have a way to verify the authenticity of a server image. But in v1.8, a new feature called Docker Content Trust was introduced to automatically sign and verify the signature of a publisher.\nSo, as soon as a server image is downloaded, it is cross-checked with the signature of the publisher to see if someone tampered with it in any way. This solves the problem of trust.\nIn addition you should scan all images for known vulnerabilities, this can fill another book\n"},{"uri":"https://gardener.cloud/documentation/guides/client_tools/bash_kubeconfig/","title":"Kubeconfig context as bash prompt","tags":[],"description":"Expose the active kubeconfig into bash","content":"Use the Kubernetes command-line tool, kubectl, to deploy and manage applications on Kubernetes. Using kubectl, you can inspect cluster resources; create, delete, and update components\nBy default, the kubectl configuration is located at ~/.kube/config.\nSuppose you have two clusters, one for development work and one for scratch work.\nHow to handle this easily without copying the used configuration always to the right place?\nExport the KUBECONFIG enviroment variable bash$ export KUBECONFIG=\u0026lt;PATH-TO-M\u0026gt;-CONFIG\u0026gt;/kubeconfig-dev.yaml How to determine which cluster is used by the kubectl command?\nDetermine active cluster bash$ kubectl cluster-info Kubernetes master is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com KubeDNS is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com/api/v1/proxy/namespaces/kube-system/services/kube-dns To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. bash$ Display cluster in the bash - Linux and alike I found this tip on Stackoverflow and find it worth to be added here. Edit your ~/.bash_profile and add the following code snippet to show the current k8s context in the shell\u0026rsquo;s prompt.\nprompt_k8s(){ k8s_current_context=$(kubectl config current-context 2\u0026gt; /dev/null) if [[ $? -eq 0 ]] ; then echo -e \u0026#34;(${k8s_current_context}) \u0026#34;; fi } PS1+=\u0026#39;$(prompt_k8s)\u0026#39; After this your bash command prompt contains the active KUBECONFIG context and you always know which cluster is active - develop or production.\ne.g.\nbash$ export KUBECONFIG=/Users/d023280/Documents/workspace/gardener-ui/kubeconfig_gardendev.yaml bash (garden_dev)$ Note the (garden_dev) prefix in the bash command prompt.\nThis helps immensely to avoid thoughtless mistakes.\nDisplay cluster in the PowerShell - Windows Display current k8s cluster in the title of PowerShell window.\nCreate a profile file for your shell under %UserProfile%\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1\nCopy following code to Microsoft.PowerShell_profile.ps1\nfunction prompt_k8s { $k8s_current_context = (kubectl config current-context) | Out-String if($?) { return $k8s_current_context }else { return \u0026#34;No K8S contenxt found\u0026#34; } } $host.ui.rawui.WindowTitle = prompt_k8s If you want to switch to different cluster, you can set KUBECONFIG to new value, and re-run the file Microsoft.PowerShell_profile.ps1\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/client_tools/bash_kubeconfig/","title":"Kubeconfig context as bash prompt","tags":[],"description":"Expose the active kubeconfig into bash","content":"Use the Kubernetes command-line tool, kubectl, to deploy and manage applications on Kubernetes. Using kubectl, you can inspect cluster resources; create, delete, and update components\nBy default, the kubectl configuration is located at ~/.kube/config.\nSuppose you have two clusters, one for development work and one for scratch work.\nHow to handle this easily without copying the used configuration always to the right place?\nExport the KUBECONFIG enviroment variable bash$ export KUBECONFIG=\u0026lt;PATH-TO-M\u0026gt;-CONFIG\u0026gt;/kubeconfig-dev.yaml How to determine which cluster is used by the kubectl command?\nDetermine active cluster bash$ kubectl cluster-info Kubernetes master is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com KubeDNS is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com/api/v1/proxy/namespaces/kube-system/services/kube-dns To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. bash$ Display cluster in the bash - Linux and alike I found this tip on Stackoverflow and find it worth to be added here. Edit your ~/.bash_profile and add the following code snippet to show the current k8s context in the shell\u0026rsquo;s prompt.\nprompt_k8s(){ k8s_current_context=$(kubectl config current-context 2\u0026gt; /dev/null) if [[ $? -eq 0 ]] ; then echo -e \u0026#34;(${k8s_current_context}) \u0026#34;; fi } PS1+=\u0026#39;$(prompt_k8s)\u0026#39; After this your bash command prompt contains the active KUBECONFIG context and you always know which cluster is active - develop or production.\ne.g.\nbash$ export KUBECONFIG=/Users/d023280/Documents/workspace/gardener-ui/kubeconfig_gardendev.yaml bash (garden_dev)$ Note the (garden_dev) prefix in the bash command prompt.\nThis helps immensely to avoid thoughtless mistakes.\nDisplay cluster in the PowerShell - Windows Display current k8s cluster in the title of PowerShell window.\nCreate a profile file for your shell under %UserProfile%\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1\nCopy following code to Microsoft.PowerShell_profile.ps1\nfunction prompt_k8s { $k8s_current_context = (kubectl config current-context) | Out-String if($?) { return $k8s_current_context }else { return \u0026#34;No K8S contenxt found\u0026#34; } } $host.ui.rawui.WindowTitle = prompt_k8s If you want to switch to different cluster, you can set KUBECONFIG to new value, and re-run the file Microsoft.PowerShell_profile.ps1\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/client_tools/bash_kubeconfig/","title":"Kubeconfig context as bash prompt","tags":[],"description":"Expose the active kubeconfig into bash","content":"Use the Kubernetes command-line tool, kubectl, to deploy and manage applications on Kubernetes. Using kubectl, you can inspect cluster resources; create, delete, and update components\nBy default, the kubectl configuration is located at ~/.kube/config.\nSuppose you have two clusters, one for development work and one for scratch work.\nHow to handle this easily without copying the used configuration always to the right place?\nExport the KUBECONFIG enviroment variable bash$ export KUBECONFIG=\u0026lt;PATH-TO-M\u0026gt;-CONFIG\u0026gt;/kubeconfig-dev.yaml How to determine which cluster is used by the kubectl command?\nDetermine active cluster bash$ kubectl cluster-info Kubernetes master is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com KubeDNS is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com/api/v1/proxy/namespaces/kube-system/services/kube-dns To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. bash$ Display cluster in the bash - Linux and alike I found this tip on Stackoverflow and find it worth to be added here. Edit your ~/.bash_profile and add the following code snippet to show the current k8s context in the shell\u0026rsquo;s prompt.\nprompt_k8s(){ k8s_current_context=$(kubectl config current-context 2\u0026gt; /dev/null) if [[ $? -eq 0 ]] ; then echo -e \u0026#34;(${k8s_current_context}) \u0026#34;; fi } PS1+=\u0026#39;$(prompt_k8s)\u0026#39; After this your bash command prompt contains the active KUBECONFIG context and you always know which cluster is active - develop or production.\ne.g.\nbash$ export KUBECONFIG=/Users/d023280/Documents/workspace/gardener-ui/kubeconfig_gardendev.yaml bash (garden_dev)$ Note the (garden_dev) prefix in the bash command prompt.\nThis helps immensely to avoid thoughtless mistakes.\nDisplay cluster in the PowerShell - Windows Display current k8s cluster in the title of PowerShell window.\nCreate a profile file for your shell under %UserProfile%\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1\nCopy following code to Microsoft.PowerShell_profile.ps1\nfunction prompt_k8s { $k8s_current_context = (kubectl config current-context) | Out-String if($?) { return $k8s_current_context }else { return \u0026#34;No K8S contenxt found\u0026#34; } } $host.ui.rawui.WindowTitle = prompt_k8s If you want to switch to different cluster, you can set KUBECONFIG to new value, and re-run the file Microsoft.PowerShell_profile.ps1\n"},{"uri":"https://gardener.cloud/documentation/guides/applications/antipattern/","title":"Kubernetes Antipatterns","tags":[],"description":"Common Antipatterns for Kubernetes and Docker","content":"This HowTo covers common kubernetes antipatterns that we have seen over the past months.\nRunning as root user. Whenever possible, do not run containers as root user. One could be tempted to say that Kubernetes Pods and Node are well separated. Host and containers running on it share the same kernel. If a container is compromised, the root user in the container has full control over the underlying node.\nWatch the very good presentation by Liz Rice at the KubeCon 2018\n Use RUN groupadd -r anygroup \u0026amp;\u0026amp; useradd -r -g anygroup myuser to create a group and add a user to it. Use the USER command to switch to this user. Note that you may also consider to provide an explicit UID/GID if required.\nFor Example:\nARG GF_UID=\u0026#34;500\u0026#34; ARG GF_GID=\u0026#34;500\u0026#34; # add group \u0026amp; user RUN groupadd -r -g $GF_GID appgroup \u0026amp;\u0026amp; \\ useradd appuser -r -u $GF_UID -g appgroup USER appuser Store data or logs in containers Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the container, as they are lost when the container is closed. Use persistence volumes instead to persist data outside of containers. Using an ELK stack is another good option for storing and processing logs.\nUsing pod IP addresses Each pod is assigned an IP address. It is necessary for pods to communicate with each other to build an application, e.g. an application must communicate with a database. Existing pods are terminated and new pods are constantly started. If you would rely on the IP address of a pod or container, you would need to update the application configuration constantly. This makes the application fragile. Create services instead. They provide a logical name that can be assigned independently of the varying number and IP addresses of containers. Services are the basic concept for load balancing within Kubernetes.\nMore than one process in a container A docker file provides a CMD and ENTRYPOINT to start the image. CMD is often used around a script that makes a configuration and then starts the container. Do not try to start multiple processes with this script. It is important to consider the separation of concerns when creating docker images. Running multiple processes in a single pod makes managing your containers, collecting logs and updating each process more difficult. You can split the image into multiple containers and manage them independently - even in one pod. Bear in mind that Kubernetes only monitors the process with PID=1. If more than one process is started within a container, then these no longer fall under the control of Kubernetes.\nCreating images in a running container A new image can be created with the docker commit command. This is useful if changes have been made to the container and you want to persist them for later error analysis. However, images created like this are not reproducible and completely worthless for a CI/CD environment. Furthermore, another developer cannot recognize which components the image contains. Instead, always make changes to the docker file, close existing containers and start a new container with the updated image.\nSaving passwords in docker image  Do not save passwords in a Docker file. They are in plain text and are checked into a repository. That makes them completely vulnerable even if you are using a private repository like the Artifactory. Always use Secrets or ConfigMaps to provision passwords or inject them by mounting a persistent volume.\nUsing the \u0026lsquo;latest\u0026rsquo; tag Starting an image with tomcat is tempting. If no tags are specified, a container is started with the tomcat:latest image. This image may no longer be up to date and refers to an older version instead. Running a production application requires complete control of the environment with exact versions of the image. Make sure you always use a tag or even better the sha256 hash of the image e.g. tomcat@sha256:c34ce3c1fcc0c7431e1392cc3abd0dfe2192ffea1898d5250f199d3ac8d8720f. Why use the sha256 hash? Tags are not immutable and can be overwritten by a developer at any time. In this case you don\u0026rsquo;t have complete control over your image - which is bad.\nDifferent images per environment Don\u0026rsquo;t create different images for development, testing, staging and production environments. The image should be the source of truth and should only be created once and pushed to the repository. This image:tag should be used for different environments in the future.\nDepend on start order of pods Applications often depend on containers being started in a certain order. For example, a database container must be up and running before an application can connect to it. The application should be resilient to such changes, as the db pod can be unreachable or restarted at any time. The application container should be able to handle such situations without terminating or crashing.\nAdditional anti-patterns and patterns\u0026hellip; In the community vast experience have been collected to improve stability and usability of Docker and Kubernetes. Refer to the following link for more information\n Kubernetes Production Patterns  "},{"uri":"https://gardener.cloud/v1.12.8/guides/applications/antipattern/","title":"Kubernetes Antipatterns","tags":[],"description":"Common Antipatterns for Kubernetes and Docker","content":"This HowTo covers common kubernetes antipatterns that we have seen over the past months.\nRunning as root user. Whenever possible, do not run containers as root user. One could be tempted to say that Kubernetes Pods and Node are well separated. Host and containers running on it share the same kernel. If a container is compromised, the root user in the container has full control over the underlying node.\nWatch the very good presentation by Liz Rice at the KubeCon 2018\n Use RUN groupadd -r anygroup \u0026amp;\u0026amp; useradd -r -g anygroup myuser to create a group and add a user to it. Use the USER command to switch to this user. Note that you may also consider to provide an explicit UID/GID if required.\nFor Example:\nARG GF_UID=\u0026#34;500\u0026#34; ARG GF_GID=\u0026#34;500\u0026#34; # add group \u0026amp; user RUN groupadd -r -g $GF_GID appgroup \u0026amp;\u0026amp; \\ useradd appuser -r -u $GF_UID -g appgroup USER appuser Store data or logs in containers Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the container, as they are lost when the container is closed. Use persistence volumes instead to persist data outside of containers. Using an ELK stack is another good option for storing and processing logs.\nUsing pod IP addresses Each pod is assigned an IP address. It is necessary for pods to communicate with each other to build an application, e.g. an application must communicate with a database. Existing pods are terminated and new pods are constantly started. If you would rely on the IP address of a pod or container, you would need to update the application configuration constantly. This makes the application fragile. Create services instead. They provide a logical name that can be assigned independently of the varying number and IP addresses of containers. Services are the basic concept for load balancing within Kubernetes.\nMore than one process in a container A docker file provides a CMD and ENTRYPOINT to start the image. CMD is often used around a script that makes a configuration and then starts the container. Do not try to start multiple processes with this script. It is important to consider the separation of concerns when creating docker images. Running multiple processes in a single pod makes managing your containers, collecting logs and updating each process more difficult. You can split the image into multiple containers and manage them independently - even in one pod. Bear in mind that Kubernetes only monitors the process with PID=1. If more than one process is started within a container, then these no longer fall under the control of Kubernetes.\nCreating images in a running container A new image can be created with the docker commit command. This is useful if changes have been made to the container and you want to persist them for later error analysis. However, images created like this are not reproducible and completely worthless for a CI/CD environment. Furthermore, another developer cannot recognize which components the image contains. Instead, always make changes to the docker file, close existing containers and start a new container with the updated image.\nSaving passwords in docker image  Do not save passwords in a Docker file. They are in plain text and are checked into a repository. That makes them completely vulnerable even if you are using a private repository like the Artifactory. Always use Secrets or ConfigMaps to provision passwords or inject them by mounting a persistent volume.\nUsing the \u0026lsquo;latest\u0026rsquo; tag Starting an image with tomcat is tempting. If no tags are specified, a container is started with the tomcat:latest image. This image may no longer be up to date and refers to an older version instead. Running a production application requires complete control of the environment with exact versions of the image. Make sure you always use a tag or even better the sha256 hash of the image e.g. tomcat@sha256:c34ce3c1fcc0c7431e1392cc3abd0dfe2192ffea1898d5250f199d3ac8d8720f. Why use the sha256 hash? Tags are not immutable and can be overwritten by a developer at any time. In this case you don\u0026rsquo;t have complete control over your image - which is bad.\nDifferent images per environment Don\u0026rsquo;t create different images for development, testing, staging and production environments. The image should be the source of truth and should only be created once and pushed to the repository. This image:tag should be used for different environments in the future.\nDepend on start order of pods Applications often depend on containers being started in a certain order. For example, a database container must be up and running before an application can connect to it. The application should be resilient to such changes, as the db pod can be unreachable or restarted at any time. The application container should be able to handle such situations without terminating or crashing.\nAdditional anti-patterns and patterns\u0026hellip; In the community vast experience have been collected to improve stability and usability of Docker and Kubernetes. Refer to the following link for more information\n Kubernetes Production Patterns  "},{"uri":"https://gardener.cloud/v1.13.2/guides/applications/antipattern/","title":"Kubernetes Antipatterns","tags":[],"description":"Common Antipatterns for Kubernetes and Docker","content":"This HowTo covers common kubernetes antipatterns that we have seen over the past months.\nRunning as root user. Whenever possible, do not run containers as root user. One could be tempted to say that Kubernetes Pods and Node are well separated. Host and containers running on it share the same kernel. If a container is compromised, the root user in the container has full control over the underlying node.\nWatch the very good presentation by Liz Rice at the KubeCon 2018\n Use RUN groupadd -r anygroup \u0026amp;\u0026amp; useradd -r -g anygroup myuser to create a group and add a user to it. Use the USER command to switch to this user. Note that you may also consider to provide an explicit UID/GID if required.\nFor Example:\nARG GF_UID=\u0026#34;500\u0026#34; ARG GF_GID=\u0026#34;500\u0026#34; # add group \u0026amp; user RUN groupadd -r -g $GF_GID appgroup \u0026amp;\u0026amp; \\ useradd appuser -r -u $GF_UID -g appgroup USER appuser Store data or logs in containers Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the container, as they are lost when the container is closed. Use persistence volumes instead to persist data outside of containers. Using an ELK stack is another good option for storing and processing logs.\nUsing pod IP addresses Each pod is assigned an IP address. It is necessary for pods to communicate with each other to build an application, e.g. an application must communicate with a database. Existing pods are terminated and new pods are constantly started. If you would rely on the IP address of a pod or container, you would need to update the application configuration constantly. This makes the application fragile. Create services instead. They provide a logical name that can be assigned independently of the varying number and IP addresses of containers. Services are the basic concept for load balancing within Kubernetes.\nMore than one process in a container A docker file provides a CMD and ENTRYPOINT to start the image. CMD is often used around a script that makes a configuration and then starts the container. Do not try to start multiple processes with this script. It is important to consider the separation of concerns when creating docker images. Running multiple processes in a single pod makes managing your containers, collecting logs and updating each process more difficult. You can split the image into multiple containers and manage them independently - even in one pod. Bear in mind that Kubernetes only monitors the process with PID=1. If more than one process is started within a container, then these no longer fall under the control of Kubernetes.\nCreating images in a running container A new image can be created with the docker commit command. This is useful if changes have been made to the container and you want to persist them for later error analysis. However, images created like this are not reproducible and completely worthless for a CI/CD environment. Furthermore, another developer cannot recognize which components the image contains. Instead, always make changes to the docker file, close existing containers and start a new container with the updated image.\nSaving passwords in docker image  Do not save passwords in a Docker file. They are in plain text and are checked into a repository. That makes them completely vulnerable even if you are using a private repository like the Artifactory. Always use Secrets or ConfigMaps to provision passwords or inject them by mounting a persistent volume.\nUsing the \u0026lsquo;latest\u0026rsquo; tag Starting an image with tomcat is tempting. If no tags are specified, a container is started with the tomcat:latest image. This image may no longer be up to date and refers to an older version instead. Running a production application requires complete control of the environment with exact versions of the image. Make sure you always use a tag or even better the sha256 hash of the image e.g. tomcat@sha256:c34ce3c1fcc0c7431e1392cc3abd0dfe2192ffea1898d5250f199d3ac8d8720f. Why use the sha256 hash? Tags are not immutable and can be overwritten by a developer at any time. In this case you don\u0026rsquo;t have complete control over your image - which is bad.\nDifferent images per environment Don\u0026rsquo;t create different images for development, testing, staging and production environments. The image should be the source of truth and should only be created once and pushed to the repository. This image:tag should be used for different environments in the future.\nDepend on start order of pods Applications often depend on containers being started in a certain order. For example, a database container must be up and running before an application can connect to it. The application should be resilient to such changes, as the db pod can be unreachable or restarted at any time. The application container should be able to handle such situations without terminating or crashing.\nAdditional anti-patterns and patterns\u0026hellip; In the community vast experience have been collected to improve stability and usability of Docker and Kubernetes. Refer to the following link for more information\n Kubernetes Production Patterns  "},{"uri":"https://gardener.cloud/components/kubify/","title":"kubify","tags":[],"description":"","content":"Kubify Kubify is a Terraform based provisioning project for setting up production ready Kubernetes clusters on public and private Cloud infrastructures. Kubify currently supports:\n OpenStack AWS Azure  Key features of Kubify are:\n Kubernetes v1.10.12 Etcd v3.3.10 multi master node setup Etcd backup and restore Supports rolling updates   To start using or developing Kubify locally See our documentation in the /docs repository or find the main documentation here.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions about our Kubernetes clusters as such or the Kubify itself as GitHub issues or join our Slack channel #gardener (Invite yourself to the Kubernetes Slack workspace here).\n"},{"uri":"https://gardener.cloud/components/kubify/","title":"kubify","tags":[],"description":"","content":"Kubify Kubify is a Terraform based provisioning project for setting up production ready Kubernetes clusters on public and private Cloud infrastructures. Kubify currently supports:\n OpenStack AWS Azure  Key features of Kubify are:\n Kubernetes v1.10.12 Etcd v3.3.10 multi master node setup Etcd backup and restore Supports rolling updates   To start using or developing Kubify locally See our documentation in the /docs repository or find the main documentation here.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions about our Kubernetes clusters as such or the Kubify itself as GitHub issues or join our Slack channel #gardener (Invite yourself to the Kubernetes Slack workspace here).\n"},{"uri":"https://gardener.cloud/components/kubify/","title":"kubify","tags":[],"description":"","content":"Kubify Kubify is a Terraform based provisioning project for setting up production ready Kubernetes clusters on public and private Cloud infrastructures. Kubify currently supports:\n OpenStack AWS Azure  Key features of Kubify are:\n Kubernetes v1.10.12 Etcd v3.3.10 multi master node setup Etcd backup and restore Supports rolling updates   To start using or developing Kubify locally See our documentation in the /docs repository or find the main documentation here.\nFeedback and Support Feedback and contributions are always welcome. Please report bugs or suggestions about our Kubernetes clusters as such or the Kubify itself as GitHub issues or join our Slack channel #gardener (Invite yourself to the Kubernetes Slack workspace here).\n"},{"uri":"https://gardener.cloud/documentation/guides/install_gardener/landscape-setup/","title":"Landscape Setup","tags":[],"description":"","content":"\u0026mdash;DEPRECATED\u0026mdash; This project is outdated and won\u0026rsquo;t be updated anymore. Please use https://github.com/gardener/garden-setup instead!\nGardener Setup Scripts This README is the installation manual for a simple Gardener setup. The installation scripts in this repo are embedded in a configuration template in the landscape-setup-template project. You can find further information there.\nWe do recommend this simplified setup for demonstration purposes only. For productive workloads we do recommend that all components (Gardener/Seed/Shoot) run in their own IaaS accounts and that network policies are enabled and properly tested on the seed clusters. A documentation on how to do this is currently work in progress.\n \u0026mdash;DEPRECATED\u0026mdash; Gardener Setup Scripts Prerequisites Gardener Installation  TL;DR  Kubectl Aliases   Step 1: Clone the Repositories and get Dependencies  Submodule Management   Step 2: Configure the Landscape  Building the \u0026lsquo;landscape.yaml\u0026rsquo; File The Base Cluster  Kubify Shoot Cluster Using an Arbitrary Base Cluster     Step 3: Build and Run Docker Container Step 4-10: Deploying Components  Undeploying Components The \u0026lsquo;all\u0026rsquo; Component   Step 4-10: Deploying Components (detailed)  Step 4: Kubify / etcd Step 5: Generate Certificates Step 6: Deploy tiller Step 7: Deploy Gardener Step 8: Register Garden Cluster as Seed Cluster  Configuring Additional Seeds Creating a Shoot   Step 9: Install Identity and Dashboard  Create CNAME Entry   Step 10: Apply Valid Certificates Letsencrypt Quota Limits  Accessing the Dashboard       Tearing Down the Landscape Cleanup  Prerequisites Before getting started make sure you have the following at hand:\n You need a cloud account with sufficient quota to set up a Kubernetes cluster with a couple of VMs. The Gardener supports AWS, Azure, GCP, and Openstack, but this simplified setup currently only supports AWS and Openstack. A Linux machine (virtual machine is fine) or a Mac with basic tools such as a git client and the Docker runtime installed.  Gardener Installation Follow these steps to install Gardener. Do not proceed to the next step in case of errors.\nTL;DR If you are already familiar with the installation procedure and just want a short summary of the commands you have to use, here it is:\n# setup git clone --recursive https://github.com/gardener/landscape-setup-template.git landscape # fill in landscape/landscape_config.yaml now cd landscape/setup ./docker_run.sh deploy all # ------------------------------------------------------------------- # teardown undeploy all ./cleanup.sh Otherwise, follow the detailed guide below.\nKubectl Aliases The following aliases can be used within the docker container:\nk =\u0026gt; kubectl ks =\u0026gt; kubectl -n kube-system kg =\u0026gt; kubectl -n garden kn =\u0026gt; kubectl -n ka =\u0026gt; kubectl get --all-namespaces Bash completion works for all of them except for ka.\nStep 1: Clone the Repositories and get Dependencies Get the landscape-setup-template from GitHub and initialize the submodules:\ngit clone --recursive https://github.com/gardener/landscape-setup-template.git landscape cd landscape After step 2, this repository will contain all passwords and keys for your landscape. You will be in trouble if you loose them so we recommend that you store this landscape configuration in a private repository. It might be a good idea to change the origin so you do not accidentally publish your secrets to the public template repository.\nSubmodule Management This project needs the Gardener and dashboard as submodules. To avoid conflicts between the checked out versions and the ones specified in the landscape_base.yaml file, automatic version management has been added. As long as the managed field in the chart area of each submodule is set to true, the version specified in the tag field will be checked out before deploying.\nTo check vor the correct version, the VERSION file in the submodule\u0026rsquo;s main folder is read and compared to the tag in the config file. If the VERSION file doesn\u0026rsquo;t exist, the component is added as a submodule. If it exists, but the versions differ, the fitting version will be checked out. If it exists and the versions are identical, nothing is done. The automatic version management will fail if a) the component is already added as a submodule, but the VERSION file is missing or b) the landscape folder is not a git repo.\nIt is also possible to trigger the version update manually: call the manage_submodule function with gardener or dashboard as an argument, or run the manage_submodules.sh script which will update Gardener and dashboard. Both will only work from inside the docker container / with sourced init.sh file.\nStep 2: Configure the Landscape There is a landscape_config.yaml file in the landscape project. This is the only file that you need to modify - all other configuration files will be derived from this and the landscape_base.yaml file. The latter one contains the merging instructions as well as technical configurations and it shouldn\u0026rsquo;t be touched unless you know what you are doing.\nBuilding the \u0026lsquo;landscape.yaml\u0026rsquo; File Both config files - landscape_config.yaml and landscape_base.yaml - are merged into one landscape.yaml file which is then used as configuration for the scripts. Sourcing the init.sh file (which happens automatically when entering the docker image) will perform this merge unless the file already exists. This means if you change something in one of the original config files after the landscape.yaml file has already been created, you need to manually rebuild it in order for the changes to take effect.\n./build_landscape_yaml.sh This script will recreate the landscape.yaml file. It will also source the init.sh file again, as some of the environment variables are extracted from this file.\nThe Base Cluster Gardener extends the Kubernetes apiserver, so in order to deploy it, you need a Kubernetes cluster first. This setup gives you two options for this:\nKubify You can use Kubify to create the initial cluster. Kubify uses Terraform to create the cluster and it is integrated into this project - you don\u0026rsquo;t need to create the cluster yourself, just make sure you fill out all relevant parts of the config file.\nShoot Cluster A shoot cluster is a cluster created by a Gardener instance and it can be used as a base cluster for this project. These flags have to be set for the kube-apiserver (if not set, the Gardener will still work, but the dashboard won\u0026rsquo;t):\n--oidc-issuer-url=https://identity.ingress.\u0026lt;yourclusterdomain\u0026gt;--oidc-client-id=kube-kubectl--oidc-username-claim=email--oidc-groups-claim=groupsFor a shoot this can be done by setting issuerUrl, clientID, usernameClaim, and groupsClaim in spec.kubernetes.kubeAPIServer.oidcConfig in the shoot manifest.\nAlso make sure that the CIDRs of your base cluster and the from your Gardener spawned shoots don\u0026rsquo;t overlap - if you want to be able to create shoots from the Gardener dashboard later, then don\u0026rsquo;t use the default CIDRs for this base cluster.\nSome fields in the landscape_config.yaml are marked with # kubify only, they can be ignored when using a shoot as the base cluster. The etcd server address defaults to the address for Kubify and needs to be changed for the shoot setup (see the comments in the config file).\nThe kubeconfig for the base cluster is expected to be named kubeconfig and reside in the directory containing this project\u0026rsquo;s directory (next to the landscape_config.yaml file).\nUsing an Arbitrary Base Cluster While this setup has only been tested for clusters created by Kubify or the Gardener (shoot clusters), it is theoretically possible to use the shoot setup method to deploy the Gardener to an arbitrary kubernetes cluster.\nStep 3: Build and Run Docker Container First, cd into the folder containing this project.\nThen run the container:\n./docker_run.sh After this,\n you will be connected to the container via an interactive shell the landscape folder will be mounted in that container your current working directory will be setup folder setup/init.sh is sourced, meaning  the environment variables will be set kubectl will be configured to communicate with your cluster landscape.yaml file will have been created if it didn\u0026rsquo;t exist before    The docker_run.sh script searches for the image locally and pulls it from an image repository, if it isn\u0026rsquo;t found. If pulling the image doesn\u0026rsquo;t work for whatever reason, you can use the docker_build.sh script to build the image locally.\nStep 4-10: Deploying Components The Gardener deployment is splitted into components. A single component can be easily deployed using\ndeploy \u0026lt;component name\u0026gt; Please take care that most of the components depend on each other and therefore have to be deployed in the order given below.\nThe deploy command is added to the PATH environment variable and can thus be called from anywhere. Bash auto-completion can be used for the component names.\nUndeploying Components It is also possible to \u0026ldquo;undeploy\u0026rdquo; a component using\nundeploy \u0026lt;component name\u0026gt; Components need to be undeployed in the inverse order. Do not undeploy components without undeploying their successors in the component order first. Take care to delete all shoots before undeploying the gardener or seed-config components (although both undeploy scripts will check for that and trigger a deletion themselves).\nThe \u0026lsquo;all\u0026rsquo; Component The all component is a special component: it serves as a dummy to deploy several components in one go. Usually, manual intervention between deploying components is not necessary and most of them are deployed directly one after the other, so the all component makes the \u0026ldquo;normal\u0026rdquo; use-case easier.\nFor better control which components are deployed, a component range can be given as an argument. The argument should have the form \u0026lt;start component name\u0026gt;:\u0026lt;end component name\u0026gt; and then the start component, the end component, and all components in between will be deployed. The order of the arguments is taken from the environment variables with the $COMPONENT_ORDER_ prefix. The variable with the suffix that matches the clusters.base_cluster entry in the config file will be used.\nIt is also possible to drop one part of the range or to drop the whole argument. For missing parts the defaults will be used, which are the first and the last component of the active component order, respectively.\nThe undeploy command can also be used with the all component, but take care that the component order is inverted.\n# Examples # (start and end component are always inclusive) deploy all # deploys all components deploy all gardener:dashboard # deploys \u0026#39;gardener\u0026#39; through \u0026#39;dashboard\u0026#39; deploy all gardener: # deploys all components starting from \u0026#39;gardener\u0026#39; deploy all :gardener # deploys all components up to \u0026#39;gardener\u0026#39; # (all undeploy commands use the inverse component order) undeploy all # undeploys all components undeploy all :helm-tiller # undeploys all components up to \u0026#39;helm-tiller\u0026#39; undeploy all dashboard:cert # undeploys \u0026#39;dashboard\u0026#39; through \u0026#39;cert\u0026#39; Step 4-10: Deploying Components (detailed) Step 4: Kubify / etcd If you want to create a Kubify cluster, deploy the component:\ndeploy kubify The script will wait some time for the cluster to come up and then partly validate that the cluster is ready.\nIf you get errors during the cluster setup, just try to run the command again.\nOnce completed the following command should show all deployed pods:\nroot@c41327633d6d:/landscape# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-operator-75dcfcf4f7-xkm4h 1/1 Running 0 6m kube-system heapster-c8fb4f746-tvts6 2/2 Running 0 2m kube-system kube-apiserver-hcdnc 1/1 Running 0 6m [...]  If you already have a cluster, you don\u0026rsquo;t need Kubify. To deploy an etcd in your cluster, run\ndeploy etcd This component is not meant to be used in combination with Kubify and might require manual steps to make it work.\nIt should also be possible to plug in your own etcd - check the deploy scripts for the etcd and gardener components for information on where to put the certificates, etc.\nStep 5: Generate Certificates This step will generate a self-signed cluster CA and sign some certificates with it.\ndeploy cert Step 6: Deploy tiller Tiller is needed to deploy the Helm charts of Gardener and other components.\ndeploy helm-tiller Step 7: Deploy Gardener Now we can deploy Gardener. If the previous steps were executed successfully this should be completed in a couple of seconds.\ndeploy gardener You might see a couple of messages like these:\nGardener API server not yet reachable. Waiting... while the script waits for the Gardener to start. Once Gardener is up when the deployment script finished you can verify the correct setup by running the following command:\nkubectl get shoots No resources found. As we do not have a seed cluster yet we cannot create any shoot clusters. The Gardener itself is installed in the garden namespace:\nkubectl get po -n garden NAME READY STATUS RESTARTS AGE gardener-apiserver-56cc665667-nvrjl 1/1 Running 0 6m gardener-controller-manager-5c9f8db55-hfcts 1/1 Running 0 6m Step 8: Register Garden Cluster as Seed Cluster In heterogeneous productive environments one would run Gardener and seed in separate clusters but for simplicity and resource consumption reasons we will register the Gardener cluster that we have just created also as the seed cluster. Make sure that the seed_config in the landscape file is correct and matches the region that you are using. Keep in mind that image ids differ between regions as well. Also, valid credentials for the seed provider have to be specified in the authentication part of the landscape_config.yaml file (the etcd backups of the shoot clusters are stored on the seed).\ndeploy seed-config Configuring Additional Seeds By default, this step will create a seed for the cloud provider the Gardener has been deployed on and thus creating shoots on this provider will be possible. If you want to create shoots on other cloud providers, you will have to configure additional seeds. There are two options for that:\nIf the seed-config deploy script is called without any arguments (as shown above), it will create seeds for all providers specified in the seed_config.seeds section in the landscape_config.yaml file. By default, the only entry in that list is the cloud provider chosen for the Gardener cluster, but you can extend the list.\nIt is also possible to provide the seed-config deploy script with additional arguments specifying which seeds should be created. Multiple arguments can be given and the script will ignore the list in the landscape_config.yaml file when called with arguments. Only the specified seeds will be created, already existing seeds are not affected. If a given seed already exists, it will be updated to the current configuration.\nIn both cases, the corresponding variant nodes in authentication and seed_config have to be filled out in the config file.\nValid values for seeds are aws, az (for Azure), gcp, and openstack. Please note, that while it is possible to create seeds for any cloud provider on any cloud provider, shoot creation may not work across cloud providers for every combination. It should always work if seed (Gardener cluster in this setup) and shoot are on the same provider, though.\nCreating a Shoot That\u0026rsquo;s it! If everything went fine you should now be able to create shoot clusters. You can start with a sample manifest and create a shoot cluster by standard Kubernetes means:\nkubectl apply -f shoot-aws.yaml Step 9: Install Identity and Dashboard Creating clusters based on a shoot manifest is quite nice but also a little complex. While almost all aspects of a shoot cluster can be configured, it can be quite difficult for beginners, so go on and install the dashboard:\ndeploy identity [...] deploy dashboard [...] Create CNAME Entry Dashboard and identity need a CNAME entry pointing the domain *.ingress.\u0026lt;your cluster domain\u0026gt; to your cluster\u0026rsquo;s nginx ingress ip/hostname. Kubify creates this entry automatically. If you are not using kubify to create your base cluster, you can create the CNAME entry with the corresponding component:\ndeploy cname The script uses the AWS CLI to create the entry, so it will only work for route53.\nStep 10: Apply Valid Certificates The following command will install the cert-manager and request valid letsencrypt certificates for both the identity and dashboard ingresses:\ndeploy certmanager After a few minutes valid certificates should be installed.\nLetsencrypt Quota Limits Letsencrypt limits how many certificates you can get for the same host within a short time. To avoid hitting these limits, you can use the letsencrypt staging server for testing, which has a significantly higher rate limit but produces untrusted certificates.\nThe charts.[certmanager].live field in the config file allows to switch between live and staging server (remember to rebuild the landscape.yaml file after you changed something in the landscape_config.yaml file).\nAccessing the Dashboard After step 9 you will be able to access the Gardener dashboard. There is a difference in how you access it depending on whether you used the letsencrypt live server or the staging one (and thus have untrusted credentials in the latter case).\nThe print_dashboard_urls.sh script constructs two URLs from the domain name given in the landscape.yaml file and prints them.\nIf you have trusted certificates, just use the second one (the one for the dashboard) and everything will be fine.\nIf you used the letsencrypt staging server, you will need to visit the first link first. Your browser will show a warning regarding untrusted certificates, you need to ignore that warning. You will then see a nearly blank page with some 404 message. After that, you can open the dashboard link, ignore the certificate warning again and should be able to login. If you skip the first link, you will still be able to see the dashboard, but the login button probably won\u0026rsquo;t work. While you will be able to login and create projects with the untrusted certificates from the letsencrypt staging server, creating secrets or shoots won\u0026rsquo;t be possible. You\u0026rsquo;ll need trusted certificates for that.\nTo log into the dashboard, use the options you have specified in the identity chart part of the landscape_config.yaml.\nTearing Down the Landscape Make sure that you delete all shoot clusters prior to tearing down the cluster. Not deleting project resources before deleting the Gardener can also cause troubles, because the namespaces associated with the projects have a finalizer which can\u0026rsquo;t be handled anymore when the Gardener is gone. Both, shoots and projects, can be deleted using the delete_all.sh script (give \u0026lsquo;shoots\u0026rsquo; or \u0026lsquo;projects\u0026rsquo; as an argument). To delete a single shoot/project, use this script.\nThe following command should not return any shoot clusters:\nkubectl get shoots --all-namespaces No resources found. If you created your base cluster with the Kubify component, you can destroy it using the undeploy command:\nundeploy kubify Cleanup After destroying the Kubify cluster, there will be some files left that prevent you from simply starting the project up again.\nATTENTION: Only do this if you are sure the cluster has been completely destroyed! Since this removes the terraform state, an automated deletion of resources won\u0026rsquo;t be possible anymore - you will have to clean up any leftovers manually.\n./cleanup.sh This will reset your landscape folder to its initial state (including the deletion of landscape.yaml).\nThe script takes an optional \u0026ldquo;-y\u0026rdquo; argument to skip the confirmation.\n"},{"uri":"https://gardener.cloud/documentation/guides/landscape-setup/","title":"Landscape Setup","tags":[],"description":"","content":"\u0026mdash;DEPRECATED\u0026mdash; This project is outdated and won\u0026rsquo;t be updated anymore. Please use https://github.com/gardener/garden-setup instead!\nGardener Setup Scripts This README is the installation manual for a simple Gardener setup. The installation scripts in this repo are embedded in a configuration template in the landscape-setup-template project. You can find further information there.\nWe do recommend this simplified setup for demonstration purposes only. For productive workloads we do recommend that all components (Gardener/Seed/Shoot) run in their own IaaS accounts and that network policies are enabled and properly tested on the seed clusters. A documentation on how to do this is currently work in progress.\n Gardener Setup Scripts Prerequisites Gardener Installation  TL;DR  Kubectl Aliases   Step 1: Clone the Repositories and get Dependencies  Submodule Management   Step 2: Configure the Landscape  Building the \u0026lsquo;landscape.yaml\u0026rsquo; File The Base Cluster  Kubify Shoot Cluster Using an Arbitrary Base Cluster     Step 3: Build and Run Docker Container Step 4-10: Deploying Components  Undeploying Components The \u0026lsquo;all\u0026rsquo; Component   Step 4-10: Deploying Components (detailed)  Step 4: Kubify / etcd Step 5: Generate Certificates Step 6: Deploy tiller Step 7: Deploy Gardener Step 8: Register Garden Cluster as Seed Cluster  Configuring Additional Seeds Creating a Shoot   Step 9: Install Identity and Dashboard  Create CNAME Entry   Step 10: Apply Valid Certificates Letsencrypt Quota Limits  Accessing the Dashboard       Tearing Down the Landscape Cleanup  Prerequisites Before getting started make sure you have the following at hand:\n You need a cloud account with sufficient quota to set up a Kubernetes cluster with a couple of VMs. The Gardener supports AWS, Azure, GCP, and Openstack, but this simplified setup currently only supports AWS and Openstack. A Linux machine (virtual machine is fine) or a Mac with basic tools such as a git client and the Docker runtime installed.  Gardener Installation Follow these steps to install Gardener. Do not proceed to the next step in case of errors.\nTL;DR If you are already familiar with the installation procedure and just want a short summary of the commands you have to use, here it is:\n# setup git clone --recursive https://github.com/gardener/landscape-setup-template.git landscape # fill in landscape/landscape_config.yaml now cd landscape/setup ./docker_run.sh deploy all # ------------------------------------------------------------------- # teardown undeploy all ./cleanup.sh Otherwise, follow the detailed guide below.\nKubectl Aliases The following aliases can be used within the docker container:\nk =\u0026gt; kubectl ks =\u0026gt; kubectl -n kube-system kg =\u0026gt; kubectl -n garden kn =\u0026gt; kubectl -n ka =\u0026gt; kubectl get --all-namespaces Bash completion works for all of them except for ka.\nStep 1: Clone the Repositories and get Dependencies Get the landscape-setup-template from GitHub and initialize the submodules:\ngit clone --recursive https://github.com/gardener/landscape-setup-template.git landscape cd landscape After step 2, this repository will contain all passwords and keys for your landscape. You will be in trouble if you loose them so we recommend that you store this landscape configuration in a private repository. It might be a good idea to change the origin so you do not accidentally publish your secrets to the public template repository.\nSubmodule Management This project needs the Gardener and dashboard as submodules. To avoid conflicts between the checked out versions and the ones specified in the landscape_base.yaml file, automatic version management has been added. As long as the managed field in the chart area of each submodule is set to true, the version specified in the tag field will be checked out before deploying.\nTo check vor the correct version, the VERSION file in the submodule\u0026rsquo;s main folder is read and compared to the tag in the config file. If the VERSION file doesn\u0026rsquo;t exist, the component is added as a submodule. If it exists, but the versions differ, the fitting version will be checked out. If it exists and the versions are identical, nothing is done. The automatic version management will fail if a) the component is already added as a submodule, but the VERSION file is missing or b) the landscape folder is not a git repo.\nIt is also possible to trigger the version update manually: call the manage_submodule function with gardener or dashboard as an argument, or run the manage_submodules.sh script which will update Gardener and dashboard. Both will only work from inside the docker container / with sourced init.sh file.\nStep 2: Configure the Landscape There is a landscape_config.yaml file in the landscape project. This is the only file that you need to modify - all other configuration files will be derived from this and the landscape_base.yaml file. The latter one contains the merging instructions as well as technical configurations and it shouldn\u0026rsquo;t be touched unless you know what you are doing.\nBuilding the \u0026lsquo;landscape.yaml\u0026rsquo; File Both config files - landscape_config.yaml and landscape_base.yaml - are merged into one landscape.yaml file which is then used as configuration for the scripts. Sourcing the init.sh file (which happens automatically when entering the docker image) will perform this merge unless the file already exists. This means if you change something in one of the original config files after the landscape.yaml file has already been created, you need to manually rebuild it in order for the changes to take effect.\n./build_landscape_yaml.sh This script will recreate the landscape.yaml file. It will also source the init.sh file again, as some of the environment variables are extracted from this file.\nThe Base Cluster Gardener extends the Kubernetes apiserver, so in order to deploy it, you need a Kubernetes cluster first. This setup gives you two options for this:\nKubify You can use Kubify to create the initial cluster. Kubify uses Terraform to create the cluster and it is integrated into this project - you don\u0026rsquo;t need to create the cluster yourself, just make sure you fill out all relevant parts of the config file.\nShoot Cluster A shoot cluster is a cluster created by a Gardener instance and it can be used as a base cluster for this project. These flags have to be set for the kube-apiserver (if not set, the Gardener will still work, but the dashboard won\u0026rsquo;t):\n--oidc-issuer-url=https://identity.ingress.\u0026lt;yourclusterdomain\u0026gt;--oidc-client-id=kube-kubectl--oidc-username-claim=email--oidc-groups-claim=groupsFor a shoot this can be done by setting issuerUrl, clientID, usernameClaim, and groupsClaim in spec.kubernetes.kubeAPIServer.oidcConfig in the shoot manifest.\nAlso make sure that the CIDRs of your base cluster and the from your Gardener spawned shoots don\u0026rsquo;t overlap - if you want to be able to create shoots from the Gardener dashboard later, then don\u0026rsquo;t use the default CIDRs for this base cluster.\nSome fields in the landscape_config.yaml are marked with # kubify only, they can be ignored when using a shoot as the base cluster. The etcd server address defaults to the address for Kubify and needs to be changed for the shoot setup (see the comments in the config file).\nThe kubeconfig for the base cluster is expected to be named kubeconfig and reside in the directory containing this project\u0026rsquo;s directory (next to the landscape_config.yaml file).\nUsing an Arbitrary Base Cluster While this setup has only been tested for clusters created by Kubify or the Gardener (shoot clusters), it is theoretically possible to use the shoot setup method to deploy the Gardener to an arbitrary kubernetes cluster.\nStep 3: Build and Run Docker Container First, cd into the folder containing this project.\nThen run the container:\n./docker_run.sh After this,\n you will be connected to the container via an interactive shell the landscape folder will be mounted in that container your current working directory will be setup folder setup/init.sh is sourced, meaning  the environment variables will be set kubectl will be configured to communicate with your cluster landscape.yaml file will have been created if it didn\u0026rsquo;t exist before    The docker_run.sh script searches for the image locally and pulls it from an image repository, if it isn\u0026rsquo;t found. If pulling the image doesn\u0026rsquo;t work for whatever reason, you can use the docker_build.sh script to build the image locally.\nStep 4-10: Deploying Components The Gardener deployment is splitted into components. A single component can be easily deployed using\ndeploy \u0026lt;component name\u0026gt; Please take care that most of the components depend on each other and therefore have to be deployed in the order given below.\nThe deploy command is added to the PATH environment variable and can thus be called from anywhere. Bash auto-completion can be used for the component names.\nUndeploying Components It is also possible to \u0026ldquo;undeploy\u0026rdquo; a component using\nundeploy \u0026lt;component name\u0026gt; Components need to be undeployed in the inverse order. Do not undeploy components without undeploying their successors in the component order first. Take care to delete all shoots before undeploying the gardener or seed-config components (although both undeploy scripts will check for that and trigger a deletion themselves).\nThe \u0026lsquo;all\u0026rsquo; Component The all component is a special component: it serves as a dummy to deploy several components in one go. Usually, manual intervention between deploying components is not necessary and most of them are deployed directly one after the other, so the all component makes the \u0026ldquo;normal\u0026rdquo; use-case easier.\nFor better control which components are deployed, a component range can be given as an argument. The argument should have the form \u0026lt;start component name\u0026gt;:\u0026lt;end component name\u0026gt; and then the start component, the end component, and all components in between will be deployed. The order of the arguments is taken from the environment variables with the $COMPONENT_ORDER_ prefix. The variable with the suffix that matches the clusters.base_cluster entry in the config file will be used.\nIt is also possible to drop one part of the range or to drop the whole argument. For missing parts the defaults will be used, which are the first and the last component of the active component order, respectively.\nThe undeploy command can also be used with the all component, but take care that the component order is inverted.\n# Examples # (start and end component are always inclusive) deploy all # deploys all components deploy all gardener:dashboard # deploys \u0026#39;gardener\u0026#39; through \u0026#39;dashboard\u0026#39; deploy all gardener: # deploys all components starting from \u0026#39;gardener\u0026#39; deploy all :gardener # deploys all components up to \u0026#39;gardener\u0026#39; # (all undeploy commands use the inverse component order) undeploy all # undeploys all components undeploy all :helm-tiller # undeploys all components up to \u0026#39;helm-tiller\u0026#39; undeploy all dashboard:cert # undeploys \u0026#39;dashboard\u0026#39; through \u0026#39;cert\u0026#39; Step 4-10: Deploying Components (detailed) Step 4: Kubify / etcd If you want to create a Kubify cluster, deploy the component:\ndeploy kubify The script will wait some time for the cluster to come up and then partly validate that the cluster is ready.\nIf you get errors during the cluster setup, just try to run the command again.\nOnce completed the following command should show all deployed pods:\nroot@c41327633d6d:/landscape# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-operator-75dcfcf4f7-xkm4h 1/1 Running 0 6m kube-system heapster-c8fb4f746-tvts6 2/2 Running 0 2m kube-system kube-apiserver-hcdnc 1/1 Running 0 6m [...]  If you already have a cluster, you don\u0026rsquo;t need Kubify. To deploy an etcd in your cluster, run\ndeploy etcd This component is not meant to be used in combination with Kubify and might require manual steps to make it work.\nIt should also be possible to plug in your own etcd - check the deploy scripts for the etcd and gardener components for information on where to put the certificates, etc.\nStep 5: Generate Certificates This step will generate a self-signed cluster CA and sign some certificates with it.\ndeploy cert Step 6: Deploy tiller Tiller is needed to deploy the Helm charts of Gardener and other components.\ndeploy helm-tiller Step 7: Deploy Gardener Now we can deploy Gardener. If the previous steps were executed successfully this should be completed in a couple of seconds.\ndeploy gardener You might see a couple of messages like these:\nGardener API server not yet reachable. Waiting... while the script waits for the Gardener to start. Once Gardener is up when the deployment script finished you can verify the correct setup by running the following command:\nkubectl get shoots No resources found. As we do not have a seed cluster yet we cannot create any shoot clusters. The Gardener itself is installed in the garden namespace:\nkubectl get po -n garden NAME READY STATUS RESTARTS AGE gardener-apiserver-56cc665667-nvrjl 1/1 Running 0 6m gardener-controller-manager-5c9f8db55-hfcts 1/1 Running 0 6m Step 8: Register Garden Cluster as Seed Cluster In heterogeneous productive environments one would run Gardener and seed in separate clusters but for simplicity and resource consumption reasons we will register the Gardener cluster that we have just created also as the seed cluster. Make sure that the seed_config in the landscape file is correct and matches the region that you are using. Keep in mind that image ids differ between regions as well. Also, valid credentials for the seed provider have to be specified in the authentication part of the landscape_config.yaml file (the etcd backups of the shoot clusters are stored on the seed).\ndeploy seed-config Configuring Additional Seeds By default, this step will create a seed for the cloud provider the Gardener has been deployed on and thus creating shoots on this provider will be possible. If you want to create shoots on other cloud providers, you will have to configure additional seeds. There are two options for that:\nIf the seed-config deploy script is called without any arguments (as shown above), it will create seeds for all providers specified in the seed_config.seeds section in the landscape_config.yaml file. By default, the only entry in that list is the cloud provider chosen for the Gardener cluster, but you can extend the list.\nIt is also possible to provide the seed-config deploy script with additional arguments specifying which seeds should be created. Multiple arguments can be given and the script will ignore the list in the landscape_config.yaml file when called with arguments. Only the specified seeds will be created, already existing seeds are not affected. If a given seed already exists, it will be updated to the current configuration.\nIn both cases, the corresponding variant nodes in authentication and seed_config have to be filled out in the config file.\nValid values for seeds are aws, az (for Azure), gcp, and openstack. Please note, that while it is possible to create seeds for any cloud provider on any cloud provider, shoot creation may not work across cloud providers for every combination. It should always work if seed (Gardener cluster in this setup) and shoot are on the same provider, though.\nCreating a Shoot That\u0026rsquo;s it! If everything went fine you should now be able to create shoot clusters. You can start with a sample manifest and create a shoot cluster by standard Kubernetes means:\nkubectl apply -f shoot-aws.yaml Step 9: Install Identity and Dashboard Creating clusters based on a shoot manifest is quite nice but also a little complex. While almost all aspects of a shoot cluster can be configured, it can be quite difficult for beginners, so go on and install the dashboard:\ndeploy identity [...] deploy dashboard [...] Create CNAME Entry Dashboard and identity need a CNAME entry pointing the domain *.ingress.\u0026lt;your cluster domain\u0026gt; to your cluster\u0026rsquo;s nginx ingress ip/hostname. Kubify creates this entry automatically. If you are not using kubify to create your base cluster, you can create the CNAME entry with the corresponding component:\ndeploy cname The script uses the AWS CLI to create the entry, so it will only work for route53.\nStep 10: Apply Valid Certificates The following command will install the cert-manager and request valid letsencrypt certificates for both the identity and dashboard ingresses:\ndeploy certmanager After a few minutes valid certificates should be installed.\nLetsencrypt Quota Limits Letsencrypt limits how many certificates you can get for the same host within a short time. To avoid hitting these limits, you can use the letsencrypt staging server for testing, which has a significantly higher rate limit but produces untrusted certificates.\nThe charts.[certmanager].live field in the config file allows to switch between live and staging server (remember to rebuild the landscape.yaml file after you changed something in the landscape_config.yaml file).\nAccessing the Dashboard After step 9 you will be able to access the Gardener dashboard. There is a difference in how you access it depending on whether you used the letsencrypt live server or the staging one (and thus have untrusted credentials in the latter case).\nThe print_dashboard_urls.sh script constructs two URLs from the domain name given in the landscape.yaml file and prints them.\nIf you have trusted certificates, just use the second one (the one for the dashboard) and everything will be fine.\nIf you used the letsencrypt staging server, you will need to visit the first link first. Your browser will show a warning regarding untrusted certificates, you need to ignore that warning. You will then see a nearly blank page with some 404 message. After that, you can open the dashboard link, ignore the certificate warning again and should be able to login. If you skip the first link, you will still be able to see the dashboard, but the login button probably won\u0026rsquo;t work. While you will be able to login and create projects with the untrusted certificates from the letsencrypt staging server, creating secrets or shoots won\u0026rsquo;t be possible. You\u0026rsquo;ll need trusted certificates for that.\nTo log into the dashboard, use the options you have specified in the identity chart part of the landscape_config.yaml.\nTearing Down the Landscape Make sure that you delete all shoot clusters prior to tearing down the cluster. Not deleting project resources before deleting the Gardener can also cause troubles, because the namespaces associated with the projects have a finalizer which can\u0026rsquo;t be handled anymore when the Gardener is gone. Both, shoots and projects, can be deleted using the delete_all.sh script (give \u0026lsquo;shoots\u0026rsquo; or \u0026lsquo;projects\u0026rsquo; as an argument). To delete a single shoot/project, use this script.\nThe following command should not return any shoot clusters:\nkubectl get shoots --all-namespaces No resources found. If you created your base cluster with the Kubify component, you can destroy it using the undeploy command:\nundeploy kubify Cleanup After destroying the Kubify cluster, there will be some files left that prevent you from simply starting the project up again.\nATTENTION: Only do this if you are sure the cluster has been completely destroyed! Since this removes the terraform state, an automated deletion of resources won\u0026rsquo;t be possible anymore - you will have to clean up any leftovers manually.\n./cleanup.sh This will reset your landscape folder to its initial state (including the deletion of landscape.yaml).\nThe script takes an optional \u0026ldquo;-y\u0026rdquo; argument to skip the confirmation.\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/install_gardener/landscape-setup/","title":"Landscape Setup","tags":[],"description":"","content":"\u0026mdash;DEPRECATED\u0026mdash; This project is outdated and won\u0026rsquo;t be updated anymore. Please use https://github.com/gardener/garden-setup instead!\nGardener Setup Scripts This README is the installation manual for a simple Gardener setup. The installation scripts in this repo are embedded in a configuration template in the landscape-setup-template project. You can find further information there.\nWe do recommend this simplified setup for demonstration purposes only. For productive workloads we do recommend that all components (Gardener/Seed/Shoot) run in their own IaaS accounts and that network policies are enabled and properly tested on the seed clusters. A documentation on how to do this is currently work in progress.\n \u0026mdash;DEPRECATED\u0026mdash; Gardener Setup Scripts Prerequisites Gardener Installation  TL;DR  Kubectl Aliases   Step 1: Clone the Repositories and get Dependencies  Submodule Management   Step 2: Configure the Landscape  Building the \u0026lsquo;landscape.yaml\u0026rsquo; File The Base Cluster  Kubify Shoot Cluster Using an Arbitrary Base Cluster     Step 3: Build and Run Docker Container Step 4-10: Deploying Components  Undeploying Components The \u0026lsquo;all\u0026rsquo; Component   Step 4-10: Deploying Components (detailed)  Step 4: Kubify / etcd Step 5: Generate Certificates Step 6: Deploy tiller Step 7: Deploy Gardener Step 8: Register Garden Cluster as Seed Cluster  Configuring Additional Seeds Creating a Shoot   Step 9: Install Identity and Dashboard  Create CNAME Entry   Step 10: Apply Valid Certificates Letsencrypt Quota Limits  Accessing the Dashboard       Tearing Down the Landscape Cleanup  Prerequisites Before getting started make sure you have the following at hand:\n You need a cloud account with sufficient quota to set up a Kubernetes cluster with a couple of VMs. The Gardener supports AWS, Azure, GCP, and Openstack, but this simplified setup currently only supports AWS and Openstack. A Linux machine (virtual machine is fine) or a Mac with basic tools such as a git client and the Docker runtime installed.  Gardener Installation Follow these steps to install Gardener. Do not proceed to the next step in case of errors.\nTL;DR If you are already familiar with the installation procedure and just want a short summary of the commands you have to use, here it is:\n# setup git clone --recursive https://github.com/gardener/landscape-setup-template.git landscape # fill in landscape/landscape_config.yaml now cd landscape/setup ./docker_run.sh deploy all # ------------------------------------------------------------------- # teardown undeploy all ./cleanup.sh Otherwise, follow the detailed guide below.\nKubectl Aliases The following aliases can be used within the docker container:\nk =\u0026gt; kubectl ks =\u0026gt; kubectl -n kube-system kg =\u0026gt; kubectl -n garden kn =\u0026gt; kubectl -n ka =\u0026gt; kubectl get --all-namespaces Bash completion works for all of them except for ka.\nStep 1: Clone the Repositories and get Dependencies Get the landscape-setup-template from GitHub and initialize the submodules:\ngit clone --recursive https://github.com/gardener/landscape-setup-template.git landscape cd landscape After step 2, this repository will contain all passwords and keys for your landscape. You will be in trouble if you loose them so we recommend that you store this landscape configuration in a private repository. It might be a good idea to change the origin so you do not accidentally publish your secrets to the public template repository.\nSubmodule Management This project needs the Gardener and dashboard as submodules. To avoid conflicts between the checked out versions and the ones specified in the landscape_base.yaml file, automatic version management has been added. As long as the managed field in the chart area of each submodule is set to true, the version specified in the tag field will be checked out before deploying.\nTo check vor the correct version, the VERSION file in the submodule\u0026rsquo;s main folder is read and compared to the tag in the config file. If the VERSION file doesn\u0026rsquo;t exist, the component is added as a submodule. If it exists, but the versions differ, the fitting version will be checked out. If it exists and the versions are identical, nothing is done. The automatic version management will fail if a) the component is already added as a submodule, but the VERSION file is missing or b) the landscape folder is not a git repo.\nIt is also possible to trigger the version update manually: call the manage_submodule function with gardener or dashboard as an argument, or run the manage_submodules.sh script which will update Gardener and dashboard. Both will only work from inside the docker container / with sourced init.sh file.\nStep 2: Configure the Landscape There is a landscape_config.yaml file in the landscape project. This is the only file that you need to modify - all other configuration files will be derived from this and the landscape_base.yaml file. The latter one contains the merging instructions as well as technical configurations and it shouldn\u0026rsquo;t be touched unless you know what you are doing.\nBuilding the \u0026lsquo;landscape.yaml\u0026rsquo; File Both config files - landscape_config.yaml and landscape_base.yaml - are merged into one landscape.yaml file which is then used as configuration for the scripts. Sourcing the init.sh file (which happens automatically when entering the docker image) will perform this merge unless the file already exists. This means if you change something in one of the original config files after the landscape.yaml file has already been created, you need to manually rebuild it in order for the changes to take effect.\n./build_landscape_yaml.sh This script will recreate the landscape.yaml file. It will also source the init.sh file again, as some of the environment variables are extracted from this file.\nThe Base Cluster Gardener extends the Kubernetes apiserver, so in order to deploy it, you need a Kubernetes cluster first. This setup gives you two options for this:\nKubify You can use Kubify to create the initial cluster. Kubify uses Terraform to create the cluster and it is integrated into this project - you don\u0026rsquo;t need to create the cluster yourself, just make sure you fill out all relevant parts of the config file.\nShoot Cluster A shoot cluster is a cluster created by a Gardener instance and it can be used as a base cluster for this project. These flags have to be set for the kube-apiserver (if not set, the Gardener will still work, but the dashboard won\u0026rsquo;t):\n--oidc-issuer-url=https://identity.ingress.\u0026lt;yourclusterdomain\u0026gt;--oidc-client-id=kube-kubectl--oidc-username-claim=email--oidc-groups-claim=groupsFor a shoot this can be done by setting issuerUrl, clientID, usernameClaim, and groupsClaim in spec.kubernetes.kubeAPIServer.oidcConfig in the shoot manifest.\nAlso make sure that the CIDRs of your base cluster and the from your Gardener spawned shoots don\u0026rsquo;t overlap - if you want to be able to create shoots from the Gardener dashboard later, then don\u0026rsquo;t use the default CIDRs for this base cluster.\nSome fields in the landscape_config.yaml are marked with # kubify only, they can be ignored when using a shoot as the base cluster. The etcd server address defaults to the address for Kubify and needs to be changed for the shoot setup (see the comments in the config file).\nThe kubeconfig for the base cluster is expected to be named kubeconfig and reside in the directory containing this project\u0026rsquo;s directory (next to the landscape_config.yaml file).\nUsing an Arbitrary Base Cluster While this setup has only been tested for clusters created by Kubify or the Gardener (shoot clusters), it is theoretically possible to use the shoot setup method to deploy the Gardener to an arbitrary kubernetes cluster.\nStep 3: Build and Run Docker Container First, cd into the folder containing this project.\nThen run the container:\n./docker_run.sh After this,\n you will be connected to the container via an interactive shell the landscape folder will be mounted in that container your current working directory will be setup folder setup/init.sh is sourced, meaning  the environment variables will be set kubectl will be configured to communicate with your cluster landscape.yaml file will have been created if it didn\u0026rsquo;t exist before    The docker_run.sh script searches for the image locally and pulls it from an image repository, if it isn\u0026rsquo;t found. If pulling the image doesn\u0026rsquo;t work for whatever reason, you can use the docker_build.sh script to build the image locally.\nStep 4-10: Deploying Components The Gardener deployment is splitted into components. A single component can be easily deployed using\ndeploy \u0026lt;component name\u0026gt; Please take care that most of the components depend on each other and therefore have to be deployed in the order given below.\nThe deploy command is added to the PATH environment variable and can thus be called from anywhere. Bash auto-completion can be used for the component names.\nUndeploying Components It is also possible to \u0026ldquo;undeploy\u0026rdquo; a component using\nundeploy \u0026lt;component name\u0026gt; Components need to be undeployed in the inverse order. Do not undeploy components without undeploying their successors in the component order first. Take care to delete all shoots before undeploying the gardener or seed-config components (although both undeploy scripts will check for that and trigger a deletion themselves).\nThe \u0026lsquo;all\u0026rsquo; Component The all component is a special component: it serves as a dummy to deploy several components in one go. Usually, manual intervention between deploying components is not necessary and most of them are deployed directly one after the other, so the all component makes the \u0026ldquo;normal\u0026rdquo; use-case easier.\nFor better control which components are deployed, a component range can be given as an argument. The argument should have the form \u0026lt;start component name\u0026gt;:\u0026lt;end component name\u0026gt; and then the start component, the end component, and all components in between will be deployed. The order of the arguments is taken from the environment variables with the $COMPONENT_ORDER_ prefix. The variable with the suffix that matches the clusters.base_cluster entry in the config file will be used.\nIt is also possible to drop one part of the range or to drop the whole argument. For missing parts the defaults will be used, which are the first and the last component of the active component order, respectively.\nThe undeploy command can also be used with the all component, but take care that the component order is inverted.\n# Examples # (start and end component are always inclusive) deploy all # deploys all components deploy all gardener:dashboard # deploys \u0026#39;gardener\u0026#39; through \u0026#39;dashboard\u0026#39; deploy all gardener: # deploys all components starting from \u0026#39;gardener\u0026#39; deploy all :gardener # deploys all components up to \u0026#39;gardener\u0026#39; # (all undeploy commands use the inverse component order) undeploy all # undeploys all components undeploy all :helm-tiller # undeploys all components up to \u0026#39;helm-tiller\u0026#39; undeploy all dashboard:cert # undeploys \u0026#39;dashboard\u0026#39; through \u0026#39;cert\u0026#39; Step 4-10: Deploying Components (detailed) Step 4: Kubify / etcd If you want to create a Kubify cluster, deploy the component:\ndeploy kubify The script will wait some time for the cluster to come up and then partly validate that the cluster is ready.\nIf you get errors during the cluster setup, just try to run the command again.\nOnce completed the following command should show all deployed pods:\nroot@c41327633d6d:/landscape# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-operator-75dcfcf4f7-xkm4h 1/1 Running 0 6m kube-system heapster-c8fb4f746-tvts6 2/2 Running 0 2m kube-system kube-apiserver-hcdnc 1/1 Running 0 6m [...]  If you already have a cluster, you don\u0026rsquo;t need Kubify. To deploy an etcd in your cluster, run\ndeploy etcd This component is not meant to be used in combination with Kubify and might require manual steps to make it work.\nIt should also be possible to plug in your own etcd - check the deploy scripts for the etcd and gardener components for information on where to put the certificates, etc.\nStep 5: Generate Certificates This step will generate a self-signed cluster CA and sign some certificates with it.\ndeploy cert Step 6: Deploy tiller Tiller is needed to deploy the Helm charts of Gardener and other components.\ndeploy helm-tiller Step 7: Deploy Gardener Now we can deploy Gardener. If the previous steps were executed successfully this should be completed in a couple of seconds.\ndeploy gardener You might see a couple of messages like these:\nGardener API server not yet reachable. Waiting... while the script waits for the Gardener to start. Once Gardener is up when the deployment script finished you can verify the correct setup by running the following command:\nkubectl get shoots No resources found. As we do not have a seed cluster yet we cannot create any shoot clusters. The Gardener itself is installed in the garden namespace:\nkubectl get po -n garden NAME READY STATUS RESTARTS AGE gardener-apiserver-56cc665667-nvrjl 1/1 Running 0 6m gardener-controller-manager-5c9f8db55-hfcts 1/1 Running 0 6m Step 8: Register Garden Cluster as Seed Cluster In heterogeneous productive environments one would run Gardener and seed in separate clusters but for simplicity and resource consumption reasons we will register the Gardener cluster that we have just created also as the seed cluster. Make sure that the seed_config in the landscape file is correct and matches the region that you are using. Keep in mind that image ids differ between regions as well. Also, valid credentials for the seed provider have to be specified in the authentication part of the landscape_config.yaml file (the etcd backups of the shoot clusters are stored on the seed).\ndeploy seed-config Configuring Additional Seeds By default, this step will create a seed for the cloud provider the Gardener has been deployed on and thus creating shoots on this provider will be possible. If you want to create shoots on other cloud providers, you will have to configure additional seeds. There are two options for that:\nIf the seed-config deploy script is called without any arguments (as shown above), it will create seeds for all providers specified in the seed_config.seeds section in the landscape_config.yaml file. By default, the only entry in that list is the cloud provider chosen for the Gardener cluster, but you can extend the list.\nIt is also possible to provide the seed-config deploy script with additional arguments specifying which seeds should be created. Multiple arguments can be given and the script will ignore the list in the landscape_config.yaml file when called with arguments. Only the specified seeds will be created, already existing seeds are not affected. If a given seed already exists, it will be updated to the current configuration.\nIn both cases, the corresponding variant nodes in authentication and seed_config have to be filled out in the config file.\nValid values for seeds are aws, az (for Azure), gcp, and openstack. Please note, that while it is possible to create seeds for any cloud provider on any cloud provider, shoot creation may not work across cloud providers for every combination. It should always work if seed (Gardener cluster in this setup) and shoot are on the same provider, though.\nCreating a Shoot That\u0026rsquo;s it! If everything went fine you should now be able to create shoot clusters. You can start with a sample manifest and create a shoot cluster by standard Kubernetes means:\nkubectl apply -f shoot-aws.yaml Step 9: Install Identity and Dashboard Creating clusters based on a shoot manifest is quite nice but also a little complex. While almost all aspects of a shoot cluster can be configured, it can be quite difficult for beginners, so go on and install the dashboard:\ndeploy identity [...] deploy dashboard [...] Create CNAME Entry Dashboard and identity need a CNAME entry pointing the domain *.ingress.\u0026lt;your cluster domain\u0026gt; to your cluster\u0026rsquo;s nginx ingress ip/hostname. Kubify creates this entry automatically. If you are not using kubify to create your base cluster, you can create the CNAME entry with the corresponding component:\ndeploy cname The script uses the AWS CLI to create the entry, so it will only work for route53.\nStep 10: Apply Valid Certificates The following command will install the cert-manager and request valid letsencrypt certificates for both the identity and dashboard ingresses:\ndeploy certmanager After a few minutes valid certificates should be installed.\nLetsencrypt Quota Limits Letsencrypt limits how many certificates you can get for the same host within a short time. To avoid hitting these limits, you can use the letsencrypt staging server for testing, which has a significantly higher rate limit but produces untrusted certificates.\nThe charts.[certmanager].live field in the config file allows to switch between live and staging server (remember to rebuild the landscape.yaml file after you changed something in the landscape_config.yaml file).\nAccessing the Dashboard After step 9 you will be able to access the Gardener dashboard. There is a difference in how you access it depending on whether you used the letsencrypt live server or the staging one (and thus have untrusted credentials in the latter case).\nThe print_dashboard_urls.sh script constructs two URLs from the domain name given in the landscape.yaml file and prints them.\nIf you have trusted certificates, just use the second one (the one for the dashboard) and everything will be fine.\nIf you used the letsencrypt staging server, you will need to visit the first link first. Your browser will show a warning regarding untrusted certificates, you need to ignore that warning. You will then see a nearly blank page with some 404 message. After that, you can open the dashboard link, ignore the certificate warning again and should be able to login. If you skip the first link, you will still be able to see the dashboard, but the login button probably won\u0026rsquo;t work. While you will be able to login and create projects with the untrusted certificates from the letsencrypt staging server, creating secrets or shoots won\u0026rsquo;t be possible. You\u0026rsquo;ll need trusted certificates for that.\nTo log into the dashboard, use the options you have specified in the identity chart part of the landscape_config.yaml.\nTearing Down the Landscape Make sure that you delete all shoot clusters prior to tearing down the cluster. Not deleting project resources before deleting the Gardener can also cause troubles, because the namespaces associated with the projects have a finalizer which can\u0026rsquo;t be handled anymore when the Gardener is gone. Both, shoots and projects, can be deleted using the delete_all.sh script (give \u0026lsquo;shoots\u0026rsquo; or \u0026lsquo;projects\u0026rsquo; as an argument). To delete a single shoot/project, use this script.\nThe following command should not return any shoot clusters:\nkubectl get shoots --all-namespaces No resources found. If you created your base cluster with the Kubify component, you can destroy it using the undeploy command:\nundeploy kubify Cleanup After destroying the Kubify cluster, there will be some files left that prevent you from simply starting the project up again.\nATTENTION: Only do this if you are sure the cluster has been completely destroyed! Since this removes the terraform state, an automated deletion of resources won\u0026rsquo;t be possible anymore - you will have to clean up any leftovers manually.\n./cleanup.sh This will reset your landscape folder to its initial state (including the deletion of landscape.yaml).\nThe script takes an optional \u0026ldquo;-y\u0026rdquo; argument to skip the confirmation.\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/landscape-setup/","title":"Landscape Setup","tags":[],"description":"","content":"\u0026mdash;DEPRECATED\u0026mdash; This project is outdated and won\u0026rsquo;t be updated anymore. Please use https://github.com/gardener/garden-setup instead!\nGardener Setup Scripts This README is the installation manual for a simple Gardener setup. The installation scripts in this repo are embedded in a configuration template in the landscape-setup-template project. You can find further information there.\nWe do recommend this simplified setup for demonstration purposes only. For productive workloads we do recommend that all components (Gardener/Seed/Shoot) run in their own IaaS accounts and that network policies are enabled and properly tested on the seed clusters. A documentation on how to do this is currently work in progress.\n Gardener Setup Scripts Prerequisites Gardener Installation  TL;DR  Kubectl Aliases   Step 1: Clone the Repositories and get Dependencies  Submodule Management   Step 2: Configure the Landscape  Building the \u0026lsquo;landscape.yaml\u0026rsquo; File The Base Cluster  Kubify Shoot Cluster Using an Arbitrary Base Cluster     Step 3: Build and Run Docker Container Step 4-10: Deploying Components  Undeploying Components The \u0026lsquo;all\u0026rsquo; Component   Step 4-10: Deploying Components (detailed)  Step 4: Kubify / etcd Step 5: Generate Certificates Step 6: Deploy tiller Step 7: Deploy Gardener Step 8: Register Garden Cluster as Seed Cluster  Configuring Additional Seeds Creating a Shoot   Step 9: Install Identity and Dashboard  Create CNAME Entry   Step 10: Apply Valid Certificates Letsencrypt Quota Limits  Accessing the Dashboard       Tearing Down the Landscape Cleanup  Prerequisites Before getting started make sure you have the following at hand:\n You need a cloud account with sufficient quota to set up a Kubernetes cluster with a couple of VMs. The Gardener supports AWS, Azure, GCP, and Openstack, but this simplified setup currently only supports AWS and Openstack. A Linux machine (virtual machine is fine) or a Mac with basic tools such as a git client and the Docker runtime installed.  Gardener Installation Follow these steps to install Gardener. Do not proceed to the next step in case of errors.\nTL;DR If you are already familiar with the installation procedure and just want a short summary of the commands you have to use, here it is:\n# setup git clone --recursive https://github.com/gardener/landscape-setup-template.git landscape # fill in landscape/landscape_config.yaml now cd landscape/setup ./docker_run.sh deploy all # ------------------------------------------------------------------- # teardown undeploy all ./cleanup.sh Otherwise, follow the detailed guide below.\nKubectl Aliases The following aliases can be used within the docker container:\nk =\u0026gt; kubectl ks =\u0026gt; kubectl -n kube-system kg =\u0026gt; kubectl -n garden kn =\u0026gt; kubectl -n ka =\u0026gt; kubectl get --all-namespaces Bash completion works for all of them except for ka.\nStep 1: Clone the Repositories and get Dependencies Get the landscape-setup-template from GitHub and initialize the submodules:\ngit clone --recursive https://github.com/gardener/landscape-setup-template.git landscape cd landscape After step 2, this repository will contain all passwords and keys for your landscape. You will be in trouble if you loose them so we recommend that you store this landscape configuration in a private repository. It might be a good idea to change the origin so you do not accidentally publish your secrets to the public template repository.\nSubmodule Management This project needs the Gardener and dashboard as submodules. To avoid conflicts between the checked out versions and the ones specified in the landscape_base.yaml file, automatic version management has been added. As long as the managed field in the chart area of each submodule is set to true, the version specified in the tag field will be checked out before deploying.\nTo check vor the correct version, the VERSION file in the submodule\u0026rsquo;s main folder is read and compared to the tag in the config file. If the VERSION file doesn\u0026rsquo;t exist, the component is added as a submodule. If it exists, but the versions differ, the fitting version will be checked out. If it exists and the versions are identical, nothing is done. The automatic version management will fail if a) the component is already added as a submodule, but the VERSION file is missing or b) the landscape folder is not a git repo.\nIt is also possible to trigger the version update manually: call the manage_submodule function with gardener or dashboard as an argument, or run the manage_submodules.sh script which will update Gardener and dashboard. Both will only work from inside the docker container / with sourced init.sh file.\nStep 2: Configure the Landscape There is a landscape_config.yaml file in the landscape project. This is the only file that you need to modify - all other configuration files will be derived from this and the landscape_base.yaml file. The latter one contains the merging instructions as well as technical configurations and it shouldn\u0026rsquo;t be touched unless you know what you are doing.\nBuilding the \u0026lsquo;landscape.yaml\u0026rsquo; File Both config files - landscape_config.yaml and landscape_base.yaml - are merged into one landscape.yaml file which is then used as configuration for the scripts. Sourcing the init.sh file (which happens automatically when entering the docker image) will perform this merge unless the file already exists. This means if you change something in one of the original config files after the landscape.yaml file has already been created, you need to manually rebuild it in order for the changes to take effect.\n./build_landscape_yaml.sh This script will recreate the landscape.yaml file. It will also source the init.sh file again, as some of the environment variables are extracted from this file.\nThe Base Cluster Gardener extends the Kubernetes apiserver, so in order to deploy it, you need a Kubernetes cluster first. This setup gives you two options for this:\nKubify You can use Kubify to create the initial cluster. Kubify uses Terraform to create the cluster and it is integrated into this project - you don\u0026rsquo;t need to create the cluster yourself, just make sure you fill out all relevant parts of the config file.\nShoot Cluster A shoot cluster is a cluster created by a Gardener instance and it can be used as a base cluster for this project. These flags have to be set for the kube-apiserver (if not set, the Gardener will still work, but the dashboard won\u0026rsquo;t):\n--oidc-issuer-url=https://identity.ingress.\u0026lt;yourclusterdomain\u0026gt;--oidc-client-id=kube-kubectl--oidc-username-claim=email--oidc-groups-claim=groupsFor a shoot this can be done by setting issuerUrl, clientID, usernameClaim, and groupsClaim in spec.kubernetes.kubeAPIServer.oidcConfig in the shoot manifest.\nAlso make sure that the CIDRs of your base cluster and the from your Gardener spawned shoots don\u0026rsquo;t overlap - if you want to be able to create shoots from the Gardener dashboard later, then don\u0026rsquo;t use the default CIDRs for this base cluster.\nSome fields in the landscape_config.yaml are marked with # kubify only, they can be ignored when using a shoot as the base cluster. The etcd server address defaults to the address for Kubify and needs to be changed for the shoot setup (see the comments in the config file).\nThe kubeconfig for the base cluster is expected to be named kubeconfig and reside in the directory containing this project\u0026rsquo;s directory (next to the landscape_config.yaml file).\nUsing an Arbitrary Base Cluster While this setup has only been tested for clusters created by Kubify or the Gardener (shoot clusters), it is theoretically possible to use the shoot setup method to deploy the Gardener to an arbitrary kubernetes cluster.\nStep 3: Build and Run Docker Container First, cd into the folder containing this project.\nThen run the container:\n./docker_run.sh After this,\n you will be connected to the container via an interactive shell the landscape folder will be mounted in that container your current working directory will be setup folder setup/init.sh is sourced, meaning  the environment variables will be set kubectl will be configured to communicate with your cluster landscape.yaml file will have been created if it didn\u0026rsquo;t exist before    The docker_run.sh script searches for the image locally and pulls it from an image repository, if it isn\u0026rsquo;t found. If pulling the image doesn\u0026rsquo;t work for whatever reason, you can use the docker_build.sh script to build the image locally.\nStep 4-10: Deploying Components The Gardener deployment is splitted into components. A single component can be easily deployed using\ndeploy \u0026lt;component name\u0026gt; Please take care that most of the components depend on each other and therefore have to be deployed in the order given below.\nThe deploy command is added to the PATH environment variable and can thus be called from anywhere. Bash auto-completion can be used for the component names.\nUndeploying Components It is also possible to \u0026ldquo;undeploy\u0026rdquo; a component using\nundeploy \u0026lt;component name\u0026gt; Components need to be undeployed in the inverse order. Do not undeploy components without undeploying their successors in the component order first. Take care to delete all shoots before undeploying the gardener or seed-config components (although both undeploy scripts will check for that and trigger a deletion themselves).\nThe \u0026lsquo;all\u0026rsquo; Component The all component is a special component: it serves as a dummy to deploy several components in one go. Usually, manual intervention between deploying components is not necessary and most of them are deployed directly one after the other, so the all component makes the \u0026ldquo;normal\u0026rdquo; use-case easier.\nFor better control which components are deployed, a component range can be given as an argument. The argument should have the form \u0026lt;start component name\u0026gt;:\u0026lt;end component name\u0026gt; and then the start component, the end component, and all components in between will be deployed. The order of the arguments is taken from the environment variables with the $COMPONENT_ORDER_ prefix. The variable with the suffix that matches the clusters.base_cluster entry in the config file will be used.\nIt is also possible to drop one part of the range or to drop the whole argument. For missing parts the defaults will be used, which are the first and the last component of the active component order, respectively.\nThe undeploy command can also be used with the all component, but take care that the component order is inverted.\n# Examples # (start and end component are always inclusive) deploy all # deploys all components deploy all gardener:dashboard # deploys \u0026#39;gardener\u0026#39; through \u0026#39;dashboard\u0026#39; deploy all gardener: # deploys all components starting from \u0026#39;gardener\u0026#39; deploy all :gardener # deploys all components up to \u0026#39;gardener\u0026#39; # (all undeploy commands use the inverse component order) undeploy all # undeploys all components undeploy all :helm-tiller # undeploys all components up to \u0026#39;helm-tiller\u0026#39; undeploy all dashboard:cert # undeploys \u0026#39;dashboard\u0026#39; through \u0026#39;cert\u0026#39; Step 4-10: Deploying Components (detailed) Step 4: Kubify / etcd If you want to create a Kubify cluster, deploy the component:\ndeploy kubify The script will wait some time for the cluster to come up and then partly validate that the cluster is ready.\nIf you get errors during the cluster setup, just try to run the command again.\nOnce completed the following command should show all deployed pods:\nroot@c41327633d6d:/landscape# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-operator-75dcfcf4f7-xkm4h 1/1 Running 0 6m kube-system heapster-c8fb4f746-tvts6 2/2 Running 0 2m kube-system kube-apiserver-hcdnc 1/1 Running 0 6m [...]  If you already have a cluster, you don\u0026rsquo;t need Kubify. To deploy an etcd in your cluster, run\ndeploy etcd This component is not meant to be used in combination with Kubify and might require manual steps to make it work.\nIt should also be possible to plug in your own etcd - check the deploy scripts for the etcd and gardener components for information on where to put the certificates, etc.\nStep 5: Generate Certificates This step will generate a self-signed cluster CA and sign some certificates with it.\ndeploy cert Step 6: Deploy tiller Tiller is needed to deploy the Helm charts of Gardener and other components.\ndeploy helm-tiller Step 7: Deploy Gardener Now we can deploy Gardener. If the previous steps were executed successfully this should be completed in a couple of seconds.\ndeploy gardener You might see a couple of messages like these:\nGardener API server not yet reachable. Waiting... while the script waits for the Gardener to start. Once Gardener is up when the deployment script finished you can verify the correct setup by running the following command:\nkubectl get shoots No resources found. As we do not have a seed cluster yet we cannot create any shoot clusters. The Gardener itself is installed in the garden namespace:\nkubectl get po -n garden NAME READY STATUS RESTARTS AGE gardener-apiserver-56cc665667-nvrjl 1/1 Running 0 6m gardener-controller-manager-5c9f8db55-hfcts 1/1 Running 0 6m Step 8: Register Garden Cluster as Seed Cluster In heterogeneous productive environments one would run Gardener and seed in separate clusters but for simplicity and resource consumption reasons we will register the Gardener cluster that we have just created also as the seed cluster. Make sure that the seed_config in the landscape file is correct and matches the region that you are using. Keep in mind that image ids differ between regions as well. Also, valid credentials for the seed provider have to be specified in the authentication part of the landscape_config.yaml file (the etcd backups of the shoot clusters are stored on the seed).\ndeploy seed-config Configuring Additional Seeds By default, this step will create a seed for the cloud provider the Gardener has been deployed on and thus creating shoots on this provider will be possible. If you want to create shoots on other cloud providers, you will have to configure additional seeds. There are two options for that:\nIf the seed-config deploy script is called without any arguments (as shown above), it will create seeds for all providers specified in the seed_config.seeds section in the landscape_config.yaml file. By default, the only entry in that list is the cloud provider chosen for the Gardener cluster, but you can extend the list.\nIt is also possible to provide the seed-config deploy script with additional arguments specifying which seeds should be created. Multiple arguments can be given and the script will ignore the list in the landscape_config.yaml file when called with arguments. Only the specified seeds will be created, already existing seeds are not affected. If a given seed already exists, it will be updated to the current configuration.\nIn both cases, the corresponding variant nodes in authentication and seed_config have to be filled out in the config file.\nValid values for seeds are aws, az (for Azure), gcp, and openstack. Please note, that while it is possible to create seeds for any cloud provider on any cloud provider, shoot creation may not work across cloud providers for every combination. It should always work if seed (Gardener cluster in this setup) and shoot are on the same provider, though.\nCreating a Shoot That\u0026rsquo;s it! If everything went fine you should now be able to create shoot clusters. You can start with a sample manifest and create a shoot cluster by standard Kubernetes means:\nkubectl apply -f shoot-aws.yaml Step 9: Install Identity and Dashboard Creating clusters based on a shoot manifest is quite nice but also a little complex. While almost all aspects of a shoot cluster can be configured, it can be quite difficult for beginners, so go on and install the dashboard:\ndeploy identity [...] deploy dashboard [...] Create CNAME Entry Dashboard and identity need a CNAME entry pointing the domain *.ingress.\u0026lt;your cluster domain\u0026gt; to your cluster\u0026rsquo;s nginx ingress ip/hostname. Kubify creates this entry automatically. If you are not using kubify to create your base cluster, you can create the CNAME entry with the corresponding component:\ndeploy cname The script uses the AWS CLI to create the entry, so it will only work for route53.\nStep 10: Apply Valid Certificates The following command will install the cert-manager and request valid letsencrypt certificates for both the identity and dashboard ingresses:\ndeploy certmanager After a few minutes valid certificates should be installed.\nLetsencrypt Quota Limits Letsencrypt limits how many certificates you can get for the same host within a short time. To avoid hitting these limits, you can use the letsencrypt staging server for testing, which has a significantly higher rate limit but produces untrusted certificates.\nThe charts.[certmanager].live field in the config file allows to switch between live and staging server (remember to rebuild the landscape.yaml file after you changed something in the landscape_config.yaml file).\nAccessing the Dashboard After step 9 you will be able to access the Gardener dashboard. There is a difference in how you access it depending on whether you used the letsencrypt live server or the staging one (and thus have untrusted credentials in the latter case).\nThe print_dashboard_urls.sh script constructs two URLs from the domain name given in the landscape.yaml file and prints them.\nIf you have trusted certificates, just use the second one (the one for the dashboard) and everything will be fine.\nIf you used the letsencrypt staging server, you will need to visit the first link first. Your browser will show a warning regarding untrusted certificates, you need to ignore that warning. You will then see a nearly blank page with some 404 message. After that, you can open the dashboard link, ignore the certificate warning again and should be able to login. If you skip the first link, you will still be able to see the dashboard, but the login button probably won\u0026rsquo;t work. While you will be able to login and create projects with the untrusted certificates from the letsencrypt staging server, creating secrets or shoots won\u0026rsquo;t be possible. You\u0026rsquo;ll need trusted certificates for that.\nTo log into the dashboard, use the options you have specified in the identity chart part of the landscape_config.yaml.\nTearing Down the Landscape Make sure that you delete all shoot clusters prior to tearing down the cluster. Not deleting project resources before deleting the Gardener can also cause troubles, because the namespaces associated with the projects have a finalizer which can\u0026rsquo;t be handled anymore when the Gardener is gone. Both, shoots and projects, can be deleted using the delete_all.sh script (give \u0026lsquo;shoots\u0026rsquo; or \u0026lsquo;projects\u0026rsquo; as an argument). To delete a single shoot/project, use this script.\nThe following command should not return any shoot clusters:\nkubectl get shoots --all-namespaces No resources found. If you created your base cluster with the Kubify component, you can destroy it using the undeploy command:\nundeploy kubify Cleanup After destroying the Kubify cluster, there will be some files left that prevent you from simply starting the project up again.\nATTENTION: Only do this if you are sure the cluster has been completely destroyed! Since this removes the terraform state, an automated deletion of resources won\u0026rsquo;t be possible anymore - you will have to clean up any leftovers manually.\n./cleanup.sh This will reset your landscape folder to its initial state (including the deletion of landscape.yaml).\nThe script takes an optional \u0026ldquo;-y\u0026rdquo; argument to skip the confirmation.\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/install_gardener/landscape-setup/","title":"Landscape Setup","tags":[],"description":"","content":"\u0026mdash;DEPRECATED\u0026mdash; This project is outdated and won\u0026rsquo;t be updated anymore. Please use https://github.com/gardener/garden-setup instead!\nGardener Setup Scripts This README is the installation manual for a simple Gardener setup. The installation scripts in this repo are embedded in a configuration template in the landscape-setup-template project. You can find further information there.\nWe do recommend this simplified setup for demonstration purposes only. For productive workloads we do recommend that all components (Gardener/Seed/Shoot) run in their own IaaS accounts and that network policies are enabled and properly tested on the seed clusters. A documentation on how to do this is currently work in progress.\n \u0026mdash;DEPRECATED\u0026mdash; Gardener Setup Scripts Prerequisites Gardener Installation  TL;DR  Kubectl Aliases   Step 1: Clone the Repositories and get Dependencies  Submodule Management   Step 2: Configure the Landscape  Building the \u0026lsquo;landscape.yaml\u0026rsquo; File The Base Cluster  Kubify Shoot Cluster Using an Arbitrary Base Cluster     Step 3: Build and Run Docker Container Step 4-10: Deploying Components  Undeploying Components The \u0026lsquo;all\u0026rsquo; Component   Step 4-10: Deploying Components (detailed)  Step 4: Kubify / etcd Step 5: Generate Certificates Step 6: Deploy tiller Step 7: Deploy Gardener Step 8: Register Garden Cluster as Seed Cluster  Configuring Additional Seeds Creating a Shoot   Step 9: Install Identity and Dashboard  Create CNAME Entry   Step 10: Apply Valid Certificates Letsencrypt Quota Limits  Accessing the Dashboard       Tearing Down the Landscape Cleanup  Prerequisites Before getting started make sure you have the following at hand:\n You need a cloud account with sufficient quota to set up a Kubernetes cluster with a couple of VMs. The Gardener supports AWS, Azure, GCP, and Openstack, but this simplified setup currently only supports AWS and Openstack. A Linux machine (virtual machine is fine) or a Mac with basic tools such as a git client and the Docker runtime installed.  Gardener Installation Follow these steps to install Gardener. Do not proceed to the next step in case of errors.\nTL;DR If you are already familiar with the installation procedure and just want a short summary of the commands you have to use, here it is:\n# setup git clone --recursive https://github.com/gardener/landscape-setup-template.git landscape # fill in landscape/landscape_config.yaml now cd landscape/setup ./docker_run.sh deploy all # ------------------------------------------------------------------- # teardown undeploy all ./cleanup.sh Otherwise, follow the detailed guide below.\nKubectl Aliases The following aliases can be used within the docker container:\nk =\u0026gt; kubectl ks =\u0026gt; kubectl -n kube-system kg =\u0026gt; kubectl -n garden kn =\u0026gt; kubectl -n ka =\u0026gt; kubectl get --all-namespaces Bash completion works for all of them except for ka.\nStep 1: Clone the Repositories and get Dependencies Get the landscape-setup-template from GitHub and initialize the submodules:\ngit clone --recursive https://github.com/gardener/landscape-setup-template.git landscape cd landscape After step 2, this repository will contain all passwords and keys for your landscape. You will be in trouble if you loose them so we recommend that you store this landscape configuration in a private repository. It might be a good idea to change the origin so you do not accidentally publish your secrets to the public template repository.\nSubmodule Management This project needs the Gardener and dashboard as submodules. To avoid conflicts between the checked out versions and the ones specified in the landscape_base.yaml file, automatic version management has been added. As long as the managed field in the chart area of each submodule is set to true, the version specified in the tag field will be checked out before deploying.\nTo check vor the correct version, the VERSION file in the submodule\u0026rsquo;s main folder is read and compared to the tag in the config file. If the VERSION file doesn\u0026rsquo;t exist, the component is added as a submodule. If it exists, but the versions differ, the fitting version will be checked out. If it exists and the versions are identical, nothing is done. The automatic version management will fail if a) the component is already added as a submodule, but the VERSION file is missing or b) the landscape folder is not a git repo.\nIt is also possible to trigger the version update manually: call the manage_submodule function with gardener or dashboard as an argument, or run the manage_submodules.sh script which will update Gardener and dashboard. Both will only work from inside the docker container / with sourced init.sh file.\nStep 2: Configure the Landscape There is a landscape_config.yaml file in the landscape project. This is the only file that you need to modify - all other configuration files will be derived from this and the landscape_base.yaml file. The latter one contains the merging instructions as well as technical configurations and it shouldn\u0026rsquo;t be touched unless you know what you are doing.\nBuilding the \u0026lsquo;landscape.yaml\u0026rsquo; File Both config files - landscape_config.yaml and landscape_base.yaml - are merged into one landscape.yaml file which is then used as configuration for the scripts. Sourcing the init.sh file (which happens automatically when entering the docker image) will perform this merge unless the file already exists. This means if you change something in one of the original config files after the landscape.yaml file has already been created, you need to manually rebuild it in order for the changes to take effect.\n./build_landscape_yaml.sh This script will recreate the landscape.yaml file. It will also source the init.sh file again, as some of the environment variables are extracted from this file.\nThe Base Cluster Gardener extends the Kubernetes apiserver, so in order to deploy it, you need a Kubernetes cluster first. This setup gives you two options for this:\nKubify You can use Kubify to create the initial cluster. Kubify uses Terraform to create the cluster and it is integrated into this project - you don\u0026rsquo;t need to create the cluster yourself, just make sure you fill out all relevant parts of the config file.\nShoot Cluster A shoot cluster is a cluster created by a Gardener instance and it can be used as a base cluster for this project. These flags have to be set for the kube-apiserver (if not set, the Gardener will still work, but the dashboard won\u0026rsquo;t):\n--oidc-issuer-url=https://identity.ingress.\u0026lt;yourclusterdomain\u0026gt;--oidc-client-id=kube-kubectl--oidc-username-claim=email--oidc-groups-claim=groupsFor a shoot this can be done by setting issuerUrl, clientID, usernameClaim, and groupsClaim in spec.kubernetes.kubeAPIServer.oidcConfig in the shoot manifest.\nAlso make sure that the CIDRs of your base cluster and the from your Gardener spawned shoots don\u0026rsquo;t overlap - if you want to be able to create shoots from the Gardener dashboard later, then don\u0026rsquo;t use the default CIDRs for this base cluster.\nSome fields in the landscape_config.yaml are marked with # kubify only, they can be ignored when using a shoot as the base cluster. The etcd server address defaults to the address for Kubify and needs to be changed for the shoot setup (see the comments in the config file).\nThe kubeconfig for the base cluster is expected to be named kubeconfig and reside in the directory containing this project\u0026rsquo;s directory (next to the landscape_config.yaml file).\nUsing an Arbitrary Base Cluster While this setup has only been tested for clusters created by Kubify or the Gardener (shoot clusters), it is theoretically possible to use the shoot setup method to deploy the Gardener to an arbitrary kubernetes cluster.\nStep 3: Build and Run Docker Container First, cd into the folder containing this project.\nThen run the container:\n./docker_run.sh After this,\n you will be connected to the container via an interactive shell the landscape folder will be mounted in that container your current working directory will be setup folder setup/init.sh is sourced, meaning  the environment variables will be set kubectl will be configured to communicate with your cluster landscape.yaml file will have been created if it didn\u0026rsquo;t exist before    The docker_run.sh script searches for the image locally and pulls it from an image repository, if it isn\u0026rsquo;t found. If pulling the image doesn\u0026rsquo;t work for whatever reason, you can use the docker_build.sh script to build the image locally.\nStep 4-10: Deploying Components The Gardener deployment is splitted into components. A single component can be easily deployed using\ndeploy \u0026lt;component name\u0026gt; Please take care that most of the components depend on each other and therefore have to be deployed in the order given below.\nThe deploy command is added to the PATH environment variable and can thus be called from anywhere. Bash auto-completion can be used for the component names.\nUndeploying Components It is also possible to \u0026ldquo;undeploy\u0026rdquo; a component using\nundeploy \u0026lt;component name\u0026gt; Components need to be undeployed in the inverse order. Do not undeploy components without undeploying their successors in the component order first. Take care to delete all shoots before undeploying the gardener or seed-config components (although both undeploy scripts will check for that and trigger a deletion themselves).\nThe \u0026lsquo;all\u0026rsquo; Component The all component is a special component: it serves as a dummy to deploy several components in one go. Usually, manual intervention between deploying components is not necessary and most of them are deployed directly one after the other, so the all component makes the \u0026ldquo;normal\u0026rdquo; use-case easier.\nFor better control which components are deployed, a component range can be given as an argument. The argument should have the form \u0026lt;start component name\u0026gt;:\u0026lt;end component name\u0026gt; and then the start component, the end component, and all components in between will be deployed. The order of the arguments is taken from the environment variables with the $COMPONENT_ORDER_ prefix. The variable with the suffix that matches the clusters.base_cluster entry in the config file will be used.\nIt is also possible to drop one part of the range or to drop the whole argument. For missing parts the defaults will be used, which are the first and the last component of the active component order, respectively.\nThe undeploy command can also be used with the all component, but take care that the component order is inverted.\n# Examples # (start and end component are always inclusive) deploy all # deploys all components deploy all gardener:dashboard # deploys \u0026#39;gardener\u0026#39; through \u0026#39;dashboard\u0026#39; deploy all gardener: # deploys all components starting from \u0026#39;gardener\u0026#39; deploy all :gardener # deploys all components up to \u0026#39;gardener\u0026#39; # (all undeploy commands use the inverse component order) undeploy all # undeploys all components undeploy all :helm-tiller # undeploys all components up to \u0026#39;helm-tiller\u0026#39; undeploy all dashboard:cert # undeploys \u0026#39;dashboard\u0026#39; through \u0026#39;cert\u0026#39; Step 4-10: Deploying Components (detailed) Step 4: Kubify / etcd If you want to create a Kubify cluster, deploy the component:\ndeploy kubify The script will wait some time for the cluster to come up and then partly validate that the cluster is ready.\nIf you get errors during the cluster setup, just try to run the command again.\nOnce completed the following command should show all deployed pods:\nroot@c41327633d6d:/landscape# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-operator-75dcfcf4f7-xkm4h 1/1 Running 0 6m kube-system heapster-c8fb4f746-tvts6 2/2 Running 0 2m kube-system kube-apiserver-hcdnc 1/1 Running 0 6m [...]  If you already have a cluster, you don\u0026rsquo;t need Kubify. To deploy an etcd in your cluster, run\ndeploy etcd This component is not meant to be used in combination with Kubify and might require manual steps to make it work.\nIt should also be possible to plug in your own etcd - check the deploy scripts for the etcd and gardener components for information on where to put the certificates, etc.\nStep 5: Generate Certificates This step will generate a self-signed cluster CA and sign some certificates with it.\ndeploy cert Step 6: Deploy tiller Tiller is needed to deploy the Helm charts of Gardener and other components.\ndeploy helm-tiller Step 7: Deploy Gardener Now we can deploy Gardener. If the previous steps were executed successfully this should be completed in a couple of seconds.\ndeploy gardener You might see a couple of messages like these:\nGardener API server not yet reachable. Waiting... while the script waits for the Gardener to start. Once Gardener is up when the deployment script finished you can verify the correct setup by running the following command:\nkubectl get shoots No resources found. As we do not have a seed cluster yet we cannot create any shoot clusters. The Gardener itself is installed in the garden namespace:\nkubectl get po -n garden NAME READY STATUS RESTARTS AGE gardener-apiserver-56cc665667-nvrjl 1/1 Running 0 6m gardener-controller-manager-5c9f8db55-hfcts 1/1 Running 0 6m Step 8: Register Garden Cluster as Seed Cluster In heterogeneous productive environments one would run Gardener and seed in separate clusters but for simplicity and resource consumption reasons we will register the Gardener cluster that we have just created also as the seed cluster. Make sure that the seed_config in the landscape file is correct and matches the region that you are using. Keep in mind that image ids differ between regions as well. Also, valid credentials for the seed provider have to be specified in the authentication part of the landscape_config.yaml file (the etcd backups of the shoot clusters are stored on the seed).\ndeploy seed-config Configuring Additional Seeds By default, this step will create a seed for the cloud provider the Gardener has been deployed on and thus creating shoots on this provider will be possible. If you want to create shoots on other cloud providers, you will have to configure additional seeds. There are two options for that:\nIf the seed-config deploy script is called without any arguments (as shown above), it will create seeds for all providers specified in the seed_config.seeds section in the landscape_config.yaml file. By default, the only entry in that list is the cloud provider chosen for the Gardener cluster, but you can extend the list.\nIt is also possible to provide the seed-config deploy script with additional arguments specifying which seeds should be created. Multiple arguments can be given and the script will ignore the list in the landscape_config.yaml file when called with arguments. Only the specified seeds will be created, already existing seeds are not affected. If a given seed already exists, it will be updated to the current configuration.\nIn both cases, the corresponding variant nodes in authentication and seed_config have to be filled out in the config file.\nValid values for seeds are aws, az (for Azure), gcp, and openstack. Please note, that while it is possible to create seeds for any cloud provider on any cloud provider, shoot creation may not work across cloud providers for every combination. It should always work if seed (Gardener cluster in this setup) and shoot are on the same provider, though.\nCreating a Shoot That\u0026rsquo;s it! If everything went fine you should now be able to create shoot clusters. You can start with a sample manifest and create a shoot cluster by standard Kubernetes means:\nkubectl apply -f shoot-aws.yaml Step 9: Install Identity and Dashboard Creating clusters based on a shoot manifest is quite nice but also a little complex. While almost all aspects of a shoot cluster can be configured, it can be quite difficult for beginners, so go on and install the dashboard:\ndeploy identity [...] deploy dashboard [...] Create CNAME Entry Dashboard and identity need a CNAME entry pointing the domain *.ingress.\u0026lt;your cluster domain\u0026gt; to your cluster\u0026rsquo;s nginx ingress ip/hostname. Kubify creates this entry automatically. If you are not using kubify to create your base cluster, you can create the CNAME entry with the corresponding component:\ndeploy cname The script uses the AWS CLI to create the entry, so it will only work for route53.\nStep 10: Apply Valid Certificates The following command will install the cert-manager and request valid letsencrypt certificates for both the identity and dashboard ingresses:\ndeploy certmanager After a few minutes valid certificates should be installed.\nLetsencrypt Quota Limits Letsencrypt limits how many certificates you can get for the same host within a short time. To avoid hitting these limits, you can use the letsencrypt staging server for testing, which has a significantly higher rate limit but produces untrusted certificates.\nThe charts.[certmanager].live field in the config file allows to switch between live and staging server (remember to rebuild the landscape.yaml file after you changed something in the landscape_config.yaml file).\nAccessing the Dashboard After step 9 you will be able to access the Gardener dashboard. There is a difference in how you access it depending on whether you used the letsencrypt live server or the staging one (and thus have untrusted credentials in the latter case).\nThe print_dashboard_urls.sh script constructs two URLs from the domain name given in the landscape.yaml file and prints them.\nIf you have trusted certificates, just use the second one (the one for the dashboard) and everything will be fine.\nIf you used the letsencrypt staging server, you will need to visit the first link first. Your browser will show a warning regarding untrusted certificates, you need to ignore that warning. You will then see a nearly blank page with some 404 message. After that, you can open the dashboard link, ignore the certificate warning again and should be able to login. If you skip the first link, you will still be able to see the dashboard, but the login button probably won\u0026rsquo;t work. While you will be able to login and create projects with the untrusted certificates from the letsencrypt staging server, creating secrets or shoots won\u0026rsquo;t be possible. You\u0026rsquo;ll need trusted certificates for that.\nTo log into the dashboard, use the options you have specified in the identity chart part of the landscape_config.yaml.\nTearing Down the Landscape Make sure that you delete all shoot clusters prior to tearing down the cluster. Not deleting project resources before deleting the Gardener can also cause troubles, because the namespaces associated with the projects have a finalizer which can\u0026rsquo;t be handled anymore when the Gardener is gone. Both, shoots and projects, can be deleted using the delete_all.sh script (give \u0026lsquo;shoots\u0026rsquo; or \u0026lsquo;projects\u0026rsquo; as an argument). To delete a single shoot/project, use this script.\nThe following command should not return any shoot clusters:\nkubectl get shoots --all-namespaces No resources found. If you created your base cluster with the Kubify component, you can destroy it using the undeploy command:\nundeploy kubify Cleanup After destroying the Kubify cluster, there will be some files left that prevent you from simply starting the project up again.\nATTENTION: Only do this if you are sure the cluster has been completely destroyed! Since this removes the terraform state, an automated deletion of resources won\u0026rsquo;t be possible anymore - you will have to clean up any leftovers manually.\n./cleanup.sh This will reset your landscape folder to its initial state (including the deletion of landscape.yaml).\nThe script takes an optional \u0026ldquo;-y\u0026rdquo; argument to skip the confirmation.\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/landscape-setup/","title":"Landscape Setup","tags":[],"description":"","content":"\u0026mdash;DEPRECATED\u0026mdash; This project is outdated and won\u0026rsquo;t be updated anymore. Please use https://github.com/gardener/garden-setup instead!\nGardener Setup Scripts This README is the installation manual for a simple Gardener setup. The installation scripts in this repo are embedded in a configuration template in the landscape-setup-template project. You can find further information there.\nWe do recommend this simplified setup for demonstration purposes only. For productive workloads we do recommend that all components (Gardener/Seed/Shoot) run in their own IaaS accounts and that network policies are enabled and properly tested on the seed clusters. A documentation on how to do this is currently work in progress.\n Gardener Setup Scripts Prerequisites Gardener Installation  TL;DR  Kubectl Aliases   Step 1: Clone the Repositories and get Dependencies  Submodule Management   Step 2: Configure the Landscape  Building the \u0026lsquo;landscape.yaml\u0026rsquo; File The Base Cluster  Kubify Shoot Cluster Using an Arbitrary Base Cluster     Step 3: Build and Run Docker Container Step 4-10: Deploying Components  Undeploying Components The \u0026lsquo;all\u0026rsquo; Component   Step 4-10: Deploying Components (detailed)  Step 4: Kubify / etcd Step 5: Generate Certificates Step 6: Deploy tiller Step 7: Deploy Gardener Step 8: Register Garden Cluster as Seed Cluster  Configuring Additional Seeds Creating a Shoot   Step 9: Install Identity and Dashboard  Create CNAME Entry   Step 10: Apply Valid Certificates Letsencrypt Quota Limits  Accessing the Dashboard       Tearing Down the Landscape Cleanup  Prerequisites Before getting started make sure you have the following at hand:\n You need a cloud account with sufficient quota to set up a Kubernetes cluster with a couple of VMs. The Gardener supports AWS, Azure, GCP, and Openstack, but this simplified setup currently only supports AWS and Openstack. A Linux machine (virtual machine is fine) or a Mac with basic tools such as a git client and the Docker runtime installed.  Gardener Installation Follow these steps to install Gardener. Do not proceed to the next step in case of errors.\nTL;DR If you are already familiar with the installation procedure and just want a short summary of the commands you have to use, here it is:\n# setup git clone --recursive https://github.com/gardener/landscape-setup-template.git landscape # fill in landscape/landscape_config.yaml now cd landscape/setup ./docker_run.sh deploy all # ------------------------------------------------------------------- # teardown undeploy all ./cleanup.sh Otherwise, follow the detailed guide below.\nKubectl Aliases The following aliases can be used within the docker container:\nk =\u0026gt; kubectl ks =\u0026gt; kubectl -n kube-system kg =\u0026gt; kubectl -n garden kn =\u0026gt; kubectl -n ka =\u0026gt; kubectl get --all-namespaces Bash completion works for all of them except for ka.\nStep 1: Clone the Repositories and get Dependencies Get the landscape-setup-template from GitHub and initialize the submodules:\ngit clone --recursive https://github.com/gardener/landscape-setup-template.git landscape cd landscape After step 2, this repository will contain all passwords and keys for your landscape. You will be in trouble if you loose them so we recommend that you store this landscape configuration in a private repository. It might be a good idea to change the origin so you do not accidentally publish your secrets to the public template repository.\nSubmodule Management This project needs the Gardener and dashboard as submodules. To avoid conflicts between the checked out versions and the ones specified in the landscape_base.yaml file, automatic version management has been added. As long as the managed field in the chart area of each submodule is set to true, the version specified in the tag field will be checked out before deploying.\nTo check vor the correct version, the VERSION file in the submodule\u0026rsquo;s main folder is read and compared to the tag in the config file. If the VERSION file doesn\u0026rsquo;t exist, the component is added as a submodule. If it exists, but the versions differ, the fitting version will be checked out. If it exists and the versions are identical, nothing is done. The automatic version management will fail if a) the component is already added as a submodule, but the VERSION file is missing or b) the landscape folder is not a git repo.\nIt is also possible to trigger the version update manually: call the manage_submodule function with gardener or dashboard as an argument, or run the manage_submodules.sh script which will update Gardener and dashboard. Both will only work from inside the docker container / with sourced init.sh file.\nStep 2: Configure the Landscape There is a landscape_config.yaml file in the landscape project. This is the only file that you need to modify - all other configuration files will be derived from this and the landscape_base.yaml file. The latter one contains the merging instructions as well as technical configurations and it shouldn\u0026rsquo;t be touched unless you know what you are doing.\nBuilding the \u0026lsquo;landscape.yaml\u0026rsquo; File Both config files - landscape_config.yaml and landscape_base.yaml - are merged into one landscape.yaml file which is then used as configuration for the scripts. Sourcing the init.sh file (which happens automatically when entering the docker image) will perform this merge unless the file already exists. This means if you change something in one of the original config files after the landscape.yaml file has already been created, you need to manually rebuild it in order for the changes to take effect.\n./build_landscape_yaml.sh This script will recreate the landscape.yaml file. It will also source the init.sh file again, as some of the environment variables are extracted from this file.\nThe Base Cluster Gardener extends the Kubernetes apiserver, so in order to deploy it, you need a Kubernetes cluster first. This setup gives you two options for this:\nKubify You can use Kubify to create the initial cluster. Kubify uses Terraform to create the cluster and it is integrated into this project - you don\u0026rsquo;t need to create the cluster yourself, just make sure you fill out all relevant parts of the config file.\nShoot Cluster A shoot cluster is a cluster created by a Gardener instance and it can be used as a base cluster for this project. These flags have to be set for the kube-apiserver (if not set, the Gardener will still work, but the dashboard won\u0026rsquo;t):\n--oidc-issuer-url=https://identity.ingress.\u0026lt;yourclusterdomain\u0026gt;--oidc-client-id=kube-kubectl--oidc-username-claim=email--oidc-groups-claim=groupsFor a shoot this can be done by setting issuerUrl, clientID, usernameClaim, and groupsClaim in spec.kubernetes.kubeAPIServer.oidcConfig in the shoot manifest.\nAlso make sure that the CIDRs of your base cluster and the from your Gardener spawned shoots don\u0026rsquo;t overlap - if you want to be able to create shoots from the Gardener dashboard later, then don\u0026rsquo;t use the default CIDRs for this base cluster.\nSome fields in the landscape_config.yaml are marked with # kubify only, they can be ignored when using a shoot as the base cluster. The etcd server address defaults to the address for Kubify and needs to be changed for the shoot setup (see the comments in the config file).\nThe kubeconfig for the base cluster is expected to be named kubeconfig and reside in the directory containing this project\u0026rsquo;s directory (next to the landscape_config.yaml file).\nUsing an Arbitrary Base Cluster While this setup has only been tested for clusters created by Kubify or the Gardener (shoot clusters), it is theoretically possible to use the shoot setup method to deploy the Gardener to an arbitrary kubernetes cluster.\nStep 3: Build and Run Docker Container First, cd into the folder containing this project.\nThen run the container:\n./docker_run.sh After this,\n you will be connected to the container via an interactive shell the landscape folder will be mounted in that container your current working directory will be setup folder setup/init.sh is sourced, meaning  the environment variables will be set kubectl will be configured to communicate with your cluster landscape.yaml file will have been created if it didn\u0026rsquo;t exist before    The docker_run.sh script searches for the image locally and pulls it from an image repository, if it isn\u0026rsquo;t found. If pulling the image doesn\u0026rsquo;t work for whatever reason, you can use the docker_build.sh script to build the image locally.\nStep 4-10: Deploying Components The Gardener deployment is splitted into components. A single component can be easily deployed using\ndeploy \u0026lt;component name\u0026gt; Please take care that most of the components depend on each other and therefore have to be deployed in the order given below.\nThe deploy command is added to the PATH environment variable and can thus be called from anywhere. Bash auto-completion can be used for the component names.\nUndeploying Components It is also possible to \u0026ldquo;undeploy\u0026rdquo; a component using\nundeploy \u0026lt;component name\u0026gt; Components need to be undeployed in the inverse order. Do not undeploy components without undeploying their successors in the component order first. Take care to delete all shoots before undeploying the gardener or seed-config components (although both undeploy scripts will check for that and trigger a deletion themselves).\nThe \u0026lsquo;all\u0026rsquo; Component The all component is a special component: it serves as a dummy to deploy several components in one go. Usually, manual intervention between deploying components is not necessary and most of them are deployed directly one after the other, so the all component makes the \u0026ldquo;normal\u0026rdquo; use-case easier.\nFor better control which components are deployed, a component range can be given as an argument. The argument should have the form \u0026lt;start component name\u0026gt;:\u0026lt;end component name\u0026gt; and then the start component, the end component, and all components in between will be deployed. The order of the arguments is taken from the environment variables with the $COMPONENT_ORDER_ prefix. The variable with the suffix that matches the clusters.base_cluster entry in the config file will be used.\nIt is also possible to drop one part of the range or to drop the whole argument. For missing parts the defaults will be used, which are the first and the last component of the active component order, respectively.\nThe undeploy command can also be used with the all component, but take care that the component order is inverted.\n# Examples # (start and end component are always inclusive) deploy all # deploys all components deploy all gardener:dashboard # deploys \u0026#39;gardener\u0026#39; through \u0026#39;dashboard\u0026#39; deploy all gardener: # deploys all components starting from \u0026#39;gardener\u0026#39; deploy all :gardener # deploys all components up to \u0026#39;gardener\u0026#39; # (all undeploy commands use the inverse component order) undeploy all # undeploys all components undeploy all :helm-tiller # undeploys all components up to \u0026#39;helm-tiller\u0026#39; undeploy all dashboard:cert # undeploys \u0026#39;dashboard\u0026#39; through \u0026#39;cert\u0026#39; Step 4-10: Deploying Components (detailed) Step 4: Kubify / etcd If you want to create a Kubify cluster, deploy the component:\ndeploy kubify The script will wait some time for the cluster to come up and then partly validate that the cluster is ready.\nIf you get errors during the cluster setup, just try to run the command again.\nOnce completed the following command should show all deployed pods:\nroot@c41327633d6d:/landscape# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-operator-75dcfcf4f7-xkm4h 1/1 Running 0 6m kube-system heapster-c8fb4f746-tvts6 2/2 Running 0 2m kube-system kube-apiserver-hcdnc 1/1 Running 0 6m [...]  If you already have a cluster, you don\u0026rsquo;t need Kubify. To deploy an etcd in your cluster, run\ndeploy etcd This component is not meant to be used in combination with Kubify and might require manual steps to make it work.\nIt should also be possible to plug in your own etcd - check the deploy scripts for the etcd and gardener components for information on where to put the certificates, etc.\nStep 5: Generate Certificates This step will generate a self-signed cluster CA and sign some certificates with it.\ndeploy cert Step 6: Deploy tiller Tiller is needed to deploy the Helm charts of Gardener and other components.\ndeploy helm-tiller Step 7: Deploy Gardener Now we can deploy Gardener. If the previous steps were executed successfully this should be completed in a couple of seconds.\ndeploy gardener You might see a couple of messages like these:\nGardener API server not yet reachable. Waiting... while the script waits for the Gardener to start. Once Gardener is up when the deployment script finished you can verify the correct setup by running the following command:\nkubectl get shoots No resources found. As we do not have a seed cluster yet we cannot create any shoot clusters. The Gardener itself is installed in the garden namespace:\nkubectl get po -n garden NAME READY STATUS RESTARTS AGE gardener-apiserver-56cc665667-nvrjl 1/1 Running 0 6m gardener-controller-manager-5c9f8db55-hfcts 1/1 Running 0 6m Step 8: Register Garden Cluster as Seed Cluster In heterogeneous productive environments one would run Gardener and seed in separate clusters but for simplicity and resource consumption reasons we will register the Gardener cluster that we have just created also as the seed cluster. Make sure that the seed_config in the landscape file is correct and matches the region that you are using. Keep in mind that image ids differ between regions as well. Also, valid credentials for the seed provider have to be specified in the authentication part of the landscape_config.yaml file (the etcd backups of the shoot clusters are stored on the seed).\ndeploy seed-config Configuring Additional Seeds By default, this step will create a seed for the cloud provider the Gardener has been deployed on and thus creating shoots on this provider will be possible. If you want to create shoots on other cloud providers, you will have to configure additional seeds. There are two options for that:\nIf the seed-config deploy script is called without any arguments (as shown above), it will create seeds for all providers specified in the seed_config.seeds section in the landscape_config.yaml file. By default, the only entry in that list is the cloud provider chosen for the Gardener cluster, but you can extend the list.\nIt is also possible to provide the seed-config deploy script with additional arguments specifying which seeds should be created. Multiple arguments can be given and the script will ignore the list in the landscape_config.yaml file when called with arguments. Only the specified seeds will be created, already existing seeds are not affected. If a given seed already exists, it will be updated to the current configuration.\nIn both cases, the corresponding variant nodes in authentication and seed_config have to be filled out in the config file.\nValid values for seeds are aws, az (for Azure), gcp, and openstack. Please note, that while it is possible to create seeds for any cloud provider on any cloud provider, shoot creation may not work across cloud providers for every combination. It should always work if seed (Gardener cluster in this setup) and shoot are on the same provider, though.\nCreating a Shoot That\u0026rsquo;s it! If everything went fine you should now be able to create shoot clusters. You can start with a sample manifest and create a shoot cluster by standard Kubernetes means:\nkubectl apply -f shoot-aws.yaml Step 9: Install Identity and Dashboard Creating clusters based on a shoot manifest is quite nice but also a little complex. While almost all aspects of a shoot cluster can be configured, it can be quite difficult for beginners, so go on and install the dashboard:\ndeploy identity [...] deploy dashboard [...] Create CNAME Entry Dashboard and identity need a CNAME entry pointing the domain *.ingress.\u0026lt;your cluster domain\u0026gt; to your cluster\u0026rsquo;s nginx ingress ip/hostname. Kubify creates this entry automatically. If you are not using kubify to create your base cluster, you can create the CNAME entry with the corresponding component:\ndeploy cname The script uses the AWS CLI to create the entry, so it will only work for route53.\nStep 10: Apply Valid Certificates The following command will install the cert-manager and request valid letsencrypt certificates for both the identity and dashboard ingresses:\ndeploy certmanager After a few minutes valid certificates should be installed.\nLetsencrypt Quota Limits Letsencrypt limits how many certificates you can get for the same host within a short time. To avoid hitting these limits, you can use the letsencrypt staging server for testing, which has a significantly higher rate limit but produces untrusted certificates.\nThe charts.[certmanager].live field in the config file allows to switch between live and staging server (remember to rebuild the landscape.yaml file after you changed something in the landscape_config.yaml file).\nAccessing the Dashboard After step 9 you will be able to access the Gardener dashboard. There is a difference in how you access it depending on whether you used the letsencrypt live server or the staging one (and thus have untrusted credentials in the latter case).\nThe print_dashboard_urls.sh script constructs two URLs from the domain name given in the landscape.yaml file and prints them.\nIf you have trusted certificates, just use the second one (the one for the dashboard) and everything will be fine.\nIf you used the letsencrypt staging server, you will need to visit the first link first. Your browser will show a warning regarding untrusted certificates, you need to ignore that warning. You will then see a nearly blank page with some 404 message. After that, you can open the dashboard link, ignore the certificate warning again and should be able to login. If you skip the first link, you will still be able to see the dashboard, but the login button probably won\u0026rsquo;t work. While you will be able to login and create projects with the untrusted certificates from the letsencrypt staging server, creating secrets or shoots won\u0026rsquo;t be possible. You\u0026rsquo;ll need trusted certificates for that.\nTo log into the dashboard, use the options you have specified in the identity chart part of the landscape_config.yaml.\nTearing Down the Landscape Make sure that you delete all shoot clusters prior to tearing down the cluster. Not deleting project resources before deleting the Gardener can also cause troubles, because the namespaces associated with the projects have a finalizer which can\u0026rsquo;t be handled anymore when the Gardener is gone. Both, shoots and projects, can be deleted using the delete_all.sh script (give \u0026lsquo;shoots\u0026rsquo; or \u0026lsquo;projects\u0026rsquo; as an argument). To delete a single shoot/project, use this script.\nThe following command should not return any shoot clusters:\nkubectl get shoots --all-namespaces No resources found. If you created your base cluster with the Kubify component, you can destroy it using the undeploy command:\nundeploy kubify Cleanup After destroying the Kubify cluster, there will be some files left that prevent you from simply starting the project up again.\nATTENTION: Only do this if you are sure the cluster has been completely destroyed! Since this removes the terraform state, an automated deletion of resources won\u0026rsquo;t be possible anymore - you will have to clean up any leftovers manually.\n./cleanup.sh This will reset your landscape folder to its initial state (including the deletion of landscape.yaml).\nThe script takes an optional \u0026ldquo;-y\u0026rdquo; argument to skip the confirmation.\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/logging-and-monitoring/","title":"Logging and Monitoring for Extensions","tags":[],"description":"","content":"Logging and Monitoring for Extensions Gardener provides an integrated logging and monitoring stack for alerting, monitoring and troubleshooting of its managed components by operators or end users. For further information how to make use of it in these roles, refer to the corresponding guides for exploring logs and for monitoring with Grafana.\nThe components that constitute the logging and monitoring stack are managed by Gardener. By default, it deploys Prometheus, Alertmanager and Grafana into the garden namespace of all seed clusters. If the Logging feature gate in the gardenlet configuration is enabled, it will deploy fluent-bit and Loki in the garden namespace too.\nEach shoot namespace hosts managed logging and monitoring components. As part of the shoot reconciliation flow, Gardener deploys a shoot-specific Prometheus, Grafana and, if configured, an Alertmanager into the shoot namespace, next to the other control plane components. If the Logging feature gate is enabled and the shoot purpose is not testing, it deploys a shoot-specific Loki in the shoot namespace too.\nThe logging and monitoring stack is extensible by configuration. Gardener extensions can take advantage of that and contribute configurations encoded in ConfigMaps for their own, specific dashboards, alerts, log parsers and other supported assets and integrate with it. As with other Gardener resources, they will be continuously reconciled.\nThis guide is about the roles and extensibility options of the logging and monitoring stack components, and how to integrate extensions with:\n Monitoring Logging  Monitoring The central Prometheus instance in the garden namespace fetches metrics and data from all seed cluster nodes and all seed cluster pods. It uses the federation concept to allow the shoot-specific instances to scrape only the metrics for the pods of the control plane they are responsible for. This mechanism allows to scrape the metrics for the nodes/pods once for the whole cluster, and to have them distributed afterwards.\nThe shoot-specific metrics are then made available to operators and users in the shoot Grafana, using the shoot Prometheus as data source.\nExtension controllers might deploy components as part of their reconciliation next to the shoot\u0026rsquo;s control plane. Examples for this would be a cloud-controller-manager or CSI controller deployments. Extensions that want to have their managed control plane components integrated with monitoring can contribute their per-shoot configuration for scraping Prometheus metrics, Alertmanager alerts or Grafana dashboards.\nExtensions monitoring integration Before deploying the shoot-specific Prometheus instance, Gardener will read all ConfigMaps in the shoot namespace, which are labeled with extensions.gardener.cloud/configuration=monitoring. Such ConfigMaps may contain four fields in their data:\n scrape_config: This field contains Prometheus scrape configuration for the component(s) and metrics that shall be scraped. alerting_rules: This field contains Alertmanager rules for alerts that shall be raised. dashboard_operators: This field contains a Grafana dashboard in JSON that is only relevant for Gardener operators. dashboard_users: This field contains a Grafana dashboard in JSON that is only relevant for Gardener users (shoot owners).  Example: A ControlPlane controller deploying a cloud-controller-manager into the shoot namespace wants to integrate monitoring configuration for scraping metrics, alerting rules and dashboards.\napiVersion:v1kind:ConfigMapmetadata:name:extension-controlplane-monitoring-ccmnamespace:shoot--project--namelabels:extensions.gardener.cloud/configuration:monitoringdata:scrape_config:| - job_name: cloud-controller-managerscheme:httpstls_config:insecure_skip_verify:truecert_file:/etc/prometheus/seed/prometheus.crtkey_file:/etc/prometheus/seed/prometheus.keyhonor_labels:falsekubernetes_sd_configs:- role:endpointsnamespaces:names:[shoot--project--name]relabel_configs:- source_labels:- __meta_kubernetes_service_name- __meta_kubernetes_endpoint_port_nameaction:keepregex:cloud-controller-manager;metrics# common metrics- action:labelmapregex:__meta_kubernetes_service_label_(.+)- source_labels:[__meta_kubernetes_pod_name]target_label:podmetric_relabel_configs:- process_max_fds- process_open_fdsalerting_rules:| cloud-controller-manager.rules.yaml: |groups:- name:cloud-controller-manager.rulesrules:- alert:CloudControllerManagerDownexpr:absent(up{job=\u0026#34;cloud-controller-manager\u0026#34;}==1)for:15mlabels:service:cloud-controller-managerseverity:criticaltype:seedvisibility:allannotations:description:Allinfrastructurespecificoperationscannotbecompleted(e.g.creatingloadbalancersorpersistentvolumes).summary:Cloudcontrollermanagerisdown.dashboard_operators:\u0026lt;some-json-describing-a-grafana-dashboard-for-operators\u0026gt; dashboard_users:\u0026lt;some-json-describing-a-grafana-dashboard-for-users\u0026gt;Logging In Kubernetes clusters, container logs are non-persistent and do not survive stopped and destroyed containers. Gardener addresses this problem for the components hosted in a seed cluster, by introducing its own managed logging solution. It is integrated with the Gardener monitoring stack to have all troubleshooting context in one place.\nGardener logging consists of components in three roles - log collectors and forwarders, log persistency and exploration/consumption interfaces. All of them live in the seed clusters in multiple instances:\n Logs are persisted by Loki instances deployed as StatefulSets - one per shoot namespace, if the Logging feature gate is enabled and the shoot purpose is not testing, and one in the garden namespace. The shoot instances store logs from the control plane components hosted there. The garden Loki instance is responsible for logs from the rest of the seed namespaces - kube-system, garden extension-* and others. Fluent-bit DaemonSets deployed on each seed node collect logs from it. A custom plugin takes care to distribute the collected log messages to the Loki instances that they are intended for. This allows to fetch the logs once for the whole cluster, and to distribute them afterwards. Grafana is the UI component used to explore monitoring and log data together for easier troubleshooting and in context. Grafana instances are configured to use the coresponding Loki instances, sharing the same namespace, as data providers. There is one Grafana Deployment in the garden namespace and two Deployments per shoot namespace (one exposed to the end users and one for the operators).  Logs can be produced from various sources, such as containers or systemd, and in different formats. The fluent-bit design supports configurable data pipeline to address that problem. Gardener provides such configuration for logs produced by all its core managed components as a ConfigMap. Extensions can contribute their own, specific configurations as ConfigMaps too. See for example the logging configuration for the Gardener AWS provider extension. The Gardener reconciliation loop watches such resources and updates the fluent-bit agents dynamically.\nExtensions logging integration Fluent-bit log parsers and filters To integrate with Gardener logging, extensions can and should specify how fluent-bit will handle the logs produced by the managed components that they contribute to Gardener. Normally, that would require to configure a parser for the specific logging format, if none of the available is applicable, and a filter defining how to apply it. For a complete reference for the configuration options, refer to fluent-bit\u0026rsquo;s documentation.\nNote: At the moment only parser and filter configurations are supported.\nTo contribute its own configuration to the fluent-bit agents data pipelines, an extension must provide it as a ConfigMap labeled extensions.gardener.cloud/configuration=logging and deployed in the seed\u0026rsquo;s garden namespace. Unlike the monitoring stack, where configurations are deployed per shoot, here a single configuration ConfigMap is sufficient and it applies to all fluent-bit agents in the seed. Its data field can have the following properties:\n filter-kubernetes.conf - configuration for data pipeline filters parser.conf - configuration for data pipeline parsers  Note: Take care to provide the correct data pipeline elements in the coresponding data field and not to mix them.\nExample: Logging configuration for provider-specific (OpenStack) worker controller deploying a machine-controller-manager component into a shoot namespace that reuses the kubeapiserverParser defined in fluent-bit-configmap.yaml to parse the component logs\napiVersion:v1kind:ConfigMapmetadata:name:gardener-extension-provider-openstack-logging-confignamespace:gardenlabels:extensions.gardener.cloud/configuration:loggingdata:filter-kubernetes.conf:| [FILTER]NameparserMatchkubernetes.machine-controller-manager*openstack-machine-controller-manager*Key_NamelogParserkubeapiserverParserReserve_DataTrueFurther details how to define parsers and use them with examples can be found in the following guide.\nGrafana The three types of Grafana instances found in a seed cluster are configured to expose logs of different origin in their dashboards:\n Garden Grafana dashboards expose logs from non-shoot namespaces of the seed clusters  Pod Logs Extensions Systemd Logs   Shoot User Grafana dashboards expose a subset of the logs shown to operators  Kube Apiserver Kube Controller Manager Kube Scheduler Cluster Autoscaler   Shoot Operator Grafana dashboards expose logs from the shoot cluster namespace where they belong  All user\u0026rsquo;s dashboards Kubernetes Pods    If the type of logs exposed in the Grafana instances needs to be changed, it is necessary to update the coresponding instance dashboard configurations.\nTips  Be careful to match exactly the log names that you need for a particular parser in your filters configuration. The regular expression you will supply will match names in the form kubernetes.pod_name.\u0026lt;metadata\u0026gt;.container_name. If there are extensions with the same container and pod names, they will all match the same parser in a filter. That may be a desired effect, if they all share the same log format. But it will be a problem if they don\u0026rsquo;t. To solve it, either the pod or container names must be unique, and the regular expression in the filter has to match that unique pattern. A recommended approach is to prefix containers with the extension name and tune the regular expression to match it. For example, using myextension-container as container name, and a regular expression kubernetes.mypod.*myextension-container will guarantee match of the right log name. Make sure that the regular expression does not match more than you expect. For example, kubernetes.systemd.*systemd.* will match both systemd-service and systemd-monitor-service. You will want to be as specific as possible. It\u0026rsquo;s a good idea to put the logging configuration into the Helm chart that also deploys the extension controller, while the monitoring configuration can be part of the Helm chart/deployment routine that deploys the component managed by the controller.  References and additional resources  GitHub issue describing the concept Exemplary implementation (monitoring) for the GCP provider Exemplary implementation (logging) for the OpenStack provider  "},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/logging-and-monitoring/","title":"Logging and Monitoring for Extensions","tags":[],"description":"","content":"Logging and Monitoring for Extensions Gardener provides an integrated logging and monitoring stack for alerting, monitoring and troubleshooting of its managed components by operators or end users. For further information how to make use of it in these roles, refer to the corresponding guides for exploring logs and for monitoring with Grafana.\nThe components that constitute the logging and monitoring stack are managed by Gardener. By default, it deploys Prometheus, Alertmanager and Grafana into the garden namespace of all seed clusters. If the Logging feature gate in the gardenlet configuration is enabled, it will deploy fluent-bit and Loki in the garden namespace too.\nEach shoot namespace hosts managed logging and monitoring components. As part of the shoot reconciliation flow, Gardener deploys a shoot-specific Prometheus, Grafana and, if configured, an Alertmanager into the shoot namespace, next to the other control plane components. If the Logging feature gate is enabled and the shoot purpose is not testing, it deploys a shoot-specific Loki in the shoot namespace too.\nThe logging and monitoring stack is extensible by configuration. Gardener extensions can take advantage of that and contribute configurations encoded in ConfigMaps for their own, specific dashboards, alerts, log parsers and other supported assets and integrate with it. As with other Gardener resources, they will be continuously reconciled.\nThis guide is about the roles and extensibility options of the logging and monitoring stack components, and how to integrate extensions with:\n Monitoring Logging  Monitoring The central Prometheus instance in the garden namespace fetches metrics and data from all seed cluster nodes and all seed cluster pods. It uses the federation concept to allow the shoot-specific instances to scrape only the metrics for the pods of the control plane they are responsible for. This mechanism allows to scrape the metrics for the nodes/pods once for the whole cluster, and to have them distributed afterwards.\nThe shoot-specific metrics are then made available to operators and users in the shoot Grafana, using the shoot Prometheus as data source.\nExtension controllers might deploy components as part of their reconciliation next to the shoot\u0026rsquo;s control plane. Examples for this would be a cloud-controller-manager or CSI controller deployments. Extensions that want to have their managed control plane components integrated with monitoring can contribute their per-shoot configuration for scraping Prometheus metrics, Alertmanager alerts or Grafana dashboards.\nExtensions monitoring integration Before deploying the shoot-specific Prometheus instance, Gardener will read all ConfigMaps in the shoot namespace, which are labeled with extensions.gardener.cloud/configuration=monitoring. Such ConfigMaps may contain four fields in their data:\n scrape_config: This field contains Prometheus scrape configuration for the component(s) and metrics that shall be scraped. alerting_rules: This field contains Alertmanager rules for alerts that shall be raised. dashboard_operators: This field contains a Grafana dashboard in JSON that is only relevant for Gardener operators. dashboard_users: This field contains a Grafana dashboard in JSON that is only relevant for Gardener users (shoot owners).  Example: A ControlPlane controller deploying a cloud-controller-manager into the shoot namespace wants to integrate monitoring configuration for scraping metrics, alerting rules and dashboards.\napiVersion:v1kind:ConfigMapmetadata:name:extension-controlplane-monitoring-ccmnamespace:shoot--project--namelabels:extensions.gardener.cloud/configuration:monitoringdata:scrape_config:| - job_name: cloud-controller-managerscheme:httpstls_config:insecure_skip_verify:truecert_file:/etc/prometheus/seed/prometheus.crtkey_file:/etc/prometheus/seed/prometheus.keyhonor_labels:falsekubernetes_sd_configs:- role:endpointsnamespaces:names:[shoot--project--name]relabel_configs:- source_labels:- __meta_kubernetes_service_name- __meta_kubernetes_endpoint_port_nameaction:keepregex:cloud-controller-manager;metrics# common metrics- action:labelmapregex:__meta_kubernetes_service_label_(.+)- source_labels:[__meta_kubernetes_pod_name]target_label:podmetric_relabel_configs:- process_max_fds- process_open_fdsalerting_rules:| cloud-controller-manager.rules.yaml: |groups:- name:cloud-controller-manager.rulesrules:- alert:CloudControllerManagerDownexpr:absent(up{job=\u0026#34;cloud-controller-manager\u0026#34;}==1)for:15mlabels:service:cloud-controller-managerseverity:criticaltype:seedvisibility:allannotations:description:Allinfrastructurespecificoperationscannotbecompleted(e.g.creatingloadbalancersorpersistentvolumes).summary:Cloudcontrollermanagerisdown.dashboard_operators:\u0026lt;some-json-describing-a-grafana-dashboard-for-operators\u0026gt; dashboard_users:\u0026lt;some-json-describing-a-grafana-dashboard-for-users\u0026gt;Logging In Kubernetes clusters, container logs are non-persistent and do not survive stopped and destroyed containers. Gardener addresses this problem for the components hosted in a seed cluster, by introducing its own managed logging solution. It is integrated with the Gardener monitoring stack to have all troubleshooting context in one place.\nGardener logging consists of components in three roles - log collectors and forwarders, log persistency and exploration/consumption interfaces. All of them live in the seed clusters in multiple instances:\n Logs are persisted by Loki instances deployed as StatefulSets - one per shoot namespace, if the Logging feature gate is enabled and the shoot purpose is not testing, and one in the garden namespace. The shoot instances store logs from the control plane components hosted there. The garden Loki instance is responsible for logs from the rest of the seed namespaces - kube-system, garden extension-* and others. Fluent-bit DaemonSets deployed on each seed node collect logs from it. A custom plugin takes care to distribute the collected log messages to the Loki instances that they are intended for. This allows to fetch the logs once for the whole cluster, and to distribute them afterwards. Grafana is the UI component used to explore monitoring and log data together for easier troubleshooting and in context. Grafana instances are configured to use the coresponding Loki instances, sharing the same namespace, as data providers. There is one Grafana Deployment in the garden namespace and two Deployments per shoot namespace (one exposed to the end users and one for the operators).  Logs can be produced from various sources, such as containers or systemd, and in different formats. The fluent-bit design supports configurable data pipeline to address that problem. Gardener provides such configuration for logs produced by all its core managed components as a ConfigMap. Extensions can contribute their own, specific configurations as ConfigMaps too. See for example the logging configuration for the Gardener AWS provider extension. The Gardener reconciliation loop watches such resources and updates the fluent-bit agents dynamically.\nExtensions logging integration Fluent-bit log parsers and filters To integrate with Gardener logging, extensions can and should specify how fluent-bit will handle the logs produced by the managed components that they contribute to Gardener. Normally, that would require to configure a parser for the specific logging format, if none of the available is applicable, and a filter defining how to apply it. For a complete reference for the configuration options, refer to fluent-bit\u0026rsquo;s documentation.\nNote: At the moment only parser and filter configurations are supported.\nTo contribute its own configuration to the fluent-bit agents data pipelines, an extension must provide it as a ConfigMap labeled extensions.gardener.cloud/configuration=logging and deployed in the seed\u0026rsquo;s garden namespace. Unlike the monitoring stack, where configurations are deployed per shoot, here a single configuration ConfigMap is sufficient and it applies to all fluent-bit agents in the seed. Its data field can have the following properties:\n filter-kubernetes.conf - configuration for data pipeline filters parser.conf - configuration for data pipeline parsers  Note: Take care to provide the correct data pipeline elements in the coresponding data field and not to mix them.\nExample: Logging configuration for provider-specific (OpenStack) worker controller deploying a machine-controller-manager component into a shoot namespace that reuses the kubeapiserverParser defined in fluent-bit-configmap.yaml to parse the component logs\napiVersion:v1kind:ConfigMapmetadata:name:gardener-extension-provider-openstack-logging-confignamespace:gardenlabels:extensions.gardener.cloud/configuration:loggingdata:filter-kubernetes.conf:| [FILTER]NameparserMatchkubernetes.machine-controller-manager*openstack-machine-controller-manager*Key_NamelogParserkubeapiserverParserReserve_DataTrueFurther details how to define parsers and use them with examples can be found in the following guide.\nGrafana The three types of Grafana instances found in a seed cluster are configured to expose logs of different origin in their dashboards:\n Garden Grafana dashboards expose logs from non-shoot namespaces of the seed clusters  Pod Logs Extensions Systemd Logs   Shoot User Grafana dashboards expose a subset of the logs shown to operators  Kube Apiserver Kube Controller Manager Kube Scheduler Cluster Autoscaler   Shoot Operator Grafana dashboards expose logs from the shoot cluster namespace where they belong  All user\u0026rsquo;s dashboards Kubernetes Pods    If the type of logs exposed in the Grafana instances needs to be changed, it is necessary to update the coresponding instance dashboard configurations.\nTips  Be careful to match exactly the log names that you need for a particular parser in your filters configuration. The regular expression you will supply will match names in the form kubernetes.pod_name.\u0026lt;metadata\u0026gt;.container_name. If there are extensions with the same container and pod names, they will all match the same parser in a filter. That may be a desired effect, if they all share the same log format. But it will be a problem if they don\u0026rsquo;t. To solve it, either the pod or container names must be unique, and the regular expression in the filter has to match that unique pattern. A recommended approach is to prefix containers with the extension name and tune the regular expression to match it. For example, using myextension-container as container name, and a regular expression kubernetes.mypod.*myextension-container will guarantee match of the right log name. Make sure that the regular expression does not match more than you expect. For example, kubernetes.systemd.*systemd.* will match both systemd-service and systemd-monitor-service. You will want to be as specific as possible. It\u0026rsquo;s a good idea to put the logging configuration into the Helm chart that also deploys the extension controller, while the monitoring configuration can be part of the Helm chart/deployment routine that deploys the component managed by the controller.  References and additional resources  GitHub issue describing the concept Exemplary implementation (monitoring) for the GCP provider Exemplary implementation (logging) for the OpenStack provider  "},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/logging-and-monitoring/","title":"Logging and Monitoring for Extensions","tags":[],"description":"","content":"Logging and Monitoring for Extensions Gardener provides an integrated logging and monitoring stack for alerting, monitoring and troubleshooting of its managed components by operators or end users. For further information how to make use of it in these roles, refer to the corresponding guides for exploring logs and for monitoring with Grafana.\nThe components that constitute the logging and monitoring stack are managed by Gardener. By default, it deploys Prometheus, Alertmanager and Grafana into the garden namespace of all seed clusters. If the Logging feature gate in the gardenlet configuration is enabled, it will deploy fluent-bit and Loki in the garden namespace too.\nEach shoot namespace hosts managed logging and monitoring components. As part of the shoot reconciliation flow, Gardener deploys a shoot-specific Prometheus, Grafana and, if configured, an Alertmanager into the shoot namespace, next to the other control plane components. If the Logging feature gate is enabled and the shoot purpose is not testing, it deploys a shoot-specific Loki in the shoot namespace too.\nThe logging and monitoring stack is extensible by configuration. Gardener extensions can take advantage of that and contribute configurations encoded in ConfigMaps for their own, specific dashboards, alerts, log parsers and other supported assets and integrate with it. As with other Gardener resources, they will be continuously reconciled.\nThis guide is about the roles and extensibility options of the logging and monitoring stack components, and how to integrate extensions with:\n Monitoring Logging  Monitoring The central Prometheus instance in the garden namespace fetches metrics and data from all seed cluster nodes and all seed cluster pods. It uses the federation concept to allow the shoot-specific instances to scrape only the metrics for the pods of the control plane they are responsible for. This mechanism allows to scrape the metrics for the nodes/pods once for the whole cluster, and to have them distributed afterwards.\nThe shoot-specific metrics are then made available to operators and users in the shoot Grafana, using the shoot Prometheus as data source.\nExtension controllers might deploy components as part of their reconciliation next to the shoot\u0026rsquo;s control plane. Examples for this would be a cloud-controller-manager or CSI controller deployments. Extensions that want to have their managed control plane components integrated with monitoring can contribute their per-shoot configuration for scraping Prometheus metrics, Alertmanager alerts or Grafana dashboards.\nExtensions monitoring integration Before deploying the shoot-specific Prometheus instance, Gardener will read all ConfigMaps in the shoot namespace, which are labeled with extensions.gardener.cloud/configuration=monitoring. Such ConfigMaps may contain four fields in their data:\n scrape_config: This field contains Prometheus scrape configuration for the component(s) and metrics that shall be scraped. alerting_rules: This field contains Alertmanager rules for alerts that shall be raised. dashboard_operators: This field contains a Grafana dashboard in JSON that is only relevant for Gardener operators. dashboard_users: This field contains a Grafana dashboard in JSON that is only relevant for Gardener users (shoot owners).  Example: A ControlPlane controller deploying a cloud-controller-manager into the shoot namespace wants to integrate monitoring configuration for scraping metrics, alerting rules and dashboards.\napiVersion:v1kind:ConfigMapmetadata:name:extension-controlplane-monitoring-ccmnamespace:shoot--project--namelabels:extensions.gardener.cloud/configuration:monitoringdata:scrape_config:| - job_name: cloud-controller-managerscheme:httpstls_config:insecure_skip_verify:truecert_file:/etc/prometheus/seed/prometheus.crtkey_file:/etc/prometheus/seed/prometheus.keyhonor_labels:falsekubernetes_sd_configs:- role:endpointsnamespaces:names:[shoot--project--name]relabel_configs:- source_labels:- __meta_kubernetes_service_name- __meta_kubernetes_endpoint_port_nameaction:keepregex:cloud-controller-manager;metrics# common metrics- action:labelmapregex:__meta_kubernetes_service_label_(.+)- source_labels:[__meta_kubernetes_pod_name]target_label:podmetric_relabel_configs:- process_max_fds- process_open_fdsalerting_rules:| cloud-controller-manager.rules.yaml: |groups:- name:cloud-controller-manager.rulesrules:- alert:CloudControllerManagerDownexpr:absent(up{job=\u0026#34;cloud-controller-manager\u0026#34;}==1)for:15mlabels:service:cloud-controller-managerseverity:criticaltype:seedvisibility:allannotations:description:Allinfrastructurespecificoperationscannotbecompleted(e.g.creatingloadbalancersorpersistentvolumes).summary:Cloudcontrollermanagerisdown.dashboard_operators:\u0026lt;some-json-describing-a-grafana-dashboard-for-operators\u0026gt; dashboard_users:\u0026lt;some-json-describing-a-grafana-dashboard-for-users\u0026gt;Logging In Kubernetes clusters, container logs are non-persistent and do not survive stopped and destroyed containers. Gardener addresses this problem for the components hosted in a seed cluster, by introducing its own managed logging solution. It is integrated with the Gardener monitoring stack to have all troubleshooting context in one place.\nGardener logging consists of components in three roles - log collectors and forwarders, log persistency and exploration/consumption interfaces. All of them live in the seed clusters in multiple instances:\n Logs are persisted by Loki instances deployed as StatefulSets - one per shoot namespace, if the Logging feature gate is enabled and the shoot purpose is not testing, and one in the garden namespace. The shoot instances store logs from the control plane components hosted there. The garden Loki instance is responsible for logs from the rest of the seed namespaces - kube-system, garden extension-* and others. Fluent-bit DaemonSets deployed on each seed node collect logs from it. A custom plugin takes care to distribute the collected log messages to the Loki instances that they are intended for. This allows to fetch the logs once for the whole cluster, and to distribute them afterwards. Grafana is the UI component used to explore monitoring and log data together for easier troubleshooting and in context. Grafana instances are configured to use the coresponding Loki instances, sharing the same namespace, as data providers. There is one Grafana Deployment in the garden namespace and two Deployments per shoot namespace (one exposed to the end users and one for the operators).  Logs can be produced from various sources, such as containers or systemd, and in different formats. The fluent-bit design supports configurable data pipeline to address that problem. Gardener provides such configuration for logs produced by all its core managed components as a ConfigMap. Extensions can contribute their own, specific configurations as ConfigMaps too. See for example the logging configuration for the Gardener AWS provider extension. The Gardener reconciliation loop watches such resources and updates the fluent-bit agents dynamically.\nExtensions logging integration Fluent-bit log parsers and filters To integrate with Gardener logging, extensions can and should specify how fluent-bit will handle the logs produced by the managed components that they contribute to Gardener. Normally, that would require to configure a parser for the specific logging format, if none of the available is applicable, and a filter defining how to apply it. For a complete reference for the configuration options, refer to fluent-bit\u0026rsquo;s documentation.\nNote: At the moment only parser and filter configurations are supported.\nTo contribute its own configuration to the fluent-bit agents data pipelines, an extension must provide it as a ConfigMap labeled extensions.gardener.cloud/configuration=logging and deployed in the seed\u0026rsquo;s garden namespace. Unlike the monitoring stack, where configurations are deployed per shoot, here a single configuration ConfigMap is sufficient and it applies to all fluent-bit agents in the seed. Its data field can have the following properties:\n filter-kubernetes.conf - configuration for data pipeline filters parser.conf - configuration for data pipeline parsers  Note: Take care to provide the correct data pipeline elements in the coresponding data field and not to mix them.\nExample: Logging configuration for provider-specific (OpenStack) worker controller deploying a machine-controller-manager component into a shoot namespace that reuses the kubeapiserverParser defined in fluent-bit-configmap.yaml to parse the component logs\napiVersion:v1kind:ConfigMapmetadata:name:gardener-extension-provider-openstack-logging-confignamespace:gardenlabels:extensions.gardener.cloud/configuration:loggingdata:filter-kubernetes.conf:| [FILTER]NameparserMatchkubernetes.machine-controller-manager*openstack-machine-controller-manager*Key_NamelogParserkubeapiserverParserReserve_DataTrueFurther details how to define parsers and use them with examples can be found in the following guide.\nGrafana The three types of Grafana instances found in a seed cluster are configured to expose logs of different origin in their dashboards:\n Garden Grafana dashboards expose logs from non-shoot namespaces of the seed clusters  Pod Logs Extensions Systemd Logs   Shoot User Grafana dashboards expose a subset of the logs shown to operators  Kube Apiserver Kube Controller Manager Kube Scheduler Cluster Autoscaler   Shoot Operator Grafana dashboards expose logs from the shoot cluster namespace where they belong  All user\u0026rsquo;s dashboards Kubernetes Pods    If the type of logs exposed in the Grafana instances needs to be changed, it is necessary to update the coresponding instance dashboard configurations.\nTips  Be careful to match exactly the log names that you need for a particular parser in your filters configuration. The regular expression you will supply will match names in the form kubernetes.pod_name.\u0026lt;metadata\u0026gt;.container_name. If there are extensions with the same container and pod names, they will all match the same parser in a filter. That may be a desired effect, if they all share the same log format. But it will be a problem if they don\u0026rsquo;t. To solve it, either the pod or container names must be unique, and the regular expression in the filter has to match that unique pattern. A recommended approach is to prefix containers with the extension name and tune the regular expression to match it. For example, using myextension-container as container name, and a regular expression kubernetes.mypod.*myextension-container will guarantee match of the right log name. Make sure that the regular expression does not match more than you expect. For example, kubernetes.systemd.*systemd.* will match both systemd-service and systemd-monitor-service. You will want to be as specific as possible. It\u0026rsquo;s a good idea to put the logging configuration into the Helm chart that also deploys the extension controller, while the monitoring configuration can be part of the Helm chart/deployment routine that deploys the component managed by the controller.  References and additional resources  GitHub issue describing the concept Exemplary implementation (monitoring) for the GCP provider Exemplary implementation (logging) for the OpenStack provider  "},{"uri":"https://gardener.cloud/documentation/concepts/mcm/","title":"Machine Controller Manager","tags":[],"description":"","content":"machine-controller-manager  \n:warning: We are in the progress of migrating and deprecating all the in-tree providers to OOT. Please avoid making any new feature enhancements to the intree providers. Kindly make it on the OOT providers available here. More details on adding new OOT providers can be found here.\nMachine Controller Manager (MCM) manages VMs as another kubernetes custom resource. It provides a declarative way to manage VMs.\nMCM supports following providers:\n AWS GCP Azure Openstack Alicloud Metal-stack Packet KubeVirt VMWare Yandex  It can easily be extended to support other cloud providers as well.\nExample of managing machine:\nkubectl create/get/delete machine vm1 Key terminologies Nodes/Machines/VMs are different terminologies used to represent similar things. We use these terms in the following way\n VM: A virtual machine running on any cloud provider. It could also refer to a physical machine (PM) in case of a bare metal setup. Node: Native kubernetes node objects. The objects you get to see when you do a \u0026ldquo;kubectl get nodes\u0026rdquo;. Although nodes can be either physical/virtual machines, for the purposes of our discussions it refers to a VM. Machine: A VM that is provisioned/managed by the Machine Controller Manager.  Design of Machine Controller Manager See the design documentation in the /docs/design repository, please find the design doc here.\nTo start using or developing the Machine Controller Manager See the documentation in the /docs repository, please find the index here.\nFAQ An FAQ is available here\nCluster-api Implementation  cluster-api branch of machine-controller-manager implements the machine-api aspect of the cluster-api project. Link: https://github.com/gardener/machine-controller-manager/tree/cluster-api Once cluster-api project gets stable, we may make master branch of MCM as well cluster-api compliant, with well-defined migration notes.  "},{"uri":"https://gardener.cloud/v1.12.8/concepts/mcm/","title":"Machine Controller Manager","tags":[],"description":"","content":"machine-controller-manager  \n:warning: We are in the progress of migrating and deprecating all the in-tree providers to OOT. Please avoid making any new feature enhancements to the intree providers. Kindly make it on the OOT providers available here. More details on adding new OOT providers can be found here.\nMachine Controller Manager (MCM) manages VMs as another kubernetes custom resource. It provides a declarative way to manage VMs.\nMCM supports following providers:\n AWS GCP Azure Openstack Alicloud Metal-stack Packet KubeVirt VMWare Yandex  It can easily be extended to support other cloud providers as well.\nExample of managing machine:\nkubectl create/get/delete machine vm1 Key terminologies Nodes/Machines/VMs are different terminologies used to represent similar things. We use these terms in the following way\n VM: A virtual machine running on any cloud provider. It could also refer to a physical machine (PM) in case of a bare metal setup. Node: Native kubernetes node objects. The objects you get to see when you do a \u0026ldquo;kubectl get nodes\u0026rdquo;. Although nodes can be either physical/virtual machines, for the purposes of our discussions it refers to a VM. Machine: A VM that is provisioned/managed by the Machine Controller Manager.  Design of Machine Controller Manager See the design documentation in the /docs/design repository, please find the design doc here.\nTo start using or developing the Machine Controller Manager See the documentation in the /docs repository, please find the index here.\nFAQ An FAQ is available here\nCluster-api Implementation  cluster-api branch of machine-controller-manager implements the machine-api aspect of the cluster-api project. Link: https://github.com/gardener/machine-controller-manager/tree/cluster-api Once cluster-api project gets stable, we may make master branch of MCM as well cluster-api compliant, with well-defined migration notes.  "},{"uri":"https://gardener.cloud/v1.13.2/concepts/mcm/","title":"Machine Controller Manager","tags":[],"description":"","content":"machine-controller-manager  \n:warning: We are in the progress of migrating and deprecating all the in-tree providers to OOT. Please avoid making any new feature enhancements to the intree providers. Kindly make it on the OOT providers available here. More details on adding new OOT providers can be found here.\nMachine Controller Manager (MCM) manages VMs as another kubernetes custom resource. It provides a declarative way to manage VMs.\nMCM supports following providers:\n AWS GCP Azure Openstack Alicloud Metal-stack Packet KubeVirt VMWare Yandex  It can easily be extended to support other cloud providers as well.\nExample of managing machine:\nkubectl create/get/delete machine vm1 Key terminologies Nodes/Machines/VMs are different terminologies used to represent similar things. We use these terms in the following way\n VM: A virtual machine running on any cloud provider. It could also refer to a physical machine (PM) in case of a bare metal setup. Node: Native kubernetes node objects. The objects you get to see when you do a \u0026ldquo;kubectl get nodes\u0026rdquo;. Although nodes can be either physical/virtual machines, for the purposes of our discussions it refers to a VM. Machine: A VM that is provisioned/managed by the Machine Controller Manager.  Design of Machine Controller Manager See the design documentation in the /docs/design repository, please find the design doc here.\nTo start using or developing the Machine Controller Manager See the documentation in the /docs repository, please find the index here.\nFAQ An FAQ is available here\nCluster-api Implementation  cluster-api branch of machine-controller-manager implements the machine-api aspect of the cluster-api project. Link: https://github.com/gardener/machine-controller-manager/tree/cluster-api Once cluster-api project gets stable, we may make master branch of MCM as well cluster-api compliant, with well-defined migration notes.  "},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/dns_providers/","title":"Manage DNS Providers","tags":[],"description":"Manage DNS providers for DNS records in your shoot cluster","content":"DNS Providers Introduction Gardener can manage DNS records on your behalf, so that you can request them via different resource types (see here) within the shoot cluster. The domains for which you are permitted to request records, are however restricted and depend on the DNS provider configuration.\nShoot provider By default, every shoot cluster is equipped with a default provider. It is the very same provider that manages the shoot cluster\u0026rsquo;s kube-apiserver public DNS record (DNS address in your Kubeconfig).\nkind: Shoot ... dns: domain: shoot.project.default-domain.gardener.cloud You are permitted to request any sub-domain of .dns.domain that is not already taken (e.g. api.shoot.project.default-domain.gardener.cloud, *.ingress.shoot.project.default-domain.gardener.cloud) with this provider.\nAdditional providers If you need to request DNS records for domains not managed by the default provider, additional providers must be configured in the shoot specification.\nFor example:\nkind: Shoot ... dns: domain: shoot.project.default-domain.gardener.cloud providers: - secretName: my-aws-account type: aws-route53 - secretName: my-gcp-account type: google-clouddns  Please consult the API-Reference to get a complete list of supported fields and configuration options.\n Referenced secrets should exist in the project namespace in the Garden cluster and must comply with the provider specific credentials format. The External-DNS-Management project provides corresponding examples (20-secret-\u0026lt;provider-name\u0026gt;-credentials.yaml) for known providers.\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/administer_shoots/dns_providers/","title":"Manage DNS Providers","tags":[],"description":"Manage DNS providers for DNS records in your shoot cluster","content":"DNS Providers Introduction Gardener can manage DNS records on your behalf, so that you can request them via different resource types (see here) within the shoot cluster. The domains for which you are permitted to request records, are however restricted and depend on the DNS provider configuration.\nShoot provider By default, every shoot cluster is equipped with a default provider. It is the very same provider that manages the shoot cluster\u0026rsquo;s kube-apiserver public DNS record (DNS address in your Kubeconfig).\nkind: Shoot ... dns: domain: shoot.project.default-domain.gardener.cloud You are permitted to request any sub-domain of .dns.domain that is not already taken (e.g. api.shoot.project.default-domain.gardener.cloud, *.ingress.shoot.project.default-domain.gardener.cloud) with this provider.\nAdditional providers If you need to request DNS records for domains not managed by the default provider, additional providers must be configured in the shoot specification.\nFor example:\nkind: Shoot ... dns: domain: shoot.project.default-domain.gardener.cloud providers: - secretName: my-aws-account type: aws-route53 - secretName: my-gcp-account type: google-clouddns  Please consult the API-Reference to get a complete list of supported fields and configuration options.\n Referenced secrets should exist in the project namespace in the Garden cluster and must comply with the provider specific credentials format. The External-DNS-Management project provides corresponding examples (20-secret-\u0026lt;provider-name\u0026gt;-credentials.yaml) for known providers.\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/administer_shoots/dns_providers/","title":"Manage DNS Providers","tags":[],"description":"Manage DNS providers for DNS records in your shoot cluster","content":"DNS Providers Introduction Gardener can manage DNS records on your behalf, so that you can request them via different resource types (see here) within the shoot cluster. The domains for which you are permitted to request records, are however restricted and depend on the DNS provider configuration.\nShoot provider By default, every shoot cluster is equipped with a default provider. It is the very same provider that manages the shoot cluster\u0026rsquo;s kube-apiserver public DNS record (DNS address in your Kubeconfig).\nkind: Shoot ... dns: domain: shoot.project.default-domain.gardener.cloud You are permitted to request any sub-domain of .dns.domain that is not already taken (e.g. api.shoot.project.default-domain.gardener.cloud, *.ingress.shoot.project.default-domain.gardener.cloud) with this provider.\nAdditional providers If you need to request DNS records for domains not managed by the default provider, additional providers must be configured in the shoot specification.\nFor example:\nkind: Shoot ... dns: domain: shoot.project.default-domain.gardener.cloud providers: - secretName: my-aws-account type: aws-route53 - secretName: my-gcp-account type: google-clouddns  Please consult the API-Reference to get a complete list of supported fields and configuration options.\n Referenced secrets should exist in the project namespace in the Garden cluster and must comply with the provider specific credentials format. The External-DNS-Management project provides corresponding examples (20-secret-\u0026lt;provider-name\u0026gt;-credentials.yaml) for known providers.\n"},{"uri":"https://gardener.cloud/documentation/guides/install_gardener/add-node-to-cluster/","title":"Manually adding a node to an existing cluster","tags":[],"description":"How to add a node to an existing cluster without the support of Gardener","content":"Manually adding a node to an existing cluster Gardener has an excellent ability to automatically scale machines for the cluster. From the point of view of scalability, there is no need for manual intervention.\nThis tutorial is useful for those end-users who need specifically configured nodes, which are not yet supported by Gardener. For example: an end-user who wants some workload that requires runnc instead of runc as container runtime.\nDisclaimer  Here we will look at the steps on how to add a node to an existing cluster without the support of Gardener. Such a node will not be managed by Gardener, and if it goes down for any reason, Gardener will not be responsible to replace it.\n How   Create a new instance in the same VPC/network as other machines in the cluster. You should be able to ssh into the machine. So save its private key, and assign a public IP to it. If adding a public IP is not preferred, then ssh into any other machine in the cluster, and then ssh from there into the new machine using its private key.\nTo ssh into a machine which is already in the cluster, use the steps defined here.\nAttach the same IAM role to the new machine which is attached to the existing machines in the cluster. This is required by kubelet in the new machine so that it can contact the cloud provider to query the node\u0026rsquo;s name.\n  On the new machine, create file /var/lib/kubelet/kubeconfig-bootstrap with the following content:\n  apiVersion:v1kind:Configcurrent-context:kubelet-bootstrap@defaultclusters:- cluster:certificate-authority-data:\u0026lt;CACertificate\u0026gt; server: \u0026lt;Server\u0026gt;name:defaultcontexts:- context:cluster:defaultuser:kubelet-bootstrapname:kubelet-bootstrap@defaultusers:- name:kubelet-bootstrapuser:as-user-extra:{}token:\u0026lt;Token\u0026gt;ssh into an existing node, and run these commands to get the values of and  to be replaced in above file:   \u0026lt;Servr\u0026gt;  /opt/bin/hyperkube kubectl \\  --kubeconfig /var/lib/kubelet/kubeconfig-real \\  config view \\  -o go-template=\u0026#39;{{index .clusters 0 \u0026#34;cluster\u0026#34; \u0026#34;server\u0026#34;}}\u0026#39; \\  --raw  \u0026lt;CA Certificate\u0026gt;  /opt/bin/hyperkube kubectl \\  --kubeconfig /var/lib/kubelet/kubeconfig-real \\  config view \\  -o go-template=\u0026#39;{{index .clusters 0 \u0026#34;cluster\u0026#34; \u0026#34;certificate-authority-data\u0026#34;}}\u0026#39; \\  --raw  \u0026lt;Token\u0026gt;\nThe kubelet on the new machine needs a bootstrap token to authenticate with the kube-apiserver when adding itself to the cluster. Kube-apiserver uses a secret in the kube-system namespace to authenticate this token. This token is valid for 90 minutes from the time of creation, and the corresponding secret captures this detail in its .data.expiration field. The name of this secret is of the format bootstrap-token-*. Gardener takes care of creating new bootstrap tokens, and the corresponding secrets. To get an unexpired token, find the secrets with the name format bootstrap-token-* in the kube-system namespace in the cluster, and pick the one with minimum age. Eg. bootstrap-token-abcdef.\nRun these commands to get the token:\ntokenid=$(kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template=\u0026#39;{{index .data \u0026#34;token-id\u0026#34;}}\u0026#39; | base64 --decode) tokensecret=$(kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template=\u0026#39;{{index .data \u0026#34;token-secret\u0026#34;}}\u0026#39; | base64 --decode) echo $tokenid.$tokensecret The value of $TOKEN will be tokenid.tokensecret. Replace $TOKEN in above file with this value\n  Copy contents of the files - /var/lib/kubelet/config/kubelet, /var/lib/kubelet/ca.crt and /etc/systemd/system/kubelet.service - from an existing node to the new node\n  Run the following command in the new node to start the kubelet:\n  systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet The new node should be added to the existing cluster within a couple of minutes.\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/install_gardener/add-node-to-cluster/","title":"Manually adding a node to an existing cluster","tags":[],"description":"How to add a node to an existing cluster without the support of Gardener","content":"Manually adding a node to an existing cluster Gardener has an excellent ability to automatically scale machines for the cluster. From the point of view of scalability, there is no need for manual intervention.\nThis tutorial is useful for those end-users who need specifically configured nodes, which are not yet supported by Gardener. For example: an end-user who wants some workload that requires runnc instead of runc as container runtime.\nDisclaimer  Here we will look at the steps on how to add a node to an existing cluster without the support of Gardener. Such a node will not be managed by Gardener, and if it goes down for any reason, Gardener will not be responsible to replace it.\n How   Create a new instance in the same VPC/network as other machines in the cluster. You should be able to ssh into the machine. So save its private key, and assign a public IP to it. If adding a public IP is not preferred, then ssh into any other machine in the cluster, and then ssh from there into the new machine using its private key.\nTo ssh into a machine which is already in the cluster, use the steps defined here.\nAttach the same IAM role to the new machine which is attached to the existing machines in the cluster. This is required by kubelet in the new machine so that it can contact the cloud provider to query the node\u0026rsquo;s name.\n  On the new machine, create file /var/lib/kubelet/kubeconfig-bootstrap with the following content:\n  apiVersion:v1kind:Configcurrent-context:kubelet-bootstrap@defaultclusters:- cluster:certificate-authority-data:\u0026lt;CACertificate\u0026gt; server: \u0026lt;Server\u0026gt;name:defaultcontexts:- context:cluster:defaultuser:kubelet-bootstrapname:kubelet-bootstrap@defaultusers:- name:kubelet-bootstrapuser:as-user-extra:{}token:\u0026lt;Token\u0026gt;ssh into an existing node, and run these commands to get the values of and  to be replaced in above file:   \u0026lt;Servr\u0026gt;  /opt/bin/hyperkube kubectl \\  --kubeconfig /var/lib/kubelet/kubeconfig-real \\  config view \\  -o go-template=\u0026#39;{{index .clusters 0 \u0026#34;cluster\u0026#34; \u0026#34;server\u0026#34;}}\u0026#39; \\  --raw  \u0026lt;CA Certificate\u0026gt;  /opt/bin/hyperkube kubectl \\  --kubeconfig /var/lib/kubelet/kubeconfig-real \\  config view \\  -o go-template=\u0026#39;{{index .clusters 0 \u0026#34;cluster\u0026#34; \u0026#34;certificate-authority-data\u0026#34;}}\u0026#39; \\  --raw  \u0026lt;Token\u0026gt;\nThe kubelet on the new machine needs a bootstrap token to authenticate with the kube-apiserver when adding itself to the cluster. Kube-apiserver uses a secret in the kube-system namespace to authenticate this token. This token is valid for 90 minutes from the time of creation, and the corresponding secret captures this detail in its .data.expiration field. The name of this secret is of the format bootstrap-token-*. Gardener takes care of creating new bootstrap tokens, and the corresponding secrets. To get an unexpired token, find the secrets with the name format bootstrap-token-* in the kube-system namespace in the cluster, and pick the one with minimum age. Eg. bootstrap-token-abcdef.\nRun these commands to get the token:\ntokenid=$(kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template=\u0026#39;{{index .data \u0026#34;token-id\u0026#34;}}\u0026#39; | base64 --decode) tokensecret=$(kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template=\u0026#39;{{index .data \u0026#34;token-secret\u0026#34;}}\u0026#39; | base64 --decode) echo $tokenid.$tokensecret The value of $TOKEN will be tokenid.tokensecret. Replace $TOKEN in above file with this value\n  Copy contents of the files - /var/lib/kubelet/config/kubelet, /var/lib/kubelet/ca.crt and /etc/systemd/system/kubelet.service - from an existing node to the new node\n  Run the following command in the new node to start the kubelet:\n  systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet The new node should be added to the existing cluster within a couple of minutes.\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/install_gardener/add-node-to-cluster/","title":"Manually adding a node to an existing cluster","tags":[],"description":"How to add a node to an existing cluster without the support of Gardener","content":"Manually adding a node to an existing cluster Gardener has an excellent ability to automatically scale machines for the cluster. From the point of view of scalability, there is no need for manual intervention.\nThis tutorial is useful for those end-users who need specifically configured nodes, which are not yet supported by Gardener. For example: an end-user who wants some workload that requires runnc instead of runc as container runtime.\nDisclaimer  Here we will look at the steps on how to add a node to an existing cluster without the support of Gardener. Such a node will not be managed by Gardener, and if it goes down for any reason, Gardener will not be responsible to replace it.\n How   Create a new instance in the same VPC/network as other machines in the cluster. You should be able to ssh into the machine. So save its private key, and assign a public IP to it. If adding a public IP is not preferred, then ssh into any other machine in the cluster, and then ssh from there into the new machine using its private key.\nTo ssh into a machine which is already in the cluster, use the steps defined here.\nAttach the same IAM role to the new machine which is attached to the existing machines in the cluster. This is required by kubelet in the new machine so that it can contact the cloud provider to query the node\u0026rsquo;s name.\n  On the new machine, create file /var/lib/kubelet/kubeconfig-bootstrap with the following content:\n  apiVersion:v1kind:Configcurrent-context:kubelet-bootstrap@defaultclusters:- cluster:certificate-authority-data:\u0026lt;CACertificate\u0026gt; server: \u0026lt;Server\u0026gt;name:defaultcontexts:- context:cluster:defaultuser:kubelet-bootstrapname:kubelet-bootstrap@defaultusers:- name:kubelet-bootstrapuser:as-user-extra:{}token:\u0026lt;Token\u0026gt;ssh into an existing node, and run these commands to get the values of and  to be replaced in above file:   \u0026lt;Servr\u0026gt;  /opt/bin/hyperkube kubectl \\  --kubeconfig /var/lib/kubelet/kubeconfig-real \\  config view \\  -o go-template=\u0026#39;{{index .clusters 0 \u0026#34;cluster\u0026#34; \u0026#34;server\u0026#34;}}\u0026#39; \\  --raw  \u0026lt;CA Certificate\u0026gt;  /opt/bin/hyperkube kubectl \\  --kubeconfig /var/lib/kubelet/kubeconfig-real \\  config view \\  -o go-template=\u0026#39;{{index .clusters 0 \u0026#34;cluster\u0026#34; \u0026#34;certificate-authority-data\u0026#34;}}\u0026#39; \\  --raw  \u0026lt;Token\u0026gt;\nThe kubelet on the new machine needs a bootstrap token to authenticate with the kube-apiserver when adding itself to the cluster. Kube-apiserver uses a secret in the kube-system namespace to authenticate this token. This token is valid for 90 minutes from the time of creation, and the corresponding secret captures this detail in its .data.expiration field. The name of this secret is of the format bootstrap-token-*. Gardener takes care of creating new bootstrap tokens, and the corresponding secrets. To get an unexpired token, find the secrets with the name format bootstrap-token-* in the kube-system namespace in the cluster, and pick the one with minimum age. Eg. bootstrap-token-abcdef.\nRun these commands to get the token:\ntokenid=$(kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template=\u0026#39;{{index .data \u0026#34;token-id\u0026#34;}}\u0026#39; | base64 --decode) tokensecret=$(kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template=\u0026#39;{{index .data \u0026#34;token-secret\u0026#34;}}\u0026#39; | base64 --decode) echo $tokenid.$tokensecret The value of $TOKEN will be tokenid.tokensecret. Replace $TOKEN in above file with this value\n  Copy contents of the files - /var/lib/kubelet/config/kubelet, /var/lib/kubelet/ca.crt and /etc/systemd/system/kubelet.service - from an existing node to the new node\n  Run the following command in the new node to start the kubelet:\n  systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet The new node should be added to the existing cluster within a couple of minutes.\n"},{"uri":"https://gardener.cloud/documentation/contribute/20_documentation/25_markup/","title":"Markdown","tags":[],"description":"","content":"Hugo uses Markdown for its simple content format. However, there are a lot of things that Markdown doesnt support well. You could use pure HTML to expand possibilities. A typical example is reducing the original dimensions of an image.\nHowever, use HTML judicially and to the minimum extent possible. Using HTML in markdowns makes it harder to maintain and publish coherent documentation bundles. This is a job typically performed by a publishing platform mechanisms, such as Hugo\u0026rsquo;s layouts. Considering that the source documentation might be published by multiple platforms you should be considerate in using markup that may bind it to a particular one.\nFor that reason we no longer support Hugo shortcodes. Instead we plan to gradually introduce mechanisms to compensate for Markdowns\u0026rsquo;s limitations with regard to creating documentation without departing from \u0026lsquo;normal\u0026rsquo; Markdown towards a publishing platform.\n"},{"uri":"https://gardener.cloud/v1.12.8/contribute/20_documentation/25_markup/","title":"Markdown","tags":[],"description":"","content":"Hugo uses Markdown for its simple content format. However, there are a lot of things that Markdown doesnt support well. You could use pure HTML to expand possibilities. A typical example is reducing the original dimensions of an image.\nHowever, use HTML judicially and to the minimum extent possible. Using HTML in markdowns makes it harder to maintain and publish coherent documentation bundles. This is a job typically performed by a publishing platform mechanisms, such as Hugo\u0026rsquo;s layouts. Considering that the source documentation might be published by multiple platforms you should be considerate in using markup that may bind it to a particular one.\nFor that reason we no longer support Hugo shortcodes. Instead we plan to gradually introduce mechanisms to compensate for Markdowns\u0026rsquo;s limitations with regard to creating documentation without departing from \u0026lsquo;normal\u0026rsquo; Markdown towards a publishing platform.\n"},{"uri":"https://gardener.cloud/v1.13.2/contribute/20_documentation/25_markup/","title":"Markdown","tags":[],"description":"","content":"Hugo uses Markdown for its simple content format. However, there are a lot of things that Markdown doesnt support well. You could use pure HTML to expand possibilities. A typical example is reducing the original dimensions of an image.\nHowever, use HTML judicially and to the minimum extent possible. Using HTML in markdowns makes it harder to maintain and publish coherent documentation bundles. This is a job typically performed by a publishing platform mechanisms, such as Hugo\u0026rsquo;s layouts. Considering that the source documentation might be published by multiple platforms you should be considerate in using markup that may bind it to a particular one.\nFor that reason we no longer support Hugo shortcodes. Instead we plan to gradually introduce mechanisms to compensate for Markdowns\u0026rsquo;s limitations with regard to creating documentation without departing from \u0026lsquo;normal\u0026rsquo; Markdown towards a publishing platform.\n"},{"uri":"https://gardener.cloud/documentation/guides/applications/network-isolation/","title":"Namespace Isolation","tags":[],"description":"","content":"\u0026hellip;or DENY all traffic from other namespaces\nYou can configure a NetworkPolicy to deny all the traffic from other namespaces while allowing all the traffic coming from the same namespace the pod was deployed into.\nThere are many reasons why you may chose to employ Kubernetes network policies:\n Isolate multi-tenant deployments Regulatory compliance Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each other  Kubernetes network policies are application centric compared to infrastructure/network centric standard firewalls. There are no explicit CIDRs or IP addresses used for matching source or destination IPs. Network policies build up on labels and selectors which are key concepts of Kubernetes that are used to organize (for e.g all DB tier pods of an app) and select subsets of objects.\nExample We create two nginx HTTP-Servers in two namespaces and block all traffic between the two namespaces. E.g. you are unable to get content from namespace1 if you are sitting in namespace2.\nSetup the namespaces # create two namespaces for test purpose kubectl create ns customer1 kubectl create ns customer2 # create a standard HTTP web server kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer1 kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer2 # expose the port 80 for external access kubectl expose deployment nginx --port=80 --type=NodePort -n=customer1 kubectl expose deployment nginx --port=80 --type=NodePort -n=customer2  Test without NP Create a pod with curl preinstalled inside the namespace customer1\n# create a \u0026#34;bash\u0026#34; pod in one namespace kubectl run -i --tty client --image=tutum/curl -n=customer1 try to curl the exposed nginx server to get the default index.html page. Execute this in the bash prompt of the pod created above.\n# get the index.html from the nginx of the namespace \u0026#34;customer1\u0026#34; =\u0026gt; success curl http://nginx.customer1 # get the index.html from the nginx of the namespace \u0026#34;customer2\u0026#34; =\u0026gt; success curl http://nginx.customer2 Both calls are done in a pod within namespace customer1 and both nginx servers are always reachable, no matter in what namespace.\n Test with NP Install the NetworkPolicy from your shell\napiVersion:networking.k8s.io/v1kind:NetworkPolicymetadata:name:deny-from-other-namespacesspec:podSelector:matchLabels:ingress:- from:- podSelector:{} it applies the policy to ALL pods in the named namespace as the spec.podSelector.matchLabels is empty and therefore selects all pods. it allows traffic from ALL pods in the named namespace, as spec.ingress.from.podSelector is empty and therefore selects all pods.  kubectl apply -f ./network-policy.yaml -n=customer1 kubectl apply -f ./network-policy.yaml -n=customer2 after this curl http://nginx.customer2 shouldn\u0026rsquo;t work anymore if you are a service inside the namespace customer1 and vice versa\nNote: This policy, once applied, will also disable all external traffic to these pods. For example you can create a service of type LoadBalancer in namespace customer1 that match the nginx pod. When you request the service by its \u0026lt;EXTERNAL_IP\u0026gt;:\u0026lt;PORT\u0026gt;, then the network policy will deny the ingress traffic from the service and the request will time out.\nMore You can get more information how to configure the NetworkPolicies on:\n Calico WebSite Kubernetes NP Recipes  "},{"uri":"https://gardener.cloud/v1.12.8/guides/applications/network-isolation/","title":"Namespace Isolation","tags":[],"description":"","content":"\u0026hellip;or DENY all traffic from other namespaces\nYou can configure a NetworkPolicy to deny all the traffic from other namespaces while allowing all the traffic coming from the same namespace the pod was deployed into.\nThere are many reasons why you may chose to employ Kubernetes network policies:\n Isolate multi-tenant deployments Regulatory compliance Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each other  Kubernetes network policies are application centric compared to infrastructure/network centric standard firewalls. There are no explicit CIDRs or IP addresses used for matching source or destination IPs. Network policies build up on labels and selectors which are key concepts of Kubernetes that are used to organize (for e.g all DB tier pods of an app) and select subsets of objects.\nExample We create two nginx HTTP-Servers in two namespaces and block all traffic between the two namespaces. E.g. you are unable to get content from namespace1 if you are sitting in namespace2.\nSetup the namespaces # create two namespaces for test purpose kubectl create ns customer1 kubectl create ns customer2 # create a standard HTTP web server kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer1 kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer2 # expose the port 80 for external access kubectl expose deployment nginx --port=80 --type=NodePort -n=customer1 kubectl expose deployment nginx --port=80 --type=NodePort -n=customer2  Test without NP Create a pod with curl preinstalled inside the namespace customer1\n# create a \u0026#34;bash\u0026#34; pod in one namespace kubectl run -i --tty client --image=tutum/curl -n=customer1 try to curl the exposed nginx server to get the default index.html page. Execute this in the bash prompt of the pod created above.\n# get the index.html from the nginx of the namespace \u0026#34;customer1\u0026#34; =\u0026gt; success curl http://nginx.customer1 # get the index.html from the nginx of the namespace \u0026#34;customer2\u0026#34; =\u0026gt; success curl http://nginx.customer2 Both calls are done in a pod within namespace customer1 and both nginx servers are always reachable, no matter in what namespace.\n Test with NP Install the NetworkPolicy from your shell\napiVersion:networking.k8s.io/v1kind:NetworkPolicymetadata:name:deny-from-other-namespacesspec:podSelector:matchLabels:ingress:- from:- podSelector:{} it applies the policy to ALL pods in the named namespace as the spec.podSelector.matchLabels is empty and therefore selects all pods. it allows traffic from ALL pods in the named namespace, as spec.ingress.from.podSelector is empty and therefore selects all pods.  kubectl apply -f ./network-policy.yaml -n=customer1 kubectl apply -f ./network-policy.yaml -n=customer2 after this curl http://nginx.customer2 shouldn\u0026rsquo;t work anymore if you are a service inside the namespace customer1 and vice versa\nNote: This policy, once applied, will also disable all external traffic to these pods. For example you can create a service of type LoadBalancer in namespace customer1 that match the nginx pod. When you request the service by its \u0026lt;EXTERNAL_IP\u0026gt;:\u0026lt;PORT\u0026gt;, then the network policy will deny the ingress traffic from the service and the request will time out.\nMore You can get more information how to configure the NetworkPolicies on:\n Calico WebSite Kubernetes NP Recipes  "},{"uri":"https://gardener.cloud/v1.13.2/guides/applications/network-isolation/","title":"Namespace Isolation","tags":[],"description":"","content":"\u0026hellip;or DENY all traffic from other namespaces\nYou can configure a NetworkPolicy to deny all the traffic from other namespaces while allowing all the traffic coming from the same namespace the pod was deployed into.\nThere are many reasons why you may chose to employ Kubernetes network policies:\n Isolate multi-tenant deployments Regulatory compliance Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each other  Kubernetes network policies are application centric compared to infrastructure/network centric standard firewalls. There are no explicit CIDRs or IP addresses used for matching source or destination IPs. Network policies build up on labels and selectors which are key concepts of Kubernetes that are used to organize (for e.g all DB tier pods of an app) and select subsets of objects.\nExample We create two nginx HTTP-Servers in two namespaces and block all traffic between the two namespaces. E.g. you are unable to get content from namespace1 if you are sitting in namespace2.\nSetup the namespaces # create two namespaces for test purpose kubectl create ns customer1 kubectl create ns customer2 # create a standard HTTP web server kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer1 kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer2 # expose the port 80 for external access kubectl expose deployment nginx --port=80 --type=NodePort -n=customer1 kubectl expose deployment nginx --port=80 --type=NodePort -n=customer2  Test without NP Create a pod with curl preinstalled inside the namespace customer1\n# create a \u0026#34;bash\u0026#34; pod in one namespace kubectl run -i --tty client --image=tutum/curl -n=customer1 try to curl the exposed nginx server to get the default index.html page. Execute this in the bash prompt of the pod created above.\n# get the index.html from the nginx of the namespace \u0026#34;customer1\u0026#34; =\u0026gt; success curl http://nginx.customer1 # get the index.html from the nginx of the namespace \u0026#34;customer2\u0026#34; =\u0026gt; success curl http://nginx.customer2 Both calls are done in a pod within namespace customer1 and both nginx servers are always reachable, no matter in what namespace.\n Test with NP Install the NetworkPolicy from your shell\napiVersion:networking.k8s.io/v1kind:NetworkPolicymetadata:name:deny-from-other-namespacesspec:podSelector:matchLabels:ingress:- from:- podSelector:{} it applies the policy to ALL pods in the named namespace as the spec.podSelector.matchLabels is empty and therefore selects all pods. it allows traffic from ALL pods in the named namespace, as spec.ingress.from.podSelector is empty and therefore selects all pods.  kubectl apply -f ./network-policy.yaml -n=customer1 kubectl apply -f ./network-policy.yaml -n=customer2 after this curl http://nginx.customer2 shouldn\u0026rsquo;t work anymore if you are a service inside the namespace customer1 and vice versa\nNote: This policy, once applied, will also disable all external traffic to these pods. For example you can create a service of type LoadBalancer in namespace customer1 that match the nginx pod. When you request the service by its \u0026lt;EXTERNAL_IP\u0026gt;:\u0026lt;PORT\u0026gt;, then the network policy will deny the ingress traffic from the service and the request will time out.\nMore You can get more information how to configure the NetworkPolicies on:\n Calico WebSite Kubernetes NP Recipes  "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/operatingsystemconfig/","title":"OperatingSystemConfig resource","tags":[],"description":"","content":"Contract: OperatingSystemConfig resource Gardener uses the machine API and leverages the functionalities of the machine-controller-manager (MCM) in order to manage the worker nodes of a shoot cluster. The machine-controller-manager itself simply takes a reference to an OS-image and (optionally) some user-data (a script or configuration that is executed when a VM is bootstrapped), and forwards both to the provider\u0026rsquo;s API when creating VMs. MCM does not have any restrictions regarding supported operating systems as it does not modify or influence the machine\u0026rsquo;s configuration in any way - it just creates/deletes machines with the provided metadata.\nConsequently, Gardener needs to provide this information when interacting with the machine-controller-manager. This means that basically every operating system is possible to be used as long as there is some implementation that generates the OS-specific configuration in order to provision/bootstrap the machines.\n:warning: Currently, there are a few requirements:\n The operating system must have built-in Docker support. The operating system must have systemd support. The operating system must have wget pre-installed. The operating system must have jq pre-installed.  The reasons for that will become evident later.\nWhat does the user-data bootstrapping the machines contain? Gardener installs a few components onto every worker machine in order to allow it to join the shoot cluster. There is the kubelet process, some scripts for continuously checking the health of kubelet and docker, but also configuration for log rotation, CA certificates, etc. The complete configuration you can find here. We are calling this the \u0026ldquo;original\u0026rdquo; user-data.\nHow does Gardener bootstrap the machines? Usually, you would submit all the components you want to install onto the machine as part of the user-data during creation time. However, some providers do have a size limitation (like ~16KB) for that user-data. That\u0026rsquo;s why we do not send the \u0026ldquo;original\u0026rdquo; user-data to the machine-controller-manager (who forwards it then to the provider\u0026rsquo;s API). Instead, we only send a small script that downloads the \u0026ldquo;original\u0026rdquo; data and applies it on the machine directly. This way we can extend the \u0026ldquo;original\u0026rdquo; user-data without any size restrictions - plus we can modify it without the necessity of re-creating the machine (because we run a script that downloads and updates it continuously).\nThe high-level flow is as follows:\n  For every worker pool X in the Shoot specification, Gardener creates a Secret named cloud-config-\u0026lt;X\u0026gt; in the kube-system namespace of the shoot cluster. The secret contains the \u0026ldquo;original\u0026rdquo; user-data.\n  Gardener generates a kubeconfig with minimal permissions just allowing reading these secrets. It is used by the downloader script later.\n  Gardener provides the downloader script, the kubeconfig, and the machine image stated in the Shoot specification to the machine-controller-manager.\n  Based on this information the machine-controller-manager creates the VM.\n  After the VM has been provisioned the downloader script starts and fetches the appropriate Secret for its worker pool (containing the \u0026ldquo;original\u0026rdquo; user-data) and applies it.\n  How does Gardener update the user-data on already existing machines? With ongoing development and new releases of Gardener some new components could be required to get installed onto every shoot worker VM, or existing components need to be changed. Gardener achieves that by simply updating the user-data inside the Secrets mentioned above (step 1). The downloader script is continuously (every 30s) reading the secret\u0026rsquo;s content (which might include an updated user-data) and storing it onto the disk. In order to re-apply the (new) downloaded data the secrets do not only contain the \u0026ldquo;original\u0026rdquo; user-data but also another short script (called \u0026ldquo;execution\u0026rdquo; script). This script checks whether the downloaded user-data differs from the one previously applied - and if required - re-applies it. After that it uses systemctl to restart the installed systemd units.\nWith the help of the execution script Gardener can centrally control how machines are updated without the need of OS providers to (re-)implement that logic. However, as stated in the mentioned requirements above, the execution script assumes existence of Docker and systemd.\nWhat needs to be implemented to support a new operating system? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:OperatingSystemConfigmetadata:name:pool-01-originalnamespace:defaultspec:type:\u0026lt;my-operating-system\u0026gt; purpose: reconcilereloadConfigFilePath:/var/lib/cloud-config-downloader/cloud-configunits:- name:docker.servicedropIns:- name:10-docker-opts.confcontent:| [Service]Environment=\u0026#34;DOCKER_OPTS=--log-opt max-size=60m --log-opt max-file=3\u0026#34;- name:docker-monitor.servicecommand:startenable:truecontent:| [Unit]Description=Docker-monitordaemonAfter=kubelet.service[Install]WantedBy=multi-user.target[Service]Restart=alwaysEnvironmentFile=/etc/environmentExecStart=/opt/bin/health-monitordockerfiles:- path:/var/lib/kubelet/ca.crtpermissions:0644encoding:b64content:secretRef:name:default-token-5dtjzdataKey:token- path:/etc/sysctl.d/99-k8s-general.confpermissions:0644content:inline:data:| # A higher vm.max_map_count is great for elasticsearch, mongo, or other mmap users# See https://github.com/kubernetes/kops/issues/1340vm.max_map_count=135217728In order to support a new operating system you need to write a controller that watches all OperatingSystemConfigs with .spec.type=\u0026lt;my-operating-system\u0026gt;. For those it shall generate a configuration blob that fits to your operating system. For example, a CoreOS controller might generate a CoreOS cloud-config or Ignition, SLES might generate cloud-init, and others might simply generate a bash script translating the .spec.units into systemd units, and .spec.files into real files on the disk.\nOperatingSystemConfigs can have two purposes which can be used (or ignored) by the extension controllers: either provision or reconcile.\n The provision purpose is used by Gardener for the user-data that it later passes to the machine-controller-manager (and then to the provider\u0026rsquo;s API) when creating new VMs. It contains the downloader unit. The reconcile purpose contains the \u0026ldquo;original\u0026rdquo; user-data (that is then stored in Secrets in the shoot\u0026rsquo;s kube-system namespace (see step 1). This is downloaded and applies late (see step 5).  As described above, the \u0026ldquo;original\u0026rdquo; user-data must be re-applicable to allow in-place updates. The way how this is done is specific to the generated operating system config (e.g., for CoreOS cloud-init the command is /usr/bin/coreos-cloudinit --from-file=\u0026lt;path\u0026gt;, whereas SLES would run cloud-init --file \u0026lt;path\u0026gt; single -n write_files --frequency=once). Consequently, besides the generated OS config, the extension controller must also provide a command for re-application an updated version of the user-data. As visible in the mentioned examples the command requires a path to the user-data file. Gardener will provide the path to the file in the OperatingSystemConfigs .spec.reloadConfigFilePath field (only if .spec.purpose=reconcile). As soon as Gardener detects that the user data has changed it will reload the systemd daemon and restart all the units provided in the .status.units[] list (see below example). The same logic applies during the very first application of the whole configuration.\nAfter generation extension controllers are asked to store their OS config inside a Secret (as it might contain confidential data) in the same namespace. The secret\u0026rsquo;s .data could look like this:\napiVersion:v1kind:Secretmetadata:name:osc-result-pool-01-originalnamespace:defaultownerReferences:- apiVersion:extensions.gardener.cloud/v1alpha1blockOwnerDeletion:truecontroller:truekind:OperatingSystemConfigname:pool-01-originaluid:99c0c5ca-19b9-11e9-9ebd-d67077b40f82data:cloud_config:base64(generated-user-data)Finally, the secret\u0026rsquo;s metadata, the OS-specific command to re-apply the configuration, and the list of systemd units that shall be considered to be restarted if an updated version of the user-data is re-applied must be provided in the OperatingSystemConfig's .status field:\n...status:cloudConfig:secretRef:name:osc-result-pool-01-originalnamespace:defaultcommand:/usr/bin/coreos-cloudinit--from-file=/var/lib/cloud-config-downloader/cloud-configlastOperation:description:SuccessfullygeneratedcloudconfiglastUpdateTime:\u0026#34;2019-01-23T07:45:23Z\u0026#34;progress:100state:Succeededtype:ReconcileobservedGeneration:5units:- docker-monitor.service(The .status.command field is optional and must only be provided if .spec.reloadConfigFilePath exists).\nOnce the .status indicates that the extension controller finished reconciling Gardener will continue with the next step of the shoot reconciliation flow.\nCRI Support Gardener supports specifying Container Runtime Interface (CRI) configuration in the OperatingSystemConfig resource. The only CRI supported at the moment is: \u0026ldquo;containerd\u0026rdquo;. For example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:OperatingSystemConfigmetadata:name:pool-01-originalnamespace:defaultspec:type:\u0026lt;my-operating-system\u0026gt; purpose: reconcilereloadConfigFilePath:/var/lib/cloud-config-downloader/cloud-configcri:name:containerd...If the .spec.cri section exists then the name property is mandatory. The only valid value at the moment is containerd. When the .spec.cri field is declared the kubelet will be configured by Gardener to work with ContainerD. Gardener expects that ContainerD service is running on the OS with the default socket path: unix:///run/containerd/containerd.sock.\nEach OS extension must support the CRI configurations by:\n The operating system must have built-in ContainerD and the Client CLI ContainerD service should be configure to work with the default configuration file in: /etc/containerd.config.toml (Created by Gardener).  If CRI configurations are not supported it is recommended create a validating webhook running in the garden cluster that prevents specifying the .spec.providers.workers[].cri section in the Shoot objects.\nReferences and additional resources  OperatingSystemConfig API (Golang specification) downloader script (fetching the \u0026ldquo;original\u0026rdquo; user-data and the execution script) Original user-data templates Execution script (applying the \u0026ldquo;original\u0026rdquo; user-data)  "},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/operatingsystemconfig/","title":"OperatingSystemConfig resource","tags":[],"description":"","content":"Contract: OperatingSystemConfig resource Gardener uses the machine API and leverages the functionalities of the machine-controller-manager (MCM) in order to manage the worker nodes of a shoot cluster. The machine-controller-manager itself simply takes a reference to an OS-image and (optionally) some user-data (a script or configuration that is executed when a VM is bootstrapped), and forwards both to the provider\u0026rsquo;s API when creating VMs. MCM does not have any restrictions regarding supported operating systems as it does not modify or influence the machine\u0026rsquo;s configuration in any way - it just creates/deletes machines with the provided metadata.\nConsequently, Gardener needs to provide this information when interacting with the machine-controller-manager. This means that basically every operating system is possible to be used as long as there is some implementation that generates the OS-specific configuration in order to provision/bootstrap the machines.\n:warning: Currently, there are a few requirements:\n The operating system must have built-in Docker support. The operating system must have systemd support. The operating system must have wget pre-installed. The operating system must have jq pre-installed.  The reasons for that will become evident later.\nWhat does the user-data bootstrapping the machines contain? Gardener installs a few components onto every worker machine in order to allow it to join the shoot cluster. There is the kubelet process, some scripts for continuously checking the health of kubelet and docker, but also configuration for log rotation, CA certificates, etc. The complete configuration you can find here. We are calling this the \u0026ldquo;original\u0026rdquo; user-data.\nHow does Gardener bootstrap the machines? Usually, you would submit all the components you want to install onto the machine as part of the user-data during creation time. However, some providers do have a size limitation (like ~16KB) for that user-data. That\u0026rsquo;s why we do not send the \u0026ldquo;original\u0026rdquo; user-data to the machine-controller-manager (who forwards it then to the provider\u0026rsquo;s API). Instead, we only send a small script that downloads the \u0026ldquo;original\u0026rdquo; data and applies it on the machine directly. This way we can extend the \u0026ldquo;original\u0026rdquo; user-data without any size restrictions - plus we can modify it without the necessity of re-creating the machine (because we run a script that downloads and updates it continuously).\nThe high-level flow is as follows:\n  For every worker pool X in the Shoot specification, Gardener creates a Secret named cloud-config-\u0026lt;X\u0026gt; in the kube-system namespace of the shoot cluster. The secret contains the \u0026ldquo;original\u0026rdquo; user-data.\n  Gardener generates a kubeconfig with minimal permissions just allowing reading these secrets. It is used by the downloader script later.\n  Gardener provides the downloader script, the kubeconfig, and the machine image stated in the Shoot specification to the machine-controller-manager.\n  Based on this information the machine-controller-manager creates the VM.\n  After the VM has been provisioned the downloader script starts and fetches the appropriate Secret for its worker pool (containing the \u0026ldquo;original\u0026rdquo; user-data) and applies it.\n  How does Gardener update the user-data on already existing machines? With ongoing development and new releases of Gardener some new components could be required to get installed onto every shoot worker VM, or existing components need to be changed. Gardener achieves that by simply updating the user-data inside the Secrets mentioned above (step 1). The downloader script is continuously (every 30s) reading the secret\u0026rsquo;s content (which might include an updated user-data) and storing it onto the disk. In order to re-apply the (new) downloaded data the secrets do not only contain the \u0026ldquo;original\u0026rdquo; user-data but also another short script (called \u0026ldquo;execution\u0026rdquo; script). This script checks whether the downloaded user-data differs from the one previously applied - and if required - re-applies it. After that it uses systemctl to restart the installed systemd units.\nWith the help of the execution script Gardener can centrally control how machines are updated without the need of OS providers to (re-)implement that logic. However, as stated in the mentioned requirements above, the execution script assumes existence of Docker and systemd.\nWhat needs to be implemented to support a new operating system? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:OperatingSystemConfigmetadata:name:pool-01-originalnamespace:defaultspec:type:\u0026lt;my-operating-system\u0026gt; purpose: reconcilereloadConfigFilePath:/var/lib/cloud-config-downloader/cloud-configunits:- name:docker.servicedropIns:- name:10-docker-opts.confcontent:| [Service]Environment=\u0026#34;DOCKER_OPTS=--log-opt max-size=60m --log-opt max-file=3\u0026#34;- name:docker-monitor.servicecommand:startenable:truecontent:| [Unit]Description=Docker-monitordaemonAfter=kubelet.service[Install]WantedBy=multi-user.target[Service]Restart=alwaysEnvironmentFile=/etc/environmentExecStart=/opt/bin/health-monitordockerfiles:- path:/var/lib/kubelet/ca.crtpermissions:0644encoding:b64content:secretRef:name:default-token-5dtjzdataKey:token- path:/etc/sysctl.d/99-k8s-general.confpermissions:0644content:inline:data:| # A higher vm.max_map_count is great for elasticsearch, mongo, or other mmap users# See https://github.com/kubernetes/kops/issues/1340vm.max_map_count=135217728In order to support a new operating system you need to write a controller that watches all OperatingSystemConfigs with .spec.type=\u0026lt;my-operating-system\u0026gt;. For those it shall generate a configuration blob that fits to your operating system. For example, a CoreOS controller might generate a CoreOS cloud-config or Ignition, SLES might generate cloud-init, and others might simply generate a bash script translating the .spec.units into systemd units, and .spec.files into real files on the disk.\nOperatingSystemConfigs can have two purposes which can be used (or ignored) by the extension controllers: either provision or reconcile.\n The provision purpose is used by Gardener for the user-data that it later passes to the machine-controller-manager (and then to the provider\u0026rsquo;s API) when creating new VMs. It contains the downloader unit. The reconcile purpose contains the \u0026ldquo;original\u0026rdquo; user-data (that is then stored in Secrets in the shoot\u0026rsquo;s kube-system namespace (see step 1). This is downloaded and applies late (see step 5).  As described above, the \u0026ldquo;original\u0026rdquo; user-data must be re-applicable to allow in-place updates. The way how this is done is specific to the generated operating system config (e.g., for CoreOS cloud-init the command is /usr/bin/coreos-cloudinit --from-file=\u0026lt;path\u0026gt;, whereas SLES would run cloud-init --file \u0026lt;path\u0026gt; single -n write_files --frequency=once). Consequently, besides the generated OS config, the extension controller must also provide a command for re-application an updated version of the user-data. As visible in the mentioned examples the command requires a path to the user-data file. Gardener will provide the path to the file in the OperatingSystemConfigs .spec.reloadConfigFilePath field (only if .spec.purpose=reconcile). As soon as Gardener detects that the user data has changed it will reload the systemd daemon and restart all the units provided in the .status.units[] list (see below example). The same logic applies during the very first application of the whole configuration.\nAfter generation extension controllers are asked to store their OS config inside a Secret (as it might contain confidential data) in the same namespace. The secret\u0026rsquo;s .data could look like this:\napiVersion:v1kind:Secretmetadata:name:osc-result-pool-01-originalnamespace:defaultownerReferences:- apiVersion:extensions.gardener.cloud/v1alpha1blockOwnerDeletion:truecontroller:truekind:OperatingSystemConfigname:pool-01-originaluid:99c0c5ca-19b9-11e9-9ebd-d67077b40f82data:cloud_config:base64(generated-user-data)Finally, the secret\u0026rsquo;s metadata, the OS-specific command to re-apply the configuration, and the list of systemd units that shall be considered to be restarted if an updated version of the user-data is re-applied must be provided in the OperatingSystemConfig's .status field:\n...status:cloudConfig:secretRef:name:osc-result-pool-01-originalnamespace:defaultcommand:/usr/bin/coreos-cloudinit--from-file=/var/lib/cloud-config-downloader/cloud-configlastOperation:description:SuccessfullygeneratedcloudconfiglastUpdateTime:\u0026#34;2019-01-23T07:45:23Z\u0026#34;progress:100state:Succeededtype:ReconcileobservedGeneration:5units:- docker-monitor.service(The .status.command field is optional and must only be provided if .spec.reloadConfigFilePath exists).\nOnce the .status indicates that the extension controller finished reconciling Gardener will continue with the next step of the shoot reconciliation flow.\nCRI Support Gardener supports specifying Container Runtime Interface (CRI) configuration in the OperatingSystemConfig resource. The only CRI supported at the moment is: \u0026ldquo;containerd\u0026rdquo;. For example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:OperatingSystemConfigmetadata:name:pool-01-originalnamespace:defaultspec:type:\u0026lt;my-operating-system\u0026gt; purpose: reconcilereloadConfigFilePath:/var/lib/cloud-config-downloader/cloud-configcri:name:containerd...If the .spec.cri section exists then the name property is mandatory. The only valid value at the moment is containerd. When the .spec.cri field is declared the kubelet will be configured by Gardener to work with ContainerD. Gardener expects that ContainerD service is running on the OS with the default socket path: unix:///run/containerd/containerd.sock.\nEach OS extension must support the CRI configurations by:\n The operating system must have built-in ContainerD and the Client CLI ContainerD service should be configure to work with the default configuration file in: /etc/containerd.config.toml (Created by Gardener).  If CRI configurations are not supported it is recommended create a validating webhook running in the garden cluster that prevents specifying the .spec.providers.workers[].cri section in the Shoot objects.\nReferences and additional resources  OperatingSystemConfig API (Golang specification) downloader script (fetching the \u0026ldquo;original\u0026rdquo; user-data and the execution script) Original user-data templates Execution script (applying the \u0026ldquo;original\u0026rdquo; user-data)  "},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/operatingsystemconfig/","title":"OperatingSystemConfig resource","tags":[],"description":"","content":"Contract: OperatingSystemConfig resource Gardener uses the machine API and leverages the functionalities of the machine-controller-manager (MCM) in order to manage the worker nodes of a shoot cluster. The machine-controller-manager itself simply takes a reference to an OS-image and (optionally) some user-data (a script or configuration that is executed when a VM is bootstrapped), and forwards both to the provider\u0026rsquo;s API when creating VMs. MCM does not have any restrictions regarding supported operating systems as it does not modify or influence the machine\u0026rsquo;s configuration in any way - it just creates/deletes machines with the provided metadata.\nConsequently, Gardener needs to provide this information when interacting with the machine-controller-manager. This means that basically every operating system is possible to be used as long as there is some implementation that generates the OS-specific configuration in order to provision/bootstrap the machines.\n:warning: Currently, there are a few requirements:\n The operating system must have built-in Docker support. The operating system must have systemd support. The operating system must have wget pre-installed. The operating system must have jq pre-installed.  The reasons for that will become evident later.\nWhat does the user-data bootstrapping the machines contain? Gardener installs a few components onto every worker machine in order to allow it to join the shoot cluster. There is the kubelet process, some scripts for continuously checking the health of kubelet and docker, but also configuration for log rotation, CA certificates, etc. The complete configuration you can find here. We are calling this the \u0026ldquo;original\u0026rdquo; user-data.\nHow does Gardener bootstrap the machines? Usually, you would submit all the components you want to install onto the machine as part of the user-data during creation time. However, some providers do have a size limitation (like ~16KB) for that user-data. That\u0026rsquo;s why we do not send the \u0026ldquo;original\u0026rdquo; user-data to the machine-controller-manager (who forwards it then to the provider\u0026rsquo;s API). Instead, we only send a small script that downloads the \u0026ldquo;original\u0026rdquo; data and applies it on the machine directly. This way we can extend the \u0026ldquo;original\u0026rdquo; user-data without any size restrictions - plus we can modify it without the necessity of re-creating the machine (because we run a script that downloads and updates it continuously).\nThe high-level flow is as follows:\n  For every worker pool X in the Shoot specification, Gardener creates a Secret named cloud-config-\u0026lt;X\u0026gt; in the kube-system namespace of the shoot cluster. The secret contains the \u0026ldquo;original\u0026rdquo; user-data.\n  Gardener generates a kubeconfig with minimal permissions just allowing reading these secrets. It is used by the downloader script later.\n  Gardener provides the downloader script, the kubeconfig, and the machine image stated in the Shoot specification to the machine-controller-manager.\n  Based on this information the machine-controller-manager creates the VM.\n  After the VM has been provisioned the downloader script starts and fetches the appropriate Secret for its worker pool (containing the \u0026ldquo;original\u0026rdquo; user-data) and applies it.\n  How does Gardener update the user-data on already existing machines? With ongoing development and new releases of Gardener some new components could be required to get installed onto every shoot worker VM, or existing components need to be changed. Gardener achieves that by simply updating the user-data inside the Secrets mentioned above (step 1). The downloader script is continuously (every 30s) reading the secret\u0026rsquo;s content (which might include an updated user-data) and storing it onto the disk. In order to re-apply the (new) downloaded data the secrets do not only contain the \u0026ldquo;original\u0026rdquo; user-data but also another short script (called \u0026ldquo;execution\u0026rdquo; script). This script checks whether the downloaded user-data differs from the one previously applied - and if required - re-applies it. After that it uses systemctl to restart the installed systemd units.\nWith the help of the execution script Gardener can centrally control how machines are updated without the need of OS providers to (re-)implement that logic. However, as stated in the mentioned requirements above, the execution script assumes existence of Docker and systemd.\nWhat needs to be implemented to support a new operating system? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:OperatingSystemConfigmetadata:name:pool-01-originalnamespace:defaultspec:type:\u0026lt;my-operating-system\u0026gt; purpose: reconcilereloadConfigFilePath:/var/lib/cloud-config-downloader/cloud-configunits:- name:docker.servicedropIns:- name:10-docker-opts.confcontent:| [Service]Environment=\u0026#34;DOCKER_OPTS=--log-opt max-size=60m --log-opt max-file=3\u0026#34;- name:docker-monitor.servicecommand:startenable:truecontent:| [Unit]Description=Docker-monitordaemonAfter=kubelet.service[Install]WantedBy=multi-user.target[Service]Restart=alwaysEnvironmentFile=/etc/environmentExecStart=/opt/bin/health-monitordockerfiles:- path:/var/lib/kubelet/ca.crtpermissions:0644encoding:b64content:secretRef:name:default-token-5dtjzdataKey:token- path:/etc/sysctl.d/99-k8s-general.confpermissions:0644content:inline:data:| # A higher vm.max_map_count is great for elasticsearch, mongo, or other mmap users# See https://github.com/kubernetes/kops/issues/1340vm.max_map_count=135217728In order to support a new operating system you need to write a controller that watches all OperatingSystemConfigs with .spec.type=\u0026lt;my-operating-system\u0026gt;. For those it shall generate a configuration blob that fits to your operating system. For example, a CoreOS controller might generate a CoreOS cloud-config or Ignition, SLES might generate cloud-init, and others might simply generate a bash script translating the .spec.units into systemd units, and .spec.files into real files on the disk.\nOperatingSystemConfigs can have two purposes which can be used (or ignored) by the extension controllers: either provision or reconcile.\n The provision purpose is used by Gardener for the user-data that it later passes to the machine-controller-manager (and then to the provider\u0026rsquo;s API) when creating new VMs. It contains the downloader unit. The reconcile purpose contains the \u0026ldquo;original\u0026rdquo; user-data (that is then stored in Secrets in the shoot\u0026rsquo;s kube-system namespace (see step 1). This is downloaded and applies late (see step 5).  As described above, the \u0026ldquo;original\u0026rdquo; user-data must be re-applicable to allow in-place updates. The way how this is done is specific to the generated operating system config (e.g., for CoreOS cloud-init the command is /usr/bin/coreos-cloudinit --from-file=\u0026lt;path\u0026gt;, whereas SLES would run cloud-init --file \u0026lt;path\u0026gt; single -n write_files --frequency=once). Consequently, besides the generated OS config, the extension controller must also provide a command for re-application an updated version of the user-data. As visible in the mentioned examples the command requires a path to the user-data file. Gardener will provide the path to the file in the OperatingSystemConfigs .spec.reloadConfigFilePath field (only if .spec.purpose=reconcile). As soon as Gardener detects that the user data has changed it will reload the systemd daemon and restart all the units provided in the .status.units[] list (see below example). The same logic applies during the very first application of the whole configuration.\nAfter generation extension controllers are asked to store their OS config inside a Secret (as it might contain confidential data) in the same namespace. The secret\u0026rsquo;s .data could look like this:\napiVersion:v1kind:Secretmetadata:name:osc-result-pool-01-originalnamespace:defaultownerReferences:- apiVersion:extensions.gardener.cloud/v1alpha1blockOwnerDeletion:truecontroller:truekind:OperatingSystemConfigname:pool-01-originaluid:99c0c5ca-19b9-11e9-9ebd-d67077b40f82data:cloud_config:base64(generated-user-data)Finally, the secret\u0026rsquo;s metadata, the OS-specific command to re-apply the configuration, and the list of systemd units that shall be considered to be restarted if an updated version of the user-data is re-applied must be provided in the OperatingSystemConfig's .status field:\n...status:cloudConfig:secretRef:name:osc-result-pool-01-originalnamespace:defaultcommand:/usr/bin/coreos-cloudinit--from-file=/var/lib/cloud-config-downloader/cloud-configlastOperation:description:SuccessfullygeneratedcloudconfiglastUpdateTime:\u0026#34;2019-01-23T07:45:23Z\u0026#34;progress:100state:Succeededtype:ReconcileobservedGeneration:5units:- docker-monitor.service(The .status.command field is optional and must only be provided if .spec.reloadConfigFilePath exists).\nOnce the .status indicates that the extension controller finished reconciling Gardener will continue with the next step of the shoot reconciliation flow.\nCRI Support Gardener supports specifying Container Runtime Interface (CRI) configuration in the OperatingSystemConfig resource. The only CRI supported at the moment is: \u0026ldquo;containerd\u0026rdquo;. For example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:OperatingSystemConfigmetadata:name:pool-01-originalnamespace:defaultspec:type:\u0026lt;my-operating-system\u0026gt; purpose: reconcilereloadConfigFilePath:/var/lib/cloud-config-downloader/cloud-configcri:name:containerd...If the .spec.cri section exists then the name property is mandatory. The only valid value at the moment is containerd. When the .spec.cri field is declared the kubelet will be configured by Gardener to work with ContainerD. Gardener expects that ContainerD service is running on the OS with the default socket path: unix:///run/containerd/containerd.sock.\nEach OS extension must support the CRI configurations by:\n The operating system must have built-in ContainerD and the Client CLI ContainerD service should be configure to work with the default configuration file in: /etc/containerd.config.toml (Created by Gardener).  If CRI configurations are not supported it is recommended create a validating webhook running in the garden cluster that prevents specifying the .spec.providers.workers[].cri section in the Shoot objects.\nReferences and additional resources  OperatingSystemConfig API (Golang specification) downloader script (fetching the \u0026ldquo;original\u0026rdquo; user-data and the execution script) Original user-data templates Execution script (applying the \u0026ldquo;original\u0026rdquo; user-data)  "},{"uri":"https://gardener.cloud/documentation/concepts/monitoring/operator_alerts/","title":"Operator Alerts","tags":[],"description":"","content":"Operator Alerts    Alertname Severity Type Description     ApiServerUnreachableViaKubernetesService critical shoot The Api server has been unreachable for 3 minutes via the kubernetes service in the shoot.   CoreDNSDown critical shoot CoreDNS could not be found. Cluster DNS resolution will not work.   ApiServerNotReachable blocker seed API server not reachable via external endpoint: {{ $labels.instance }}.   KubeApiserverDown blocker seed All API server replicas are down/unreachable, or all API server could not be found.   KubeApiServerTooManyAuditlogFailures warning seed The API servers cumulative failure rate in logging audit events is greater than 2%.   KubeletTooManyOpenFileDescriptorsSeed critical seed Seed-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubePersistentVolumeUsageCritical critical seed The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} is only {{ printf \u0026quot;%0.2f\u0026quot; $value }}% free.   KubePersistentVolumeFullInFourDays warning seed Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ printf \u0026quot;%0.2f\u0026quot; $value }}% is available.   KubePodPendingControlPlane warning seed Pod {{ $labels.pod }} is stuck in \u0026quot;Pending\u0026quot; state for more than 30 minutes.   KubePodNotReadyControlPlane warning  Pod {{ $labels.pod }} is not ready for more than 30 minutes.   KubeStateMetricsShootDown info seed There are no running kube-state-metric pods for the shoot cluster. No kubernetes resource metrics can be scraped.   KubeStateMetricsSeedDown critical seed There are no running kube-state-metric pods for the seed cluster. No kubernetes resource metrics can be scraped.   NoWorkerNodes blocker  There are no worker nodes in the cluster or all of the worker nodes in the cluster are not schedulable.   PrometheusCantScrape warning seed Prometheus failed to scrape metrics. Instance {{ $labels.instance }}, job {{ $labels.job }}.   PrometheusConfigurationFailure warning seed Latest Prometheus configuration is broken and Prometheus is using the previous one.   VPNShootNoPods critical shoot vpn-shoot deployment in Shoot cluster has 0 available pods. VPN won't work.   VPNProbeAPIServerProxyFailed critical shoot The API Server proxy functionality is not working. Probably the vpn connection from an API Server pod to the vpn-shoot endpoint on the Shoot workers does not work.    "},{"uri":"https://gardener.cloud/v1.12.8/concepts/monitoring/operator_alerts/","title":"Operator Alerts","tags":[],"description":"","content":"Operator Alerts    Alertname Severity Type Description     ApiServerUnreachableViaKubernetesService critical shoot The Api server has been unreachable for 3 minutes via the kubernetes service in the shoot.   CoreDNSDown critical shoot CoreDNS could not be found. Cluster DNS resolution will not work.   ApiServerNotReachable blocker seed API server not reachable via external endpoint: {{ $labels.instance }}.   KubeApiserverDown blocker seed All API server replicas are down/unreachable, or all API server could not be found.   KubeApiServerTooManyAuditlogFailures warning seed The API servers cumulative failure rate in logging audit events is greater than 2%.   KubeletTooManyOpenFileDescriptorsSeed critical seed Seed-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubePersistentVolumeUsageCritical critical seed The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} is only {{ printf \u0026quot;%0.2f\u0026quot; $value }}% free.   KubePersistentVolumeFullInFourDays warning seed Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ printf \u0026quot;%0.2f\u0026quot; $value }}% is available.   KubePodPendingControlPlane warning seed Pod {{ $labels.pod }} is stuck in \u0026quot;Pending\u0026quot; state for more than 30 minutes.   KubePodNotReadyControlPlane warning  Pod {{ $labels.pod }} is not ready for more than 30 minutes.   KubeStateMetricsShootDown info seed There are no running kube-state-metric pods for the shoot cluster. No kubernetes resource metrics can be scraped.   KubeStateMetricsSeedDown critical seed There are no running kube-state-metric pods for the seed cluster. No kubernetes resource metrics can be scraped.   NoWorkerNodes blocker  There are no worker nodes in the cluster or all of the worker nodes in the cluster are not schedulable.   PrometheusCantScrape warning seed Prometheus failed to scrape metrics. Instance {{ $labels.instance }}, job {{ $labels.job }}.   PrometheusConfigurationFailure warning seed Latest Prometheus configuration is broken and Prometheus is using the previous one.   VPNShootNoPods critical shoot vpn-shoot deployment in Shoot cluster has 0 available pods. VPN won't work.   VPNProbeAPIServerProxyFailed critical shoot The API Server proxy functionality is not working. Probably the vpn connection from an API Server pod to the vpn-shoot endpoint on the Shoot workers does not work.    "},{"uri":"https://gardener.cloud/v1.13.2/concepts/monitoring/operator_alerts/","title":"Operator Alerts","tags":[],"description":"","content":"Operator Alerts    Alertname Severity Type Description     ApiServerUnreachableViaKubernetesService critical shoot The Api server has been unreachable for 3 minutes via the kubernetes service in the shoot.   CoreDNSDown critical shoot CoreDNS could not be found. Cluster DNS resolution will not work.   ApiServerNotReachable blocker seed API server not reachable via external endpoint: {{ $labels.instance }}.   KubeApiserverDown blocker seed All API server replicas are down/unreachable, or all API server could not be found.   KubeApiServerTooManyAuditlogFailures warning seed The API servers cumulative failure rate in logging audit events is greater than 2%.   KubeletTooManyOpenFileDescriptorsSeed critical seed Seed-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubePersistentVolumeUsageCritical critical seed The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} is only {{ printf \u0026quot;%0.2f\u0026quot; $value }}% free.   KubePersistentVolumeFullInFourDays warning seed Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ printf \u0026quot;%0.2f\u0026quot; $value }}% is available.   KubePodPendingControlPlane warning seed Pod {{ $labels.pod }} is stuck in \u0026quot;Pending\u0026quot; state for more than 30 minutes.   KubePodNotReadyControlPlane warning  Pod {{ $labels.pod }} is not ready for more than 30 minutes.   KubeStateMetricsShootDown info seed There are no running kube-state-metric pods for the shoot cluster. No kubernetes resource metrics can be scraped.   KubeStateMetricsSeedDown critical seed There are no running kube-state-metric pods for the seed cluster. No kubernetes resource metrics can be scraped.   NoWorkerNodes blocker  There are no worker nodes in the cluster or all of the worker nodes in the cluster are not schedulable.   PrometheusCantScrape warning seed Prometheus failed to scrape metrics. Instance {{ $labels.instance }}, job {{ $labels.job }}.   PrometheusConfigurationFailure warning seed Latest Prometheus configuration is broken and Prometheus is using the previous one.   VPNShootNoPods critical shoot vpn-shoot deployment in Shoot cluster has 0 available pods. VPN won't work.   VPNProbeAPIServerProxyFailed critical shoot The API Server proxy functionality is not working. Probably the vpn connection from an API Server pod to the vpn-shoot endpoint on the Shoot workers does not work.    "},{"uri":"https://gardener.cloud/documentation/guides/applications/container-startup/","title":"Orchestration of container startup","tags":[],"description":"How to orchestrate startup sequence of multiple containers","content":"Disclaimer If an application depends on other services deployed separately do not rely on a certain start sequence of containers but ensure that the application can cope with unavailability of the services it depends on.\nIntroduction Kubernetes offers a feature called InitContainers to perform some tasks during a pod\u0026rsquo;s initialization. In this tutorial we demonstrate how to use it orchestrate starting sequence of multiple containers. The tutorial uses the example app url-shortener which consists of two components:\n postgresql database webapp which depends on postgresql database and provides two endpoints: create a short url from a given location, and redirect from a given short URL to the corresponding target location.  This app represents the minimal example where an application relies on another service or database. In this example, if the application starts before database is ready, the application will fail as shown below:\n$ kubectl logs webapp-958cf5567-h247n time=\u0026#34;2018-06-12T11:02:42Z\u0026#34; level=info msg=\u0026#34;Connecting to Postgres database using: host=`postgres:5432` dbname=`url_shortener_db` username=`user`\\n\u0026#34; time=\u0026#34;2018-06-12T11:02:42Z\u0026#34; level=fatal msg=\u0026#34;failed to start: failed to open connection to database: dial tcp: lookup postgres on 100.64.0.10:53: no such host\\n\u0026#34; $ kubectl get po -w NAME READY STATUS RESTARTS AGE webapp-958cf5567-h247n 0/1 Pending 0 0s webapp-958cf5567-h247n 0/1 Pending 0 0s webapp-958cf5567-h247n 0/1 ContainerCreating 0 0s webapp-958cf5567-h247n 0/1 ContainerCreating 0 1s webapp-958cf5567-h247n 0/1 Error 0 2s webapp-958cf5567-h247n 0/1 Error 1 3s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 1 4s webapp-958cf5567-h247n 0/1 Error 2 18s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 2 29s webapp-958cf5567-h247n 0/1 Error 3 43s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 3 56s If the restartPolicy is set to Always (default) in yaml, the application will continue to restart the pod with an exponential back-off delay in case of failure.\nUsing InitContaniner To avoid such situation, InitContainers  can be defined which are executed prior to the application container. If one InitContainers fails, the application container won\u0026rsquo;t be triggered.\napiVersion:apps/v1kind:Deploymentmetadata:name:webappspec:selector:matchLabels:app:webapptemplate:metadata:labels:app:webappspec:initContainers:# check if DB is ready, and only continue when true- name:check-db-readyimage:postgres:9.6.5command:[\u0026#39;sh\u0026#39;,\u0026#39;-c\u0026#39;,\u0026#39;until pg_isready -h postgres -p 5432; do echo waiting for database; sleep 2; done;\u0026#39;]containers:- image:xcoulon/go-url-shortener:0.1.0name:go-url-shortenerenv:- name:POSTGRES_HOSTvalue:postgres- name:POSTGRES_PORTvalue:\u0026#34;5432\u0026#34;- name:POSTGRES_DATABASEvalue:url_shortener_db- name:POSTGRES_USERvalue:user- name:POSTGRES_PASSWORDvalue:mysecretpasswordports:- containerPort:8080In above example, the InitContainers uses docker image postgres:9.6.5 which is different from the application container. This also brings the advantage of not having to include unnecessary tools (e.g. pg_isready) in the application container.\nWith introduction of InitContainers, the pod startup will look like following in case database is not available yet:\n$ kubectl get po -w NAME READY STATUS RESTARTS AGE nginx-deployment-5cc79d6bfd-t9n8h 1/1 Running 0 5d privileged-pod 1/1 Running 0 4d webapp-fdcb49cbc-4gs4n 0/1 Pending 0 0s webapp-fdcb49cbc-4gs4n 0/1 Pending 0 0s webapp-fdcb49cbc-4gs4n 0/1 Init:0/1 0 0s webapp-fdcb49cbc-4gs4n 0/1 Init:0/1 0 1s $ kubectl logs webapp-fdcb49cbc-4gs4n Error from server (BadRequest): container \u0026#34;go-url-shortener\u0026#34; in pod \u0026#34;webapp-fdcb49cbc-4gs4n\u0026#34; is waiting to start: PodInitializing "},{"uri":"https://gardener.cloud/v1.12.8/guides/applications/container-startup/","title":"Orchestration of container startup","tags":[],"description":"How to orchestrate startup sequence of multiple containers","content":"Disclaimer If an application depends on other services deployed separately do not rely on a certain start sequence of containers but ensure that the application can cope with unavailability of the services it depends on.\nIntroduction Kubernetes offers a feature called InitContainers to perform some tasks during a pod\u0026rsquo;s initialization. In this tutorial we demonstrate how to use it orchestrate starting sequence of multiple containers. The tutorial uses the example app url-shortener which consists of two components:\n postgresql database webapp which depends on postgresql database and provides two endpoints: create a short url from a given location, and redirect from a given short URL to the corresponding target location.  This app represents the minimal example where an application relies on another service or database. In this example, if the application starts before database is ready, the application will fail as shown below:\n$ kubectl logs webapp-958cf5567-h247n time=\u0026#34;2018-06-12T11:02:42Z\u0026#34; level=info msg=\u0026#34;Connecting to Postgres database using: host=`postgres:5432` dbname=`url_shortener_db` username=`user`\\n\u0026#34; time=\u0026#34;2018-06-12T11:02:42Z\u0026#34; level=fatal msg=\u0026#34;failed to start: failed to open connection to database: dial tcp: lookup postgres on 100.64.0.10:53: no such host\\n\u0026#34; $ kubectl get po -w NAME READY STATUS RESTARTS AGE webapp-958cf5567-h247n 0/1 Pending 0 0s webapp-958cf5567-h247n 0/1 Pending 0 0s webapp-958cf5567-h247n 0/1 ContainerCreating 0 0s webapp-958cf5567-h247n 0/1 ContainerCreating 0 1s webapp-958cf5567-h247n 0/1 Error 0 2s webapp-958cf5567-h247n 0/1 Error 1 3s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 1 4s webapp-958cf5567-h247n 0/1 Error 2 18s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 2 29s webapp-958cf5567-h247n 0/1 Error 3 43s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 3 56s If the restartPolicy is set to Always (default) in yaml, the application will continue to restart the pod with an exponential back-off delay in case of failure.\nUsing InitContaniner To avoid such situation, InitContainers  can be defined which are executed prior to the application container. If one InitContainers fails, the application container won\u0026rsquo;t be triggered.\napiVersion:apps/v1kind:Deploymentmetadata:name:webappspec:selector:matchLabels:app:webapptemplate:metadata:labels:app:webappspec:initContainers:# check if DB is ready, and only continue when true- name:check-db-readyimage:postgres:9.6.5command:[\u0026#39;sh\u0026#39;,\u0026#39;-c\u0026#39;,\u0026#39;until pg_isready -h postgres -p 5432; do echo waiting for database; sleep 2; done;\u0026#39;]containers:- image:xcoulon/go-url-shortener:0.1.0name:go-url-shortenerenv:- name:POSTGRES_HOSTvalue:postgres- name:POSTGRES_PORTvalue:\u0026#34;5432\u0026#34;- name:POSTGRES_DATABASEvalue:url_shortener_db- name:POSTGRES_USERvalue:user- name:POSTGRES_PASSWORDvalue:mysecretpasswordports:- containerPort:8080In above example, the InitContainers uses docker image postgres:9.6.5 which is different from the application container. This also brings the advantage of not having to include unnecessary tools (e.g. pg_isready) in the application container.\nWith introduction of InitContainers, the pod startup will look like following in case database is not available yet:\n$ kubectl get po -w NAME READY STATUS RESTARTS AGE nginx-deployment-5cc79d6bfd-t9n8h 1/1 Running 0 5d privileged-pod 1/1 Running 0 4d webapp-fdcb49cbc-4gs4n 0/1 Pending 0 0s webapp-fdcb49cbc-4gs4n 0/1 Pending 0 0s webapp-fdcb49cbc-4gs4n 0/1 Init:0/1 0 0s webapp-fdcb49cbc-4gs4n 0/1 Init:0/1 0 1s $ kubectl logs webapp-fdcb49cbc-4gs4n Error from server (BadRequest): container \u0026#34;go-url-shortener\u0026#34; in pod \u0026#34;webapp-fdcb49cbc-4gs4n\u0026#34; is waiting to start: PodInitializing "},{"uri":"https://gardener.cloud/v1.13.2/guides/applications/container-startup/","title":"Orchestration of container startup","tags":[],"description":"How to orchestrate startup sequence of multiple containers","content":"Disclaimer If an application depends on other services deployed separately do not rely on a certain start sequence of containers but ensure that the application can cope with unavailability of the services it depends on.\nIntroduction Kubernetes offers a feature called InitContainers to perform some tasks during a pod\u0026rsquo;s initialization. In this tutorial we demonstrate how to use it orchestrate starting sequence of multiple containers. The tutorial uses the example app url-shortener which consists of two components:\n postgresql database webapp which depends on postgresql database and provides two endpoints: create a short url from a given location, and redirect from a given short URL to the corresponding target location.  This app represents the minimal example where an application relies on another service or database. In this example, if the application starts before database is ready, the application will fail as shown below:\n$ kubectl logs webapp-958cf5567-h247n time=\u0026#34;2018-06-12T11:02:42Z\u0026#34; level=info msg=\u0026#34;Connecting to Postgres database using: host=`postgres:5432` dbname=`url_shortener_db` username=`user`\\n\u0026#34; time=\u0026#34;2018-06-12T11:02:42Z\u0026#34; level=fatal msg=\u0026#34;failed to start: failed to open connection to database: dial tcp: lookup postgres on 100.64.0.10:53: no such host\\n\u0026#34; $ kubectl get po -w NAME READY STATUS RESTARTS AGE webapp-958cf5567-h247n 0/1 Pending 0 0s webapp-958cf5567-h247n 0/1 Pending 0 0s webapp-958cf5567-h247n 0/1 ContainerCreating 0 0s webapp-958cf5567-h247n 0/1 ContainerCreating 0 1s webapp-958cf5567-h247n 0/1 Error 0 2s webapp-958cf5567-h247n 0/1 Error 1 3s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 1 4s webapp-958cf5567-h247n 0/1 Error 2 18s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 2 29s webapp-958cf5567-h247n 0/1 Error 3 43s webapp-958cf5567-h247n 0/1 CrashLoopBackOff 3 56s If the restartPolicy is set to Always (default) in yaml, the application will continue to restart the pod with an exponential back-off delay in case of failure.\nUsing InitContaniner To avoid such situation, InitContainers  can be defined which are executed prior to the application container. If one InitContainers fails, the application container won\u0026rsquo;t be triggered.\napiVersion:apps/v1kind:Deploymentmetadata:name:webappspec:selector:matchLabels:app:webapptemplate:metadata:labels:app:webappspec:initContainers:# check if DB is ready, and only continue when true- name:check-db-readyimage:postgres:9.6.5command:[\u0026#39;sh\u0026#39;,\u0026#39;-c\u0026#39;,\u0026#39;until pg_isready -h postgres -p 5432; do echo waiting for database; sleep 2; done;\u0026#39;]containers:- image:xcoulon/go-url-shortener:0.1.0name:go-url-shortenerenv:- name:POSTGRES_HOSTvalue:postgres- name:POSTGRES_PORTvalue:\u0026#34;5432\u0026#34;- name:POSTGRES_DATABASEvalue:url_shortener_db- name:POSTGRES_USERvalue:user- name:POSTGRES_PASSWORDvalue:mysecretpasswordports:- containerPort:8080In above example, the InitContainers uses docker image postgres:9.6.5 which is different from the application container. This also brings the advantage of not having to include unnecessary tools (e.g. pg_isready) in the application container.\nWith introduction of InitContainers, the pod startup will look like following in case database is not available yet:\n$ kubectl get po -w NAME READY STATUS RESTARTS AGE nginx-deployment-5cc79d6bfd-t9n8h 1/1 Running 0 5d privileged-pod 1/1 Running 0 4d webapp-fdcb49cbc-4gs4n 0/1 Pending 0 0s webapp-fdcb49cbc-4gs4n 0/1 Pending 0 0s webapp-fdcb49cbc-4gs4n 0/1 Init:0/1 0 0s webapp-fdcb49cbc-4gs4n 0/1 Init:0/1 0 1s $ kubectl logs webapp-fdcb49cbc-4gs4n Error from server (BadRequest): container \u0026#34;go-url-shortener\u0026#34; in pod \u0026#34;webapp-fdcb49cbc-4gs4n\u0026#34; is waiting to start: PodInitializing "},{"uri":"https://gardener.cloud/documentation/contribute/20_documentation/10_organisation/","title":"Organization","tags":[],"description":"","content":"The Gardener project implements the documentation-as-code paradigm. Essentially this means that:\n Documentation resides close to the code it describes - in the corresponding GitHub repositories. Only documentation with regards to cross-cutting concerns that cannot be affiliated to a specific component repository is hosted in the general gardener/documentation repository. We use tools to develop, validate and integrate documentation sources The change management process is largely automated with automatic validation, integration and deployment using docforge and docs-toolbelt. The documentation sources are intended for reuse and not bound to a specific publishing platform. The physical organization in a repository is irrelevant for the tool support. What needs to be maintained is the intended result in a docforge documentation bundle manifest configuration, very much like virtual machines configurations, that docforge can reliably recreate in any case. We use GitHub as distributed, versioning storage system and docforge to pull sources in their desired state to forge documentation bundles according to a desired specification provided as a manifest.  Content Organization Documentation that can be affiliated to component is hosted and maintained in the component repository.\nA recommended template for organizing documentation sources is to place them all in a docs folder and organize it there per role activity. For example:\nrepositoryX |_ docs |_ usage | |_ images | |_ 01.png | |_ hibernation.md |_ operations |_ deployment Do not use folders just because they are in the template. Stick to the predefined roles and corresponding activities for naming convention. A system makes it easier to maintain and get oriented.\n User: usage Operator: operations Gardener (service) provider: deployment Gardener Developer: development Gardener Extension Developer: extensions  Publishing on gardener.cloud The Gardener website is one of the multiple optional publishing channels where the source material might end up as documentation. We use docforge and automated integration and publish process to enable transparent change management.\nTo have documentation published on the website it is necessary to use the docforge manifests available at [gardener/documentation/.docforge] adn register a reference to your documentation.\n Note: This is work in progress and we are transitioning to a more transparent way of integrating component documentation. This guide will be updated as we progress.\n These manifests describe a particular publishing goal, i.e. using Hugo to publish on the website, and you will find out that they contain Hugo-specific front-matter properties. Consult with the documentation maintainers for details. Use the gardener channel in slack or open a PR.\n"},{"uri":"https://gardener.cloud/v1.12.8/contribute/20_documentation/10_organisation/","title":"Organization","tags":[],"description":"","content":"The Gardener project implements the documentation-as-code paradigm. Essentially this means that:\n Documentation resides close to the code it describes - in the corresponding GitHub repositories. Only documentation with regards to cross-cutting concerns that cannot be affiliated to a specific component repository is hosted in the general gardener/documentation repository. We use tools to develop, validate and integrate documentation sources The change management process is largely automated with automatic validation, integration and deployment using docforge and docs-toolbelt. The documentation sources are intended for reuse and not bound to a specific publishing platform. The physical organization in a repository is irrelevant for the tool support. What needs to be maintained is the intended result in a docforge documentation bundle manifest configuration, very much like virtual machines configurations, that docforge can reliably recreate in any case. We use GitHub as distributed, versioning storage system and docforge to pull sources in their desired state to forge documentation bundles according to a desired specification provided as a manifest.  Content Organization Documentation that can be affiliated to component is hosted and maintained in the component repository.\nA recommended template for organizing documentation sources is to place them all in a docs folder and organize it there per role activity. For example:\nrepositoryX |_ docs |_ usage | |_ images | |_ 01.png | |_ hibernation.md |_ operations |_ deployment Do not use folders just because they are in the template. Stick to the predefined roles and corresponding activities for naming convention. A system makes it easier to maintain and get oriented.\n User: usage Operator: operations Gardener (service) provider: deployment Gardener Developer: development Gardener Extension Developer: extensions  Publishing on gardener.cloud The Gardener website is one of the multiple optional publishing channels where the source material might end up as documentation. We use docforge and automated integration and publish process to enable transparent change management.\nTo have documentation published on the website it is necessary to use the docforge manifests available at [gardener/documentation/.docforge] adn register a reference to your documentation.\n Note: This is work in progress and we are transitioning to a more transparent way of integrating component documentation. This guide will be updated as we progress.\n These manifests describe a particular publishing goal, i.e. using Hugo to publish on the website, and you will find out that they contain Hugo-specific front-matter properties. Consult with the documentation maintainers for details. Use the gardener channel in slack or open a PR.\n"},{"uri":"https://gardener.cloud/v1.13.2/contribute/20_documentation/10_organisation/","title":"Organization","tags":[],"description":"","content":"The Gardener project implements the documentation-as-code paradigm. Essentially this means that:\n Documentation resides close to the code it describes - in the corresponding GitHub repositories. Only documentation with regards to cross-cutting concerns that cannot be affiliated to a specific component repository is hosted in the general gardener/documentation repository. We use tools to develop, validate and integrate documentation sources The change management process is largely automated with automatic validation, integration and deployment using docforge and docs-toolbelt. The documentation sources are intended for reuse and not bound to a specific publishing platform. The physical organization in a repository is irrelevant for the tool support. What needs to be maintained is the intended result in a docforge documentation bundle manifest configuration, very much like virtual machines configurations, that docforge can reliably recreate in any case. We use GitHub as distributed, versioning storage system and docforge to pull sources in their desired state to forge documentation bundles according to a desired specification provided as a manifest.  Content Organization Documentation that can be affiliated to component is hosted and maintained in the component repository.\nA recommended template for organizing documentation sources is to place them all in a docs folder and organize it there per role activity. For example:\nrepositoryX |_ docs |_ usage | |_ images | |_ 01.png | |_ hibernation.md |_ operations |_ deployment Do not use folders just because they are in the template. Stick to the predefined roles and corresponding activities for naming convention. A system makes it easier to maintain and get oriented.\n User: usage Operator: operations Gardener (service) provider: deployment Gardener Developer: development Gardener Extension Developer: extensions  Publishing on gardener.cloud The Gardener website is one of the multiple optional publishing channels where the source material might end up as documentation. We use docforge and automated integration and publish process to enable transparent change management.\nTo have documentation published on the website it is necessary to use the docforge manifests available at [gardener/documentation/.docforge] adn register a reference to your documentation.\n Note: This is work in progress and we are transitioning to a more transparent way of integrating component documentation. This guide will be updated as we progress.\n These manifests describe a particular publishing goal, i.e. using Hugo to publish on the website, and you will find out that they contain Hugo-specific front-matter properties. Consult with the documentation maintainers for details. Use the gardener channel in slack or open a PR.\n"},{"uri":"https://gardener.cloud/documentation/guides/client_tools/working-with-kubeconfig/","title":"Organizing Access Using kubeconfig Files","tags":[],"description":"","content":"Organizing Access Using kubeconfig Files The kubectl command-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.\nProblem If you\u0026rsquo;ve become aware of a security breach that affects you, you may want to revoke or cycle credentials in case anything was leaked. However, this is not possible with the initial or master kubeconfig from your cluster.\nPitfall Never distribute the kubeconfig, which you can download directly within the Gardener dashboard, for a productive cluster.\nCreate custom kubeconfig file for each user Create a separate kubeconfig for each user. One of the big advantages is, that you can revoke them and control the permissions better. A limitation to single namespaces is also possible here.\nThe script creates a new ServiceAccount with read privileges in the whole cluster (Secretes are excluded). To run the script jq, a lightweight and flexible command-line JSON processor, must be installed.\n#!/bin/bash  if [[ -z \u0026#34;$1\u0026#34; ]] ;then echo \u0026#34;usage: $0\u0026lt;username\u0026gt;\u0026#34; exit 1 fi user=$1 kubectl create sa ${user} secret=$(kubectl get sa ${user} -o json | jq -r .secrets[].name) kubectl get secret ${secret} -o json | jq -r \u0026#39;.data[\u0026#34;ca.crt\u0026#34;]\u0026#39; | base64 -D \u0026gt; ca.crt user_token=$(kubectl get secret ${secret} -o json | jq -r \u0026#39;.data[\u0026#34;token\u0026#34;]\u0026#39; | base64 -D) c=`kubectl config current-context` cluster_name=`kubectl config get-contexts $c | awk \u0026#39;{print $3}\u0026#39; | tail -n 1` endpoint=`kubectl config view -o jsonpath=\u0026#34;{.clusters[?(@.name == \\\u0026#34;${cluster_name}\\\u0026#34;)].cluster.server}\u0026#34;` # Set up the config KUBECONFIG=k8s-${user}-conf kubectl config set-cluster ${cluster_name} \\  --embed-certs=true \\  --server=${endpoint} \\  --certificate-authority=./ca.crt KUBECONFIG=k8s-${user}-conf kubectl config set-credentials ${user}-${cluster_name#cluster-} --token=${user_token} KUBECONFIG=k8s-${user}-conf kubectl config set-context ${user}-${cluster_name#cluster-} \\  --cluster=${cluster_name} \\  --user=${user}-${cluster_name#cluster-} KUBECONFIG=k8s-${user}-conf kubectl config use-context ${user}-${cluster_name#cluster-} cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: view-${user}-global subjects: - kind: ServiceAccount name: ${user} namespace: default roleRef: kind: ClusterRole name: view apiGroup: rbac.authorization.k8s.io EOF echo \u0026#34;done! Test with: \u0026#34; echo \u0026#34;export KUBECONFIG=k8s-${user}-conf\u0026#34; echo \u0026#34;kubectl get pods\u0026#34; If edit or admin rights are to be assigned, the ClusterRoleBinding must be adapted in the roleRef section with the roles listed below.\nFurthermore, you can restrict this to a single namespace by not creating a ClusterRoleBinding but only a RoleBinding within the desired namespace.\n   Default ClusterRole Default ClusterRoleBinding Description     cluster-admin system:masters group Allows super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding\u0026rsquo;s namespace, including the namespace itself.   admin None Allows admin access, intended to be granted within a namespace using a RoleBinding. If used in a RoleBinding, allows read/write access to most resources in a namespace, including the ability to create roles and rolebindings within the namespace. It does not allow write access to resource quota or to the namespace itself.   edit None Allows read/write access to most objects in a namespace. It does not allow viewing or modifying roles or rolebindings.   view None Allows read-only access to see most objects in a namespace. It does not allow viewing roles or rolebindings. It does not allow viewing secrets, since those are escalating.    "},{"uri":"https://gardener.cloud/v1.12.8/guides/client_tools/working-with-kubeconfig/","title":"Organizing Access Using kubeconfig Files","tags":[],"description":"","content":"Organizing Access Using kubeconfig Files The kubectl command-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.\nProblem If you\u0026rsquo;ve become aware of a security breach that affects you, you may want to revoke or cycle credentials in case anything was leaked. However, this is not possible with the initial or master kubeconfig from your cluster.\nPitfall Never distribute the kubeconfig, which you can download directly within the Gardener dashboard, for a productive cluster.\nCreate custom kubeconfig file for each user Create a separate kubeconfig for each user. One of the big advantages is, that you can revoke them and control the permissions better. A limitation to single namespaces is also possible here.\nThe script creates a new ServiceAccount with read privileges in the whole cluster (Secretes are excluded). To run the script jq, a lightweight and flexible command-line JSON processor, must be installed.\n#!/bin/bash  if [[ -z \u0026#34;$1\u0026#34; ]] ;then echo \u0026#34;usage: $0\u0026lt;username\u0026gt;\u0026#34; exit 1 fi user=$1 kubectl create sa ${user} secret=$(kubectl get sa ${user} -o json | jq -r .secrets[].name) kubectl get secret ${secret} -o json | jq -r \u0026#39;.data[\u0026#34;ca.crt\u0026#34;]\u0026#39; | base64 -D \u0026gt; ca.crt user_token=$(kubectl get secret ${secret} -o json | jq -r \u0026#39;.data[\u0026#34;token\u0026#34;]\u0026#39; | base64 -D) c=`kubectl config current-context` cluster_name=`kubectl config get-contexts $c | awk \u0026#39;{print $3}\u0026#39; | tail -n 1` endpoint=`kubectl config view -o jsonpath=\u0026#34;{.clusters[?(@.name == \\\u0026#34;${cluster_name}\\\u0026#34;)].cluster.server}\u0026#34;` # Set up the config KUBECONFIG=k8s-${user}-conf kubectl config set-cluster ${cluster_name} \\  --embed-certs=true \\  --server=${endpoint} \\  --certificate-authority=./ca.crt KUBECONFIG=k8s-${user}-conf kubectl config set-credentials ${user}-${cluster_name#cluster-} --token=${user_token} KUBECONFIG=k8s-${user}-conf kubectl config set-context ${user}-${cluster_name#cluster-} \\  --cluster=${cluster_name} \\  --user=${user}-${cluster_name#cluster-} KUBECONFIG=k8s-${user}-conf kubectl config use-context ${user}-${cluster_name#cluster-} cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: view-${user}-global subjects: - kind: ServiceAccount name: ${user} namespace: default roleRef: kind: ClusterRole name: view apiGroup: rbac.authorization.k8s.io EOF echo \u0026#34;done! Test with: \u0026#34; echo \u0026#34;export KUBECONFIG=k8s-${user}-conf\u0026#34; echo \u0026#34;kubectl get pods\u0026#34; If edit or admin rights are to be assigned, the ClusterRoleBinding must be adapted in the roleRef section with the roles listed below.\nFurthermore, you can restrict this to a single namespace by not creating a ClusterRoleBinding but only a RoleBinding within the desired namespace.\n   Default ClusterRole Default ClusterRoleBinding Description     cluster-admin system:masters group Allows super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding\u0026rsquo;s namespace, including the namespace itself.   admin None Allows admin access, intended to be granted within a namespace using a RoleBinding. If used in a RoleBinding, allows read/write access to most resources in a namespace, including the ability to create roles and rolebindings within the namespace. It does not allow write access to resource quota or to the namespace itself.   edit None Allows read/write access to most objects in a namespace. It does not allow viewing or modifying roles or rolebindings.   view None Allows read-only access to see most objects in a namespace. It does not allow viewing roles or rolebindings. It does not allow viewing secrets, since those are escalating.    "},{"uri":"https://gardener.cloud/v1.13.2/guides/client_tools/working-with-kubeconfig/","title":"Organizing Access Using kubeconfig Files","tags":[],"description":"","content":"Organizing Access Using kubeconfig Files The kubectl command-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.\nProblem If you\u0026rsquo;ve become aware of a security breach that affects you, you may want to revoke or cycle credentials in case anything was leaked. However, this is not possible with the initial or master kubeconfig from your cluster.\nPitfall Never distribute the kubeconfig, which you can download directly within the Gardener dashboard, for a productive cluster.\nCreate custom kubeconfig file for each user Create a separate kubeconfig for each user. One of the big advantages is, that you can revoke them and control the permissions better. A limitation to single namespaces is also possible here.\nThe script creates a new ServiceAccount with read privileges in the whole cluster (Secretes are excluded). To run the script jq, a lightweight and flexible command-line JSON processor, must be installed.\n#!/bin/bash  if [[ -z \u0026#34;$1\u0026#34; ]] ;then echo \u0026#34;usage: $0\u0026lt;username\u0026gt;\u0026#34; exit 1 fi user=$1 kubectl create sa ${user} secret=$(kubectl get sa ${user} -o json | jq -r .secrets[].name) kubectl get secret ${secret} -o json | jq -r \u0026#39;.data[\u0026#34;ca.crt\u0026#34;]\u0026#39; | base64 -D \u0026gt; ca.crt user_token=$(kubectl get secret ${secret} -o json | jq -r \u0026#39;.data[\u0026#34;token\u0026#34;]\u0026#39; | base64 -D) c=`kubectl config current-context` cluster_name=`kubectl config get-contexts $c | awk \u0026#39;{print $3}\u0026#39; | tail -n 1` endpoint=`kubectl config view -o jsonpath=\u0026#34;{.clusters[?(@.name == \\\u0026#34;${cluster_name}\\\u0026#34;)].cluster.server}\u0026#34;` # Set up the config KUBECONFIG=k8s-${user}-conf kubectl config set-cluster ${cluster_name} \\  --embed-certs=true \\  --server=${endpoint} \\  --certificate-authority=./ca.crt KUBECONFIG=k8s-${user}-conf kubectl config set-credentials ${user}-${cluster_name#cluster-} --token=${user_token} KUBECONFIG=k8s-${user}-conf kubectl config set-context ${user}-${cluster_name#cluster-} \\  --cluster=${cluster_name} \\  --user=${user}-${cluster_name#cluster-} KUBECONFIG=k8s-${user}-conf kubectl config use-context ${user}-${cluster_name#cluster-} cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: view-${user}-global subjects: - kind: ServiceAccount name: ${user} namespace: default roleRef: kind: ClusterRole name: view apiGroup: rbac.authorization.k8s.io EOF echo \u0026#34;done! Test with: \u0026#34; echo \u0026#34;export KUBECONFIG=k8s-${user}-conf\u0026#34; echo \u0026#34;kubectl get pods\u0026#34; If edit or admin rights are to be assigned, the ClusterRoleBinding must be adapted in the roleRef section with the roles listed below.\nFurthermore, you can restrict this to a single namespace by not creating a ClusterRoleBinding but only a RoleBinding within the desired namespace.\n   Default ClusterRole Default ClusterRoleBinding Description     cluster-admin system:masters group Allows super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding\u0026rsquo;s namespace, including the namespace itself.   admin None Allows admin access, intended to be granted within a namespace using a RoleBinding. If used in a RoleBinding, allows read/write access to most resources in a namespace, including the ability to create roles and rolebindings within the namespace. It does not allow write access to resource quota or to the namespace itself.   edit None Allows read/write access to most objects in a namespace. It does not allow viewing or modifying roles or rolebindings.   view None Allows read-only access to see most objects in a namespace. It does not allow viewing roles or rolebindings. It does not allow viewing secrets, since those are escalating.    "},{"uri":"https://gardener.cloud/documentation/guides/applications/service-cache-control/","title":"Out-Dated HTML and JS files delivered","tags":[],"description":"Why is my application always outdated?","content":"Problem After updating your HTML and JavaScript sources in your web application, the kubernetes cluster delivers outdated versions - why?\nPreamble By default, Kubernetes service pods are not accessible from the external network, but only from other pods within the same Kubernetes cluster.\nThe Gardener cluster has a built-in configuration for HTTP load balancing called Ingress, defining rules for external connectivity to Kubernetes services. Users who want external access to their Kubernetes services create an ingress resource that defines rules, including the URI path, backing service name, and other information. The Ingress controller can then automatically program a frontend load balancer to enable Ingress configuration.\nExample Ingress Configuration apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:vuejs-ingressspec:rules:- host:test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080where:\n \u0026lt;GARDENER-CLUSTER\u0026gt;: The cluster name in the Gardener \u0026lt;GARDENER-PROJECT\u0026gt;: You project name in the Gardener  What is the underlying problem? The ingress controller we are using is NGINX.\n NGINX is a software load balancer, web server, and content cache built on top of open source NGINX.\n NGINX caches the content as specified in the HTTP header. If the HTTP header is missing, it is assumed that the cache is forever and NGINX never updates the content in the stupidest case.\nSolution In general you can avoid this pitfall with one of the solutions below:\n use a cache buster + HTTP-Cache-Control(prefered) use HTTP-Cache-Control with a lower retention period disable the caching in the ingress (just for dev purpose)  Learning how to set the HTTP header or setup a cache buster is left to the read as an exercise for your web framework (e.g. Express/NodeJS, SpringBoot,\u0026hellip;)\nHere an example how to disable the cache control for your ingress done with an annotation in your ingress YAML (during development).\n---apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:annotations:ingress.kubernetes.io/cache-enable:\u0026#34;false\u0026#34;name:vuejs-ingressspec:rules:- host:test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080"},{"uri":"https://gardener.cloud/v1.12.8/guides/applications/service-cache-control/","title":"Out-Dated HTML and JS files delivered","tags":[],"description":"Why is my application always outdated?","content":"Problem After updating your HTML and JavaScript sources in your web application, the kubernetes cluster delivers outdated versions - why?\nPreamble By default, Kubernetes service pods are not accessible from the external network, but only from other pods within the same Kubernetes cluster.\nThe Gardener cluster has a built-in configuration for HTTP load balancing called Ingress, defining rules for external connectivity to Kubernetes services. Users who want external access to their Kubernetes services create an ingress resource that defines rules, including the URI path, backing service name, and other information. The Ingress controller can then automatically program a frontend load balancer to enable Ingress configuration.\nExample Ingress Configuration apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:vuejs-ingressspec:rules:- host:test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080where:\n \u0026lt;GARDENER-CLUSTER\u0026gt;: The cluster name in the Gardener \u0026lt;GARDENER-PROJECT\u0026gt;: You project name in the Gardener  What is the underlying problem? The ingress controller we are using is NGINX.\n NGINX is a software load balancer, web server, and content cache built on top of open source NGINX.\n NGINX caches the content as specified in the HTTP header. If the HTTP header is missing, it is assumed that the cache is forever and NGINX never updates the content in the stupidest case.\nSolution In general you can avoid this pitfall with one of the solutions below:\n use a cache buster + HTTP-Cache-Control(prefered) use HTTP-Cache-Control with a lower retention period disable the caching in the ingress (just for dev purpose)  Learning how to set the HTTP header or setup a cache buster is left to the read as an exercise for your web framework (e.g. Express/NodeJS, SpringBoot,\u0026hellip;)\nHere an example how to disable the cache control for your ingress done with an annotation in your ingress YAML (during development).\n---apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:annotations:ingress.kubernetes.io/cache-enable:\u0026#34;false\u0026#34;name:vuejs-ingressspec:rules:- host:test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080"},{"uri":"https://gardener.cloud/v1.13.2/guides/applications/service-cache-control/","title":"Out-Dated HTML and JS files delivered","tags":[],"description":"Why is my application always outdated?","content":"Problem After updating your HTML and JavaScript sources in your web application, the kubernetes cluster delivers outdated versions - why?\nPreamble By default, Kubernetes service pods are not accessible from the external network, but only from other pods within the same Kubernetes cluster.\nThe Gardener cluster has a built-in configuration for HTTP load balancing called Ingress, defining rules for external connectivity to Kubernetes services. Users who want external access to their Kubernetes services create an ingress resource that defines rules, including the URI path, backing service name, and other information. The Ingress controller can then automatically program a frontend load balancer to enable Ingress configuration.\nExample Ingress Configuration apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:vuejs-ingressspec:rules:- host:test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080where:\n \u0026lt;GARDENER-CLUSTER\u0026gt;: The cluster name in the Gardener \u0026lt;GARDENER-PROJECT\u0026gt;: You project name in the Gardener  What is the underlying problem? The ingress controller we are using is NGINX.\n NGINX is a software load balancer, web server, and content cache built on top of open source NGINX.\n NGINX caches the content as specified in the HTTP header. If the HTTP header is missing, it is assumed that the cache is forever and NGINX never updates the content in the stupidest case.\nSolution In general you can avoid this pitfall with one of the solutions below:\n use a cache buster + HTTP-Cache-Control(prefered) use HTTP-Cache-Control with a lower retention period disable the caching in the ingress (just for dev purpose)  Learning how to set the HTTP header or setup a cache buster is left to the read as an exercise for your web framework (e.g. Express/NodeJS, SpringBoot,\u0026hellip;)\nHere an example how to disable the cache control for your ingress done with an annotation in your ingress YAML (during development).\n---apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:annotations:ingress.kubernetes.io/cache-enable:\u0026#34;false\u0026#34;name:vuejs-ingressspec:rules:- host:test.ingress.\u0026lt;GARDENER-CLUSTER\u0026gt;.\u0026lt;GARDENER-PROJECT\u0026gt;.shoot.canary.k8s-hana.ondemand.comhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080"},{"uri":"https://gardener.cloud/documentation/contribute/10_code/37_process/","title":"Process","tags":[],"description":"","content":"Creating a new Feature If you want to contribute to the Gardener, please do that always on a dedicated branch on your own fork named after the purpose of the code changes, for example feature/helm-integration. Please do not forget to rebase your branch regularly.\nIf you have finished your work, please create a pull request based on master. It will be reviewed and merged if no further changes are requested from you.\n:warning: Please ensure that your modifications pass the lint checks, formatting checks, static code checks, and unit tests by executing\nmake verify Please do not file your pull request unless you receive a successful response from here!\nCreating a new Release Please refer to the Gardener contributor guide.\n"},{"uri":"https://gardener.cloud/v1.12.8/contribute/10_code/37_process/","title":"Process","tags":[],"description":"","content":"Creating a new Feature If you want to contribute to the Gardener, please do that always on a dedicated branch on your own fork named after the purpose of the code changes, for example feature/helm-integration. Please do not forget to rebase your branch regularly.\nIf you have finished your work, please create a pull request based on master. It will be reviewed and merged if no further changes are requested from you.\n:warning: Please ensure that your modifications pass the lint checks, formatting checks, static code checks, and unit tests by executing\nmake verify Please do not file your pull request unless you receive a successful response from here!\nCreating a new Release Please refer to the Gardener contributor guide.\n"},{"uri":"https://gardener.cloud/v1.13.2/contribute/10_code/37_process/","title":"Process","tags":[],"description":"","content":"Creating a new Feature If you want to contribute to the Gardener, please do that always on a dedicated branch on your own fork named after the purpose of the code changes, for example feature/helm-integration. Please do not forget to rebase your branch regularly.\nIf you have finished your work, please create a pull request based on master. It will be reviewed and merged if no further changes are requested from you.\n:warning: Please ensure that your modifications pass the lint checks, formatting checks, static code checks, and unit tests by executing\nmake verify Please do not file your pull request unless you receive a successful response from here!\nCreating a new Release Please refer to the Gardener contributor guide.\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/referenced-resources/","title":"Referenced Resources","tags":[],"description":"","content":"Referenced Resources The Shoot resource can include a list of resources (usually secrets) that can be referenced by name in extension providerConfig and other Shoot sections, for example:\nkind:ShootapiVersion:core.gardener.cloud/v1beta1metadata:name:crazy-botanynamespace:garden-dev...spec:...extensions:- type:foobarproviderConfig:apiVersion:foobar.extensions.gardener.cloud/v1alpha1kind:FooBarConfigfoo:barsecretRef:foobar-secretresources:- name:foobar-secretresourceRef:apiVersion:v1kind:Secretname:my-foobar-secretGardener expects to find these referenced resources in the project namespace (e.g. garden-dev) and will copy them to the Shoot namespace in the Seed cluster when reconciling a Shoot, adding a prefix to their names to avoid naming collisions with Gardener\u0026rsquo;s own resources.\nExtension controllers can resolve the references to these resources by accessing the Shoot via the Cluster resource. To properly read a referenced resources, extension controllers should use the utility function GetObjectByReference from the extensions/pkg/controller package, for example:\n... ref = \u0026amp;autoscalingv1.CrossVersionObjectReference{ APIVersion: \u0026#34;v1\u0026#34;, Kind: \u0026#34;Secret\u0026#34;, Name: \u0026#34;foo\u0026#34;, } secret := \u0026amp;corev1.Secret{} if err := controller.GetObjectByReference(ctx, client, ref, \u0026#34;shoot--test--foo\u0026#34;, secret); err != nil { return err } // Use secret  ... "},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/referenced-resources/","title":"Referenced Resources","tags":[],"description":"","content":"Referenced Resources The Shoot resource can include a list of resources (usually secrets) that can be referenced by name in extension providerConfig and other Shoot sections, for example:\nkind:ShootapiVersion:core.gardener.cloud/v1beta1metadata:name:crazy-botanynamespace:garden-dev...spec:...extensions:- type:foobarproviderConfig:apiVersion:foobar.extensions.gardener.cloud/v1alpha1kind:FooBarConfigfoo:barsecretRef:foobar-secretresources:- name:foobar-secretresourceRef:apiVersion:v1kind:Secretname:my-foobar-secretGardener expects to find these referenced resources in the project namespace (e.g. garden-dev) and will copy them to the Shoot namespace in the Seed cluster when reconciling a Shoot, adding a prefix to their names to avoid naming collisions with Gardener\u0026rsquo;s own resources.\nExtension controllers can resolve the references to these resources by accessing the Shoot via the Cluster resource. To properly read a referenced resources, extension controllers should use the utility function GetObjectByReference from the extensions/pkg/controller package, for example:\n... ref = \u0026amp;autoscalingv1.CrossVersionObjectReference{ APIVersion: \u0026#34;v1\u0026#34;, Kind: \u0026#34;Secret\u0026#34;, Name: \u0026#34;foo\u0026#34;, } secret := \u0026amp;corev1.Secret{} if err := controller.GetObjectByReference(ctx, client, ref, \u0026#34;shoot--test--foo\u0026#34;, secret); err != nil { return err } // Use secret  ... "},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/referenced-resources/","title":"Referenced Resources","tags":[],"description":"","content":"Referenced Resources The Shoot resource can include a list of resources (usually secrets) that can be referenced by name in extension providerConfig and other Shoot sections, for example:\nkind:ShootapiVersion:core.gardener.cloud/v1beta1metadata:name:crazy-botanynamespace:garden-dev...spec:...extensions:- type:foobarproviderConfig:apiVersion:foobar.extensions.gardener.cloud/v1alpha1kind:FooBarConfigfoo:barsecretRef:foobar-secretresources:- name:foobar-secretresourceRef:apiVersion:v1kind:Secretname:my-foobar-secretGardener expects to find these referenced resources in the project namespace (e.g. garden-dev) and will copy them to the Shoot namespace in the Seed cluster when reconciling a Shoot, adding a prefix to their names to avoid naming collisions with Gardener\u0026rsquo;s own resources.\nExtension controllers can resolve the references to these resources by accessing the Shoot via the Cluster resource. To properly read a referenced resources, extension controllers should use the utility function GetObjectByReference from the extensions/pkg/controller package, for example:\n... ref = \u0026amp;autoscalingv1.CrossVersionObjectReference{ APIVersion: \u0026#34;v1\u0026#34;, Kind: \u0026#34;Secret\u0026#34;, Name: \u0026#34;foo\u0026#34;, } secret := \u0026amp;corev1.Secret{} if err := controller.GetObjectByReference(ctx, client, ref, \u0026#34;shoot--test--foo\u0026#34;, secret); err != nil { return err } // Use secret  ... "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/controllerregistration/","title":"Registering Extension Controllers","tags":[],"description":"","content":"Registering Extension Controllers Extensions are registered in the garden cluster via ControllerRegistration resources. Gardener is evaluating the registrations and creates ControllerInstallation resources which describe the request \u0026ldquo;please install this controller X to this seed Y\u0026rdquo;.\nSimilar to how CloudProfile or Seed resources get into the system, the Gardener administrator must deploy the ControllerRegistration resources (this does not happen automatically in any way - the administrator decides which extensions shall be enabled).\nThe specification mainly describes which of Gardener\u0026rsquo;s extension CRDs are managed, for example:\napiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationmetadata:name:os-coreosspec:resources:- kind:OperatingSystemConfigtype:coreosprimary:trueThis information tells Gardener that there is an extension controller that can handle OperatingSystemConfig resources of type coreos.\nAlso, it specifies that this controller is the primary one responsible for the lifecycle of the OperatingSystemConfig resource. Setting primary to false would allow to register additional, secondary controllers that may also watch/react on the OperatingSystemConfig/coreos resources, however, only the primary controller may change/update the main status of the extension object (that are used to \u0026ldquo;communicate\u0026rdquo; with the Gardenlet). Particularly, only the primary controller may set .status.lastOperation, .status.lastError, .status.observedGeneration, and .status.state. Secondary controllers may contribute to the .status.conditions[] if they like, of course.\nSecondary controllers might be helpful in scenarios where additional tasks need to be completed which are not part of the reconciliation logic of the primary controller but separated out into a dedicated extension.\n There must be exactly one primary controller for every registered kind/type combination. Also, please note that the primary field cannot be changed after creation of the ControllerRegistration.\nDeploying Extension Controllers Submitting above ControllerRegistration will create a ControllerInstallation resource:\napiVersion:core.gardener.cloud/v1beta1kind:ControllerInstallationmetadata:name:os-coreosspec:registrationRef:apiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationname:os-coreosseedRef:apiVersion:core.gardener.cloud/v1beta1kind:Seedname:aws-eu1This resource expresses that Gardener requires the os-coreos extension controller to run on the aws-eu1 seed cluster.\nThe Gardener Controller Manager does automatically determine which extension is required on which seed cluster and will only create ControllerInstallation objects for those. Also, it will automatically delete ControllerInstallations referencing extension controllers that are no longer required on a seed (e.g., because all shoots on it have been deleted). There are additional configuration options, please see this section.\nHow do extension controllers get deployed to seeds? After Gardener has written the ControllerInstallation resource some component must satisfy this request and start deploying the extension controller to the seed. Depending on the complexity of the controllers lifecycle management, configuration, etc. there are two possible scenarios:\nScenario 1: Deployed by Gardener In many cases the extension controllers are easy to deploy and configure. It is sufficient to simply create a Helm chart (standardized way of packaging software in the Kubernetes context) and deploy it together with some static configuration values. Gardener supports this scenario and allows to provide arbitrary deployment information in the ControllerRegistration resource\u0026rsquo;s .spec section:\n...spec:...deployment:type:helmproviderConfig:chart:H4sIFAAAAAAA/yk...values:foo:barIf .spec.deployment.type=helm then Gardener itself will take over the responsibility the deployment. It base64-decodes the provided Helm chart (.spec.deployment.providerConfig.chart) and deploys it with the provided static configuration (.spec.deployment.providerConfig.values). The chart and the values can be updated at any time - Gardener will recognize and re-trigger the deployment process.\nIn order to allow extensions to get information about the garden and the seed cluster Gardener does mix-in certain properties into the values (root level) of every deployed Helm chart:\ngardener:garden:identifier:\u0026lt;uuid-of-gardener-installation\u0026gt; seed:identifier:\u0026lt;seed-name\u0026gt; region: europespec:\u0026lt;complete-seed-spec\u0026gt;Extensions can use this information in their Helm chart in case they require knowledge about the garden and the seed environment. The list might be extended in the future.\n:information_source: Gardener uses the UUID of the garden Namespace object in the .gardener.garden.identifier property.\nScenario 2: Deployed by a (non-human) Kubernetes operator Some extension controllers might be more complex and require additional domain-specific knowledge wrt. lifecycle or configuration. In this case, we encourage to follow the Kubernetes operator pattern and deploy a dedicated operator for this extension into the garden cluster. The ControllerResource's .spec.deployment.type field would then not be helm, and no Helm chart or values need to be provided there. Instead, the operator itself knows how to deploy the extension into the seed. It must watch ControllerInstallation resources and act one those referencing a ControllerRegistration the operator is responsible for.\nIn order to let Gardener know that the extension controller is ready and running in the seed the ControllerInstallation's .status field supports two conditions: RegistrationValid and InstallationSuccessful - both must be provided by the responsible operator:\n...status:conditions:- lastTransitionTime:\u0026#34;2019-01-22T11:51:11Z\u0026#34;lastUpdateTime:\u0026#34;2019-01-22T11:51:11Z\u0026#34;message:Chartcouldberenderedsuccessfully.reason:RegistrationValidstatus:\u0026#34;True\u0026#34;type:Valid- lastTransitionTime:\u0026#34;2019-01-22T11:51:12Z\u0026#34;lastUpdateTime:\u0026#34;2019-01-22T11:51:12Z\u0026#34;message:Installationofnewresourcessucceeded.reason:InstallationSuccessfulstatus:\u0026#34;True\u0026#34;type:InstalledAdditionally, the .status field has a providerStatus section into which the operator can (optionally) put any arbitrary data associated with this installation.\nExtensions in the garden cluster itself The Shoot resource itself will contain some provider-specific data blobs. As a result, some extensions might also want to run in the garden cluster, e.g., to provide ValidatingWebhookConfigurations for validating the correctness of their provider-specific blobs:\napiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:name:johndoe-awsnamespace:garden-devspec:...cloud:type:awsregion:eu-west-1providerConfig:apiVersion:aws.cloud.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:vpc:# specify either \u0026#39;id\u0026#39; or \u0026#39;cidr\u0026#39;# id: vpc-123456cidr:10.250.0.0/16internal:- 10.250.112.0/22public:- 10.250.96.0/22workers:- 10.250.0.0/19zones:- eu-west-1a...In the above example, Gardener itself does not understand the AWS-specific provider configuration for the infrastructure. However, if this part of the Shoot resource should be validated then you should run an AWS-specific component in the garden cluster that registers a webhook. You can do it similarly if you want to default some fields of a resource (by using a MutatingWebhookConfiguration).\nAgain, similar to how Gardener is deployed to the garden cluster, these components must be deployed and managed by the Gardener administrator.\nExtension resource configurations The Extension resource allows injecting arbitrary steps into the shoot reconciliation flow that are unknown to Gardener. Hence, it is slightly special and allows further configuration when registering it:\napiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationmetadata:name:extension-foospec:resources:- kind:Extensiontype:fooprimary:truegloballyEnabled:truereconcileTimeout:30sThe globallyEnabled=true option specifies that the Extension/foo object shall be created by default for all shoots (unless they opted out by setting .spec.extensions[].enabled=false in the Shoot spec).\nThe reconcileTimeout tells Gardener how long it should wait during its shoot reconciliation flow for the Extension/foo's reconciliation to finish.\nDeployment configuration options The .spec.deployment resource allows to configure a deployment policy. There are the following policies:\n OnDemand (default): Gardener will demand the deployment and deletion of the extension controller to/from seed clusters dynamically. It will automatically determine (based on other resources like Shoots) whether it is required and decide accordingly. Always: Gardener will demand the deployment of the extension controller to seed clusters independent of whether it is actually required or not. This might be helpful if you want to add a new component/controller to all seed clusters by default. Another use-case is to minimize the durations until extension controllers get deployed and ready in case you have highly fluctuating seed clusters. AlwaysExceptNoShoots: Similar to Always, but if the seed does not have any shoots then the extension is not being deployed. It will be deleted from a seed after the last shoot has been removed from it.  Also, the .spec.deployment.seedSelector allows to specify a label selector for seed clusters. Only if it matches the labels of a seed then it will be deployed to it. Please note that a seed selector can only be specified for secondary controllers (primary=false for all .spec.resources[]).\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/controllerregistration/","title":"Registering Extension Controllers","tags":[],"description":"","content":"Registering Extension Controllers Extensions are registered in the garden cluster via ControllerRegistration resources. Gardener is evaluating the registrations and creates ControllerInstallation resources which describe the request \u0026ldquo;please install this controller X to this seed Y\u0026rdquo;.\nSimilar to how CloudProfile or Seed resources get into the system, the Gardener administrator must deploy the ControllerRegistration resources (this does not happen automatically in any way - the administrator decides which extensions shall be enabled).\nThe specification mainly describes which of Gardener\u0026rsquo;s extension CRDs are managed, for example:\napiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationmetadata:name:os-coreosspec:resources:- kind:OperatingSystemConfigtype:coreosprimary:trueThis information tells Gardener that there is an extension controller that can handle OperatingSystemConfig resources of type coreos.\nAlso, it specifies that this controller is the primary one responsible for the lifecycle of the OperatingSystemConfig resource. Setting primary to false would allow to register additional, secondary controllers that may also watch/react on the OperatingSystemConfig/coreos resources, however, only the primary controller may change/update the main status of the extension object (that are used to \u0026ldquo;communicate\u0026rdquo; with the Gardenlet). Particularly, only the primary controller may set .status.lastOperation, .status.lastError, .status.observedGeneration, and .status.state. Secondary controllers may contribute to the .status.conditions[] if they like, of course.\nSecondary controllers might be helpful in scenarios where additional tasks need to be completed which are not part of the reconciliation logic of the primary controller but separated out into a dedicated extension.\n There must be exactly one primary controller for every registered kind/type combination. Also, please note that the primary field cannot be changed after creation of the ControllerRegistration.\nDeploying Extension Controllers Submitting above ControllerRegistration will create a ControllerInstallation resource:\napiVersion:core.gardener.cloud/v1beta1kind:ControllerInstallationmetadata:name:os-coreosspec:registrationRef:apiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationname:os-coreosseedRef:apiVersion:core.gardener.cloud/v1beta1kind:Seedname:aws-eu1This resource expresses that Gardener requires the os-coreos extension controller to run on the aws-eu1 seed cluster.\nThe Gardener Controller Manager does automatically determine which extension is required on which seed cluster and will only create ControllerInstallation objects for those. Also, it will automatically delete ControllerInstallations referencing extension controllers that are no longer required on a seed (e.g., because all shoots on it have been deleted). There are additional configuration options, please see this section.\nHow do extension controllers get deployed to seeds? After Gardener has written the ControllerInstallation resource some component must satisfy this request and start deploying the extension controller to the seed. Depending on the complexity of the controllers lifecycle management, configuration, etc. there are two possible scenarios:\nScenario 1: Deployed by Gardener In many cases the extension controllers are easy to deploy and configure. It is sufficient to simply create a Helm chart (standardized way of packaging software in the Kubernetes context) and deploy it together with some static configuration values. Gardener supports this scenario and allows to provide arbitrary deployment information in the ControllerRegistration resource\u0026rsquo;s .spec section:\n...spec:...deployment:type:helmproviderConfig:chart:H4sIFAAAAAAA/yk...values:foo:barIf .spec.deployment.type=helm then Gardener itself will take over the responsibility the deployment. It base64-decodes the provided Helm chart (.spec.deployment.providerConfig.chart) and deploys it with the provided static configuration (.spec.deployment.providerConfig.values). The chart and the values can be updated at any time - Gardener will recognize and re-trigger the deployment process.\nIn order to allow extensions to get information about the garden and the seed cluster Gardener does mix-in certain properties into the values (root level) of every deployed Helm chart:\ngardener:garden:identifier:\u0026lt;uuid-of-gardener-installation\u0026gt; seed:identifier:\u0026lt;seed-name\u0026gt; region: europespec:\u0026lt;complete-seed-spec\u0026gt;Extensions can use this information in their Helm chart in case they require knowledge about the garden and the seed environment. The list might be extended in the future.\n:information_source: Gardener uses the UUID of the garden Namespace object in the .gardener.garden.identifier property.\nScenario 2: Deployed by a (non-human) Kubernetes operator Some extension controllers might be more complex and require additional domain-specific knowledge wrt. lifecycle or configuration. In this case, we encourage to follow the Kubernetes operator pattern and deploy a dedicated operator for this extension into the garden cluster. The ControllerResource's .spec.deployment.type field would then not be helm, and no Helm chart or values need to be provided there. Instead, the operator itself knows how to deploy the extension into the seed. It must watch ControllerInstallation resources and act one those referencing a ControllerRegistration the operator is responsible for.\nIn order to let Gardener know that the extension controller is ready and running in the seed the ControllerInstallation's .status field supports two conditions: RegistrationValid and InstallationSuccessful - both must be provided by the responsible operator:\n...status:conditions:- lastTransitionTime:\u0026#34;2019-01-22T11:51:11Z\u0026#34;lastUpdateTime:\u0026#34;2019-01-22T11:51:11Z\u0026#34;message:Chartcouldberenderedsuccessfully.reason:RegistrationValidstatus:\u0026#34;True\u0026#34;type:Valid- lastTransitionTime:\u0026#34;2019-01-22T11:51:12Z\u0026#34;lastUpdateTime:\u0026#34;2019-01-22T11:51:12Z\u0026#34;message:Installationofnewresourcessucceeded.reason:InstallationSuccessfulstatus:\u0026#34;True\u0026#34;type:InstalledAdditionally, the .status field has a providerStatus section into which the operator can (optionally) put any arbitrary data associated with this installation.\nExtensions in the garden cluster itself The Shoot resource itself will contain some provider-specific data blobs. As a result, some extensions might also want to run in the garden cluster, e.g., to provide ValidatingWebhookConfigurations for validating the correctness of their provider-specific blobs:\napiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:name:johndoe-awsnamespace:garden-devspec:...cloud:type:awsregion:eu-west-1providerConfig:apiVersion:aws.cloud.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:vpc:# specify either \u0026#39;id\u0026#39; or \u0026#39;cidr\u0026#39;# id: vpc-123456cidr:10.250.0.0/16internal:- 10.250.112.0/22public:- 10.250.96.0/22workers:- 10.250.0.0/19zones:- eu-west-1a...In the above example, Gardener itself does not understand the AWS-specific provider configuration for the infrastructure. However, if this part of the Shoot resource should be validated then you should run an AWS-specific component in the garden cluster that registers a webhook. You can do it similarly if you want to default some fields of a resource (by using a MutatingWebhookConfiguration).\nAgain, similar to how Gardener is deployed to the garden cluster, these components must be deployed and managed by the Gardener administrator.\nExtension resource configurations The Extension resource allows injecting arbitrary steps into the shoot reconciliation flow that are unknown to Gardener. Hence, it is slightly special and allows further configuration when registering it:\napiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationmetadata:name:extension-foospec:resources:- kind:Extensiontype:fooprimary:truegloballyEnabled:truereconcileTimeout:30sThe globallyEnabled=true option specifies that the Extension/foo object shall be created by default for all shoots (unless they opted out by setting .spec.extensions[].enabled=false in the Shoot spec).\nThe reconcileTimeout tells Gardener how long it should wait during its shoot reconciliation flow for the Extension/foo's reconciliation to finish.\nDeployment configuration options The .spec.deployment resource allows to configure a deployment policy. There are the following policies:\n OnDemand (default): Gardener will demand the deployment and deletion of the extension controller to/from seed clusters dynamically. It will automatically determine (based on other resources like Shoots) whether it is required and decide accordingly. Always: Gardener will demand the deployment of the extension controller to seed clusters independent of whether it is actually required or not. This might be helpful if you want to add a new component/controller to all seed clusters by default. Another use-case is to minimize the durations until extension controllers get deployed and ready in case you have highly fluctuating seed clusters. AlwaysExceptNoShoots: Similar to Always, but if the seed does not have any shoots then the extension is not being deployed. It will be deleted from a seed after the last shoot has been removed from it.  Also, the .spec.deployment.seedSelector allows to specify a label selector for seed clusters. Only if it matches the labels of a seed then it will be deployed to it. Please note that a seed selector can only be specified for secondary controllers (primary=false for all .spec.resources[]).\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/controllerregistration/","title":"Registering Extension Controllers","tags":[],"description":"","content":"Registering Extension Controllers Extensions are registered in the garden cluster via ControllerRegistration resources. Gardener is evaluating the registrations and creates ControllerInstallation resources which describe the request \u0026ldquo;please install this controller X to this seed Y\u0026rdquo;.\nSimilar to how CloudProfile or Seed resources get into the system, the Gardener administrator must deploy the ControllerRegistration resources (this does not happen automatically in any way - the administrator decides which extensions shall be enabled).\nThe specification mainly describes which of Gardener\u0026rsquo;s extension CRDs are managed, for example:\napiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationmetadata:name:os-coreosspec:resources:- kind:OperatingSystemConfigtype:coreosprimary:trueThis information tells Gardener that there is an extension controller that can handle OperatingSystemConfig resources of type coreos.\nAlso, it specifies that this controller is the primary one responsible for the lifecycle of the OperatingSystemConfig resource. Setting primary to false would allow to register additional, secondary controllers that may also watch/react on the OperatingSystemConfig/coreos resources, however, only the primary controller may change/update the main status of the extension object (that are used to \u0026ldquo;communicate\u0026rdquo; with the Gardenlet). Particularly, only the primary controller may set .status.lastOperation, .status.lastError, .status.observedGeneration, and .status.state. Secondary controllers may contribute to the .status.conditions[] if they like, of course.\nSecondary controllers might be helpful in scenarios where additional tasks need to be completed which are not part of the reconciliation logic of the primary controller but separated out into a dedicated extension.\n There must be exactly one primary controller for every registered kind/type combination. Also, please note that the primary field cannot be changed after creation of the ControllerRegistration.\nDeploying Extension Controllers Submitting above ControllerRegistration will create a ControllerInstallation resource:\napiVersion:core.gardener.cloud/v1beta1kind:ControllerInstallationmetadata:name:os-coreosspec:registrationRef:apiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationname:os-coreosseedRef:apiVersion:core.gardener.cloud/v1beta1kind:Seedname:aws-eu1This resource expresses that Gardener requires the os-coreos extension controller to run on the aws-eu1 seed cluster.\nThe Gardener Controller Manager does automatically determine which extension is required on which seed cluster and will only create ControllerInstallation objects for those. Also, it will automatically delete ControllerInstallations referencing extension controllers that are no longer required on a seed (e.g., because all shoots on it have been deleted). There are additional configuration options, please see this section.\nHow do extension controllers get deployed to seeds? After Gardener has written the ControllerInstallation resource some component must satisfy this request and start deploying the extension controller to the seed. Depending on the complexity of the controllers lifecycle management, configuration, etc. there are two possible scenarios:\nScenario 1: Deployed by Gardener In many cases the extension controllers are easy to deploy and configure. It is sufficient to simply create a Helm chart (standardized way of packaging software in the Kubernetes context) and deploy it together with some static configuration values. Gardener supports this scenario and allows to provide arbitrary deployment information in the ControllerRegistration resource\u0026rsquo;s .spec section:\n...spec:...deployment:type:helmproviderConfig:chart:H4sIFAAAAAAA/yk...values:foo:barIf .spec.deployment.type=helm then Gardener itself will take over the responsibility the deployment. It base64-decodes the provided Helm chart (.spec.deployment.providerConfig.chart) and deploys it with the provided static configuration (.spec.deployment.providerConfig.values). The chart and the values can be updated at any time - Gardener will recognize and re-trigger the deployment process.\nIn order to allow extensions to get information about the garden and the seed cluster Gardener does mix-in certain properties into the values (root level) of every deployed Helm chart:\ngardener:garden:identifier:\u0026lt;uuid-of-gardener-installation\u0026gt; seed:identifier:\u0026lt;seed-name\u0026gt; region: europespec:\u0026lt;complete-seed-spec\u0026gt;Extensions can use this information in their Helm chart in case they require knowledge about the garden and the seed environment. The list might be extended in the future.\n:information_source: Gardener uses the UUID of the garden Namespace object in the .gardener.garden.identifier property.\nScenario 2: Deployed by a (non-human) Kubernetes operator Some extension controllers might be more complex and require additional domain-specific knowledge wrt. lifecycle or configuration. In this case, we encourage to follow the Kubernetes operator pattern and deploy a dedicated operator for this extension into the garden cluster. The ControllerResource's .spec.deployment.type field would then not be helm, and no Helm chart or values need to be provided there. Instead, the operator itself knows how to deploy the extension into the seed. It must watch ControllerInstallation resources and act one those referencing a ControllerRegistration the operator is responsible for.\nIn order to let Gardener know that the extension controller is ready and running in the seed the ControllerInstallation's .status field supports two conditions: RegistrationValid and InstallationSuccessful - both must be provided by the responsible operator:\n...status:conditions:- lastTransitionTime:\u0026#34;2019-01-22T11:51:11Z\u0026#34;lastUpdateTime:\u0026#34;2019-01-22T11:51:11Z\u0026#34;message:Chartcouldberenderedsuccessfully.reason:RegistrationValidstatus:\u0026#34;True\u0026#34;type:Valid- lastTransitionTime:\u0026#34;2019-01-22T11:51:12Z\u0026#34;lastUpdateTime:\u0026#34;2019-01-22T11:51:12Z\u0026#34;message:Installationofnewresourcessucceeded.reason:InstallationSuccessfulstatus:\u0026#34;True\u0026#34;type:InstalledAdditionally, the .status field has a providerStatus section into which the operator can (optionally) put any arbitrary data associated with this installation.\nExtensions in the garden cluster itself The Shoot resource itself will contain some provider-specific data blobs. As a result, some extensions might also want to run in the garden cluster, e.g., to provide ValidatingWebhookConfigurations for validating the correctness of their provider-specific blobs:\napiVersion:core.gardener.cloud/v1beta1kind:Shootmetadata:name:johndoe-awsnamespace:garden-devspec:...cloud:type:awsregion:eu-west-1providerConfig:apiVersion:aws.cloud.gardener.cloud/v1alpha1kind:InfrastructureConfignetworks:vpc:# specify either \u0026#39;id\u0026#39; or \u0026#39;cidr\u0026#39;# id: vpc-123456cidr:10.250.0.0/16internal:- 10.250.112.0/22public:- 10.250.96.0/22workers:- 10.250.0.0/19zones:- eu-west-1a...In the above example, Gardener itself does not understand the AWS-specific provider configuration for the infrastructure. However, if this part of the Shoot resource should be validated then you should run an AWS-specific component in the garden cluster that registers a webhook. You can do it similarly if you want to default some fields of a resource (by using a MutatingWebhookConfiguration).\nAgain, similar to how Gardener is deployed to the garden cluster, these components must be deployed and managed by the Gardener administrator.\nExtension resource configurations The Extension resource allows injecting arbitrary steps into the shoot reconciliation flow that are unknown to Gardener. Hence, it is slightly special and allows further configuration when registering it:\napiVersion:core.gardener.cloud/v1beta1kind:ControllerRegistrationmetadata:name:extension-foospec:resources:- kind:Extensiontype:fooprimary:truegloballyEnabled:truereconcileTimeout:30sThe globallyEnabled=true option specifies that the Extension/foo object shall be created by default for all shoots (unless they opted out by setting .spec.extensions[].enabled=false in the Shoot spec).\nThe reconcileTimeout tells Gardener how long it should wait during its shoot reconciliation flow for the Extension/foo's reconciliation to finish.\nDeployment configuration options The .spec.deployment resource allows to configure a deployment policy. There are the following policies:\n OnDemand (default): Gardener will demand the deployment and deletion of the extension controller to/from seed clusters dynamically. It will automatically determine (based on other resources like Shoots) whether it is required and decide accordingly. Always: Gardener will demand the deployment of the extension controller to seed clusters independent of whether it is actually required or not. This might be helpful if you want to add a new component/controller to all seed clusters by default. Another use-case is to minimize the durations until extension controllers get deployed and ready in case you have highly fluctuating seed clusters. AlwaysExceptNoShoots: Similar to Always, but if the seed does not have any shoots then the extension is not being deployed. It will be deleted from a seed after the last shoot has been removed from it.  Also, the .spec.deployment.seedSelector allows to specify a label selector for seed clusters. Only if it matches the labels of a seed then it will be deployed to it. Please note that a seed selector can only be specified for secondary controllers (primary=false for all .spec.resources[]).\n"},{"uri":"https://gardener.cloud/documentation/contribute/10_code/40_repositories/","title":"Repositories","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.12.8/contribute/10_code/40_repositories/","title":"Repositories","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/v1.13.2/contribute/10_code/40_repositories/","title":"Repositories","tags":[],"description":"","content":""},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/dns_names/","title":"Request DNS Names","tags":[],"description":"Requesting DNS Names for Ingresses and Services in Shoot Clusters","content":"Request DNS Names in Shoot Clusters Introduction Within a shoot cluster, it is possible to request DNS records via the following resource types:\n Ingress Service DNSEntry  It is necessary that the Gardener installation your shoot cluster runs in is equipped with a shoot-dns-service extension. This extension uses the seed\u0026rsquo;s dns management infrastructure to maintain DNS names for shoot clusters. Please ask your Gardener operator if the extension is available in your environment.\nShoot Feature Gate In some Gardener setups the shoot-dns-service extension is not enabled globally and thus must be configured per shoot cluster. Please adapt the shoot specification by the configuration shown below to activate the extension individually.\nkind:Shoot...spec:extensions:- type:shoot-dns-service...DNS providers, domain scope Gardener can only manage DNS records on your behalf if you have proper DNS providers in place. Please consult this page for more information.\nRequest DNS records via Service/Ingress resources To request a DNS name for an Ingress or Service object in the shoot cluster it must be annotated with the DNS class garden and an annotation denoting the desired DNS names.\nFor a Service (it must have the type LoadBalancer) this looks like this:\napiVersion:v1kind:Servicemetadata:annotations:dns.gardener.cloud/class:gardendns.gardener.cloud/dnsnames:my.subdomain.for.shootsomain.cloudname:my-servicenamespace:defaultspec:ports:- port:80protocol:TCPtargetPort:80type:LoadBalancerThe dnsnames annotation accepts a comma-separated list of DNS names, if multiple names are required.\nFor an Ingress, the dns names are already declared in the specification. Nevertheless the dnsnames annotation must be present. Here a subset of the dns names of the ingress can be specified. If DNS names for all names are desired, the value all can be used.\nIf one of the accepted dns names is a direct subname of the shoot\u0026rsquo;s ingress domain, this is already handled by the standard wildcard entry for the ingress domain. Therefore this name should be excluded from the dnsnames list in the annotation. If only this dns name is configured in the ingress, no explicit dns entry is required, and the dns annotations should be omitted at all.\nRequest DNS records via DNSEntry resources apiVersion:dns.gardener.cloud/v1alpha1kind:DNSEntrymetadata:annotations:dns.gardener.cloud/class:gardenname:dnsnamespace:defaultspec:dnsName:\u0026#34;my.subdomain.for.shootsomain.cloud\u0026#34;ttl:600# txt records, either text or targets must be specified# text:# - foo-bartargets:# target records (CNAME or A records)- 8.8.8.8DNS record events The DNS controller publishes Kubernetes events for the resource which requested the DNS record (Ingress, Service, DNSEntry). These events reveal more information about the DNS requests being processed and are especially useful to check any kind of misconfiguration, e.g. requests for a domain you don\u0026rsquo;t own.\nEvents for a successfully created DNS record:\n$ kubectl -n default describe service my-service Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal dns-annotation 19s dns-controller-manager my.subdomain.for.shootsomain.cloud: dns entry is pending Normal dns-annotation 19s (x3 over 19s) dns-controller-manager my.subdomain.for.shootsomain.cloud: dns entry pending: waiting for dns reconciliation Normal dns-annotation 9s (x3 over 10s) dns-controller-manager my.subdomain.for.shootsomain.cloud: dns entry active Please note, events vanish after their retention period (usually 1h).\nDNSEntry status DNSEntry resources offer a .status sub-resource which can be used to check the current state of the object.\nStatus of a erroneous DNSEntry.\nstatus: message: No responsible provider found observedGeneration: 3 provider: remote state: Error "},{"uri":"https://gardener.cloud/v1.12.8/guides/administer_shoots/dns_names/","title":"Request DNS Names","tags":[],"description":"Requesting DNS Names for Ingresses and Services in Shoot Clusters","content":"Request DNS Names in Shoot Clusters Introduction Within a shoot cluster, it is possible to request DNS records via the following resource types:\n Ingress Service DNSEntry  It is necessary that the Gardener installation your shoot cluster runs in is equipped with a shoot-dns-service extension. This extension uses the seed\u0026rsquo;s dns management infrastructure to maintain DNS names for shoot clusters. Please ask your Gardener operator if the extension is available in your environment.\nShoot Feature Gate In some Gardener setups the shoot-dns-service extension is not enabled globally and thus must be configured per shoot cluster. Please adapt the shoot specification by the configuration shown below to activate the extension individually.\nkind:Shoot...spec:extensions:- type:shoot-dns-service...DNS providers, domain scope Gardener can only manage DNS records on your behalf if you have proper DNS providers in place. Please consult this page for more information.\nRequest DNS records via Service/Ingress resources To request a DNS name for an Ingress or Service object in the shoot cluster it must be annotated with the DNS class garden and an annotation denoting the desired DNS names.\nFor a Service (it must have the type LoadBalancer) this looks like this:\napiVersion:v1kind:Servicemetadata:annotations:dns.gardener.cloud/class:gardendns.gardener.cloud/dnsnames:my.subdomain.for.shootsomain.cloudname:my-servicenamespace:defaultspec:ports:- port:80protocol:TCPtargetPort:80type:LoadBalancerThe dnsnames annotation accepts a comma-separated list of DNS names, if multiple names are required.\nFor an Ingress, the dns names are already declared in the specification. Nevertheless the dnsnames annotation must be present. Here a subset of the dns names of the ingress can be specified. If DNS names for all names are desired, the value all can be used.\nIf one of the accepted dns names is a direct subname of the shoot\u0026rsquo;s ingress domain, this is already handled by the standard wildcard entry for the ingress domain. Therefore this name should be excluded from the dnsnames list in the annotation. If only this dns name is configured in the ingress, no explicit dns entry is required, and the dns annotations should be omitted at all.\nRequest DNS records via DNSEntry resources apiVersion:dns.gardener.cloud/v1alpha1kind:DNSEntrymetadata:annotations:dns.gardener.cloud/class:gardenname:dnsnamespace:defaultspec:dnsName:\u0026#34;my.subdomain.for.shootsomain.cloud\u0026#34;ttl:600# txt records, either text or targets must be specified# text:# - foo-bartargets:# target records (CNAME or A records)- 8.8.8.8DNS record events The DNS controller publishes Kubernetes events for the resource which requested the DNS record (Ingress, Service, DNSEntry). These events reveal more information about the DNS requests being processed and are especially useful to check any kind of misconfiguration, e.g. requests for a domain you don\u0026rsquo;t own.\nEvents for a successfully created DNS record:\n$ kubectl -n default describe service my-service Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal dns-annotation 19s dns-controller-manager my.subdomain.for.shootsomain.cloud: dns entry is pending Normal dns-annotation 19s (x3 over 19s) dns-controller-manager my.subdomain.for.shootsomain.cloud: dns entry pending: waiting for dns reconciliation Normal dns-annotation 9s (x3 over 10s) dns-controller-manager my.subdomain.for.shootsomain.cloud: dns entry active Please note, events vanish after their retention period (usually 1h).\nDNSEntry status DNSEntry resources offer a .status sub-resource which can be used to check the current state of the object.\nStatus of a erroneous DNSEntry.\nstatus: message: No responsible provider found observedGeneration: 3 provider: remote state: Error "},{"uri":"https://gardener.cloud/v1.13.2/guides/administer_shoots/dns_names/","title":"Request DNS Names","tags":[],"description":"Requesting DNS Names for Ingresses and Services in Shoot Clusters","content":"Request DNS Names in Shoot Clusters Introduction Within a shoot cluster, it is possible to request DNS records via the following resource types:\n Ingress Service DNSEntry  It is necessary that the Gardener installation your shoot cluster runs in is equipped with a shoot-dns-service extension. This extension uses the seed\u0026rsquo;s dns management infrastructure to maintain DNS names for shoot clusters. Please ask your Gardener operator if the extension is available in your environment.\nShoot Feature Gate In some Gardener setups the shoot-dns-service extension is not enabled globally and thus must be configured per shoot cluster. Please adapt the shoot specification by the configuration shown below to activate the extension individually.\nkind:Shoot...spec:extensions:- type:shoot-dns-service...DNS providers, domain scope Gardener can only manage DNS records on your behalf if you have proper DNS providers in place. Please consult this page for more information.\nRequest DNS records via Service/Ingress resources To request a DNS name for an Ingress or Service object in the shoot cluster it must be annotated with the DNS class garden and an annotation denoting the desired DNS names.\nFor a Service (it must have the type LoadBalancer) this looks like this:\napiVersion:v1kind:Servicemetadata:annotations:dns.gardener.cloud/class:gardendns.gardener.cloud/dnsnames:my.subdomain.for.shootsomain.cloudname:my-servicenamespace:defaultspec:ports:- port:80protocol:TCPtargetPort:80type:LoadBalancerThe dnsnames annotation accepts a comma-separated list of DNS names, if multiple names are required.\nFor an Ingress, the dns names are already declared in the specification. Nevertheless the dnsnames annotation must be present. Here a subset of the dns names of the ingress can be specified. If DNS names for all names are desired, the value all can be used.\nIf one of the accepted dns names is a direct subname of the shoot\u0026rsquo;s ingress domain, this is already handled by the standard wildcard entry for the ingress domain. Therefore this name should be excluded from the dnsnames list in the annotation. If only this dns name is configured in the ingress, no explicit dns entry is required, and the dns annotations should be omitted at all.\nRequest DNS records via DNSEntry resources apiVersion:dns.gardener.cloud/v1alpha1kind:DNSEntrymetadata:annotations:dns.gardener.cloud/class:gardenname:dnsnamespace:defaultspec:dnsName:\u0026#34;my.subdomain.for.shootsomain.cloud\u0026#34;ttl:600# txt records, either text or targets must be specified# text:# - foo-bartargets:# target records (CNAME or A records)- 8.8.8.8DNS record events The DNS controller publishes Kubernetes events for the resource which requested the DNS record (Ingress, Service, DNSEntry). These events reveal more information about the DNS requests being processed and are especially useful to check any kind of misconfiguration, e.g. requests for a domain you don\u0026rsquo;t own.\nEvents for a successfully created DNS record:\n$ kubectl -n default describe service my-service Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal dns-annotation 19s dns-controller-manager my.subdomain.for.shootsomain.cloud: dns entry is pending Normal dns-annotation 19s (x3 over 19s) dns-controller-manager my.subdomain.for.shootsomain.cloud: dns entry pending: waiting for dns reconciliation Normal dns-annotation 9s (x3 over 10s) dns-controller-manager my.subdomain.for.shootsomain.cloud: dns entry active Please note, events vanish after their retention period (usually 1h).\nDNSEntry status DNSEntry resources offer a .status sub-resource which can be used to check the current state of the object.\nStatus of a erroneous DNSEntry.\nstatus: message: No responsible provider found observedGeneration: 3 provider: remote state: Error "},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/request_cert/","title":"Request X.509 Certificates","tags":[],"description":"X.509 Certificates For TLS Communication","content":"Request X.509 Certificates Introduction Dealing with applications on Kubernetes which offer service endpoints (e.g. HTTP) may also require you to enable a secured communication via SSL/TLS. Gardener let\u0026rsquo;s you request a commonly trusted X.509 certificate for your application endpoint. Furthermore, Gardener takes care about the renewal process for your requested certificate.\nLet\u0026rsquo;s get the basics straight first. If this is too long for you, you can read below how to get certificates by\n Certificate Resources Ingress Service  Restrictions Service Scope This service enables users to request managed X.509 certificates with the help of ACME and Let\u0026rsquo;s Encrypt. It does not equip or manage DNS records for cluster assets like Services or Ingresses. Thus, you can obtain a valid certificate but your service might still not be resolvable or reachable due to missing DNS configuration. Please consult this page if your services require managed DNS records.\nSupported Domains Certificates may be obtained for any subdomain of your shoot\u0026rsquo;s domain (see .spec.dns.domain of your shoot resource) with the default issuer. For custom domains, please consult custom domains.\nCharacter Restrictions Due to the ACME protocol specification, at least one domain of the domains you request a certificate for must not exceed a character limit of 64 (CN - Common Name).\nFor example, the following request is invalid:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-invalidnamespace:defaultspec:commonName:morethan64characters.ingress.shoot.project.default-domain.gardener.cloudBut it is valid to request a certificate for this domain if you have at least one domain which does not exceed the mentioned limit:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-examplenamespace:defaultspec:commonName:short.ingress.shoot.project.default-domain.gardener.clouddnsNames:- morethan64characters.ingress.shoot.project.default-domain.gardener.cloudCertificate Resources Every X.509 certificate is represented by a Kubernetes custom resource certificate.cert.gardener.cloud in your cluster. A Certificate resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener\u0026rsquo;s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.\n Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.\n Certificates can either be requested by creating Certificate resources in the Kubernetes cluster or by configuring Ingress or Service (type LoadBalancer) resources. If the latter is used, a Certificate resource will automatically be created by Gardener\u0026rsquo;s certificate service.\nIf you\u0026rsquo;re interested in the current progress of your request, you\u0026rsquo;re advised to consult the Certificate's status subresource. You\u0026rsquo;ll also find error descriptions in the status in case the issuance failed.\nCertificate status example:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificate...status:commonName:short.ingress.shoot.project.default-domain.gardener.cloudexpirationDate:\u0026#34;2020-02-27T15:39:10Z\u0026#34;issuerRef:name:gardennamespace:shoot--foo--barlastPendingTimestamp:\u0026#34;2019-11-29T16:38:40Z\u0026#34;observedGeneration:11state:ReadyCustom Domains If you want to request certificates for domains other then any subdomain of shoot.spec.dns.domain, the following configuration is required:\nDNS provider In order to issue certificates for a custom domain you need to specify a DNS provider which is permitted to create DNS records for subdomains of your requested domain in the certificate. For example, if you request a certificate for host.example.com your DNS provider must be capable of managing subdomains of host.example.com.\nDNS providers are specified in the shoot manifest:\nkind:Shoot...spec:dns:providers:- type:aws-route53# consult the DNS provisioning controllers group (dnscontrollers) in https://github.com/gardener/external-dns-management#using-the-dns-controller-manager for possible valuessecretName:provider-example-com# contains credentials for service account, see any 20-secret-\u0026lt;provider\u0026gt;-credentials.yaml in https://github.com/gardener/external-dns-management/tree/master/examplesThe secret referenced by secretName can also be conveniently created via the Gardener dashboard.\nIssuer Another prerequisite to request certificates for custom domains is a dedicated issuer.\nkind:Shoot...spec:extensions:- type:shoot-cert-serviceproviderConfig:apiVersion:service.cert.extensions.gardener.cloud/v1alpha1kind:CertConfigissuers:- email:your-email@example.comname:custom-issuer# issuer name must be specified in every custom issuer request, must not be \u0026#34;garden\u0026#34;server:\u0026#39;https://acme-v02.api.letsencrypt.org/directory\u0026#39;Examples Request a certificate via Certificate apiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-examplenamespace:defaultspec:commonName:short.ingress.shoot.project.default-domain.gardener.clouddnsNames:- morethan64characters.ingress.shoot.project.default-domain.gardener.cloudsecretRef:name:cert-examplenamespace:default# issuerRef:# name: custom-issuer   Path Description     spec.commonName (required) Specifies for which domain the certificate request will be created. This entry must comply with the 64 character limit.   spec.dnsName Additional domains the certificate should be valid for. Entries in this list can be longer than 64 characters.   spec.secretRef Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the X.509 certificate has been issued.   spec.issuerRef Specifies the issuer you want to use. Only necessary if you request certificates for custom domains.    Request a wildcard certificate via Certificate apiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-wildcardnamespace:defaultspec:commonName:\u0026#39;*.ingress.shoot.project.default-domain.gardener.cloud\u0026#39;secretRef:name:cert-wildcardnamespace:default# issuerRef:# name: custom-issuer   Path Description     spec.commonName (required) Specifies for which domain the certificate request will be created. This entry must comply with the 64 character limit.   spec.secretRef Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the X.509 certificate has been issued.   spec.issuerRef Specifies the issuer you want to use. Only necessary if you request certificates for custom domains.    Request a certificate via Ingress apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:vuejs-ingressannotations:cert.gardener.cloud/purpose:managed# cert.gardener.cloud/issuer: custom-issuerspec:tls:# Must not exceed 64 characters.- hosts:- short.ingress.shoot.project.default-domain.gardener.cloud- morethan64characters.ingress.shoot.project.default-domain.gardener.cloud# Certificate and private key reside in this secret.secretName:testsecret-tlsrules:- host:morethan64characters.ingress.shoot.project.default-domain.gardener.cloudhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080   Path Description     metadata.annotations Annotations should have cert.gardener.cloud/purpose: managed to activate the certificate service on this resource. cert.gardener.cloud/issuer: \u0026lt;name\u0026gt; is optional and may be specified if the certificate is request for a custom domains.   spec.tls[].hosts Specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.   spec.tls[].secretName Specifies the secret which contains the certificate/key pair to be used by this Ingress. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued. Once configured, you\u0026rsquo;re not advised to change the name while the Ingress is still managed by the certificate service.    Request a wildcard certificate via Ingress apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:vuejs-ingressannotations:cert.gardener.cloud/purpose:managed# cert.gardener.cloud/issuer: custom-issuerspec:tls:# Must not exceed 64 characters.- hosts:- \u0026#34;*.ingress.shoot.project.default-domain.gardener.cloud\u0026#34;# Certificate and private key reside in this secret.secretName:testsecret-tlsrules:- host:morethan64characters.ingress.shoot.project.default-domain.gardener.cloudhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080 Domains must not overlap when requesting a wildcard certificate. For example, requests for *.example.com must not contain foo.example.com at the same time.\n    Path Description     metadata.annotations Annotations should have cert.gardener.cloud/purpose: managed to activate the certificate service on this resource. cert.gardener.cloud/issuer: \u0026lt;name\u0026gt; is optional and may be specified if the certificate is request for a custom domains.   spec.tls[].hosts Specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.   spec.tls[].secretName Specifies the secret which contains the certificate/key pair to be used by this Ingress. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued. Once configured, you\u0026rsquo;re not advised to change the name while the Ingress is still managed by the certificate service.    Request a certificate via Service apiVersion:v1kind:Servicemetadata:annotations:cert.gardener.cloud/secretname:test-service-secret# cert.gardener.cloud/issuer: custom-issuerdns.gardener.cloud/dnsnames:\u0026#34;service.shoot.project.default-domain.gardener.cloud, morethan64characters.svc.shoot.project.default-domain.gardener.cloud\u0026#34;dns.gardener.cloud/ttl:\u0026#34;600\u0026#34;name:test-servicenamespace:defaultspec:ports:- name:httpport:80protocol:TCPtargetPort:8080type:LoadBalancer   Path Description     metadata.annotations[cert.gardener.cloud/secretname] Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued.   metadata.annotations[cert.gardener.cloud/issuer] Optional and may be specified if the certificate is request for a custom domains.   metadata.annotations[dns.gardener.cloud/dnsnames] Specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.    Request a wildcard certificate via Service apiVersion:v1kind:Servicemetadata:annotations:cert.gardener.cloud/secretname:test-service-secret# cert.gardener.cloud/issuer: custom-issuerdns.gardener.cloud/dnsnames:\u0026#34;*.service.shoot.project.default-domain.gardener.cloud\u0026#34;dns.gardener.cloud/ttl:\u0026#34;600\u0026#34;name:test-servicenamespace:defaultspec:ports:- name:httpport:80protocol:TCPtargetPort:8080type:LoadBalancer Domains must not overlap when requesting a wildcard certificate. For example, requests for *.example.com must not contain foo.example.com at the same time.\n    Path Description     metadata.annotations[cert.gardener.cloud/secretname] Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued.   metadata.annotations[cert.gardener.cloud/issuer] Optional and may be specified if the certificate is request for a custom domains.   metadata.annotations[dns.gardener.cloud/dnsnames] Specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.     #body-inner blockquote { border: 0; padding: 10px; margin-top: 40px; margin-bottom: 40px; border-radius: 4px; background-color: rgba(0,0,0,0.05); box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23); position:relative; padding-left:60px; } #body-inner blockquote:before { content: \"!\"; font-weight: bold; position: absolute; top: 0; bottom: 0; left: 0; background-color: #00a273; color: white; vertical-align: middle; margin: auto; width: 36px; font-size: 30px; text-align: center; }  "},{"uri":"https://gardener.cloud/v1.12.8/guides/administer_shoots/request_cert/","title":"Request X.509 Certificates","tags":[],"description":"X.509 Certificates For TLS Communication","content":"Request X.509 Certificates Introduction Dealing with applications on Kubernetes which offer service endpoints (e.g. HTTP) may also require you to enable a secured communication via SSL/TLS. Gardener let\u0026rsquo;s you request a commonly trusted X.509 certificate for your application endpoint. Furthermore, Gardener takes care about the renewal process for your requested certificate.\nLet\u0026rsquo;s get the basics straight first. If this is too long for you, you can read below how to get certificates by\n Certificate Resources Ingress Service  Restrictions Service Scope This service enables users to request managed X.509 certificates with the help of ACME and Let\u0026rsquo;s Encrypt. It does not equip or manage DNS records for cluster assets like Services or Ingresses. Thus, you can obtain a valid certificate but your service might still not be resolvable or reachable due to missing DNS configuration. Please consult this page if your services require managed DNS records.\nSupported Domains Certificates may be obtained for any subdomain of your shoot\u0026rsquo;s domain (see .spec.dns.domain of your shoot resource) with the default issuer. For custom domains, please consult custom domains.\nCharacter Restrictions Due to the ACME protocol specification, at least one domain of the domains you request a certificate for must not exceed a character limit of 64 (CN - Common Name).\nFor example, the following request is invalid:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-invalidnamespace:defaultspec:commonName:morethan64characters.ingress.shoot.project.default-domain.gardener.cloudBut it is valid to request a certificate for this domain if you have at least one domain which does not exceed the mentioned limit:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-examplenamespace:defaultspec:commonName:short.ingress.shoot.project.default-domain.gardener.clouddnsNames:- morethan64characters.ingress.shoot.project.default-domain.gardener.cloudCertificate Resources Every X.509 certificate is represented by a Kubernetes custom resource certificate.cert.gardener.cloud in your cluster. A Certificate resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener\u0026rsquo;s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.\n Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.\n Certificates can either be requested by creating Certificate resources in the Kubernetes cluster or by configuring Ingress or Service (type LoadBalancer) resources. If the latter is used, a Certificate resource will automatically be created by Gardener\u0026rsquo;s certificate service.\nIf you\u0026rsquo;re interested in the current progress of your request, you\u0026rsquo;re advised to consult the Certificate's status subresource. You\u0026rsquo;ll also find error descriptions in the status in case the issuance failed.\nCertificate status example:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificate...status:commonName:short.ingress.shoot.project.default-domain.gardener.cloudexpirationDate:\u0026#34;2020-02-27T15:39:10Z\u0026#34;issuerRef:name:gardennamespace:shoot--foo--barlastPendingTimestamp:\u0026#34;2019-11-29T16:38:40Z\u0026#34;observedGeneration:11state:ReadyCustom Domains If you want to request certificates for domains other then any subdomain of shoot.spec.dns.domain, the following configuration is required:\nDNS provider In order to issue certificates for a custom domain you need to specify a DNS provider which is permitted to create DNS records for subdomains of your requested domain in the certificate. For example, if you request a certificate for host.example.com your DNS provider must be capable of managing subdomains of host.example.com.\nDNS providers are specified in the shoot manifest:\nkind:Shoot...spec:dns:providers:- type:aws-route53# consult the DNS provisioning controllers group (dnscontrollers) in https://github.com/gardener/external-dns-management#using-the-dns-controller-manager for possible valuessecretName:provider-example-com# contains credentials for service account, see any 20-secret-\u0026lt;provider\u0026gt;-credentials.yaml in https://github.com/gardener/external-dns-management/tree/master/examplesThe secret referenced by secretName can also be conveniently created via the Gardener dashboard.\nIssuer Another prerequisite to request certificates for custom domains is a dedicated issuer.\nkind:Shoot...spec:extensions:- type:shoot-cert-serviceproviderConfig:apiVersion:service.cert.extensions.gardener.cloud/v1alpha1kind:CertConfigissuers:- email:your-email@example.comname:custom-issuer# issuer name must be specified in every custom issuer request, must not be \u0026#34;garden\u0026#34;server:\u0026#39;https://acme-v02.api.letsencrypt.org/directory\u0026#39;Examples Request a certificate via Certificate apiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-examplenamespace:defaultspec:commonName:short.ingress.shoot.project.default-domain.gardener.clouddnsNames:- morethan64characters.ingress.shoot.project.default-domain.gardener.cloudsecretRef:name:cert-examplenamespace:default# issuerRef:# name: custom-issuer   Path Description     spec.commonName (required) Specifies for which domain the certificate request will be created. This entry must comply with the 64 character limit.   spec.dnsName Additional domains the certificate should be valid for. Entries in this list can be longer than 64 characters.   spec.secretRef Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the X.509 certificate has been issued.   spec.issuerRef Specifies the issuer you want to use. Only necessary if you request certificates for custom domains.    Request a wildcard certificate via Certificate apiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-wildcardnamespace:defaultspec:commonName:\u0026#39;*.ingress.shoot.project.default-domain.gardener.cloud\u0026#39;secretRef:name:cert-wildcardnamespace:default# issuerRef:# name: custom-issuer   Path Description     spec.commonName (required) Specifies for which domain the certificate request will be created. This entry must comply with the 64 character limit.   spec.secretRef Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the X.509 certificate has been issued.   spec.issuerRef Specifies the issuer you want to use. Only necessary if you request certificates for custom domains.    Request a certificate via Ingress apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:vuejs-ingressannotations:cert.gardener.cloud/purpose:managed# cert.gardener.cloud/issuer: custom-issuerspec:tls:# Must not exceed 64 characters.- hosts:- short.ingress.shoot.project.default-domain.gardener.cloud- morethan64characters.ingress.shoot.project.default-domain.gardener.cloud# Certificate and private key reside in this secret.secretName:testsecret-tlsrules:- host:morethan64characters.ingress.shoot.project.default-domain.gardener.cloudhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080   Path Description     metadata.annotations Annotations should have cert.gardener.cloud/purpose: managed to activate the certificate service on this resource. cert.gardener.cloud/issuer: \u0026lt;name\u0026gt; is optional and may be specified if the certificate is request for a custom domains.   spec.tls[].hosts Specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.   spec.tls[].secretName Specifies the secret which contains the certificate/key pair to be used by this Ingress. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued. Once configured, you\u0026rsquo;re not advised to change the name while the Ingress is still managed by the certificate service.    Request a wildcard certificate via Ingress apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:vuejs-ingressannotations:cert.gardener.cloud/purpose:managed# cert.gardener.cloud/issuer: custom-issuerspec:tls:# Must not exceed 64 characters.- hosts:- \u0026#34;*.ingress.shoot.project.default-domain.gardener.cloud\u0026#34;# Certificate and private key reside in this secret.secretName:testsecret-tlsrules:- host:morethan64characters.ingress.shoot.project.default-domain.gardener.cloudhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080 Domains must not overlap when requesting a wildcard certificate. For example, requests for *.example.com must not contain foo.example.com at the same time.\n    Path Description     metadata.annotations Annotations should have cert.gardener.cloud/purpose: managed to activate the certificate service on this resource. cert.gardener.cloud/issuer: \u0026lt;name\u0026gt; is optional and may be specified if the certificate is request for a custom domains.   spec.tls[].hosts Specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.   spec.tls[].secretName Specifies the secret which contains the certificate/key pair to be used by this Ingress. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued. Once configured, you\u0026rsquo;re not advised to change the name while the Ingress is still managed by the certificate service.    Request a certificate via Service apiVersion:v1kind:Servicemetadata:annotations:cert.gardener.cloud/secretname:test-service-secret# cert.gardener.cloud/issuer: custom-issuerdns.gardener.cloud/dnsnames:\u0026#34;service.shoot.project.default-domain.gardener.cloud, morethan64characters.svc.shoot.project.default-domain.gardener.cloud\u0026#34;dns.gardener.cloud/ttl:\u0026#34;600\u0026#34;name:test-servicenamespace:defaultspec:ports:- name:httpport:80protocol:TCPtargetPort:8080type:LoadBalancer   Path Description     metadata.annotations[cert.gardener.cloud/secretname] Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued.   metadata.annotations[cert.gardener.cloud/issuer] Optional and may be specified if the certificate is request for a custom domains.   metadata.annotations[dns.gardener.cloud/dnsnames] Specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.    Request a wildcard certificate via Service apiVersion:v1kind:Servicemetadata:annotations:cert.gardener.cloud/secretname:test-service-secret# cert.gardener.cloud/issuer: custom-issuerdns.gardener.cloud/dnsnames:\u0026#34;*.service.shoot.project.default-domain.gardener.cloud\u0026#34;dns.gardener.cloud/ttl:\u0026#34;600\u0026#34;name:test-servicenamespace:defaultspec:ports:- name:httpport:80protocol:TCPtargetPort:8080type:LoadBalancer Domains must not overlap when requesting a wildcard certificate. For example, requests for *.example.com must not contain foo.example.com at the same time.\n    Path Description     metadata.annotations[cert.gardener.cloud/secretname] Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued.   metadata.annotations[cert.gardener.cloud/issuer] Optional and may be specified if the certificate is request for a custom domains.   metadata.annotations[dns.gardener.cloud/dnsnames] Specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.     #body-inner blockquote { border: 0; padding: 10px; margin-top: 40px; margin-bottom: 40px; border-radius: 4px; background-color: rgba(0,0,0,0.05); box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23); position:relative; padding-left:60px; } #body-inner blockquote:before { content: \"!\"; font-weight: bold; position: absolute; top: 0; bottom: 0; left: 0; background-color: #00a273; color: white; vertical-align: middle; margin: auto; width: 36px; font-size: 30px; text-align: center; }  "},{"uri":"https://gardener.cloud/v1.13.2/guides/administer_shoots/request_cert/","title":"Request X.509 Certificates","tags":[],"description":"X.509 Certificates For TLS Communication","content":"Request X.509 Certificates Introduction Dealing with applications on Kubernetes which offer service endpoints (e.g. HTTP) may also require you to enable a secured communication via SSL/TLS. Gardener let\u0026rsquo;s you request a commonly trusted X.509 certificate for your application endpoint. Furthermore, Gardener takes care about the renewal process for your requested certificate.\nLet\u0026rsquo;s get the basics straight first. If this is too long for you, you can read below how to get certificates by\n Certificate Resources Ingress Service  Restrictions Service Scope This service enables users to request managed X.509 certificates with the help of ACME and Let\u0026rsquo;s Encrypt. It does not equip or manage DNS records for cluster assets like Services or Ingresses. Thus, you can obtain a valid certificate but your service might still not be resolvable or reachable due to missing DNS configuration. Please consult this page if your services require managed DNS records.\nSupported Domains Certificates may be obtained for any subdomain of your shoot\u0026rsquo;s domain (see .spec.dns.domain of your shoot resource) with the default issuer. For custom domains, please consult custom domains.\nCharacter Restrictions Due to the ACME protocol specification, at least one domain of the domains you request a certificate for must not exceed a character limit of 64 (CN - Common Name).\nFor example, the following request is invalid:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-invalidnamespace:defaultspec:commonName:morethan64characters.ingress.shoot.project.default-domain.gardener.cloudBut it is valid to request a certificate for this domain if you have at least one domain which does not exceed the mentioned limit:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-examplenamespace:defaultspec:commonName:short.ingress.shoot.project.default-domain.gardener.clouddnsNames:- morethan64characters.ingress.shoot.project.default-domain.gardener.cloudCertificate Resources Every X.509 certificate is represented by a Kubernetes custom resource certificate.cert.gardener.cloud in your cluster. A Certificate resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener\u0026rsquo;s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.\n Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.\n Certificates can either be requested by creating Certificate resources in the Kubernetes cluster or by configuring Ingress or Service (type LoadBalancer) resources. If the latter is used, a Certificate resource will automatically be created by Gardener\u0026rsquo;s certificate service.\nIf you\u0026rsquo;re interested in the current progress of your request, you\u0026rsquo;re advised to consult the Certificate's status subresource. You\u0026rsquo;ll also find error descriptions in the status in case the issuance failed.\nCertificate status example:\napiVersion:cert.gardener.cloud/v1alpha1kind:Certificate...status:commonName:short.ingress.shoot.project.default-domain.gardener.cloudexpirationDate:\u0026#34;2020-02-27T15:39:10Z\u0026#34;issuerRef:name:gardennamespace:shoot--foo--barlastPendingTimestamp:\u0026#34;2019-11-29T16:38:40Z\u0026#34;observedGeneration:11state:ReadyCustom Domains If you want to request certificates for domains other then any subdomain of shoot.spec.dns.domain, the following configuration is required:\nDNS provider In order to issue certificates for a custom domain you need to specify a DNS provider which is permitted to create DNS records for subdomains of your requested domain in the certificate. For example, if you request a certificate for host.example.com your DNS provider must be capable of managing subdomains of host.example.com.\nDNS providers are specified in the shoot manifest:\nkind:Shoot...spec:dns:providers:- type:aws-route53# consult the DNS provisioning controllers group (dnscontrollers) in https://github.com/gardener/external-dns-management#using-the-dns-controller-manager for possible valuessecretName:provider-example-com# contains credentials for service account, see any 20-secret-\u0026lt;provider\u0026gt;-credentials.yaml in https://github.com/gardener/external-dns-management/tree/master/examplesThe secret referenced by secretName can also be conveniently created via the Gardener dashboard.\nIssuer Another prerequisite to request certificates for custom domains is a dedicated issuer.\nkind:Shoot...spec:extensions:- type:shoot-cert-serviceproviderConfig:apiVersion:service.cert.extensions.gardener.cloud/v1alpha1kind:CertConfigissuers:- email:your-email@example.comname:custom-issuer# issuer name must be specified in every custom issuer request, must not be \u0026#34;garden\u0026#34;server:\u0026#39;https://acme-v02.api.letsencrypt.org/directory\u0026#39;Examples Request a certificate via Certificate apiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-examplenamespace:defaultspec:commonName:short.ingress.shoot.project.default-domain.gardener.clouddnsNames:- morethan64characters.ingress.shoot.project.default-domain.gardener.cloudsecretRef:name:cert-examplenamespace:default# issuerRef:# name: custom-issuer   Path Description     spec.commonName (required) Specifies for which domain the certificate request will be created. This entry must comply with the 64 character limit.   spec.dnsName Additional domains the certificate should be valid for. Entries in this list can be longer than 64 characters.   spec.secretRef Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the X.509 certificate has been issued.   spec.issuerRef Specifies the issuer you want to use. Only necessary if you request certificates for custom domains.    Request a wildcard certificate via Certificate apiVersion:cert.gardener.cloud/v1alpha1kind:Certificatemetadata:name:cert-wildcardnamespace:defaultspec:commonName:\u0026#39;*.ingress.shoot.project.default-domain.gardener.cloud\u0026#39;secretRef:name:cert-wildcardnamespace:default# issuerRef:# name: custom-issuer   Path Description     spec.commonName (required) Specifies for which domain the certificate request will be created. This entry must comply with the 64 character limit.   spec.secretRef Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the X.509 certificate has been issued.   spec.issuerRef Specifies the issuer you want to use. Only necessary if you request certificates for custom domains.    Request a certificate via Ingress apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:vuejs-ingressannotations:cert.gardener.cloud/purpose:managed# cert.gardener.cloud/issuer: custom-issuerspec:tls:# Must not exceed 64 characters.- hosts:- short.ingress.shoot.project.default-domain.gardener.cloud- morethan64characters.ingress.shoot.project.default-domain.gardener.cloud# Certificate and private key reside in this secret.secretName:testsecret-tlsrules:- host:morethan64characters.ingress.shoot.project.default-domain.gardener.cloudhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080   Path Description     metadata.annotations Annotations should have cert.gardener.cloud/purpose: managed to activate the certificate service on this resource. cert.gardener.cloud/issuer: \u0026lt;name\u0026gt; is optional and may be specified if the certificate is request for a custom domains.   spec.tls[].hosts Specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.   spec.tls[].secretName Specifies the secret which contains the certificate/key pair to be used by this Ingress. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued. Once configured, you\u0026rsquo;re not advised to change the name while the Ingress is still managed by the certificate service.    Request a wildcard certificate via Ingress apiVersion:networking.k8s.io/v1beta1kind:Ingressmetadata:name:vuejs-ingressannotations:cert.gardener.cloud/purpose:managed# cert.gardener.cloud/issuer: custom-issuerspec:tls:# Must not exceed 64 characters.- hosts:- \u0026#34;*.ingress.shoot.project.default-domain.gardener.cloud\u0026#34;# Certificate and private key reside in this secret.secretName:testsecret-tlsrules:- host:morethan64characters.ingress.shoot.project.default-domain.gardener.cloudhttp:paths:- backend:serviceName:vuejs-svcservicePort:8080 Domains must not overlap when requesting a wildcard certificate. For example, requests for *.example.com must not contain foo.example.com at the same time.\n    Path Description     metadata.annotations Annotations should have cert.gardener.cloud/purpose: managed to activate the certificate service on this resource. cert.gardener.cloud/issuer: \u0026lt;name\u0026gt; is optional and may be specified if the certificate is request for a custom domains.   spec.tls[].hosts Specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.   spec.tls[].secretName Specifies the secret which contains the certificate/key pair to be used by this Ingress. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued. Once configured, you\u0026rsquo;re not advised to change the name while the Ingress is still managed by the certificate service.    Request a certificate via Service apiVersion:v1kind:Servicemetadata:annotations:cert.gardener.cloud/secretname:test-service-secret# cert.gardener.cloud/issuer: custom-issuerdns.gardener.cloud/dnsnames:\u0026#34;service.shoot.project.default-domain.gardener.cloud, morethan64characters.svc.shoot.project.default-domain.gardener.cloud\u0026#34;dns.gardener.cloud/ttl:\u0026#34;600\u0026#34;name:test-servicenamespace:defaultspec:ports:- name:httpport:80protocol:TCPtargetPort:8080type:LoadBalancer   Path Description     metadata.annotations[cert.gardener.cloud/secretname] Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued.   metadata.annotations[cert.gardener.cloud/issuer] Optional and may be specified if the certificate is request for a custom domains.   metadata.annotations[dns.gardener.cloud/dnsnames] Specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.    Request a wildcard certificate via Service apiVersion:v1kind:Servicemetadata:annotations:cert.gardener.cloud/secretname:test-service-secret# cert.gardener.cloud/issuer: custom-issuerdns.gardener.cloud/dnsnames:\u0026#34;*.service.shoot.project.default-domain.gardener.cloud\u0026#34;dns.gardener.cloud/ttl:\u0026#34;600\u0026#34;name:test-servicenamespace:defaultspec:ports:- name:httpport:80protocol:TCPtargetPort:8080type:LoadBalancer Domains must not overlap when requesting a wildcard certificate. For example, requests for *.example.com must not contain foo.example.com at the same time.\n    Path Description     metadata.annotations[cert.gardener.cloud/secretname] Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it\u0026rsquo;ll be created automatically as soon as the certificate has been issued.   metadata.annotations[cert.gardener.cloud/issuer] Optional and may be specified if the certificate is request for a custom domains.   metadata.annotations[dns.gardener.cloud/dnsnames] Specifies for which domains the certificate request will be created. The first entry is always taken to fill the Common Name field and must therefore comply with the 64 character limit.     #body-inner blockquote { border: 0; padding: 10px; margin-top: 40px; margin-bottom: 40px; border-radius: 4px; background-color: rgba(0,0,0,0.05); box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23); position:relative; padding-left:60px; } #body-inner blockquote:before { content: \"!\"; font-weight: bold; position: absolute; top: 0; bottom: 0; left: 0; background-color: #00a273; color: white; vertical-align: middle; margin: auto; width: 36px; font-size: 30px; text-align: center; }  "},{"uri":"https://gardener.cloud/documentation/contribute/10_code/12-security_guide/","title":"Security Release Process","tags":[],"description":"","content":"Gardener Security Release Process Gardener is a growing community of volunteers and users. The Gardener community has adopted this security disclosure and response policy to ensure we responsibly handle critical issues.\nGardener Security Team Security vulnerabilities should be handled quickly and sometimes privately. The primary goal of this process is to reduce the total time users are vulnerable to publicly known exploits. The Gardener Security Team is responsible for organizing the entire response including internal communication and external disclosure but will need help from relevant developers and release managers to successfully run this process. The initial Gardener Security Team will consist of the following volunteers:\n Olaf Beier, (@olafbeier) Vasu Chandrasekhara, (@vasu1124) Alban Crequy, (@alban) Norbert Hamann, (@norberthamann) Claudia Hlters, (@hoeltcl) Oliver Kling, (@oliverkling) Vedran Lerenc, (@vlerenc) Dirk Marwinski, (@marwinski) Michael Schubert, (@schu) Matthias Sohn, (@msohn) Frederik Thormaehlen, (@ThormaehlenFred) Christian Cwienk (@ccwienk)  Disclosures Private Disclosure Processes The Gardener community asks that all suspected vulnerabilities be privately and responsibly disclosed. If you\u0026rsquo;ve found a vulnerability or a potential vulnerability in Gardener please let us know by writing an e-mail to secure@sap.com. We\u0026rsquo;ll send a confirmation e-mail to acknowledge your report, and we\u0026rsquo;ll send an additional e-mail when we\u0026rsquo;ve identified the issue positively or negatively.\nPublic Disclosure Processes If you know of a publicly disclosed vulnerability please IMMEDIATELY e-mail to secure@sap.com to inform the Gardener Security Team about the vulnerability so they may start the patch, release, and communication process.\nIf possible the Gardener Security Team will ask the person making the public report if the issue can be handled via a private disclosure process (for example if the full exploit details have not yet been published). If the reporter denies the request for private disclosure, the Gardener Security Team will move swiftly with the fix and release process. In extreme cases GitHub can be asked to delete the issue but this generally isn\u0026rsquo;t necessary and is unlikely to make a public disclosure less damaging.\nPatch, Release, and Public Communication For each vulnerability a member of the Gardener Security Team will volunteer to lead coordination with the \u0026ldquo;Fix Team\u0026rdquo; and is responsible for sending disclosure e-mails to the rest of the community. This lead will be referred to as the \u0026ldquo;Fix Lead.\u0026rdquo; The role of the Fix Lead should rotate round-robin across the Gardener Security Team. Note that given the current size of the Gardener community it is likely that the Gardener Security Team is the same as the \u0026ldquo;Fix team.\u0026rdquo; (I.e., all maintainers). The Gardener Security Team may decide to bring in additional contributors for added expertise depending on the area of the code that contains the vulnerability. All of the time lines below are suggestions and assume a private disclosure. The Fix Lead drives the schedule using his best judgment based on severity and development time. If the Fix Lead is dealing with a public disclosure all time lines become ASAP (assuming the vulnerability has a CVSS score \u0026gt;= 7; see below). If the fix relies on another upstream project\u0026rsquo;s disclosure time line, that will adjust the process as well. We will work with the upstream project to fit their time line and best protect our users.\nFix Team Organization The Fix Lead will work quickly to identify relevant engineers from the affected projects and packages and CC those engineers into the disclosure thread. These selected developers are the Fix Team. The Fix Lead will give the Fix Team access to a private security repository to develop the fix.\nFix Development Process The Fix Lead and the Fix Team will create a CVSS using the CVSS Calculator. The Fix Lead makes the final call on the calculated CVSS; it is better to move quickly than make the CVSS perfect. The Fix Team will notify the Fix Lead that work on the fix branch is complete once there are LGTMs on all commits in the private repository from one or more maintainers. If the CVSS score is under 7.0 (a medium severity score) the Fix Team can decide to slow the release process down in the face of holidays, developer bandwidth, etc. These decisions must be discussed on the private Gardener Security mailing list.\nFix Disclosure Process With the fix development underway, the Fix Lead needs to come up with an overall communication plan for the wider community. This Disclosure process should begin after the Fix Team has developed a Fix or mitigation so that a realistic time line can be communicated to users. The Fix Lead will inform the Gardener mailing list that a security vulnerability has been disclosed and that a fix will be made available in the future on a certain release date. The Fix Lead will include any mitigating steps users can take until a fix is available. The communication to Gardener users should be actionable. They should know when to block time to apply patches, understand exact mitigation steps, etc.\nFix Release Day The Release Managers will ensure all the binaries are built, publicly available, and functional before the Release Date. The Release Managers will create a new patch release branch from the latest patch release tag + the fix from the security branch. As a practical example if v0.12.0 is the latest patch release in gardener.git a new branch will be created called v0.12.1 which includes only patches required to fix the issue. The Fix Lead will cherry-pick the patches onto the master branch and all relevant release branches. The Fix Team will LGTM and merge. The Release Managers will merge these PRs as quickly as possible. Changes shouldn\u0026rsquo;t be made to the commits even for a typo in the CHANGELOG as this will change the git sha of the already built and commits leading to confusion and potentially conflicts as the fix is cherry-picked around branches. The Fix Lead will request a CVE from the SAP Product Security Response Team via email to cna@sap.com with all the relevant information (description, potential impact, affected version, fixed version, CVSS v3 base score and supporting documentation for the CVSS score) for every vulnerability. The Fix Lead will inform the Gardener mailing list and announce the new releases, the CVE number (if available), the location of the binaries, and the relevant merged PRs to get wide distribution and user action.\nAs much as possible this e-mail should be actionable and include links how to apply the fix to users environments; this can include links to external distributor documentation. The recommended target time is 4pm UTC on a non-Friday weekday. This means the announcement will be seen morning Pacific, early evening Europe, and late evening Asia. The Fix Lead will remove the Fix Team from the private security repository.\nRetrospective These steps should be completed after the Release Date. The retrospective process should be blameless.\nThe Fix Lead will send a retrospective of the process to the Gardener mailing list including details on everyone involved, the time line of the process, links to relevant PRs that introduced the issue, if relevant, and any critiques of the response and release process. The Release Managers and Fix Team are also encouraged to send their own feedback on the process to the Gardener mailing list. Honest critique is the only way we are going to get good at this as a community.\nCommunication Channel The private or public disclosure process should be triggered exclusively by writing an e-mail to secure@sap.com.\nGardener security announcements will be communicated by the Fix Lead sending an e-mail to the Gardener mailing list (reachable via gardener@googlegroups.com) as well as posting a link in the Gardener Slack channel. Public discussions about Gardener security announcements and retrospectives, will primarily happen in the Gardener mailing list. Thus Gardener community members who are interested in participating in discussions related to the Gardener Security Release Process are encouraged to join the Gardener mailing list (how to find and join a group)\nThe members of the Gardener Security Team are subscribed to the private Gardener Security mailing list (reachable via gardener-security@googlegroups.com).\n"},{"uri":"https://gardener.cloud/v1.12.8/contribute/10_code/12-security_guide/","title":"Security Release Process","tags":[],"description":"","content":"Gardener Security Release Process Gardener is a growing community of volunteers and users. The Gardener community has adopted this security disclosure and response policy to ensure we responsibly handle critical issues.\nGardener Security Team Security vulnerabilities should be handled quickly and sometimes privately. The primary goal of this process is to reduce the total time users are vulnerable to publicly known exploits. The Gardener Security Team is responsible for organizing the entire response including internal communication and external disclosure but will need help from relevant developers and release managers to successfully run this process. The initial Gardener Security Team will consist of the following volunteers:\n Olaf Beier, (@olafbeier) Vasu Chandrasekhara, (@vasu1124) Alban Crequy, (@alban) Norbert Hamann, (@norberthamann) Claudia Hlters, (@hoeltcl) Oliver Kling, (@oliverkling) Vedran Lerenc, (@vlerenc) Dirk Marwinski, (@marwinski) Michael Schubert, (@schu) Matthias Sohn, (@msohn) Frederik Thormaehlen, (@ThormaehlenFred) Christian Cwienk (@ccwienk)  Disclosures Private Disclosure Processes The Gardener community asks that all suspected vulnerabilities be privately and responsibly disclosed. If you\u0026rsquo;ve found a vulnerability or a potential vulnerability in Gardener please let us know by writing an e-mail to secure@sap.com. We\u0026rsquo;ll send a confirmation e-mail to acknowledge your report, and we\u0026rsquo;ll send an additional e-mail when we\u0026rsquo;ve identified the issue positively or negatively.\nPublic Disclosure Processes If you know of a publicly disclosed vulnerability please IMMEDIATELY e-mail to secure@sap.com to inform the Gardener Security Team about the vulnerability so they may start the patch, release, and communication process.\nIf possible the Gardener Security Team will ask the person making the public report if the issue can be handled via a private disclosure process (for example if the full exploit details have not yet been published). If the reporter denies the request for private disclosure, the Gardener Security Team will move swiftly with the fix and release process. In extreme cases GitHub can be asked to delete the issue but this generally isn\u0026rsquo;t necessary and is unlikely to make a public disclosure less damaging.\nPatch, Release, and Public Communication For each vulnerability a member of the Gardener Security Team will volunteer to lead coordination with the \u0026ldquo;Fix Team\u0026rdquo; and is responsible for sending disclosure e-mails to the rest of the community. This lead will be referred to as the \u0026ldquo;Fix Lead.\u0026rdquo; The role of the Fix Lead should rotate round-robin across the Gardener Security Team. Note that given the current size of the Gardener community it is likely that the Gardener Security Team is the same as the \u0026ldquo;Fix team.\u0026rdquo; (I.e., all maintainers). The Gardener Security Team may decide to bring in additional contributors for added expertise depending on the area of the code that contains the vulnerability. All of the time lines below are suggestions and assume a private disclosure. The Fix Lead drives the schedule using his best judgment based on severity and development time. If the Fix Lead is dealing with a public disclosure all time lines become ASAP (assuming the vulnerability has a CVSS score \u0026gt;= 7; see below). If the fix relies on another upstream project\u0026rsquo;s disclosure time line, that will adjust the process as well. We will work with the upstream project to fit their time line and best protect our users.\nFix Team Organization The Fix Lead will work quickly to identify relevant engineers from the affected projects and packages and CC those engineers into the disclosure thread. These selected developers are the Fix Team. The Fix Lead will give the Fix Team access to a private security repository to develop the fix.\nFix Development Process The Fix Lead and the Fix Team will create a CVSS using the CVSS Calculator. The Fix Lead makes the final call on the calculated CVSS; it is better to move quickly than make the CVSS perfect. The Fix Team will notify the Fix Lead that work on the fix branch is complete once there are LGTMs on all commits in the private repository from one or more maintainers. If the CVSS score is under 7.0 (a medium severity score) the Fix Team can decide to slow the release process down in the face of holidays, developer bandwidth, etc. These decisions must be discussed on the private Gardener Security mailing list.\nFix Disclosure Process With the fix development underway, the Fix Lead needs to come up with an overall communication plan for the wider community. This Disclosure process should begin after the Fix Team has developed a Fix or mitigation so that a realistic time line can be communicated to users. The Fix Lead will inform the Gardener mailing list that a security vulnerability has been disclosed and that a fix will be made available in the future on a certain release date. The Fix Lead will include any mitigating steps users can take until a fix is available. The communication to Gardener users should be actionable. They should know when to block time to apply patches, understand exact mitigation steps, etc.\nFix Release Day The Release Managers will ensure all the binaries are built, publicly available, and functional before the Release Date. The Release Managers will create a new patch release branch from the latest patch release tag + the fix from the security branch. As a practical example if v0.12.0 is the latest patch release in gardener.git a new branch will be created called v0.12.1 which includes only patches required to fix the issue. The Fix Lead will cherry-pick the patches onto the master branch and all relevant release branches. The Fix Team will LGTM and merge. The Release Managers will merge these PRs as quickly as possible. Changes shouldn\u0026rsquo;t be made to the commits even for a typo in the CHANGELOG as this will change the git sha of the already built and commits leading to confusion and potentially conflicts as the fix is cherry-picked around branches. The Fix Lead will request a CVE from the SAP Product Security Response Team via email to cna@sap.com with all the relevant information (description, potential impact, affected version, fixed version, CVSS v3 base score and supporting documentation for the CVSS score) for every vulnerability. The Fix Lead will inform the Gardener mailing list and announce the new releases, the CVE number (if available), the location of the binaries, and the relevant merged PRs to get wide distribution and user action.\nAs much as possible this e-mail should be actionable and include links how to apply the fix to users environments; this can include links to external distributor documentation. The recommended target time is 4pm UTC on a non-Friday weekday. This means the announcement will be seen morning Pacific, early evening Europe, and late evening Asia. The Fix Lead will remove the Fix Team from the private security repository.\nRetrospective These steps should be completed after the Release Date. The retrospective process should be blameless.\nThe Fix Lead will send a retrospective of the process to the Gardener mailing list including details on everyone involved, the time line of the process, links to relevant PRs that introduced the issue, if relevant, and any critiques of the response and release process. The Release Managers and Fix Team are also encouraged to send their own feedback on the process to the Gardener mailing list. Honest critique is the only way we are going to get good at this as a community.\nCommunication Channel The private or public disclosure process should be triggered exclusively by writing an e-mail to secure@sap.com.\nGardener security announcements will be communicated by the Fix Lead sending an e-mail to the Gardener mailing list (reachable via gardener@googlegroups.com) as well as posting a link in the Gardener Slack channel. Public discussions about Gardener security announcements and retrospectives, will primarily happen in the Gardener mailing list. Thus Gardener community members who are interested in participating in discussions related to the Gardener Security Release Process are encouraged to join the Gardener mailing list (how to find and join a group)\nThe members of the Gardener Security Team are subscribed to the private Gardener Security mailing list (reachable via gardener-security@googlegroups.com).\n"},{"uri":"https://gardener.cloud/v1.13.2/contribute/10_code/12-security_guide/","title":"Security Release Process","tags":[],"description":"","content":"Gardener Security Release Process Gardener is a growing community of volunteers and users. The Gardener community has adopted this security disclosure and response policy to ensure we responsibly handle critical issues.\nGardener Security Team Security vulnerabilities should be handled quickly and sometimes privately. The primary goal of this process is to reduce the total time users are vulnerable to publicly known exploits. The Gardener Security Team is responsible for organizing the entire response including internal communication and external disclosure but will need help from relevant developers and release managers to successfully run this process. The initial Gardener Security Team will consist of the following volunteers:\n Olaf Beier, (@olafbeier) Vasu Chandrasekhara, (@vasu1124) Alban Crequy, (@alban) Norbert Hamann, (@norberthamann) Claudia Hlters, (@hoeltcl) Oliver Kling, (@oliverkling) Vedran Lerenc, (@vlerenc) Dirk Marwinski, (@marwinski) Michael Schubert, (@schu) Matthias Sohn, (@msohn) Frederik Thormaehlen, (@ThormaehlenFred) Christian Cwienk (@ccwienk)  Disclosures Private Disclosure Processes The Gardener community asks that all suspected vulnerabilities be privately and responsibly disclosed. If you\u0026rsquo;ve found a vulnerability or a potential vulnerability in Gardener please let us know by writing an e-mail to secure@sap.com. We\u0026rsquo;ll send a confirmation e-mail to acknowledge your report, and we\u0026rsquo;ll send an additional e-mail when we\u0026rsquo;ve identified the issue positively or negatively.\nPublic Disclosure Processes If you know of a publicly disclosed vulnerability please IMMEDIATELY e-mail to secure@sap.com to inform the Gardener Security Team about the vulnerability so they may start the patch, release, and communication process.\nIf possible the Gardener Security Team will ask the person making the public report if the issue can be handled via a private disclosure process (for example if the full exploit details have not yet been published). If the reporter denies the request for private disclosure, the Gardener Security Team will move swiftly with the fix and release process. In extreme cases GitHub can be asked to delete the issue but this generally isn\u0026rsquo;t necessary and is unlikely to make a public disclosure less damaging.\nPatch, Release, and Public Communication For each vulnerability a member of the Gardener Security Team will volunteer to lead coordination with the \u0026ldquo;Fix Team\u0026rdquo; and is responsible for sending disclosure e-mails to the rest of the community. This lead will be referred to as the \u0026ldquo;Fix Lead.\u0026rdquo; The role of the Fix Lead should rotate round-robin across the Gardener Security Team. Note that given the current size of the Gardener community it is likely that the Gardener Security Team is the same as the \u0026ldquo;Fix team.\u0026rdquo; (I.e., all maintainers). The Gardener Security Team may decide to bring in additional contributors for added expertise depending on the area of the code that contains the vulnerability. All of the time lines below are suggestions and assume a private disclosure. The Fix Lead drives the schedule using his best judgment based on severity and development time. If the Fix Lead is dealing with a public disclosure all time lines become ASAP (assuming the vulnerability has a CVSS score \u0026gt;= 7; see below). If the fix relies on another upstream project\u0026rsquo;s disclosure time line, that will adjust the process as well. We will work with the upstream project to fit their time line and best protect our users.\nFix Team Organization The Fix Lead will work quickly to identify relevant engineers from the affected projects and packages and CC those engineers into the disclosure thread. These selected developers are the Fix Team. The Fix Lead will give the Fix Team access to a private security repository to develop the fix.\nFix Development Process The Fix Lead and the Fix Team will create a CVSS using the CVSS Calculator. The Fix Lead makes the final call on the calculated CVSS; it is better to move quickly than make the CVSS perfect. The Fix Team will notify the Fix Lead that work on the fix branch is complete once there are LGTMs on all commits in the private repository from one or more maintainers. If the CVSS score is under 7.0 (a medium severity score) the Fix Team can decide to slow the release process down in the face of holidays, developer bandwidth, etc. These decisions must be discussed on the private Gardener Security mailing list.\nFix Disclosure Process With the fix development underway, the Fix Lead needs to come up with an overall communication plan for the wider community. This Disclosure process should begin after the Fix Team has developed a Fix or mitigation so that a realistic time line can be communicated to users. The Fix Lead will inform the Gardener mailing list that a security vulnerability has been disclosed and that a fix will be made available in the future on a certain release date. The Fix Lead will include any mitigating steps users can take until a fix is available. The communication to Gardener users should be actionable. They should know when to block time to apply patches, understand exact mitigation steps, etc.\nFix Release Day The Release Managers will ensure all the binaries are built, publicly available, and functional before the Release Date. The Release Managers will create a new patch release branch from the latest patch release tag + the fix from the security branch. As a practical example if v0.12.0 is the latest patch release in gardener.git a new branch will be created called v0.12.1 which includes only patches required to fix the issue. The Fix Lead will cherry-pick the patches onto the master branch and all relevant release branches. The Fix Team will LGTM and merge. The Release Managers will merge these PRs as quickly as possible. Changes shouldn\u0026rsquo;t be made to the commits even for a typo in the CHANGELOG as this will change the git sha of the already built and commits leading to confusion and potentially conflicts as the fix is cherry-picked around branches. The Fix Lead will request a CVE from the SAP Product Security Response Team via email to cna@sap.com with all the relevant information (description, potential impact, affected version, fixed version, CVSS v3 base score and supporting documentation for the CVSS score) for every vulnerability. The Fix Lead will inform the Gardener mailing list and announce the new releases, the CVE number (if available), the location of the binaries, and the relevant merged PRs to get wide distribution and user action.\nAs much as possible this e-mail should be actionable and include links how to apply the fix to users environments; this can include links to external distributor documentation. The recommended target time is 4pm UTC on a non-Friday weekday. This means the announcement will be seen morning Pacific, early evening Europe, and late evening Asia. The Fix Lead will remove the Fix Team from the private security repository.\nRetrospective These steps should be completed after the Release Date. The retrospective process should be blameless.\nThe Fix Lead will send a retrospective of the process to the Gardener mailing list including details on everyone involved, the time line of the process, links to relevant PRs that introduced the issue, if relevant, and any critiques of the response and release process. The Release Managers and Fix Team are also encouraged to send their own feedback on the process to the Gardener mailing list. Honest critique is the only way we are going to get good at this as a community.\nCommunication Channel The private or public disclosure process should be triggered exclusively by writing an e-mail to secure@sap.com.\nGardener security announcements will be communicated by the Fix Lead sending an e-mail to the Gardener mailing list (reachable via gardener@googlegroups.com) as well as posting a link in the Gardener Slack channel. Public discussions about Gardener security announcements and retrospectives, will primarily happen in the Gardener mailing list. Thus Gardener community members who are interested in participating in discussions related to the Gardener Security Release Process are encouraged to join the Gardener mailing list (how to find and join a group)\nThe members of the Gardener Security Team are subscribed to the private Gardener Security mailing list (reachable via gardener-security@googlegroups.com).\n"},{"uri":"https://gardener.cloud/documentation/guides/install_gardener/setup-seed/","title":"Setting up a Seed Cluster","tags":[],"description":"How to configure a Kubernetes cluster as a Gardener seed","content":"The Seed Cluster The landscape-setup-template is meant to provide an as-simple-as-possible Gardener installation. Therefore it just registers the cluster where the Gardener is deployed on as a seed cluster. While this is easy, it might be insecure. Clusters created with Kubify don\u0026rsquo;t have network policies, for example. See Hardening the Gardener Community Setup for more information.\nTo have network policies on the seed cluster and avoid having the seed on the same cluster as the Gardener, the easiest option is probably to simply create a shoot and then register that shoot as seed. This way you can also leverage other advantages of shooted clusters for your seed, e.g. autoscaling.\nSetting up the Shoot The first step is to create a shoot cluster. Unfortunately, the Gardener dashboard currently does not allow to change the CIDRs for the created shoot clusters, and your shoots won\u0026rsquo;t work if they have overlapping CIDR ranges with their corresponding seed cluster. So either your seed cluster is deployed with different CIDRs - not using the dashboard, but kubectl apply and a yaml file - or all of your shoots on that seed need to be created this way. In order to be able to use the dashboard for the shoots, it makes sense to create the seed with different CIDRs.\nSo, create yourself a shoot with modified CIDRs. You can find templates for the shoot manifest here. You could, for example, change the CIDRs to this:\n...networks:internal:- 10.254.112.0/22nodes:10.254.0.0/19pods:10.255.0.0/17public:- 10.254.96.0/22services:10.255.128.0/17vpc:cidr:10.254.0.0/16workers:- 10.254.0.0/19...Also make sure that your new seed cluster has enough resources for the expected number of shoots.\nRegistering the Shoot as Seed The seed itself is a Kubernetes resource that can be deployed via a yaml file, but it has some dependencies. You can find templated versions of these files in the seed-config component of the landscape-setup-template project. If you have set up your Gardener using this project, there should also be rendered versions of these files in the state/seed-config/ directory of your landscape folder (they are probably easier to work with). Examples for all these files can also be found in the aforementioned example folder in the Gardener repo.\n1. Seed Namespace First, you should create a namespace for your new seed and everything that belongs to it. This is not necessary, but it will keep your cluster organized. For this example, the namespace will be called seed-test.\n2. Cloud Provider Secret The Gardener needs to create resources on the seed and thus needs a kubeconfig for it. It is provided with the cloud provider secret (below is an example for AWS).\napiVersion:v1kind:Secretmetadata:name:test-seed-secretnamespace:seed-testlabels:cloudprofile.garden.sapcloud.io/name:awstype:Opaquedata:accessKeyID:\u0026lt;base64-encodedAWSaccesskey\u0026gt; secretAccessKey: \u0026lt;base64-encoded AWS secret key\u0026gt;kubeconfig:\u0026lt;base64-encodedkubeconfig\u0026gt;Deploy the secret into your seed namespace. Apart from the kubeconfig, also infrastructure credentials are required. They will only be used for the etcd backup, so in case for AWS, S3 privileges should be sufficient.\n3. Secretbinding for Cloud Provider Secret Create a secretbinding for your cloud provider secret:\napiVersion:core.gardener.cloud/v1beta1kind:SecretBindingmetadata:name:test-seed-secretnamespace:seed-testlabels:cloudprofile.garden.sapcloud.io/name:awssecretRef:name:test-seed-secret# namespace: only required if in different namespace than referenced secretquotas:[]You can give it the same name as the referenced secret.\n4. Cloudprofile The cloudprofile contains the information which shoots can be created with this seed. You could create a new cloudprofile, but you can also just reference the existing cloudprofile if you don\u0026rsquo;t want to change anything.\n5. Seed Now the seed resource can be created. Choose a name, reference cloudprofile and secretbinding, fill in your ingress domain, and set the CIDRs to the same values as in the underlying shoot cluster.\napiVersion:core.gardener.cloud/v1beta1kind:Seedmetadata:name:aws-securespec:provider:type:awsregion:eu-west-1secretRef:name:test-seed-secretnamespace:seed-testdns:ingressDomain:ingress.\u0026lt;yourclusterdomain\u0026gt; networks:nodes:10.254.0.0/19pods:10.255.0.0/17services:10.255.128.0/176. Hide Original Seed In the dashboard, it is not possible to select the seed for a shoot (it is possible when deploying the shoot using a yaml file, however). Since both seeds probably reference the same cloudprofile, the Gardener will try to distribute the shoots equally among both seeds.\nTo solve this problem, edit the original seed and set its spec.visible field to false. This will prevent the Gardener from choosing this seed, so now all shoots created via the dashboard should have their control plane on the new, more secure seed.\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/install_gardener/setup-seed/","title":"Setting up a Seed Cluster","tags":[],"description":"How to configure a Kubernetes cluster as a Gardener seed","content":"The Seed Cluster The landscape-setup-template is meant to provide an as-simple-as-possible Gardener installation. Therefore it just registers the cluster where the Gardener is deployed on as a seed cluster. While this is easy, it might be insecure. Clusters created with Kubify don\u0026rsquo;t have network policies, for example. See Hardening the Gardener Community Setup for more information.\nTo have network policies on the seed cluster and avoid having the seed on the same cluster as the Gardener, the easiest option is probably to simply create a shoot and then register that shoot as seed. This way you can also leverage other advantages of shooted clusters for your seed, e.g. autoscaling.\nSetting up the Shoot The first step is to create a shoot cluster. Unfortunately, the Gardener dashboard currently does not allow to change the CIDRs for the created shoot clusters, and your shoots won\u0026rsquo;t work if they have overlapping CIDR ranges with their corresponding seed cluster. So either your seed cluster is deployed with different CIDRs - not using the dashboard, but kubectl apply and a yaml file - or all of your shoots on that seed need to be created this way. In order to be able to use the dashboard for the shoots, it makes sense to create the seed with different CIDRs.\nSo, create yourself a shoot with modified CIDRs. You can find templates for the shoot manifest here. You could, for example, change the CIDRs to this:\n...networks:internal:- 10.254.112.0/22nodes:10.254.0.0/19pods:10.255.0.0/17public:- 10.254.96.0/22services:10.255.128.0/17vpc:cidr:10.254.0.0/16workers:- 10.254.0.0/19...Also make sure that your new seed cluster has enough resources for the expected number of shoots.\nRegistering the Shoot as Seed The seed itself is a Kubernetes resource that can be deployed via a yaml file, but it has some dependencies. You can find templated versions of these files in the seed-config component of the landscape-setup-template project. If you have set up your Gardener using this project, there should also be rendered versions of these files in the state/seed-config/ directory of your landscape folder (they are probably easier to work with). Examples for all these files can also be found in the aforementioned example folder in the Gardener repo.\n1. Seed Namespace First, you should create a namespace for your new seed and everything that belongs to it. This is not necessary, but it will keep your cluster organized. For this example, the namespace will be called seed-test.\n2. Cloud Provider Secret The Gardener needs to create resources on the seed and thus needs a kubeconfig for it. It is provided with the cloud provider secret (below is an example for AWS).\napiVersion:v1kind:Secretmetadata:name:test-seed-secretnamespace:seed-testlabels:cloudprofile.garden.sapcloud.io/name:awstype:Opaquedata:accessKeyID:\u0026lt;base64-encodedAWSaccesskey\u0026gt; secretAccessKey: \u0026lt;base64-encoded AWS secret key\u0026gt;kubeconfig:\u0026lt;base64-encodedkubeconfig\u0026gt;Deploy the secret into your seed namespace. Apart from the kubeconfig, also infrastructure credentials are required. They will only be used for the etcd backup, so in case for AWS, S3 privileges should be sufficient.\n3. Secretbinding for Cloud Provider Secret Create a secretbinding for your cloud provider secret:\napiVersion:core.gardener.cloud/v1beta1kind:SecretBindingmetadata:name:test-seed-secretnamespace:seed-testlabels:cloudprofile.garden.sapcloud.io/name:awssecretRef:name:test-seed-secret# namespace: only required if in different namespace than referenced secretquotas:[]You can give it the same name as the referenced secret.\n4. Cloudprofile The cloudprofile contains the information which shoots can be created with this seed. You could create a new cloudprofile, but you can also just reference the existing cloudprofile if you don\u0026rsquo;t want to change anything.\n5. Seed Now the seed resource can be created. Choose a name, reference cloudprofile and secretbinding, fill in your ingress domain, and set the CIDRs to the same values as in the underlying shoot cluster.\napiVersion:core.gardener.cloud/v1beta1kind:Seedmetadata:name:aws-securespec:provider:type:awsregion:eu-west-1secretRef:name:test-seed-secretnamespace:seed-testdns:ingressDomain:ingress.\u0026lt;yourclusterdomain\u0026gt; networks:nodes:10.254.0.0/19pods:10.255.0.0/17services:10.255.128.0/176. Hide Original Seed In the dashboard, it is not possible to select the seed for a shoot (it is possible when deploying the shoot using a yaml file, however). Since both seeds probably reference the same cloudprofile, the Gardener will try to distribute the shoots equally among both seeds.\nTo solve this problem, edit the original seed and set its spec.visible field to false. This will prevent the Gardener from choosing this seed, so now all shoots created via the dashboard should have their control plane on the new, more secure seed.\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/install_gardener/setup-seed/","title":"Setting up a Seed Cluster","tags":[],"description":"How to configure a Kubernetes cluster as a Gardener seed","content":"The Seed Cluster The landscape-setup-template is meant to provide an as-simple-as-possible Gardener installation. Therefore it just registers the cluster where the Gardener is deployed on as a seed cluster. While this is easy, it might be insecure. Clusters created with Kubify don\u0026rsquo;t have network policies, for example. See Hardening the Gardener Community Setup for more information.\nTo have network policies on the seed cluster and avoid having the seed on the same cluster as the Gardener, the easiest option is probably to simply create a shoot and then register that shoot as seed. This way you can also leverage other advantages of shooted clusters for your seed, e.g. autoscaling.\nSetting up the Shoot The first step is to create a shoot cluster. Unfortunately, the Gardener dashboard currently does not allow to change the CIDRs for the created shoot clusters, and your shoots won\u0026rsquo;t work if they have overlapping CIDR ranges with their corresponding seed cluster. So either your seed cluster is deployed with different CIDRs - not using the dashboard, but kubectl apply and a yaml file - or all of your shoots on that seed need to be created this way. In order to be able to use the dashboard for the shoots, it makes sense to create the seed with different CIDRs.\nSo, create yourself a shoot with modified CIDRs. You can find templates for the shoot manifest here. You could, for example, change the CIDRs to this:\n...networks:internal:- 10.254.112.0/22nodes:10.254.0.0/19pods:10.255.0.0/17public:- 10.254.96.0/22services:10.255.128.0/17vpc:cidr:10.254.0.0/16workers:- 10.254.0.0/19...Also make sure that your new seed cluster has enough resources for the expected number of shoots.\nRegistering the Shoot as Seed The seed itself is a Kubernetes resource that can be deployed via a yaml file, but it has some dependencies. You can find templated versions of these files in the seed-config component of the landscape-setup-template project. If you have set up your Gardener using this project, there should also be rendered versions of these files in the state/seed-config/ directory of your landscape folder (they are probably easier to work with). Examples for all these files can also be found in the aforementioned example folder in the Gardener repo.\n1. Seed Namespace First, you should create a namespace for your new seed and everything that belongs to it. This is not necessary, but it will keep your cluster organized. For this example, the namespace will be called seed-test.\n2. Cloud Provider Secret The Gardener needs to create resources on the seed and thus needs a kubeconfig for it. It is provided with the cloud provider secret (below is an example for AWS).\napiVersion:v1kind:Secretmetadata:name:test-seed-secretnamespace:seed-testlabels:cloudprofile.garden.sapcloud.io/name:awstype:Opaquedata:accessKeyID:\u0026lt;base64-encodedAWSaccesskey\u0026gt; secretAccessKey: \u0026lt;base64-encoded AWS secret key\u0026gt;kubeconfig:\u0026lt;base64-encodedkubeconfig\u0026gt;Deploy the secret into your seed namespace. Apart from the kubeconfig, also infrastructure credentials are required. They will only be used for the etcd backup, so in case for AWS, S3 privileges should be sufficient.\n3. Secretbinding for Cloud Provider Secret Create a secretbinding for your cloud provider secret:\napiVersion:core.gardener.cloud/v1beta1kind:SecretBindingmetadata:name:test-seed-secretnamespace:seed-testlabels:cloudprofile.garden.sapcloud.io/name:awssecretRef:name:test-seed-secret# namespace: only required if in different namespace than referenced secretquotas:[]You can give it the same name as the referenced secret.\n4. Cloudprofile The cloudprofile contains the information which shoots can be created with this seed. You could create a new cloudprofile, but you can also just reference the existing cloudprofile if you don\u0026rsquo;t want to change anything.\n5. Seed Now the seed resource can be created. Choose a name, reference cloudprofile and secretbinding, fill in your ingress domain, and set the CIDRs to the same values as in the underlying shoot cluster.\napiVersion:core.gardener.cloud/v1beta1kind:Seedmetadata:name:aws-securespec:provider:type:awsregion:eu-west-1secretRef:name:test-seed-secretnamespace:seed-testdns:ingressDomain:ingress.\u0026lt;yourclusterdomain\u0026gt; networks:nodes:10.254.0.0/19pods:10.255.0.0/17services:10.255.128.0/176. Hide Original Seed In the dashboard, it is not possible to select the seed for a shoot (it is possible when deploying the shoot using a yaml file, however). Since both seeds probably reference the same cloudprofile, the Gardener will try to distribute the shoots equally among both seeds.\nTo solve this problem, edit the original seed and set its spec.visible field to false. This will prevent the Gardener from choosing this seed, so now all shoots created via the dashboard should have their control plane on the new, more secure seed.\n"},{"uri":"https://gardener.cloud/documentation/references/settings/","title":"Settings","tags":[],"description":"","content":"Packages:\n  settings.gardener.cloud/v1alpha1   settings.gardener.cloud/v1alpha1  Package v1alpha1 is a version of the API.\nResource Types:  ClusterOpenIDConnectPreset  OpenIDConnectPreset  ClusterOpenIDConnectPreset   ClusterOpenIDConnectPreset is a OpenID Connect configuration that is applied to a Shoot objects cluster-wide.\n   Field Description      apiVersion string   settings.gardener.cloud/v1alpha1      kind string  ClusterOpenIDConnectPreset    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ClusterOpenIDConnectPresetSpec     Spec is the specification of this OpenIDConnect preset.\n     OpenIDConnectPresetSpec  OpenIDConnectPresetSpec      (Members of OpenIDConnectPresetSpec are embedded into this type.)     projectSelector  Kubernetes meta/v1.LabelSelector     (Optional) Project decides whether to apply the configuration if the Shoot is in a specific Project matching the label selector. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Defaults to the empty LabelSelector, which matches everything.\n       OpenIDConnectPreset   OpenIDConnectPreset is a OpenID Connect configuration that is applied to a Shoot in a namespace.\n   Field Description      apiVersion string   settings.gardener.cloud/v1alpha1      kind string  OpenIDConnectPreset    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  OpenIDConnectPresetSpec     Spec is the specification of this OpenIDConnect preset.\n     server  KubeAPIServerOpenIDConnect     Server contains the kube-apiserver\u0026rsquo;s OpenID Connect configuration. This configuration is not overwritting any existing OpenID Connect configuration already set on the Shoot object.\n    client  OpenIDConnectClientAuthentication     (Optional) Client contains the configuration used for client OIDC authentication of Shoot clusters. This configuration is not overwritting any existing OpenID Connect client authentication already set on the Shoot object.\n    shootSelector  Kubernetes meta/v1.LabelSelector     (Optional) ShootSelector decides whether to apply the configuration if the Shoot has matching labels. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Default to the empty LabelSelector, which matches everything.\n    weight  int32    Weight associated with matching the corresponding preset, in the range 1-100. Required.\n       ClusterOpenIDConnectPresetSpec   (Appears on: ClusterOpenIDConnectPreset)  ClusterOpenIDConnectPresetSpec contains the OpenIDConnect specification and project selector matching Shoots in Projects.\n   Field Description      OpenIDConnectPresetSpec  OpenIDConnectPresetSpec      (Members of OpenIDConnectPresetSpec are embedded into this type.)     projectSelector  Kubernetes meta/v1.LabelSelector     (Optional) Project decides whether to apply the configuration if the Shoot is in a specific Project matching the label selector. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Defaults to the empty LabelSelector, which matches everything.\n    KubeAPIServerOpenIDConnect   (Appears on: OpenIDConnectPresetSpec)  KubeAPIServerOpenIDConnect contains configuration settings for the OIDC provider. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      caBundle  string    (Optional) If set, the OpenID server\u0026rsquo;s certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host\u0026rsquo;s root CA set will be used.\n    clientID  string    The client ID for the OpenID Connect client. Required.\n    groupsClaim  string    (Optional) If provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This field is experimental, please see the authentication documentation for further details.\n    groupsPrefix  string    (Optional) If provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies.\n    issuerURL  string    The URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT). Required.\n    requiredClaims  map[string]string    (Optional) key=value pairs that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value. Only applied when the Kubernetes version of the Shoot is \u0026gt;= 1.11\n    signingAlgs  []string    (Optional) List of allowed JOSE asymmetric signing algorithms. JWTs with a \u0026lsquo;alg\u0026rsquo; header value not in this list will be rejected. Values are defined by RFC 7518 https://tools.ietf.org/html/rfc7518#section-3.1 Defaults to [RS256]\n    usernameClaim  string    (Optional) The OpenID claim to use as the user name. Note that claims other than the default (\u0026lsquo;sub\u0026rsquo;) is not guaranteed to be unique and immutable. This field is experimental, please see the authentication documentation for further details. Defaults to \u0026ldquo;sub\u0026rdquo;.\n    usernamePrefix  string    (Optional) If provided, all usernames will be prefixed with this value. If not provided, username claims other than \u0026lsquo;email\u0026rsquo; are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value \u0026lsquo;-\u0026rsquo;.\n    OpenIDConnectClientAuthentication   (Appears on: OpenIDConnectPresetSpec)  OpenIDConnectClientAuthentication contains configuration for OIDC clients.\n   Field Description      secret  string    (Optional) The client Secret for the OpenID Connect client.\n    extraConfig  map[string]string    (Optional) Extra configuration added to kubeconfig\u0026rsquo;s auth-provider. Must not be any of idp-issuer-url, client-id, client-secret, idp-certificate-authority, idp-certificate-authority-data, id-token or refresh-token\n    OpenIDConnectPresetSpec   (Appears on: OpenIDConnectPreset, ClusterOpenIDConnectPresetSpec)  OpenIDConnectPresetSpec contains the Shoot selector for which a specific OpenID Connect configuration is applied.\n   Field Description      server  KubeAPIServerOpenIDConnect     Server contains the kube-apiserver\u0026rsquo;s OpenID Connect configuration. This configuration is not overwritting any existing OpenID Connect configuration already set on the Shoot object.\n    client  OpenIDConnectClientAuthentication     (Optional) Client contains the configuration used for client OIDC authentication of Shoot clusters. This configuration is not overwritting any existing OpenID Connect client authentication already set on the Shoot object.\n    shootSelector  Kubernetes meta/v1.LabelSelector     (Optional) ShootSelector decides whether to apply the configuration if the Shoot has matching labels. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Default to the empty LabelSelector, which matches everything.\n    weight  int32    Weight associated with matching the corresponding preset, in the range 1-100. Required.\n      Generated with gen-crd-api-reference-docs \n"},{"uri":"https://gardener.cloud/v1.12.8/references/settings/","title":"Settings","tags":[],"description":"","content":"Packages:\n  settings.gardener.cloud/v1alpha1   settings.gardener.cloud/v1alpha1  Package v1alpha1 is a version of the API.\nResource Types:  ClusterOpenIDConnectPreset  OpenIDConnectPreset  ClusterOpenIDConnectPreset   ClusterOpenIDConnectPreset is a OpenID Connect configuration that is applied to a Shoot objects cluster-wide.\n   Field Description      apiVersion string   settings.gardener.cloud/v1alpha1      kind string  ClusterOpenIDConnectPreset    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ClusterOpenIDConnectPresetSpec     Spec is the specification of this OpenIDConnect preset.\n     OpenIDConnectPresetSpec  OpenIDConnectPresetSpec      (Members of OpenIDConnectPresetSpec are embedded into this type.)     projectSelector  Kubernetes meta/v1.LabelSelector     (Optional) Project decides whether to apply the configuration if the Shoot is in a specific Project matching the label selector. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Defaults to the empty LabelSelector, which matches everything.\n       OpenIDConnectPreset   OpenIDConnectPreset is a OpenID Connect configuration that is applied to a Shoot in a namespace.\n   Field Description      apiVersion string   settings.gardener.cloud/v1alpha1      kind string  OpenIDConnectPreset    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  OpenIDConnectPresetSpec     Spec is the specification of this OpenIDConnect preset.\n     server  KubeAPIServerOpenIDConnect     Server contains the kube-apiserver\u0026rsquo;s OpenID Connect configuration. This configuration is not overwritting any existing OpenID Connect configuration already set on the Shoot object.\n    client  OpenIDConnectClientAuthentication     (Optional) Client contains the configuration used for client OIDC authentication of Shoot clusters. This configuration is not overwritting any existing OpenID Connect client authentication already set on the Shoot object.\n    shootSelector  Kubernetes meta/v1.LabelSelector     (Optional) ShootSelector decides whether to apply the configuration if the Shoot has matching labels. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Default to the empty LabelSelector, which matches everything.\n    weight  int32    Weight associated with matching the corresponding preset, in the range 1-100. Required.\n       ClusterOpenIDConnectPresetSpec   (Appears on: ClusterOpenIDConnectPreset)  ClusterOpenIDConnectPresetSpec contains the OpenIDConnect specification and project selector matching Shoots in Projects.\n   Field Description      OpenIDConnectPresetSpec  OpenIDConnectPresetSpec      (Members of OpenIDConnectPresetSpec are embedded into this type.)     projectSelector  Kubernetes meta/v1.LabelSelector     (Optional) Project decides whether to apply the configuration if the Shoot is in a specific Project matching the label selector. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Defaults to the empty LabelSelector, which matches everything.\n    KubeAPIServerOpenIDConnect   (Appears on: OpenIDConnectPresetSpec)  KubeAPIServerOpenIDConnect contains configuration settings for the OIDC provider. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      caBundle  string    (Optional) If set, the OpenID server\u0026rsquo;s certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host\u0026rsquo;s root CA set will be used.\n    clientID  string    The client ID for the OpenID Connect client. Required.\n    groupsClaim  string    (Optional) If provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This field is experimental, please see the authentication documentation for further details.\n    groupsPrefix  string    (Optional) If provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies.\n    issuerURL  string    The URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT). Required.\n    requiredClaims  map[string]string    (Optional) key=value pairs that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value. Only applied when the Kubernetes version of the Shoot is \u0026gt;= 1.11\n    signingAlgs  []string    (Optional) List of allowed JOSE asymmetric signing algorithms. JWTs with a \u0026lsquo;alg\u0026rsquo; header value not in this list will be rejected. Values are defined by RFC 7518 https://tools.ietf.org/html/rfc7518#section-3.1 Defaults to [RS256]\n    usernameClaim  string    (Optional) The OpenID claim to use as the user name. Note that claims other than the default (\u0026lsquo;sub\u0026rsquo;) is not guaranteed to be unique and immutable. This field is experimental, please see the authentication documentation for further details. Defaults to \u0026ldquo;sub\u0026rdquo;.\n    usernamePrefix  string    (Optional) If provided, all usernames will be prefixed with this value. If not provided, username claims other than \u0026lsquo;email\u0026rsquo; are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value \u0026lsquo;-\u0026rsquo;.\n    OpenIDConnectClientAuthentication   (Appears on: OpenIDConnectPresetSpec)  OpenIDConnectClientAuthentication contains configuration for OIDC clients.\n   Field Description      secret  string    (Optional) The client Secret for the OpenID Connect client.\n    extraConfig  map[string]string    (Optional) Extra configuration added to kubeconfig\u0026rsquo;s auth-provider. Must not be any of idp-issuer-url, client-id, client-secret, idp-certificate-authority, idp-certificate-authority-data, id-token or refresh-token\n    OpenIDConnectPresetSpec   (Appears on: OpenIDConnectPreset, ClusterOpenIDConnectPresetSpec)  OpenIDConnectPresetSpec contains the Shoot selector for which a specific OpenID Connect configuration is applied.\n   Field Description      server  KubeAPIServerOpenIDConnect     Server contains the kube-apiserver\u0026rsquo;s OpenID Connect configuration. This configuration is not overwritting any existing OpenID Connect configuration already set on the Shoot object.\n    client  OpenIDConnectClientAuthentication     (Optional) Client contains the configuration used for client OIDC authentication of Shoot clusters. This configuration is not overwritting any existing OpenID Connect client authentication already set on the Shoot object.\n    shootSelector  Kubernetes meta/v1.LabelSelector     (Optional) ShootSelector decides whether to apply the configuration if the Shoot has matching labels. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Default to the empty LabelSelector, which matches everything.\n    weight  int32    Weight associated with matching the corresponding preset, in the range 1-100. Required.\n      Generated with gen-crd-api-reference-docs \n"},{"uri":"https://gardener.cloud/v1.13.2/references/settings/","title":"Settings","tags":[],"description":"","content":"Packages:\n  settings.gardener.cloud/v1alpha1   settings.gardener.cloud/v1alpha1  Package v1alpha1 is a version of the API.\nResource Types:  ClusterOpenIDConnectPreset  OpenIDConnectPreset  ClusterOpenIDConnectPreset   ClusterOpenIDConnectPreset is a OpenID Connect configuration that is applied to a Shoot objects cluster-wide.\n   Field Description      apiVersion string   settings.gardener.cloud/v1alpha1      kind string  ClusterOpenIDConnectPreset    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  ClusterOpenIDConnectPresetSpec     Spec is the specification of this OpenIDConnect preset.\n     OpenIDConnectPresetSpec  OpenIDConnectPresetSpec      (Members of OpenIDConnectPresetSpec are embedded into this type.)     projectSelector  Kubernetes meta/v1.LabelSelector     (Optional) Project decides whether to apply the configuration if the Shoot is in a specific Project matching the label selector. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Defaults to the empty LabelSelector, which matches everything.\n       OpenIDConnectPreset   OpenIDConnectPreset is a OpenID Connect configuration that is applied to a Shoot in a namespace.\n   Field Description      apiVersion string   settings.gardener.cloud/v1alpha1      kind string  OpenIDConnectPreset    metadata  Kubernetes meta/v1.ObjectMeta     Standard object metadata.\nRefer to the Kubernetes API documentation for the fields of the metadata field.     spec  OpenIDConnectPresetSpec     Spec is the specification of this OpenIDConnect preset.\n     server  KubeAPIServerOpenIDConnect     Server contains the kube-apiserver\u0026rsquo;s OpenID Connect configuration. This configuration is not overwritting any existing OpenID Connect configuration already set on the Shoot object.\n    client  OpenIDConnectClientAuthentication     (Optional) Client contains the configuration used for client OIDC authentication of Shoot clusters. This configuration is not overwritting any existing OpenID Connect client authentication already set on the Shoot object.\n    shootSelector  Kubernetes meta/v1.LabelSelector     (Optional) ShootSelector decides whether to apply the configuration if the Shoot has matching labels. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Default to the empty LabelSelector, which matches everything.\n    weight  int32    Weight associated with matching the corresponding preset, in the range 1-100. Required.\n       ClusterOpenIDConnectPresetSpec   (Appears on: ClusterOpenIDConnectPreset)  ClusterOpenIDConnectPresetSpec contains the OpenIDConnect specification and project selector matching Shoots in Projects.\n   Field Description      OpenIDConnectPresetSpec  OpenIDConnectPresetSpec      (Members of OpenIDConnectPresetSpec are embedded into this type.)     projectSelector  Kubernetes meta/v1.LabelSelector     (Optional) Project decides whether to apply the configuration if the Shoot is in a specific Project matching the label selector. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Defaults to the empty LabelSelector, which matches everything.\n    KubeAPIServerOpenIDConnect   (Appears on: OpenIDConnectPresetSpec)  KubeAPIServerOpenIDConnect contains configuration settings for the OIDC provider. Note: Descriptions were taken from the Kubernetes documentation.\n   Field Description      caBundle  string    (Optional) If set, the OpenID server\u0026rsquo;s certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host\u0026rsquo;s root CA set will be used.\n    clientID  string    The client ID for the OpenID Connect client. Required.\n    groupsClaim  string    (Optional) If provided, the name of a custom OpenID Connect claim for specifying user groups. The claim value is expected to be a string or array of strings. This field is experimental, please see the authentication documentation for further details.\n    groupsPrefix  string    (Optional) If provided, all groups will be prefixed with this value to prevent conflicts with other authentication strategies.\n    issuerURL  string    The URL of the OpenID issuer, only HTTPS scheme will be accepted. If set, it will be used to verify the OIDC JSON Web Token (JWT). Required.\n    requiredClaims  map[string]string    (Optional) key=value pairs that describes a required claim in the ID Token. If set, the claim is verified to be present in the ID Token with a matching value. Only applied when the Kubernetes version of the Shoot is \u0026gt;= 1.11\n    signingAlgs  []string    (Optional) List of allowed JOSE asymmetric signing algorithms. JWTs with a \u0026lsquo;alg\u0026rsquo; header value not in this list will be rejected. Values are defined by RFC 7518 https://tools.ietf.org/html/rfc7518#section-3.1 Defaults to [RS256]\n    usernameClaim  string    (Optional) The OpenID claim to use as the user name. Note that claims other than the default (\u0026lsquo;sub\u0026rsquo;) is not guaranteed to be unique and immutable. This field is experimental, please see the authentication documentation for further details. Defaults to \u0026ldquo;sub\u0026rdquo;.\n    usernamePrefix  string    (Optional) If provided, all usernames will be prefixed with this value. If not provided, username claims other than \u0026lsquo;email\u0026rsquo; are prefixed by the issuer URL to avoid clashes. To skip any prefixing, provide the value \u0026lsquo;-\u0026rsquo;.\n    OpenIDConnectClientAuthentication   (Appears on: OpenIDConnectPresetSpec)  OpenIDConnectClientAuthentication contains configuration for OIDC clients.\n   Field Description      secret  string    (Optional) The client Secret for the OpenID Connect client.\n    extraConfig  map[string]string    (Optional) Extra configuration added to kubeconfig\u0026rsquo;s auth-provider. Must not be any of idp-issuer-url, client-id, client-secret, idp-certificate-authority, idp-certificate-authority-data, id-token or refresh-token\n    OpenIDConnectPresetSpec   (Appears on: OpenIDConnectPreset, ClusterOpenIDConnectPresetSpec)  OpenIDConnectPresetSpec contains the Shoot selector for which a specific OpenID Connect configuration is applied.\n   Field Description      server  KubeAPIServerOpenIDConnect     Server contains the kube-apiserver\u0026rsquo;s OpenID Connect configuration. This configuration is not overwritting any existing OpenID Connect configuration already set on the Shoot object.\n    client  OpenIDConnectClientAuthentication     (Optional) Client contains the configuration used for client OIDC authentication of Shoot clusters. This configuration is not overwritting any existing OpenID Connect client authentication already set on the Shoot object.\n    shootSelector  Kubernetes meta/v1.LabelSelector     (Optional) ShootSelector decides whether to apply the configuration if the Shoot has matching labels. Use the selector only if the OIDC Preset is opt-in, because end users may skip the admission by setting the labels. Default to the empty LabelSelector, which matches everything.\n    weight  int32    Weight associated with matching the corresponding preset, in the range 1-100. Required.\n      Generated with gen-crd-api-reference-docs \n"},{"uri":"https://gardener.cloud/documentation/tutorials/s3/","title":"Shared storage with S3 backend","tags":[],"description":"Using S3 bucket as shared storage for pods","content":"Shared storage with S3 backend The storage is definitely the most complex and important part of an application setup, once this part is completed, 80% of the tasks are completed.\nMounting an S3 bucket into a pod using FUSE allows you to access the data as if it were on the local disk. The mount is a pointer to an S3 location, so the data is never synced locally. Once mounted, any pod can read or even write from that directory without the need for explicit keys.\nHowever, it can be used to import and parse large amounts of data into a database.\nOverview Limitations Generally S3 cannot offer the same performance or semantics as a local file system. More specifically:\n random writes or appends to files require rewriting the entire file metadata operations such as listing directories have poor performance due to network latency eventual consistency can temporarily yield stale data(Amazon S3 Data Consistency Model) no atomic renames of files or directories no coordination between multiple clients mounting the same bucket no hard links  Before you Begin You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using the Gardener.\nEnsure that you have create the \u0026ldquo;imagePullSecret\u0026rdquo; in your cluster.\nkubectl create secret docker-registry artifactory --docker-server=\u0026lt;YOUR-REGISTRY\u0026gt;.docker.repositories.sap.ondemand.com --docker-username=\u0026lt;USERNAME\u0026gt; --docker-password=\u0026lt;PASSWORD\u0026gt; --docker-email=\u0026lt;EMAIL\u0026gt; -n \u0026lt;NAMESPACE\u0026gt; Setup The first step is to clone this repository. Next is the Secret for the AWS API credentials of the user that has full access to our S3 bucket. Copy the configmap_secrets_template.yaml to configmap_secrets.yaml and place your secrets at the right place\napiVersion:v1kind:ConfigMapmetadata:name:s3-configdata:S3_BUCKET:\u0026lt;YOUR-S3-BUCKET-NAME\u0026gt; AWS_KEY: \u0026lt;YOUR-AWS-TECH-USER-ACCESS-KEY\u0026gt;AWS_SECRET_KEY:\u0026lt;YOUR-AWS-TECH-USER-SECRET\u0026gt;Build and deploy Change the settings in the build.sh file with your docker registry settings.\n#!/usr/bin/env bash  ######################################################################################################################## # PREREQUISTITS ######################################################################################################################## # # - ensure that you have a valid Artifactory or other Docker registry account # - Create your image pull secret in your namespace # kubectl create secret docker-registry artifactory --docker-server=\u0026lt;YOUR-REGISTRY\u0026gt;.docker.repositories.sap.ondemand.com --docker-username=\u0026lt;USERNAME\u0026gt; --docker-password=\u0026lt;PASSWORD\u0026gt; --docker-email=\u0026lt;EMAIL\u0026gt; -n \u0026lt;NAMESPACE\u0026gt; # - change the settings below arcording your settings # # usage: # Call this script with the version to build and push to the registry. After build/push the # yaml/* files are deployed into your cluster # # ./build.sh 1.0 # VERSION=$1 PROJECT=kube-s3 REPOSITORY=cp-enablement.docker.repositories.sap.ondemand.com # causes the shell to exit if any subcommand or pipeline returns a non-zero status. set -e # set debug mode #set -x . . . . Create the S3Fuse Pod and check the status:\n# build and push the image to your docker registry ./build.sh 1.0 # check that the pods are up and running kubectl get pods Check success Create a demo Pod and check the status:\nkubectl apply -f ./yaml/example_pod.yaml # wait some second to get the pod up and running... kubectl get pods # go into the pd and check that the /var/s3 is mounted with your S3 bucket content inside kubectl exec -ti test-pd sh # inside the pod ls -la /var/s3 Why does this work? Docker engine 1.10 added a new feature which allows containers to share the host mount namespace. This feature makes it possible to mount a s3fs container file system to a host file system through a shared mount, providing a persistent network storage with S3 backend.\nThe key part is mountPath: /var/s3:shared which enables the volume to be mounted as shared inside the pod. When the container starts it will mount the S3 bucket onto /var/s3 and consequently the data will be available under /mnt/data-s3fs on the host and thus to any other container/pod running on it (and has /mnt/data-s3fs mounted too).\n"},{"uri":"https://gardener.cloud/v1.12.8/tutorials/s3/","title":"Shared storage with S3 backend","tags":[],"description":"Using S3 bucket as shared storage for pods","content":"Shared storage with S3 backend The storage is definitely the most complex and important part of an application setup, once this part is completed, 80% of the tasks are completed.\nMounting an S3 bucket into a pod using FUSE allows you to access the data as if it were on the local disk. The mount is a pointer to an S3 location, so the data is never synced locally. Once mounted, any pod can read or even write from that directory without the need for explicit keys.\nHowever, it can be used to import and parse large amounts of data into a database.\nOverview Limitations Generally S3 cannot offer the same performance or semantics as a local file system. More specifically:\n random writes or appends to files require rewriting the entire file metadata operations such as listing directories have poor performance due to network latency eventual consistency can temporarily yield stale data(Amazon S3 Data Consistency Model) no atomic renames of files or directories no coordination between multiple clients mounting the same bucket no hard links  Before you Begin You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using the Gardener.\nEnsure that you have create the \u0026ldquo;imagePullSecret\u0026rdquo; in your cluster.\nkubectl create secret docker-registry artifactory --docker-server=\u0026lt;YOUR-REGISTRY\u0026gt;.docker.repositories.sap.ondemand.com --docker-username=\u0026lt;USERNAME\u0026gt; --docker-password=\u0026lt;PASSWORD\u0026gt; --docker-email=\u0026lt;EMAIL\u0026gt; -n \u0026lt;NAMESPACE\u0026gt; Setup The first step is to clone this repository. Next is the Secret for the AWS API credentials of the user that has full access to our S3 bucket. Copy the configmap_secrets_template.yaml to configmap_secrets.yaml and place your secrets at the right place\napiVersion:v1kind:ConfigMapmetadata:name:s3-configdata:S3_BUCKET:\u0026lt;YOUR-S3-BUCKET-NAME\u0026gt; AWS_KEY: \u0026lt;YOUR-AWS-TECH-USER-ACCESS-KEY\u0026gt;AWS_SECRET_KEY:\u0026lt;YOUR-AWS-TECH-USER-SECRET\u0026gt;Build and deploy Change the settings in the build.sh file with your docker registry settings.\n#!/usr/bin/env bash  ######################################################################################################################## # PREREQUISTITS ######################################################################################################################## # # - ensure that you have a valid Artifactory or other Docker registry account # - Create your image pull secret in your namespace # kubectl create secret docker-registry artifactory --docker-server=\u0026lt;YOUR-REGISTRY\u0026gt;.docker.repositories.sap.ondemand.com --docker-username=\u0026lt;USERNAME\u0026gt; --docker-password=\u0026lt;PASSWORD\u0026gt; --docker-email=\u0026lt;EMAIL\u0026gt; -n \u0026lt;NAMESPACE\u0026gt; # - change the settings below arcording your settings # # usage: # Call this script with the version to build and push to the registry. After build/push the # yaml/* files are deployed into your cluster # # ./build.sh 1.0 # VERSION=$1 PROJECT=kube-s3 REPOSITORY=cp-enablement.docker.repositories.sap.ondemand.com # causes the shell to exit if any subcommand or pipeline returns a non-zero status. set -e # set debug mode #set -x . . . . Create the S3Fuse Pod and check the status:\n# build and push the image to your docker registry ./build.sh 1.0 # check that the pods are up and running kubectl get pods Check success Create a demo Pod and check the status:\nkubectl apply -f ./yaml/example_pod.yaml # wait some second to get the pod up and running... kubectl get pods # go into the pd and check that the /var/s3 is mounted with your S3 bucket content inside kubectl exec -ti test-pd sh # inside the pod ls -la /var/s3 Why does this work? Docker engine 1.10 added a new feature which allows containers to share the host mount namespace. This feature makes it possible to mount a s3fs container file system to a host file system through a shared mount, providing a persistent network storage with S3 backend.\nThe key part is mountPath: /var/s3:shared which enables the volume to be mounted as shared inside the pod. When the container starts it will mount the S3 bucket onto /var/s3 and consequently the data will be available under /mnt/data-s3fs on the host and thus to any other container/pod running on it (and has /mnt/data-s3fs mounted too).\n"},{"uri":"https://gardener.cloud/v1.13.2/tutorials/s3/","title":"Shared storage with S3 backend","tags":[],"description":"Using S3 bucket as shared storage for pods","content":"Shared storage with S3 backend The storage is definitely the most complex and important part of an application setup, once this part is completed, 80% of the tasks are completed.\nMounting an S3 bucket into a pod using FUSE allows you to access the data as if it were on the local disk. The mount is a pointer to an S3 location, so the data is never synced locally. Once mounted, any pod can read or even write from that directory without the need for explicit keys.\nHowever, it can be used to import and parse large amounts of data into a database.\nOverview Limitations Generally S3 cannot offer the same performance or semantics as a local file system. More specifically:\n random writes or appends to files require rewriting the entire file metadata operations such as listing directories have poor performance due to network latency eventual consistency can temporarily yield stale data(Amazon S3 Data Consistency Model) no atomic renames of files or directories no coordination between multiple clients mounting the same bucket no hard links  Before you Begin You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by using the Gardener.\nEnsure that you have create the \u0026ldquo;imagePullSecret\u0026rdquo; in your cluster.\nkubectl create secret docker-registry artifactory --docker-server=\u0026lt;YOUR-REGISTRY\u0026gt;.docker.repositories.sap.ondemand.com --docker-username=\u0026lt;USERNAME\u0026gt; --docker-password=\u0026lt;PASSWORD\u0026gt; --docker-email=\u0026lt;EMAIL\u0026gt; -n \u0026lt;NAMESPACE\u0026gt; Setup The first step is to clone this repository. Next is the Secret for the AWS API credentials of the user that has full access to our S3 bucket. Copy the configmap_secrets_template.yaml to configmap_secrets.yaml and place your secrets at the right place\napiVersion:v1kind:ConfigMapmetadata:name:s3-configdata:S3_BUCKET:\u0026lt;YOUR-S3-BUCKET-NAME\u0026gt; AWS_KEY: \u0026lt;YOUR-AWS-TECH-USER-ACCESS-KEY\u0026gt;AWS_SECRET_KEY:\u0026lt;YOUR-AWS-TECH-USER-SECRET\u0026gt;Build and deploy Change the settings in the build.sh file with your docker registry settings.\n#!/usr/bin/env bash  ######################################################################################################################## # PREREQUISTITS ######################################################################################################################## # # - ensure that you have a valid Artifactory or other Docker registry account # - Create your image pull secret in your namespace # kubectl create secret docker-registry artifactory --docker-server=\u0026lt;YOUR-REGISTRY\u0026gt;.docker.repositories.sap.ondemand.com --docker-username=\u0026lt;USERNAME\u0026gt; --docker-password=\u0026lt;PASSWORD\u0026gt; --docker-email=\u0026lt;EMAIL\u0026gt; -n \u0026lt;NAMESPACE\u0026gt; # - change the settings below arcording your settings # # usage: # Call this script with the version to build and push to the registry. After build/push the # yaml/* files are deployed into your cluster # # ./build.sh 1.0 # VERSION=$1 PROJECT=kube-s3 REPOSITORY=cp-enablement.docker.repositories.sap.ondemand.com # causes the shell to exit if any subcommand or pipeline returns a non-zero status. set -e # set debug mode #set -x . . . . Create the S3Fuse Pod and check the status:\n# build and push the image to your docker registry ./build.sh 1.0 # check that the pods are up and running kubectl get pods Check success Create a demo Pod and check the status:\nkubectl apply -f ./yaml/example_pod.yaml # wait some second to get the pod up and running... kubectl get pods # go into the pd and check that the /var/s3 is mounted with your S3 bucket content inside kubectl exec -ti test-pd sh # inside the pod ls -la /var/s3 Why does this work? Docker engine 1.10 added a new feature which allows containers to share the host mount namespace. This feature makes it possible to mount a s3fs container file system to a host file system through a shared mount, providing a persistent network storage with S3 backend.\nThe key part is mountPath: /var/s3:shared which enables the volume to be mounted as shared inside the pod. When the container starts it will mount the S3 bucket onto /var/s3 and consequently the data will be available under /mnt/data-s3fs on the host and thus to any other container/pod running on it (and has /mnt/data-s3fs mounted too).\n"},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/maintain-shoot/","title":"Shoot Cluster Maintenance","tags":[],"description":"Understanding and configuring Gardener&#39;s Day-2 operations for Shoot clusters.","content":"Shoot Cluster Maintenance Day two operations for shoot clusters are related to:\n The Kubernetes version of the control plane and the worker nodes the operating system version of the worker nodes   When referring to an update of the \"operating system version\" in this document, the update of the machine image of the shoot cluster's worker nodes is meant. For example, Amazon Machine Images (AMI) for AWS.  The following table summarizes what options Gardener offers to maintain these versions:\n    Auto-Update Forceful Updates Manual updates     Kubernetes version Patches only Patches and consecutive minor updates only yes   Operating system version yes yes yes    Allowed Target Versions in the CloudProfile Administrators maintain the allowed target versions that you can update to in the CloudProfile for each IaaS-Provider. Users with access to a Gardener project can check supported target versions with:\nkubectl get cloudprofile [IAAS-SPECIFIC-PROFILE] -o yaml    Path Description More information     spec.kubernetes.versions The supported Kubernetes version major.minor.patch. Patch releases   spec.machineImages The supported operating system versions for worker nodes.     Both the Kubernetes version, and the operating system version follow semantic versioning that allows Gardener to handle updates automatically.\nMore information: Semantic Versioning.\nImpact of Version Classifications on Updates Gardener allows to classify versions in the CloudProfile as preview, supported, deprecated, or expired. During maintenance operations, preview versions are excluded from updates, because theyre often recently released versions that havent yet undergone thorough testing and may contain bugs or security issues.\nMore information: Version Classifications.\nLet Gardener manage your updates The Maintenance Window Gardener can manage updates for you automatically. It offers users to specify a maintenance window during which updates are scheduled:\n The time interval of the maintenance window cant be less than 30 minutes or more than 6 hours. If theres no maintenance window specified during the creation of a shoot cluster, Gardener chooses a maintenance window randomly to spread the load.  You can either specify the maintenance window in the shoot cluster specification (.spec.maintenance.timeWindow) or the start time of the maintenance window using the Gardener dashboard (CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; OVERVIEW \u0026gt; Lifecycle \u0026gt; Maintenance).\nAuto-Update and Forceful Updates To trigger updates during the maintenance window automatically, Gardener offers the following methods:\n  Auto-update: Gardener starts an update during the next maintenance window whenever theres a version available in the CloudProfile that is higher than the one of your shoot cluster specification, and that isnt classified as preview version. For Kubernetes versions, auto-update only updates to higher patch levels.\nYou can either activate auto-update on the Gardener dashboard (CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; OVERVIEW \u0026gt; Lifecycle \u0026gt; Maintenance) or in the shoot cluster specification:\n .spec.maintenance.autoUpdate.kubernetesVersion: true .spec.maintenance.autoUpdate.machineImageVersion: true    Forceful updates: In the maintenance window, Gardener compares the current version given in the shoot cluster specification with the version list in the CloudProfile. If the version has an expiration date and if the date is before the start of the maintenance window, Gardener starts an update to the highest version available in the CloudProfile that isnt classified as preview version. The highest version in CloudProfile cant have an expiration date. For Kubernetes versions, Gardener only updates to higher patch levels or consecutive minor versions.\n  If you dont want to wait for the next maintenance window, you can annotate the shoot cluster specification with shoot.gardener.cloud/operation: maintain. Gardener then checks immediately if theres an auto-update or a forceful update needed.\n Forceful version updates are even executed if the auto-update for the Kubernetes version, or the auto-update for the machine image version is deactivated (set to `false`).  With expiration dates, administrators can give shoot cluster owners more time for testing before the actual version update happens, which allows smoother transitions to new versions.\nKubernetes Update Paths The bigger the delta of the Kubernetes source version and the Kubernetes target version, the better it must be planned and executed by operators. Gardener only provides automatic support for updates that can be applied safely to the cluster workload:\n   Update Type Example Update method     Patches 1.10.12 to 1.10.13 auto-update or Forceful update   Update to consecutive minor version 1.10.12 to 1.11.10 Forceful update   Other 1.10.12 to 1.12.0 manual update    Gardener doesnt support automatic updates of nonconsecutive minor versions, because Kubernetes doesnt guarantee updateability in this case. However, multiple minor version updates are possible if not only the minor source version is expired, but also the minor target version is expired. Gardener then updates the Kubernetes version first to the expired target version, and waits for the next maintenance window to update this version to the next minor target version.\n The administrator who maintains the `CloudProfile` has to ensure that the list of Kubernetes versions consists of consecutive minor versions, for example, from `1.10.x` to `1.11.y`. If the minor version increases in bigger steps, for example, from `1.10.x` to `1.12.y`, shoot cluster updates fail during the maintenance window.  Manual Updates To update the Kubernetes version or the node operating system manually, change the .spec.kubernetes.version field or the .spec.provider.workers.machine.image.version field correspondingly.\nManual updates are required if you would like to do a minor update of the Kubernetes version. Gardener doesnt do such updates automatically as they can have breaking changes that could impact the cluster workload.\nManual updates are either executed immediately (default) or can be confined to the maintenance time window.\nChoosing the latter option, causes changes to the cluster (for example, node pool rolling-updates) and the subsequent reconciliation, to only predictably happen during a defined time window (available since Gardener version 1.4).\nMore information: Confine Specification Changes/Update Roll Out.\n Before applying such update on minor or major releases, operators should check for all the breaking changes introduced in the target Kubernetes release changelog.  Examples In the examples for the CloudProfile and the shoot cluster specification, only the fields relevant for the example are shown.\nAuto-Update of Kubernetes Version Let\u0026rsquo;s assume Kubernetes version 1.10.5 and 1.11.0 were added in the following CloudProfile:\nspec:kubernetes:versions:- version:1.11.0- version:1.10.5- version:1.10.0Before this change, the shoot cluster specification looked like this:\nspec:kubernetes:version:1.10.0maintenance:timeWindow:begin:220000+0000end:230000+0000autoUpdate:kubernetesVersion:trueAs a consequence, the shoot cluster is updated to Kubernetes version 1.10.5 between 22:00-23:00 UTC. Your shoot cluster isn\u0026rsquo;t updated automatically to 1.11.0 even though it\u0026rsquo;s the highest Kubernetes version in the CloudProfile, because Gardener does only do automatic updates of the Kubernetes patch level.\nForceful Update Due to Expired Kubernetes Version Let\u0026rsquo;s assume the following CloudProfile:\nspec:kubernetes:versions:- version:1.12.8- version:1.11.10- version:1.10.13- version:1.10.12expirationDate:\u0026#34;2019-04-13T08:00:00Z\u0026#34;Let\u0026rsquo;s assume the shoot cluster has the following specification:\nspec:kubernetes:version:1.10.12maintenance:timeWindow:begin:220000+0100end:230000+0100autoUpdate:kubernetesVersion:falseThe shoot cluster specification refers a Kubernetes version that has an expirationDate. In the maintenance window on 2019-04-12, the Kubernetes version stays the same as its still not expired. But in the maintenance window on 2019-04-14 the Kubernetes version of the shoot cluster is updated to 1.10.13 (independently of the value of .spec.maintenance.autoUpdate.kubernetesVersion).\nForceful Update to New Minor Kubernetes Version Let\u0026rsquo;s assume the following CloudProfile:\nspec:kubernetes:versions:- version:1.12.8- version:1.11.10- version:1.11.09- version:1.10.12expirationDate:\u0026#34;2019-04-13T08:00:00Z\u0026#34;Let\u0026rsquo;s assume the shoot cluster has the following specification:\nspec:kubernetes:version:1.10.12maintenance:timeWindow:begin:220000+0100end:230000+0100autoUpdate:kubernetesVersion:falseThe shoot cluster specification refers a Kubernetes version that has an expirationDate. In the maintenance window on 2019-04-14, the Kubernetes version of the shoot cluster is updated to 1.11.10, which is the highest patch version of minor target version 1.11 that follows source version 1.10.\nAutomatic Update from Expired Machine Image Version Let\u0026rsquo;s assume the following CloudProfile:\nspec:machineImages:- name:coreosversions:- version:2191.5.0- version:2191.4.1- version:2135.6.0expirationDate:\u0026#34;2019-04-13T08:00:00Z\u0026#34;Let\u0026rsquo;s assume the shoot cluster has the following specification:\nspec:provider:type:awsworkers:- name:namemaximum:1minimum:1maxSurge:1maxUnavailable:0image:name:coreosversion:2135.6.0type:m5.largevolume:type:gp2size:20Gimaintenance:timeWindow:begin:220000+0100end:230000+0100autoUpdate:machineImageVersion:falseThe shoot cluster specification refers a machine image version that has an expirationDate. In the maintenance window on 2019-04-12, the machine image version stays the same as its still not expired. But in the maintenance window on 2019-04-14 the machine image version of the shoot cluster is updated to 2191.5.0 (independently of the value of .spec.maintenance.autoUpdate.machineImageVersion) as version 2135.6.0 is expired.\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/administer_shoots/maintain-shoot/","title":"Shoot Cluster Maintenance","tags":[],"description":"Understanding and configuring Gardener&#39;s Day-2 operations for Shoot clusters.","content":"Shoot Cluster Maintenance Day two operations for shoot clusters are related to:\n The Kubernetes version of the control plane and the worker nodes the operating system version of the worker nodes   When referring to an update of the \"operating system version\" in this document, the update of the machine image of the shoot cluster's worker nodes is meant. For example, Amazon Machine Images (AMI) for AWS.  The following table summarizes what options Gardener offers to maintain these versions:\n    Auto-Update Forceful Updates Manual updates     Kubernetes version Patches only Patches and consecutive minor updates only yes   Operating system version yes yes yes    Allowed Target Versions in the CloudProfile Administrators maintain the allowed target versions that you can update to in the CloudProfile for each IaaS-Provider. Users with access to a Gardener project can check supported target versions with:\nkubectl get cloudprofile [IAAS-SPECIFIC-PROFILE] -o yaml    Path Description More information     spec.kubernetes.versions The supported Kubernetes version major.minor.patch. Patch releases   spec.machineImages The supported operating system versions for worker nodes.     Both the Kubernetes version, and the operating system version follow semantic versioning that allows Gardener to handle updates automatically.\nMore information: Semantic Versioning.\nImpact of Version Classifications on Updates Gardener allows to classify versions in the CloudProfile as preview, supported, deprecated, or expired. During maintenance operations, preview versions are excluded from updates, because theyre often recently released versions that havent yet undergone thorough testing and may contain bugs or security issues.\nMore information: Version Classifications.\nLet Gardener manage your updates The Maintenance Window Gardener can manage updates for you automatically. It offers users to specify a maintenance window during which updates are scheduled:\n The time interval of the maintenance window cant be less than 30 minutes or more than 6 hours. If theres no maintenance window specified during the creation of a shoot cluster, Gardener chooses a maintenance window randomly to spread the load.  You can either specify the maintenance window in the shoot cluster specification (.spec.maintenance.timeWindow) or the start time of the maintenance window using the Gardener dashboard (CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; OVERVIEW \u0026gt; Lifecycle \u0026gt; Maintenance).\nAuto-Update and Forceful Updates To trigger updates during the maintenance window automatically, Gardener offers the following methods:\n  Auto-update: Gardener starts an update during the next maintenance window whenever theres a version available in the CloudProfile that is higher than the one of your shoot cluster specification, and that isnt classified as preview version. For Kubernetes versions, auto-update only updates to higher patch levels.\nYou can either activate auto-update on the Gardener dashboard (CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; OVERVIEW \u0026gt; Lifecycle \u0026gt; Maintenance) or in the shoot cluster specification:\n .spec.maintenance.autoUpdate.kubernetesVersion: true .spec.maintenance.autoUpdate.machineImageVersion: true    Forceful updates: In the maintenance window, Gardener compares the current version given in the shoot cluster specification with the version list in the CloudProfile. If the version has an expiration date and if the date is before the start of the maintenance window, Gardener starts an update to the highest version available in the CloudProfile that isnt classified as preview version. The highest version in CloudProfile cant have an expiration date. For Kubernetes versions, Gardener only updates to higher patch levels or consecutive minor versions.\n  If you dont want to wait for the next maintenance window, you can annotate the shoot cluster specification with shoot.gardener.cloud/operation: maintain. Gardener then checks immediately if theres an auto-update or a forceful update needed.\n Forceful version updates are even executed if the auto-update for the Kubernetes version, or the auto-update for the machine image version is deactivated (set to `false`).  With expiration dates, administrators can give shoot cluster owners more time for testing before the actual version update happens, which allows smoother transitions to new versions.\nKubernetes Update Paths The bigger the delta of the Kubernetes source version and the Kubernetes target version, the better it must be planned and executed by operators. Gardener only provides automatic support for updates that can be applied safely to the cluster workload:\n   Update Type Example Update method     Patches 1.10.12 to 1.10.13 auto-update or Forceful update   Update to consecutive minor version 1.10.12 to 1.11.10 Forceful update   Other 1.10.12 to 1.12.0 manual update    Gardener doesnt support automatic updates of nonconsecutive minor versions, because Kubernetes doesnt guarantee updateability in this case. However, multiple minor version updates are possible if not only the minor source version is expired, but also the minor target version is expired. Gardener then updates the Kubernetes version first to the expired target version, and waits for the next maintenance window to update this version to the next minor target version.\n The administrator who maintains the `CloudProfile` has to ensure that the list of Kubernetes versions consists of consecutive minor versions, for example, from `1.10.x` to `1.11.y`. If the minor version increases in bigger steps, for example, from `1.10.x` to `1.12.y`, shoot cluster updates fail during the maintenance window.  Manual Updates To update the Kubernetes version or the node operating system manually, change the .spec.kubernetes.version field or the .spec.provider.workers.machine.image.version field correspondingly.\nManual updates are required if you would like to do a minor update of the Kubernetes version. Gardener doesnt do such updates automatically as they can have breaking changes that could impact the cluster workload.\nManual updates are either executed immediately (default) or can be confined to the maintenance time window.\nChoosing the latter option, causes changes to the cluster (for example, node pool rolling-updates) and the subsequent reconciliation, to only predictably happen during a defined time window (available since Gardener version 1.4).\nMore information: Confine Specification Changes/Update Roll Out.\n Before applying such update on minor or major releases, operators should check for all the breaking changes introduced in the target Kubernetes release changelog.  Examples In the examples for the CloudProfile and the shoot cluster specification, only the fields relevant for the example are shown.\nAuto-Update of Kubernetes Version Let\u0026rsquo;s assume Kubernetes version 1.10.5 and 1.11.0 were added in the following CloudProfile:\nspec:kubernetes:versions:- version:1.11.0- version:1.10.5- version:1.10.0Before this change, the shoot cluster specification looked like this:\nspec:kubernetes:version:1.10.0maintenance:timeWindow:begin:220000+0000end:230000+0000autoUpdate:kubernetesVersion:trueAs a consequence, the shoot cluster is updated to Kubernetes version 1.10.5 between 22:00-23:00 UTC. Your shoot cluster isn\u0026rsquo;t updated automatically to 1.11.0 even though it\u0026rsquo;s the highest Kubernetes version in the CloudProfile, because Gardener does only do automatic updates of the Kubernetes patch level.\nForceful Update Due to Expired Kubernetes Version Let\u0026rsquo;s assume the following CloudProfile:\nspec:kubernetes:versions:- version:1.12.8- version:1.11.10- version:1.10.13- version:1.10.12expirationDate:\u0026#34;2019-04-13T08:00:00Z\u0026#34;Let\u0026rsquo;s assume the shoot cluster has the following specification:\nspec:kubernetes:version:1.10.12maintenance:timeWindow:begin:220000+0100end:230000+0100autoUpdate:kubernetesVersion:falseThe shoot cluster specification refers a Kubernetes version that has an expirationDate. In the maintenance window on 2019-04-12, the Kubernetes version stays the same as its still not expired. But in the maintenance window on 2019-04-14 the Kubernetes version of the shoot cluster is updated to 1.10.13 (independently of the value of .spec.maintenance.autoUpdate.kubernetesVersion).\nForceful Update to New Minor Kubernetes Version Let\u0026rsquo;s assume the following CloudProfile:\nspec:kubernetes:versions:- version:1.12.8- version:1.11.10- version:1.11.09- version:1.10.12expirationDate:\u0026#34;2019-04-13T08:00:00Z\u0026#34;Let\u0026rsquo;s assume the shoot cluster has the following specification:\nspec:kubernetes:version:1.10.12maintenance:timeWindow:begin:220000+0100end:230000+0100autoUpdate:kubernetesVersion:falseThe shoot cluster specification refers a Kubernetes version that has an expirationDate. In the maintenance window on 2019-04-14, the Kubernetes version of the shoot cluster is updated to 1.11.10, which is the highest patch version of minor target version 1.11 that follows source version 1.10.\nAutomatic Update from Expired Machine Image Version Let\u0026rsquo;s assume the following CloudProfile:\nspec:machineImages:- name:coreosversions:- version:2191.5.0- version:2191.4.1- version:2135.6.0expirationDate:\u0026#34;2019-04-13T08:00:00Z\u0026#34;Let\u0026rsquo;s assume the shoot cluster has the following specification:\nspec:provider:type:awsworkers:- name:namemaximum:1minimum:1maxSurge:1maxUnavailable:0image:name:coreosversion:2135.6.0type:m5.largevolume:type:gp2size:20Gimaintenance:timeWindow:begin:220000+0100end:230000+0100autoUpdate:machineImageVersion:falseThe shoot cluster specification refers a machine image version that has an expirationDate. In the maintenance window on 2019-04-12, the machine image version stays the same as its still not expired. But in the maintenance window on 2019-04-14 the machine image version of the shoot cluster is updated to 2191.5.0 (independently of the value of .spec.maintenance.autoUpdate.machineImageVersion) as version 2135.6.0 is expired.\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/administer_shoots/maintain-shoot/","title":"Shoot Cluster Maintenance","tags":[],"description":"Understanding and configuring Gardener&#39;s Day-2 operations for Shoot clusters.","content":"Shoot Cluster Maintenance Day two operations for shoot clusters are related to:\n The Kubernetes version of the control plane and the worker nodes the operating system version of the worker nodes   When referring to an update of the \"operating system version\" in this document, the update of the machine image of the shoot cluster's worker nodes is meant. For example, Amazon Machine Images (AMI) for AWS.  The following table summarizes what options Gardener offers to maintain these versions:\n    Auto-Update Forceful Updates Manual updates     Kubernetes version Patches only Patches and consecutive minor updates only yes   Operating system version yes yes yes    Allowed Target Versions in the CloudProfile Administrators maintain the allowed target versions that you can update to in the CloudProfile for each IaaS-Provider. Users with access to a Gardener project can check supported target versions with:\nkubectl get cloudprofile [IAAS-SPECIFIC-PROFILE] -o yaml    Path Description More information     spec.kubernetes.versions The supported Kubernetes version major.minor.patch. Patch releases   spec.machineImages The supported operating system versions for worker nodes.     Both the Kubernetes version, and the operating system version follow semantic versioning that allows Gardener to handle updates automatically.\nMore information: Semantic Versioning.\nImpact of Version Classifications on Updates Gardener allows to classify versions in the CloudProfile as preview, supported, deprecated, or expired. During maintenance operations, preview versions are excluded from updates, because theyre often recently released versions that havent yet undergone thorough testing and may contain bugs or security issues.\nMore information: Version Classifications.\nLet Gardener manage your updates The Maintenance Window Gardener can manage updates for you automatically. It offers users to specify a maintenance window during which updates are scheduled:\n The time interval of the maintenance window cant be less than 30 minutes or more than 6 hours. If theres no maintenance window specified during the creation of a shoot cluster, Gardener chooses a maintenance window randomly to spread the load.  You can either specify the maintenance window in the shoot cluster specification (.spec.maintenance.timeWindow) or the start time of the maintenance window using the Gardener dashboard (CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; OVERVIEW \u0026gt; Lifecycle \u0026gt; Maintenance).\nAuto-Update and Forceful Updates To trigger updates during the maintenance window automatically, Gardener offers the following methods:\n  Auto-update: Gardener starts an update during the next maintenance window whenever theres a version available in the CloudProfile that is higher than the one of your shoot cluster specification, and that isnt classified as preview version. For Kubernetes versions, auto-update only updates to higher patch levels.\nYou can either activate auto-update on the Gardener dashboard (CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; OVERVIEW \u0026gt; Lifecycle \u0026gt; Maintenance) or in the shoot cluster specification:\n .spec.maintenance.autoUpdate.kubernetesVersion: true .spec.maintenance.autoUpdate.machineImageVersion: true    Forceful updates: In the maintenance window, Gardener compares the current version given in the shoot cluster specification with the version list in the CloudProfile. If the version has an expiration date and if the date is before the start of the maintenance window, Gardener starts an update to the highest version available in the CloudProfile that isnt classified as preview version. The highest version in CloudProfile cant have an expiration date. For Kubernetes versions, Gardener only updates to higher patch levels or consecutive minor versions.\n  If you dont want to wait for the next maintenance window, you can annotate the shoot cluster specification with shoot.gardener.cloud/operation: maintain. Gardener then checks immediately if theres an auto-update or a forceful update needed.\n Forceful version updates are even executed if the auto-update for the Kubernetes version, or the auto-update for the machine image version is deactivated (set to `false`).  With expiration dates, administrators can give shoot cluster owners more time for testing before the actual version update happens, which allows smoother transitions to new versions.\nKubernetes Update Paths The bigger the delta of the Kubernetes source version and the Kubernetes target version, the better it must be planned and executed by operators. Gardener only provides automatic support for updates that can be applied safely to the cluster workload:\n   Update Type Example Update method     Patches 1.10.12 to 1.10.13 auto-update or Forceful update   Update to consecutive minor version 1.10.12 to 1.11.10 Forceful update   Other 1.10.12 to 1.12.0 manual update    Gardener doesnt support automatic updates of nonconsecutive minor versions, because Kubernetes doesnt guarantee updateability in this case. However, multiple minor version updates are possible if not only the minor source version is expired, but also the minor target version is expired. Gardener then updates the Kubernetes version first to the expired target version, and waits for the next maintenance window to update this version to the next minor target version.\n The administrator who maintains the `CloudProfile` has to ensure that the list of Kubernetes versions consists of consecutive minor versions, for example, from `1.10.x` to `1.11.y`. If the minor version increases in bigger steps, for example, from `1.10.x` to `1.12.y`, shoot cluster updates fail during the maintenance window.  Manual Updates To update the Kubernetes version or the node operating system manually, change the .spec.kubernetes.version field or the .spec.provider.workers.machine.image.version field correspondingly.\nManual updates are required if you would like to do a minor update of the Kubernetes version. Gardener doesnt do such updates automatically as they can have breaking changes that could impact the cluster workload.\nManual updates are either executed immediately (default) or can be confined to the maintenance time window.\nChoosing the latter option, causes changes to the cluster (for example, node pool rolling-updates) and the subsequent reconciliation, to only predictably happen during a defined time window (available since Gardener version 1.4).\nMore information: Confine Specification Changes/Update Roll Out.\n Before applying such update on minor or major releases, operators should check for all the breaking changes introduced in the target Kubernetes release changelog.  Examples In the examples for the CloudProfile and the shoot cluster specification, only the fields relevant for the example are shown.\nAuto-Update of Kubernetes Version Let\u0026rsquo;s assume Kubernetes version 1.10.5 and 1.11.0 were added in the following CloudProfile:\nspec:kubernetes:versions:- version:1.11.0- version:1.10.5- version:1.10.0Before this change, the shoot cluster specification looked like this:\nspec:kubernetes:version:1.10.0maintenance:timeWindow:begin:220000+0000end:230000+0000autoUpdate:kubernetesVersion:trueAs a consequence, the shoot cluster is updated to Kubernetes version 1.10.5 between 22:00-23:00 UTC. Your shoot cluster isn\u0026rsquo;t updated automatically to 1.11.0 even though it\u0026rsquo;s the highest Kubernetes version in the CloudProfile, because Gardener does only do automatic updates of the Kubernetes patch level.\nForceful Update Due to Expired Kubernetes Version Let\u0026rsquo;s assume the following CloudProfile:\nspec:kubernetes:versions:- version:1.12.8- version:1.11.10- version:1.10.13- version:1.10.12expirationDate:\u0026#34;2019-04-13T08:00:00Z\u0026#34;Let\u0026rsquo;s assume the shoot cluster has the following specification:\nspec:kubernetes:version:1.10.12maintenance:timeWindow:begin:220000+0100end:230000+0100autoUpdate:kubernetesVersion:falseThe shoot cluster specification refers a Kubernetes version that has an expirationDate. In the maintenance window on 2019-04-12, the Kubernetes version stays the same as its still not expired. But in the maintenance window on 2019-04-14 the Kubernetes version of the shoot cluster is updated to 1.10.13 (independently of the value of .spec.maintenance.autoUpdate.kubernetesVersion).\nForceful Update to New Minor Kubernetes Version Let\u0026rsquo;s assume the following CloudProfile:\nspec:kubernetes:versions:- version:1.12.8- version:1.11.10- version:1.11.09- version:1.10.12expirationDate:\u0026#34;2019-04-13T08:00:00Z\u0026#34;Let\u0026rsquo;s assume the shoot cluster has the following specification:\nspec:kubernetes:version:1.10.12maintenance:timeWindow:begin:220000+0100end:230000+0100autoUpdate:kubernetesVersion:falseThe shoot cluster specification refers a Kubernetes version that has an expirationDate. In the maintenance window on 2019-04-14, the Kubernetes version of the shoot cluster is updated to 1.11.10, which is the highest patch version of minor target version 1.11 that follows source version 1.10.\nAutomatic Update from Expired Machine Image Version Let\u0026rsquo;s assume the following CloudProfile:\nspec:machineImages:- name:coreosversions:- version:2191.5.0- version:2191.4.1- version:2135.6.0expirationDate:\u0026#34;2019-04-13T08:00:00Z\u0026#34;Let\u0026rsquo;s assume the shoot cluster has the following specification:\nspec:provider:type:awsworkers:- name:namemaximum:1minimum:1maxSurge:1maxUnavailable:0image:name:coreosversion:2135.6.0type:m5.largevolume:type:gp2size:20Gimaintenance:timeWindow:begin:220000+0100end:230000+0100autoUpdate:machineImageVersion:falseThe shoot cluster specification refers a machine image version that has an expirationDate. In the maintenance window on 2019-04-12, the machine image version stays the same as its still not expired. But in the maintenance window on 2019-04-14 the machine image version of the shoot cluster is updated to 2191.5.0 (independently of the value of .spec.maintenance.autoUpdate.machineImageVersion) as version 2135.6.0 is expired.\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/shoot-maintenance/","title":"Shoot maintenance","tags":[],"description":"","content":"Shoot maintenance There is a general document about shoot maintenance that you might want to read. Here, we describe how you can influence certain operations that happen during a shoot maintenance.\nRestart Control Plane Controllers As outlined in above linked document, Gardener offers to restart certain control plane controllers running in the seed during a shoot maintenance.\nExtension controllers can extend the amount of pods being affected by these restarts. If your Gardener extension manages pods of a shoot\u0026rsquo;s control plane (shoot namespace in seed) and it could potentially profit from a regular restart please consider labeling it with maintenance.gardener.cloud/restart=true.\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/shoot-maintenance/","title":"Shoot maintenance","tags":[],"description":"","content":"Shoot maintenance There is a general document about shoot maintenance that you might want to read. Here, we describe how you can influence certain operations that happen during a shoot maintenance.\nRestart Control Plane Controllers As outlined in above linked document, Gardener offers to restart certain control plane controllers running in the seed during a shoot maintenance.\nExtension controllers can extend the amount of pods being affected by these restarts. If your Gardener extension manages pods of a shoot\u0026rsquo;s control plane (shoot namespace in seed) and it could potentially profit from a regular restart please consider labeling it with maintenance.gardener.cloud/restart=true.\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/shoot-maintenance/","title":"Shoot maintenance","tags":[],"description":"","content":"Shoot maintenance There is a general document about shoot maintenance that you might want to read. Here, we describe how you can influence certain operations that happen during a shoot maintenance.\nRestart Control Plane Controllers As outlined in above linked document, Gardener offers to restart certain control plane controllers running in the seed during a shoot maintenance.\nExtension controllers can extend the amount of pods being affected by these restarts. If your Gardener extension manages pods of a shoot\u0026rsquo;s control plane (shoot namespace in seed) and it could potentially profit from a regular restart please consider labeling it with maintenance.gardener.cloud/restart=true.\n"},{"uri":"https://gardener.cloud/documentation/concepts/extensions/shoot-webhooks/","title":"Shoot resource customization webhooks","tags":[],"description":"","content":"Shoot resource customization webhooks Gardener deploys several components/resources into the shoot cluster. Some of these resources are essential (like the kube-proxy), others are optional addons (like the kubernetes-dashboard or the nginx-ingress-controller). In either case, some provider extensions might need to mutate these resources and inject provider-specific bits into it.\nWhat\u0026rsquo;s the approach to implement such mutations? Similar to how control plane components in the seed are modified we are using MutatingWebhookConfigurations to achieve the same for resources in the shoot. Both, the provider extension and the kube-apiserver of the shoot cluster are running in the same seed. Consequently, the kube-apiserver can talk cluster-internally to the provider extension webhook which makes such operations even faster.\nHow is the MutatingWebhookConfiguration object created in the shoot? The preferred approach is to use a ManagedResource (see also this document) in the seed cluster. This way the gardener-resource-manager ensures that end-users cannot delete/modify the webhook configuration. The provider extension doesn\u0026rsquo;t need to care about the same.\nWhat else is needed? The shoot\u0026rsquo;s kube-apiserver must be allowed to talk to the provider extension. To achieve this you need to create a NetworkPolicy in the shoot namespace. Our extension controller library provides easy-to-use utilities and hooks to implement such a webhook. Please find an exemplary implementation here and here.\n"},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/shoot-webhooks/","title":"Shoot resource customization webhooks","tags":[],"description":"","content":"Shoot resource customization webhooks Gardener deploys several components/resources into the shoot cluster. Some of these resources are essential (like the kube-proxy), others are optional addons (like the kubernetes-dashboard or the nginx-ingress-controller). In either case, some provider extensions might need to mutate these resources and inject provider-specific bits into it.\nWhat\u0026rsquo;s the approach to implement such mutations? Similar to how control plane components in the seed are modified we are using MutatingWebhookConfigurations to achieve the same for resources in the shoot. Both, the provider extension and the kube-apiserver of the shoot cluster are running in the same seed. Consequently, the kube-apiserver can talk cluster-internally to the provider extension webhook which makes such operations even faster.\nHow is the MutatingWebhookConfiguration object created in the shoot? The preferred approach is to use a ManagedResource (see also this document) in the seed cluster. This way the gardener-resource-manager ensures that end-users cannot delete/modify the webhook configuration. The provider extension doesn\u0026rsquo;t need to care about the same.\nWhat else is needed? The shoot\u0026rsquo;s kube-apiserver must be allowed to talk to the provider extension. To achieve this you need to create a NetworkPolicy in the shoot namespace. Our extension controller library provides easy-to-use utilities and hooks to implement such a webhook. Please find an exemplary implementation here and here.\n"},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/shoot-webhooks/","title":"Shoot resource customization webhooks","tags":[],"description":"","content":"Shoot resource customization webhooks Gardener deploys several components/resources into the shoot cluster. Some of these resources are essential (like the kube-proxy), others are optional addons (like the kubernetes-dashboard or the nginx-ingress-controller). In either case, some provider extensions might need to mutate these resources and inject provider-specific bits into it.\nWhat\u0026rsquo;s the approach to implement such mutations? Similar to how control plane components in the seed are modified we are using MutatingWebhookConfigurations to achieve the same for resources in the shoot. Both, the provider extension and the kube-apiserver of the shoot cluster are running in the same seed. Consequently, the kube-apiserver can talk cluster-internally to the provider extension webhook which makes such operations even faster.\nHow is the MutatingWebhookConfiguration object created in the shoot? The preferred approach is to use a ManagedResource (see also this document) in the seed cluster. This way the gardener-resource-manager ensures that end-users cannot delete/modify the webhook configuration. The provider extension doesn\u0026rsquo;t need to care about the same.\nWhat else is needed? The shoot\u0026rsquo;s kube-apiserver must be allowed to talk to the provider extension. To achieve this you need to create a NetworkPolicy in the shoot namespace. Our extension controller library provides easy-to-use utilities and hooks to implement such a webhook. Please find an exemplary implementation here and here.\n"},{"uri":"https://gardener.cloud/documentation/guides/applications/commit_secret_fail/","title":"Storing secrets in git ","tags":[],"description":"Never ever commit a kubeconfig.yaml into github","content":"Problem If you commit sensitive data, such as a kubeconfig.yaml or SSH key into a Git repository, you can remove it from the history. To entirely remove unwanted files from a repository\u0026rsquo;s history you can use the git filter-branch command.\nThe git filter-branch command rewrite your repository\u0026rsquo;s history, which changes the SHAs for existing commits that you alter and any dependent commits. Changed commit SHAs may affect open pull requests in your repository. I recommend merging or closing all open pull requests before removing files from your repository.\n Warning: - if someone has already checked out the repository, then of course he has the secret on his computer. So ALWAYS revoke the OAuthToken/Password or whatever it was imediately.\n Purging a file from your repository\u0026rsquo;s history  Warning: If you run git filter-branch after stashing changes, you won\u0026rsquo;t be able to retrieve your changes with other stash commands. Before running git filter-branch, we recommend unstashing any changes you\u0026rsquo;ve made. To unstash the last set of changes you\u0026rsquo;ve stashed, run git stash show -p | git apply -R. For more information, see Git Tools Stashing.\n To illustrate how git filter-branch works, we\u0026rsquo;ll show you how to remove your file with sensitive data from the history of your repository and add it to .gitignore to ensure that it is not accidentally re-committed.\nNavigate into the repository\u0026rsquo;s working directory.\ncd YOUR-REPOSITORY Run the following command, replacing PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA with the path to the file you want to remove, not just its filename.\nThese arguments will:\n Force Git to process, but not check out, the entire history of every branch and tag Remove the specified file, as well as any empty commits generated as a result Overwrite your existing tags  git filter-branch --force --index-filter \\ \u0026#39;git rm --cached --ignore-unmatch PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA\u0026#39; \\ --prune-empty --tag-name-filter cat -- --all Add your file with sensitive data to .gitignore to ensure that you don\u0026rsquo;t accidentally commit it again.\necho \u0026#34;YOUR-FILE-WITH-SENSITIVE-DATA\u0026#34; \u0026gt;\u0026gt; .gitignore Double-check that you\u0026rsquo;ve removed everything you wanted to from your repository\u0026rsquo;s history, and that all of your branches are checked out.\nOnce you\u0026rsquo;re happy with the state of your repository, force-push your local changes to overwrite your GitHub repository, as well as all the branches you\u0026rsquo;ve pushed up:\ngit push origin --force --all In order to remove the sensitive file from your tagged releases, you\u0026rsquo;ll also need to force-push against your Git tags:\ngit push origin --force --tags  Warning: Tell your collaborators to rebase, not merge, any branches they created off of your old (tainted) repository history. One merge commit could reintroduce some or all of the tainted history that you just went to the trouble of purging.\n References:\n https://help.github.com/articles/removing-sensitive-data-from-a-repository/   blockquote { border:1px solid red; padding:10px; margin-top:40px; margin-bottom:40px; } blockquote p { font-size: 1.5rem; color: black; }  "},{"uri":"https://gardener.cloud/v1.12.8/guides/applications/commit_secret_fail/","title":"Storing secrets in git ","tags":[],"description":"Never ever commit a kubeconfig.yaml into github","content":"Problem If you commit sensitive data, such as a kubeconfig.yaml or SSH key into a Git repository, you can remove it from the history. To entirely remove unwanted files from a repository\u0026rsquo;s history you can use the git filter-branch command.\nThe git filter-branch command rewrite your repository\u0026rsquo;s history, which changes the SHAs for existing commits that you alter and any dependent commits. Changed commit SHAs may affect open pull requests in your repository. I recommend merging or closing all open pull requests before removing files from your repository.\n Warning: - if someone has already checked out the repository, then of course he has the secret on his computer. So ALWAYS revoke the OAuthToken/Password or whatever it was imediately.\n Purging a file from your repository\u0026rsquo;s history  Warning: If you run git filter-branch after stashing changes, you won\u0026rsquo;t be able to retrieve your changes with other stash commands. Before running git filter-branch, we recommend unstashing any changes you\u0026rsquo;ve made. To unstash the last set of changes you\u0026rsquo;ve stashed, run git stash show -p | git apply -R. For more information, see Git Tools Stashing.\n To illustrate how git filter-branch works, we\u0026rsquo;ll show you how to remove your file with sensitive data from the history of your repository and add it to .gitignore to ensure that it is not accidentally re-committed.\nNavigate into the repository\u0026rsquo;s working directory.\ncd YOUR-REPOSITORY Run the following command, replacing PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA with the path to the file you want to remove, not just its filename.\nThese arguments will:\n Force Git to process, but not check out, the entire history of every branch and tag Remove the specified file, as well as any empty commits generated as a result Overwrite your existing tags  git filter-branch --force --index-filter \\ \u0026#39;git rm --cached --ignore-unmatch PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA\u0026#39; \\ --prune-empty --tag-name-filter cat -- --all Add your file with sensitive data to .gitignore to ensure that you don\u0026rsquo;t accidentally commit it again.\necho \u0026#34;YOUR-FILE-WITH-SENSITIVE-DATA\u0026#34; \u0026gt;\u0026gt; .gitignore Double-check that you\u0026rsquo;ve removed everything you wanted to from your repository\u0026rsquo;s history, and that all of your branches are checked out.\nOnce you\u0026rsquo;re happy with the state of your repository, force-push your local changes to overwrite your GitHub repository, as well as all the branches you\u0026rsquo;ve pushed up:\ngit push origin --force --all In order to remove the sensitive file from your tagged releases, you\u0026rsquo;ll also need to force-push against your Git tags:\ngit push origin --force --tags  Warning: Tell your collaborators to rebase, not merge, any branches they created off of your old (tainted) repository history. One merge commit could reintroduce some or all of the tainted history that you just went to the trouble of purging.\n References:\n https://help.github.com/articles/removing-sensitive-data-from-a-repository/   blockquote { border:1px solid red; padding:10px; margin-top:40px; margin-bottom:40px; } blockquote p { font-size: 1.5rem; color: black; }  "},{"uri":"https://gardener.cloud/v1.13.2/guides/applications/commit_secret_fail/","title":"Storing secrets in git ","tags":[],"description":"Never ever commit a kubeconfig.yaml into github","content":"Problem If you commit sensitive data, such as a kubeconfig.yaml or SSH key into a Git repository, you can remove it from the history. To entirely remove unwanted files from a repository\u0026rsquo;s history you can use the git filter-branch command.\nThe git filter-branch command rewrite your repository\u0026rsquo;s history, which changes the SHAs for existing commits that you alter and any dependent commits. Changed commit SHAs may affect open pull requests in your repository. I recommend merging or closing all open pull requests before removing files from your repository.\n Warning: - if someone has already checked out the repository, then of course he has the secret on his computer. So ALWAYS revoke the OAuthToken/Password or whatever it was imediately.\n Purging a file from your repository\u0026rsquo;s history  Warning: If you run git filter-branch after stashing changes, you won\u0026rsquo;t be able to retrieve your changes with other stash commands. Before running git filter-branch, we recommend unstashing any changes you\u0026rsquo;ve made. To unstash the last set of changes you\u0026rsquo;ve stashed, run git stash show -p | git apply -R. For more information, see Git Tools Stashing.\n To illustrate how git filter-branch works, we\u0026rsquo;ll show you how to remove your file with sensitive data from the history of your repository and add it to .gitignore to ensure that it is not accidentally re-committed.\nNavigate into the repository\u0026rsquo;s working directory.\ncd YOUR-REPOSITORY Run the following command, replacing PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA with the path to the file you want to remove, not just its filename.\nThese arguments will:\n Force Git to process, but not check out, the entire history of every branch and tag Remove the specified file, as well as any empty commits generated as a result Overwrite your existing tags  git filter-branch --force --index-filter \\ \u0026#39;git rm --cached --ignore-unmatch PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA\u0026#39; \\ --prune-empty --tag-name-filter cat -- --all Add your file with sensitive data to .gitignore to ensure that you don\u0026rsquo;t accidentally commit it again.\necho \u0026#34;YOUR-FILE-WITH-SENSITIVE-DATA\u0026#34; \u0026gt;\u0026gt; .gitignore Double-check that you\u0026rsquo;ve removed everything you wanted to from your repository\u0026rsquo;s history, and that all of your branches are checked out.\nOnce you\u0026rsquo;re happy with the state of your repository, force-push your local changes to overwrite your GitHub repository, as well as all the branches you\u0026rsquo;ve pushed up:\ngit push origin --force --all In order to remove the sensitive file from your tagged releases, you\u0026rsquo;ll also need to force-push against your Git tags:\ngit push origin --force --tags  Warning: Tell your collaborators to rebase, not merge, any branches they created off of your old (tainted) repository history. One merge commit could reintroduce some or all of the tainted history that you just went to the trouble of purging.\n References:\n https://help.github.com/articles/removing-sensitive-data-from-a-repository/   blockquote { border:1px solid red; padding:10px; margin-top:40px; margin-bottom:40px; } blockquote p { font-size: 1.5rem; color: black; }  "},{"uri":"https://gardener.cloud/documentation/contribute/20_documentation/20_style/","title":"Style Guide","tags":[],"description":"","content":"This page gives writing style guidelines for the Gardener documentation. These are guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.\n Language Formatting of Inline Elements Code Snippet Formatting  Language  Gardener documentation uses US English. Keep it simple and use words that non-native English speakers are also familiar with.  Formatting of Inline Elements    Type of Text Formatting Markdown Syntax     User Interface Elements italics Choose *CLUSTERS*.   New Terms and Emphasis bold Do **not** stop it.   Technical Names code Open file `root.yaml`.   API Objects and Technical Components code Deploy a `Pod`.   Inline Code and Inline Commands code For declarative management, use `kubectl apply`.   Object Field Names and Field Values code Set the value of `image` to `nginx:1.8`.    User Interface Elements When referring to UI elements, refrain from using verbs like \u0026ldquo;Click\u0026rdquo; or \u0026ldquo;Select with right mouse button\u0026rdquo;. This level of detail is hardly ever needed and also invalidates a procedure if other devices are used. For example, for a tablet you\u0026rsquo;d say \u0026ldquo;Tap on\u0026rdquo;.\nUse italics when you refer to UI elements.\n   UI Element Standard Formulation Markdown Syntax     Button, Menu path Choose UI Element. Choose *UI Element*.   Menu path, context menu, navigation path Choose System \u0026gt; User Profile \u0026gt; Own Data. Choose *System* \\\u0026gt; *User Profile* \\\u0026gt; *Own Data*.   Entry fields Enter your password. Enter your password.   Checkbox, radio button Select Filter. Select *Filter*.   Expandable screen elements Expand User Settings.\nCollapse User Settings. Expand *User Settings*.\nCollapse *User Settings*.    New Terms and Emphasis Use bold to emphasize something or to introduce a new term.\n   Do Don\u0026rsquo;t     A cluster is a set of nodes \u0026hellip; A \u0026ldquo;cluster\u0026rdquo; is a set of nodes \u0026hellip;   The system does not delete your objects. The system does not(!) delete your objects.    Technical Names Use code style (using backticks) for filenames, technical componentes, directories, and paths.\n   Do Don\u0026rsquo;t     Open file envars.yaml. Open the envars.yaml file.   Go to directory /docs/tutorials. Go to the /docs/tutorials directory.   Open file /_data/concepts.yaml. Open the /_data/concepts.yaml file.    API Objects and Technical Components When you refer to an API object, use the same uppercase and lowercase letters that are used in the actual object name, and use backticks to format them. Typically, the names of API objects use camel case.\nDon\u0026rsquo;t split the API object name into separate words. For example, use PodTemplateList, not Pod Template List.\nRefer to API objects without saying \u0026ldquo;object,\u0026rdquo; unless omitting \u0026ldquo;object\u0026rdquo; leads to an awkward construction.\n   Do Don\u0026rsquo;t     The Pod has two containers. The pod has two containers.   The Deployment is responsible for \u0026hellip; The Deployment object is responsible for \u0026hellip;   A PodList is a list of Pods. A Pod List is a list of pods.   The gardener-control-manager has control loops\u0026hellip; The gardener-control-manager has control loops\u0026hellip;   The gardenlet starts up with a bootstrap kubeconfig having a bootstrap token that allows to create CertificateSigningRequest (CSR) resources. The gardenlet starts up with a bootstrap kubeconfig having a bootstrap token that allows to create CertificateSigningRequest (CSR) resources.    Inline Code and Inline Commands Use backticks (`) for inline code.\n   Do Don\u0026rsquo;t     The kubectl run command creates a Deployment. The \u0026ldquo;kubectl run\u0026rdquo; command creates a Deployment.   For declarative management, use kubectl apply. For declarative management, use \u0026ldquo;kubectl apply\u0026rdquo;.    Object Field Names and Field Values Use backticks (`) for field names, and field values.\n   Do Don\u0026rsquo;t     Set the value of the replicas field in the configuration file. Set the value of the \u0026ldquo;replicas\u0026rdquo; field in the configuration file.   The value of the exec field is an ExecAction object. The value of the \u0026ldquo;exec\u0026rdquo; field is an ExecAction object.   Set the value of imagePullPolicy to Always. Set the value of imagePullPolicy to \u0026ldquo;Always\u0026rdquo;.   Set the value of image to nginx:1.8. the value of image to nginx:1.8.    Code Snippet Formatting Don\u0026rsquo;t include the command prompt    Do Don\u0026rsquo;t     kubectl get pods $ kubectl get pods    Separate commands from output  Verify that the pod is running on your chosen node: kubectl get pods --output=wide  The output is similar to:\nNAME READY STATUS RESTARTS AGE IP NODE nginx 1/1 Running 0 13s 10.200.0.4 worker0   Placeholders Use angle brackets for placeholders. Tell the reader what a placeholder represents, for example:\n Display information about a pod:\nkubectl describe pod \u0026lt;pod-name\u0026gt; \u0026lt;pod-name\u0026gt; is the name of one of your pods.\n Versioning Kubernetes examples Make code examples and configuration examples that include version information consistent with the accompanying text. Identify the Kubernetes version in the Prerequisites section.\n"},{"uri":"https://gardener.cloud/v1.12.8/contribute/20_documentation/20_style/","title":"Style Guide","tags":[],"description":"","content":"This page gives writing style guidelines for the Gardener documentation. These are guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.\n Language Formatting of Inline Elements Code Snippet Formatting  Language  Gardener documentation uses US English. Keep it simple and use words that non-native English speakers are also familiar with.  Formatting of Inline Elements    Type of Text Formatting Markdown Syntax     User Interface Elements italics Choose *CLUSTERS*.   New Terms and Emphasis bold Do **not** stop it.   Technical Names code Open file `root.yaml`.   API Objects and Technical Components code Deploy a `Pod`.   Inline Code and Inline Commands code For declarative management, use `kubectl apply`.   Object Field Names and Field Values code Set the value of `image` to `nginx:1.8`.    User Interface Elements When referring to UI elements, refrain from using verbs like \u0026ldquo;Click\u0026rdquo; or \u0026ldquo;Select with right mouse button\u0026rdquo;. This level of detail is hardly ever needed and also invalidates a procedure if other devices are used. For example, for a tablet you\u0026rsquo;d say \u0026ldquo;Tap on\u0026rdquo;.\nUse italics when you refer to UI elements.\n   UI Element Standard Formulation Markdown Syntax     Button, Menu path Choose UI Element. Choose *UI Element*.   Menu path, context menu, navigation path Choose System \u0026gt; User Profile \u0026gt; Own Data. Choose *System* \\\u0026gt; *User Profile* \\\u0026gt; *Own Data*.   Entry fields Enter your password. Enter your password.   Checkbox, radio button Select Filter. Select *Filter*.   Expandable screen elements Expand User Settings.\nCollapse User Settings. Expand *User Settings*.\nCollapse *User Settings*.    New Terms and Emphasis Use bold to emphasize something or to introduce a new term.\n   Do Don\u0026rsquo;t     A cluster is a set of nodes \u0026hellip; A \u0026ldquo;cluster\u0026rdquo; is a set of nodes \u0026hellip;   The system does not delete your objects. The system does not(!) delete your objects.    Technical Names Use code style (using backticks) for filenames, technical componentes, directories, and paths.\n   Do Don\u0026rsquo;t     Open file envars.yaml. Open the envars.yaml file.   Go to directory /docs/tutorials. Go to the /docs/tutorials directory.   Open file /_data/concepts.yaml. Open the /_data/concepts.yaml file.    API Objects and Technical Components When you refer to an API object, use the same uppercase and lowercase letters that are used in the actual object name, and use backticks to format them. Typically, the names of API objects use camel case.\nDon\u0026rsquo;t split the API object name into separate words. For example, use PodTemplateList, not Pod Template List.\nRefer to API objects without saying \u0026ldquo;object,\u0026rdquo; unless omitting \u0026ldquo;object\u0026rdquo; leads to an awkward construction.\n   Do Don\u0026rsquo;t     The Pod has two containers. The pod has two containers.   The Deployment is responsible for \u0026hellip; The Deployment object is responsible for \u0026hellip;   A PodList is a list of Pods. A Pod List is a list of pods.   The gardener-control-manager has control loops\u0026hellip; The gardener-control-manager has control loops\u0026hellip;   The gardenlet starts up with a bootstrap kubeconfig having a bootstrap token that allows to create CertificateSigningRequest (CSR) resources. The gardenlet starts up with a bootstrap kubeconfig having a bootstrap token that allows to create CertificateSigningRequest (CSR) resources.    Inline Code and Inline Commands Use backticks (`) for inline code.\n   Do Don\u0026rsquo;t     The kubectl run command creates a Deployment. The \u0026ldquo;kubectl run\u0026rdquo; command creates a Deployment.   For declarative management, use kubectl apply. For declarative management, use \u0026ldquo;kubectl apply\u0026rdquo;.    Object Field Names and Field Values Use backticks (`) for field names, and field values.\n   Do Don\u0026rsquo;t     Set the value of the replicas field in the configuration file. Set the value of the \u0026ldquo;replicas\u0026rdquo; field in the configuration file.   The value of the exec field is an ExecAction object. The value of the \u0026ldquo;exec\u0026rdquo; field is an ExecAction object.   Set the value of imagePullPolicy to Always. Set the value of imagePullPolicy to \u0026ldquo;Always\u0026rdquo;.   Set the value of image to nginx:1.8. the value of image to nginx:1.8.    Code Snippet Formatting Don\u0026rsquo;t include the command prompt    Do Don\u0026rsquo;t     kubectl get pods $ kubectl get pods    Separate commands from output  Verify that the pod is running on your chosen node: kubectl get pods --output=wide  The output is similar to:\nNAME READY STATUS RESTARTS AGE IP NODE nginx 1/1 Running 0 13s 10.200.0.4 worker0   Placeholders Use angle brackets for placeholders. Tell the reader what a placeholder represents, for example:\n Display information about a pod:\nkubectl describe pod \u0026lt;pod-name\u0026gt; \u0026lt;pod-name\u0026gt; is the name of one of your pods.\n Versioning Kubernetes examples Make code examples and configuration examples that include version information consistent with the accompanying text. Identify the Kubernetes version in the Prerequisites section.\n"},{"uri":"https://gardener.cloud/v1.13.2/contribute/20_documentation/20_style/","title":"Style Guide","tags":[],"description":"","content":"This page gives writing style guidelines for the Gardener documentation. These are guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.\n Language Formatting of Inline Elements Code Snippet Formatting  Language  Gardener documentation uses US English. Keep it simple and use words that non-native English speakers are also familiar with.  Formatting of Inline Elements    Type of Text Formatting Markdown Syntax     User Interface Elements italics Choose *CLUSTERS*.   New Terms and Emphasis bold Do **not** stop it.   Technical Names code Open file `root.yaml`.   API Objects and Technical Components code Deploy a `Pod`.   Inline Code and Inline Commands code For declarative management, use `kubectl apply`.   Object Field Names and Field Values code Set the value of `image` to `nginx:1.8`.    User Interface Elements When referring to UI elements, refrain from using verbs like \u0026ldquo;Click\u0026rdquo; or \u0026ldquo;Select with right mouse button\u0026rdquo;. This level of detail is hardly ever needed and also invalidates a procedure if other devices are used. For example, for a tablet you\u0026rsquo;d say \u0026ldquo;Tap on\u0026rdquo;.\nUse italics when you refer to UI elements.\n   UI Element Standard Formulation Markdown Syntax     Button, Menu path Choose UI Element. Choose *UI Element*.   Menu path, context menu, navigation path Choose System \u0026gt; User Profile \u0026gt; Own Data. Choose *System* \\\u0026gt; *User Profile* \\\u0026gt; *Own Data*.   Entry fields Enter your password. Enter your password.   Checkbox, radio button Select Filter. Select *Filter*.   Expandable screen elements Expand User Settings.\nCollapse User Settings. Expand *User Settings*.\nCollapse *User Settings*.    New Terms and Emphasis Use bold to emphasize something or to introduce a new term.\n   Do Don\u0026rsquo;t     A cluster is a set of nodes \u0026hellip; A \u0026ldquo;cluster\u0026rdquo; is a set of nodes \u0026hellip;   The system does not delete your objects. The system does not(!) delete your objects.    Technical Names Use code style (using backticks) for filenames, technical componentes, directories, and paths.\n   Do Don\u0026rsquo;t     Open file envars.yaml. Open the envars.yaml file.   Go to directory /docs/tutorials. Go to the /docs/tutorials directory.   Open file /_data/concepts.yaml. Open the /_data/concepts.yaml file.    API Objects and Technical Components When you refer to an API object, use the same uppercase and lowercase letters that are used in the actual object name, and use backticks to format them. Typically, the names of API objects use camel case.\nDon\u0026rsquo;t split the API object name into separate words. For example, use PodTemplateList, not Pod Template List.\nRefer to API objects without saying \u0026ldquo;object,\u0026rdquo; unless omitting \u0026ldquo;object\u0026rdquo; leads to an awkward construction.\n   Do Don\u0026rsquo;t     The Pod has two containers. The pod has two containers.   The Deployment is responsible for \u0026hellip; The Deployment object is responsible for \u0026hellip;   A PodList is a list of Pods. A Pod List is a list of pods.   The gardener-control-manager has control loops\u0026hellip; The gardener-control-manager has control loops\u0026hellip;   The gardenlet starts up with a bootstrap kubeconfig having a bootstrap token that allows to create CertificateSigningRequest (CSR) resources. The gardenlet starts up with a bootstrap kubeconfig having a bootstrap token that allows to create CertificateSigningRequest (CSR) resources.    Inline Code and Inline Commands Use backticks (`) for inline code.\n   Do Don\u0026rsquo;t     The kubectl run command creates a Deployment. The \u0026ldquo;kubectl run\u0026rdquo; command creates a Deployment.   For declarative management, use kubectl apply. For declarative management, use \u0026ldquo;kubectl apply\u0026rdquo;.    Object Field Names and Field Values Use backticks (`) for field names, and field values.\n   Do Don\u0026rsquo;t     Set the value of the replicas field in the configuration file. Set the value of the \u0026ldquo;replicas\u0026rdquo; field in the configuration file.   The value of the exec field is an ExecAction object. The value of the \u0026ldquo;exec\u0026rdquo; field is an ExecAction object.   Set the value of imagePullPolicy to Always. Set the value of imagePullPolicy to \u0026ldquo;Always\u0026rdquo;.   Set the value of image to nginx:1.8. the value of image to nginx:1.8.    Code Snippet Formatting Don\u0026rsquo;t include the command prompt    Do Don\u0026rsquo;t     kubectl get pods $ kubectl get pods    Separate commands from output  Verify that the pod is running on your chosen node: kubectl get pods --output=wide  The output is similar to:\nNAME READY STATUS RESTARTS AGE IP NODE nginx 1/1 Running 0 13s 10.200.0.4 worker0   Placeholders Use angle brackets for placeholders. Tell the reader what a placeholder represents, for example:\n Display information about a pod:\nkubectl describe pod \u0026lt;pod-name\u0026gt; \u0026lt;pod-name\u0026gt; is the name of one of your pods.\n Versioning Kubernetes examples Make code examples and configuration examples that include version information consistent with the accompanying text. Identify the Kubernetes version in the Prerequisites section.\n"},{"uri":"https://gardener.cloud/documentation/guides/monitoring_and_troubleshooting/tail-logfile/","title":"tail -f /var/log/my-application.log","tags":[],"description":"Aggregate log files from different pods","content":"Problem One thing that always bothered me was that I couldn\u0026rsquo;t get logs of several pods at once with kubectl. A simple tail -f \u0026lt;path-to-logfile\u0026gt; isn\u0026rsquo;t possible at all. Certainly you can use kubectl logs -f \u0026lt;pod-id\u0026gt;, but it doesn\u0026rsquo;t help if you want to monitor more than one pod at a time.\nThis is something you really need a lot, at least if you run several instances of a pod behind a deployment. This is even more so if you don\u0026rsquo;t have a Kibana setup or similar.\nSolution Luckily, there are smart developers out there who always come up with solutions. The finding of the week is a small bash script that allows you to aggregate log files of several pods at the same time in a simple way. The script is called kubetail and is available at GitHub.\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/monitoring_and_troubleshooting/tail-logfile/","title":"tail -f /var/log/my-application.log","tags":[],"description":"Aggregate log files from different pods","content":"Problem One thing that always bothered me was that I couldn\u0026rsquo;t get logs of several pods at once with kubectl. A simple tail -f \u0026lt;path-to-logfile\u0026gt; isn\u0026rsquo;t possible at all. Certainly you can use kubectl logs -f \u0026lt;pod-id\u0026gt;, but it doesn\u0026rsquo;t help if you want to monitor more than one pod at a time.\nThis is something you really need a lot, at least if you run several instances of a pod behind a deployment. This is even more so if you don\u0026rsquo;t have a Kibana setup or similar.\nSolution Luckily, there are smart developers out there who always come up with solutions. The finding of the week is a small bash script that allows you to aggregate log files of several pods at the same time in a simple way. The script is called kubetail and is available at GitHub.\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/monitoring_and_troubleshooting/tail-logfile/","title":"tail -f /var/log/my-application.log","tags":[],"description":"Aggregate log files from different pods","content":"Problem One thing that always bothered me was that I couldn\u0026rsquo;t get logs of several pods at once with kubectl. A simple tail -f \u0026lt;path-to-logfile\u0026gt; isn\u0026rsquo;t possible at all. Certainly you can use kubectl logs -f \u0026lt;pod-id\u0026gt;, but it doesn\u0026rsquo;t help if you want to monitor more than one pod at a time.\nThis is something you really need a lot, at least if you run several instances of a pod behind a deployment. This is even more so if you don\u0026rsquo;t have a Kibana setup or similar.\nSolution Luckily, there are smart developers out there who always come up with solutions. The finding of the week is a small bash script that allows you to aggregate log files of several pods at the same time in a simple way. The script is called kubetail and is available at GitHub.\n"},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/trigger-shoot-operations/","title":"Trigger Shoot operations","tags":[],"description":"","content":"Trigger shoot operations You can trigger a few explicit operations by annotating the Shoot with an operation annotation. This might allow you to induct certain behavior without the need to change the Shoot specification. Some of the operations can also not be caused by changing something in the shoot specification because they can\u0026rsquo;t properly be reflected here.\nPlease note: If .spec.maintenance.confineSpecUpdateRollout=true then the only way to trigger a shoot reconciliation is by setting the reconcile operation, see below.\nImmediate reconciliation Annotate the shoot with gardener.cloud/operation=reconcile to make the gardenlet start a reconciliation operation without changing the shoot spec and possibly without being in its maintenance time window:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=reconcile Immediate maintenance Annotate the shoot with gardener.cloud/operation=maintain to make the gardener-controller-manager start maintaining your shoot immediately (possibly without being in its maintenance time window). If no reconciliation starts then nothing needed to be maintained:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=maintain Retry failed operation Annotate the shoot with gardener.cloud/operation=retry to make the gardenlet start a new reconciliation loop on a failed shoot. Failed shoots are only reconciled again if a new Gardener version is deployed, the shoot specification is changed or this annotation is set\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=retry Rotate kubeconfig credentials Annotate the shoot with gardener.cloud/operation=rotate-kubeconfig-credentials to make the gardenlet exchange the credentials in your shoot cluster\u0026rsquo;s kubeconfig. Please note that only the token (and basic auth password, if enabled) are exchanged. The cluster CAs remain the same.\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=rotate-kubeconfig-credentials Restart systemd services on particular worker nodes It is possible to make Gardener restart particular systemd services on your shoot worker nodes if needed. The annotation is not set on the Shoot resource but directly on the Node object you want to target. For example, the following will restart both the kubelet and the docker services:\nkubectl annotate node \u0026lt;node-name\u0026gt; worker.gardener.cloud/restart-systemd-services=kubelet,docker It may take up to a minute until the service is restarted. The annotation will be removed from the Node object after all specified systemd services have been restarted. It will also be removed even if the restart of one or more services failed.\n  In the example mentioned above, you could additionally verify when/whether the kubelet restarted by using kubectl describe node \u0026lt;node-name\u0026gt; and looking for such a Starting kubelet event.\n "},{"uri":"https://gardener.cloud/documentation/guides/administer_shoots/trigger-shoot-operations/","title":"Trigger Shoot operations","tags":[],"description":"","content":"Trigger shoot operations You can trigger a few explicit operations by annotating the Shoot with an operation annotation. This might allow you to induct certain behavior without the need to change the Shoot specification. Some of the operations can also not be caused by changing something in the shoot specification because they can\u0026rsquo;t properly be reflected here.\nPlease note: If .spec.maintenance.confineSpecUpdateRollout=true then the only way to trigger a shoot reconciliation is by setting the reconcile operation, see below.\nImmediate reconciliation Annotate the shoot with gardener.cloud/operation=reconcile to make the gardenlet start a reconciliation operation without changing the shoot spec and possibly without being in its maintenance time window:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=reconcile Immediate maintenance Annotate the shoot with gardener.cloud/operation=maintain to make the gardener-controller-manager start maintaining your shoot immediately (possibly without being in its maintenance time window). If no reconciliation starts then nothing needed to be maintained:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=maintain Retry failed operation Annotate the shoot with gardener.cloud/operation=retry to make the gardenlet start a new reconciliation loop on a failed shoot. Failed shoots are only reconciled again if a new Gardener version is deployed, the shoot specification is changed or this annotation is set\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=retry Rotate kubeconfig credentials Annotate the shoot with gardener.cloud/operation=rotate-kubeconfig-credentials to make the gardenlet exchange the credentials in your shoot cluster\u0026rsquo;s kubeconfig. Please note that only the token (and basic auth password, if enabled) are exchanged. The cluster CAs remain the same.\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=rotate-kubeconfig-credentials "},{"uri":"https://gardener.cloud/v1.12.8/guides/administer_shoots/trigger-shoot-operations/","title":"Trigger Shoot operations","tags":[],"description":"","content":"Trigger shoot operations You can trigger a few explicit operations by annotating the Shoot with an operation annotation. This might allow you to induct certain behavior without the need to change the Shoot specification. Some of the operations can also not be caused by changing something in the shoot specification because they can\u0026rsquo;t properly be reflected here.\nPlease note: If .spec.maintenance.confineSpecUpdateRollout=true then the only way to trigger a shoot reconciliation is by setting the reconcile operation, see below.\nImmediate reconciliation Annotate the shoot with gardener.cloud/operation=reconcile to make the gardenlet start a reconciliation operation without changing the shoot spec and possibly without being in its maintenance time window:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=reconcile Immediate maintenance Annotate the shoot with gardener.cloud/operation=maintain to make the gardener-controller-manager start maintaining your shoot immediately (possibly without being in its maintenance time window). If no reconciliation starts then nothing needed to be maintained:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=maintain Retry failed operation Annotate the shoot with gardener.cloud/operation=retry to make the gardenlet start a new reconciliation loop on a failed shoot. Failed shoots are only reconciled again if a new Gardener version is deployed, the shoot specification is changed or this annotation is set\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=retry Rotate kubeconfig credentials Annotate the shoot with gardener.cloud/operation=rotate-kubeconfig-credentials to make the gardenlet exchange the credentials in your shoot cluster\u0026rsquo;s kubeconfig. Please note that only the token (and basic auth password, if enabled) are exchanged. The cluster CAs remain the same.\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=rotate-kubeconfig-credentials Restart systemd services on particular worker nodes It is possible to make Gardener restart particular systemd services on your shoot worker nodes if needed. The annotation is not set on the Shoot resource but directly on the Node object you want to target. For example, the following will restart both the kubelet and the docker services:\nkubectl annotate node \u0026lt;node-name\u0026gt; worker.gardener.cloud/restart-systemd-services=kubelet,docker It may take up to a minute until the service is restarted. The annotation will be removed from the Node object after all specified systemd services have been restarted. It will also be removed even if the restart of one or more services failed.\n  In the example mentioned above, you could additionally verify when/whether the kubelet restarted by using kubectl describe node \u0026lt;node-name\u0026gt; and looking for such a Starting kubelet event.\n "},{"uri":"https://gardener.cloud/v1.12.8/guides/administer_shoots/trigger-shoot-operations/","title":"Trigger Shoot operations","tags":[],"description":"","content":"Trigger shoot operations You can trigger a few explicit operations by annotating the Shoot with an operation annotation. This might allow you to induct certain behavior without the need to change the Shoot specification. Some of the operations can also not be caused by changing something in the shoot specification because they can\u0026rsquo;t properly be reflected here.\nPlease note: If .spec.maintenance.confineSpecUpdateRollout=true then the only way to trigger a shoot reconciliation is by setting the reconcile operation, see below.\nImmediate reconciliation Annotate the shoot with gardener.cloud/operation=reconcile to make the gardenlet start a reconciliation operation without changing the shoot spec and possibly without being in its maintenance time window:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=reconcile Immediate maintenance Annotate the shoot with gardener.cloud/operation=maintain to make the gardener-controller-manager start maintaining your shoot immediately (possibly without being in its maintenance time window). If no reconciliation starts then nothing needed to be maintained:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=maintain Retry failed operation Annotate the shoot with gardener.cloud/operation=retry to make the gardenlet start a new reconciliation loop on a failed shoot. Failed shoots are only reconciled again if a new Gardener version is deployed, the shoot specification is changed or this annotation is set\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=retry Rotate kubeconfig credentials Annotate the shoot with gardener.cloud/operation=rotate-kubeconfig-credentials to make the gardenlet exchange the credentials in your shoot cluster\u0026rsquo;s kubeconfig. Please note that only the token (and basic auth password, if enabled) are exchanged. The cluster CAs remain the same.\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=rotate-kubeconfig-credentials "},{"uri":"https://gardener.cloud/v1.13.2/guides/administer_shoots/trigger-shoot-operations/","title":"Trigger Shoot operations","tags":[],"description":"","content":"Trigger shoot operations You can trigger a few explicit operations by annotating the Shoot with an operation annotation. This might allow you to induct certain behavior without the need to change the Shoot specification. Some of the operations can also not be caused by changing something in the shoot specification because they can\u0026rsquo;t properly be reflected here.\nPlease note: If .spec.maintenance.confineSpecUpdateRollout=true then the only way to trigger a shoot reconciliation is by setting the reconcile operation, see below.\nImmediate reconciliation Annotate the shoot with gardener.cloud/operation=reconcile to make the gardenlet start a reconciliation operation without changing the shoot spec and possibly without being in its maintenance time window:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=reconcile Immediate maintenance Annotate the shoot with gardener.cloud/operation=maintain to make the gardener-controller-manager start maintaining your shoot immediately (possibly without being in its maintenance time window). If no reconciliation starts then nothing needed to be maintained:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=maintain Retry failed operation Annotate the shoot with gardener.cloud/operation=retry to make the gardenlet start a new reconciliation loop on a failed shoot. Failed shoots are only reconciled again if a new Gardener version is deployed, the shoot specification is changed or this annotation is set\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=retry Rotate kubeconfig credentials Annotate the shoot with gardener.cloud/operation=rotate-kubeconfig-credentials to make the gardenlet exchange the credentials in your shoot cluster\u0026rsquo;s kubeconfig. Please note that only the token (and basic auth password, if enabled) are exchanged. The cluster CAs remain the same.\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=rotate-kubeconfig-credentials Restart systemd services on particular worker nodes It is possible to make Gardener restart particular systemd services on your shoot worker nodes if needed. The annotation is not set on the Shoot resource but directly on the Node object you want to target. For example, the following will restart both the kubelet and the docker services:\nkubectl annotate node \u0026lt;node-name\u0026gt; worker.gardener.cloud/restart-systemd-services=kubelet,docker It may take up to a minute until the service is restarted. The annotation will be removed from the Node object after all specified systemd services have been restarted. It will also be removed even if the restart of one or more services failed.\n  In the example mentioned above, you could additionally verify when/whether the kubelet restarted by using kubectl describe node \u0026lt;node-name\u0026gt; and looking for such a Starting kubelet event.\n "},{"uri":"https://gardener.cloud/v1.13.2/guides/administer_shoots/trigger-shoot-operations/","title":"Trigger Shoot operations","tags":[],"description":"","content":"Trigger shoot operations You can trigger a few explicit operations by annotating the Shoot with an operation annotation. This might allow you to induct certain behavior without the need to change the Shoot specification. Some of the operations can also not be caused by changing something in the shoot specification because they can\u0026rsquo;t properly be reflected here.\nPlease note: If .spec.maintenance.confineSpecUpdateRollout=true then the only way to trigger a shoot reconciliation is by setting the reconcile operation, see below.\nImmediate reconciliation Annotate the shoot with gardener.cloud/operation=reconcile to make the gardenlet start a reconciliation operation without changing the shoot spec and possibly without being in its maintenance time window:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=reconcile Immediate maintenance Annotate the shoot with gardener.cloud/operation=maintain to make the gardener-controller-manager start maintaining your shoot immediately (possibly without being in its maintenance time window). If no reconciliation starts then nothing needed to be maintained:\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=maintain Retry failed operation Annotate the shoot with gardener.cloud/operation=retry to make the gardenlet start a new reconciliation loop on a failed shoot. Failed shoots are only reconciled again if a new Gardener version is deployed, the shoot specification is changed or this annotation is set\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=retry Rotate kubeconfig credentials Annotate the shoot with gardener.cloud/operation=rotate-kubeconfig-credentials to make the gardenlet exchange the credentials in your shoot cluster\u0026rsquo;s kubeconfig. Please note that only the token (and basic auth password, if enabled) are exchanged. The cluster CAs remain the same.\nkubectl -n garden-\u0026lt;project-name\u0026gt; annotate shoot \u0026lt;shoot-name\u0026gt; gardener.cloud/operation=rotate-kubeconfig-credentials "},{"uri":"https://gardener.cloud/documentation/guides/client_tools/helm/","title":"Use a Helm chart to deploy some application or service","tags":[],"description":"","content":"Basically, Helm Charts can be installed as described e.g. in the Helm QuickStart Guide. However, our clusters come with RBAC enabled by default hence Helm must be installed as follows:\nCreate a Service Account Create a service account via the following command:\ncat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: v1 kind: ServiceAccount metadata: name: helm namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: helm roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: helm namespace: kube-system EOF Initialize Helm Initialise Helm via helm init --service-account helm. You can now use helm.\nIn case of failure In case you have already executed helm init, but without the above service account, you will get the following error: Error: User \u0026quot;system:serviceaccount:kube-system:default\u0026quot; cannot list configmaps in the namespace \u0026quot;kube-system\u0026quot;. (get configmaps) (e.g. when you run helm list). You will now need to delete the Tiller deployment (Helm backend implicitly deployed to the Kubernetes cluster when you call helm init) as well as the local Helm files (usually $HELM_HOME is set to ~/.helm):\nkubectl delete deployment tiller-deploy --namespace=kube-system kubectl delete service tiller-deploy --namespace=kube-system rm -rf ~/.helm/ Now follow the instructions above. For more details see this Kubernetes Helm issue #2687.\n"},{"uri":"https://gardener.cloud/v1.12.8/guides/client_tools/helm/","title":"Use a Helm chart to deploy some application or service","tags":[],"description":"","content":"Basically, Helm Charts can be installed as described e.g. in the Helm QuickStart Guide. However, our clusters come with RBAC enabled by default hence Helm must be installed as follows:\nCreate a Service Account Create a service account via the following command:\ncat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: v1 kind: ServiceAccount metadata: name: helm namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: helm roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: helm namespace: kube-system EOF Initialize Helm Initialise Helm via helm init --service-account helm. You can now use helm.\nIn case of failure In case you have already executed helm init, but without the above service account, you will get the following error: Error: User \u0026quot;system:serviceaccount:kube-system:default\u0026quot; cannot list configmaps in the namespace \u0026quot;kube-system\u0026quot;. (get configmaps) (e.g. when you run helm list). You will now need to delete the Tiller deployment (Helm backend implicitly deployed to the Kubernetes cluster when you call helm init) as well as the local Helm files (usually $HELM_HOME is set to ~/.helm):\nkubectl delete deployment tiller-deploy --namespace=kube-system kubectl delete service tiller-deploy --namespace=kube-system rm -rf ~/.helm/ Now follow the instructions above. For more details see this Kubernetes Helm issue #2687.\n"},{"uri":"https://gardener.cloud/v1.13.2/guides/client_tools/helm/","title":"Use a Helm chart to deploy some application or service","tags":[],"description":"","content":"Basically, Helm Charts can be installed as described e.g. in the Helm QuickStart Guide. However, our clusters come with RBAC enabled by default hence Helm must be installed as follows:\nCreate a Service Account Create a service account via the following command:\ncat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: v1 kind: ServiceAccount metadata: name: helm namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: helm roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: helm namespace: kube-system EOF Initialize Helm Initialise Helm via helm init --service-account helm. You can now use helm.\nIn case of failure In case you have already executed helm init, but without the above service account, you will get the following error: Error: User \u0026quot;system:serviceaccount:kube-system:default\u0026quot; cannot list configmaps in the namespace \u0026quot;kube-system\u0026quot;. (get configmaps) (e.g. when you run helm list). You will now need to delete the Tiller deployment (Helm backend implicitly deployed to the Kubernetes cluster when you call helm init) as well as the local Helm files (usually $HELM_HOME is set to ~/.helm):\nkubectl delete deployment tiller-deploy --namespace=kube-system kubectl delete service tiller-deploy --namespace=kube-system rm -rf ~/.helm/ Now follow the instructions above. For more details see this Kubernetes Helm issue #2687.\n"},{"uri":"https://gardener.cloud/documentation/concepts/monitoring/user_alerts/","title":"User Alerts","tags":[],"description":"","content":"User Alerts    Alertname Severity Type Description     ApiServerUnreachableViaKubernetesService critical shoot The Api server has been unreachable for 3 minutes via the kubernetes service in the shoot.   CoreDNSDown critical shoot CoreDNS could not be found. Cluster DNS resolution will not work.   ApiServerNotReachable blocker seed API server not reachable via external endpoint: {{ $labels.instance }}.   KubeApiServerTooManyOpenFileDescriptors warning seed The API server ({{ $labels.instance }}) is using {{ $value }}% of the available file/socket descriptors.   KubeApiServerTooManyOpenFileDescriptors critical seed The API server ({{ $labels.instance }}) is using {{ $value }}% of the available file/socket descriptors.   KubeApiServerLatency warning seed Kube API server latency for verb {{ $labels.verb }} is high. This could be because the shoot workers and the control plane are in different regions. 99th percentile of request latency is greater than 3 seconds.   KubeKubeletNodeDown warning shoot The kubelet {{ $labels.instance }} has been unavailable/unreachable for more than 1 hour. Workloads on the affected node may not be schedulable.   KubeletTooManyOpenFileDescriptorsShoot warning shoot Shoot-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubeletTooManyOpenFileDescriptorsShoot critical shoot Shoot-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubePodPendingShoot warning shoot Pod {{ $labels.pod }} is stuck in \u0026quot;Pending\u0026quot; state for more than 1 hour.   KubePodNotReadyShoot warning shoot Pod {{ $labels.pod }} is not ready for more than 1 hour.   NoWorkerNodes blocker  There are no worker nodes in the cluster or all of the worker nodes in the cluster are not schedulable.   NodeExporterDown warning shoot The NodeExporter has been down or unreachable from Prometheus for more than 1 hour.   K8SNodeOutOfDisk critical shoot Node {{ $labels.node }} has run out of disk space.   K8SNodeMemoryPressure warning shoot Node {{ $labels.node }} is under memory pressure.   K8SNodeDiskPressure warning shoot Node {{ $labels.node }} is under disk pressure   VMRootfsFull critical shoot Root filesystem device on instance {{ $labels.instance }} is almost full.   VMConntrackTableFull critical shoot The nf_conntrack table is {{ $value }}% full.   VPNProbeAPIServerProxyFailed critical shoot The API Server proxy functionality is not working. Probably the vpn connection from an API Server pod to the vpn-shoot endpoint on the Shoot workers does not work.    "},{"uri":"https://gardener.cloud/v1.12.8/concepts/monitoring/user_alerts/","title":"User Alerts","tags":[],"description":"","content":"User Alerts    Alertname Severity Type Description     ApiServerUnreachableViaKubernetesService critical shoot The Api server has been unreachable for 3 minutes via the kubernetes service in the shoot.   CoreDNSDown critical shoot CoreDNS could not be found. Cluster DNS resolution will not work.   ApiServerNotReachable blocker seed API server not reachable via external endpoint: {{ $labels.instance }}.   KubeApiServerTooManyOpenFileDescriptors warning seed The API server ({{ $labels.instance }}) is using {{ $value }}% of the available file/socket descriptors.   KubeApiServerTooManyOpenFileDescriptors critical seed The API server ({{ $labels.instance }}) is using {{ $value }}% of the available file/socket descriptors.   KubeApiServerLatency warning seed Kube API server latency for verb {{ $labels.verb }} is high. This could be because the shoot workers and the control plane are in different regions. 99th percentile of request latency is greater than 3 seconds.   KubeKubeletNodeDown warning shoot The kubelet {{ $labels.instance }} has been unavailable/unreachable for more than 1 hour. Workloads on the affected node may not be schedulable.   KubeletTooManyOpenFileDescriptorsShoot warning shoot Shoot-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubeletTooManyOpenFileDescriptorsShoot critical shoot Shoot-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubePodPendingShoot warning shoot Pod {{ $labels.pod }} is stuck in \u0026quot;Pending\u0026quot; state for more than 1 hour.   KubePodNotReadyShoot warning shoot Pod {{ $labels.pod }} is not ready for more than 1 hour.   NoWorkerNodes blocker  There are no worker nodes in the cluster or all of the worker nodes in the cluster are not schedulable.   NodeExporterDown warning shoot The NodeExporter has been down or unreachable from Prometheus for more than 1 hour.   K8SNodeOutOfDisk critical shoot Node {{ $labels.node }} has run out of disk space.   K8SNodeMemoryPressure warning shoot Node {{ $labels.node }} is under memory pressure.   K8SNodeDiskPressure warning shoot Node {{ $labels.node }} is under disk pressure   VMRootfsFull critical shoot Root filesystem device on instance {{ $labels.instance }} is almost full.   VMConntrackTableFull critical shoot The nf_conntrack table is {{ $value }}% full.   VPNProbeAPIServerProxyFailed critical shoot The API Server proxy functionality is not working. Probably the vpn connection from an API Server pod to the vpn-shoot endpoint on the Shoot workers does not work.    "},{"uri":"https://gardener.cloud/v1.13.2/concepts/monitoring/user_alerts/","title":"User Alerts","tags":[],"description":"","content":"User Alerts    Alertname Severity Type Description     ApiServerUnreachableViaKubernetesService critical shoot The Api server has been unreachable for 3 minutes via the kubernetes service in the shoot.   CoreDNSDown critical shoot CoreDNS could not be found. Cluster DNS resolution will not work.   ApiServerNotReachable blocker seed API server not reachable via external endpoint: {{ $labels.instance }}.   KubeApiServerTooManyOpenFileDescriptors warning seed The API server ({{ $labels.instance }}) is using {{ $value }}% of the available file/socket descriptors.   KubeApiServerTooManyOpenFileDescriptors critical seed The API server ({{ $labels.instance }}) is using {{ $value }}% of the available file/socket descriptors.   KubeApiServerLatency warning seed Kube API server latency for verb {{ $labels.verb }} is high. This could be because the shoot workers and the control plane are in different regions. 99th percentile of request latency is greater than 3 seconds.   KubeKubeletNodeDown warning shoot The kubelet {{ $labels.instance }} has been unavailable/unreachable for more than 1 hour. Workloads on the affected node may not be schedulable.   KubeletTooManyOpenFileDescriptorsShoot warning shoot Shoot-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubeletTooManyOpenFileDescriptorsShoot critical shoot Shoot-kubelet ({{ $labels.kubernetes_io_hostname }}) is using {{ $value }}% of the available file/socket descriptors. Kubelet could be under heavy load.   KubePodPendingShoot warning shoot Pod {{ $labels.pod }} is stuck in \u0026quot;Pending\u0026quot; state for more than 1 hour.   KubePodNotReadyShoot warning shoot Pod {{ $labels.pod }} is not ready for more than 1 hour.   NoWorkerNodes blocker  There are no worker nodes in the cluster or all of the worker nodes in the cluster are not schedulable.   NodeExporterDown warning shoot The NodeExporter has been down or unreachable from Prometheus for more than 1 hour.   K8SNodeOutOfDisk critical shoot Node {{ $labels.node }} has run out of disk space.   K8SNodeMemoryPressure warning shoot Node {{ $labels.node }} is under memory pressure.   K8SNodeDiskPressure warning shoot Node {{ $labels.node }} is under disk pressure   VMRootfsFull critical shoot Root filesystem device on instance {{ $labels.instance }} is almost full.   VMConntrackTableFull critical shoot The nf_conntrack table is {{ $value }}% full.   VPNProbeAPIServerProxyFailed critical shoot The API Server proxy functionality is not working. Probably the vpn connection from an API Server pod to the vpn-shoot endpoint on the Shoot workers does not work.    "},{"uri":"https://gardener.cloud/documentation/guides/applications/prometheus/","title":"Using Prometheus and Grafana to monitor K8s","tags":[],"description":"How to deploy and configure Prometheus and Grafana to collect and monitor kubelet container metrics","content":"Disclaimer This post is meant to give a basic end-to-end description for deploying and using Prometheus and Grafana. Both applications offer a wide range of flexibility which needs to be considered in case you have specific requirenments. Such advanced details are not in the scope of this post.\nIntroduction Prometheus is an open-source systems monitoring and alerting toolkit for recording numeric time series. It fits both machine-centric monitoring as well as monitoring of highly dynamic service-oriented architectures. In a world of microservices, its support for multi-dimensional data collection and querying is a particular strength.\nPrometheus graduates within CNCF second hosted project.\nThe following characteristics make Prometheus a good match for monitoring Kubernetes clusters:\n  Pull-based monitoring\nPrometheus is a pull-based monitoring system, which means that the Prometheus server dynamically discovers and pulls metrics from your services running in Kubernetes.\n  Labels Prometheus and Kubernetes share the same label (key-value) concept that can be used to select objects in the system.\nLabels are used to identify time series and sets of label matchers can be used in the query language ( PromQL ) to select the time series to be aggregated..\n  Exporters\nThere are many exporters available which enable integration of databases or even other monitoring systems not already providing a way to export metrics to Prometheus. One prominent exporter is the so called node-exporter, which allows to monitor hardware and OS related metrics of Unix systems.\n  Powerful query language\nThe Prometheus query language PromQL lets the user select and aggregate time series data in real time. Results can either be shown as a graph, viewed as tabular data in the Prometheus expression browser, or consumed by external systems via the HTTP API.\n  Find query examples on Prometheus Query Examples.\nOne very popular open-source visualization tool not only for Prometheus is Grafana. Grafana is a metric analytics and visualization suite. It is popular for for visualizing time series data for infrastructure and application analytics but many use it in other domains including industrial sensors, home automation, weather, and process control [see Grafana Documentation].\nGrafana accesses data via Data Sources. The continuously growing list of supported backends includes Prometheus.\nDashboards are created by combining panels, e.g. Graph and Dashlist.\nIn this example we describe an End-To-End scenario including the deployment of Prometheus and a basic monitoring configuration as the one provided for Kubernetes clusters created by Gardener.\nIf you miss elements on the Prometheus web page when accessing it via its service URL https://\u0026lt;your K8s FQN\u0026gt;/api/v1/namespaces/\u0026lt;your-prometheus-namespace\u0026gt;/services/prometheus-prometheus-server:80/proxy this is probably caused by Prometheus issue #1583 To workaround this issue setup a port forward kubectl port-forward -n \u0026lt;your-prometheus-namespace\u0026gt; \u0026lt;prometheus-pod\u0026gt; 9090:9090 on your client and access the Prometheus UI from there with your locally installed web browser. This issue is not relevant in case you use the service type LoadBalancer.\nPreparation The deployment of Prometheus and Grafana is based on Helm charts.\nMake sure to implement the Helm settings before deploying the Helm charts.\nThe Kubernetes clusters provided by Gardener use role based access control (RBAC). To authorize the Prometheus node-exporter to access hardware and OS relevant metrics of your cluster\u0026rsquo;s worker nodes specific artifacts need to be deployed.\nBind the prometheus service account to the garden.sapcloud.io:monitoring:prometheus cluster role by running the command kubectl apply -f crbinding.yaml.\nContent of crbinding.yaml\napiVersion:rbac.authorization.k8s.io/v1beta1kind:ClusterRoleBindingmetadata:name:\u0026lt;your-prometheus-name\u0026gt;-serverroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:garden.sapcloud.io:monitoring:prometheussubjects:- kind:ServiceAccountname:\u0026lt;your-prometheus-name\u0026gt;-servernamespace:\u0026lt;your-prometheus-namespace\u0026gt;Deployment of Prometheus and Grafana Only minor changes are needed to deploy Prometheus and Grafana based on Helm charts.\nCopy the following configuration into a file called values.yaml and deploy Prometheus: helm install \u0026lt;your-prometheus-name\u0026gt; --namespace \u0026lt;your-prometheus-namespace\u0026gt; stable/prometheus -f values.yaml\nTypically, Prometheus and Grafana are deployed into the same namespace. There is no technical reason behind this so feel free to choose different namespaces.\nContent of values.yaml for Prometheus:\nrbac:create:false# Already created in Preparation stepnodeExporter:enabled:false# The node-exporter is already deployed by defaultserver:global:scrape_interval:30sscrape_timeout:30sserverFiles:prometheus.yml:rule_files:- /etc/config/rules- /etc/config/alertsscrape_configs:- job_name:\u0026#39;kube-kubelet\u0026#39;honor_labels:falsescheme:httpstls_config:# This is needed because the kubelets\u0026#39; certificates are not generated# for a specific pod IPinsecure_skip_verify:truebearer_token_file:/var/run/secrets/kubernetes.io/serviceaccount/tokenkubernetes_sd_configs:- role:noderelabel_configs:- target_label:__metrics_path__replacement:/metrics- source_labels:[__meta_kubernetes_node_address_InternalIP]target_label:instance- action:labelmapregex:__meta_kubernetes_node_label_(.+)- job_name:\u0026#39;kube-kubelet-cadvisor\u0026#39;honor_labels:falsescheme:httpstls_config:# This is needed because the kubelets\u0026#39; certificates are not generated# for a specific pod IPinsecure_skip_verify:truebearer_token_file:/var/run/secrets/kubernetes.io/serviceaccount/tokenkubernetes_sd_configs:- role:noderelabel_configs:- target_label:__metrics_path__replacement:/metrics/cadvisor- source_labels:[__meta_kubernetes_node_address_InternalIP]target_label:instance- action:labelmapregex:__meta_kubernetes_node_label_(.+)# Example scrape config for probing services via the Blackbox Exporter.## Relabelling allows to configure the actual service scrape endpoint using the following annotations:## * `prometheus.io/probe`: Only probe services that have a value of `true`- job_name:\u0026#39;kubernetes-services\u0026#39;metrics_path:/probeparams:module:[http_2xx]kubernetes_sd_configs:- role:servicerelabel_configs:- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_probe]action:keepregex:true- source_labels:[__address__]target_label:__param_target- target_label:__address__replacement:blackbox- source_labels:[__param_target]target_label:instance- action:labelmapregex:__meta_kubernetes_service_label_(.+)- source_labels:[__meta_kubernetes_namespace]target_label:kubernetes_namespace- source_labels:[__meta_kubernetes_service_name]target_label:kubernetes_name# Example scrape config for pods## Relabelling allows to configure the actual service scrape endpoint using the following annotations:## * `prometheus.io/scrape`: Only scrape pods that have a value of `true`# * `prometheus.io/path`: If the metrics path is not `/metrics` override this.# * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.- job_name:\u0026#39;kubernetes-pods\u0026#39;kubernetes_sd_configs:- role:podrelabel_configs:- source_labels:[__meta_kubernetes_pod_annotation_prometheus_io_scrape]action:keepregex:true- source_labels:[__meta_kubernetes_pod_annotation_prometheus_io_path]action:replacetarget_label:__metrics_path__regex:(.+)- source_labels:[__address__,__meta_kubernetes_pod_annotation_prometheus_io_port]action:replaceregex:(.+):(?:\\d+);(\\d+)replacement:${1}:${2}target_label:__address__- action:labelmapregex:__meta_kubernetes_pod_label_(.+)- source_labels:[__meta_kubernetes_namespace]action:replacetarget_label:kubernetes_namespace- source_labels:[__meta_kubernetes_pod_name]action:replacetarget_label:kubernetes_pod_name# Scrape config for service endpoints.## The relabeling allows the actual service scrape endpoint to be configured# via the following annotations:## * `prometheus.io/scrape`: Only scrape services that have a value of `true`# * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need# to set this to `https` \u0026amp; most likely set the `tls_config` of the scrape config.# * `prometheus.io/path`: If the metrics path is not `/metrics` override this.# * `prometheus.io/port`: If the metrics are exposed on a different port to the# service then set this appropriately.- job_name:\u0026#39;kubernetes-service-endpoints\u0026#39;kubernetes_sd_configs:- role:endpointsrelabel_configs:- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_scrape]action:keepregex:true- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_scheme]action:replacetarget_label:__scheme__regex:(https?)- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_path]action:replacetarget_label:__metrics_path__regex:(.+)- source_labels:[__address__,__meta_kubernetes_service_annotation_prometheus_io_port]action:replacetarget_label:__address__regex:(.+)(?::\\d+);(\\d+)replacement:$1:$2- action:labelmapregex:__meta_kubernetes_service_label_(.+)- source_labels:[__meta_kubernetes_namespace]action:replacetarget_label:kubernetes_namespace- source_labels:[__meta_kubernetes_service_name]action:replacetarget_label:kubernetes_name# Add your additional configuration here...Next, deploy Grafana. Since the deployment in this post is based on the Helm default values, the settings below are set explicitly in case the default changed. Deploy Grafana via helm install grafana --namespace \u0026lt;your-prometheus-namespace\u0026gt; stable/grafana -f values.yaml. Here, the same namespace is chosen for Prometheus and for Grafana.\nContent of values.yaml for Grafana:\nserver:ingress:enabled:falseservice:type:ClusterIPCheck the running state of the pods on the Kubernetes Dashboard or by running kubectl get pods -n \u0026lt;your-prometheus-namespace\u0026gt;. In case of errors check the log files of the pod(s) in question.\nThe text output of Helm after the deployment of Prometheus and Grafana contains very useful information, e.g. the user and password of the Grafana Admin user. The credentials are stored as secrets in the namespace \u0026lt;your-prometheus-namespace\u0026gt; and could be decoded via kubectl get secret --namespace \u0026lt;my-grafana-namespace\u0026gt; grafana -o jsonpath=\u0026quot;{.data.admin-password}\u0026quot; | base64 --decode ; echo.\nBasic functional tests To access the web UI of both applications use port forwarding of port 9090.\nSetup port forwarding for port 9090:\nkubectl port-forward -n \u0026lt;your-prometheus-namespace\u0026gt; \u0026lt;your-prometheus-server-pod\u0026gt; 9090:9090 Open http://localhost:9090 in your web browser. Select Graph from the top tab and enter the following expressing to show the overall CPU usage for a server (see Prometheus Query Examples)\n100 * (1 - avg by(instance)(irate(node_cpu{mode=\u0026#39;idle\u0026#39;}[5m]))) This should show some data in a graph.\nTo show the same data in Grafana setup port forwarding for port 3000 for the Grafana pod and open the Grafana Web UI by opening http://localhost:3000 in a browser. Enter the credentials of the admin user.\nNext, you need to enter the server name of your Prometheus deployment. This name is shown directly after the installation via helm.\nRun\nhelm status \u0026lt;your-prometheus-name\u0026gt; to find this name. Below this server name is referenced by \u0026lt;your-prometheus-server-name\u0026gt;.\nFirst, you need to add your Prometheus server as data source.\n select Dashboards  Data Sources select Add data source enter Name: \u0026lt;your-prometheus-datasource-name\u0026gt;\nType: Prometheus\nURL: http://\u0026lt;your-prometheus-server-name\u0026gt;\n_Access: proxy select Save \u0026amp; Test  In case of failure check the Prometheus URL in the Kubernetes Dashboard.\nTo add a Graph follow these steps:\n in the left corner, select Dashboards  New to create a new dashboard select Graph to create a new graph next, select the Panel Title  Edit select your Prometheus Data Source in the drop down list enter the expression 100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m]))) in the entry field A select the floppy disk symbol (Save) on top  Now you should have a very basic Prometheus and Grafana setup for your Kubernetes cluster.\nAs a next step you can implement monitoring for your applications by implementing the Prometheus client API.\nLinks  Prometheus Prometheus Helm Chart Prometheus and Kubernetes: A Perfect Match Grafana Grafana Helm Chart  "},{"uri":"https://gardener.cloud/v1.12.8/guides/applications/prometheus/","title":"Using Prometheus and Grafana to monitor K8s","tags":[],"description":"How to deploy and configure Prometheus and Grafana to collect and monitor kubelet container metrics","content":"Disclaimer This post is meant to give a basic end-to-end description for deploying and using Prometheus and Grafana. Both applications offer a wide range of flexibility which needs to be considered in case you have specific requirenments. Such advanced details are not in the scope of this post.\nIntroduction Prometheus is an open-source systems monitoring and alerting toolkit for recording numeric time series. It fits both machine-centric monitoring as well as monitoring of highly dynamic service-oriented architectures. In a world of microservices, its support for multi-dimensional data collection and querying is a particular strength.\nPrometheus graduates within CNCF second hosted project.\nThe following characteristics make Prometheus a good match for monitoring Kubernetes clusters:\n  Pull-based monitoring\nPrometheus is a pull-based monitoring system, which means that the Prometheus server dynamically discovers and pulls metrics from your services running in Kubernetes.\n  Labels Prometheus and Kubernetes share the same label (key-value) concept that can be used to select objects in the system.\nLabels are used to identify time series and sets of label matchers can be used in the query language ( PromQL ) to select the time series to be aggregated..\n  Exporters\nThere are many exporters available which enable integration of databases or even other monitoring systems not already providing a way to export metrics to Prometheus. One prominent exporter is the so called node-exporter, which allows to monitor hardware and OS related metrics of Unix systems.\n  Powerful query language\nThe Prometheus query language PromQL lets the user select and aggregate time series data in real time. Results can either be shown as a graph, viewed as tabular data in the Prometheus expression browser, or consumed by external systems via the HTTP API.\n  Find query examples on Prometheus Query Examples.\nOne very popular open-source visualization tool not only for Prometheus is Grafana. Grafana is a metric analytics and visualization suite. It is popular for for visualizing time series data for infrastructure and application analytics but many use it in other domains including industrial sensors, home automation, weather, and process control [see Grafana Documentation].\nGrafana accesses data via Data Sources. The continuously growing list of supported backends includes Prometheus.\nDashboards are created by combining panels, e.g. Graph and Dashlist.\nIn this example we describe an End-To-End scenario including the deployment of Prometheus and a basic monitoring configuration as the one provided for Kubernetes clusters created by Gardener.\nIf you miss elements on the Prometheus web page when accessing it via its service URL https://\u0026lt;your K8s FQN\u0026gt;/api/v1/namespaces/\u0026lt;your-prometheus-namespace\u0026gt;/services/prometheus-prometheus-server:80/proxy this is probably caused by Prometheus issue #1583 To workaround this issue setup a port forward kubectl port-forward -n \u0026lt;your-prometheus-namespace\u0026gt; \u0026lt;prometheus-pod\u0026gt; 9090:9090 on your client and access the Prometheus UI from there with your locally installed web browser. This issue is not relevant in case you use the service type LoadBalancer.\nPreparation The deployment of Prometheus and Grafana is based on Helm charts.\nMake sure to implement the Helm settings before deploying the Helm charts.\nThe Kubernetes clusters provided by Gardener use role based access control (RBAC). To authorize the Prometheus node-exporter to access hardware and OS relevant metrics of your cluster\u0026rsquo;s worker nodes specific artifacts need to be deployed.\nBind the prometheus service account to the garden.sapcloud.io:monitoring:prometheus cluster role by running the command kubectl apply -f crbinding.yaml.\nContent of crbinding.yaml\napiVersion:rbac.authorization.k8s.io/v1beta1kind:ClusterRoleBindingmetadata:name:\u0026lt;your-prometheus-name\u0026gt;-serverroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:garden.sapcloud.io:monitoring:prometheussubjects:- kind:ServiceAccountname:\u0026lt;your-prometheus-name\u0026gt;-servernamespace:\u0026lt;your-prometheus-namespace\u0026gt;Deployment of Prometheus and Grafana Only minor changes are needed to deploy Prometheus and Grafana based on Helm charts.\nCopy the following configuration into a file called values.yaml and deploy Prometheus: helm install \u0026lt;your-prometheus-name\u0026gt; --namespace \u0026lt;your-prometheus-namespace\u0026gt; stable/prometheus -f values.yaml\nTypically, Prometheus and Grafana are deployed into the same namespace. There is no technical reason behind this so feel free to choose different namespaces.\nContent of values.yaml for Prometheus:\nrbac:create:false# Already created in Preparation stepnodeExporter:enabled:false# The node-exporter is already deployed by defaultserver:global:scrape_interval:30sscrape_timeout:30sserverFiles:prometheus.yml:rule_files:- /etc/config/rules- /etc/config/alertsscrape_configs:- job_name:\u0026#39;kube-kubelet\u0026#39;honor_labels:falsescheme:httpstls_config:# This is needed because the kubelets\u0026#39; certificates are not generated# for a specific pod IPinsecure_skip_verify:truebearer_token_file:/var/run/secrets/kubernetes.io/serviceaccount/tokenkubernetes_sd_configs:- role:noderelabel_configs:- target_label:__metrics_path__replacement:/metrics- source_labels:[__meta_kubernetes_node_address_InternalIP]target_label:instance- action:labelmapregex:__meta_kubernetes_node_label_(.+)- job_name:\u0026#39;kube-kubelet-cadvisor\u0026#39;honor_labels:falsescheme:httpstls_config:# This is needed because the kubelets\u0026#39; certificates are not generated# for a specific pod IPinsecure_skip_verify:truebearer_token_file:/var/run/secrets/kubernetes.io/serviceaccount/tokenkubernetes_sd_configs:- role:noderelabel_configs:- target_label:__metrics_path__replacement:/metrics/cadvisor- source_labels:[__meta_kubernetes_node_address_InternalIP]target_label:instance- action:labelmapregex:__meta_kubernetes_node_label_(.+)# Example scrape config for probing services via the Blackbox Exporter.## Relabelling allows to configure the actual service scrape endpoint using the following annotations:## * `prometheus.io/probe`: Only probe services that have a value of `true`- job_name:\u0026#39;kubernetes-services\u0026#39;metrics_path:/probeparams:module:[http_2xx]kubernetes_sd_configs:- role:servicerelabel_configs:- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_probe]action:keepregex:true- source_labels:[__address__]target_label:__param_target- target_label:__address__replacement:blackbox- source_labels:[__param_target]target_label:instance- action:labelmapregex:__meta_kubernetes_service_label_(.+)- source_labels:[__meta_kubernetes_namespace]target_label:kubernetes_namespace- source_labels:[__meta_kubernetes_service_name]target_label:kubernetes_name# Example scrape config for pods## Relabelling allows to configure the actual service scrape endpoint using the following annotations:## * `prometheus.io/scrape`: Only scrape pods that have a value of `true`# * `prometheus.io/path`: If the metrics path is not `/metrics` override this.# * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.- job_name:\u0026#39;kubernetes-pods\u0026#39;kubernetes_sd_configs:- role:podrelabel_configs:- source_labels:[__meta_kubernetes_pod_annotation_prometheus_io_scrape]action:keepregex:true- source_labels:[__meta_kubernetes_pod_annotation_prometheus_io_path]action:replacetarget_label:__metrics_path__regex:(.+)- source_labels:[__address__,__meta_kubernetes_pod_annotation_prometheus_io_port]action:replaceregex:(.+):(?:\\d+);(\\d+)replacement:${1}:${2}target_label:__address__- action:labelmapregex:__meta_kubernetes_pod_label_(.+)- source_labels:[__meta_kubernetes_namespace]action:replacetarget_label:kubernetes_namespace- source_labels:[__meta_kubernetes_pod_name]action:replacetarget_label:kubernetes_pod_name# Scrape config for service endpoints.## The relabeling allows the actual service scrape endpoint to be configured# via the following annotations:## * `prometheus.io/scrape`: Only scrape services that have a value of `true`# * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need# to set this to `https` \u0026amp; most likely set the `tls_config` of the scrape config.# * `prometheus.io/path`: If the metrics path is not `/metrics` override this.# * `prometheus.io/port`: If the metrics are exposed on a different port to the# service then set this appropriately.- job_name:\u0026#39;kubernetes-service-endpoints\u0026#39;kubernetes_sd_configs:- role:endpointsrelabel_configs:- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_scrape]action:keepregex:true- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_scheme]action:replacetarget_label:__scheme__regex:(https?)- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_path]action:replacetarget_label:__metrics_path__regex:(.+)- source_labels:[__address__,__meta_kubernetes_service_annotation_prometheus_io_port]action:replacetarget_label:__address__regex:(.+)(?::\\d+);(\\d+)replacement:$1:$2- action:labelmapregex:__meta_kubernetes_service_label_(.+)- source_labels:[__meta_kubernetes_namespace]action:replacetarget_label:kubernetes_namespace- source_labels:[__meta_kubernetes_service_name]action:replacetarget_label:kubernetes_name# Add your additional configuration here...Next, deploy Grafana. Since the deployment in this post is based on the Helm default values, the settings below are set explicitly in case the default changed. Deploy Grafana via helm install grafana --namespace \u0026lt;your-prometheus-namespace\u0026gt; stable/grafana -f values.yaml. Here, the same namespace is chosen for Prometheus and for Grafana.\nContent of values.yaml for Grafana:\nserver:ingress:enabled:falseservice:type:ClusterIPCheck the running state of the pods on the Kubernetes Dashboard or by running kubectl get pods -n \u0026lt;your-prometheus-namespace\u0026gt;. In case of errors check the log files of the pod(s) in question.\nThe text output of Helm after the deployment of Prometheus and Grafana contains very useful information, e.g. the user and password of the Grafana Admin user. The credentials are stored as secrets in the namespace \u0026lt;your-prometheus-namespace\u0026gt; and could be decoded via kubectl get secret --namespace \u0026lt;my-grafana-namespace\u0026gt; grafana -o jsonpath=\u0026quot;{.data.admin-password}\u0026quot; | base64 --decode ; echo.\nBasic functional tests To access the web UI of both applications use port forwarding of port 9090.\nSetup port forwarding for port 9090:\nkubectl port-forward -n \u0026lt;your-prometheus-namespace\u0026gt; \u0026lt;your-prometheus-server-pod\u0026gt; 9090:9090 Open http://localhost:9090 in your web browser. Select Graph from the top tab and enter the following expressing to show the overall CPU usage for a server (see Prometheus Query Examples)\n100 * (1 - avg by(instance)(irate(node_cpu{mode=\u0026#39;idle\u0026#39;}[5m]))) This should show some data in a graph.\nTo show the same data in Grafana setup port forwarding for port 3000 for the Grafana pod and open the Grafana Web UI by opening http://localhost:3000 in a browser. Enter the credentials of the admin user.\nNext, you need to enter the server name of your Prometheus deployment. This name is shown directly after the installation via helm.\nRun\nhelm status \u0026lt;your-prometheus-name\u0026gt; to find this name. Below this server name is referenced by \u0026lt;your-prometheus-server-name\u0026gt;.\nFirst, you need to add your Prometheus server as data source.\n select Dashboards  Data Sources select Add data source enter Name: \u0026lt;your-prometheus-datasource-name\u0026gt;\nType: Prometheus\nURL: http://\u0026lt;your-prometheus-server-name\u0026gt;\n_Access: proxy select Save \u0026amp; Test  In case of failure check the Prometheus URL in the Kubernetes Dashboard.\nTo add a Graph follow these steps:\n in the left corner, select Dashboards  New to create a new dashboard select Graph to create a new graph next, select the Panel Title  Edit select your Prometheus Data Source in the drop down list enter the expression 100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m]))) in the entry field A select the floppy disk symbol (Save) on top  Now you should have a very basic Prometheus and Grafana setup for your Kubernetes cluster.\nAs a next step you can implement monitoring for your applications by implementing the Prometheus client API.\nLinks  Prometheus Prometheus Helm Chart Prometheus and Kubernetes: A Perfect Match Grafana Grafana Helm Chart  "},{"uri":"https://gardener.cloud/v1.13.2/guides/applications/prometheus/","title":"Using Prometheus and Grafana to monitor K8s","tags":[],"description":"How to deploy and configure Prometheus and Grafana to collect and monitor kubelet container metrics","content":"Disclaimer This post is meant to give a basic end-to-end description for deploying and using Prometheus and Grafana. Both applications offer a wide range of flexibility which needs to be considered in case you have specific requirenments. Such advanced details are not in the scope of this post.\nIntroduction Prometheus is an open-source systems monitoring and alerting toolkit for recording numeric time series. It fits both machine-centric monitoring as well as monitoring of highly dynamic service-oriented architectures. In a world of microservices, its support for multi-dimensional data collection and querying is a particular strength.\nPrometheus graduates within CNCF second hosted project.\nThe following characteristics make Prometheus a good match for monitoring Kubernetes clusters:\n  Pull-based monitoring\nPrometheus is a pull-based monitoring system, which means that the Prometheus server dynamically discovers and pulls metrics from your services running in Kubernetes.\n  Labels Prometheus and Kubernetes share the same label (key-value) concept that can be used to select objects in the system.\nLabels are used to identify time series and sets of label matchers can be used in the query language ( PromQL ) to select the time series to be aggregated..\n  Exporters\nThere are many exporters available which enable integration of databases or even other monitoring systems not already providing a way to export metrics to Prometheus. One prominent exporter is the so called node-exporter, which allows to monitor hardware and OS related metrics of Unix systems.\n  Powerful query language\nThe Prometheus query language PromQL lets the user select and aggregate time series data in real time. Results can either be shown as a graph, viewed as tabular data in the Prometheus expression browser, or consumed by external systems via the HTTP API.\n  Find query examples on Prometheus Query Examples.\nOne very popular open-source visualization tool not only for Prometheus is Grafana. Grafana is a metric analytics and visualization suite. It is popular for for visualizing time series data for infrastructure and application analytics but many use it in other domains including industrial sensors, home automation, weather, and process control [see Grafana Documentation].\nGrafana accesses data via Data Sources. The continuously growing list of supported backends includes Prometheus.\nDashboards are created by combining panels, e.g. Graph and Dashlist.\nIn this example we describe an End-To-End scenario including the deployment of Prometheus and a basic monitoring configuration as the one provided for Kubernetes clusters created by Gardener.\nIf you miss elements on the Prometheus web page when accessing it via its service URL https://\u0026lt;your K8s FQN\u0026gt;/api/v1/namespaces/\u0026lt;your-prometheus-namespace\u0026gt;/services/prometheus-prometheus-server:80/proxy this is probably caused by Prometheus issue #1583 To workaround this issue setup a port forward kubectl port-forward -n \u0026lt;your-prometheus-namespace\u0026gt; \u0026lt;prometheus-pod\u0026gt; 9090:9090 on your client and access the Prometheus UI from there with your locally installed web browser. This issue is not relevant in case you use the service type LoadBalancer.\nPreparation The deployment of Prometheus and Grafana is based on Helm charts.\nMake sure to implement the Helm settings before deploying the Helm charts.\nThe Kubernetes clusters provided by Gardener use role based access control (RBAC). To authorize the Prometheus node-exporter to access hardware and OS relevant metrics of your cluster\u0026rsquo;s worker nodes specific artifacts need to be deployed.\nBind the prometheus service account to the garden.sapcloud.io:monitoring:prometheus cluster role by running the command kubectl apply -f crbinding.yaml.\nContent of crbinding.yaml\napiVersion:rbac.authorization.k8s.io/v1beta1kind:ClusterRoleBindingmetadata:name:\u0026lt;your-prometheus-name\u0026gt;-serverroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:garden.sapcloud.io:monitoring:prometheussubjects:- kind:ServiceAccountname:\u0026lt;your-prometheus-name\u0026gt;-servernamespace:\u0026lt;your-prometheus-namespace\u0026gt;Deployment of Prometheus and Grafana Only minor changes are needed to deploy Prometheus and Grafana based on Helm charts.\nCopy the following configuration into a file called values.yaml and deploy Prometheus: helm install \u0026lt;your-prometheus-name\u0026gt; --namespace \u0026lt;your-prometheus-namespace\u0026gt; stable/prometheus -f values.yaml\nTypically, Prometheus and Grafana are deployed into the same namespace. There is no technical reason behind this so feel free to choose different namespaces.\nContent of values.yaml for Prometheus:\nrbac:create:false# Already created in Preparation stepnodeExporter:enabled:false# The node-exporter is already deployed by defaultserver:global:scrape_interval:30sscrape_timeout:30sserverFiles:prometheus.yml:rule_files:- /etc/config/rules- /etc/config/alertsscrape_configs:- job_name:\u0026#39;kube-kubelet\u0026#39;honor_labels:falsescheme:httpstls_config:# This is needed because the kubelets\u0026#39; certificates are not generated# for a specific pod IPinsecure_skip_verify:truebearer_token_file:/var/run/secrets/kubernetes.io/serviceaccount/tokenkubernetes_sd_configs:- role:noderelabel_configs:- target_label:__metrics_path__replacement:/metrics- source_labels:[__meta_kubernetes_node_address_InternalIP]target_label:instance- action:labelmapregex:__meta_kubernetes_node_label_(.+)- job_name:\u0026#39;kube-kubelet-cadvisor\u0026#39;honor_labels:falsescheme:httpstls_config:# This is needed because the kubelets\u0026#39; certificates are not generated# for a specific pod IPinsecure_skip_verify:truebearer_token_file:/var/run/secrets/kubernetes.io/serviceaccount/tokenkubernetes_sd_configs:- role:noderelabel_configs:- target_label:__metrics_path__replacement:/metrics/cadvisor- source_labels:[__meta_kubernetes_node_address_InternalIP]target_label:instance- action:labelmapregex:__meta_kubernetes_node_label_(.+)# Example scrape config for probing services via the Blackbox Exporter.## Relabelling allows to configure the actual service scrape endpoint using the following annotations:## * `prometheus.io/probe`: Only probe services that have a value of `true`- job_name:\u0026#39;kubernetes-services\u0026#39;metrics_path:/probeparams:module:[http_2xx]kubernetes_sd_configs:- role:servicerelabel_configs:- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_probe]action:keepregex:true- source_labels:[__address__]target_label:__param_target- target_label:__address__replacement:blackbox- source_labels:[__param_target]target_label:instance- action:labelmapregex:__meta_kubernetes_service_label_(.+)- source_labels:[__meta_kubernetes_namespace]target_label:kubernetes_namespace- source_labels:[__meta_kubernetes_service_name]target_label:kubernetes_name# Example scrape config for pods## Relabelling allows to configure the actual service scrape endpoint using the following annotations:## * `prometheus.io/scrape`: Only scrape pods that have a value of `true`# * `prometheus.io/path`: If the metrics path is not `/metrics` override this.# * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.- job_name:\u0026#39;kubernetes-pods\u0026#39;kubernetes_sd_configs:- role:podrelabel_configs:- source_labels:[__meta_kubernetes_pod_annotation_prometheus_io_scrape]action:keepregex:true- source_labels:[__meta_kubernetes_pod_annotation_prometheus_io_path]action:replacetarget_label:__metrics_path__regex:(.+)- source_labels:[__address__,__meta_kubernetes_pod_annotation_prometheus_io_port]action:replaceregex:(.+):(?:\\d+);(\\d+)replacement:${1}:${2}target_label:__address__- action:labelmapregex:__meta_kubernetes_pod_label_(.+)- source_labels:[__meta_kubernetes_namespace]action:replacetarget_label:kubernetes_namespace- source_labels:[__meta_kubernetes_pod_name]action:replacetarget_label:kubernetes_pod_name# Scrape config for service endpoints.## The relabeling allows the actual service scrape endpoint to be configured# via the following annotations:## * `prometheus.io/scrape`: Only scrape services that have a value of `true`# * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need# to set this to `https` \u0026amp; most likely set the `tls_config` of the scrape config.# * `prometheus.io/path`: If the metrics path is not `/metrics` override this.# * `prometheus.io/port`: If the metrics are exposed on a different port to the# service then set this appropriately.- job_name:\u0026#39;kubernetes-service-endpoints\u0026#39;kubernetes_sd_configs:- role:endpointsrelabel_configs:- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_scrape]action:keepregex:true- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_scheme]action:replacetarget_label:__scheme__regex:(https?)- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_path]action:replacetarget_label:__metrics_path__regex:(.+)- source_labels:[__address__,__meta_kubernetes_service_annotation_prometheus_io_port]action:replacetarget_label:__address__regex:(.+)(?::\\d+);(\\d+)replacement:$1:$2- action:labelmapregex:__meta_kubernetes_service_label_(.+)- source_labels:[__meta_kubernetes_namespace]action:replacetarget_label:kubernetes_namespace- source_labels:[__meta_kubernetes_service_name]action:replacetarget_label:kubernetes_name# Add your additional configuration here...Next, deploy Grafana. Since the deployment in this post is based on the Helm default values, the settings below are set explicitly in case the default changed. Deploy Grafana via helm install grafana --namespace \u0026lt;your-prometheus-namespace\u0026gt; stable/grafana -f values.yaml. Here, the same namespace is chosen for Prometheus and for Grafana.\nContent of values.yaml for Grafana:\nserver:ingress:enabled:falseservice:type:ClusterIPCheck the running state of the pods on the Kubernetes Dashboard or by running kubectl get pods -n \u0026lt;your-prometheus-namespace\u0026gt;. In case of errors check the log files of the pod(s) in question.\nThe text output of Helm after the deployment of Prometheus and Grafana contains very useful information, e.g. the user and password of the Grafana Admin user. The credentials are stored as secrets in the namespace \u0026lt;your-prometheus-namespace\u0026gt; and could be decoded via kubectl get secret --namespace \u0026lt;my-grafana-namespace\u0026gt; grafana -o jsonpath=\u0026quot;{.data.admin-password}\u0026quot; | base64 --decode ; echo.\nBasic functional tests To access the web UI of both applications use port forwarding of port 9090.\nSetup port forwarding for port 9090:\nkubectl port-forward -n \u0026lt;your-prometheus-namespace\u0026gt; \u0026lt;your-prometheus-server-pod\u0026gt; 9090:9090 Open http://localhost:9090 in your web browser. Select Graph from the top tab and enter the following expressing to show the overall CPU usage for a server (see Prometheus Query Examples)\n100 * (1 - avg by(instance)(irate(node_cpu{mode=\u0026#39;idle\u0026#39;}[5m]))) This should show some data in a graph.\nTo show the same data in Grafana setup port forwarding for port 3000 for the Grafana pod and open the Grafana Web UI by opening http://localhost:3000 in a browser. Enter the credentials of the admin user.\nNext, you need to enter the server name of your Prometheus deployment. This name is shown directly after the installation via helm.\nRun\nhelm status \u0026lt;your-prometheus-name\u0026gt; to find this name. Below this server name is referenced by \u0026lt;your-prometheus-server-name\u0026gt;.\nFirst, you need to add your Prometheus server as data source.\n select Dashboards  Data Sources select Add data source enter Name: \u0026lt;your-prometheus-datasource-name\u0026gt;\nType: Prometheus\nURL: http://\u0026lt;your-prometheus-server-name\u0026gt;\n_Access: proxy select Save \u0026amp; Test  In case of failure check the Prometheus URL in the Kubernetes Dashboard.\nTo add a Graph follow these steps:\n in the left corner, select Dashboards  New to create a new dashboard select Graph to create a new graph next, select the Panel Title  Edit select your Prometheus Data Source in the drop down list enter the expression 100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m]))) in the entry field A select the floppy disk symbol (Save) on top  Now you should have a very basic Prometheus and Grafana setup for your Kubernetes cluster.\nAs a next step you can implement monitoring for your applications by implementing the Prometheus client API.\nLinks  Prometheus Prometheus Helm Chart Prometheus and Kubernetes: A Perfect Match Grafana Grafana Helm Chart  "},{"uri":"https://gardener.cloud/documentation/concepts/extensions/worker/","title":"Worker resource","tags":[],"description":"","content":"Contract: Worker resource While the control plane of a shoot cluster is living in the seed and deployed as native Kubernetes workload, the worker nodes of the shoot clusters are normal virtual machines (VMs) in the end-users infrastructure account. The Gardener project features a sub-project called machine-controller-manager. This controller is extending the Kubernetes API using custom resource definitions to represent actual VMs as Machine objects inside a Kubernetes system. This approach unlocks the possibility to manage virtual machines in the Kubernetes style and benefit from all its design principles.\nWhat is the machine-controller-manager exactly doing? Generally, there are provider-specific MachineClass objects (AWSMachineClass, AzureMachineClass, etc.; similar to StorageClass), and MachineDeployment, MachineSet, and Machine objects (similar to Deployment, ReplicaSet, and Pod). A machine class describes where and how to create virtual machines (in which networks, region, availability zone, SSH key, user-data for bootstrapping, etc.) while a Machine results in an actual virtual machine. You can read up more information in the machine-controller-manager\u0026rsquo;s repository.\nBefore the introduction of the Worker extension resource Gardener was deploying the machine-controller-manager, the machine classes, and the machine deployments itself. Now, Gardener commissions an external, provider-specific controller to take over these tasks.\nWhat needs to be implemented to support a new worker provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Workermetadata:name:barnamespace:shoot--foo--barspec:type:azureregion:eu-west-1secretRef:name:cloudprovidernamespace:shoot--foo--barinfrastructureProviderStatus:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusec2:keyName:shoot--foo--bar-ssh-publickeyiam:instanceProfiles:- name:shoot--foo--bar-nodespurpose:nodesroles:- arn:arn:aws:iam::0123456789:role/shoot--foo--bar-nodespurpose:nodesvpc:id:vpc-0123456789securityGroups:- id:sg-1234567890purpose:nodessubnets:- id:subnet-01234purpose:nodeszone:eu-west-1b- id:subnet-56789purpose:publiczone:eu-west-1b- id:subnet-0123apurpose:nodeszone:eu-west-1c- id:subnet-5678apurpose:publiczone:eu-west-1cpools:- name:cpu-workerminimum:3maximum:5maxSurge:1maxUnavailable:0machineType:m4.largemachineImage:name:coreosversion:1967.5.0userData:c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0Kvolume:size:20Gitype:gp2zones:- eu-west-1b- eu-west-1cmachineControllerManager:drainTimeout:10mhealthTimeout:10mcreationTimeout:10mmaxEvictRetries:30nodeConditions:- ReadonlyFilesystem- DiskPressure- KernelDeadlockThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed virtual machines. Also, as you can see, Gardener copies the output of the infrastructure creation (.spec.infrastructureProviderStatus), see Infrastructure resource, into the .spec.\nIn the .spec.pools[] field the desired worker pools are listed. In the above example, one pool with machine type m4.large and min=3, max=5 machines shall be spread over two availability zones (eu-west-1b, eu-west-1c). This information together with the infrastructure status must be used to determine the proper configuration for the machine classes.\nThe spec.pools[].machineControllerManager field allows to configure the settings for machine-controller-manager component. Providers must populate these settings on worker-pool to the related fields in MachineDeployment.\nWhen seeing such a resource your controller must make sure that it deploys the machine-controller-manager next to the control plane in the seed cluster. After that, it must compute the desired machine classes and the desired machine deployments. Typically, one class maps to one deployment, and one class/deployment is created per availability zone. Following this convention, the created resource would look like this:\napiVersion:v1kind:Secretmetadata:name:shoot--foo--bar-cpu-worker-z1-3db65namespace:shoot--foo--barlabels:gardener.cloud/purpose:machineclasstype:Opaquedata:providerAccessKeyId:eW91ci1hd3MtYWNjZXNzLWtleS1pZAo=providerSecretAccessKey:eW91ci1hd3Mtc2VjcmV0LWFjY2Vzcy1rZXkKuserData:c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0K---apiVersion:machine.sapcloud.io/v1alpha1kind:AWSMachineClassmetadata:name:shoot--foo--bar-cpu-worker-z1-3db65namespace:shoot--foo--barspec:ami:ami-0123456789# Your controller must map the stated version to the provider specific machine image information, in the AWS case the AMI.blockDevices:- ebs:volumeSize:20volumeType:gp2iam:name:shoot--foo--bar-nodeskeyName:shoot--foo--bar-ssh-publickeymachineType:m4.largenetworkInterfaces:- securityGroupIDs:- sg-1234567890subnetID:subnet-01234region:eu-west-1secretRef:name:shoot--foo--bar-cpu-worker-z1-3db65namespace:shoot--foo--bartags:kubernetes.io/cluster/shoot--foo--bar:\u0026#34;1\u0026#34;kubernetes.io/role/node:\u0026#34;1\u0026#34;---apiVersion:machine.sapcloud.io/v1alpha1kind:MachineDeploymentmetadata:name:shoot--foo--bar-cpu-worker-z1namespace:shoot--foo--barspec:replicas:2selector:matchLabels:name:shoot--foo--bar-cpu-worker-z1strategy:type:RollingUpdaterollingUpdate:maxSurge:1maxUnavailable:0template:metadata:labels:name:shoot--foo--bar-cpu-worker-z1spec:class:kind:AWSMachineClassname:shoot--foo--bar-cpu-worker-z1-3db65for the first availability zone eu-west-1b, and\napiVersion:v1kind:Secretmetadata:name:shoot--foo--bar-cpu-worker-z2-5z6asnamespace:shoot--foo--barlabels:gardener.cloud/purpose:machineclasstype:Opaquedata:providerAccessKeyId:eW91ci1hd3MtYWNjZXNzLWtleS1pZAo=providerSecretAccessKey:eW91ci1hd3Mtc2VjcmV0LWFjY2Vzcy1rZXkKuserData:c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0K---apiVersion:machine.sapcloud.io/v1alpha1kind:AWSMachineClassmetadata:name:shoot--foo--bar-cpu-worker-z2-5z6asnamespace:shoot--foo--barspec:ami:ami-0123456789# Your controller must map the stated version to the provider specific machine image information, in the AWS case the AMI.blockDevices:- ebs:volumeSize:20volumeType:gp2iam:name:shoot--foo--bar-nodeskeyName:shoot--foo--bar-ssh-publickeymachineType:m4.largenetworkInterfaces:- securityGroupIDs:- sg-1234567890subnetID:subnet-0123aregion:eu-west-1secretRef:name:shoot--foo--bar-cpu-worker-z2-5z6asnamespace:shoot--foo--bartags:kubernetes.io/cluster/shoot--foo--bar:\u0026#34;1\u0026#34;kubernetes.io/role/node:\u0026#34;1\u0026#34;---apiVersion:machine.sapcloud.io/v1alpha1kind:MachineDeploymentmetadata:name:shoot--foo--bar-cpu-worker-z1namespace:shoot--foo--barspec:replicas:1selector:matchLabels:name:shoot--foo--bar-cpu-worker-z1strategy:type:RollingUpdaterollingUpdate:maxSurge:1maxUnavailable:0template:metadata:labels:name:shoot--foo--bar-cpu-worker-z1spec:class:kind:AWSMachineClassname:shoot--foo--bar-cpu-worker-z2-5z6asfor the second availability zone eu-west-1c.\nAnother convention is the 5-letter hash at the end of the machine class names. Most controllers compute a checksum out of the specification of the machine class. This helps to trigger a rolling update of the worker nodes if, for example, the machine image version changes. In this case, a new checksum will be generated which results in the creation of a new machine class. The MachineDeployment's machine class reference (.spec.template.spec.class.name) is updated which triggers the rolling update process in the machine-controller-manager. However, all of this is only a convention that eases writing the controller, but you can do it completely differently if you desire - as long as you make sure that the described behaviours are implemented correctly.\nAfter the machine classes and machine deployments have been created the machine-controller-manager will start talking to the provider\u0026rsquo;s IaaS API and create the virtual machines. Gardener makes sure that the content of the userData field that is used to bootstrap the machines contain the required configuration for installation of the kubelet and registering the VM as worker node in the shoot cluster. The Worker extension controller shall wait until all the created MachineDeployments indicate healthiness/readiness before it ends the control loop.\nDoes Gardener need some information that must be returned back? Another important benefit of the machine-controller-manager\u0026rsquo;s design principles (extending the Kubernetes API using CRDs) is that the cluster-autoscaler can be used without any provider-specific implementation. We have forked the upstream Kubernetes community\u0026rsquo;s cluster-autoscaler and extended it so that it understands the machine API. Definitely, we will merge it back into the community\u0026rsquo;s versions once it has been adapted properly.\nOur cluster-autoscaler only needs to know the minimum and maximum number of replicas per MachineDeployment and is ready to act without that it needs to talk to the provider APIs (it just modifies the .spec.replicas field in the MachineDeployment object). Gardener deploys this autoscaler if there is at least one worker pool that specifies max\u0026gt;min. In order to know how it needs to configure it, the provider-specific Worker extension controller must expose which MachineDeployments it had created and how the min/max numbers should look like.\nConsequently, your controller should write this information into the Worker resource\u0026rsquo;s .status.machineDeployments field:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Workermetadata:name:workernamespace:shoot--foo--barspec:...status:lastOperation:...machineDeployments:- name:shoot--foo--bar-cpu-worker-z1minimum:2maximum:3- name:shoot--foo--bar-cpu-worker-z2minimum:1maximum:2In order to support a new worker provider you need to write a controller that watches all Workers with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the AWS provider.\nThat sounds like a lot that needs to be done, can you help me? All of the described behaviour is mostly the same for every provider. The only difference is maybe the version/configuration of the machine-controller-manager, and the machine class specification itself. You can take a look at our extension library, especially the worker controller part where you will find a lot of utilities that you can use. Also, using the library you only need to implement your provider specifics - all the things that can be handled generically can be taken for free and do not need to be re-implemented. Take a look at the AWS worker controller for finding an example.\nNon-provider specific information required for worker creation All the providers require further information that is not provider specific but already part of the shoot resource. One example for such information is whether the shoot is hibernated or not. In this case all the virtual machines should be deleted/terminated, and after that the machine controller-manager should be scaled down. You can take a look at the AWS worker controller to see how it reads this information and how it is used. As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information that is not part of the Worker resource itself.\nReferences and additional resources  Worker API (Golang specification) Extension controller library Generic worker controller Exemplary implementation for the AWS provider  "},{"uri":"https://gardener.cloud/v1.12.8/concepts/extensions/worker/","title":"Worker resource","tags":[],"description":"","content":"Contract: Worker resource While the control plane of a shoot cluster is living in the seed and deployed as native Kubernetes workload, the worker nodes of the shoot clusters are normal virtual machines (VMs) in the end-users infrastructure account. The Gardener project features a sub-project called machine-controller-manager. This controller is extending the Kubernetes API using custom resource definitions to represent actual VMs as Machine objects inside a Kubernetes system. This approach unlocks the possibility to manage virtual machines in the Kubernetes style and benefit from all its design principles.\nWhat is the machine-controller-manager exactly doing? Generally, there are provider-specific MachineClass objects (AWSMachineClass, AzureMachineClass, etc.; similar to StorageClass), and MachineDeployment, MachineSet, and Machine objects (similar to Deployment, ReplicaSet, and Pod). A machine class describes where and how to create virtual machines (in which networks, region, availability zone, SSH key, user-data for bootstrapping, etc.) while a Machine results in an actual virtual machine. You can read up more information in the machine-controller-manager\u0026rsquo;s repository.\nBefore the introduction of the Worker extension resource Gardener was deploying the machine-controller-manager, the machine classes, and the machine deployments itself. Now, Gardener commissions an external, provider-specific controller to take over these tasks.\nWhat needs to be implemented to support a new worker provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Workermetadata:name:barnamespace:shoot--foo--barspec:type:azureregion:eu-west-1secretRef:name:cloudprovidernamespace:shoot--foo--barinfrastructureProviderStatus:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusec2:keyName:shoot--foo--bar-ssh-publickeyiam:instanceProfiles:- name:shoot--foo--bar-nodespurpose:nodesroles:- arn:arn:aws:iam::0123456789:role/shoot--foo--bar-nodespurpose:nodesvpc:id:vpc-0123456789securityGroups:- id:sg-1234567890purpose:nodessubnets:- id:subnet-01234purpose:nodeszone:eu-west-1b- id:subnet-56789purpose:publiczone:eu-west-1b- id:subnet-0123apurpose:nodeszone:eu-west-1c- id:subnet-5678apurpose:publiczone:eu-west-1cpools:- name:cpu-workerminimum:3maximum:5maxSurge:1maxUnavailable:0machineType:m4.largemachineImage:name:coreosversion:1967.5.0userData:c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0Kvolume:size:20Gitype:gp2zones:- eu-west-1b- eu-west-1cmachineControllerManager:drainTimeout:10mhealthTimeout:10mcreationTimeout:10mmaxEvictRetries:30nodeConditions:- ReadonlyFilesystem- DiskPressure- KernelDeadlockThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed virtual machines. Also, as you can see, Gardener copies the output of the infrastructure creation (.spec.infrastructureProviderStatus), see Infrastructure resource, into the .spec.\nIn the .spec.pools[] field the desired worker pools are listed. In the above example, one pool with machine type m4.large and min=3, max=5 machines shall be spread over two availability zones (eu-west-1b, eu-west-1c). This information together with the infrastructure status must be used to determine the proper configuration for the machine classes.\nThe spec.pools[].machineControllerManager field allows to configure the settings for machine-controller-manager component. Providers must populate these settings on worker-pool to the related fields in MachineDeployment.\nWhen seeing such a resource your controller must make sure that it deploys the machine-controller-manager next to the control plane in the seed cluster. After that, it must compute the desired machine classes and the desired machine deployments. Typically, one class maps to one deployment, and one class/deployment is created per availability zone. Following this convention, the created resource would look like this:\napiVersion:v1kind:Secretmetadata:name:shoot--foo--bar-cpu-worker-z1-3db65namespace:shoot--foo--barlabels:gardener.cloud/purpose:machineclasstype:Opaquedata:providerAccessKeyId:eW91ci1hd3MtYWNjZXNzLWtleS1pZAo=providerSecretAccessKey:eW91ci1hd3Mtc2VjcmV0LWFjY2Vzcy1rZXkKuserData:c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0K---apiVersion:machine.sapcloud.io/v1alpha1kind:AWSMachineClassmetadata:name:shoot--foo--bar-cpu-worker-z1-3db65namespace:shoot--foo--barspec:ami:ami-0123456789# Your controller must map the stated version to the provider specific machine image information, in the AWS case the AMI.blockDevices:- ebs:volumeSize:20volumeType:gp2iam:name:shoot--foo--bar-nodeskeyName:shoot--foo--bar-ssh-publickeymachineType:m4.largenetworkInterfaces:- securityGroupIDs:- sg-1234567890subnetID:subnet-01234region:eu-west-1secretRef:name:shoot--foo--bar-cpu-worker-z1-3db65namespace:shoot--foo--bartags:kubernetes.io/cluster/shoot--foo--bar:\u0026#34;1\u0026#34;kubernetes.io/role/node:\u0026#34;1\u0026#34;---apiVersion:machine.sapcloud.io/v1alpha1kind:MachineDeploymentmetadata:name:shoot--foo--bar-cpu-worker-z1namespace:shoot--foo--barspec:replicas:2selector:matchLabels:name:shoot--foo--bar-cpu-worker-z1strategy:type:RollingUpdaterollingUpdate:maxSurge:1maxUnavailable:0template:metadata:labels:name:shoot--foo--bar-cpu-worker-z1spec:class:kind:AWSMachineClassname:shoot--foo--bar-cpu-worker-z1-3db65for the first availability zone eu-west-1b, and\napiVersion:v1kind:Secretmetadata:name:shoot--foo--bar-cpu-worker-z2-5z6asnamespace:shoot--foo--barlabels:gardener.cloud/purpose:machineclasstype:Opaquedata:providerAccessKeyId:eW91ci1hd3MtYWNjZXNzLWtleS1pZAo=providerSecretAccessKey:eW91ci1hd3Mtc2VjcmV0LWFjY2Vzcy1rZXkKuserData:c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0K---apiVersion:machine.sapcloud.io/v1alpha1kind:AWSMachineClassmetadata:name:shoot--foo--bar-cpu-worker-z2-5z6asnamespace:shoot--foo--barspec:ami:ami-0123456789# Your controller must map the stated version to the provider specific machine image information, in the AWS case the AMI.blockDevices:- ebs:volumeSize:20volumeType:gp2iam:name:shoot--foo--bar-nodeskeyName:shoot--foo--bar-ssh-publickeymachineType:m4.largenetworkInterfaces:- securityGroupIDs:- sg-1234567890subnetID:subnet-0123aregion:eu-west-1secretRef:name:shoot--foo--bar-cpu-worker-z2-5z6asnamespace:shoot--foo--bartags:kubernetes.io/cluster/shoot--foo--bar:\u0026#34;1\u0026#34;kubernetes.io/role/node:\u0026#34;1\u0026#34;---apiVersion:machine.sapcloud.io/v1alpha1kind:MachineDeploymentmetadata:name:shoot--foo--bar-cpu-worker-z1namespace:shoot--foo--barspec:replicas:1selector:matchLabels:name:shoot--foo--bar-cpu-worker-z1strategy:type:RollingUpdaterollingUpdate:maxSurge:1maxUnavailable:0template:metadata:labels:name:shoot--foo--bar-cpu-worker-z1spec:class:kind:AWSMachineClassname:shoot--foo--bar-cpu-worker-z2-5z6asfor the second availability zone eu-west-1c.\nAnother convention is the 5-letter hash at the end of the machine class names. Most controllers compute a checksum out of the specification of the machine class. This helps to trigger a rolling update of the worker nodes if, for example, the machine image version changes. In this case, a new checksum will be generated which results in the creation of a new machine class. The MachineDeployment's machine class reference (.spec.template.spec.class.name) is updated which triggers the rolling update process in the machine-controller-manager. However, all of this is only a convention that eases writing the controller, but you can do it completely differently if you desire - as long as you make sure that the described behaviours are implemented correctly.\nAfter the machine classes and machine deployments have been created the machine-controller-manager will start talking to the provider\u0026rsquo;s IaaS API and create the virtual machines. Gardener makes sure that the content of the userData field that is used to bootstrap the machines contain the required configuration for installation of the kubelet and registering the VM as worker node in the shoot cluster. The Worker extension controller shall wait until all the created MachineDeployments indicate healthiness/readiness before it ends the control loop.\nDoes Gardener need some information that must be returned back? Another important benefit of the machine-controller-manager\u0026rsquo;s design principles (extending the Kubernetes API using CRDs) is that the cluster-autoscaler can be used without any provider-specific implementation. We have forked the upstream Kubernetes community\u0026rsquo;s cluster-autoscaler and extended it so that it understands the machine API. Definitely, we will merge it back into the community\u0026rsquo;s versions once it has been adapted properly.\nOur cluster-autoscaler only needs to know the minimum and maximum number of replicas per MachineDeployment and is ready to act without that it needs to talk to the provider APIs (it just modifies the .spec.replicas field in the MachineDeployment object). Gardener deploys this autoscaler if there is at least one worker pool that specifies max\u0026gt;min. In order to know how it needs to configure it, the provider-specific Worker extension controller must expose which MachineDeployments it had created and how the min/max numbers should look like.\nConsequently, your controller should write this information into the Worker resource\u0026rsquo;s .status.machineDeployments field:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Workermetadata:name:workernamespace:shoot--foo--barspec:...status:lastOperation:...machineDeployments:- name:shoot--foo--bar-cpu-worker-z1minimum:2maximum:3- name:shoot--foo--bar-cpu-worker-z2minimum:1maximum:2In order to support a new worker provider you need to write a controller that watches all Workers with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the AWS provider.\nThat sounds like a lot that needs to be done, can you help me? All of the described behaviour is mostly the same for every provider. The only difference is maybe the version/configuration of the machine-controller-manager, and the machine class specification itself. You can take a look at our extension library, especially the worker controller part where you will find a lot of utilities that you can use. Also, using the library you only need to implement your provider specifics - all the things that can be handled generically can be taken for free and do not need to be re-implemented. Take a look at the AWS worker controller for finding an example.\nNon-provider specific information required for worker creation All the providers require further information that is not provider specific but already part of the shoot resource. One example for such information is whether the shoot is hibernated or not. In this case all the virtual machines should be deleted/terminated, and after that the machine controller-manager should be scaled down. You can take a look at the AWS worker controller to see how it reads this information and how it is used. As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information that is not part of the Worker resource itself.\nReferences and additional resources  Worker API (Golang specification) Extension controller library Generic worker controller Exemplary implementation for the AWS provider  "},{"uri":"https://gardener.cloud/v1.13.2/concepts/extensions/worker/","title":"Worker resource","tags":[],"description":"","content":"Contract: Worker resource While the control plane of a shoot cluster is living in the seed and deployed as native Kubernetes workload, the worker nodes of the shoot clusters are normal virtual machines (VMs) in the end-users infrastructure account. The Gardener project features a sub-project called machine-controller-manager. This controller is extending the Kubernetes API using custom resource definitions to represent actual VMs as Machine objects inside a Kubernetes system. This approach unlocks the possibility to manage virtual machines in the Kubernetes style and benefit from all its design principles.\nWhat is the machine-controller-manager exactly doing? Generally, there are provider-specific MachineClass objects (AWSMachineClass, AzureMachineClass, etc.; similar to StorageClass), and MachineDeployment, MachineSet, and Machine objects (similar to Deployment, ReplicaSet, and Pod). A machine class describes where and how to create virtual machines (in which networks, region, availability zone, SSH key, user-data for bootstrapping, etc.) while a Machine results in an actual virtual machine. You can read up more information in the machine-controller-manager\u0026rsquo;s repository.\nBefore the introduction of the Worker extension resource Gardener was deploying the machine-controller-manager, the machine classes, and the machine deployments itself. Now, Gardener commissions an external, provider-specific controller to take over these tasks.\nWhat needs to be implemented to support a new worker provider? As part of the shoot flow Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Workermetadata:name:barnamespace:shoot--foo--barspec:type:azureregion:eu-west-1secretRef:name:cloudprovidernamespace:shoot--foo--barinfrastructureProviderStatus:apiVersion:aws.provider.extensions.gardener.cloud/v1alpha1kind:InfrastructureStatusec2:keyName:shoot--foo--bar-ssh-publickeyiam:instanceProfiles:- name:shoot--foo--bar-nodespurpose:nodesroles:- arn:arn:aws:iam::0123456789:role/shoot--foo--bar-nodespurpose:nodesvpc:id:vpc-0123456789securityGroups:- id:sg-1234567890purpose:nodessubnets:- id:subnet-01234purpose:nodeszone:eu-west-1b- id:subnet-56789purpose:publiczone:eu-west-1b- id:subnet-0123apurpose:nodeszone:eu-west-1c- id:subnet-5678apurpose:publiczone:eu-west-1cpools:- name:cpu-workerminimum:3maximum:5maxSurge:1maxUnavailable:0machineType:m4.largemachineImage:name:coreosversion:1967.5.0userData:c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0Kvolume:size:20Gitype:gp2zones:- eu-west-1b- eu-west-1cmachineControllerManager:drainTimeout:10mhealthTimeout:10mcreationTimeout:10mmaxEvictRetries:30nodeConditions:- ReadonlyFilesystem- DiskPressure- KernelDeadlockThe .spec.secretRef contains a reference to the provider secret pointing to the account that shall be used to create the needed virtual machines. Also, as you can see, Gardener copies the output of the infrastructure creation (.spec.infrastructureProviderStatus), see Infrastructure resource, into the .spec.\nIn the .spec.pools[] field the desired worker pools are listed. In the above example, one pool with machine type m4.large and min=3, max=5 machines shall be spread over two availability zones (eu-west-1b, eu-west-1c). This information together with the infrastructure status must be used to determine the proper configuration for the machine classes.\nThe spec.pools[].machineControllerManager field allows to configure the settings for machine-controller-manager component. Providers must populate these settings on worker-pool to the related fields in MachineDeployment.\nWhen seeing such a resource your controller must make sure that it deploys the machine-controller-manager next to the control plane in the seed cluster. After that, it must compute the desired machine classes and the desired machine deployments. Typically, one class maps to one deployment, and one class/deployment is created per availability zone. Following this convention, the created resource would look like this:\napiVersion:v1kind:Secretmetadata:name:shoot--foo--bar-cpu-worker-z1-3db65namespace:shoot--foo--barlabels:gardener.cloud/purpose:machineclasstype:Opaquedata:providerAccessKeyId:eW91ci1hd3MtYWNjZXNzLWtleS1pZAo=providerSecretAccessKey:eW91ci1hd3Mtc2VjcmV0LWFjY2Vzcy1rZXkKuserData:c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0K---apiVersion:machine.sapcloud.io/v1alpha1kind:AWSMachineClassmetadata:name:shoot--foo--bar-cpu-worker-z1-3db65namespace:shoot--foo--barspec:ami:ami-0123456789# Your controller must map the stated version to the provider specific machine image information, in the AWS case the AMI.blockDevices:- ebs:volumeSize:20volumeType:gp2iam:name:shoot--foo--bar-nodeskeyName:shoot--foo--bar-ssh-publickeymachineType:m4.largenetworkInterfaces:- securityGroupIDs:- sg-1234567890subnetID:subnet-01234region:eu-west-1secretRef:name:shoot--foo--bar-cpu-worker-z1-3db65namespace:shoot--foo--bartags:kubernetes.io/cluster/shoot--foo--bar:\u0026#34;1\u0026#34;kubernetes.io/role/node:\u0026#34;1\u0026#34;---apiVersion:machine.sapcloud.io/v1alpha1kind:MachineDeploymentmetadata:name:shoot--foo--bar-cpu-worker-z1namespace:shoot--foo--barspec:replicas:2selector:matchLabels:name:shoot--foo--bar-cpu-worker-z1strategy:type:RollingUpdaterollingUpdate:maxSurge:1maxUnavailable:0template:metadata:labels:name:shoot--foo--bar-cpu-worker-z1spec:class:kind:AWSMachineClassname:shoot--foo--bar-cpu-worker-z1-3db65for the first availability zone eu-west-1b, and\napiVersion:v1kind:Secretmetadata:name:shoot--foo--bar-cpu-worker-z2-5z6asnamespace:shoot--foo--barlabels:gardener.cloud/purpose:machineclasstype:Opaquedata:providerAccessKeyId:eW91ci1hd3MtYWNjZXNzLWtleS1pZAo=providerSecretAccessKey:eW91ci1hd3Mtc2VjcmV0LWFjY2Vzcy1rZXkKuserData:c29tZSBkYXRhIHRvIGJvb3RzdHJhcCB0aGUgVk0K---apiVersion:machine.sapcloud.io/v1alpha1kind:AWSMachineClassmetadata:name:shoot--foo--bar-cpu-worker-z2-5z6asnamespace:shoot--foo--barspec:ami:ami-0123456789# Your controller must map the stated version to the provider specific machine image information, in the AWS case the AMI.blockDevices:- ebs:volumeSize:20volumeType:gp2iam:name:shoot--foo--bar-nodeskeyName:shoot--foo--bar-ssh-publickeymachineType:m4.largenetworkInterfaces:- securityGroupIDs:- sg-1234567890subnetID:subnet-0123aregion:eu-west-1secretRef:name:shoot--foo--bar-cpu-worker-z2-5z6asnamespace:shoot--foo--bartags:kubernetes.io/cluster/shoot--foo--bar:\u0026#34;1\u0026#34;kubernetes.io/role/node:\u0026#34;1\u0026#34;---apiVersion:machine.sapcloud.io/v1alpha1kind:MachineDeploymentmetadata:name:shoot--foo--bar-cpu-worker-z1namespace:shoot--foo--barspec:replicas:1selector:matchLabels:name:shoot--foo--bar-cpu-worker-z1strategy:type:RollingUpdaterollingUpdate:maxSurge:1maxUnavailable:0template:metadata:labels:name:shoot--foo--bar-cpu-worker-z1spec:class:kind:AWSMachineClassname:shoot--foo--bar-cpu-worker-z2-5z6asfor the second availability zone eu-west-1c.\nAnother convention is the 5-letter hash at the end of the machine class names. Most controllers compute a checksum out of the specification of the machine class. This helps to trigger a rolling update of the worker nodes if, for example, the machine image version changes. In this case, a new checksum will be generated which results in the creation of a new machine class. The MachineDeployment's machine class reference (.spec.template.spec.class.name) is updated which triggers the rolling update process in the machine-controller-manager. However, all of this is only a convention that eases writing the controller, but you can do it completely differently if you desire - as long as you make sure that the described behaviours are implemented correctly.\nAfter the machine classes and machine deployments have been created the machine-controller-manager will start talking to the provider\u0026rsquo;s IaaS API and create the virtual machines. Gardener makes sure that the content of the userData field that is used to bootstrap the machines contain the required configuration for installation of the kubelet and registering the VM as worker node in the shoot cluster. The Worker extension controller shall wait until all the created MachineDeployments indicate healthiness/readiness before it ends the control loop.\nDoes Gardener need some information that must be returned back? Another important benefit of the machine-controller-manager\u0026rsquo;s design principles (extending the Kubernetes API using CRDs) is that the cluster-autoscaler can be used without any provider-specific implementation. We have forked the upstream Kubernetes community\u0026rsquo;s cluster-autoscaler and extended it so that it understands the machine API. Definitely, we will merge it back into the community\u0026rsquo;s versions once it has been adapted properly.\nOur cluster-autoscaler only needs to know the minimum and maximum number of replicas per MachineDeployment and is ready to act without that it needs to talk to the provider APIs (it just modifies the .spec.replicas field in the MachineDeployment object). Gardener deploys this autoscaler if there is at least one worker pool that specifies max\u0026gt;min. In order to know how it needs to configure it, the provider-specific Worker extension controller must expose which MachineDeployments it had created and how the min/max numbers should look like.\nConsequently, your controller should write this information into the Worker resource\u0026rsquo;s .status.machineDeployments field:\n---apiVersion:extensions.gardener.cloud/v1alpha1kind:Workermetadata:name:workernamespace:shoot--foo--barspec:...status:lastOperation:...machineDeployments:- name:shoot--foo--bar-cpu-worker-z1minimum:2maximum:3- name:shoot--foo--bar-cpu-worker-z2minimum:1maximum:2In order to support a new worker provider you need to write a controller that watches all Workers with .spec.type=\u0026lt;my-provider-name\u0026gt;. You can take a look at the below referenced example implementation for the AWS provider.\nThat sounds like a lot that needs to be done, can you help me? All of the described behaviour is mostly the same for every provider. The only difference is maybe the version/configuration of the machine-controller-manager, and the machine class specification itself. You can take a look at our extension library, especially the worker controller part where you will find a lot of utilities that you can use. Also, using the library you only need to implement your provider specifics - all the things that can be handled generically can be taken for free and do not need to be re-implemented. Take a look at the AWS worker controller for finding an example.\nNon-provider specific information required for worker creation All the providers require further information that is not provider specific but already part of the shoot resource. One example for such information is whether the shoot is hibernated or not. In this case all the virtual machines should be deleted/terminated, and after that the machine controller-manager should be scaled down. You can take a look at the AWS worker controller to see how it reads this information and how it is used. As Gardener cannot know which information is required by providers it simply mirrors the Shoot, Seed, and CloudProfile resources into the seed. They are part of the Cluster extension resource and can be used to extract information that is not part of the Worker resource itself.\nReferences and additional resources  Worker API (Golang specification) Extension controller library Generic worker controller Exemplary implementation for the AWS provider  "}]