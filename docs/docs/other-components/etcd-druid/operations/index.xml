<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener – Operations</title><link>https://gardener.cloud/docs/other-components/etcd-druid/operations/</link><description>Recent content in Operations on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://gardener.cloud/docs/other-components/etcd-druid/operations/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Compaction Metrics</title><link>https://gardener.cloud/docs/other-components/etcd-druid/operations/compaction-metrics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/etcd-druid/operations/compaction-metrics/</guid><description>
&lt;h1 id="monitoring">Monitoring&lt;/h1>
&lt;p>etcd-druid uses &lt;a href="http://prometheus.io/">Prometheus&lt;/a> for metrics reporting. The metrics can be used for real-time monitoring and debugging of compaction jobs.&lt;/p>
&lt;p>The simplest way to see the available metrics is to cURL the metrics endpoint &lt;code>/metrics&lt;/code>. The format is described &lt;a href="http://prometheus.io/docs/instrumenting/exposition_formats/">here&lt;/a>.&lt;/p>
&lt;p>Follow the &lt;a href="http://prometheus.io/docs/introduction/getting_started/">Prometheus getting started doc&lt;/a> to spin up a Prometheus server to collect etcd metrics.&lt;/p>
&lt;p>The naming of metrics follows the suggested &lt;a href="http://prometheus.io/docs/practices/naming/">Prometheus best practices&lt;/a>. All compaction related metrics are put under namespace &lt;code>etcddruid&lt;/code> and subsystem &lt;code>compaction&lt;/code>.&lt;/p>
&lt;h3 id="compaction">Compaction&lt;/h3>
&lt;p>These metrics give an idea about the compaction jobs that run after some interval in shoot control planes. Studying the metrices, we can deduce how many compaction job ran successfully, how many failed, how many delta events compacted etc.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Description&lt;/th>
&lt;th>Type&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>etcddruid_compaction_jobs_total&lt;/td>
&lt;td>Total number of compaction jobs initiated by compaction controller.&lt;/td>
&lt;td>Counter&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>etcddruid_compaction_jobs_current&lt;/td>
&lt;td>Number of currently running compaction job.&lt;/td>
&lt;td>Gauge&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>etcddruid_compaction_job_duration_seconds&lt;/td>
&lt;td>Total time taken in seconds to finish a running compaction job.&lt;/td>
&lt;td>Histogram&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>etcddruid_compaction_num_delta_events&lt;/td>
&lt;td>Total number of etcd events to be compacted by a compaction job.&lt;/td>
&lt;td>Gauge&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>There are two labels for &lt;code>etcddruid_compaction_jobs_total&lt;/code> metrics. The label &lt;code>succeeded&lt;/code> shows how many of the compaction jobs are succeeded and label &lt;code>failed&lt;/code> shows how many of compaction jobs are failed.&lt;/p>
&lt;p>There are two labels for &lt;code>etcddruid_compaction_job_duration_seconds&lt;/code> metrics. The label &lt;code>succeeded&lt;/code> shows how much time taken by a successful job to complete and label &lt;code>failed&lt;/code> shows how much time taken by a failed compaction job.&lt;/p>
&lt;p>&lt;code>etcddruid_compaction_jobs_current&lt;/code> metric comes with label &lt;code>etcd_namespace&lt;/code> that indicates the namespace of the ETCD running in the control plane of a shoot cluster..&lt;/p>
&lt;h2 id="prometheus-supplied-metrics">Prometheus supplied metrics&lt;/h2>
&lt;p>The Prometheus client library provides a number of metrics under the &lt;code>go&lt;/code> and &lt;code>process&lt;/code> namespaces.&lt;/p></description></item><item><title>Docs: Multinode Metrics</title><link>https://gardener.cloud/docs/other-components/etcd-druid/operations/multinode-metrics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/etcd-druid/operations/multinode-metrics/</guid><description>
&lt;h2 id="metrics-multi-node-etcd">Metrics: Multi-node etcd&lt;/h2>
&lt;h4 id="etcd">Etcd&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>No.&lt;/th>
&lt;th>Metrics Name&lt;/th>
&lt;th>Description&lt;/th>
&lt;th>Comments&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>etcd_disk_wal_fsync_duration_seconds&lt;/td>
&lt;td>latency distributions of fsync called by WAL.&lt;/td>
&lt;td>High disk operation latencies indicate disk issues.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>etcd_disk_backend_commit_duration_seconds&lt;/td>
&lt;td>latency distributions of commit called by backend.&lt;/td>
&lt;td>High disk operation latencies indicate disk issues.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>etcd_server_has_leader&lt;/td>
&lt;td>whether or not a leader exists. 1: leader exists, 0: leader not exists.&lt;/td>
&lt;td>To capture quorum loss or to check the availability of etcd cluster.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>etcd_server_is_leader&lt;/td>
&lt;td>whether or not this member is a leader. 1 if it is, 0 otherwise.&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>5&lt;/td>
&lt;td>etcd_server_leader_changes_seen_total&lt;/td>
&lt;td>number of leader changes seen.&lt;/td>
&lt;td>Helpful in fine tuning the zonal cluster like etcd-heartbeat time etc, it can also indicates the etcd load and network issues.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>etcd_server_is_learner&lt;/td>
&lt;td>whether or not this member is a learner. 1 if it is, 0 otherwise.&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>7&lt;/td>
&lt;td>etcd_server_learner_promote_successes&lt;/td>
&lt;td>total number of successful learner promotions while this member is leader.&lt;/td>
&lt;td>Might be helpful in checking the success of API calls called by backup-restore.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8&lt;/td>
&lt;td>etcd_network_client_grpc_received_bytes_total&lt;/td>
&lt;td>total number of bytes received from grpc clients.&lt;/td>
&lt;td>Client Traffic In.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>9&lt;/td>
&lt;td>etcd_network_client_grpc_sent_bytes_total&lt;/td>
&lt;td>total number of bytes sent to grpc clients.&lt;/td>
&lt;td>Client Traffic Out.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>etcd_network_peer_sent_bytes_total&lt;/td>
&lt;td>total number of bytes sent to peers.&lt;/td>
&lt;td>Useful for network usage.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>11&lt;/td>
&lt;td>etcd_network_peer_received_bytes_total&lt;/td>
&lt;td>total number of bytes received from peers.&lt;/td>
&lt;td>Useful for network usage.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12&lt;/td>
&lt;td>etcd_network_active_peers&lt;/td>
&lt;td>current number of active peer connections.&lt;/td>
&lt;td>Might be useful in detecting issues like network partition.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>13&lt;/td>
&lt;td>etcd_server_proposals_committed_total&lt;/td>
&lt;td>total number of consensus proposals committed.&lt;/td>
&lt;td>A consistently large lag between a single member and its leader indicates that member is slow or unhealthy.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>14&lt;/td>
&lt;td>etcd_server_proposals_pending&lt;/td>
&lt;td>current number of pending proposals to commit.&lt;/td>
&lt;td>Pending proposals suggests there is a high client load or the member cannot commit proposals.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>15&lt;/td>
&lt;td>etcd_server_proposals_failed_total&lt;/td>
&lt;td>total number of failed proposals seen.&lt;/td>
&lt;td>Might indicates downtime caused by a loss of quorum.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>16&lt;/td>
&lt;td>etcd_server_proposals_applied_total&lt;/td>
&lt;td>total number of consensus proposals applied.&lt;/td>
&lt;td>Difference between etcd_server_proposals_committed_total and etcd_server_proposals_applied_total should usually be small.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>17&lt;/td>
&lt;td>etcd_mvcc_db_total_size_in_bytes&lt;/td>
&lt;td>total size of the underlying database physically allocated in bytes.&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>18&lt;/td>
&lt;td>etcd_server_heartbeat_send_failures_total&lt;/td>
&lt;td>total number of leader heartbeat send failures.&lt;/td>
&lt;td>Might be helpful in fine-tuning the cluster or detecting slow disk or any network issues.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>19&lt;/td>
&lt;td>etcd_network_peer_round_trip_time_seconds&lt;/td>
&lt;td>round-trip-time histogram between peers.&lt;/td>
&lt;td>Might be helpful in fine-tuning network usage specially for zonal etcd cluster.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>etcd_server_slow_apply_total&lt;/td>
&lt;td>total number of slow apply requests.&lt;/td>
&lt;td>Might indicate overloaded from slow disk.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>21&lt;/td>
&lt;td>etcd_server_slow_read_indexes_total&lt;/td>
&lt;td>total number of pending read indexes not in sync with leader&amp;rsquo;s or timed out read index requests.&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="backup-restore">Backup-restore&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>No.&lt;/th>
&lt;th>Metrics Name&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1.&lt;/td>
&lt;td>etcdbr_cluster_size&lt;/td>
&lt;td>to capture the scale-up/scale-down scenarios.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2.&lt;/td>
&lt;td>etcdbr_is_learner&lt;/td>
&lt;td>whether or not this member is a learner. 1 if it is, 0 otherwise.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3.&lt;/td>
&lt;td>etcdbr_is_learner_count_total&lt;/td>
&lt;td>total number times member added as the learner.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4.&lt;/td>
&lt;td>etcdbr_restoration_duration_seconds&lt;/td>
&lt;td>total latency distribution required to restore the etcd member.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>5.&lt;/td>
&lt;td>etcdbr_add_learner_duration_seconds&lt;/td>
&lt;td>total latency distribution of adding the etcd member as a learner to the cluster.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6.&lt;/td>
&lt;td>etcdbr_member_remove_duration_seconds&lt;/td>
&lt;td>total latency distribution removing the etcd member from the cluster.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>7.&lt;/td>
&lt;td>etcdbr_member_promote_duration_seconds&lt;/td>
&lt;td>total latency distribution of promoting the learner to the voting member.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8.&lt;/td>
&lt;td>etcdbr_defragmentation_duration_seconds&lt;/td>
&lt;td>total latency distribution of defragmentation of each etcd cluster member.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>Docs: Recover from etcd Permanent Quorum Loss</title><link>https://gardener.cloud/docs/other-components/etcd-druid/operations/recover_from_etcd_permanent_quorum_loss/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/etcd-druid/operations/recover_from_etcd_permanent_quorum_loss/</guid><description>
&lt;h2 id="quorum-loss-in-etcd-cluster">Quorum loss in ETCD Cluster&lt;/h2>
&lt;p>&lt;a href="https://etcd.io/docs/v3.4/op-guide/recovery/">Quorum loss&lt;/a> means when majority of ETCD pods(greater than or equal to n/2 + 1) are down simultaneously for some reason.&lt;/p>
&lt;p>There are two types of quorum loss that can happen to &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/multi-node/">ETCD multinode cluster&lt;/a> :&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Transient quorum loss&lt;/strong> - A quorum loss is called transient when majority of ETCD pods are down simultaneously for some time. The pods may be down due to network unavailability, high resource usages etc. When the pods come back after some time, they can re-join to the cluster and the quorum is recovered automatically without any manual intervention. There should not be a permanent failure for majority of etcd pods due to hardware failure or disk corruption.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Permanent quorum loss&lt;/strong> - A quorum loss is called permanent when majority of ETCD cluster members experience permanent failure, whether due to hardware failure or disk corruption etc. then the etcd cluster is not going to recover automatically from the quorum loss. A human operator will now need to intervene and execute the following steps to recover the multi-node ETCD cluster.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>If permanent quorum loss occurs to a multinode ETCD cluster, the operator needs to note down the PVCs, configmaps, statefulsets, CRs etc related to that ETCD cluster and work on those resources only. Following steps guide a human operator to recover from permanent quorum loss of a ETCD cluster. We assume the name of the ETCD CR for the ETCD cluster is &lt;code>etcd-main&lt;/code>.&lt;/p>
&lt;p>&lt;strong>ETCD cluster in shoot control plane of gardener deployment:&lt;/strong>
There are two &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/multi-node/">ETCD clusters&lt;/a> running in shoot control plane. One is named as &lt;code>etcd-events&lt;/code> and another is named &lt;code>etcd-main&lt;/code>. The operator needs to take care of permanent quorum loss to a specific cluster. If permanent quorum loss occurs to &lt;code>etcd-events&lt;/code> cluster, the operator needs to note down the PVCs, configmaps, statefulsets, CRs etc related to &lt;code>etcd-events&lt;/code> cluster and work on those resources only.&lt;/p>
&lt;p>⚠️ &lt;strong>Note:&lt;/strong> Please note that manually restoring etcd can result in data loss. This guide is the last resort to bring an ETCD cluster up and running again.&lt;/p>
&lt;p>If etcd-druid and etcd-backup-restore is being used with gardener, then&lt;/p>
&lt;p>Target the control plane of affected shoot cluster via &lt;code>kubectl&lt;/code>. Alternatively, you can use &lt;a href="https://github.com/gardener/gardenctl-v2">gardenctl&lt;/a> to target the control plane of the affected shoot cluster. You can get the details to target the control plane from the Access tile in the shoot cluster details page on the Gardener dashboard. Ensure that you are targeting the correct namespace.&lt;/p>
&lt;ol>
&lt;li>Add the following annotation to the &lt;code>Etcd&lt;/code> resource &lt;code>kubectl annotate etcd etcd-main druid.gardener.cloud/ignore-reconciliation=&amp;quot;true&amp;quot;&lt;/code>&lt;/li>
&lt;li>Note down the configmap name that is attached to the &lt;code>etcd-main&lt;/code> statefulset. If you describe the statefulset with &lt;code>kubectl describe sts etcd-main&lt;/code>, look for the lines similar to following lines to identify attached configmap name. It will be needed at later stages:&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code> Volumes:
etcd-config-file:
Type: ConfigMap (a volume populated by a ConfigMap)
Name: etcd-bootstrap-4785b0
Optional: false
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code> Alternatively, the related configmap name can be obtained by executing following command as well:
&lt;/code>&lt;/pre>&lt;p>&lt;code>kubectl get sts etcd-main -o jsonpath='{.spec.template.spec.volumes[?(@.name==&amp;quot;etcd-config-file&amp;quot;)].configMap.name}'&lt;/code>&lt;/p>
&lt;ol start="3">
&lt;li>
&lt;p>Scale down the &lt;code>etcd-main&lt;/code> statefulset replicas to &lt;code>0&lt;/code>&lt;/p>
&lt;p>&lt;code>kubectl scale sts etcd-main --replicas=0&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The PVCs will look like the following on listing them with the command &lt;code>kubectl get pvc&lt;/code> :&lt;/p>
&lt;pre tabindex="0">&lt;code>main-etcd-etcd-main-0 Bound pv-shoot--garden--aws-ha-dcb51848-49fa-4501-b2f2-f8d8f1fad111 80Gi RWO gardener.cloud-fast 13d
main-etcd-etcd-main-1 Bound pv-shoot--garden--aws-ha-b4751b28-c06e-41b7-b08c-6486e03090dd 80Gi RWO gardener.cloud-fast 13d
main-etcd-etcd-main-2 Bound pv-shoot--garden--aws-ha-ff17323b-d62e-4d5e-a742-9de823621490 80Gi RWO gardener.cloud-fast 13d
&lt;/code>&lt;/pre>&lt;p>Delete all PVCs that are attached to &lt;code>etcd-main&lt;/code> cluster.&lt;/p>
&lt;p>&lt;code>kubectl delete pvc -l instance=etcd-main&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Edit the &lt;code>etcd-main&lt;/code> cluster&amp;rsquo;s configmap (ex: &lt;code>etcd-bootstrap-4785b0&lt;/code>) as follows:&lt;/p>
&lt;p>Find the &lt;code>initial-cluster&lt;/code> field in the configmap. It will look like the following:&lt;/p>
&lt;pre tabindex="0">&lt;code># Initial cluster
initial-cluster: etcd-main-0=https://etcd-main-0.etcd-main-peer.default.svc:2380,etcd-main-1=https://etcd-main-1.etcd-main-peer.default.svc:2380,etcd-main-2=https://etcd-main-2.etcd-main-peer.default.svc:2380
&lt;/code>&lt;/pre>&lt;p>Change the &lt;code>initial-cluster&lt;/code> field to have only one member (&lt;code>etcd-main-0&lt;/code>) in the string. It should now look like this:&lt;/p>
&lt;pre tabindex="0">&lt;code># Initial cluster
initial-cluster: etcd-main-0=https://etcd-main-0.etcd-main-peer.default.svc:2380
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Scale up the &lt;code>etcd-main&lt;/code> statefulset replicas to &lt;code>1&lt;/code>&lt;/p>
&lt;p>&lt;code>kubectl scale sts etcd-main --replicas=1&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Wait for the single-member etcd cluster to be completely ready.&lt;/p>
&lt;p>&lt;code>kubectl get pods etcd-main-0&lt;/code> will give the following output when ready:&lt;/p>
&lt;pre tabindex="0">&lt;code>NAME READY STATUS RESTARTS AGE
etcd-main-0 2/2 Running 0 1m
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Remove the following annotation from the &lt;code>Etcd&lt;/code> resource &lt;code>etcd-main&lt;/code>: &lt;code>kubectl annotate etcd etcd-main druid.gardener.cloud/ignore-reconciliation-&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Finally add the following annotation to the &lt;code>Etcd&lt;/code> resource &lt;code>etcd-main&lt;/code>: &lt;code>kubectl annotate etcd etcd-main gardener.cloud/operation=&amp;quot;reconcile&amp;quot;&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Verify that the etcd cluster is formed correctly.&lt;/p>
&lt;p>All the &lt;code>etcd-main&lt;/code> pods will have outputs similar to following:&lt;/p>
&lt;pre tabindex="0">&lt;code>NAME READY STATUS RESTARTS AGE
etcd-main-0 2/2 Running 0 5m
etcd-main-1 2/2 Running 0 1m
etcd-main-2 2/2 Running 0 1m
&lt;/code>&lt;/pre>&lt;p>Additionally, check if the ETCD CR is ready with &lt;code>kubectl get etcd etcd-main&lt;/code> :&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-✹" data-lang="✹">NAME READY AGE
etcd-main true 13d
&lt;/code>&lt;/pre>&lt;p>Additionally, check the leases for 30 seconds at least. There should be leases starting with &lt;code>etcd-main&lt;/code> as many as &lt;code>etcd-main&lt;/code> replicas. One of those leases will have holder identity as &lt;code>&amp;lt;etcd-member-id&amp;gt;:Leader&lt;/code> and rest of those leases have holder identities as &lt;code>&amp;lt;etcd-member-id&amp;gt;:Member&lt;/code>. The &lt;code>AGE&lt;/code> of those leases can also be inspected to identify if those leases were updated in conjunction with the restart of the ETCD cluster: Example:&lt;/p>
&lt;pre tabindex="0">&lt;code>NAME HOLDER AGE
etcd-main-0 4c37667312a3912b:Member 1m
etcd-main-1 75a9b74cfd3077cc:Member 1m
etcd-main-2 c62ee6af755e890d:Leader 1m
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol></description></item><item><title>Docs: Scale Up</title><link>https://gardener.cloud/docs/other-components/etcd-druid/operations/scale-up/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/etcd-druid/operations/scale-up/</guid><description>
&lt;h2 id="scaling-up-a-single-node-to-multi-node-etcd-cluster-deployed-by-etcd-druid">Scaling-up a single-node to multi-node etcd cluster deployed by etcd-druid&lt;/h2>
&lt;p>To mark a cluster for scale-up from single node to multi-node etcd, just patch the etcd custom resource&amp;rsquo;s &lt;code>.spec.replicas&lt;/code> from &lt;code>1&lt;/code> to &lt;code>3&lt;/code> (for example).&lt;/p>
&lt;h3 id="challenges-for-scale-up">Challenges for scale-up&lt;/h3>
&lt;ol>
&lt;li>Etcd cluster with single replica don&amp;rsquo;t have any peers, so no peer communication is required hence peer URL may or may not be TLS enabled. However, while scaling up from single node etcd to multi-node etcd, there will be a requirement to have peer communication between members of the etcd cluster. Peer communication is required for various reasons, for instance for members to sync up cluster state, data, and to perform leader election or any cluster wide operation like removal or addition of a member etc. Hence in a multi-node etcd cluster we need to have TLS enable peer URL for peer communication.&lt;/li>
&lt;li>Providing the correct configuration to start new etcd members as it is different from boostrapping a cluster since these new etcd members will join an existing cluster.&lt;/li>
&lt;/ol>
&lt;h3 id="approach">Approach&lt;/h3>
&lt;p>We first went through the etcd doc of &lt;a href="https://etcd.io/docs/v3.4/op-guide/runtime-configuration/#update-advertise-peer-urls">update-advertise-peer-urls&lt;/a> to find out information regarding peer URL updation. Interestingly, etcd doc has mentioned the following:&lt;/p>
&lt;pre tabindex="0">&lt;code>To update the advertise peer URLs of a member, first update it explicitly via member command and then restart the member.
&lt;/code>&lt;/pre>&lt;p>But we can&amp;rsquo;t assume peer URL is not TLS enabled for single-node cluster as it depends on end-user. A user may or may not enable the TLS for peer URL for a single node etcd cluster. So, How do we detect whether peer URL was enabled or not when cluster is marked for scale-up?&lt;/p>
&lt;h3 id="detecting-if-peerurl-tls-is-enabled-or-not">Detecting if peerURL TLS is enabled or not&lt;/h3>
&lt;p>For this we use an annotation in member lease object &lt;code>member.etcd.gardener.cloud/tls-enabled&lt;/code> set by backup-restore sidecar of etcd. As etcd configuration is provided by backup-restore, so it can find out whether TLS is enabled or not and accordingly set this annotation &lt;code>member.etcd.gardener.cloud/tls-enabled&lt;/code> to either &lt;code>true&lt;/code> or &lt;code>false&lt;/code> in member lease object.
And with the help of this annotation and config-map values etcd-druid is able to detect whether there is a change in a peer URL or not.&lt;/p>
&lt;h3 id="etcd-druid-helps-in-scaling-up-etcd-cluster">Etcd-Druid helps in scaling up etcd cluster&lt;/h3>
&lt;p>Now, it is detected whether peer URL was TLS enabled or not for single node etcd cluster. Etcd-druid can now use this information to take action:&lt;/p>
&lt;ul>
&lt;li>If peer URL was already TLS enabled then no action is required from etcd-druid side. Etcd-druid can proceed with scaling up the cluster.&lt;/li>
&lt;li>If peer URL was not TLS enabled then etcd-druid has to intervene and make sure peer URL should be TLS enabled first for the single node before marking the cluster for scale-up.&lt;/li>
&lt;/ul>
&lt;h3 id="action-taken-by-etcd-druid-to-enable-the-peerurl-tls">Action taken by etcd-druid to enable the peerURL TLS&lt;/h3>
&lt;ol>
&lt;li>Etcd-druid will update the &lt;code>etcd-bootstrap&lt;/code> config-map with new config like initial-cluster,initial-advertise-peer-urls etc. Backup-restore will detect this change and update the member lease annotation to &lt;code>member.etcd.gardener.cloud/tls-enabled: &amp;quot;true&amp;quot;&lt;/code>.&lt;/li>
&lt;li>In case the peer URL TLS has been changed to &lt;code>enabled&lt;/code>: Etcd-druid will add tasks to the deployment flow.
&lt;ul>
&lt;li>To ensure that the TLS enablement of peer URL is properly reflected in etcd, the existing etcd StatefulSet pods should be restarted twice.&lt;/li>
&lt;li>The first restart pushes a new configuration which contains Peer URL TLS configuration. Backup-restore will update the member peer url. This will result in the change of the peer url in the etcd&amp;rsquo;s database, but it may not reflect in the already running etcd container. Ideally a restart of an etcd container would have been sufficient but currently k8s doesn&amp;rsquo;t expose an API to force restart a single container within a pod. Therefore, we need to restart the StatefulSet pod(s) once again. When the pod(s) is restarted the second time it will now start etcd with the correct peer url which will be TLS enabled.&lt;/li>
&lt;li>To achieve 2 restarts following is done:
&lt;ul>
&lt;li>An update is made to the spec mounting the peer URL TLS secrets. This will cause a rolling update of the existing pod.&lt;/li>
&lt;li>Once the update is successfully completed, then we delete StatefulSet pods, causing a restart by the StatefulSet controller.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h3 id="after-peerurl-is-tls-enabled">After PeerURL is TLS enabled&lt;/h3>
&lt;p>After peer URL TLS enablement for single node etcd cluster, now etcd-druid adds a scale-up annotation: &lt;code>gardener.cloud/scaled-to-multi-node&lt;/code> to the etcd statefulset and etcd-druid will patch the statefulsets &lt;code>.spec.replicas&lt;/code> to &lt;code>3&lt;/code>(for example). The statefulset controller will then bring up new pods(etcd with backup-restore as a sidecar). Now etcd&amp;rsquo;s sidecar i.e backup-restore will check whether this member is already a part of a cluster or not and incase it is unable to check (may be due to some network issues) then backup-restore checks presence of this annotation: &lt;code>gardener.cloud/scaled-to-multi-node&lt;/code> in etcd statefulset to detect scale-up. If it finds out it is the scale-up case then backup-restore adds new etcd member as a &lt;a href="https://etcd.io/docs/v3.3/learning/learner/">learner&lt;/a> first and then starts the etcd learner by providing the correct configuration. Once learner gets in sync with the etcd cluster leader, it will get promoted to a voting member.&lt;/p>
&lt;h3 id="providing-the-correct-etcd-config">Providing the correct etcd config&lt;/h3>
&lt;p>As backup-restore detects that it&amp;rsquo;s a scale-up scenario, backup-restore sets &lt;code>initial-cluster-state&lt;/code> to &lt;code>existing&lt;/code> as this member will join an existing cluster and it calculates the rest of the config from the updated config-map provided by etcd-druid.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/scale-up-sequenceDiagram_046066.png" alt="Sequence diagram">&lt;/p>
&lt;h4 id="future-improvements">Future improvements:&lt;/h4>
&lt;p>The need of restarting etcd pods twice will change in the future. please refer: &lt;a href="https://github.com/gardener/etcd-backup-restore/issues/538">https://github.com/gardener/etcd-backup-restore/issues/538&lt;/a>&lt;/p></description></item><item><title>Docs: Single Member Restoration</title><link>https://gardener.cloud/docs/other-components/etcd-druid/operations/single_member_restoration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/etcd-druid/operations/single_member_restoration/</guid><description>
&lt;h1 id="restoration-of-a-single-member-in-multi-node-etcd-deployed-by-etcd-druid">Restoration of a single member in multi-node etcd deployed by etcd-druid.&lt;/h1>
&lt;p>&lt;strong>Note&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>For a cluster with n members, we are proposing the solution to only single member restoration within a etcd cluster not the quorum loss scenario (when majority of members within a cluster fail).&lt;/li>
&lt;li>In this proposal we are not targetting the recovery of single member which got separated from cluster due to &lt;a href="https://etcd.io/docs/v3.3/op-guide/failures/#network-partition">network partition&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>If a single etcd member within a multi-node etcd cluster goes down due to DB corruption/PVC corruption/Invalid data-dir then it needs to be brought back. Unlike in the single-node case, a minority member of a multi-node cluster can&amp;rsquo;t be restored from the snapshots present in storage container as you can&amp;rsquo;t restore from the old snapshots as it contains the metadata information of cluster which leads to &lt;strong>memberID mismatch&lt;/strong> that prevents the new member from coming up as new member is getting its metadata information from db which got restore from old snapshots.&lt;/p>
&lt;h2 id="solution">Solution&lt;/h2>
&lt;ul>
&lt;li>If a corresponding backup-restore sidecar detects that its corresponding etcd is down due to &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/7d27a47f5793b0949492d225ada5fd8344b6b6a2/pkg/initializer/validator/datavalidator.go#L177">data-dir corruption&lt;/a> or &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/7d27a47f5793b0949492d225ada5fd8344b6b6a2/pkg/initializer/validator/datavalidator.go#L204">Invalid data-dir&lt;/a>&lt;/li>
&lt;li>Then backup-restore will first remove the failing etcd member from the cluster using the &lt;a href="https://github.com/etcd-io/etcd/blob/ae9734ed278b7a1a7dfc82e800471ebbf9fce56f/clientv3/cluster.go#L45-L46">MemberRemove API&lt;/a> call and clean the data-dir of failed etcd member.&lt;/li>
&lt;li>It won&amp;rsquo;t affect the etcd cluster as quorum is still maintained.&lt;/li>
&lt;li>After successfully removing failed etcd member from the cluster, backup-restore sidecar will try to add a new etcd member to a cluster to get the same cluster size as before.&lt;/li>
&lt;li>Backup-restore firstly adds new member as a &lt;a href="https://etcd.io/docs/v3.3/learning/learner/">Learner&lt;/a> using the &lt;a href="https://github.com/etcd-io/etcd/blob/ae9734ed278b7a1a7dfc82e800471ebbf9fce56f/clientv3/cluster.go#L42-L43">MemberAddAsLearner API&lt;/a> call, once learner is added to the cluster and it&amp;rsquo;s get in sync with leader and becomes up-to-date then promote the learner(non-voting member) to a voting member using &lt;a href="https://github.com/etcd-io/etcd/blob/ae9734ed278b7a1a7dfc82e800471ebbf9fce56f/clientv3/cluster.go#L51-L52">MemberPromote API&lt;/a> call.&lt;/li>
&lt;li>So, the failed member first needs to be removed from the cluster and then added as a new member.&lt;/li>
&lt;/ul>
&lt;h3 id="example">Example:&lt;/h3>
&lt;ol>
&lt;li>If a &lt;code>3&lt;/code> member etcd cluster has 1 downed member(due to invalid data-dir), the cluster can still make forward progress because the quorum is &lt;code>2&lt;/code>.&lt;/li>
&lt;li>Etcd downed member get restarted and it&amp;rsquo;s corresponding backup-restore sidecar receives an &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/master/doc/proposals/design.md#workflow">initialization&lt;/a> request.&lt;/li>
&lt;li>Then, backup-restore sidecar checks for data corruption/invalid data-dir.&lt;/li>
&lt;li>Backup-restore sidecar detects that data-dir is invalid and its a multi-node etcd cluster.&lt;/li>
&lt;li>Then, backup-restore sidecar removed the downed etcd member from cluster.&lt;/li>
&lt;li>The number of members in a cluster becomes &lt;code>2&lt;/code> and the quorum remains at &lt;code>2&lt;/code>, so it won&amp;rsquo;t affect the etcd cluster.&lt;/li>
&lt;li>Clean the data-dir and add a member as a learner(non-voting member).&lt;/li>
&lt;li>As soon as learner gets in sync with leader, promote the learner to a voting member, hence increasing number of members in a cluster back to &lt;code>3&lt;/code>.&lt;/li>
&lt;/ol></description></item></channel></rss>