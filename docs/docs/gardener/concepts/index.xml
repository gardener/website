<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener – Concepts</title><link>https://gardener.cloud/docs/gardener/concepts/</link><description>Recent content in Concepts on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://gardener.cloud/docs/gardener/concepts/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Admission Controller</title><link>https://gardener.cloud/docs/gardener/concepts/admission-controller/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/admission-controller/</guid><description>
&lt;h1 id="gardener-admission-controller">Gardener Admission Controller&lt;/h1>
&lt;p>While the Gardener API server works with &lt;a href="https://gardener.cloud/docs/gardener/concepts/apiserver_admission_plugins/">admission plugins&lt;/a> to validate and mutate resources belonging to Gardener related API groups, e.g. &lt;code>core.gardener.cloud&lt;/code>, the same is needed for resources belonging to non-Gardener API groups as well, e.g. &lt;code>Secret&lt;/code>s in the &lt;code>core&lt;/code> API group.
Therefore, the Gardener Admission Controller runs a http(s) server with the following handlers which serve as validating/mutating endpoints for &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">admission webhooks&lt;/a>.
It is also used to serve http(s) handlers for authorization webhooks.&lt;/p>
&lt;h2 id="admission-webhook-handlers">Admission Webhook Handlers&lt;/h2>
&lt;p>This section describes the admission webhook handlers that are currently served.&lt;/p>
&lt;h3 id="kubeconfig-secret-validator">Kubeconfig Secret Validator&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/kubectl/issues/697">Malicious Kubeconfigs&lt;/a> applied by end users may cause a leakage of sensitive data.
This handler checks if the incoming request contains a Kubernetes secret with a &lt;code>.data.kubeconfig&lt;/code> field and denies the request if the Kubeconfig structure violates Gardener&amp;rsquo;s security standards.&lt;/p>
&lt;h3 id="namespace-validator">Namespace Validator&lt;/h3>
&lt;p>Namespaces are the backing entities of Gardener projects in which shoot clusters objects reside.
This validation handler protects active namespaces against premature deletion requests.
Therefore, it denies deletion requests if a namespace still contains shoot clusters or if it belongs to a non-deleting Gardener project (w/o &lt;code>.metadata.deletionTimestamp&lt;/code>).&lt;/p>
&lt;h3 id="resource-size-validator">Resource Size Validator&lt;/h3>
&lt;p>Since users directly apply Kubernetes native objects to the Garden cluster, it also involves the risk of being vulnerable to DoS attacks because these resources are read continuously watched and read by controllers.
One example is the creation of &lt;code>Shoot&lt;/code> resources with large annotation values (up to 256 kB per value) which can cause severe out-of-memory issues for the Gardenlet component.
&lt;a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">Vertical autoscaling&lt;/a> can help to mitigate such situations, but we cannot expect to scale infinitely, and thus need means to block the attack itself.&lt;/p>
&lt;p>The Resource Size Validator checks arbitrary incoming admission requests against a configured maximum size for the resource&amp;rsquo;s group-version-kind combination and denies the request if the contained object exceeds the quota.&lt;/p>
&lt;p>Example for Gardener Admission Controller configuration:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">server:
resourceAdmissionConfiguration:
limits:
- apiGroups: [&lt;span style="color:#a31515">&amp;#34;core.gardener.cloud&amp;#34;&lt;/span>]
apiVersions: [&lt;span style="color:#a31515">&amp;#34;*&amp;#34;&lt;/span>]
resources: [&lt;span style="color:#a31515">&amp;#34;shoots&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;plants&amp;#34;&lt;/span>]
size: 100k
- apiGroups: [&lt;span style="color:#a31515">&amp;#34;&amp;#34;&lt;/span>]
apiVersions: [&lt;span style="color:#a31515">&amp;#34;v1&amp;#34;&lt;/span>]
resources: [&lt;span style="color:#a31515">&amp;#34;secrets&amp;#34;&lt;/span>]
size: 100k
unrestrictedSubjects:
- kind: Group
name: gardener.cloud:system:seeds
apiGroup: rbac.authorization.k8s.io
&lt;span style="color:#008000"># - kind: User&lt;/span>
&lt;span style="color:#008000"># name: admin&lt;/span>
&lt;span style="color:#008000"># apiGroup: rbac.authorization.k8s.io&lt;/span>
&lt;span style="color:#008000"># - kind: ServiceAccount&lt;/span>
&lt;span style="color:#008000"># name: &amp;#34;*&amp;#34;&lt;/span>
&lt;span style="color:#008000"># namespace: garden&lt;/span>
&lt;span style="color:#008000"># apiGroup: &amp;#34;&amp;#34;&lt;/span>
operationMode: block &lt;span style="color:#008000">#log&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>With the configuration above, the Resource Size Validator denies requests for shoots and plants with Gardener&amp;rsquo;s core API group which exceed a size of 100 kB. The same is done for Kubernetes secrets.&lt;/p>
&lt;p>As this feature is meant to protect the system from malicious requests sent by users, it is recommended to exclude trusted groups, users or service accounts from the size restriction via &lt;code>resourceAdmissionConfiguration.unrestrictedSubjects&lt;/code>.
For example, the backing user for the Gardenlet should always be capable of changing the shoot resource instead of being blocked due to size restrictions.
This is because the Gardenlet itself occasionally changes the shoot specification, labels or annotations, and might violate the quota if the existing resource is already close to the quota boundary.
Also, operators are supposed to be trusted users and subjecting them to a size limitation can inhibit important operational tasks.
Wildcard (&amp;quot;*&amp;quot;) in subject &lt;code>name&lt;/code> is supported.&lt;/p>
&lt;p>Size limitations depend on the individual Gardener setup and choosing the wrong values can affect the availability of your Gardener service.
&lt;code>resourceAdmissionConfiguration.operationMode&lt;/code> allows to control if a violating request is actually denied (default) or only logged.
It&amp;rsquo;s recommended to start with &lt;code>log&lt;/code>, check the logs for exceeding requests, adjust the limits if necessary and finally switch to &lt;code>block&lt;/code>.&lt;/p>
&lt;h3 id="seedrestriction">SeedRestriction&lt;/h3>
&lt;p>Please refer to &lt;a href="https://gardener.cloud/docs/gardener/deployment/gardenlet_api_access/">this document&lt;/a> for more information.&lt;/p>
&lt;h2 id="authorization-webhook-handlers">Authorization Webhook Handlers&lt;/h2>
&lt;p>This section describes the authorization webhook handlers that are currently served.&lt;/p>
&lt;h3 id="seedauthorization">SeedAuthorization&lt;/h3>
&lt;p>Please refer to &lt;a href="https://gardener.cloud/docs/gardener/deployment/gardenlet_api_access/">this document&lt;/a> for more information.&lt;/p></description></item><item><title>Docs: Apiserver</title><link>https://gardener.cloud/docs/gardener/concepts/apiserver/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/apiserver/</guid><description>
&lt;h1 id="gardener-api-server">Gardener API server&lt;/h1>
&lt;p>The Gardener API server is a Kubernetes-native extension based on its &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">aggregation layer&lt;/a>.
It is registered via an &lt;code>APIService&lt;/code> object and designed to run inside a Kubernetes cluster whose API it wants to extend.&lt;/p>
&lt;p>After registration, it exposes the following resources:&lt;/p>
&lt;h2 id="cloudprofiles">&lt;code>CloudProfile&lt;/code>s&lt;/h2>
&lt;p>&lt;code>CloudProfile&lt;/code>s are resources that describe a specific environment of an underlying infrastructure provider, e.g. AWS, Azure, etc.
Each shoot has to reference a &lt;code>CloudProfile&lt;/code> to declare the environment it should be created in.
In a &lt;code>CloudProfile&lt;/code> the gardener operator specifies certain constraints like available machine types, regions, which Kubernetes versions they want to offer, etc.
End-users can read &lt;code>CloudProfile&lt;/code>s to see these values, but only operators can change the content or create/delete them.
When a shoot is created or updated then an admission plugin checks that only values are used that are allowed via the referenced &lt;code>CloudProfile&lt;/code>.&lt;/p>
&lt;p>Additionally, a &lt;code>CloudProfile&lt;/code> may contain a &lt;code>providerConfig&lt;/code> which is a special configuration dedicated for the infrastructure provider.
Gardener does not evaluate or understand this config, but extension controllers might need for declaration of provider-specific constraints, or global settings.&lt;/p>
&lt;p>Please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml">this&lt;/a> example manifest and consult the documentation of your provider extension controller to get information about its &lt;code>providerConfig&lt;/code>.&lt;/p>
&lt;h2 id="seeds">&lt;code>Seed&lt;/code>s&lt;/h2>
&lt;p>&lt;code>Seed&lt;/code>s are resources that represent seed clusters.
Gardener does not care about how a seed cluster got created - the only requirement is that it is of at least Kubernetes v1.17 and passes the Kubernetes conformance tests.
The Gardener operator has to either deploy the Gardenlet into the cluster they want to use as seed (recommended, then the Gardenlet will create the &lt;code>Seed&lt;/code> object itself after bootstrapping), or they provide the kubeconfig to the cluster inside a secret (that is referenced by the &lt;code>Seed&lt;/code> resource) and create the &lt;code>Seed&lt;/code> resource themselves.&lt;/p>
&lt;p>Please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/45-secret-seed-backup.yaml">this&lt;/a>, &lt;a href="https://github.com/gardener/gardener/blob/master/example/50-seed.yaml">this&lt;/a>(, and optionally &lt;a href="https://github.com/gardener/gardener/blob/master/example/40-secret-seed.yaml">this&lt;/a>) example manifests.&lt;/p>
&lt;h2 id="shootquotas">Shoot&lt;code>Quota&lt;/code>s&lt;/h2>
&lt;p>In order to allow end-users not having their own dedicated infrastructure account to try out Gardener the operator can register an account owned by them that they allow to be used for trial clusters.
Trial clusters can be put under quota such that they don&amp;rsquo;t consume too many resources (resulting in costs), and so that one user cannot consume all resources on their own.
These clusters are automatically terminated after a specified time, but end-users may extend the lifetime manually if needed.&lt;/p>
&lt;p>Please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/60-quota.yaml">this&lt;/a> example manifest.&lt;/p>
&lt;h2 id="projects">&lt;code>Project&lt;/code>s&lt;/h2>
&lt;p>The first thing before creating a shoot cluster is to create a &lt;code>Project&lt;/code>.
A project is used to group multiple shoot clusters together.
End-users can invite colleagues to the project to enable collaboration, and they can either make them &lt;code>admin&lt;/code> or &lt;code>viewer&lt;/code>.
After an end-user has created a project they will get a dedicated namespace in the garden cluster for all their shoots.&lt;/p>
&lt;p>Please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/05-project-dev.yaml">this&lt;/a> example manifest.&lt;/p>
&lt;h2 id="secretbindings">&lt;code>SecretBinding&lt;/code>s&lt;/h2>
&lt;p>Now that the end-user has a namespace the next step is registering their infrastructure provider account.&lt;/p>
&lt;p>Please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/70-secret-provider.yaml">this&lt;/a> example manifest and consult the documentation of the extension controller for the respective infrastructure provider to get information about which keys are required in this secret.&lt;/p>
&lt;p>After the secret has been created the end-user has to create a special &lt;code>SecretBinding&lt;/code> resource that binds this secret.
Later when creating shoot clusters they will reference such a binding.&lt;/p>
&lt;p>Please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/80-secretbinding.yaml">this&lt;/a> example manifest.&lt;/p>
&lt;h2 id="shoots">&lt;code>Shoot&lt;/code>s&lt;/h2>
&lt;p>Shoot cluster contain various settings that influence how end-user Kubernetes clusters will look like in the end.
As Gardener heavily relies on extension controllers for operating system configuration, networking, and infrastructure specifics, the end-user has the possibility (and responsibility) to provide these provider-specific configurations as well.
Such configurations are not evaluated by Gardener (because it doesn&amp;rsquo;t know/understand them), but they are only transported to the respective extension controller.&lt;/p>
&lt;p>⚠️ This means that any configuration issues/mistake on the end-user side that relates to a provider-specific flag or setting cannot be caught during the update request itself but only later during the reconciliation (unless a validator webhook has been registered in the garden cluster by an operator).&lt;/p>
&lt;p>Please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml">this&lt;/a> example manifest and consult the documentation of the provider extension controller to get information about its &lt;code>spec.provider.controlPlaneConfig&lt;/code>, &lt;code>.spec.provider.infrastructureConfig&lt;/code>, and &lt;code>.spec.provider.workers[].providerConfig&lt;/code>.&lt;/p>
&lt;h2 id="clusteropenidconnectpresets">&lt;code>(Cluster)OpenIDConnectPreset&lt;/code>s&lt;/h2>
&lt;p>Please see &lt;a href="https://gardener.cloud/docs/gardener/usage/openidconnect-presets/">this&lt;/a> separate documentation file.&lt;/p>
&lt;h2 id="overview-data-model">Overview Data Model&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/gardener-data-model-overview_c5b559.png" alt="Gardener Overview Data Model">&lt;/p></description></item><item><title>Docs: Apiserver Admission Plugins</title><link>https://gardener.cloud/docs/gardener/concepts/apiserver_admission_plugins/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/apiserver_admission_plugins/</guid><description>
&lt;h1 id="admission-plugins">Admission Plugins&lt;/h1>
&lt;p>Similar to the kube-apiserver, the gardener-apiserver comes with a few in-tree managed admission plugins.
If you want to get an overview of the what and why of admission plugins then &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">this document&lt;/a> might be a good start.&lt;/p>
&lt;p>This document lists all existing admission plugins with a short explanation of what it is responsible for.&lt;/p>
&lt;h2 id="clusteropenidconnectpreset-openidconnectpreset">&lt;code>ClusterOpenIDConnectPreset&lt;/code>, &lt;code>OpenIDConnectPreset&lt;/code>&lt;/h2>
&lt;p>&lt;em>(both enabled by default)&lt;/em>&lt;/p>
&lt;p>These admission controllers react on &lt;code>CREATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
If the &lt;code>Shoot&lt;/code> does not specify any OIDC configuration (&lt;code>.spec.kubernetes.kubeAPIServer.oidcConfig=nil&lt;/code>) then it tries to find a matching &lt;code>ClusterOpenIDConnectPreset&lt;/code> or &lt;code>OpenIDConnectPreset&lt;/code>, respectively.
If there are multiples that match then the one with the highest weight &amp;ldquo;wins&amp;rdquo;.
In this case, the admission controller will default the OIDC configuration in the &lt;code>Shoot&lt;/code>.&lt;/p>
&lt;h2 id="controllerregistrationresources">&lt;code>ControllerRegistrationResources&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>ControllerRegistration&lt;/code>s.
It validates that there exists only one &lt;code>ControllerRegistration&lt;/code> in the system that is primarily responsible for a given kind/type resource combination.
This prevents misconfiguration by the Gardener administrator/operator.&lt;/p>
&lt;h2 id="customverbauthorizer">&lt;code>CustomVerbAuthorizer&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>Project&lt;/code>s.
It validates whether the user is bound to a RBAC role with the &lt;code>modify-spec-tolerations-whitelist&lt;/code> verb in case the user tries to change the &lt;code>.spec.tolerations.whitelist&lt;/code> field of the respective &lt;code>Project&lt;/code> resource.
Usually, regular project members are not bound to this custom verb, allowing the Gardener administrator to manage certain toleration whitelists on &lt;code>Project&lt;/code> basis.&lt;/p>
&lt;h2 id="deletionconfirmation">&lt;code>DeletionConfirmation&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>DELETE&lt;/code> operations for &lt;code>Project&lt;/code>s and &lt;code>Shoot&lt;/code>s and &lt;code>ShootState&lt;/code>s.
It validates that the respective resource is annotated with a deletion confirmation annotation, namely &lt;code>confirmation.gardener.cloud/deletion=true&lt;/code>.
Only if this annotation is present it allows the &lt;code>DELETE&lt;/code> operation to pass.
This prevents users from accidental/undesired deletions.&lt;/p>
&lt;h2 id="exposureclass">&lt;code>ExposureClass&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>Create&lt;/code> operations for &lt;code>Shoots&lt;/code>s.
It mutates &lt;code>Shoot&lt;/code> resources which has an &lt;code>ExposureClass&lt;/code> referenced by merging their both &lt;code>shootSelectors&lt;/code> and/or &lt;code>tolerations&lt;/code> into the &lt;code>Shoot&lt;/code> resource.&lt;/p>
&lt;h2 id="extensionvalidator">&lt;code>ExtensionValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>BackupEntry&lt;/code>s, &lt;code>BackupBucket&lt;/code>s, &lt;code>Seed&lt;/code>s, and &lt;code>Shoot&lt;/code>s.
For all the various extension types in the specifications of these objects, it validates whether there exists a &lt;code>ControllerRegistration&lt;/code> in the system that is primarily responsible for the stated extension type(s).
This prevents misconfigurations that would otherwise allow users to create such resources with extension types that don&amp;rsquo;t exist in the cluster, effectively leading to failing reconciliation loops.&lt;/p>
&lt;h2 id="extensionlabels">&lt;code>ExtensionLabels&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>BackupBucket&lt;/code>s, &lt;code>BackupEntry&lt;/code>s, &lt;code>CloudProfile&lt;/code>s, &lt;code>Seed&lt;/code>s and &lt;code>Shoot&lt;/code>s. For all the various extension types in the specifications of these objects, it adds a corresponding label in the resource. This would allow extension admission webhooks to filter out the resources they are responsible for and ignore all others. This label is of the form &lt;code>&amp;lt;extension-type&amp;gt;.extensions.gardener.cloud/&amp;lt;extension-name&amp;gt; : &amp;quot;true&amp;quot;&lt;/code>. For example, an extension label for provider extension type &lt;code>aws&lt;/code>, looks like &lt;code>provider.extensions.gardener.cloud/aws : &amp;quot;true&amp;quot;&lt;/code>.&lt;/p>
&lt;h2 id="plantvalidator">&lt;code>PlantValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>Plant&lt;/code>s.
It sets the &lt;code>gardener.cloud/created-by&lt;/code> annotation for newly created &lt;code>Plant&lt;/code> resources.
Also, it prevents creating new &lt;code>Plant&lt;/code> resources in &lt;code>Project&lt;/code>s that are already have a deletion timestamp.&lt;/p>
&lt;h2 id="projectvalidator">&lt;code>ProjectValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> operations for &lt;code>Project&lt;/code>s.
It prevents creating &lt;code>Project&lt;/code>s with a non-empty &lt;code>.spec.namespace&lt;/code> if the value in &lt;code>.spec.namespace&lt;/code> does not start with &lt;code>garden-&lt;/code>.&lt;/p>
&lt;p>⚠️ This admission plugin will be removed in a future release and its business logic will be incorporated into the static validation of the &lt;code>gardener-apiserver&lt;/code>.&lt;/p>
&lt;h2 id="resourcequota">&lt;code>ResourceQuota&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller enables &lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/#object-count-quota">object count ResourceQuotas&lt;/a> for Gardener resources, e.g. &lt;code>Shoots&lt;/code>, &lt;code>SecretBindings&lt;/code>, &lt;code>Projects&lt;/code>, etc..&lt;/p>
&lt;blockquote>
&lt;p>⚠️ In addition to this admission plugin, the &lt;a href="https://github.com/kubernetes/kubernetes/blob/release-1.2/docs/design/admission_control_resource_quota.md#resource-quota-controller">ResourceQuota controller&lt;/a> must be enabled for the Kube-Controller-Manager of your Garden cluster.&lt;/p>
&lt;/blockquote>
&lt;h2 id="resourcereferencemanager">&lt;code>ResourceReferenceManager&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>CloudProfile&lt;/code>s, &lt;code>Project&lt;/code>s, &lt;code>SecretBinding&lt;/code>s, &lt;code>Seed&lt;/code>s, and &lt;code>Shoot&lt;/code>s.
Generally, it checks whether referred resources stated in the specifications of these objects exist in the system (e.g., if a referenced &lt;code>Secret&lt;/code> exists).
However, it also has some special behaviours for certain resources:&lt;/p>
&lt;ul>
&lt;li>&lt;code>CloudProfile&lt;/code>s: It rejects removing Kubernetes or machine image versions if there is at least one &lt;code>Shoot&lt;/code> that refers to them.&lt;/li>
&lt;li>&lt;code>Project&lt;/code>s: It sets the &lt;code>.spec.createdBy&lt;/code> field for newly created &lt;code>Project&lt;/code> resources, and defaults the &lt;code>.spec.owner&lt;/code> field in case it is empty (to the same value of &lt;code>.spec.createdBy&lt;/code>).&lt;/li>
&lt;li>&lt;code>Seed&lt;/code>s: It rejects changing the &lt;code>.spec.settings.shootDNS.enabled&lt;/code> value if there is at least one &lt;code>Shoot&lt;/code> that refers to this seed.&lt;/li>
&lt;li>&lt;code>Shoot&lt;/code>s: It sets the &lt;code>gardener.cloud/created-by=&amp;lt;username&amp;gt;&lt;/code> annotation for newly created &lt;code>Shoot&lt;/code> resources.&lt;/li>
&lt;/ul>
&lt;h2 id="seedvalidator">&lt;code>SeedValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>DELETE&lt;/code> operations for &lt;code>Seed&lt;/code>s.
Rejects the deletion if &lt;code>Shoot&lt;/code>(s) reference the seed cluster.&lt;/p>
&lt;h2 id="shootdns">&lt;code>ShootDNS&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It tries to assign a default domain to the &lt;code>Shoot&lt;/code> if it gets scheduled to a seed that enables DNS for shoots (&lt;code>.spec.settings.shootDNS.enabled=true&lt;/code>).
It also validates that the DNS configuration (&lt;code>.spec.dns&lt;/code>) is not set if the seed disables DNS for shoots.&lt;/p>
&lt;h2 id="shootquotavalidator">&lt;code>ShootQuotaValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It validates the resource consumption declared in the specification against applicable &lt;code>Quota&lt;/code> resources.
Only if the applicable &lt;code>Quota&lt;/code> resources admit the configured resources in the &lt;code>Shoot&lt;/code> then it allows the request.
Applicable &lt;code>Quota&lt;/code>s are referred in the &lt;code>SecretBinding&lt;/code> that is used by the &lt;code>Shoot&lt;/code>.&lt;/p>
&lt;h2 id="shootvpaenabledbydefault">&lt;code>ShootVPAEnabledByDefault&lt;/code>&lt;/h2>
&lt;p>&lt;em>(disabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
If enabled, it will enable the managed &lt;code>VerticalPodAutoscaler&lt;/code> components (see &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_autoscaling/#vertical-pod-auto-scaling">this doc&lt;/a>)
by setting &lt;code>spec.kubernetes.verticalPodAutoscaler.enabled=true&lt;/code> for newly created Shoots.
Already existing Shoots and new Shoots that explicitly disable VPA (&lt;code>spec.kubernetes.verticalPodAutoscaler.enabled=false&lt;/code>)
will not be affected by this admission plugin.&lt;/p>
&lt;h2 id="shoottolerationrestriction">&lt;code>ShootTolerationRestriction&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It validates the &lt;code>.spec.tolerations&lt;/code> used in &lt;code>Shoot&lt;/code>s against the whitelist of its &lt;code>Project&lt;/code>, or against the whitelist configured in the admission controller&amp;rsquo;s configuration, respectively.
Additionally, it defaults the &lt;code>.spec.tolerations&lt;/code> in &lt;code>Shoot&lt;/code>s with those configured in its &lt;code>Project&lt;/code>, and those configured in the admission controller&amp;rsquo;s configuration, respectively.&lt;/p>
&lt;h2 id="shootvalidator">&lt;code>ShootValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It validates certain configurations in the specification against the referred &lt;code>CloudProfile&lt;/code> (e.g., machine images, machine types, used Kubernetes version, &amp;hellip;).
Generally, it performs validations that cannot be handled by the static API validation due to their dynamic nature (e.g., when something needs to be checked against referred resources).
Additionally, it takes over certain defaulting tasks (e.g., default machine image for worker pools).&lt;/p>
&lt;h2 id="shootmanagedseed">&lt;code>ShootManagedSeed&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>DELETE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It rejects the deletion if the &lt;code>Shoot&lt;/code> is referred to by a &lt;code>ManagedSeed&lt;/code>.&lt;/p>
&lt;h2 id="managedseedvalidator">&lt;code>ManagedSeedValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>ManagedSeeds&lt;/code>s.
It validates certain configuration values in the specification against the referred &lt;code>Shoot&lt;/code>, for example Seed provider, network ranges, DNS domain, etc.
Similarly to &lt;code>ShootValidator&lt;/code>, it performs validations that cannot be handled by the static API validation due to their dynamic nature.
Additionally, it performs certain defaulting tasks, making sure that configuration values that are not specified are defaulted to the values of the referred &lt;code>Shoot&lt;/code>, for example Seed provider, network ranges, DNS domain, etc.&lt;/p>
&lt;h2 id="managedseedshoot">&lt;code>ManagedSeedShoot&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>DELETE&lt;/code> operations for &lt;code>ManagedSeed&lt;/code>s.
It rejects the deletion if there are &lt;code>Shoot&lt;/code>s that are scheduled onto the &lt;code>Seed&lt;/code> that is registered by the &lt;code>ManagedSeed&lt;/code>.&lt;/p></description></item><item><title>Docs: Architecture</title><link>https://gardener.cloud/docs/gardener/concepts/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/architecture/</guid><description>
&lt;h4 id="official-definition---what-is-kubernetes">Official Definition - What is Kubernetes?&lt;/h4>
&lt;blockquote>
&lt;p>&amp;ldquo;Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.&amp;rdquo;&lt;/p>
&lt;/blockquote>
&lt;h4 id="introduction---basic-principle">Introduction - Basic Principle&lt;/h4>
&lt;p>The foundation of the Gardener (providing &lt;em>&lt;strong>Kubernetes Clusters as a Service&lt;/strong>&lt;/em>) is Kubernetes itself, because Kubernetes is the go-to solution to manage software in the Cloud, even when it&amp;rsquo;s Kubernetes itself (see also OpenStack which is provisioned more and more on top of Kubernetes as well).&lt;/p>
&lt;p>While self-hosting, meaning to run Kubernetes components inside Kubernetes, is a popular topic in the community, we apply a special pattern catering to the needs of our cloud platform to provision hundreds or even thousands of clusters. We take a so-called &amp;ldquo;seed&amp;rdquo; cluster and seed the control plane (such as the API server, scheduler, controllers, etcd persistence and others) of an end-user cluster, which we call &amp;ldquo;shoot&amp;rdquo; cluster, as pods into the &amp;ldquo;seed&amp;rdquo; cluster. That means one &amp;ldquo;seed&amp;rdquo; cluster, of which we will have one per IaaS and region, hosts the control planes of multiple &amp;ldquo;shoot&amp;rdquo; clusters. That allows us to avoid dedicated hardware/virtual machines for the &amp;ldquo;shoot&amp;rdquo; cluster control planes. We simply put the control plane into pods/containers and since the &amp;ldquo;seed&amp;rdquo; cluster watches them, they can be deployed with a replica count of 1 and only need to be scaled out when the control plane gets under pressure, but no longer for HA reasons. At the same time, the deployments get simpler (standard Kubernetes deployment) and easier to update (standard Kubernetes rolling update). The actual &amp;ldquo;shoot&amp;rdquo; cluster consists only out of the worker nodes (no control plane) and therefore the users may get full administrative access to their clusters.&lt;/p>
&lt;h4 id="setting-the-scene---components-and-procedure">Setting The Scene - Components and Procedure&lt;/h4>
&lt;p>We provide a central operator UI, which we call the &amp;ldquo;Gardener Dashboard&amp;rdquo;. It talks to a dedicated cluster, which we call the &amp;ldquo;Garden&amp;rdquo; cluster and uses custom resources managed by an &lt;a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#api-server-aggregation">aggregated API server&lt;/a>, one of the general extension concepts of Kubernetes) to represent &amp;ldquo;shoot&amp;rdquo; clusters. In this &amp;ldquo;Garden&amp;rdquo; cluster runs the &amp;ldquo;Gardener&amp;rdquo;, which is basically a Kubernetes controller that watches the custom resources and acts upon them, i.e. creates, updates/modifies, or deletes &amp;ldquo;shoot&amp;rdquo; clusters. The creation follows basically these steps:&lt;/p>
&lt;ul>
&lt;li>Create a namespace in the &amp;ldquo;seed&amp;rdquo; cluster for the &amp;ldquo;shoot&amp;rdquo; cluster which will host the &amp;ldquo;shoot&amp;rdquo; cluster control plane&lt;/li>
&lt;li>Generate secrets and credentials which the worker nodes will need to talk to the control plane&lt;/li>
&lt;li>Create the infrastructure (using &lt;a href="https://www.terraform.io/">Terraform&lt;/a>), which basically consists out of the network setup)&lt;/li>
&lt;li>Deploy the &amp;ldquo;shoot&amp;rdquo; cluster control plane into the &amp;ldquo;shoot&amp;rdquo; namespace in the &amp;ldquo;seed&amp;rdquo; cluster, containing the &amp;ldquo;machine-controller-manager&amp;rdquo; pod&lt;/li>
&lt;li>Create machine CRDs in the &amp;ldquo;seed&amp;rdquo; cluster, describing the configuration and the number of worker machines for the &amp;ldquo;shoot&amp;rdquo; (the machine-controller-manager watches the CRDs and creates virtual machines out of it)&lt;/li>
&lt;li>Wait for the &amp;ldquo;shoot&amp;rdquo; cluster API server to become responsive (pods will be scheduled, persistent volumes and load balancers are created by Kubernetes via the respective cloud provider)&lt;/li>
&lt;li>Finally we deploy &lt;code>kube-system&lt;/code> daemons like &lt;code>kube-proxy&lt;/code> and further add-ons like the &lt;code>dashboard&lt;/code> into the &amp;ldquo;shoot&amp;rdquo; cluster and the cluster becomes active&lt;/li>
&lt;/ul>
&lt;h4 id="overview-architecture-diagram">Overview Architecture Diagram&lt;/h4>
&lt;p>&lt;img src="https://gardener.cloud/__resources/gardener-architecture-overview_2bd462.png" alt="Gardener Overview Architecture Diagram">&lt;/p>
&lt;h4 id="detailed-architecture-diagram">Detailed Architecture Diagram&lt;/h4>
&lt;p>&lt;img src="https://gardener.cloud/__resources/gardener-architecture-detailed_945c90.png" alt="Gardener Detailed Architecture Diagram">&lt;/p>
&lt;p>Note: The &lt;code>kubelet&lt;/code> as well as the pods inside the &amp;ldquo;shoot&amp;rdquo; cluster talk through the front-door (load balancer IP; public Internet) to its &amp;ldquo;shoot&amp;rdquo; cluster API server running in the &amp;ldquo;seed&amp;rdquo; cluster. The reverse communication from the API server to the pod, service, and node networks happens through a VPN connection that we deploy into &amp;ldquo;seed&amp;rdquo; and &amp;ldquo;shoot&amp;rdquo; clusters.&lt;/p></description></item><item><title>Docs: Backup Restore</title><link>https://gardener.cloud/docs/gardener/concepts/backup-restore/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/backup-restore/</guid><description>
&lt;h1 id="backup-and-restore">Backup and restore&lt;/h1>
&lt;p>Kubernetes uses Etcd as the key-value store for its resource definitions. Gardener supports the backup and restore of etcd. It is the responsibility of the shoot owners to backup the workload data.&lt;/p>
&lt;p>Gardener uses &lt;a href="https://github.com/gardener/etcd-backup-restore">etcd-backup-restore&lt;/a> component to backup the etcd backing the Shoot cluster regularly and restore in case of disaster. It is deployed as sidecar via &lt;a href="https://github.com/gardener/etcd-druid">etcd-druid&lt;/a>. This doc mainly focuses on the backup and restore configuration used by Gardener when deploying these components. For more details on the design and internal implementation details, please refer &lt;a href="https://gardener.cloud/docs/gardener/proposals/06-etcd-druid/">GEP-06&lt;/a> and documentation on individual repository.&lt;/p>
&lt;h2 id="bucket-provisioning">Bucket provisioning&lt;/h2>
&lt;p>Refer the &lt;a href="https://gardener.cloud/docs/gardener/extensions/backupbucket/">backup bucket extension document&lt;/a> to know details about configuring backup bucket.&lt;/p>
&lt;h2 id="backup-policy">Backup Policy&lt;/h2>
&lt;p>etcd-backup-restore supports full snapshot and delta snapshots over full snapshot. In Gardener, this configuration is currently hard-coded to following parameters:&lt;/p>
&lt;ul>
&lt;li>Full Snapshot Schedule:
&lt;ul>
&lt;li>Daily, &lt;code>24hr&lt;/code> interval.&lt;/li>
&lt;li>For each Shoot, the schedule time in a day is randomized based on the configured Shoot maintenance window.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Delta Snapshot schedule:
&lt;ul>
&lt;li>At &lt;code>5min&lt;/code> interval.&lt;/li>
&lt;li>If aggregated events size since last snapshot goes beyond &lt;code>100Mib&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup History / Garbage backup deletion policy:
&lt;ul>
&lt;li>Gardener configure backup restore to have &lt;code>Exponential&lt;/code> garbage collection policy.&lt;/li>
&lt;li>As per policy, following backups are retained.&lt;/li>
&lt;li>All full backups and delta backups for the previous hour.&lt;/li>
&lt;li>Latest full snapshot of each previous hour for the day.&lt;/li>
&lt;li>Latest full snapshot of each previous day for 7 days.&lt;/li>
&lt;li>Latest full snapshot of the previous 4 weeks.&lt;/li>
&lt;li>Garbage Collection is configured at &lt;code>12hr&lt;/code> interval.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Listing:
&lt;ul>
&lt;li>Gardener don&amp;rsquo;t have any API to list out the backups.&lt;/li>
&lt;li>To find the backup list, admin can checkout the &lt;code>BackupEntry&lt;/code> resource associated with Shoot which holds the bucket and prefix details on object store.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="restoration">Restoration&lt;/h2>
&lt;p>Restoration process of etcd is automated through the etcd-backup-restore component from latest snapshot. Gardener dosen&amp;rsquo;t support Point-In-Time-Recovery (PITR) of etcd. In case of etcd disaster, the etcd is recovered from latest backup automatically. For further details, please refer the &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/master/doc/proposals/restoration.md">doc&lt;/a>. Post restoration of etcd, the Shoot reconciliation loop brings back the cluster to same state.&lt;/p>
&lt;p>Again, Shoot owner is responsible for maintaining the backup/restore of his workload. Gardener does only take care of the cluster&amp;rsquo;s etcd.&lt;/p></description></item><item><title>Docs: Cluster Api</title><link>https://gardener.cloud/docs/gardener/concepts/cluster-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/cluster-api/</guid><description>
&lt;h1 id="relation-between-gardener-api-and-cluster-api-sig-cluster-lifecycle">Relation between Gardener API and Cluster API (SIG Cluster Lifecycle)&lt;/h1>
&lt;p>In essence, the Cluster API harmonizes how to get to clusters, while Gardener goes one step further and also harmonizes the clusters themselves. The Cluster API delegates the specifics to so-called providers for infrastructures or control planes via specific CR(D)s while Gardener only has one cluster CR(D). Different Cluster API providers, e.g. for AWS, Azure, GCP, etc. give you vastly different Kubernetes clusters. In contrast, Gardener gives you the exact same clusters with the exact same K8s version, operating system, control plane configuration like for API server or kubelet, add-ons like overlay network, HPA/VPA, DNS and certificate controllers, ingress and network policy controllers, control plane monitoring and logging stacks, down to the behavior of update procedures, auto-scaling, self-healing, etc. on all supported infrastructures. These homogeneous clusters are an essential goal for Gardener as its main purpose is to simplify operations for teams that need to develop and ship software on Kubernetes clusters on a plethora of infrastructures (a.k.a. multi-cloud).&lt;/p>
&lt;p>Incidentally, Gardener influenced the Machine API in the Cluster API with its &lt;a href="https://github.com/gardener/machine-controller-manager">Machine Controller Manager&lt;/a> and was the &lt;a href="https://github.com/kubernetes-sigs/cluster-api/commit/00b1ead264aea6f88585559056c180771cce3815">first to adopt it&lt;/a>, see also &lt;a href="https://www.youtube.com/watch?v=Mtg8jygK3Hs">joint SIG Cluster Lifecycle KubeCon talk&lt;/a> where @hardikdr from our Gardener team in India spoke.&lt;/p>
&lt;p>That means, we follow the &lt;a href="https://github.com/kubernetes-sigs/cluster-api#cluster-api">Cluster API&lt;/a> with great interest and are active members. It was completely overhauled from &lt;code>v1alpha1&lt;/code> to &lt;code>v1alpha2&lt;/code>. But because &lt;code>v1alpha2&lt;/code> made too many assumptions about the bring-up of masters and was enforcing master machine operations (see &lt;a href="https://cluster-api.sigs.k8s.io/user/concepts.html#control-plane">here&lt;/a>: “As of &lt;code>v1alpha2&lt;/code>, Machine-Based is the only control plane type that Cluster API supports”), services that managed their control planes differently like GKE or Gardener couldn&amp;rsquo;t adopt it (e.g. &lt;a href="https://cloud.google.com/anthos/gke/docs/on-prem/concepts/cluster-api">Google only supports &lt;code>v1alpha1&lt;/code>&lt;/a>). In 2020 &lt;a href="https://kubernetes.io/blog/2020/04/21/cluster-api-v1alpha3-delivers-new-features-and-an-improved-user-experience/">&lt;code>v1alpha3&lt;/code>&lt;/a> was introduced and made it possible (again) to integrate managed services like GKE or Gardener. The mapping from the Gardener API to the Cluster API is mostly syntactic.&lt;/p>
&lt;p>To wrap it up, while the Cluster API knows about clusters, it doesn&amp;rsquo;t know about their make-up. With Gardener, we wanted to go beyond that and harmonize the make-up of the clusters themselves and make them homogeneous across all supported infrastructures. Gardener can therefore deliver homogeneous clusters with exactly the same configuration and behavior on all infrastructures (see also &lt;a href="https://k8s-testgrid.appspot.com/conformance-all">Gardener&amp;rsquo;s coverage in the official conformance test grid&lt;/a>).&lt;/p>
&lt;p>With &lt;a href="https://kubernetes.io/blog/2020/04/21/cluster-api-v1alpha3-delivers-new-features-and-an-improved-user-experience">Cluster API &lt;code>v1alpha3&lt;/code>&lt;/a> and the support for declarative control plane management, it became now possible (again) to enable Kubernetes managed services like GKE or Gardener. We would be more than happy, if the community would be interested, to contribute a Gardener control plane provider.&lt;/p></description></item><item><title>Docs: Controller Manager</title><link>https://gardener.cloud/docs/gardener/concepts/controller-manager/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/controller-manager/</guid><description>
&lt;h1 id="gardener-controller-manager">Gardener Controller Manager&lt;/h1>
&lt;p>The Gardener Controller Manager (often refered to as &amp;ldquo;GCM&amp;rdquo;) is a component that runs next to the Gardener API server, similar to the Kubernetes Controller Manager.
It runs several control loops that do not require talking to any seed or shoot cluster.
Also, as of today it exposes a HTTPS server that is serving several endpoints for webhooks for certain resources.&lt;/p>
&lt;p>This document explains the various functionalities of the Gardener Controller Manager and their purpose.&lt;/p>
&lt;h2 id="control-loops">Control Loops&lt;/h2>
&lt;h3 id="project-controller">&lt;code>Project&lt;/code> Controller&lt;/h3>
&lt;p>This controller consists out of three reconciliation loops:
The main loop is reconciling &lt;code>Project&lt;/code> resources while the second loop is controlling the necessary actions for stale projects.&lt;/p>
&lt;h4 id="main-reconciler">&amp;ldquo;Main&amp;rdquo; Reconciler&lt;/h4>
&lt;p>This reconciler will create a dedicated &lt;code>Namespace&lt;/code> prefixed with &lt;code>garden-&lt;/code> for each &lt;code>Project&lt;/code> resource.
The name of the namespace can either be stated in the &lt;code>.spec.namespace&lt;/code>, or it will be auto-generated by the reconciler.
If &lt;code>.spec.namespace&lt;/code> is set then it creates it if it does not exist yet.
Otherwise, it tries to adopt it.
This will only succeed if the &lt;code>Namespace&lt;/code> was previously labeled with &lt;code>gardener.cloud/role=project&lt;/code> and &lt;code>project.gardener.cloud/name=&amp;lt;project-name&amp;gt;&lt;/code>.
This is to prevent that end-users can adopt arbitrary namespaces and escalate their privileges, e.g. the &lt;code>kube-system&lt;/code> namespace.&lt;/p>
&lt;p>After the namespace was created/adopted the reconciler creates several &lt;code>ClusterRole&lt;/code>s and &lt;code>ClusterRoleBinding&lt;/code>s that allow the project members to access related resources based on their roles.
These RBAC resources are prefixed with &lt;code>gardener.cloud:system:project{-member,-viewer}:&amp;lt;project-name&amp;gt;&lt;/code>.
Gardener administrators and extension developers can define their own roles, see &lt;a href="https://gardener.cloud/docs/gardener/extensions/project-roles/">this document&lt;/a> for more information.&lt;/p>
&lt;p>In addition, operators can configure the Project controller to maintain a default &lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">ResourceQuota&lt;/a> for project namespaces.
Quotas can especially limit the creation of user facing resources, e.g. &lt;code>Shoots&lt;/code>, &lt;code>SecretBindings&lt;/code>, &lt;code>Secrets&lt;/code> and thus protect the Garden cluster from massive resource exhaustion but also enable operators to align quotas with respective enterprise policies.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ &lt;strong>Gardener itself is not exempted from configured quotas&lt;/strong>. For example, Gardener creates &lt;code>Secrets&lt;/code> for every shoot cluster in the project namespace and at the same time increases the available quota count. Please mind this additional resource consumption.&lt;/p>
&lt;/blockquote>
&lt;p>The GCM configuration provides a template section &lt;code>controllers.project.quotas&lt;/code> where such a ResourceQuota (see example below) can be deposited.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">controllers:
project:
quotas:
- config:
apiVersion: v1
kind: ResourceQuota
spec:
hard:
count/shoots.core.gardener.cloud: &lt;span style="color:#a31515">&amp;#34;100&amp;#34;&lt;/span>
count/secretbindings.core.gardener.cloud: &lt;span style="color:#a31515">&amp;#34;10&amp;#34;&lt;/span>
count/secrets: &lt;span style="color:#a31515">&amp;#34;800&amp;#34;&lt;/span>
projectSelector: {}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The Project controller takes the shown &lt;code>config&lt;/code> and creates a &lt;code>ResourceQuota&lt;/code> with the name &lt;code>gardener&lt;/code> in the project namespace.
If a &lt;code>ResourceQuota&lt;/code> resource with the name &lt;code>gardener&lt;/code> already exists, the controller will only update fields in &lt;code>spec.hard&lt;/code> which are &lt;strong>unavailable&lt;/strong> at that time.
Labels and annotations on the &lt;code>ResourceQuota&lt;/code> &lt;code>config&lt;/code> get merged with the respective fields on existing &lt;code>ResourceQuota&lt;/code>s.
An optional &lt;code>projectSelector&lt;/code> narrows down the amount of projects that are equipped with the given &lt;code>config&lt;/code>.
If multiple configs match for a project, then only the first match in the list is applied to the project namespace.&lt;/p>
&lt;p>The &lt;code>.status.phase&lt;/code> of the &lt;code>Project&lt;/code> resources will be set to &lt;code>Ready&lt;/code> or &lt;code>Failed&lt;/code> by the reconciler to indicate whether the reconciliation loop was performed successfully.
Also, it will generate &lt;code>Event&lt;/code>s to provide further information about its operations.&lt;/p>
&lt;h4 id="stale-projects-reconciler">&amp;ldquo;Stale Projects&amp;rdquo; Reconciler&lt;/h4>
&lt;p>As Gardener is a large-scale Kubernetes as a Service it is designed for being used by a large amount of end-users.
Over time, it is likely to happen that some of the hundreds or thousands of &lt;code>Project&lt;/code> resources are no longer actively used.&lt;/p>
&lt;p>Gardener offers the &amp;ldquo;stale projects&amp;rdquo; reconciler which will take care of identifying such stale projects, marking them with a &amp;ldquo;warning&amp;rdquo;, and eventually deleting them after a certain time period.
This reconciler is enabled by default and works as following:&lt;/p>
&lt;ol>
&lt;li>Projects are considered as &amp;ldquo;stale&amp;rdquo;/not actively used when all of the following conditions apply: The namespace associated with the &lt;code>Project&lt;/code> does not have any&amp;hellip;
&lt;ol>
&lt;li>&lt;code>Shoot&lt;/code> resources.&lt;/li>
&lt;li>&lt;code>Plant&lt;/code> resources.&lt;/li>
&lt;li>&lt;code>BackupEntry&lt;/code> resources.&lt;/li>
&lt;li>&lt;code>Secret&lt;/code> resources that are referenced by a &lt;code>SecretBinding&lt;/code> that is in use by a &lt;code>Shoot&lt;/code> (not necessarily in the same namespace).&lt;/li>
&lt;li>&lt;code>Quota&lt;/code> resources that are referenced by a &lt;code>SecretBinding&lt;/code> that is in use by a &lt;code>Shoot&lt;/code> (not necessarily in the same namespace).&lt;/li>
&lt;li>The time period when the projet was used for the last time (&lt;code>status.lastActivityTimestamp&lt;/code>) is longer than the configured &lt;code>minimumLifetimeDays&lt;/code>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>If a project is considered &amp;ldquo;stale&amp;rdquo; then its &lt;code>.status.staleSinceTimestamp&lt;/code> will be set to the time when it was first detected to be stale.
If it gets actively used again this timestamp will be removed.
After some time the &lt;code>.status.staleAutoDeleteTimestamp&lt;/code> will be set to a timestamp after which Gardener will auto-delete the &lt;code>Project&lt;/code> resource if it still is not actively used.&lt;/p>
&lt;p>The component configuration of the Gardener Controller Manager offers to configure the following options:&lt;/p>
&lt;ul>
&lt;li>&lt;code>minimumLifetimeDays&lt;/code>: Don&amp;rsquo;t consider newly created &lt;code>Project&lt;/code>s as &amp;ldquo;stale&amp;rdquo; too early to give people/end-users some time to onboard and get familiar with the system. The &amp;ldquo;stale project&amp;rdquo; reconciler won&amp;rsquo;t set any timestamp for &lt;code>Project&lt;/code>s younger than &lt;code>minimumLifetimeDays&lt;/code>. When you change this value then projects marked as &amp;ldquo;stale&amp;rdquo; may be no longer marked as &amp;ldquo;stale&amp;rdquo; in case they are young enough, or vice versa.&lt;/li>
&lt;li>&lt;code>staleGracePeriodDays&lt;/code>: Don&amp;rsquo;t compute auto-delete timestamps for stale &lt;code>Project&lt;/code>s that are unused for only less than &lt;code>staleGracePeriodDays&lt;/code>. This is to not unnecessarily make people/end-users nervous &amp;ldquo;just because&amp;rdquo; they haven&amp;rsquo;t actively used their &lt;code>Project&lt;/code> for a given amount of time. When you change this value then already assigned auto-delete timestamps may be removed again if the new grace period is not yet exceeded.&lt;/li>
&lt;li>&lt;code>staleExpirationTimeDays&lt;/code>: Expiration time after which stale &lt;code>Project&lt;/code>s are finally auto-deleted (after &lt;code>.status.staleSinceTimestamp&lt;/code>). If this value is changed and an auto-delete timestamp got already assigned to the projects then the new value will only take effect if it&amp;rsquo;s increased. Hence, decreasing the &lt;code>staleExpirationTimeDays&lt;/code> will not decrease already assigned auto-delete timestamps.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Gardener administrators/operators can exclude specific &lt;code>Project&lt;/code>s from the stale check by annotating the related &lt;code>Namespace&lt;/code> resource with &lt;code>project.gardener.cloud/skip-stale-check=true&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;h4 id="activity-reconciler">&amp;ldquo;Activity&amp;rdquo; Reconciler&lt;/h4>
&lt;p>Since the other two reconcilers are unable to actively monitor the relevant objects that are used in a &lt;code>Project&lt;/code> (&lt;code>Shoot&lt;/code>, &lt;code>Plant&lt;/code>, etc.), there could be a situation where the user creates and deletes objects in a short period of time. In that case the &lt;code>Stale Project Reconciler&lt;/code> could not see that there was any activity on that project and it will still mark it as a &lt;code>Stale&lt;/code>, even though it is actively used.&lt;/p>
&lt;p>The &lt;code>Project Activity Reconciler&lt;/code> is implemented to take care of such cases. An event handler will notify the reconciler for any acitivity (Currently only for &lt;code>Shoots&lt;/code>) and then it will update the &lt;code>status.lastActivityTimestamp&lt;/code>. This update will also trigger the &lt;code>Stale Project Reconciler&lt;/code>.&lt;/p>
&lt;h3 id="event-controller">Event Controller&lt;/h3>
&lt;p>With the Gardener Event Controller you can prolong the lifespan of events related to Shoot clusters.
This is an optional controller which will become active once you provide the below mentioned configuration.&lt;/p>
&lt;p>All events in K8s are deleted after a configurable time-to-live (controlled via a &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">kube-apiserver argument&lt;/a> called &lt;code>--event-ttl&lt;/code> (defaulting to 1 hour)).
The need to prolong the time-to-live for Shoot cluster events frequently arises when debugging customer issues on live systems.
This controller leaves events involving Shoots untouched while deleting all other events after a configured time.
In order to activate it, provide the following configuration:&lt;/p>
&lt;ul>
&lt;li>&lt;code>concurrentSyncs&lt;/code>: The amount of goroutines scheduled for reconciling events.&lt;/li>
&lt;li>&lt;code>ttlNonShootEvents&lt;/code>: When an event reaches this time-to-live it gets deleted unless it is a Shoot-related event (defaults to &lt;code>1h&lt;/code>, equivalent to the &lt;code>event-ttl&lt;/code> default).&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>⚠️ In addition, you should also configure the &lt;code>--event-ttl&lt;/code> for the kube-apiserver to define an upper-limit of how long Shoot-related events should be stored.
The &lt;code>--event-ttl&lt;/code> should be larger than the &lt;code>ttlNonShootEvents&lt;/code> or this controller will have no effect.&lt;/p>
&lt;/blockquote>
&lt;h3 id="shoot-reference-controller">Shoot Reference Controller&lt;/h3>
&lt;p>Shoot objects may specify references to further objects in the Garden cluster which are required for certain features.
For example, users can configure various DNS providers via &lt;code>.spec.dns.providers&lt;/code> and usually need to refer to a corresponding &lt;code>secret&lt;/code> with valid DNS provider credentials inside.
Such objects need a special protection against deletion requests as long as they are still being referenced by one or multiple shoots.&lt;/p>
&lt;p>Therefore, the Shoot Reference Controller scans shoot clusters for referenced objects and adds the finalizer &lt;code>gardener.cloud/reference-protection&lt;/code> to their &lt;code>.metadata.finalizers&lt;/code> list.
The scanned shoot also gets this finalizer to enable a proper garbage collection in case the Gardener-Controller-Manager is offline at the moment of an incoming deletion request.
When an object is not actively referenced anymore because the shoot specification has changed or all related shoots were deleted (are in deletion), the controller will remove the added finalizer again, so that the object can safely be deleted or garbage collected.&lt;/p>
&lt;p>The Shoot Reference Controller inspects the following references:&lt;/p>
&lt;ul>
&lt;li>DNS provider secrets (&lt;code>.spec.dns.provider&lt;/code>)&lt;/li>
&lt;li>Audit policy configmaps (&lt;code>.spec.kubernetes.kubeAPIServer.auditConfig.auditPolicy.configMapRef&lt;/code>)&lt;/li>
&lt;/ul>
&lt;p>Further checks might be added in the future.&lt;/p>
&lt;h3 id="shoot-retry-controller">Shoot Retry Controller&lt;/h3>
&lt;p>The Shoot Retry Controller is responsible for retrying certain failed Shoots. Currently the controller retries only failed Shoots with error code &lt;code>ERR_INFRA_RATE_LIMITS_EXCEEDED&lt;/code>.&lt;/p>
&lt;h3 id="seed-controller">Seed Controller&lt;/h3>
&lt;p>The Seed controller in the Gardener Controller Manager reconciles &lt;code>Seed&lt;/code> objects with the help of the following reconcilers.&lt;/p>
&lt;h4 id="main-reconciler-1">&amp;ldquo;Main&amp;rdquo; Reconciler&lt;/h4>
&lt;p>This reconciliation loop takes care about seed related operations in the Garden cluster. When a new &lt;code>Seed&lt;/code> object is created
the reconciler creates a new &lt;code>Namespace&lt;/code> in the garden cluster &lt;code>seed-&amp;lt;seed-name&amp;gt;&lt;/code>. &lt;code>Namespaces&lt;/code> dedicated to single
seed clusters allow us to segregate access permissions i.e., a Gardenlet must not have permissions to access objects in
all &lt;code>Namespaces&lt;/code> in the Garden cluster.
There are objects in a Garden environment which are created once by the operator e.g., default domain secret,
alerting credentials, and required for operations happening in the Gardenlet. Therefore, we not only need a seed specific
&lt;code>Namespace&lt;/code> but also a copy of these &amp;ldquo;shared&amp;rdquo; objects.&lt;/p>
&lt;p>The &amp;ldquo;main&amp;rdquo; reconciler takes care about this replication:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Kind&lt;/th>
&lt;th style="text-align:center">Namespace&lt;/th>
&lt;th style="text-align:center">Label Selector&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">Secret&lt;/td>
&lt;td style="text-align:center">garden&lt;/td>
&lt;td style="text-align:center">gardener.cloud/role&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="backup-bucket-reconciler">&amp;ldquo;Backup Bucket&amp;rdquo; Reconciler&lt;/h4>
&lt;p>Every time a &lt;code>BackupBucket&lt;/code> object is created or updated, the referenced &lt;code>Seed&lt;/code> object is enqueued for reconciliation.
It&amp;rsquo;s the reconciler&amp;rsquo;s task to check the &lt;code>status&lt;/code> subresource of all existing &lt;code>BackupBuckets&lt;/code> that belong to this seed.
If at least one &lt;code>BackupBucket&lt;/code> has &lt;code>.status.lastError&lt;/code>, the seed condition &lt;code>BackupBucketsReady&lt;/code> will turn &lt;code>false&lt;/code> and
consequently the seed is considered as &lt;code>NotReady&lt;/code>. Once the &lt;code>BackupBucket&lt;/code> is healthy again, the seed will be re-queued
and the condition will turn &lt;code>true&lt;/code>.&lt;/p>
&lt;h4 id="lifecycle-reconciler">&amp;ldquo;Lifecycle&amp;rdquo; Reconciler&lt;/h4>
&lt;p>The &amp;ldquo;Lifecycle&amp;rdquo; reconciler processes &lt;code>Seed&lt;/code> objects which are enqueued every 10 seconds in order to check if the responsible
Gardenlet is still responding and operable. Therefore, it checks renewals via &lt;code>Lease&lt;/code> objects of the seed in the garden cluster
which are renewed regularly by the Gardenlet.&lt;/p>
&lt;p>In case a &lt;code>Lease&lt;/code> is not renewed for the configured amount in &lt;code>config.controllers.seed.monitorPeriod.duration&lt;/code>:&lt;/p>
&lt;ol>
&lt;li>The reconciler assumes that the Gardenlet stopped operating and updates the &lt;code>GardenletReady&lt;/code> condition to &lt;code>Unknown&lt;/code>.&lt;/li>
&lt;li>Additionally, conditions and constraints of all &lt;code>Shoot&lt;/code> resources scheduled on the affected seed are set to &lt;code>Unknown&lt;/code> as well
because a striking Gardenlet won&amp;rsquo;t be able to maintain these conditions any more.&lt;/li>
&lt;li>If the gardenlet&amp;rsquo;s client certificate has expired (identified based on the &lt;code>.status.clientCertificateExpirationTimestamp&lt;/code> field in the &lt;code>Seed&lt;/code> resource) and if it is managed by a &lt;code>ManagedSeed&lt;/code> then this will be triggered for a reconciliation. This will trigger the bootstrapping process again and allows gardenlets to obtain a fresh client certificate.&lt;/li>
&lt;/ol>
&lt;h3 id="controllerregistration-controller">ControllerRegistration Controller&lt;/h3>
&lt;p>The &lt;code>ControllerRegistration&lt;/code> controller makes sure that the required &lt;a href="https://github.com/gardener/gardener/blob/master/docs/README.md#extensions">Gardener extensions&lt;/a> specified by the &lt;a href="https://gardener.cloud/docs/gardener/extensions/controllerregistration/">&lt;code>ControllerRegistration&lt;/code>&lt;/a> resources are present in the seed clusters. It also takes care of the creation and deletion of &lt;code>ControllerInstallation&lt;/code> objects for a given seed cluster.
The controller has three reconciliation loops.&lt;/p>
&lt;h4 id="main-reconciler-2">&amp;ldquo;Main&amp;rdquo; Reconciler&lt;/h4>
&lt;p>This reconciliation loop watches the &lt;code>Seed&lt;/code> objects and determines which &lt;code>ControllerRegistrations&lt;/code> are required for them and creates/deletes the corresponding extension controller to reach the determined state. To begin with, it computes the kind/type combinations of extensions required for the seed. For this, the controller examines a live list of &lt;code>ControllerRegistration&lt;/code>s, &lt;code>ControllerInstallation&lt;/code>s, &lt;code>BackupBucket&lt;/code>s, &lt;code>BackupEntry&lt;/code>s, &lt;code>Shoot&lt;/code>s, and &lt;code>Secret&lt;/code>s from the garden cluster. For example, it examines the shoots running on the seed and deducts kind/type like &lt;code>Infrastructure/gcp&lt;/code>. It also decides whether they should always be deployed based on the &lt;code>.spec.deployment.policy&lt;/code>.
For the configuration options, please see this &lt;a href="https://gardener.cloud/docs/gardener/extensions/controllerregistration/#deployment-configuration-options">section&lt;/a>.&lt;/p>
&lt;p>Based on these required combinations, each of them are mapped to &lt;code>ControllerRegistration&lt;/code> objects and then to their corresponding &lt;code>ControllerInstallation&lt;/code> objects (if existing). The controller then creates or updates the required &lt;code>ControllerInstallation&lt;/code> objects for the given seed. It also deletes every existing &lt;code>ControllerInstallation&lt;/code> whose referenced &lt;code>ControllerRegistration&lt;/code> is not part of the required list. For example, if the shoots in the seed are no longer using the DNS provider &lt;code>aws-route53&lt;/code>, then the controller proceeds to delete the respective &lt;code>ControllerInstallation&lt;/code> object.&lt;/p>
&lt;h4 id="controllerregistration-reconciler">&amp;ldquo;ControllerRegistration&amp;rdquo; Reconciler&lt;/h4>
&lt;p>This reconciliation loop watches the &lt;code>ControllerRegistration&lt;/code> resource and adds finalizers to it when they are created. In case a deletion request comes in for the resource, i.e., if a &lt;code>.metadata.deletionTimestamp&lt;/code> is set, it actively scans for a &lt;code>ControllerInstallation&lt;/code> resource using this &lt;code>ControllerRegistration&lt;/code>, and decides whether the deletion can be allowed. In case no related &lt;code>ControllerInstallation&lt;/code> is present, it removes the finalizer and marks it for deletion.&lt;/p>
&lt;h4 id="seed-reconciler">&amp;ldquo;Seed&amp;rdquo; Reconciler&lt;/h4>
&lt;p>This loop also watches the &lt;code>Seed&lt;/code> object and adds finalizers to it at creation. If a &lt;code>.metadata.deletionTimestamp&lt;/code> is set for the seed then the controller checks for existing &lt;code>ControllerInstallation&lt;/code> objects which reference this seed. If no such objects exist then it removes the finalizer and allows the deletion.&lt;/p>
&lt;h3 id="certificatesigningrequest-controller">&amp;ldquo;CertificateSigningRequest&amp;rdquo; controller&lt;/h3>
&lt;p>After the &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/">gardenlet&lt;/a> gets deployed on the Seed cluster it needs to establish itself as a trusted party to communicate with the Gardener API server. It runs through a bootstrap flow similar to the &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/">kubelet bootstrap&lt;/a> process.&lt;/p>
&lt;p>On startup the gardenlet uses a &lt;code>kubeconfig&lt;/code> with a &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/">bootstrap token&lt;/a> which authenticates it as being part of the &lt;code>system:bootstrappers&lt;/code> group. This kubeconfig is used to create a &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">&lt;code>CertificateSigningRequest&lt;/code>&lt;/a> (CSR) against the Gardener API server.&lt;/p>
&lt;p>The controller in &lt;code>gardener-controller-manager&lt;/code> checks whether the &lt;code>CertificateSigningRequest&lt;/code> has the expected organisation, common name and usages which the gardenlet would request.&lt;/p>
&lt;p>It only auto-approves the CSR if the client making the request is allowed to &amp;ldquo;create&amp;rdquo; the
&lt;code>certificatesigningrequests/seedclient&lt;/code> subresource. Clients with the &lt;code>system:bootstrappers&lt;/code> group are bound to the &lt;code>gardener.cloud:system:seed-bootstrapper&lt;/code> &lt;code>ClusterRole&lt;/code>, hence, they have such privileges. As the bootstrap kubeconfig for the gardenlet contains a bootstrap token which is authenticated as being part of the &lt;a href="https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/charts/application/templates/clusterrolebinding-seed-bootstrapper.yaml">&lt;code>systems:bootstrappers&lt;/code> group&lt;/a>, its created CSR gets auto-approved.&lt;/p>
&lt;h3 id="bastion-controller">&amp;ldquo;Bastion&amp;rdquo; Controller&lt;/h3>
&lt;p>&lt;code>Bastion&lt;/code> resources have a limited lifetime, which can be extended up to a certain amount by performing a heartbeat on
them. The &lt;code>Bastion&lt;/code> controller is responsible for deleting expired or rotten &lt;code>Bastion&lt;/code>s.&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;expired&amp;rdquo; means a &lt;code>Bastion&lt;/code> has exceeded its &lt;code>status.ExpirationTimestamp&lt;/code>.&lt;/li>
&lt;li>&amp;ldquo;rotten&amp;rdquo; means a &lt;code>Bastion&lt;/code> is older than the configured &lt;code>maxLifetime&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>The &lt;code>maxLifetime&lt;/code> is an option on the &lt;code>Bastion&lt;/code> controller and defaults to 24 hours.&lt;/p>
&lt;p>The deletion triggers the gardenlet to perform the necessary cleanups in the Seed cluster, so some time can pass between
deletion and the &lt;code>Bastion&lt;/code> actually disappearing. Clients like &lt;code>gardenctl&lt;/code> are advised to not re-use &lt;code>Bastion&lt;/code>s whose
deletion timestamp has been set already.&lt;/p>
&lt;p>Refer to &lt;a href="https://gardener.cloud/docs/gardener/proposals/15-manage-bastions-and-ssh-key-pair-rotation/">GEP-15&lt;/a> for more information on the lifecycle of
&lt;code>Bastion&lt;/code> resources.&lt;/p>
&lt;h3 id="plant-controller">&amp;ldquo;Plant&amp;rdquo; Controller&lt;/h3>
&lt;p>Using the &lt;code>Plant&lt;/code> resource, an external Kubernetes cluster (not managed by Gardener) can be registered to Gardener. Gardener Controller Manager is the component that is responsible for the &lt;code>Plant&lt;/code> resource reconciliation. As part of the reconciliation loop, the Gardener Controller Manager performs health checks on the external Kubernetes cluster and gathers more information about it - all of this information serves for monitoring purposes of the external Kubernetes cluster.&lt;/p>
&lt;p>The component configuration of the Gardener Controller Manager offers to configure the following options for the plant controller:&lt;/p>
&lt;ul>
&lt;li>&lt;code>syncPeriod&lt;/code>: The duration of how often the Plant resource is reconciled, i.e., how often health checks are performed. The default value is &lt;code>30s&lt;/code>.&lt;/li>
&lt;li>&lt;code>concurrentSyncs&lt;/code>: The number of goroutines scheduled for reconciling events, i.e., the number of possible parallel reconciliations. The default value is &lt;code>5&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>The &lt;code>Plant&lt;/code> resource reports the following information for the external Kubernetes cluster:&lt;/p>
&lt;ul>
&lt;li>Cluster information
&lt;ul>
&lt;li>Cloud provider information - the cloud provider type and region are maintained in the &lt;code>Plant&lt;/code> status (&lt;code>.status.clusterInfo.cloud&lt;/code>).&lt;/li>
&lt;li>Kubernetes version - the Kubernetes version is maintained in the &lt;code>Plant&lt;/code> status (&lt;code>.status.clusterInfo.kubernetes.version&lt;/code>).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Cluster status
&lt;ul>
&lt;li>API Server availability - maintained as condition with type &lt;code>APIServerAvailable&lt;/code>.&lt;/li>
&lt;li>Cluster &lt;code>Node&lt;/code>s healthiness - maintained as condition with type &lt;code>EveryNodeReady&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: Etc</title><link>https://gardener.cloud/docs/gardener/concepts/etcd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/etcd/</guid><description>
&lt;h1 id="etcd---key-value-store-for-kubernetes">etcd - Key-Value Store for Kubernetes&lt;/h1>
&lt;p>&lt;a href="https://etcd.io/">etcd&lt;/a> is a strongly consistent key-value store and the most prevalent choice for the Kubernetes
persistence layer. All API cluster objects like &lt;code>Pod&lt;/code>s, &lt;code>Deployment&lt;/code>s, &lt;code>Secret&lt;/code>s, etc. are stored in &lt;code>etcd&lt;/code> which
makes it an essential part of a &lt;a href="https://kubernetes.io/docs/concepts/overview/components/#control-plane-components">Kubernetes control plane&lt;/a>.&lt;/p>
&lt;h2 id="shoot-cluster-persistence">Shoot cluster persistence&lt;/h2>
&lt;p>Each shoot cluster gets its very own persistence for the control plane. It runs in the shoot namespace on the respective
seed cluster. Concretely, there are two etcd instances per shoot cluster which the &lt;code>Kube-Apiserver&lt;/code> is configured
to use in the following way:&lt;/p>
&lt;ul>
&lt;li>etcd-main&lt;/li>
&lt;/ul>
&lt;p>A store that contains all &amp;ldquo;cluster critical&amp;rdquo; or &amp;ldquo;long-term&amp;rdquo; objects. These object kinds are typically considered
for a backup to prevent any data loss.&lt;/p>
&lt;ul>
&lt;li>etcd-events&lt;/li>
&lt;/ul>
&lt;p>A store that contains all &lt;code>Event&lt;/code> objects (&lt;code>events.k8s.io&lt;/code>) of a cluster. &lt;code>Events&lt;/code> have usually a short retention
period, occur frequently but are not essential for a disaster recovery.&lt;/p>
&lt;p>The setup above prevents both, the critical &lt;code>etcd-main&lt;/code> is not flooded by Kubernetes &lt;code>Events&lt;/code> as well as backup space is
not occupied by non-critical data. This segmentation saves time and resources.&lt;/p>
&lt;h2 id="etcd-operator">etcd Operator&lt;/h2>
&lt;p>Configuring, maintaining and health-checking &lt;code>etcd&lt;/code> is outsourced to a dedicated operator called &lt;a href="https://github.com/gardener/etcd-druid/">ETCD Druid&lt;/a>.
When &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/">Gardenlet&lt;/a> reconciles a &lt;code>Shoot&lt;/code> resource, it creates or updates an &lt;a href="https://github.com/gardener/etcd-druid/blob/1d427e9167adac1476d1847c0e265c2c09d6bc62/config/samples/druid_v1alpha1_etcd.yaml">Etcd&lt;/a>
resources in the seed cluster, containing necessary information (backup information, defragmentation schedule, resources, etc.) &lt;code>etcd-druid&lt;/code>
needs to manage the lifecycle of the desired etcd instance (today &lt;code>main&lt;/code> or &lt;code>events&lt;/code>). Likewise, when the shoot is deleted,
Gardenlet deletes the &lt;code>Etcd&lt;/code> resource and &lt;a href="https://github.com/gardener/etcd-druid/">ETCD Druid&lt;/a> takes care about cleaning up
all related objects, e.g. the backing &lt;code>StatefulSet&lt;/code>.&lt;/p>
&lt;h2 id="autoscaling">Autoscaling&lt;/h2>
&lt;p>Gardenlet maintains &lt;a href="https://github.com/gardener/hvpa-controller/blob/master/config/samples/autoscaling_v1alpha1_hvpa.yaml">HVPA&lt;/a>
objects for etcd &lt;code>StatefulSet&lt;/code>s if the corresponding &lt;a href="https://gardener.cloud/docs/gardener/deployment/feature_gates/">feature gate&lt;/a> is enabled. This enables
a vertical scaling for &lt;code>etcd&lt;/code>. Downscaling is handled more pessimistic to prevent many subsequent &lt;code>etcd&lt;/code> restarts. Thus,
for &lt;code>production&lt;/code> clusters downscaling is deactivated and for all other clusters lower advertised requests/limits are only
applied during a shoot&amp;rsquo;s maintenance time window.&lt;/p>
&lt;h2 id="backup">Backup&lt;/h2>
&lt;p>If &lt;code>Seed&lt;/code>s specify backups for etcd (&lt;a href="https://github.com/gardener/gardener/blob/e9bf88a7a091a8cf8c495bef298bdada17a03c7f/example/50-seed.yaml#L19">example&lt;/a>),
then Gardener and the respective &lt;a href="https://gardener.cloud/docs/gardener/extensions/overview/">provider extensions&lt;/a> are responsible for creating a bucket
on the cloud provider&amp;rsquo;s side (modelled through &lt;a href="https://gardener.cloud/docs/gardener/extensions/backupbucket/">BackupBucket resource&lt;/a>). The bucket stores
backups of shoots scheduled on that seed. Furthermore, Gardener creates a &lt;a href="https://gardener.cloud/docs/gardener/extensions/backupentry/">BackupEntry&lt;/a>
which subdivides the bucket and thus makes it possible to store backups of multiple shoot clusters.&lt;/p>
&lt;p>The &lt;code>etcd-main&lt;/code> instance itself is configured to run with a special backup-restore &lt;em>sidecar&lt;/em>. It takes care about regularly
backing up etcd data and restoring it in case of data loss. More information can be found on the component&amp;rsquo;s GitHub
page &lt;a href="https://github.com/gardener/etcd-backup-restore">https://github.com/gardener/etcd-backup-restore&lt;/a>.&lt;/p>
&lt;p>How long backups are stored in the bucket after a shoot has been deleted, depends on the configured &lt;em>retention period&lt;/em> in the
&lt;code>Seed&lt;/code> resource. Please see this &lt;a href="https://github.com/gardener/gardener/blob/849cd857d0d20e5dde26b9740ca2814603a56dfd/example/20-componentconfig-gardenlet.yaml#L20">example configuration&lt;/a> for more information.&lt;/p>
&lt;h2 id="housekeeping">Housekeeping&lt;/h2>
&lt;p>&lt;a href="https://etcd.io/docs/v3.3/op-guide/maintenance/">etcd maintenance tasks&lt;/a> must be performed from time to time in order
to re-gain database storage and to ensure the system&amp;rsquo;s reliability. The &lt;a href="https://github.com/gardener/etcd-backup-restore">backup-restore&lt;/a>
&lt;em>sidecar&lt;/em> takes care about this job as well. Gardener chooses a random time &lt;strong>within the shoot&amp;rsquo;s maintenance time&lt;/strong> to
schedule these tasks.&lt;/p></description></item><item><title>Docs: Gardenlet</title><link>https://gardener.cloud/docs/gardener/concepts/gardenlet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/gardenlet/</guid><description>
&lt;h1 id="gardenlet">Gardenlet&lt;/h1>
&lt;p>Gardener is implemented using the &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/">operator pattern&lt;/a>:
It uses custom controllers that act on our own custom resources,
and apply Kubernetes principles to manage clusters instead of containers.
Following this analogy, you can recognize components of the Gardener architecture
as well-known Kubernetes components, for example, shoot clusters can be compared with pods,
and seed clusters can be seen as worker nodes.&lt;/p>
&lt;p>The following Gardener components play a similar role as the corresponding components
in the Kubernetes architecture:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Gardener Component&lt;/th>
&lt;th style="text-align:left">Kubernetes Component&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">&lt;code>gardener-apiserver&lt;/code>&lt;/td>
&lt;td style="text-align:left">&lt;code>kube-apiserver&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">&lt;code>gardener-controller-manager&lt;/code>&lt;/td>
&lt;td style="text-align:left">&lt;code>kube-controller-manager&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">&lt;code>gardener-scheduler&lt;/code>&lt;/td>
&lt;td style="text-align:left">&lt;code>kube-scheduler&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">&lt;code>gardenlet&lt;/code>&lt;/td>
&lt;td style="text-align:left">&lt;code>kubelet&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Similar to how the &lt;code>kube-scheduler&lt;/code> of Kubernetes finds an appropriate node
for newly created pods, the &lt;code>gardener-scheduler&lt;/code> of Gardener finds an appropriate seed cluster
to host the control plane for newly ordered clusters.
By providing multiple seed clusters for a region or provider, and distributing the workload,
Gardener also reduces the blast radius of potential issues.&lt;/p>
&lt;p>Kubernetes runs a primary &amp;ldquo;agent&amp;rdquo; on every node, the kubelet,
which is responsible for managing pods and containers on its particular node.
Decentralizing the responsibility to the kubelet has the advantage that the overall system
is scalable. Gardener achieves the same for cluster management by using a &lt;strong>gardenlet&lt;/strong>
as primary &amp;ldquo;agent&amp;rdquo; on every seed cluster, and is only responsible for shoot clusters
located in its particular seed cluster:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/gardenlet-architecture-similarities_ba8a1c.png" alt="Counterparts in the Gardener Architecture and the Kubernetes Architecture">&lt;/p>
&lt;p>The &lt;code>gardener-controller-manager&lt;/code> has control loops to manage resources of the Gardener API. However, instead of letting the &lt;code>gardener-controller-manager&lt;/code> talk directly to seed clusters or shoot clusters, the responsibility isn’t only delegated to the gardenlet, but also managed using a reversed control flow: It&amp;rsquo;s up to the gardenlet to contact the Gardener API server, for example, to share a status for its managed seed clusters.&lt;/p>
&lt;p>Reversing the control flow allows placing seed clusters or shoot clusters behind firewalls without the necessity of direct access via VPN tunnels anymore.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/gardenlet-architecture-detailed_6f3172.png" alt="Reversed Control Flow Using a Gardenlet">&lt;/p>
&lt;h2 id="tls-bootstrapping">TLS Bootstrapping&lt;/h2>
&lt;p>Kubernetes doesn’t manage worker nodes itself, and it’s also not
responsible for the lifecycle of the kubelet running on the workers.
Similarly, Gardener doesn’t manage seed clusters itself,
so Gardener is also not responsible for the lifecycle of the gardenlet running on the seeds.
As a consequence, both the gardenlet and the kubelet need to prepare
a trusted connection to the Gardener API server
and the Kubernetes API server correspondingly.&lt;/p>
&lt;p>To prepare a trusted connection between the gardenlet
and the Gardener API server, the gardenlet initializes
a bootstrapping process after you deployed it into your seed clusters:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The gardenlet starts up with a bootstrap &lt;code>kubeconfig&lt;/code>
having a bootstrap token that allows to create &lt;code>CertificateSigningRequest&lt;/code> (CSR) resources.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>After the CSR is signed, the gardenlet downloads
the created client certificate, creates a new &lt;code>kubeconfig&lt;/code> with it,
and stores it inside a &lt;code>Secret&lt;/code> in the seed cluster.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The gardenlet deletes the bootstrap &lt;code>kubeconfig&lt;/code> secret,
and starts up with its new &lt;code>kubeconfig&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The gardenlet starts normal operation.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>The &lt;code>gardener-controller-manager&lt;/code> runs a control loop
that automatically signs CSRs created by gardenlets.&lt;/p>
&lt;blockquote>
&lt;p>The gardenlet bootstrapping process is based on the
kubelet bootstrapping process. More information:
&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/">Kubelet&amp;rsquo;s TLS bootstrapping&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>If you don&amp;rsquo;t want to run this bootstrap process you can create
a &lt;code>kubeconfig&lt;/code> pointing to the garden cluster for the gardenlet yourself,
and use field &lt;code>gardenClientConnection.kubeconfig&lt;/code> in the
gardenlet configuration to share it with the gardenlet.&lt;/p>
&lt;h2 id="gardenlet-certificate-rotation">Gardenlet Certificate Rotation&lt;/h2>
&lt;p>The certificate used to authenticate the gardenlet against the API server
has a certain validity based on the configuration of the garden cluster
(&lt;code>--cluster-signing-duration&lt;/code> flag of the &lt;code>kube-controller-manager&lt;/code> (default &lt;code>1y&lt;/code>)).
After about 80% of the validity expired, the gardenlet tries to automatically replace
the current certificate with a new one (certificate rotation).&lt;/p>
&lt;p>To use certificate rotation, you need to specify the secret to store
the &lt;code>kubeconfig&lt;/code> with the rotated certificate in field
&lt;code>.gardenClientConnection.kubeconfigSecret&lt;/code> of the
gardenlet &lt;a href="#component-configuration">component configuration&lt;/a>.&lt;/p>
&lt;h3 id="rotate-certificates-using-bootstrap-kubeconfig">Rotate certificates using bootstrap &lt;code>kubeconfig&lt;/code>&lt;/h3>
&lt;p>If the gardenlet created the certificate during the initial TLS Bootstrapping
using the Bootstrap &lt;code>kubeconfig&lt;/code>, certificates can be rotated automatically.
The same control loop in the &lt;code>gardener-controller-manager&lt;/code> that signs
the CSRs during the initial TLS Bootstrapping also automatically signs
the CSR during a certificate rotation.&lt;/p>
&lt;p>ℹ️ You can trigger an immediate renewal by annotating the &lt;code>Secret&lt;/code> in the seed
cluster stated in the &lt;code>.gardenClientConnection.kubeconfigSecret&lt;/code> field with
&lt;code>gardener.cloud/operation=renew&lt;/code> and restarting the gardenlet. After it booted
up again, gardenlet will issue a new certificate independent of the remaining
validity of the existing one.&lt;/p>
&lt;h3 id="rotate-certificate-using-custom-kubeconfig">Rotate Certificate Using Custom &lt;code>kubeconfig&lt;/code>&lt;/h3>
&lt;p>When trying to rotate a custom certificate that wasn’t created by gardenlet
as part of the TLS Bootstrap, the x509 certificate&amp;rsquo;s &lt;code>Subject&lt;/code> field
needs to conform to the following:&lt;/p>
&lt;ul>
&lt;li>the Common Name (CN) is prefixed with &lt;code>gardener.cloud:system:seed:&lt;/code>&lt;/li>
&lt;li>the Organization (O) equals &lt;code>gardener.cloud:system:seeds&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Otherwise, the &lt;code>gardener-controller-manager&lt;/code> doesn’t automatically
sign the CSR.
In this case, an external component or user needs to approve the CSR manually,
for example, using command &lt;code>kubectl certificate approve seed-csr-&amp;lt;...&amp;gt;&lt;/code>).
If that doesn’t happen within 15 minutes,
the gardenlet repeats the process and creates another CSR.&lt;/p>
&lt;h2 id="configuring-the-seed-to-work-with">Configuring the Seed to work with&lt;/h2>
&lt;p>The Gardenlet works with a single seed, which must be configured in the
&lt;code>GardenletConfiguration&lt;/code> under &lt;code>.seedConfig&lt;/code>. This must be a copy of the
&lt;code>Seed&lt;/code> resource, for example (see &lt;code>example/20-componentconfig-gardenlet.yaml&lt;/code>
for a more complete example):&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">apiVersion: gardenlet.config.gardener.cloud/v1alpha1
kind: GardenletConfiguration
seedConfig:
metadata:
name: my-seed
spec:
provider:
type: aws
&lt;span style="color:#008000"># ...&lt;/span>
secretRef:
name: my-seed-secret
namespace: garden
&lt;/code>&lt;/pre>&lt;/div>&lt;p>When using &lt;code>make start-gardenlet&lt;/code>, the corresponding script will automatically
fetch the seed cluster&amp;rsquo;s &lt;code>kubeconfig&lt;/code> based on the &lt;code>seedConfig.spec.secretRef&lt;/code>
and set the environment accordingly.&lt;/p>
&lt;p>On startup, gardenlet registers a &lt;code>Seed&lt;/code> resource using the given template
in &lt;code>seedConfig&lt;/code> if it&amp;rsquo;s not present already.&lt;/p>
&lt;h2 id="component-configuration">Component Configuration&lt;/h2>
&lt;p>In the component configuration for the gardenlet, it’s possible to define:&lt;/p>
&lt;ul>
&lt;li>settings for the Kubernetes clients interacting with the various clusters&lt;/li>
&lt;li>settings for the control loops inside the gardenlet&lt;/li>
&lt;li>settings for leader election and log levels, feature gates, and seed selection or seed configuration.&lt;/li>
&lt;/ul>
&lt;p>More information: &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml">Example Gardenlet Component Configuration&lt;/a>.&lt;/p>
&lt;h2 id="heartbeats">Heartbeats&lt;/h2>
&lt;p>Similar to how Kubernetes uses &lt;code>Lease&lt;/code> objects for node heart beats
(see &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/589-efficient-node-heartbeats/README.md">KEP&lt;/a>),
the gardenlet is using &lt;code>Lease&lt;/code> objects for heart beats of the seed cluster.
Every two seconds, the gardenlet checks that the seed cluster&amp;rsquo;s &lt;code>/healthz&lt;/code>
endpoint returns HTTP status code 200.
If that is the case, the gardenlet renews the lease in the Garden cluster in the &lt;code>gardener-system-seed-lease&lt;/code> namespace and updates
the &lt;code>GardenletReady&lt;/code> condition in the &lt;code>status.conditions&lt;/code> field of the &lt;code>Seed&lt;/code> resource(s).&lt;/p>
&lt;p>Similarly to the &lt;code>node-lifecycle-controller&lt;/code> inside the &lt;code>kube-controller-manager&lt;/code>,
the &lt;code>gardener-controller-manager&lt;/code> features a &lt;code>seed-lifecycle-controller&lt;/code> that sets
the &lt;code>GardenletReady&lt;/code> condition to &lt;code>Unknown&lt;/code> in case the gardenlet fails to renew the lease.
As a consequence, the &lt;code>gardener-scheduler&lt;/code> doesn’t consider this seed cluster for newly created shoot clusters anymore.&lt;/p>
&lt;h3 id="healthz-endpoint">&lt;code>/healthz&lt;/code> Endpoint&lt;/h3>
&lt;p>The gardenlet includes an HTTPS server that serves a &lt;code>/healthz&lt;/code> endpoint.
It’s used as a liveness probe in the &lt;code>Deployment&lt;/code> of the gardenlet.
If the gardenlet fails to renew its lease
then the endpoint returns &lt;code>500 Internal Server Error&lt;/code>, otherwise it returns &lt;code>200 OK&lt;/code>.&lt;/p>
&lt;p>Please note that the &lt;code>/healthz&lt;/code> only indicates whether the gardenlet
could successfully probe the Seed&amp;rsquo;s API server and renew the lease with
the Garden cluster.
It does &lt;em>not&lt;/em> show that the Gardener extension API server (with the Gardener resource groups)
is available.
However, the Gardenlet is designed to withstand such connection outages and
retries until the connection is reestablished.&lt;/p>
&lt;h2 id="control-loops">Control Loops&lt;/h2>
&lt;p>The gardenlet consists out of several controllers which are now described in more detail.&lt;/p>
&lt;p>⚠️ This section is not necessarily complete and might be under construction.&lt;/p>
&lt;h3 id="backupentry-controller">&lt;code>BackupEntry&lt;/code> Controller&lt;/h3>
&lt;p>The &lt;code>BackupEntry&lt;/code> controller reconciles those &lt;code>core.gardener.cloud/v1beta1.BackupEntry&lt;/code> resources whose &lt;code>.spec.seedName&lt;/code> value is equal to the name of a &lt;code>Seed&lt;/code> the respective gardenlet is responsible for.
Those resources are created by the &lt;code>Shoot&lt;/code> controller (only if backup is enabled for the respective &lt;code>Seed&lt;/code>) and there is exactly one &lt;code>BackupEntry&lt;/code> per &lt;code>Shoot&lt;/code>.&lt;/p>
&lt;p>The controller creates an &lt;code>extensions.gardener.cloud/v1alpha1.BackupEntry&lt;/code> resource (non-namespaced) in the seed cluster and waits until the responsible extension controller reconciled it (see &lt;a href="https://gardener.cloud/docs/gardener/extensions/backupentry/">this&lt;/a> for more details).
The status is populated in the &lt;code>.status.lastOperation&lt;/code> field.&lt;/p>
&lt;p>The &lt;code>core.gardener.cloud/v1beta1.BackupEntry&lt;/code> resource has an owner reference pointing to the corresponding &lt;code>Shoot&lt;/code>.
Hence, if the &lt;code>Shoot&lt;/code> is deleted, also the &lt;code>BackupEntry&lt;/code> resource gets deleted.
In this case, the controller deletes the &lt;code>extensions.gardener.cloud/v1alpha1.BackupEntry&lt;/code> resource in the seed cluster and waits until the responsible extension controller has deleted it.
Afterwards, the finalizer of the &lt;code>core.gardener.cloud/v1beta1.BackupEntry&lt;/code> resource is released so that it finally disappears from the system.&lt;/p>
&lt;h4 id="keep-backup-for-deleted-shoots">Keep Backup for Deleted Shoots&lt;/h4>
&lt;p>In some scenarios it might be beneficial to not immediately delete the &lt;code>BackupEntry&lt;/code>s (and with them, the etcd backup) for deleted &lt;code>Shoot&lt;/code>s.&lt;/p>
&lt;p>In this case you can configure the &lt;code>.controllers.backupEntry.deletionGracePeriodHours&lt;/code> field in the component configuration of the gardenlet.
For example, if you set it to &lt;code>48&lt;/code>, then the &lt;code>BackupEntry&lt;/code>s for deleted &lt;code>Shoot&lt;/code>s will only be deleted &lt;code>48&lt;/code> hours after the &lt;code>Shoot&lt;/code> was deleted.&lt;/p>
&lt;p>Additionally, you can limit the &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_purposes/">shoot purposes&lt;/a> for which this applies by setting &lt;code>.controllers.backupEntry.deletionGracePeriodShootPurposes[]&lt;/code>.
For example, if you set it to &lt;code>[production]&lt;/code> then only the &lt;code>BackupEntry&lt;/code>s for &lt;code>Shoot&lt;/code>s with &lt;code>.spec.purpose=production&lt;/code> will be deleted after the configured grace period. All others will be deleted immediately after the &lt;code>Shoot&lt;/code> deletion.&lt;/p>
&lt;h2 id="managed-seeds">Managed Seeds&lt;/h2>
&lt;p>Gardener users can use shoot clusters as seed clusters, so-called &amp;ldquo;managed seeds&amp;rdquo; (aka &amp;ldquo;shooted seeds&amp;rdquo;),
by creating &lt;code>ManagedSeed&lt;/code> resources.
By default, the gardenlet that manages this shoot cluster then automatically
creates a clone of itself with the same version and the same configuration
that it currently has.
Then it deploys the gardenlet clone into the managed seed cluster.&lt;/p>
&lt;p>If you want to prevent the automatic gardenlet deployment,
specify the &lt;code>seedTemplate&lt;/code> section in the &lt;code>ManagedSeed&lt;/code> resource, and don&amp;rsquo;t specify
the &lt;code>gardenlet&lt;/code> section.
In this case, you have to deploy the gardenlet on your own into the seed cluster.&lt;/p>
&lt;p>More information: &lt;a href="https://gardener.cloud/docs/gardener/usage/managed_seed/">Register Shoot as Seed&lt;/a>&lt;/p>
&lt;h2 id="migrating-from-previous-gardener-versions">Migrating from Previous Gardener Versions&lt;/h2>
&lt;p>If your Gardener version doesn’t support gardenlets yet,
no special migration is required, but the following prerequisites must be met:&lt;/p>
&lt;ul>
&lt;li>Your Gardener version is at least 0.31 before upgrading to v1.&lt;/li>
&lt;li>You have to make sure that your garden cluster is exposed in a way
that it’s reachable from all your seed clusters.&lt;/li>
&lt;/ul>
&lt;p>With previous Gardener versions, you had deployed the Gardener Helm chart
(incorporating the API server, &lt;code>controller-manager&lt;/code>, and scheduler).
With v1, this stays the same, but you now have to deploy the gardenlet Helm chart as well
into all of your seeds (if they aren’t managed, as mentioned earlier).&lt;/p>
&lt;p>More information: &lt;a href="https://gardener.cloud/docs/gardener/deployment/deploy_gardenlet/">Deploy a Gardenlet&lt;/a> for all instructions.&lt;/p>
&lt;h2 id="related-links">Related Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/gardener/documentation/wiki/Architecture">Gardener Architecture&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/gardener/gardener/issues/356">Issue #356: Implement Gardener Scheduler&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/gardener/gardener/pull/2309">PR #2309: Add /healthz endpoint for Gardenlet&lt;/a>&lt;/p></description></item><item><title>Docs: Network Policies</title><link>https://gardener.cloud/docs/gardener/concepts/network_policies/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/network_policies/</guid><description>
&lt;h1 id="network-policies-in-gardener">Network Policies in Gardener&lt;/h1>
&lt;p>As &lt;code>Seed&lt;/code> clusters can host the &lt;a href="https://kubernetes.io/docs/concepts/#kubernetes-control-plane">Kubernetes control planes&lt;/a> of many &lt;code>Shoot&lt;/code> clusters, it is necessary to isolate the control planes from each other for security reasons.
Besides deploying each control plane in its own namespace, Gardener creates &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">network policies&lt;/a> to also isolate the networks.
Essentially, network policies make sure that pods can only talk to other pods over the network they are supposed to.
As such, network policies are an important part of Gardener&amp;rsquo;s tenant isolation.&lt;/p>
&lt;p>Gardener deploys network policies into&lt;/p>
&lt;ul>
&lt;li>each namespace hosting the Kubernetes control plane of the Shoot cluster.&lt;/li>
&lt;li>the namespace dedicated to Gardener seed-wide global controllers. This namespace is often called &lt;code>garden&lt;/code> and contains e.g. the &lt;a href="https://github.com/gardener/gardener/blob/15cae57db802cbe460ff4cb3f80c26b2fc15e26f/docs/concepts/gardenlet.md">Gardenlet&lt;/a>.&lt;/li>
&lt;li>the &lt;code>kube-system&lt;/code> namespace in the Shoot.&lt;/li>
&lt;/ul>
&lt;p>The aforementioned namespaces in the Seed contain a &lt;code>deny-all&lt;/code> network policy that &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-and-all-egress-traffic">denies all ingress and egress traffic&lt;/a>.
This &lt;a href="https://en.wikipedia.org/wiki/Secure_by_default">secure by default&lt;/a> setting requires pods to allow network traffic.
This is done by pods having &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#networkpolicy-resource">labels matching to the selectors of the network policies&lt;/a> deployed by Gardener.&lt;/p>
&lt;p>More details on the deployed network policies can be found in the &lt;a href="https://github.com/gardener/gardener/tree/master/docs/development/seed_network_policies.md">development&lt;/a> and &lt;a href="https://github.com/gardener/gardener/tree/master/docs/usage/shoot_network_policies.md">usage&lt;/a> sections.&lt;/p></description></item><item><title>Docs: Resource Manager</title><link>https://gardener.cloud/docs/gardener/concepts/resource-manager/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/resource-manager/</guid><description>
&lt;h1 id="gardener-resource-manager">Gardener Resource Manager&lt;/h1>
&lt;p>Initially, the gardener-resource-manager was a project similar to the &lt;a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager">kube-addon-manager&lt;/a>.
It manages Kubernetes resources in a target cluster which means that it creates, updates, and deletes them.
Also, it makes sure that manual modifications to these resources are reconciled back to the desired state.&lt;/p>
&lt;p>In the Gardener project we were using the kube-addon-manager since more than two years.
While we have progressed with our &lt;a href="https://gardener.cloud/docs/gardener/proposals/01-extensibility/">extensibility story&lt;/a> (moving cloud providers out-of-tree) we had decided that the kube-addon-manager is no longer suitable for this use-case.
The problem with it is that it needs to have its managed resources on its file system.
This requires storing the resources in &lt;code>ConfigMap&lt;/code>s or &lt;code>Secret&lt;/code>s and mounting them to the kube-addon-manager pod during deployment time.
The gardener-resource-manager uses &lt;code>CustomResourceDefinition&lt;/code>s which allows to dynamically add, change, and remove resources with immediate action and without the need to reconfigure the volume mounts/restarting the pod.&lt;/p>
&lt;p>Meanwhile, the &lt;code>gardener-resource-manager&lt;/code> has evolved to a more generic component comprising several controllers and webhook handlers.
It is deployed by gardenlet once per seed (in the &lt;code>garden&lt;/code> namespace) and once per shoot (in the respective shoot namespaces in the seed).&lt;/p>
&lt;h2 id="controllers">Controllers&lt;/h2>
&lt;h3 id="managedresource-controller">&lt;code>ManagedResource&lt;/code> controller&lt;/h3>
&lt;p>This controller watches custom objects called &lt;code>ManagedResource&lt;/code>s in the &lt;code>resources.gardener.cloud/v1alpha1&lt;/code> API group.
These objects contain references to secrets which itself contain the resources to be managed.
The reason why a &lt;code>Secret&lt;/code> is used to store the resources is that they could contain confidential information like credentials.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">---
apiVersion: v1
kind: Secret
metadata:
name: managedresource-example1
namespace: default
type: Opaque
data:
objects.yaml: YXBpVmVyc2lvbjogdjEKa2luZDogQ29uZmlnTWFwCm1ldGFkYXRhOgogIG5hbWU6IHRlc3QtMTIzNAogIG5hbWVzcGFjZTogZGVmYXVsdAotLS0KYXBpVmVyc2lvbjogdjEKa2luZDogQ29uZmlnTWFwCm1ldGFkYXRhOgogIG5hbWU6IHRlc3QtNTY3OAogIG5hbWVzcGFjZTogZGVmYXVsdAo=
&lt;span style="color:#008000"># apiVersion: v1&lt;/span>
&lt;span style="color:#008000"># kind: ConfigMap&lt;/span>
&lt;span style="color:#008000"># metadata:&lt;/span>
&lt;span style="color:#008000"># name: test-1234&lt;/span>
&lt;span style="color:#008000"># namespace: default&lt;/span>
&lt;span style="color:#008000"># ---&lt;/span>
&lt;span style="color:#008000"># apiVersion: v1&lt;/span>
&lt;span style="color:#008000"># kind: ConfigMap&lt;/span>
&lt;span style="color:#008000"># metadata:&lt;/span>
&lt;span style="color:#008000"># name: test-5678&lt;/span>
&lt;span style="color:#008000"># namespace: default&lt;/span>
---
apiVersion: resources.gardener.cloud/v1alpha1
kind: ManagedResource
metadata:
name: example
namespace: default
spec:
secretRefs:
- name: managedresource-example1
&lt;/code>&lt;/pre>&lt;/div>&lt;p>In the above example, the controller creates two &lt;code>ConfigMap&lt;/code>s in the &lt;code>default&lt;/code> namespace.
When a user is manually modifying them they will be reconciled back to the desired state stored in the &lt;code>managedresource-example&lt;/code> secret.&lt;/p>
&lt;p>It is also possible to inject labels into all the resources:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">---
apiVersion: v1
kind: Secret
metadata:
name: managedresource-example2
namespace: default
type: Opaque
data:
other-objects.yaml: YXBpVmVyc2lvbjogYXBwcy92MSAjIGZvciB2ZXJzaW9ucyBiZWZvcmUgMS45LjAgdXNlIGFwcHMvdjFiZXRhMgpraW5kOiBEZXBsb3ltZW50Cm1ldGFkYXRhOgogIG5hbWU6IG5naW54LWRlcGxveW1lbnQKc3BlYzoKICBzZWxlY3RvcjoKICAgIG1hdGNoTGFiZWxzOgogICAgICBhcHA6IG5naW54CiAgcmVwbGljYXM6IDIgIyB0ZWxscyBkZXBsb3ltZW50IHRvIHJ1biAyIHBvZHMgbWF0Y2hpbmcgdGhlIHRlbXBsYXRlCiAgdGVtcGxhdGU6CiAgICBtZXRhZGF0YToKICAgICAgbGFiZWxzOgogICAgICAgIGFwcDogbmdpbngKICAgIHNwZWM6CiAgICAgIGNvbnRhaW5lcnM6CiAgICAgIC0gbmFtZTogbmdpbngKICAgICAgICBpbWFnZTogbmdpbng6MS43LjkKICAgICAgICBwb3J0czoKICAgICAgICAtIGNvbnRhaW5lclBvcnQ6IDgwCg==
&lt;span style="color:#008000"># apiVersion: apps/v1&lt;/span>
&lt;span style="color:#008000"># kind: Deployment&lt;/span>
&lt;span style="color:#008000"># metadata:&lt;/span>
&lt;span style="color:#008000"># name: nginx-deployment&lt;/span>
&lt;span style="color:#008000"># spec:&lt;/span>
&lt;span style="color:#008000"># selector:&lt;/span>
&lt;span style="color:#008000"># matchLabels:&lt;/span>
&lt;span style="color:#008000"># app: nginx&lt;/span>
&lt;span style="color:#008000"># replicas: 2 # tells deployment to run 2 pods matching the template&lt;/span>
&lt;span style="color:#008000"># template:&lt;/span>
&lt;span style="color:#008000"># metadata:&lt;/span>
&lt;span style="color:#008000"># labels:&lt;/span>
&lt;span style="color:#008000"># app: nginx&lt;/span>
&lt;span style="color:#008000"># spec:&lt;/span>
&lt;span style="color:#008000"># containers:&lt;/span>
&lt;span style="color:#008000"># - name: nginx&lt;/span>
&lt;span style="color:#008000"># image: nginx:1.7.9&lt;/span>
&lt;span style="color:#008000"># ports:&lt;/span>
&lt;span style="color:#008000"># - containerPort: 80&lt;/span>
---
apiVersion: resources.gardener.cloud/v1alpha1
kind: ManagedResource
metadata:
name: example
namespace: default
spec:
secretRefs:
- name: managedresource-example2
injectLabels:
foo: bar
&lt;/code>&lt;/pre>&lt;/div>&lt;p>In this example the label &lt;code>foo=bar&lt;/code> will be injected into the &lt;code>Deployment&lt;/code> as well as into all created &lt;code>ReplicaSet&lt;/code>s and &lt;code>Pod&lt;/code>s.&lt;/p>
&lt;h4 id="preventing-reconciliations">Preventing Reconciliations&lt;/h4>
&lt;p>If a ManagedResource is annotated with &lt;code>resources.gardener.cloud/ignore=true&lt;/code> then it will be skipped entirely by the controller (no reconciliations or deletions of managed resources at all).
However, when the ManagedResource itself is deleted (for example when a shoot is deleted) then the annotation is not respected and all resources will be deleted as usual.
This feature can be helpful to temporarily patch/change resources managed as part of such ManagedResource.&lt;/p>
&lt;h4 id="modes">Modes&lt;/h4>
&lt;p>The gardener-resource-manager can manage a resource in different modes. The supported modes are:&lt;/p>
&lt;ul>
&lt;li>&lt;code>Ignore&lt;/code>
&lt;ul>
&lt;li>The corresponding resource is removed from the ManagedResource status (&lt;code>.status.resources&lt;/code>). No action is performed on the cluster - the resource is no longer &amp;ldquo;managed&amp;rdquo; (updated or deleted).&lt;/li>
&lt;li>The primary use case is a migration of a resource from one ManagedResource to another one.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>The mode for a resource can be specified with the &lt;code>resources.gardener.cloud/mode&lt;/code> annotation. The annotation should be specified in the encoded resource manifest in the Secret that is referenced by the ManagedResource.&lt;/p>
&lt;h4 id="resource-class">Resource Class&lt;/h4>
&lt;p>By default, gardener-resource-manager controller watches for ManagedResources in all namespaces. &lt;code>--namespace&lt;/code> flag can be specified to gardener-resource-manager binary to restrict the watch to ManagedResources in a single namespace.
A ManagedResource has an optional &lt;code>.spec.class&lt;/code> field that allows to indicate that it belongs to given class of resources. &lt;code>--resource-class&lt;/code> flag can be specified to gardener-resource-manager binary to restrict the watch to ManagedResources with the given &lt;code>.spec.class&lt;/code>. A default class is assumed if no class is specified.&lt;/p>
&lt;h4 id="conditions">Conditions&lt;/h4>
&lt;p>A ManagedResource has a ManagedResourceStatus, which has an array of Conditions. Conditions currently include:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Condition&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>ResourcesApplied&lt;/code>&lt;/td>
&lt;td>&lt;code>True&lt;/code> if all resources are applied to the target cluster&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>ResourcesHealthy&lt;/code>&lt;/td>
&lt;td>&lt;code>True&lt;/code> if all resources are present and healthy&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;code>ResourcesApplied&lt;/code> may be &lt;code>False&lt;/code> when:&lt;/p>
&lt;ul>
&lt;li>the resource &lt;code>apiVersion&lt;/code> is not known to the target cluster&lt;/li>
&lt;li>the resource spec is invalid (for example the label value does not match the required regex for it)&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>&lt;code>ResourcesHealthy&lt;/code> may be &lt;code>False&lt;/code> when:&lt;/p>
&lt;ul>
&lt;li>the resource is not found&lt;/li>
&lt;li>the resource is a Deployment and the Deployment does not have the minimum availability.&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>Each Kubernetes resources has different notion for being healthy. For example, a Deployment is considered healthy if the controller observed its current revision and if the number of updated replicas is equal to the number of replicas.&lt;/p>
&lt;p>The following section describes a healthy ManagedResource:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">&lt;span style="color:#a31515">&amp;#34;conditions&amp;#34;&lt;/span>&lt;span style="">:&lt;/span> [
{
&amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;ResourcesApplied&amp;#34;&lt;/span>,
&amp;#34;status&amp;#34;: &lt;span style="color:#a31515">&amp;#34;True&amp;#34;&lt;/span>,
&amp;#34;reason&amp;#34;: &lt;span style="color:#a31515">&amp;#34;ApplySucceeded&amp;#34;&lt;/span>,
&amp;#34;message&amp;#34;: &lt;span style="color:#a31515">&amp;#34;All resources are applied.&amp;#34;&lt;/span>,
&amp;#34;lastUpdateTime&amp;#34;: &lt;span style="color:#a31515">&amp;#34;2019-09-09T11:31:21Z&amp;#34;&lt;/span>,
&amp;#34;lastTransitionTime&amp;#34;: &lt;span style="color:#a31515">&amp;#34;2019-09-08T19:53:23Z&amp;#34;&lt;/span>
},
{
&amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;ResourcesHealthy&amp;#34;&lt;/span>,
&amp;#34;status&amp;#34;: &lt;span style="color:#a31515">&amp;#34;True&amp;#34;&lt;/span>,
&amp;#34;reason&amp;#34;: &lt;span style="color:#a31515">&amp;#34;ResourcesHealthy&amp;#34;&lt;/span>,
&amp;#34;message&amp;#34;: &lt;span style="color:#a31515">&amp;#34;All resources are healthy.&amp;#34;&lt;/span>,
&amp;#34;lastUpdateTime&amp;#34;: &lt;span style="color:#a31515">&amp;#34;2019-09-09T11:31:21Z&amp;#34;&lt;/span>,
&amp;#34;lastTransitionTime&amp;#34;: &lt;span style="color:#a31515">&amp;#34;2019-09-09T11:31:21Z&amp;#34;&lt;/span>
}
]
&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="ignoring-updates">Ignoring Updates&lt;/h4>
&lt;p>In some cases it is not desirable to update or re-apply some of the cluster components (for example, if customization is required or needs to be applied by the end-user).
For these resources, the annotation &amp;ldquo;resources.gardener.cloud/ignore&amp;rdquo; needs to be set to &amp;ldquo;true&amp;rdquo; or a truthy value (Truthy values are &amp;ldquo;1&amp;rdquo;, &amp;ldquo;t&amp;rdquo;, &amp;ldquo;T&amp;rdquo;, &amp;ldquo;true&amp;rdquo;, &amp;ldquo;TRUE&amp;rdquo;, &amp;ldquo;True&amp;rdquo;) in the corresponding managed resource secrets,
this can be done from the components that create the managed resource secrets, for example Gardener extensions or Gardener. Once this is done, the resource will be initially created and later ignored during reconciliation.&lt;/p>
&lt;h4 id="preserving-replicas-or-resources-in-workload-resources">Preserving &lt;code>replicas&lt;/code> or &lt;code>resources&lt;/code> in Workload Resources&lt;/h4>
&lt;p>The objects which are part of the &lt;code>ManagedResource&lt;/code> can be annotated with&lt;/p>
&lt;ul>
&lt;li>&lt;code>resources.gardener.cloud/preserve-replicas=true&lt;/code> in case the &lt;code>.spec.replicas&lt;/code> field of workload resources like &lt;code>Deployment&lt;/code>s, &lt;code>StatefulSet&lt;/code>s, etc. shall be preserved during updates.&lt;/li>
&lt;li>&lt;code>resources.gardener.cloud/preserve-resources=true&lt;/code> in case the &lt;code>.spec.containers[*].resources&lt;/code> fields of all containers of workload resources like &lt;code>Deployment&lt;/code>s, &lt;code>StatefulSet&lt;/code>s, etc. shall be preserved during updates.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>This can be useful if there are non-standard horizontal/vertical auto-scaling mechanisms in place.
Standard mechanisms like &lt;code>HorizontalPodAutoscaler&lt;/code> or &lt;code>VerticalPodAutoscaler&lt;/code> will be auto-recognized by &lt;code>gardener-resource-manager&lt;/code>, i.e., in such cases the annotations are not needed.&lt;/p>
&lt;/blockquote>
&lt;h4 id="origin">Origin&lt;/h4>
&lt;p>All the objects managed by the resource manager get a dedicated annotation
&lt;code>resources.gardener.cloud/origin&lt;/code> describing the &lt;code>ManagedResource&lt;/code> object that describes
this object.&lt;/p>
&lt;p>By default this is in this format &amp;lt;namespace&amp;gt;/&amp;lt;objectname&amp;gt;.
In multi-cluster scenarios (the &lt;code>ManagedResource&lt;/code> objects are maintained in a
cluster different from the one the described objects are managed), it might
be useful to include the cluster identity, as well.&lt;/p>
&lt;p>This can be enforced by setting the &lt;code>--cluster-id&lt;/code> option. Here, several
possibilities are supported:&lt;/p>
&lt;ul>
&lt;li>given a direct value: use this as id for the source cluster&lt;/li>
&lt;li>&lt;code>&amp;lt;cluster&amp;gt;&lt;/code>: read the cluster identity from a &lt;code>cluster-identity&lt;/code> config map
in the &lt;code>kube-system&lt;/code> namespace (attribute &lt;code>cluster-identity&lt;/code>). This is
automatically maintained in all clusters managed or involved in a gardener landscape.&lt;/li>
&lt;li>&lt;code>&amp;lt;default&amp;gt;&lt;/code>: try to read the cluster identity from the config map. If not found,
no identity is used&lt;/li>
&lt;li>empty string: no cluster identity is used (completely cluster local scenarios)&lt;/li>
&lt;/ul>
&lt;p>The format of the origin annotation with a cluster id is &amp;lt;cluster id&amp;gt;:&amp;lt;namespace&amp;gt;/&amp;lt;objectname&amp;gt;.&lt;/p>
&lt;p>The default for the cluster id is the empty value (do not use cluster id).&lt;/p>
&lt;h3 id="garbage-collector-for-immutable-configmapssecrets">Garbage Collector For Immutable &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s&lt;/h3>
&lt;p>In Kubernetes, workload resources (e.g., &lt;code>Pod&lt;/code>s) can mount &lt;code>ConfigMap&lt;/code>s or &lt;code>Secret&lt;/code>s or reference them via environment variables in containers.
Typically, when the content of such &lt;code>ConfigMap&lt;/code>/&lt;code>Secret&lt;/code> gets changed then the respective workload is usually not dynamically reloading the configuration, i.e., a restart is required.
The most commonly used approach is probably having so-called &lt;a href="https://helm.sh/docs/howto/charts_tips_and_tricks/#automatically-roll-deployments">checksum annotations in the pod template&lt;/a> which makes Kubernetes to recreate the pod if the checksum changes.
However, it has the downside that old, still running versions of the workload might not be able to properly work with the already updated content in the &lt;code>ConfigMap&lt;/code>/&lt;code>Secret&lt;/code>, potentially causing application outages.&lt;/p>
&lt;p>In order to protect users from such outages (and to also improve the performance of the cluster), the Kubernetes community provides the &lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/#configmap-immutable">&amp;ldquo;immutable &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s feature&amp;rdquo;&lt;/a>.
Enabling immutability requires &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s to have unique names.
Having unique names requires the client to delete &lt;code>ConfigMap&lt;/code>s&lt;code>/&lt;/code>Secret`s no longer in use.&lt;/p>
&lt;p>In order to provide a similarly lightweight experience for clients (compared to the well-established checksum annotation approach), the Gardener Resource Manager features an optional garbage collector controller (disabled by default).
The purpose of this controller is cleaning up such immutable &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s if they are no longer in use.&lt;/p>
&lt;h4 id="how-does-the-garbage-collector-work">How does the garbage collector work?&lt;/h4>
&lt;p>The following algorithm is implemented in the GC controller:&lt;/p>
&lt;ol>
&lt;li>List all &lt;code>ConfigMap&lt;/code>s and &lt;code>Secret&lt;/code>s labeled with &lt;code>resources.gardener.cloud/garbage-collectable-reference=true&lt;/code>.&lt;/li>
&lt;li>List all &lt;code>Deployment&lt;/code>s, &lt;code>StatefulSet&lt;/code>s, &lt;code>DaemonSet&lt;/code>s, &lt;code>Job&lt;/code>s, &lt;code>CronJob&lt;/code>s, &lt;code>Pod&lt;/code>s and for each of them
&lt;ol>
&lt;li>iterate over the &lt;code>.metadata.annotations&lt;/code> and for each of them
&lt;ol>
&lt;li>If the annotation key follows the &lt;code>reference.resources.gardener.cloud/{configmap,secret}-&amp;lt;hash&amp;gt;&lt;/code> scheme and the value equals &lt;code>&amp;lt;name&amp;gt;&lt;/code> then consider it as &amp;ldquo;in-use&amp;rdquo;.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Delete all &lt;code>ConfigMap&lt;/code>s and &lt;code>Secret&lt;/code>s not considered as &amp;ldquo;in-use&amp;rdquo;.&lt;/li>
&lt;/ol>
&lt;p>Consequently, clients need to&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Create immutable &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s with unique names (e.g., a checksum suffix based on the &lt;code>.data&lt;/code>).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Label such &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s with &lt;code>resources.gardener.cloud/garbage-collectable-reference=true&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Annotate their workload resources with &lt;code>reference.resources.gardener.cloud/{configmap,secret}-&amp;lt;hash&amp;gt;=&amp;lt;name&amp;gt;&lt;/code> for all &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s used by the containers of the respective &lt;code>Pod&lt;/code>s.&lt;/p>
&lt;p>⚠️ Add such annotations to &lt;code>.metadata.annotations&lt;/code> as well as to all templates of other resources (e.g., &lt;code>.spec.template.metadata.annotations&lt;/code> in &lt;code>Deployment&lt;/code>s or &lt;code>.spec.jobTemplate.metadata.annotations&lt;/code> and &lt;code>.spec.jobTemplate.spec.template.metadata.annotations&lt;/code> for &lt;code>CronJob&lt;/code>s.
This ensures that the GC controller does not unintentionally consider &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s as &amp;ldquo;not in use&amp;rdquo; just because there isn&amp;rsquo;t a &lt;code>Pod&lt;/code> referencing them anymore (e.g., they could still be used by a &lt;code>Deployment&lt;/code> scaled down to &lt;code>0&lt;/code>).&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>ℹ️ For the last step, there is a helper function &lt;code>InjectAnnotations&lt;/code> in the &lt;code>pkg/controller/garbagecollector/references&lt;/code> which you can use for your convenience.&lt;/p>
&lt;p>&lt;strong>Example:&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">---
apiVersion: v1
kind: ConfigMap
metadata:
name: test-1234
namespace: default
labels:
resources.gardener.cloud/garbage-collectable-reference: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
---
apiVersion: v1
kind: ConfigMap
metadata:
name: test-5678
namespace: default
labels:
resources.gardener.cloud/garbage-collectable-reference: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
---
apiVersion: v1
kind: Pod
metadata:
name: example
namespace: default
annotations:
reference.resources.gardener.cloud/configmap-82a3537f: test-5678
spec:
containers:
- name: nginx
image: nginx:1.14.2
terminationGracePeriodSeconds: 2
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The GC controller would delete the &lt;code>ConfigMap/test-1234&lt;/code> because it is considered as not &amp;ldquo;in-use&amp;rdquo;.&lt;/p>
&lt;p>ℹ️ If the GC controller is activated then the &lt;code>ManagedResource&lt;/code> controller will no longer delete &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s having the above label.&lt;/p>
&lt;h4 id="how-to-activate-the-garbage-collector">How to activate the garbage collector?&lt;/h4>
&lt;p>The GC controller can be activated by providing the &lt;code>--garbage-collector-sync-period&lt;/code> flag with a value larger than &lt;code>0&lt;/code> (e.g., &lt;code>1h&lt;/code>) to the Gardener Resource Manager.&lt;/p>
&lt;h3 id="tokeninvalidator">TokenInvalidator&lt;/h3>
&lt;p>The Kubernetes community is slowly transitioning from static &lt;code>ServiceAccount&lt;/code> token &lt;code>Secret&lt;/code>s to &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection">&lt;code>ServiceAccount&lt;/code> Token Volume Projection&lt;/a>.
Typically, when you create a &lt;code>ServiceAccount&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: ServiceAccount
metadata:
name: default
&lt;/code>&lt;/pre>&lt;/div>&lt;p>then the &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/serviceaccount/tokens_controller.go">&lt;code>serviceaccount-token&lt;/code>&lt;/a> controller (part of &lt;code>kube-controller-manager&lt;/code>) auto-generates a &lt;code>Secret&lt;/code> with a static token:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Secret
metadata:
annotations:
kubernetes.io/service-account.name: default
kubernetes.io/service-account.uid: 86e98645-2e05-11e9-863a-b2d4d086dd5a)
name: default-token-ntxs9
type: kubernetes.io/service-account-token
data:
ca.crt: base64(cluster-ca-cert)
namespace: base64(namespace)
token: base64(static-jwt-token)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Unfortunately, when using &lt;code>ServiceAccount&lt;/code> Token Volume Projection in a &lt;code>Pod&lt;/code>, this static token is actually not used at all:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
name: nginx
spec:
serviceAccountName: default
containers:
- image: nginx
name: nginx
volumeMounts:
- mountPath: /var/run/secrets/tokens
name: token
volumes:
- name: token
projected:
sources:
- serviceAccountToken:
path: token
expirationSeconds: 7200
&lt;/code>&lt;/pre>&lt;/div>&lt;p>While the &lt;code>Pod&lt;/code> is now using an expiring and auto-rotated token, the static token is still generated and valid.&lt;/p>
&lt;p>As of Kubernetes v1.22, there is neither a way of preventing &lt;code>kube-controller-manager&lt;/code> to generate such static tokens, nor a way to proactively remove or invalidate them:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/77599">https://github.com/kubernetes/kubernetes/issues/77599&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/77600">https://github.com/kubernetes/kubernetes/issues/77600&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Disabling the &lt;code>serviceaccount-token&lt;/code> controller is an option, however, especially in the Gardener context it may either break end-users or it may not even be possible to control such settings.
Also, even if a future Kubernetes version supports native configuration of above behaviour, Gardener still supports older versions which won&amp;rsquo;t get such features but need a solution as well.&lt;/p>
&lt;p>This is where the &lt;em>TokenInvalidator&lt;/em> comes into play:
Since it is not possible to prevent &lt;code>kube-controller-manager&lt;/code> from generating static &lt;code>ServiceAccount&lt;/code> &lt;code>Secret&lt;/code>s, the &lt;em>TokenInvalidator&lt;/em> is - as its name suggests - just invalidating these tokens.
It considers all such &lt;code>Secret&lt;/code>s belonging to &lt;code>ServiceAccount&lt;/code>s with &lt;code>.automountServiceAccountToken=false&lt;/code>.
By default, all namespaces in the target cluster are watched, however, this can be configured by specifying the &lt;code>--target-namespace&lt;/code> flag.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: ServiceAccount
metadata:
name: my-serviceaccount
automountServiceAccountToken: &lt;span style="color:#00f">false&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will result in a static &lt;code>ServiceAccount&lt;/code> token secret whose &lt;code>token&lt;/code> value is invalid:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Secret
metadata:
annotations:
kubernetes.io/service-account.name: my-serviceaccount
kubernetes.io/service-account.uid: 86e98645-2e05-11e9-863a-b2d4d086dd5a
name: my-serviceaccount-token-ntxs9
type: kubernetes.io/service-account-token
data:
ca.crt: base64(cluster-ca-cert)
namespace: base64(namespace)
token: AAAA
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Any attempt to regenerate the token or creating a new such secret will again make the component invalidating it.&lt;/p>
&lt;blockquote>
&lt;p>You can opt-out of this behaviour for &lt;code>ServiceAccount&lt;/code>s setting &lt;code>.automountServiceAccountToken=false&lt;/code> by labeling them with &lt;code>token-invalidator.resources.gardener.cloud/skip=true&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>In order to enable the &lt;em>TokenInvalidator&lt;/em> you have to set &lt;code>--token-invalidator-max-concurrent-workers&lt;/code> to a value larger than &lt;code>0&lt;/code>.&lt;/p>
&lt;p>Below graphic shows an overview of the Token Invalidator for Service account secrets in the Shoot cluster.
&lt;img src="https://gardener.cloud/__resources/resource-manager-token-invalidator_b43fa2.jpg" alt="image">&lt;/p>
&lt;h3 id="tokenrequestor">TokenRequestor&lt;/h3>
&lt;p>This controller provides the service to create and auto-renew tokens via the &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-request-v1/">&lt;code>TokenRequest&lt;/code> API&lt;/a>.&lt;/p>
&lt;p>It provides a functionality similar to the kubelet&amp;rsquo;s &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection">Service Account Token Volume Projection&lt;/a>.
It was created to handle the special case of issuing tokens to pods that run in a different cluster than the API server they communicate with (hence, using the native token volume projection feature is not possible).&lt;/p>
&lt;p>The controller differentiates between &lt;code>source cluster&lt;/code> and &lt;code>target cluster&lt;/code>.
The &lt;code>source cluster&lt;/code> hosts the gardener-resource-manager pod. Secrets in this cluster are watched and modified by the controller.
The &lt;code>target cluster&lt;/code> &lt;em>can&lt;/em> be configured to point to another cluster. The existence of ServiceAccounts are ensured and token requests are issued against the target.
When the gardener-resource-manager is deployed next to the Shoot&amp;rsquo;s controlplane in the Seed the &lt;code>source cluster&lt;/code> is the Seed while the &lt;code>target cluster&lt;/code> points to the Shoot.&lt;/p>
&lt;h4 id="reconciliation-loop">Reconciliation Loop&lt;/h4>
&lt;p>This controller reconciles secrets in all namespaces in the source cluster with the label: &lt;code>resources.gardener.cloud/purpose: token-requestor&lt;/code>.
See &lt;a href="https://github.com/gardener/gardener/blob/master/example/resource-manager/30-secret-tokenrequestor.yaml">here&lt;/a> for an example of the secret.&lt;/p>
&lt;p>The controller ensures a &lt;code>ServiceAccount&lt;/code> exists in the target cluster as specified in the annotations of the &lt;code>Secret&lt;/code> in the source cluster:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">serviceaccount.resources.gardener.cloud/name: &amp;lt;sa-name&amp;gt;
serviceaccount.resources.gardener.cloud/namespace: &amp;lt;sa-namespace&amp;gt;
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The requested tokens will act with the privileges which are assigned to this &lt;code>ServiceAccount&lt;/code>.&lt;/p>
&lt;p>The controller will then request a token via the &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-request-v1/">&lt;code>TokenRequest&lt;/code> API&lt;/a> and populate it into the &lt;code>.data.token&lt;/code> field to the &lt;code>Secret&lt;/code> in the source cluster.&lt;/p>
&lt;p>Alternatively, the client can provide a raw kubeconfig (in YAML or JSON format) via the &lt;code>Secret&lt;/code>&amp;rsquo;s &lt;code>.data.kubeconfig&lt;/code> field.
The controller will then populate the requested token in the kubeconfig for the user used in the &lt;code>.current-context&lt;/code>.
For example, if &lt;code>.data.kubeconfig&lt;/code> is&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">apiVersion: v1
clusters:
- cluster:
certificate-authority-data: AAAA
server: some-server-url
name: shoot--foo--bar
contexts:
- context:
cluster: shoot--foo--bar
user: shoot--foo--bar-token
name: shoot--foo--bar
current-context: shoot--foo--bar
kind: Config
preferences: {}
users:
- name: shoot--foo--bar-token
user:
token: &lt;span style="color:#a31515">&amp;#34;&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>then the &lt;code>.users[0].user.token&lt;/code> field of the kubeconfig will be updated accordingly.&lt;/p>
&lt;p>The controller also adds an annotation to the &lt;code>Secret&lt;/code> to keep track when to renew the token before it expires.
By default, the tokens are issued to expire after 12 hours. The expiration time can be set with the following annotation:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">serviceaccount.resources.gardener.cloud/token-expiration-duration: 6h
&lt;/code>&lt;/pre>&lt;/div>&lt;p>It automatically renews once 80% of the lifetime is reached or after &lt;code>24h&lt;/code>.&lt;/p>
&lt;p>Optionally, the controller can also populate the token into a &lt;code>Secret&lt;/code> in the target cluster. This can be requested by annotating the &lt;code>Secret&lt;/code> in the source cluster with&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">token-requestor.resources.gardener.cloud/target-secret-name: &lt;span style="color:#a31515">&amp;#34;foo&amp;#34;&lt;/span>
token-requestor.resources.gardener.cloud/target-secret-namespace: &lt;span style="color:#a31515">&amp;#34;bar&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Overall, the TokenRequestor controller provides credentials with limited lifetime (JWT tokens) used by Shoot control plane components running in the Seed
to talk to the Shoot API Server.
Please see the graphic below:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/resource-manager-projected-token-controlplane-to-shoot-apiserver_da4cda.jpg" alt="image">&lt;/p>
&lt;h2 id="webhooks">Webhooks&lt;/h2>
&lt;h3 id="auto-mounting-projected-serviceaccount-tokens">Auto-Mounting Projected &lt;code>ServiceAccount&lt;/code> Tokens&lt;/h3>
&lt;p>When this webhook is activated then it automatically injects projected &lt;code>ServiceAccount&lt;/code> token volumes into &lt;code>Pod&lt;/code>s and all its containers if all of the following preconditions are fulfilled:&lt;/p>
&lt;ol>
&lt;li>The &lt;code>Pod&lt;/code> is NOT labeled with &lt;code>projected-token-mount.resources.gardener.cloud/skip=true&lt;/code>.&lt;/li>
&lt;li>The &lt;code>Pod&lt;/code>&amp;rsquo;s &lt;code>.spec.serviceAccountName&lt;/code> field is NOT empty and NOT set to &lt;code>default&lt;/code>.&lt;/li>
&lt;li>The &lt;code>ServiceAccount&lt;/code> specified in the &lt;code>Pod&lt;/code>&amp;rsquo;s &lt;code>.spec.serviceAccountName&lt;/code> sets &lt;code>.automountServiceAccountToken=false&lt;/code>.&lt;/li>
&lt;li>The &lt;code>Pod&lt;/code>&amp;rsquo;s &lt;code>.spec.volumes[]&lt;/code> DO NOT already contain a volume with a name prefixed with &lt;code>kube-api-access-&lt;/code>.&lt;/li>
&lt;/ol>
&lt;p>The projected volume will look as follows:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">spec:
volumes:
- name: kube-api-access-gardener
projected:
defaultMode: 420
sources:
- serviceAccountToken:
expirationSeconds: 43200
path: token
- configMap:
items:
- key: ca.crt
path: ca.crt
name: kube-root-ca.crt
- downwardAPI:
items:
- fieldRef:
apiVersion: v1
fieldPath: metadata.namespace
path: namespace
&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>The &lt;code>expirationSeconds&lt;/code> are defaulted to &lt;code>12h&lt;/code> and can be overwritten with the &lt;code>--projected-token-mount-expiration-seconds&lt;/code> flag, or with the &lt;code>projected-token-mount.resources.gardener.cloud/expiration-seconds&lt;/code> annotation on a &lt;code>Pod&lt;/code> resource.&lt;/p>
&lt;/blockquote>
&lt;p>The volume will be mounted into all containers specified in the &lt;code>Pod&lt;/code> to the path &lt;code>/var/run/secrets/kubernetes.io/serviceaccount&lt;/code>.
This is the default location where client libraries expect to find the tokens and mimics the &lt;a href="https://github.com/kubernetes/kubernetes/tree/v1.22.2/plugin/pkg/admission/serviceaccount">upstream &lt;code>ServiceAccount&lt;/code> admission plugin&lt;/a>, see &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#serviceaccount-admission-controller">this document&lt;/a> for more information.&lt;/p>
&lt;p>Overall, this webhook is used to inject projected service account tokens into pods running in the Shoot and the Seed cluster.
Hence, it is served from the Seed GRM and each Shoot GRM.
Please find an overview below for pods deployed in the Shoot cluster:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/resource-manager-projected-token-shoot-to-shoot-apiserver_4fdaf3.jpg" alt="image">&lt;/p></description></item><item><title>Docs: Scheduler</title><link>https://gardener.cloud/docs/gardener/concepts/scheduler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/scheduler/</guid><description>
&lt;h1 id="gardener-scheduler">Gardener Scheduler&lt;/h1>
&lt;p>The Gardener Scheduler is in essence a controller that watches newly created shoots and assigns a seed cluster to them.
Conceptually, the task of the Gardener Scheduler is very similar to the task of the Kubernetes Scheduler: finding a seed for a shoot instead of a node for a pod.&lt;/p>
&lt;p>Either the scheduling strategy or the shoot cluster purpose hereby determines how the scheduler is operating.
The following sections explain the configuration and flow in greater detail.&lt;/p>
&lt;h2 id="why-is-the-gardener-scheduler-needed">Why is the Gardener Scheduler needed?&lt;/h2>
&lt;h3 id="1-decoupling">1. Decoupling&lt;/h3>
&lt;p>Previously, an admission plugin in the Gardener API server conducted the scheduling decisions.
This implies changes to the API server whenever adjustments of the scheduling are needed.
Decoupling the API server and the scheduler comes with greater flexibility to develop these components independently from each other.&lt;/p>
&lt;h3 id="2-extensibility">2. Extensibility&lt;/h3>
&lt;p>It should be possible to easily extend and tweak the scheduler in the future.
Possibly, similar to the Kubernetes scheduler, hooks could be provided which influence the scheduling decisions.
It should be also possible to completely replace the standard Gardener Scheduler with a custom implementation.&lt;/p>
&lt;h2 id="algorithm-overview">Algorithm overview&lt;/h2>
&lt;p>The following &lt;strong>sequence&lt;/strong> describes the steps involved to determine a seed candidate:&lt;/p>
&lt;ol>
&lt;li>Determine usable seeds with &amp;ldquo;usable&amp;rdquo; defined as follows:
&lt;ul>
&lt;li>no &lt;code>.metadata.deletionTimestamp&lt;/code>&lt;/li>
&lt;li>&lt;code>.spec.settings.scheduling.visible&lt;/code> is &lt;code>true&lt;/code>&lt;/li>
&lt;li>conditions &lt;code>Bootstrapped&lt;/code>, &lt;code>GardenletReady&lt;/code>, &lt;code>BackupBucketsReady&lt;/code> (if available) are &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Filter seeds:
&lt;ul>
&lt;li>matching &lt;code>.spec.seedSelector&lt;/code> in &lt;code>CloudProfile&lt;/code> used by the &lt;code>Shoot&lt;/code>&lt;/li>
&lt;li>matching &lt;code>.spec.seedSelector&lt;/code> in &lt;code>Shoot&lt;/code>&lt;/li>
&lt;li>having no network intersection with the &lt;code>Shoot&lt;/code>&amp;rsquo;s networks (due to the VPN connectivity between seeds and shoots their networks must be disjoint)&lt;/li>
&lt;li>having &lt;code>.spec.settings.shootDNS.enabled=false&lt;/code> (only if the shoot specifies a DNS domain or does not use the &lt;code>unmanaged&lt;/code> DNS provider)&lt;/li>
&lt;li>whose taints (&lt;code>.spec.taints&lt;/code>) are tolerated by the &lt;code>Shoot&lt;/code> (&lt;code>.spec.tolerations&lt;/code>)&lt;/li>
&lt;li>whose capacity for shoots would not be exceeded if the shoot is scheduled onto the seed, see &lt;a href="#ensuring-seeds-capacity-for-shoots-is-not-exceeded">Ensuring seeds capacity for shoots is not exceeded&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Apply active &lt;a href="#strategies">strategy&lt;/a> e.g., &lt;em>Minimal Distance strategy&lt;/em>&lt;/li>
&lt;li>Choose least utilized seed, i.e., the one with the least number of shoot control planes, will be the winner and written to the &lt;code>.spec.seedName&lt;/code> field of the &lt;code>Shoot&lt;/code>.&lt;/li>
&lt;/ol>
&lt;h2 id="configuration">Configuration&lt;/h2>
&lt;p>The Gardener Scheduler configuration has to be supplied on startup. It is a mandatory and also the only available flag.
&lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-scheduler.yaml">Here&lt;/a> is an example scheduler configuration.&lt;/p>
&lt;p>Most of the configuration options are the same as in the Gardener Controller Manager (leader election, client connection, &amp;hellip;).
However, the Gardener Scheduler on the other hand does not need a TLS configuration, because there are currently no webhooks configurable.&lt;/p>
&lt;h2 id="strategies">Strategies&lt;/h2>
&lt;p>The scheduling strategy is defined in the &lt;em>&lt;strong>candidateDeterminationStrategy&lt;/strong>&lt;/em> of the scheduler&amp;rsquo;s configuration and can have the possible values &lt;code>SameRegion&lt;/code> and &lt;code>MinimalDistance&lt;/code>.
The &lt;code>SameRegion&lt;/code> strategy is the default strategy.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;em>Same Region strategy&lt;/em>&lt;/p>
&lt;p>The Gardener Scheduler reads the &lt;code>spec.provider.type&lt;/code> and &lt;code>.spec.region&lt;/code> fields from the &lt;code>Shoot&lt;/code> resource.
It tries to find a seed that has the identical &lt;code>.spec.provider.type&lt;/code> and &lt;code>.spec.provider.region&lt;/code> fields set.
If it cannot find a suitable seed, it adds an event to the shoot stating, that it is unschedulable.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Minimal Distance strategy&lt;/em>&lt;/p>
&lt;p>The Gardener Scheduler tries to find a valid seed with minimal distance to the shoot&amp;rsquo;s intended region.
The distance is calculated based on the Levenshtein distance of the region. Therefore the region name
is split into a base name and an orientation. Possible orientations are &lt;code>north&lt;/code>, &lt;code>south&lt;/code>, &lt;code>east&lt;/code>, &lt;code>west&lt;/code> and &lt;code>central&lt;/code>.
The distance then is twice the Levenshtein distance of the region&amp;rsquo;s base name plus a correction value based on the
orientation and the provider.&lt;/p>
&lt;p>If the orientations of shoot and seed candidate match, the correction value is 0, if they differ it is 2 and if
either the seed&amp;rsquo;s or the shoot&amp;rsquo;s region does not have an orientation it is 1.
If the provider differs the correction value is additionally incremented by 2.&lt;/p>
&lt;p>Because of this a matching region with a matching provider is always prefered.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>In order to put the scheduling decision into effect, the scheduler sends an update request for the &lt;code>Shoot&lt;/code> resource to
the API server. After validation, the Gardener Aggregated API server updates the shoot to have the &lt;code>spec.seedName&lt;/code> field set.
Subsequently, the Gardenlet picks up and starts to create the cluster on the specified seed.&lt;/p>
&lt;ol start="3">
&lt;li>&lt;em>Special handling based on shoot cluster purpose&lt;/em>&lt;/li>
&lt;/ol>
&lt;p>Every shoot cluster can have a purpose that describes what the cluster is used for, and also influences how the cluster is setup (see &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_purposes/">this document&lt;/a> for more information).&lt;/p>
&lt;p>In case the shoot has the &lt;code>testing&lt;/code> purpose then the scheduler only reads the &lt;code>.spec.provider.type&lt;/code> from the &lt;code>Shoot&lt;/code> resource and tries to find a &lt;code>Seed&lt;/code> that has the identical &lt;code>.spec.provider.type&lt;/code>.
The region does not matter, i.e., &lt;code>testing&lt;/code> shoots may also be scheduled on a seed in a complete different region if it is better for balancing the whole Gardener system.&lt;/p>
&lt;h2 id="seedselector-field-in-the-shoot-specification">&lt;code>seedSelector&lt;/code> field in the &lt;code>Shoot&lt;/code> specification&lt;/h2>
&lt;p>Similar to the &lt;code>.spec.nodeSelector&lt;/code> field in &lt;code>Pod&lt;/code>s, the &lt;code>Shoot&lt;/code> specification has an optional &lt;code>.spec.seedSelector&lt;/code> field.
It allows the user to provide a label selector that must match the labels of &lt;code>Seed&lt;/code>s in order to be scheduled to one of them.
The labels on &lt;code>Seed&lt;/code>s are usually controlled by Gardener administrators/operators - end users cannot add arbitrary labels themselves.
If provided, the Gardener Scheduler will only consider those seeds as &amp;ldquo;suitable&amp;rdquo; whose labels match those provided in the &lt;code>.spec.seedSelector&lt;/code> of the &lt;code>Shoot&lt;/code>.&lt;/p>
&lt;p>By default only seeds with the same provider than the shoot are selected. By adding a &lt;code>providerTypes&lt;/code> field to the &lt;code>seedSelector&lt;/code>
a dedicated set of possible providers (&lt;code>*&lt;/code> means all provider types) can be selected.&lt;/p>
&lt;h2 id="ensuring-seeds-capacity-for-shoots-is-not-exceeded">Ensuring seeds capacity for shoots is not exceeded&lt;/h2>
&lt;p>Seeds have a practical limit of how many shoots they can accommodate. Exceeding this limit is undesirable as the system performance will be noticeably impacted. Therefore, the scheduler ensures that a seed&amp;rsquo;s capacity for shoots is not exceeded by taking into account a maximum number of shoots that can be scheduled onto a seed.&lt;/p>
&lt;p>This mechanism works as follows:&lt;/p>
&lt;ul>
&lt;li>The &lt;code>gardenlet&lt;/code> is configured with certain &lt;em>resources&lt;/em> and their total &lt;em>capacity&lt;/em> (and, for certain resources, the amount &lt;em>reserved&lt;/em> for Gardener), see &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml">/example/20-componentconfig-gardenlet.yaml&lt;/a>. Currently, the only such resource is the maximum number of shoots that can be scheduled onto a seed.&lt;/li>
&lt;li>The &lt;code>gardenlet&lt;/code> seed controller updates the &lt;code>capacity&lt;/code> and &lt;code>allocatable&lt;/code> fields in Seed status with the capacity of each resource and how much of it is actually available to be consumed by shoots. The &lt;code>allocatable&lt;/code> value of a resource is equal to &lt;code>capacity&lt;/code> minus &lt;code>reserved&lt;/code>.&lt;/li>
&lt;li>When scheduling shoots, the scheduler filters out all candidate seeds whose allocatable capacity for shoots would be exceeded if the shoot is scheduled onto the seed.&lt;/li>
&lt;/ul>
&lt;h2 id="failure-to-determine-a-suitable-seed">Failure to determine a suitable seed&lt;/h2>
&lt;p>In case the scheduler fails to find a suitable seed, the operation is being retried with exponential backoff.&lt;/p>
&lt;h2 id="current-limitation--future-plans">Current Limitation / Future Plans&lt;/h2>
&lt;ul>
&lt;li>Azure has unfortunately a geographically non-hierarchical naming pattern and does not start with the continent. This is the reason why we will exchange the implementation of the &lt;code>MinimalDistance&lt;/code> strategy with a more suitable one in the future.&lt;/li>
&lt;/ul></description></item><item><title>Docs: Seed Admission Controller</title><link>https://gardener.cloud/docs/gardener/concepts/seed-admission-controller/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/seed-admission-controller/</guid><description>
&lt;h1 id="gardener-seed-admission-controller">Gardener Seed Admission Controller&lt;/h1>
&lt;p>The Gardener Seed admission controller is deployed by the Gardenlet as part of its seed bootstrapping phase and, consequently, running in every seed cluster.
It&amp;rsquo;s main purpose is to serve webhooks (validating or mutating) in order to admit or deny certain requests to the seed&amp;rsquo;s API server.&lt;/p>
&lt;h2 id="what-is-it-doing-concretely">What is it doing concretely?&lt;/h2>
&lt;h3 id="validating-webhooks">Validating Webhooks&lt;/h3>
&lt;h4 id="unconfirmed-deletion-prevention">Unconfirmed Deletion Prevention&lt;/h4>
&lt;p>As part of Gardener&amp;rsquo;s &lt;a href="https://gardener.cloud/docs/gardener/extensions/overview/">extensibility concepts&lt;/a> a lot of &lt;code>CustomResourceDefinition&lt;/code>s are deployed to the seed clusters that serve as extension points for provider-specific controllers.
For example, the &lt;a href="https://gardener.cloud/docs/gardener/extensions/infrastructure/">&lt;code>Infrastructure&lt;/code> CRD&lt;/a> triggers the provider extension to prepare the IaaS infrastructure of the underlying cloud provider for a to-be-created shoot cluster.
Consequently, these extension CRDs have a lot of power and control large portions of the end-user&amp;rsquo;s shoot cluster.
Accidental or undesired deletions of those resource can cause tremendous and hard-to-recover-from outages and should be prevented.&lt;/p>
&lt;p>Together with the deployment of the Gardener seed admission controller a &lt;code>ValidatingWebhookConfiguration&lt;/code> for &lt;code>CustomResourceDefinitions&lt;/code> and most (custom) resources in the &lt;code>extensions.gardener.cloud/v1alpha1&lt;/code> API group is registered.
It prevents &lt;code>DELETE&lt;/code> requests for those &lt;code>CustomResourceDefinitions&lt;/code> labeled with &lt;code>gardener.cloud/deletion-protected=true&lt;/code>, and for all mentioned custom resources if they were not previously annotated with the &lt;code>confirmation.gardener.cloud/deletion=true&lt;/code>.
This prevents that undesired &lt;code>kubectl delete &amp;lt;...&amp;gt;&lt;/code> requests are accepted.&lt;/p>
&lt;h3 id="mutating-webhooks">Mutating Webhooks&lt;/h3>
&lt;p>The admission controller endpoint &lt;code>/webhooks/default-pod-scheduler-name/gardener-kube-scheduler&lt;/code> mutates &lt;code>pods&lt;/code> and adds &lt;code>gardener-kube-scheduler&lt;/code> to &lt;code>.spec.scheduleName&lt;/code>.&lt;/p>
&lt;p>When &lt;code>SeedKubeScheduler&lt;/code> feature gate is enabled, all control plane components are mutated. The scheduler scores &lt;code>Nodes&lt;/code> with most resource usage higher than the rest, resulting in greater resource utilization.&lt;/p></description></item></channel></rss>